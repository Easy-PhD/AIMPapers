<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ASOC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="asoc">ASOC - 1235</h2>
<ul>
<li><details>
<summary>
(2025). Detail-aware semantic segmentation network for brain tumor MRI images combining multi-frequency directional filtering and lifting wavelets. <em>ASOC</em>, <em>185</em>, 113969. (<a href='https://doi.org/10.1016/j.asoc.2025.113969'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors segmentation in Magnetic Resonance Imaging (MRI) images poses significant challenges owing to the uncertain location and size of the tumors, the difficulty in describing their boundaries, and the fuzzy demarcation of diseased tissues. Although U-Net and its recent variants have emerged as leading models for semantic segmentation in medical imaging, they still face structural limitations. These limitations cause the erosion of detail information during downsampling and poor performance in segmenting small lesions when handling targets of varying sizes, indicating a lack of detail handling capability. To counteract these issues, we designed a segmentation model that enhances detail features using frequency information. To reduce the loss of feature information during downsampling, we developed a downsampling module based on lifting wavelets. By lifting wavelets to group and integrate features according to frequency from high to low, we reduce feature resolution while enhancing information transmission and minimizing feature information loss. In our designed multi-frequency directional filtering edge feature extraction module, we extract low-frequency and high-frequency features and construct a dual-channel multi-directional filtering combination. This combination extracts directional information from low-frequency and high-frequency features separately, increasing the multi-angle directional information of the features and enriching the detailed information such as direction and position within the features. On the BraTS2018, BraTS2020, and BraTS2024 brain tumor datasets, our model demonstrated optimal results compared to 14 other advanced models. The average Dice Similarity Coefficients are 78.48 %, 79.80 %, and 74.35 %, while the 95th percentile Hausdorff Distances are 5.75, 6.60, and 7.72. Our code link is https://github.com/Eric-H8/BraTS_Seg_Model .},
  archive      = {J_ASOC},
  author       = {Xin Hua and Zhijiang Du and Hongjian Yu and Zibo Li and Qiaohui Lu and Hui Zhao},
  doi          = {10.1016/j.asoc.2025.113969},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113969},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detail-aware semantic segmentation network for brain tumor MRI images combining multi-frequency directional filtering and lifting wavelets},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DT and LLM driven intelligent maintenance system for L-DED and DAG-based LLM fault diagnosis evaluation framework. <em>ASOC</em>, <em>185</em>, 113942. (<a href='https://doi.org/10.1016/j.asoc.2025.113942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metal additive manufacturing (AM) has revolutionized industries such as aerospace and automotive manufacturing due to its ability to rapidly prototype complex structures. Laser Directed Energy Deposition (L-DED) is a key AM technique, offering high deposition rates and superior mechanical properties. However, the inherent complexity and high cost of L-DED equipment demand reliable maintenance management to minimize downtime. Traditional maintenance approaches struggle to keep pace with escalating production demands and to cope with growing equipment complexity. To address this, we propose a dual-driven intelligent maintenance system for L-DED, integrating Digital Twins (DT) and Large Language Models (LLMs). The system features a comprehensive DT framework that synchronizes the virtual entity with the physical one in real time, it also incorporates an intelligent maintenance Q&A assistant powered by Retrieval-Augmented Generation (RAG), leveraging L-DED maintenance knowledge bases to provide accurate operational support. Additionally, we propose a Directed Acyclic Graphs (DAG)-based framework to assess LLMs’ ability to guide users through complete fault diagnosis. Our work aims to enhance the reliability and efficiency of L-DED maintenance through advanced digital technologies, ultimately improving productivity and reducing downtime in additive manufacturing.},
  archive      = {J_ASOC},
  author       = {Jian Tang and Shitong Peng and Jianan Guo and Danya Song and Dongna Gao and Weiwei Liu and Fengtao Wang},
  doi          = {10.1016/j.asoc.2025.113942},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113942},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DT and LLM driven intelligent maintenance system for L-DED and DAG-based LLM fault diagnosis evaluation framework},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new large model with multi-scale feature fusion for fault diagnosis based on unified time series model. <em>ASOC</em>, <em>185</em>, 113941. (<a href='https://doi.org/10.1016/j.asoc.2025.113941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large model technology exemplified by large language models has been applied in the field of industrial fault diagnosis. However, existing large models are optimized for specific equipment types and have yet to fully exploit the potential of time-series monitoring data to enable widespread application across diverse mechanical equipment in various industrial scenarios. To address this challenge, a fault diagnosis large model (UniTS-FD) is designed based on unified time series model (UniTS). First, a multi-scale feature fusion backbone network is developed based on UniTS backbone to capture general mechanical fault features. Second, the fault classification head integrates the Pearson correlation coefficient to assess the similarity of class information within linear space for enabling adaptive classification. Third, P-LoRA fine-tuning approach incorporating LoRA and prompt technology is proposed to fine-tune the fault classification head, which enhances the generalization ability of the UniTS-FD model for fault diagnosis tasks of various mechanical equipment. Finally, the UniTS-FD model is pre-trained on 11 fault datasets and fine-tuning experiments were conducted on four different fault datasets to achieve cross-machine fault diagnosis. Experimental results demonstrate the effectiveness of the UniTS-FD in fault diagnosis tasks.},
  archive      = {J_ASOC},
  author       = {Zhiwei Zhang and Chengbin Wei and Weimin Zhang and Long Wen},
  doi          = {10.1016/j.asoc.2025.113941},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113941},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new large model with multi-scale feature fusion for fault diagnosis based on unified time series model},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards agentic smart design: An industrial large model-driven human-in-the-loop agentic workflow for geometric modelling. <em>ASOC</em>, <em>185</em>, 113920. (<a href='https://doi.org/10.1016/j.asoc.2025.113920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agentic workflows, powered by Industrial Large Models (ILMs), represent a significant development emphasizing collaboration between humans and intelligent systems. This paper presents a structured perspective on the role of agentic workflows for smart design and manufacturing, grounded in integrating ILMs. We define an agentic workflow as a labeled Activity-on-Vertex (AOV) graph, where each node represents a functionally closed subtask and is executed by an ILM-based agent, a human operator, or an automated system. This formalism supports analyzable, modular, and hybrid execution, offering a foundation for modeling complex, mixed-initiative processes in manufacturing environments. To support real-world deployment, we introduce a set of reusable agentic workflow patterns that describe how ILM agents perceive, plan, and act in coordination with other components. Besides, a proof-of-concept case study illustrates the practical application of the human-in-the-loop framework through the agentic generation of CAD models. The study covers task decomposition, workflow implementation, and benchmarking, providing evidence for the feasibility of agentic workflows. Building upon these findings, this work contributes to advancing the development and application of agentic workflows in smart manufacturing contexts.},
  archive      = {J_ASOC},
  author       = {Keyou Zheng and Yuanwei Zhong and Xuyang Su and Jiewu Leng and Qiang Liu and Xin Chen},
  doi          = {10.1016/j.asoc.2025.113920},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113920},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards agentic smart design: An industrial large model-driven human-in-the-loop agentic workflow for geometric modelling},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time traffic flow optimization using large language models and reinforcement learning for smart urban mobility. <em>ASOC</em>, <em>185</em>, 113917. (<a href='https://doi.org/10.1016/j.asoc.2025.113917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of metropolitan populations causes transportation network congestion, which increases fuel usage, travel time, and environmental damage. Traditional traffic management systems (TMS) seldom handle these issues in real time. Recently developed Large Language Models (LLMs), especially those using Reinforcement Learning (RL), may enhance urban transportation systems. Traffic management technology's real-time flexibility and shifting congestion patterns provide improved potential. Traditional approaches cannot estimate traffic flow or adapt to urban settings. A strong AI-driven method is needed to improve urban mobility and traffic flow. This paper introduces the LLM-RL Traffic Optimization Framework (LLM-RL-TOF). LLMs analyze real-time traffic data and give predictive insights in this context. Due to these new insights, the RL algorithm can improve traffic flow in real time and reduce congestion via dynamic traffic management. IoT sensors and urban traffic cameras capture real-time traffic data, including traffic volume and incidents. This data helps the LLM estimate bottlenecks, accidents, and traffic congestion. An RL agent uses LLM outputs to adjust traffic signal timing and suggest alternate routes. With real-time alternatives, traffic flow and urban mobility may be optimized. The junction throughput rate rose 17.5 %, the queue length accumulation index fell 22.3 %, and the average vehicle delay fell 18.6 %. The decrease in average vehicle delay enabled all these gains.},
  archive      = {J_ASOC},
  author       = {Arvind R. Singh and Muhammad Wasim Abbas Ashraf and Rajkumar Singh Rathore and Bin Li and M.S. Sujatha},
  doi          = {10.1016/j.asoc.2025.113917},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113917},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time traffic flow optimization using large language models and reinforcement learning for smart urban mobility},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wind turbine blades defect detection based on global and local attention with multi-feature fusion. <em>ASOC</em>, <em>185</em>, 113914. (<a href='https://doi.org/10.1016/j.asoc.2025.113914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind turbine blades are prone to small-scale defects—such as cracks, corrosion, and contamination—during long-term operation. Accurate detection of these defects is essential for ensuring the safety and efficiency of wind power systems. However, small-object detection remains challenging due to limited feature representation and weak discriminative cues. To address this, an enhanced YOLOX-s-based framework called Global-Frequency Dual-aware YOLOX (GFD-YOLOX) is proposed. GFD-YOLOX introduces three main improvements. First, the Path Aggregation Feature Pyramid Network (PAFPN) in the neck is replaced with Dual-Frequency Fused Bidirectional Feature Pyramid Network (DFF-BiFPN) to strengthen multi-scale contextual representation. Second, the backbone bottleneck is redesigned with a lightweight structure, improving computational efficiency and convergence speed. Third, a Hierarchical Frequency-Adaptive Fusion (HFAF) module is integrated to enhance cross-scale feature interaction by combining fine-grained and global information. On the self-constructed WTBlade-Defect dataset (3570 annotated images, five defect types: corrosion, hide-craze, surface-eye, thunderstrike, dirt), GFD-YOLOX achieves mAP@0.5 and mAP@0.5:0.95 scores of 94.5 % and 68.9 %, respectively, with 44.3 FPS inference—improving by 13.6 % and 14.4 % over state-of-the-art models. On the public dataset of Ashley Foster et al., it achieves 94.8 % and 69.3 %, with gains of 10.4 % and 10.9 %. These results demonstrate that GFD-YOLOX delivers substantial accuracy gains while maintaining real-time speed and strong cross-dataset generalization, indicating high potential for deployment in operational wind turbine inspection systems.},
  archive      = {J_ASOC},
  author       = {Dandan Liu and Mingjie Liu},
  doi          = {10.1016/j.asoc.2025.113914},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113914},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Wind turbine blades defect detection based on global and local attention with multi-feature fusion},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval multi-feature sparse transformer-CNN: A synergistic approach to precise and efficient spatio-temporal wind speed interval-value prediction. <em>ASOC</em>, <em>185</em>, 113910. (<a href='https://doi.org/10.1016/j.asoc.2025.113910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and reliable wind speed prediction is critical for stabilizing wind power integration. However, current methods are limited by accuracy and stability issues, hindering their large-scale application in wind farms. To overcome these problems, this study innovatively proposes an IMFSformer-CNN model integrating three core components. First, the spatio-temporal and multi-factor feature extraction technology comprehensively captures the spatio-temporal patterns and complex dependencies of wind speed dynamics, incorporating multiple factors such as meteorological variables and spatial correlation. Second, the multi-feature sparse attention mechanism reduces computational complexity by combining sparse attention with multi-feature attention, enhancing representation ability and scalability for precise interval value prediction. Finally, the enhanced interval spatio-temporal prediction fusion model combines the global dependency modeling capabilities of the improved Transformer architecture with the local receptive field advantages of CNN. This hybrid design facilitates the simultaneous capture of both macro-scale atmospheric patterns and micro-scale wind speed fluctuations. The model achieved prediction interval coverage probabilities of 0.921 and 0.899, and coverage width criteria of 1.493 and 3.776, outperforming other models on both datasets. This significantly enhances accuracy and practical value for wind farm cluster forecasting, supporting more reliable and efficient wind energy integration into power grids.},
  archive      = {J_ASOC},
  author       = {Weiyi Jiang and Jujie Wang and Xuecheng He},
  doi          = {10.1016/j.asoc.2025.113910},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113910},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval multi-feature sparse transformer-CNN: A synergistic approach to precise and efficient spatio-temporal wind speed interval-value prediction},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-granularity spatiotemporal fusion neural network with denoising reconstruction for human motion prediction. <em>ASOC</em>, <em>185</em>, 113909. (<a href='https://doi.org/10.1016/j.asoc.2025.113909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion prediction has significant application value in many fields. However, existing methods often fail to fully capture the spatial relationships between joints and the temporal flow of information when modeling complex spatiotemporal dependencies. Additionally, these methods are prone to overfitting dominant features while neglecting other important aspects, and struggle with perceiving contour features effectively. To address these issues, this study introduces a novel encoder-decoder framework. The encoder generates a dual-layer adaptive adjacency matrix using a distance partition strategy to parameterize joint relationships, while incorporating a gating mechanism to control the temporal flow of information. The decoder then employs separate spatiotemporal attention modules to decode temporal and spatial features independently. These features are subsequently reconstructed through a spatiotemporal fusion strategy, effectively decoupling and modeling complex spatiotemporal dependencies. To address the issue of overfitting to dominant features, we introduce a denoising reconstruction strategy that allows the model to learn richer combinations of spatiotemporal features under multiple constraints. Furthermore, a multi-granularity information adaptive fusion module is incorporated to achieve adaptive fusion of both local and contour features. Experimental results across several benchmark datasets demonstrate that our method significantly outperforms the state-of-the-art approaches, showcasing its effectiveness in human motion prediction tasks.},
  archive      = {J_ASOC},
  author       = {Yong Li and Linfeng Zhu and Haofei Xie and Xinchang Yi},
  doi          = {10.1016/j.asoc.2025.113909},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113909},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-granularity spatiotemporal fusion neural network with denoising reconstruction for human motion prediction},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of the risk level of saltwater intrusion in the modaomen waterway of the pearl river estuary based on the deep temporal clustering model. <em>ASOC</em>, <em>185</em>, 113897. (<a href='https://doi.org/10.1016/j.asoc.2025.113897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, extreme events caused by global climate change have intensified the phenomenon of saltwater intrusion (SWI) in estuaries. The nonlinear and non-stationary characteristics of estuarine SWI have led to an exponential decline in the timeliness of traditional regression prediction models, making it difficult to meet the operational needs of SWI forecasting. To address this, this study proposed a technical framework for SWI risk level forecasting based on temporal clustering, with its core innovation lying in algorithmic improvements for accurately characterizing complex disaster systems. The key challenges in forecasting SWI risk levels involved capturing the dynamic nonlinear relationships between multidimensional disaster factors (such as runoff, tide level, and wind) and SWI severity, as well as enhancing feature discriminability in label-limited scenarios. Accordingly, this study optimized algorithms through dual-path supervised and unsupervised learning: In the supervised learning framework, LightGBM, RF, XGBoost, and Extra trees were introduced as base learners into the Deep Forest (DF) model. The complementary feature-space partitioning of diverse learners was leveraged to improve the model’s ability to distinguish risk -level boundaries, achieving an average performance gain of 7.8 %. In the unsupervised learning framework, discriminative regularization was incorporated into the Extreme Learning Machine-Autoencoder (ELM-AE) model. By forcing features of samples from the same class to cluster toward the class center, the model’s feature separability for rare events (e.g., severe SWI) was enhanced, leading to an average performance improvement of 11 %. Finally, the optimal model was used to extract dynamic evolution patterns between multidimensional disaster factors and SWI risk levels, with interpretability analysis conducted for real-world forecasting. Notably, upstream flow sequences exhibited high distinguishability between no-SWI and severe-SWI, while mild and moderate SWI showed similar flow patterns, with tidal sequences being the primary differentiator. The algorithmic advancements not only enhanced the accuracy and efficiency of SWI forecasting but also provided a generalizable framework for risk classification in nonlinear hydrological systems.},
  archive      = {J_ASOC},
  author       = {Qingqing Tian and Hongyu Yang and Yu Tian and Peiyao Weng},
  doi          = {10.1016/j.asoc.2025.113897},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113897},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction of the risk level of saltwater intrusion in the modaomen waterway of the pearl river estuary based on the deep temporal clustering model},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive learning with mirror-task for multimodal sentiment analysis. <em>ASOC</em>, <em>185</em>, 113896. (<a href='https://doi.org/10.1016/j.asoc.2025.113896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Task Learning (MTL) in Multimodal Sentiment Analysis (MSA) involves implementing various parameter-sharing strategies among tasks. Currently, MSA primarily focuses on hard parameter-sharing mechanisms based on encoder sharing, while soft parameter-sharing is often neglected. To explore a reasonable combination of soft and hard mechanisms in MSA and optimize multimodal representations, along with multimodal contrastive learning, we propose D 3 MSA. It consists of D ouble network (primaryNet and MirrorNet), D ouble parameter-sharing strategies and D ouble contrastive learning modes for multimodal sentiment analysis. D 3 MSA utilizes hard-sharing to consolidate correlations between positive samples of intra-sample contrastive learning. In soft-sharing, we propose a pre-trained MirrorNet (MN) that generates negative samples by the learned inverse distributions. This optimizes the feature space of negative samples. MN interacts with the MSA task through soft-sharing during inter-sample contrastive learning. Experimental results demonstrate that our proposed method can achieve advanced performance on the CMU-MOSI and CMU-MOSEI datasets with lightweight training that requires only a small number of parameters.},
  archive      = {J_ASOC},
  author       = {Hang Shi and Lianmin Zhou and Yuanyuan Pu and Zhengpeng Zhao and Jinjing Gu and Dan Xu},
  doi          = {10.1016/j.asoc.2025.113896},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113896},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive learning with mirror-task for multimodal sentiment analysis},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A nearest-best neighbors optimization algorithm for constrained multimodal multiobjective problems. <em>ASOC</em>, <em>185</em>, 113895. (<a href='https://doi.org/10.1016/j.asoc.2025.113895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained multimodal multiobjective problems (CMMOPs) have multiple equivalent constrained Pareto optimal sets in the decision space corresponding to the identical constrained Pareto front in the objective space. The key to solving CMMOPs is how to balance feasibility, convergence, and diversity of solutions in both the decision and objective spaces. In view of this, this paper proposes a Nearest-Best neighbors optimization algorithm with constraint-based fitness (NBNOA) to solve CMMOPs. First, a constraint-based fitness assignment scheme is designed to assign specific fitness values to individuals in the population. Then, the Nearest-better-neighbor clustering method is adopted to identify the nearest-better neighbor and best neighbor of each individual according to the specific fitness values. On this basis, a Nearest-Best neighbors guided strategy is developed to guide the search direction of individuals, striking a better balance between exploration and exploitation capabilities. Moreover, a CDP-density elite selection mechanism is constructed to obtain feasible Pareto optimal solutions with higher precision and better diversity. Extensive experiments on two CMMOPs test suites demonstrated that the proposed NBNOA significantly outperforms nine state-of-the-art algorithms. Notably, NBNOA ranks first among all ten algorithms and achieves the best values for 23 out of 31 benchmark functions regarding the reciprocal of Pareto sets proximity and inverted generational distance. Furthermore, NBNOA is applied to a real-world CMMOP, verifying its effective practical application capability. Additionally, NBNOA is tested on two high-dimensional constrained multiobjective optimization problems test suites, further proving its competitive performance in solving complex problems.},
  archive      = {J_ASOC},
  author       = {Xuming Han and Ting Zhou and Limin Wang and Yali Chu},
  doi          = {10.1016/j.asoc.2025.113895},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113895},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A nearest-best neighbors optimization algorithm for constrained multimodal multiobjective problems},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency decomposition and patch modeling framework for time-series forecasting. <em>ASOC</em>, <em>185</em>, 113890. (<a href='https://doi.org/10.1016/j.asoc.2025.113890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is widely applied across diverse fields, including finance, transportation, and energy, and has made significant contributions in these areas. However, in real-world applications, time series data can be complex and dynamic. Current methodologies still encounter several challenges in managing high-dimensional data, extracting intricate features, and making long-term forecasts. In this study, we propose a Frequency Decomposition and Patch Modeling Framework (FPF). Our FPF consists of the Frequency Domain Decomposition Block (FDB) and the Dual Patch Modeling Block (DPMB). DPMB consists of Patch Enhancement Block and Patch Mixing Block. First, FDB transforms the input sequence to the frequency domain through the Fast Fourier Transform and designs frequency masks to decompose the data into high-frequency and low-frequency components, to extract fast-changing patterns and trend information respectively. Subsequently, DPMB divides the components into patches, where the high-frequency components are modeled by MLP-based Patch Enhancement Block to capture local features, and the low-frequency components are modeled by Transformer-based Patch Mixing Block to capture global dependencies and cross-patch correlations. We conducted comprehensive experiments using seven real-world time series forecasting datasets, including ETT, Traffic, Electricity, and Weather. The findings indicate that this method demonstrates superior performance in the field of time series forecasting.},
  archive      = {J_ASOC},
  author       = {Denghui Xu and Hua Wang and Fan Zhang},
  doi          = {10.1016/j.asoc.2025.113890},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113890},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Frequency decomposition and patch modeling framework for time-series forecasting},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Density peak clustering algorithm based on boundary elimination and backbone construction. <em>ASOC</em>, <em>185</em>, 113880. (<a href='https://doi.org/10.1016/j.asoc.2025.113880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peak clustering (DPC) is an effective clustering algorithm, but it still has some problems and faces some challenges. For instance, it cannot identify the variable density datasets, the assignment strategy is easy to produce domino phenomenon, and the clustering results of DPC and its improved algorithms are easily affected by the intersection points between clusters. To solve these problems, in this paper, we propose a novel density peak clustering algorithm based on boundary elimination and backbone construction, called BEBC-DPC. A new local density is defined based on the natural neighbor search, and the boundary degree is defined by the position relationship between each point and its neighbors, which accurately describes the local distribution information of the point. The boundary points of clusters are eliminated by fusing the density and the boundary degree, which reduces the influence of the intersection points on the cluster division. In addition, the cluster backbone construction method based on representative points and representative sets is proposed. The density relationship among non-boundary points is used to form representative sets, and the similarity between representative sets is used to construct the cluster backbones, which can effectively describe the overall distribution structure characteristics of the clusters. Moreover, the adjacency degree of each boundary point is defined by using the neighbor information and distance information, and the boundary points are gradually assigned to the most appropriate cluster backbone based on it to complete the clustering. Finally, sufficient experiments are performed on synthetic, UCI and image datasets, and the proposed BEBC-DPC is compared with DPC and its improved algorithms. Experimental results show the effectiveness of the proposed BEBC-DPC on various types of datasets.},
  archive      = {J_ASOC},
  author       = {Zhizhong Zhao and Sugen Chen and Cong Hu},
  doi          = {10.1016/j.asoc.2025.113880},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113880},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Density peak clustering algorithm based on boundary elimination and backbone construction},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable type 2 fuzzy Min–Max neural networks for pattern classification. <em>ASOC</em>, <em>185</em>, 113875. (<a href='https://doi.org/10.1016/j.asoc.2025.113875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fuzzy Min–Max (FMM) algorithm is a powerful classification method capable of handling non-linear class boundaries and making both hard and soft decisions while learning from online data. However, it faces significant challenges, including sensitivity to the expansion coefficient, information loss during the contraction stage, and the overlap problem. To address these limitations, we propose a Reliable Type-2 Fuzzy Min–Max (RT2FMM) algorithm, which incorporates type-2 fuzzy logic to consider hyperbox uncertainty and effectively resolve the overlap problem. By assigning distinct certainties to overlapping regions, RT2FMM eliminates the need for the contraction stage and the overlap test. Additionally, we introduce weighted factors for hyperboxes, which enhances the reliability of membership values and models their mutual effects. Our comprehensive experimental evaluation across twenty datasets demonstrates that RT2FMM significantly outperforms existing FMM-based models in terms of robustness and accuracy. The Friedman test further confirms the superior performance of RT2FMM compared to commonly used classifiers, highlighting its potential as a robust solution for complex classification tasks.},
  archive      = {J_ASOC},
  author       = {Ali Nik-Khorasani and Mohammad-R. Akbarzadeh-T.},
  doi          = {10.1016/j.asoc.2025.113875},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113875},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reliable type 2 fuzzy Min–Max neural networks for pattern classification},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fortifying vision models: A comprehensive survey of defences against adversarial examples. <em>ASOC</em>, <em>185</em>, 113874. (<a href='https://doi.org/10.1016/j.asoc.2025.113874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence and Machine learning (ML) have seen many advancements in the past two decades. It has led to the creation of several techniques, including Deep Neural Networks (DNN), Convolution Neural Networks (CNN), Autoencoders, Generative Adversarial Networks (GAN) and Diffusion models. These techniques have been applied to various real-world applications, such as self-driving cars, medical diagnosis and voice assistants. Despite these advancements, a carefully crafted input can fool the ML model. Such attacks are known as adversarial examples. It is a serious threat to safety critical systems. This survey provides a comprehensive review of defences against adversarial examples by tracing their evolution from early empirical methods to more principled, theoretically grounded approaches. We systematically categorise defences based on their underlying mechanisms. In addition to surveying state-of-the-art techniques, we spotlight emerging trends such as generative defences and diffusion-based purification. Finally, we identify persistent vulnerabilities and outline promising directions for future research towards building truly resilient vision models. This work aims to equip researchers and practitioners with a deep understanding of current defences and inspire innovation in adversarial robustness for the next generation of vision applications.},
  archive      = {J_ASOC},
  author       = {Siddheshwar Kumar and Shashank Srivastava and Shashwati Banerjea},
  doi          = {10.1016/j.asoc.2025.113874},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113874},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fortifying vision models: A comprehensive survey of defences against adversarial examples},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AOT-PixelNet: Lightweight and interpretable detection of forged images via adaptive orthogonal transform. <em>ASOC</em>, <em>185</em>, 113873. (<a href='https://doi.org/10.1016/j.asoc.2025.113873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative image detection faces persistent challenges in terms of generalization and interpretability, limiting its reliability in complex scenarios. To address these issues, we propose AOT-PixelNet, a lightweight and interpretable detection framework that integrates an Adaptive Orthogonal Transform (AOT) module with a streamlined 1 × 1 convolution-based PixelNet architecture. The AOT module leverages diverse orthogonal transforms, such as FFT and DCT, to extract informative frequency-domain features, thereby enhancing sensitivity to medium- and high-frequency artifacts. Meanwhile, PixelNet minimizes parameter count (only 0.98 million) while effectively capturing cross-channel inconsistencies and mitigating overfitting. Experimental evaluations on multiple unseen GAN and diffusion-based datasets demonstrate that AOT-PixelNet achieves superior performance with minimal computational cost. Specifically, it outperforms the NPR method by 0.6% and 11.76% on the ForenSynths and GenImage datasets, respectively, validating the framework’s robustness, effectiveness, and interpretability.},
  archive      = {J_ASOC},
  author       = {Dengtai Tan and Deyi Yang and Boao Tan and Chengyu Niu and Yang Yang and Shichao Li},
  doi          = {10.1016/j.asoc.2025.113873},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113873},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AOT-PixelNet: Lightweight and interpretable detection of forged images via adaptive orthogonal transform},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Status prediction using attentive and cascaded deep network by fault tolerant and priority-based scheduling for load balancing in cloud. <em>ASOC</em>, <em>185</em>, 113872. (<a href='https://doi.org/10.1016/j.asoc.2025.113872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud facilitates the user to complete their work utilizing the cost strategy of pay-as-you-go, which is based on the consumed Virtual Machine (VM) hours. Thus, the scheduler must offer the highest throughput to attain efficient allocation of resources in the cloud paradigm. Cloud services are dependent on characteristics such as fault tolerance, security, scalability, and availability. Hence, an effective scheduler is necessary to arrange the scheduling tasks and adjust the server loads. Typically, a load-balancing task focuses on detecting the overloaded and under-loaded nodes and adjusting the load between them. When considering the significant role of fault-tolerance in load-balancing algorithms, it seems to suffer from poor organization and a lack of in-depth experiments in this sector. This paper proposes a new task for the load-balancing operation. Initially, task scheduling is performed where the fault tolerance and the priority-aided scheduling approach are adopted. Furthermore, resource optimization is carried out in the scheduling task using Randomly Improved Electric Fish Optimization (RIEFO). To validate the load balancing operation, several multi-objective functions such as resource utilization, delay, time, makespan, active servers, throughput, success rate, fault tolerance rate, and energy consumption are derived. Moreover, because of the system’s dynamic environment, the status of the server varies simultaneously. The server status prediction is significant in allocating the tasks to the server or the VM resources. Thus, the Attention-based Cascaded Residual Bidirectional Long Short-Term Memory (ACRes-BiLSTM) is employed to predict the server status before performing the resource allocation. Finally, the tasks are scheduled effectively using the predicted server status. The performance is estimated using numerous performance metrics.},
  archive      = {J_ASOC},
  author       = {Gudivada Lokesh and Kasarapu Ramani and K.K. Baseer},
  doi          = {10.1016/j.asoc.2025.113872},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113872},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Status prediction using attentive and cascaded deep network by fault tolerant and priority-based scheduling for load balancing in cloud},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid deep learning framework for detecting grape leaf diseases by integrating ResNeSt-50 and CBAM attention into DeepLabv3 +. <em>ASOC</em>, <em>185</em>, 113871. (<a href='https://doi.org/10.1016/j.asoc.2025.113871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grape leaf diseases, such as black rot, powdery mildew, and downy mildew, pose a significant threat to global viticulture, leading to substantial yield losses and reduced fruit quality. Early and accurate identification of these diseases is essential for precision agriculture and sustainable crop management. This study presents a comprehensive comparison of traditional and deep learning-based image segmentation methods for detecting grape leaf lesions. A series of classical segmentation techniques, including Mean Shift, Fuzzy C-Means (FCM), Normalized Cut, K-Means, and Fuzzy K-Means (FKM), were evaluated alongside an advanced DeepLabv3 + model. The baseline DeepLabv3 + architecture was further enhanced by integrating a ResNeSt-50 backbone with various attention mechanisms, including Squeeze-and-Excitation (SE) Block, Convolutional Block Attention Module (CBAM), Bottleneck Attention Module (BAM), Self-Attention, and Dual Attention Network (DANet). Among all models, DeepLabv3 + with ResNeSt-50 and CBAM achieved the highest performance, attaining 98.2 % accuracy, 97.1 % precision, 96.7 % recall, 96.6 % mean Intersection over Union (mIoU), and a 96.8 % Dice Score. The results demonstrate that attention-augmented deep networks significantly outperform classical methods, especially in handling complex lesion structures under diverse environmental conditions. While traditional algorithms remain useful in resource-constrained scenarios, deep learning models, particularly those enhanced with spatial and channel-wise attention, offer greater accuracy and robustness, making them ideal for integration into intelligent agricultural platforms such as drones, mobile scanners, and automated disease monitoring systems. Future work will focus on incorporating temporal and multimodal data, expanding dataset diversity, and optimizing lightweight models for real-time deployment on edge devices.},
  archive      = {J_ASOC},
  author       = {Kittipol Wisaeng},
  doi          = {10.1016/j.asoc.2025.113871},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113871},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid deep learning framework for detecting grape leaf diseases by integrating ResNeSt-50 and CBAM attention into DeepLabv3 +},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A stochastic proportional–differential TLBO algorithm and its applications for wafer transfer finger. <em>ASOC</em>, <em>185</em>, 113870. (<a href='https://doi.org/10.1016/j.asoc.2025.113870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Teaching-Learning-Based optimization (TLBO) algorithm, which includes teacher phase and learner phase, is a widely used method for global optimization. However, TLBO will experience premature convergence and get stuck in local optimum when faced with complex optimization challenges. Especially when tackling complex problems in practical engineering applications, which involves multiple variables and numerous constraints. To address this issue, a new variant termed Stochastic Proportional–Differential TLBO (SPD-TLBO) has been developed. The SPD phase allows students to learn not only from the current population but also from previous stochastic errors and their generation differences using adaptive random operators. By incorporating an SPD operator into the original TLBO framework, the algorithm’s search diversity is enhanced, reducing the likelihood of premature convergence to local optimum. The experimental results conducted at the IEEE Conference on Evolutionary Computation 2014 (CEC 2014) indicated that the proposed SPD-TLBO algorithm achieved an effective balance between exploration and exploitation capabilities. Specifically, the SPD-TLBO algorithm achieves the highest ranking in 21 out of 30 cases (70%) for 30-dimensional problems and 18 out of 30 cases (60%) for 50-dimensional problems. Statistical tests and convergence analyses show that the SPD-TLBO algorithm outperforms other algorithms in solving global optimization problems. Additionally, when applied to engineering optimization problems, the SPD-TLBO algorithm shows significant advantages over other algorithms. Therefore, the SPD-TLBO algorithm is further applied to optimize the structure of a wafer transfer finger in semiconductor manufacturing.},
  archive      = {J_ASOC},
  author       = {Jinfeng Sun and Yunlang Xu and Haibo Zhou},
  doi          = {10.1016/j.asoc.2025.113870},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113870},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A stochastic proportional–differential TLBO algorithm and its applications for wafer transfer finger},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of the health effects of COVID using hybrid classifier with attention-based LSTM-deep temporal convolution network. <em>ASOC</em>, <em>185</em>, 113867. (<a href='https://doi.org/10.1016/j.asoc.2025.113867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus Disease 2019 (COVID-19) is an infectious illness that affects both humans and animals. Individuals infected with COVID-19 are prone to lung complications during the recovery phase . Radiography and Computed Tomography (CT) are the most commonly used methods for diagnosing lung-related diseases. The primary aim of this paper is to assess the impact of COVID-19 on patients’ lungs, heart, and blood sugar levels using a deep learning-based approach. Initially, data related to the heart, blood sugar levels, and lungs of COVID-19-infected individuals are collected. From this dataset, three types of features are extracted. Deep features are obtained using Iterated Dilated Convolutional Neural Networks (IDCNN). From these deep features, which are obtained from the IDCNN, the optimal weighted features are derived by implementing the Hybrid Dolphin Pod Cuttlefish Optimization (HDPCO) algorithm. Subsequently, the HDPCO algorithm is also employed for optimal feature extraction. In addition, dimensionality reduction is performed using Principal Component Analysis (PCA). These three sets of features from the IDCNN, HDPCO, and PCA, are then fused into a single feature set . This fused feature set is fed into a hybrid classifier composed of a Deep Temporal Convolutional Network (DTCN) and an Attention-based Long Short-Term Memory (ALSTM) network . The classifier parameters are optimized using the HDPCO algorithm. The output from the hybrid classifier provides the final prediction result. Experimental results demonstrate that the proposed COVID-19 impact prediction model significantly outperforms existing models in terms of prediction accuracy and efficiency.},
  archive      = {J_ASOC},
  author       = {Sadanandam Kalvala and B. Baranidharan},
  doi          = {10.1016/j.asoc.2025.113867},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113867},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction of the health effects of COVID using hybrid classifier with attention-based LSTM-deep temporal convolution network},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum fuzzy logic for edge detection: A demonstration on NISQ hardware. <em>ASOC</em>, <em>185</em>, 113866. (<a href='https://doi.org/10.1016/j.asoc.2025.113866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing offers the potential to enhance computational efficiency beyond classical methods, but practical implementation remains challenging due to the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, namely, restricted qubit counts, limited connectivity, and the presence of noise and decoherence. This study presents a novel approach to edge detection by leveraging a recently developed Quantum Fuzzy Inference Engine, implemented on a NISQ device. We introduce an optimized quantum circuit for its implementation, reducing qubit requirements and gate depth to improve execution on NISQ hardware. To overcome constraints related to large-scale image processing, a hybrid quantum–classical lookup table approach is employed. Edge detection performance is evaluated on the Berkeley Segmentation Data Set and Benchmarks 500 dataset under different conditions, including classical execution, ideal quantum simulation, noisy quantum simulation, and NISQ hardware calculation. Results demonstrate that the quantum fuzzy logic-based edge detection achieves outcomes comparable to classical methods by using fewer operations, marking a step toward practical quantum-enhanced image processing.},
  archive      = {J_ASOC},
  author       = {G. Nunziata and S. Crisci and G. De Gregorio and R. Schiattarella and G. Acampora and L. Coraggio and N. Itaco},
  doi          = {10.1016/j.asoc.2025.113866},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113866},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum fuzzy logic for edge detection: A demonstration on NISQ hardware},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompted complex context generation guided fine-grained ship recognition. <em>ASOC</em>, <em>185</em>, 113856. (<a href='https://doi.org/10.1016/j.asoc.2025.113856'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained ship recognition in complex marine environments is challenged by background interference, high inter-class similarity, and limited labeled data. Existing methods often rely on inefficient cascades or holistic feature extraction, which limits both accuracy and efficiency. To address these issues, we propose a Prompted Complex Context Generation Guided Fine-Grained Ship Recognition framework, consisting of two core modules. The Cross-Attention Context Generation Module utilizes a diffusion model to generate diverse background images from prompts, maintaining target consistency and enriching the training data to mitigate data scarcity. It also employs a cross-attention map to highlight target-relevant regions, guiding the Attention Map Guided Fusion Module. The Attention Map Guided Fusion Module adopts a dual-branch transformer architecture: one branch extracts global features from background-enhanced images, and the other captures local features through attention-guided cropping of target-specific regions. By integrating both global and local features, our method effectively identifies key target characteristics. Experimental results demonstrate that our approach achieves 97.04% accuracy on the publicly available MAR-ships dataset and 84.57% accuracy on the challenging GCS dataset, outperforming state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Runtian Wang and Kejun Wu and Renjie Qiao and Chunsheng Yang and Chengtao Cai},
  doi          = {10.1016/j.asoc.2025.113856},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113856},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prompted complex context generation guided fine-grained ship recognition},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-granularity decision tree algorithm based on variable precision rough sets and zentropy. <em>ASOC</em>, <em>185</em>, 113851. (<a href='https://doi.org/10.1016/j.asoc.2025.113851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing decision tree algorithms often use a single-layer measure to process data, which cannot fully consider the complex interactions and dependencies between different granularity levels. In addition, decision tree algorithms inevitably face the issue of multi-value preference, which may lead to the selection of unreasonable attributes in the process of partition, thereby affecting the performance of the algorithms. Therefore, this paper proposes an improved decision tree algorithm, called Ze-VNDT, which combines variable precision rough sets with Zentropy. First, to avoid the information loss caused by data discretization, this paper introduces variable precision neighborhood rough sets for data processing. Second, by analyzing the granularity level structure within the variable precision neighborhood rough set model, knowledge uncertainty is analyzed from three granularity levels: decision classes, approximate relations, and similarity classes. We describe the uncertain knowledge from the overall to the internal using the idea of going from coarse to fine, and design a Zentropy to measure uncertainty. To address the issue of multi-value preference, an adaptive weighted Zentropy uncertainty measure is designed based on the definition of uncertainty measure based on Zentropy. Third, when constructing the improved decision tree algorithm, the optimal attributes are selected based on the designed uncertainty measure. Finally, numerical experiments on 18 UCI datasets validated the effectiveness and rationality of the proposed algorithm. The experimental results showed that, compared to traditional algorithms and the latest improved algorithms, the proposed algorithm achieved an average accuracy of 94.79%, an average precision of 85.77%, an average recall rate of 84.68%, and an F1-score of 84.97% across the 18 datasets. It ranked first in all five evaluation metrics, demonstrating higher stability and accuracy.},
  archive      = {J_ASOC},
  author       = {Hui Dong and Caihui Liu and Xiying Chen and Duoqian Miao},
  doi          = {10.1016/j.asoc.2025.113851},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113851},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-granularity decision tree algorithm based on variable precision rough sets and zentropy},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skin lesion classification with mini-batch sampling and deep metric learning. <em>ASOC</em>, <em>185</em>, 113850. (<a href='https://doi.org/10.1016/j.asoc.2025.113850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin lesion image classification based on deep learning has recently garnered significant attention. However, directly applying methods that perform well in general computer vision tasks to skin lesion image classification is not ideal, as skin lesion image datasets possess intrinsic characteristics, such as class imbalance, intra-class variability, and inter-class similarity. To tackle these challenges simultaneously, we propose a novel unified learning framework, named mBSML, which integrates mini-batch sampling and deep metric learning. In this framework, mini-batch sampling re-samples data in real-time during each iteration of learning, while a new loss function combines mini-batch distance metric-based loss with cross-entropy loss. Through the alternating training procedure on both imbalanced training data and balanced re-sampling data, mBSML effectively learns from global distribution information and local similarity information, not only from the original dataset but also from the minority classes. Extensive experiments conducted on two publicly available datasets demonstrate the effectiveness of mBSML for skin lesion image classification.},
  archive      = {J_ASOC},
  author       = {Shengdan Hu and Zhifei Zhang and Li Ying and Guangming Lang},
  doi          = {10.1016/j.asoc.2025.113850},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113850},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Skin lesion classification with mini-batch sampling and deep metric learning},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AOD-YOLO: A self-modulating multi-scale feature aggregation mechanism for small object detection in airport surface scenes. <em>ASOC</em>, <em>185</em>, 113849. (<a href='https://doi.org/10.1016/j.asoc.2025.113849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in airport surface scenes is crucial for enhancing safety. However, the coexistence of objects with significant scale disparities within the same region complicates feature representation, limiting existing models’ ability to capture fine-grained details, especially for small objects. To address this challenge, we propose AOD-YOLO, an Airport Object Detection (AOD) model incorporating a Self-Modulating Multi-Scale Feature Aggregation Mechanism. This model introduces two key innovations: (1) Enhanced Context Modeling: By leveraging large-kernel convolution, frequency-domain modulation, and statistical feature analysis, our approach effectively adjusts feature contributions across different object scales, improving contextual understanding in complex scenes; (2) Optimized Small Object Representation: A dynamic gradient gain allocation strategy refines small-object features, enhancing detection accuracy and overall feature presentation. AOD-YOLO consistently improves performance across model scales. On our self-constructed Airport dataset and the public VisDrone-DET2019 dataset, it achieves mean Average Precision (mAP 0.5 ) of 87.9% and 44.9%, respectively—outperforming state-of-the-art models like YOLOv11 and Gold-YOLO by substantial margins. Additionally, through optimized network module placement, AOD-YOLO achieves 112 FPS, striking a balance between computational efficiency and accuracy, making it well-suited for real-time airport object detection.},
  archive      = {J_ASOC},
  author       = {Yingqing Wang and Weili Zeng and Ziyu Zhao and Baogeng Li and Zhibin Quan},
  doi          = {10.1016/j.asoc.2025.113849},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113849},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AOD-YOLO: A self-modulating multi-scale feature aggregation mechanism for small object detection in airport surface scenes},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient mathematical-based optimization method to optimize multi-hydropower operating rules. <em>ASOC</em>, <em>185</em>, 113846. (<a href='https://doi.org/10.1016/j.asoc.2025.113846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing hydropower multi-reservoir systems requires both effective operating rules and efficient optimization techniques. The main contribution of this paper is offering a unique approach that elegantly combines two important parts: creating an efficient optimization method and developing hydropower operating rules. In this regard, a nonlinear rule curve (NLRC) and a linear rule curve (LRC), are tailored for the coordination of a hydropower multi-reservoir system (HMRS) in Iran. To optimize operating rules, the study fabricates a novel algorithm termed the multi-operator weighted mean of vectors (MINFO). The algorithm combines a powerful global search strategy (GSS) that thoroughly searches the solution space with an efficient local search (LS), striking a balance between solution diversity and convergence speed. To fine-tune this balance, an adaptive parameter-tuning strategy is applied. Furthermore, the active-set sequential quadratic programming (ASQP) serves as a localized escaping operator to enhance the algorithm's convergence speed. The effectiveness of the proposed MINFO algorithm is first evaluated through a nonlinear five-reservoir problem. The findings indicate that the MINFO algorithm outperforms a set of 14 distinct optimization methods. Subsequently, the MINFO algorithm is applied to identify optimal NLRC and LRC for a six-reservoir hydropower system. The results underscore the superiority of optimized NLRC, yielding a potential power augmentation of up to 17 % in comparison to the LRC approach. In summation, this study constitutes a seminal contribution by cultivating an efficient rule curve framework for the management of HMRSs.},
  archive      = {J_ASOC},
  author       = {Shuguang Li and Iman Ahmadianfar and Aitazaz A. Farooque and Zaher Mundher Yaseen},
  doi          = {10.1016/j.asoc.2025.113846},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113846},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient mathematical-based optimization method to optimize multi-hydropower operating rules},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced YOLOv4 for real-time underwater object detection: An application-oriented approach. <em>ASOC</em>, <em>185</em>, 113837. (<a href='https://doi.org/10.1016/j.asoc.2025.113837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting debris and monitoring marine life in sea aquaculture face challenges due to limited visibility and the presence of diverse. Underwater object detection by Autonomous Unmanned Vehicle(AUV) is inherently more challenging than land due to light attenuation and water turbidity, especially for small and dense objects in murky images, where extracting high-quality features is hindered. In this paper, we present an efficient approach for real-time underwater object detection through improvements in image enhancement, data augmentation, and feature aggregation. Initially, U-Shape Transformer is applied to enhance the original images. For data augmentation, it is observable that while Mosaic data augmentation enhances complex images but fails to improve small-object detection due generation of less number of images with small objects. To address this limitation, we propose Underwater-Mosaic (U-Mosaic), a modified Mosaic data augmentation technique designed to enhance small-object detection. Additionally, it was noted that existing YOLOv4 struggles with detecting small and densely populated objects in underwater images as unable to get sufficient features for small objects due to downsampling, image quality and also found difficulty in selecting anchor box size. Therefore, we propose a model called Advanced YOLOv4, tailored for underwater object detection. The proposed Advanced YOLOv4 aims to improve object detection efficiency by altering the neck and prediction layers of YOLOv4. Moreover, we introduce an additional spatial pyramid pooling layer to aggregate features and reduce feature dimensions thereby improving object detection rates. Also, the proposed work concentrates on very large object detection and for this purpose used downsampling during the detection of large objects. The proposed approach is validated through two distinct application areas: (i) detecting and locating debris (ii) detecting fish from underwater images. For validation, the Trash ICRA19 dataset is used for debris detection, while the Brackish dataset is employed for fish detection. UIQM and UCIQE, image enhancement assessment metrics are used to measure quality of enhanced images and found more than 20% better result for both the datasets. The proposed real-time underwater object detection model outperformed single-stage object detectors like YOLOv3, YOLOv4, YOLOv5, YOLOv7, and KPE-YOLOv5 by 5% in terms of mean Average Precision(mAP). Also proposed work compared with two-stage detector RCNN and found 8% better mAP than RCNN.},
  archive      = {J_ASOC},
  author       = {Pratima Sarkar and Sourav De and Prasenjit Dey and Sandeep Gurung},
  doi          = {10.1016/j.asoc.2025.113837},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113837},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advanced YOLOv4 for real-time underwater object detection: An application-oriented approach},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniCon: Unified image-guiding generation with noise consistency. <em>ASOC</em>, <em>185</em>, 113832. (<a href='https://doi.org/10.1016/j.asoc.2025.113832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have demonstrated remarkable capabilities in image-to-image tasks. However, existing methods typically focus either on structural (e.g., layout, content) or stylistic guidance, with few approaches effectively excel at both. On the other hand, many methods require time-consuming fine-tuning or high inference latency, making interactive generation applications challenging to realize. To address these issues, we propose a two-stage framework referred as UniCon ( Uni fied Image-guiding Generation with Noise Con sistency). To improve time efficiency, we follow the paradigm of inversion-based image manipulation and introduce a novel method called Noise Consistency Inversion . Leveraging the nature of Consistency Models, this inversion process is highly efficient, requiring only a single neural function evaluation (NFE) in the inversion process. To achieve high consistency and finer control, we introduce a unified attention-based guidance mechanism that supports structural, stylistic, or joint reference inputs, without any additional fine-tuning. Experiments with structure- and style-specific methods show that our approach performs competitively or better in each individual aspect. In comparison of style transfer tasks that demand both structure and style, our method outperforms state-of-the-art baselines, confirming the effectiveness of our union control strategy. And overall, our approach also achieves the best efficiency in terms of runtime performance.},
  archive      = {J_ASOC},
  author       = {Yuanjun Liao and Yuning Gong and Yanci Zhang},
  doi          = {10.1016/j.asoc.2025.113832},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113832},
  shortjournal = {Appl. Soft. Comput.},
  title        = {UniCon: Unified image-guiding generation with noise consistency},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of the financial performance of the textile and apparel industry in interval type-2 fuzzy environment. <em>ASOC</em>, <em>185</em>, 113830. (<a href='https://doi.org/10.1016/j.asoc.2025.113830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Turkish textile and apparel sector plays a crucial role in the national economy through employment, exports, and investment. The financial performance of companies is a key determinant of their sustainability and competitiveness, especially in global markets. The Turkish textile and apparel sector is one of the essential industries in terms of macro-economic indicators such as net foreign exchange inflow, employment and investment. This sector is also one of the critical actors in world trade. A robust performance evaluation model is essential for stakeholders such as investors, creditors, and managers. However, the assessment of firms is a very critical decision involving uncertainty due to various conflicting criteria based on judgements. In this study, an integrated multi-criteria decision-making (MCDM) model including interval type-2 fuzzy hierarchy process (IT2FAHP) and Compromise Ranking of Alternatives from Distance to Ideal Solution (CRADIS) approaches are proposed to assess the financial performance of Turkish textile and clothing firms that are traded in Borsa İstanbul (BİST) in the period from 2006 to 2020. In line with the determined purpose, the arithmetic average of the determined financial ratios during the analysis period covering 15 years is computed to obtain long-term performance indicators. The importance weights of the selected financial criteria for the performance evaluation model are identified by employing the IT2FAHP approach. Then, the firms are ranked according to their financial performances with the CRADIS method. In addition, the results from the sensitivity analysis validate the proposed approach and prove that it is practical. Moreover, practical and managerial implications are discussed based on the results. The results offer valuable insights for strategic decision-making and can support efforts to enhance financial stability in the textile and apparel sector. According to the results, "LUKSK" had the highest long-term financial performance among the 11 companies discussed. This company is followed by BOSSA, YATAS, and ATEKS companies. The alternatives confirm the robustness of the proposed model in maintaining its place in the ranking in 190 scenarios. In addition, the comparative analysis confirms the consistency of the proposed ranking framework.},
  archive      = {J_ASOC},
  author       = {Ömer Faruk Görçün and Mohsin Shabir and Ahmet Çalık and Özcan Işık},
  doi          = {10.1016/j.asoc.2025.113830},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113830},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of the financial performance of the textile and apparel industry in interval type-2 fuzzy environment},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable dual-output multivariate wind speed interval prediction scheme using hyper-heuristic optimization algorithm and deep neural networks. <em>ASOC</em>, <em>185</em>, 113829. (<a href='https://doi.org/10.1016/j.asoc.2025.113829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty analysis of wind speed forecasting using the Lower Upper Bound Estimation (LUBE) represents an advanced interval prediction method that does not require assumptions about data distribution. Previous studies, however, have exclusively focused on univariate prediction models, neglecting the information from other variables, and have not fully exploited the prediction errors in their loss function during training. To address these issues, an interpretable dual-output multivariate wind speed interval prediction scheme (IMWSIPS) that utilizes a hyper-heuristic optimization algorithm and a deep neural network is proposed, along with a novel loss function for training. The system initially takes multiple inputs such as historical wind speed and other influencing factors including wind direction, density, temperature, and pressure into a deep neural network. The actual wind speeds are then scaled up and down by factors of 1 + θ 1 (0 <θ 1 <1) and 1 + θ 2 (-1 <θ 2 <0), respectively, to produce two outputs from the network. On this basis, an optimization problem to minimize interval width under a given coverage probability is formulated and solved using the developed hyper-heuristic algorithm, yielding optimal values for θ 1 and θ 2 and the prediction intervals for sub-models. Subsequently, the advantages of five deep neural network models are leveraged to construct an ensemble model, with weights optimized by the hyper-heuristic algorithm to derive the final prediction intervals. Ultimately, the system's interpretability is analyzed at both variable and sub-model levels. Experimental and discussion results demonstrate that the introduction of IMWSIPS not only signifies enhancements in forecasting performance but also implies improvements in wind energy utilization efficiency and reductions in operational costs for power systems.},
  archive      = {J_ASOC},
  author       = {Mengzheng Lv and Jianzhou Wang and Shuai Wang and Yang Zhao and Jialu Gao and Yuansheng Qian},
  doi          = {10.1016/j.asoc.2025.113829},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113829},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interpretable dual-output multivariate wind speed interval prediction scheme using hyper-heuristic optimization algorithm and deep neural networks},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging the semantic-numerical gap: A numerical reasoning method of cross-modal knowledge graph for material property prediction. <em>ASOC</em>, <em>185</em>, 113776. (<a href='https://doi.org/10.1016/j.asoc.2025.113776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material properties are illustrated by numerical data and semantic factors. In general, existing methods typically adopt machine learning (ML) algorithms to regress numerical properties or transfer other pre-trained knowledge graphs (KGs) to the material, due to the limitations of small-sample datasets. However, integrating semantic and numerical information from multi-modal data which across diverse experimental conditions remains a significant challenge in materials science. In this paper, a numerical reasoning method for material KGs (NR-KG) 1 was proposed, which constructs a cross-modal KG using semantic nodes and numerical proxy nodes. Both types of information by projecting KG into a canonical KG were captured and a graph neural network to predict material properties was utilized. In process, a novel projection prediction loss is proposed to extract semantic features from numerical information. NR-KG facilitates end-to-end processing of cross-modal data, mining relationships and cross-modal information in small-sample datasets, and fully utilizes effective experimental data to enhance the accuracy of material prediction. We propose two new high-entropy alloys (HEA) property datasets with semantic descriptions. NR-KG outperforms state-of-the-art (SOTA) methods on two material datasets, with MSE values of 3520 and 2.210, and achieving relative improvements of 25.9% and 16.1%, respectively, over the second-best methods, KANO and PCHMLP (semantic). It also achieves RMSE values of 0.584 and 0.521 on the FreeSolv and ESOL public molecular datasets, surpassing SOTA methods by 48.8% and 22.2% over KANO, highlighting its potential application and generalizability.},
  archive      = {J_ASOC},
  author       = {Guangxuan Song and Dongmei Fu and Zhongwei Qiu and Zijiang Yang and Jiaxin Dai and Lingwei Ma and Dawei Zhang},
  doi          = {10.1016/j.asoc.2025.113776},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113776},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bridging the semantic-numerical gap: A numerical reasoning method of cross-modal knowledge graph for material property prediction},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-based actor–critic for adaptive cryptocurrency portfolio rebalancing. <em>ASOC</em>, <em>185</em>, 113697. (<a href='https://doi.org/10.1016/j.asoc.2025.113697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-frequency trading in volatile markets, such as cryptocurrencies, requires portfolio models that can swiftly adapt to regime shifts while controlling risk. We propose a novel approach that frames portfolio management as a dynamic strategy-selection problem. Instead of directly predicting asset weights, our agent selects from a pool of expert strategies based on recent market trends. We introduce a Transformer-based Variational Autoencoder (VAE) to extract disentangled trend representations, and a trend-aware actor–critic model to perform expert selection. Experiments demonstrate that this modular, strategy-level control mechanism outperforms existing methods in risk-sensitive crypto portfolio management.},
  archive      = {J_ASOC},
  author       = {Ahmad Asadi and Reza Safabakhsh},
  doi          = {10.1016/j.asoc.2025.113697},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113697},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transformer-based actor–critic for adaptive cryptocurrency portfolio rebalancing},
  volume       = {185},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sustainable bridge management using refined feature selection for machine learning-aided bridge condition prediction: Incorporation of pareto distribution in MRMR method. <em>ASOC</em>, <em>184</em>, 113878. (<a href='https://doi.org/10.1016/j.asoc.2025.113878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the critical phases of bridge management is condition prediction, which was enhanced after machine learning emerged. Researchers have used feature selection (FS) methods to reduce the predictors and optimise the prediction models. Prior studies have either explored a unified feature set for the overall bridge condition or focused solely on the deck, leading to limited predictions. This study proposes a refined feature selection method highlighting the importance of specific predictors for different bridge elements’ conditions using the California State inspection database from 1983 to 2021. The implemented FS approach consists of a verified Minimum-Redundancy Maximum-Relevance (MRMR) method followed by a Pareto analysis that identifies the most contributory factors in predicting the condition of bridges’ decks, superstructures, and substructures. The applied experiment on the US National Bridge Inventory database reveals that 28–33 predictor variables, out of more than 140 available features, contribute the most to each component’s health prediction with a cumulative importance score of over 95 %. Additionally, 22 mutual data items among the selected features are proposed as the minimum required predictors to be gathered by the asset management authorities. This study’s achievements help both researchers reduce the running costs of their prediction models and asset managers with data gathering and registration optimisation and, consequently, whole-of-life cycle cost reduction for sustainable asset management.},
  archive      = {J_ASOC},
  author       = {Vandad Dayan and Nicholas Chileshe and Reza Hassanli and Amin Parvaneh},
  doi          = {10.1016/j.asoc.2025.113878},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113878},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sustainable bridge management using refined feature selection for machine learning-aided bridge condition prediction: Incorporation of pareto distribution in MRMR method},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of fuzzy rule-based models in the presence of the big data environment. <em>ASOC</em>, <em>184</em>, 113869. (<a href='https://doi.org/10.1016/j.asoc.2025.113869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a series of methods to build fuzzy rule-based models (FRBMs) in the presence of the big data environment such that the formed predictive models are more accurate, efficient, and robust. We follow two major steps to realize this target. In the first step, we build numeric FRBMs with the big data set such that the formed predictive models are more accurate and efficient. Specifically, based on the divide-and-conquer strategy, the big data set is divided into subsets through either the hyperplane division-based method or the K -Means clustering-based method; then either a global-based strategy or a local-based strategy is used to build numeric FRBMs. As a result, four Options are generated to develop numeric FRBMs. In the second step, we build the granular FRBMs based on the four Options developing the numeric FRBMs. Specifically, given a certain Option, based on the Principle of Justifiable Granularity (PJG), we granulate both condition parts and conclusion parts of the rules, forming the granular FRBMs; then the predictive models are further evaluated based on the PJG and optimized based on the Particle Swarm Optimization (PSO) algorithm to enhance the robustness. Finally, experimental studies on both synthetic datasets and publicly available datasets are conducted to prove the effectiveness of the proposed methods.},
  archive      = {J_ASOC},
  author       = {Yinghua Shen and Dan Zhao and Yan Li and Xingchen Hu and Yuan Chen and Bingsheng Liu},
  doi          = {10.1016/j.asoc.2025.113869},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113869},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development of fuzzy rule-based models in the presence of the big data environment},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMFF: A cross-modal multi-layer feature fusion network for multimodal sentiment analysis. <em>ASOC</em>, <em>184</em>, 113868. (<a href='https://doi.org/10.1016/j.asoc.2025.113868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis seeks to interpret speaker sentiment by integrating information from multiple modalities, typically text and audio. While existing methods often focus on fusing deep-layer features extracted from the final stages of unimodal encoders, they may overlook crucial fine-grained information present in shallow-layer features (e.g., subtle phonetic variations or basic syntactic structures) relevant for nuanced sentiment understanding. Furthermore, effectively fusing features from different modalities presents the dual challenges of dynamically weighting each modality’s contribution and accommodating their inherent data heterogeneity. To address these limitations, we propose a novel Cross-modal Multi-layer Feature Fusion (CMFF) network. CMFF explicitly leverages the hierarchical information contained in both shallow-layer and deep-layer features from text and audio modalities. It employs multi-head cross-modal attention mechanisms within its fusion layers to facilitate interaction across feature layers and modalities. Crucially, CMFF incorporates a Mixture of Gated Experts (MoGE) network within these fusion layers. The MoGE utilizes modality-specific expert sub-networks, each tailored to process the distinct characteristics of text or audio data, thereby directly addressing data heterogeneity. Concurrently, each expert employs an internal gated feed-forward mechanism. This allows the model to dynamically control the information flow for each feature vector, effectively learning to weigh the importance of different feature dimensions from each layer and modality based on the input context. Extensive experiments conducted on the benchmark CMU-MOSI and CMU-MOSEI datasets demonstrate that the proposed CMFF model achieves competitive or superior performance compared to state-of-the-art methods across various standard evaluation metrics.},
  archive      = {J_ASOC},
  author       = {Shuting Zheng and Jingling Zhang and Yuanzhao Deng and Lanxiang Chen},
  doi          = {10.1016/j.asoc.2025.113868},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113868},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CMFF: A cross-modal multi-layer feature fusion network for multimodal sentiment analysis},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating over-squashing in graph few-shot learning by leveraging local and global similarities. <em>ASOC</em>, <em>184</em>, 113863. (<a href='https://doi.org/10.1016/j.asoc.2025.113863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning models, particularly neural networks, often fail to deliver satisfactory results in scenarios with insufficient data. This becomes even more challenging when dealing with inherently complex data, such as graph data. This paper addresses the issue of learning with a limited number of samples, known as n -way k -shot learning, within the context of graph data. Our research extends the concept of similarity from neighboring nodes to the entire graph by leveraging transitivity relations. By employing edges and strong transitivity relations, we utilize a bipartite graph neural network that capitalizes on both local neighborhoods and distant, yet similar, nodes to generate node embeddings. This approach has demonstrated effectiveness in tasks such as node classification. Our proposed model’s ability to mitigate the over-squashing problem enhances its generalizability, resulting in a task-invariant model. Experimental results on various graph datasets show that the embeddings produced by our model are not task-specific. Consequently, our model outperforms other models in few-shot learning scenarios, where only a limited number of labeled nodes are available for each distinct downstream task.},
  archive      = {J_ASOC},
  author       = {Yassin Mohamadi and Mostafa Haghir Chehreghani},
  doi          = {10.1016/j.asoc.2025.113863},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113863},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mitigating over-squashing in graph few-shot learning by leveraging local and global similarities},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal intervention for feedback information in fairness recommendation. <em>ASOC</em>, <em>184</em>, 113862. (<a href='https://doi.org/10.1016/j.asoc.2025.113862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recommender systems, users’ feedback information is greatly affected by individual differences between users and usually interferes with the interaction between users and items, causing unfairness in the recommendation results. To alleviate the problem of unfair recommendation results caused by the confounding of feedback information, we propose a novel causal intervention for feedback information in fairness recommendation (CIFair). First, we construct a causal graph based on the interaction between users and items and analyze the reasons for unfair recommendation results caused by feedback information as the confounding factor through the causal graph. Then, we design a two-phase predicted rating generation, namely, the elimination and reconstruction phases. In the elimination phase, we analyze the dual effects of the confounding factor on the recommender system and eliminate it through causal intervention to improve the fairness of the recommendation results and obtain a fair predicted rating. In the reconstruction phase, we design personalized feedback information based on user and item attributes to ensure recommendation performance and obtain a personalized predicted rating. Finally, we combine the predicted ratings generated in the elimination and reconstruction phases and provide users with personalized recommendation results with fairness. We conduct experiments on three publicly available datasets (MovieLens-1M, Last.fm, and Yelp) to verify the significance of the CIFair. The ablation experiment confirms that the CIFair model can achieve relatively fair recommendations from both the user and item sides while ensuring recommendation performance.},
  archive      = {J_ASOC},
  author       = {Chenyu Wang and Guanxi Wang and Guowei Yang and Dun Li},
  doi          = {10.1016/j.asoc.2025.113862},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113862},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Causal intervention for feedback information in fairness recommendation},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fixing deep early exit ensembles for sensor-based human activity recognition through uncertainty quantification. <em>ASOC</em>, <em>184</em>, 113861. (<a href='https://doi.org/10.1016/j.asoc.2025.113861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic deep neural networks (DNNs) have recently achieved outstanding success on a wide range of resource-constrained human activity recognition (HAR) tasks. However, prior most works are deterministic and could not provide any uncertainty estimate. Though an early exit can facilitate adaptive activity inference by producing intermediate predictions at multiple stages during forward pass, these predictions are only meaningful in real-world while complemented with reliable uncertainty estimates. Until now, the quality of uncertainty estimates has always been ignored in the context of early exit HAR. How to quantify predictive uncertainty in dynamic DNNs still remains challenging and yet unsolved. To address this issue, this paper introduces a new framework of early exit ensembles, which provides a probabilistic treatment of such dynamic DNNs to capture uncertainty estimates through an implicit ensemble of sub-networks sharing weights. We evaluate the proposed approach using three strong state-of-the-art DNN backbones on several mainstream HAR benchmarks, i.e., UCI-HAR, UniMib-SHAR and WISDM. Depending on the backbones and datasets, our approach can lower calibration error up to around 11 × , while increasing accuracy by up to 0.588% over its single counterpart. Both theoretical computational efficiency and practical runtime latency are analyzed. We provide an intuitive illustration of accounting for both aleatoric and epistemic uncertainty, which validates that such probabilistic treatment can adequately capture uncertainty estimates to aid decision-making while varying the computational budgets.},
  archive      = {J_ASOC},
  author       = {Xin Liu and Lei Zhang and Wenbo Huang and Dongzhou Cheng and Hao Wu and Aiguo Song},
  doi          = {10.1016/j.asoc.2025.113861},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113861},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fixing deep early exit ensembles for sensor-based human activity recognition through uncertainty quantification},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A transformer-based water quality prediction model with mixed fractional brownian features and multi-feature bottleneck transformation. <em>ASOC</em>, <em>184</em>, 113860. (<a href='https://doi.org/10.1016/j.asoc.2025.113860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven water-quality model inference typically depends on large datasets. In emergency or resource-constrained settings, however, small sample sizes hinder a model’s ability to faithfully capture the complex nonlinear and multiscale dynamics of water quality. To address this challenge, we propose an improved Transformer. First, we enrich the inputs with features from mixed fractional Brownian motion with a perturbation factor (rMFBM). The rMFBM module captures heterogeneous temporal dependencies via multiple Hurst exponents and applies a Cholesky-based covariance sampler with a diagonal perturbation to ensure numerical stability. Second, a multi-feature bottleneck layer performs compression and dimensionality reduction to yield compact yet information-dense representations. Finally, the optimized features are modeled by a lightweight Transformer trained with the AMSGrad optimizer and an early-stopping strategy. On the task of predicting the N S F W Q I water-quality index, the proposed model reduces root-mean-square error (RMSE) by approximately 43.1% relative to a standard Transformer and improves the coefficient of determination ( R 2 ) by 1.89% across four small-sample datasets. It also outperforms widely used alternatives, including GNNs and PINNs. These results suggest that certain water-quality parameters exhibit mixed fractional Brownian-motion characteristics, enabling rapid and reliable prediction under data scarcity and in emergency scenarios.},
  archive      = {J_ASOC},
  author       = {Genghao Cui and Zhiyao Zhao and Li Wang and Huiyan Zhang and Jiabin Yu},
  doi          = {10.1016/j.asoc.2025.113860},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113860},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A transformer-based water quality prediction model with mixed fractional brownian features and multi-feature bottleneck transformation},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPCMamba: Privacy-preserving inference for mamba models via secure multi-party computation. <em>ASOC</em>, <em>184</em>, 113859. (<a href='https://doi.org/10.1016/j.asoc.2025.113859'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is increasingly critical across high-stakes domains, but privacy risks during model inference remain a major concern. Secure multi-party computation (SMPC) offers a promising solution by enabling privacy-preserving collaborative inference without exposing sensitive data. The recently proposed Mamba model, which outperforms Transformer in certain tasks, presents unique challenges for SMPC due to its state-space architecture and nonlinear operations. This paper introduces a framework for executing Mamba model inference under SMPC while preserving privacy. The framework natively supports linear operations and securely computes nonlinear functions – including square roots, exponentials, logarithms, and SiLU activations – without altering the original model architecture. Experimental results demonstrate that the proposed method achieves accuracy improvements of 2.4%, 2.84%, and 8.23% on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets, respectively, compared to existing SMPC-based vision Transformer approaches, MPCViT. Additionally, inference latency is reduced by factors of 2.14 × , 2.14 × , and 26.13 × on these benchmarks, significantly advancing efficient and secure deployment of state-space models in privacy-sensitive scenarios.},
  archive      = {J_ASOC},
  author       = {Yongqiang Yu and Yuliang Lu and Xuehu Yan and Wei Yan and Shengyang Luo},
  doi          = {10.1016/j.asoc.2025.113859},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113859},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MPCMamba: Privacy-preserving inference for mamba models via secure multi-party computation},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A depth-estimation-based method for multi-view synthesis applied to chinese landscape paintings. <em>ASOC</em>, <em>184</em>, 113858. (<a href='https://doi.org/10.1016/j.asoc.2025.113858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenging task of novel view synthesis for traditional Chinese landscape paintings, which typically offer only a single perspective and lack clear depth information. To overcome the limitations of existing methods that rely on multi-view input and depth estimation, we propose a multi-view synthesis method for Chinese landscape paintings, termed MVSM-CLP. The proposed CLPDepth Module employs a high-low resolution fusion mechanism to enhance detail expression while preserving the original scene structure. We introduce an image restoration technique guided by landscape ink lines, termed LInpainting, to improve edge extraction and the accuracy of painting inpainting Additionally, our method tackles the issue of scarce 3D data in current view synthesis efforts by constructing multi-view data from a single ancient painting. Our approach effectively bridges the gap between 2D art and 3D visualization, creating vivid and realistic virtual environments while preserving the traditional style and essence of Chinese paintings. Experimental results demonstrate the effectiveness of our method in achieving high-quality multi-view synthesis, offering new possibilities for the digital preservation of cultural heritage. A preprint has previously been published (Peng et al., 2024 [1] ). The code and dataset is available at https://github.com/LPDLG/DepCLP_Dataset .},
  archive      = {J_ASOC},
  author       = {Xianlin Peng and Wanlin Zhou and Qiyao Hu and Tengfei Li and Dong Zhang and Rui Cao},
  doi          = {10.1016/j.asoc.2025.113858},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113858},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A depth-estimation-based method for multi-view synthesis applied to chinese landscape paintings},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SGBA: Subspace guidance backdoor attack with feature alignment in image classification. <em>ASOC</em>, <em>184</em>, 113857. (<a href='https://doi.org/10.1016/j.asoc.2025.113857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widely use of deep neural networks (DNNs) in a variety of classification and generation tasks with remarkable performance, the security of DNN models has attracted further attention. Recently, backdoor attacks have made a significant threat to DNN models, where attackers intentionally introduce malicious patterns into the model and manipulate the behavior of the model upon encountering the specific trigger. In this paper, we analyze the workflow of backdoor attacks with feature-guide classifier, and propose a novel method to conduct the backdoor attack based on feature alignment. We find that using samples with adversarial perturbation for training can mislead the model during the inference stage. Inspired by this observation, we leverage the intrinsic distribution of target class features in the latent space to generate effective and invisible triggers by adding directional perturbations to target images. We evaluate the attack under three widely used datasets and results show that our method can achieve considerable performance in comparison with other four state-of-the-art attacks. Furthermore, we also make extensive experiment to emphasize the robustness and stealthiness of our attack method.},
  archive      = {J_ASOC},
  author       = {Hao Luo and Zhi Qin and Lin Wang and Ziyue Wu and Min Yang},
  doi          = {10.1016/j.asoc.2025.113857},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113857},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SGBA: Subspace guidance backdoor attack with feature alignment in image classification},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based kiwifruit flower recognition method to facilitate automated pollination. <em>ASOC</em>, <em>184</em>, 113855. (<a href='https://doi.org/10.1016/j.asoc.2025.113855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate flowering-stage recognition is vital for intelligent orchard management, automated pollination, and yield forecasting. However, in complex natural scenes, existing models struggle to balance detection accuracy with inference speed. To bridge this gap, we propose the Kiwifruit Recognition Network (KiwiRecNet), a lightweight yet high-performance framework tailored to kiwifruit blossoms in the wild. KiwiRecNet first employs the Kiwifruit Generative Adversarial Network for Low-light Improvement (KiwiGAN-LI) to enhance under-exposed images. We then design a novel backbone, the Multi-Scale Shuffle Block (MSBlock), which combines structural re-parameterisation with channel–spatial shuffling to shrink the network footprint. Next, we propose the Partial-Mixing Vision Transformer (PMVIT), a convolution-Transformer hybrid that captures fine-grained features and remains robust to occlusion. Finally, we devise a Bidirectional Cross-Scale Fusion module (Bi-CSF) to enrich multiscale perception. Evaluated on the NWAFU Kiwifruit_F dataset, KiwiRecNet achieves 94.07 % mAP at 82.67 FPS with only 0.93 million parameters, outperforming existing lightweight detectors while approaching heavyweight baselines at a fraction of their cost. Consistent gains across multiple flower datasets confirm its generalisation ability. These results demonstrate an effective route to high-accuracy, real-time flowering-phase recognition on resource-constrained devices, paving the way for scalable agricultural automation.},
  archive      = {J_ASOC},
  author       = {Xiaopeng Li and Jinzhi Du and Xiaoyu Chen and Fuxi Shi and Shuqin Li},
  doi          = {10.1016/j.asoc.2025.113855},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113855},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning-based kiwifruit flower recognition method to facilitate automated pollination},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Searching for local pareto fronts based on the non-dominance range in the decision space. <em>ASOC</em>, <em>184</em>, 113853. (<a href='https://doi.org/10.1016/j.asoc.2025.113853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal multi-objective optimisation problems (MMOPs) involve multiple equivalent Pareto sets that share the same Pareto front, and a special subclass is known as MMOPs with local Pareto fronts (MMOPLs). Conventional multi-modal multi-objective optimisation evolutionary algorithms (MMOEAs) struggle to effectively identify local Pareto fronts, and existing methods tailored for MMOPLs exhibit notable limitations. Additionally, commonly used performance metrics lack systematic evaluation and suffer from inherent shortcomings. A simple yet effective MMOEA that leverages the non-dominance range in the decision space is proposed to address these challenges. By prioritising individuals with above-average non-dominance ranges, the algorithm enhances the identification and retention of both global and local optima. Convergence is further improved using the local outlier factor method. The limitations of existing performance metrics are analysed and six new performance metrics tailored for MMOPLs are introduced. To facilitate evaluation, benchmark problems are modified to create scenarios in which global and local optima coexist on the same Pareto set. Extensive experimental results confirm that the proposed algorithm achieves competitive performance, effectively identifying global and local optima while ensuring well-distributed solutions.},
  archive      = {J_ASOC},
  author       = {Yimin Shen and Yu Guo and Shaohua Huang and Weiwei Qian and Shengbo Wang and Litong Zhang},
  doi          = {10.1016/j.asoc.2025.113853},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113853},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Searching for local pareto fronts based on the non-dominance range in the decision space},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Normalizing flow defect detection model based on similar self-supervision. <em>ASOC</em>, <em>184</em>, 113847. (<a href='https://doi.org/10.1016/j.asoc.2025.113847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the limitations in unsupervised industrial anomaly detection, such as poor modeling of the appearance differences between synthetic and real defects and the failure to capture multi-scale feature relationships, this paper proposes a Normalizing Flow Defect Detection Model Based on Similar Self-Supervision (NF-SS). First, a feature-level defect generation method is introduced. It synthesizes realistic defect samples by fusing defect background features, known defect features from related domains, and other auxiliary features. This enhances the model's understanding of defect characteristics and reduces the gap between synthetic and real defects. Second, a multi-scale joint Normalizing Flow decoder is proposed. It replaces independent NF layers with progressive multi-scale fusion, allowing the model to integrate features hierarchically across scales. This preserves spatial relationships, reduces edge blurring, and improves defect localization accuracy. Extensive experiments on the MVTec AD dataset show that NF-SS outperforms state-of-the-art models, achieving an average image-level AUC of 99.54 % and pixel-level AUC of 98.76 %. It significantly improves the detection of subtle and ambiguous defects. NF-SS combines unsupervised and self-supervised learning, providing a robust solution for industrial quality inspection with limited defect samples.},
  archive      = {J_ASOC},
  author       = {Zhenlian Miao and Guangzhu Chen and Herui Cao and Yuan Tang and Xiaojuan Liao},
  doi          = {10.1016/j.asoc.2025.113847},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113847},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Normalizing flow defect detection model based on similar self-supervision},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Refined vision to obtain the vibration trajectory of rotating body. <em>ASOC</em>, <em>184</em>, 113845. (<a href='https://doi.org/10.1016/j.asoc.2025.113845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-contact vision technology has been widely used in many fields, but it still has limited accuracy in dealing with complex motion, small displacement and boundary recognition, especially in the rotating body scene, where it is difficult to balance global robustness and local sensitivity. To achieve high-precision visual measurement of rotating structure vibration, this paper proposes a detection-guided target tracking method to address the shortcomings of existing visual algorithms in target identity preservation and temporal consistency. In the face of the above challenges, this paper constructs lightweight encoder–decoder network to extract multi-scale semantic information to enhance the target edge modeling capability, and designs a multidimensional regression mechanism to predict the target boundary from the center point and extract sub-pixel vibration signals. At the same time, a guide term is introduced to optimize trajectory association and improve the temporal continuity and spatial consistency of tracking. Experimental results show that the average performance of the proposed method is improved by about 51.2% on the basis of multiple mainstream visual algorithms, and the main frequency recognition error is reduced to 0.32 Hz, which effectively suppresses high-frequency vibration errors. Ablation experiments verify the contribution of each module to the measurement accuracy, and low-quality image tests further show that the method has good generalization ability and application prospects.},
  archive      = {J_ASOC},
  author       = {Rongliang Yang and Tao Liu and Sen Wang},
  doi          = {10.1016/j.asoc.2025.113845},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113845},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Refined vision to obtain the vibration trajectory of rotating body},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving lightweight semi-supervised text classification via teacher intervention. <em>ASOC</em>, <em>184</em>, 113844. (<a href='https://doi.org/10.1016/j.asoc.2025.113844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most recent semi-supervised text classification frameworks have focused on pre-trained models like BERT. While effective, their large-scale parameters and slow inference speed hinder deployment in many practical scenarios. In this work, we develop a general Li ghtweight S emi-supervised T ext classification framework (LiST), which significantly enhances the performance of lightweight models in semi-supervised settings, thus improving inference efficiency. LiST employs a teacher intervention strategy, where pseudo-labels initially rely on the teacher model and gradually shift to the lightweight model’s own predictions, progressively correcting the teacher’s incorrect predictions over time. Experimental results on multiple benchmark datasets demonstrate the generality and effectiveness of LiST. It not only approaches or exceeds the performance of the teacher model (UDA) at a 20 × faster inference speed but also outperforms existing lightweight methods in both performance and efficiency. Notably, LiST achieves significant performance improvements even with extremely few labeled samples, such as a 36.4% increase in accuracy on AG News with only 2 labeled samples per class.},
  archive      = {J_ASOC},
  author       = {Shaohuan Cheng and Wenyu Chen and Wanlong Liu and Hong Qu},
  doi          = {10.1016/j.asoc.2025.113844},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113844},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving lightweight semi-supervised text classification via teacher intervention},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chaotic property based multi-interval informer modeling method for long-term photovoltaic power generation prediction. <em>ASOC</em>, <em>184</em>, 113843. (<a href='https://doi.org/10.1016/j.asoc.2025.113843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The long-term prediction of photovoltaic (PV) power generation capacity can significantly enhance the maintenance planning of photovoltaic power stations and support the long-term development strategies of power supply and distribution networks. Currently, two major challenges hinder the realization of effective long-term prediction for PV power generation: First, PV power generation data is heavily influenced by environmental factors, leading to high randomness and significant volatility in long-term data sequences; Second, the temporal continuity of long-term data must be considered, which complicates the construction of an accurate predictive model. To address these issues, this paper proposes a long-term PV power generation prediction method based on interval division, modeled using an improved Informer architecture with a modified activation function. Initially, by analyzing the fluctuation characteristics of PV power generation data, the K-nearest neighbors algorithm is applied for data interpolation, while the moving average (MA) method is employed for data smoothing, effectively reducing data randomness while preserving the overall trend. Subsequently, the Kmeans＋＋ clustering algorithm is utilized to group the generation data into multiple intervals, thereby enhancing feature similarity within each cluster. Finally, the activation function of the Informer model is replaced with ReLU to improve its adaptability to photovoltaic data characteristics. Additionally, this study introduces evaluation metrics such as the Hurst exponent and the Maximum Lyapunov Exponent (MLE) to assess the predictability of partitioned data and identify the interval to which predicted values belong. Simulation experiments demonstrate that, compared to several commonly used prediction models, the proposed interval-based approach achieves superior performance in long-term PV power generation prediction.},
  archive      = {J_ASOC},
  author       = {Ying Han and Xinggang Hu and Kun Li},
  doi          = {10.1016/j.asoc.2025.113843},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113843},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chaotic property based multi-interval informer modeling method for long-term photovoltaic power generation prediction},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A noisy multi-objective evolutionary optimization algorithm based on elman neural network. <em>ASOC</em>, <em>184</em>, 113842. (<a href='https://doi.org/10.1016/j.asoc.2025.113842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have emerged as potent tools for tackling multi-objective optimization problems with remarkable success. This research delves into the performance of dynamic neural networks in resolving noisy multi-objective optimization problems (NMOPs). The local regression structure of the Elman neural network enables it to deal with the dynamic noise problem well. This paper integrates the Elman neural network into the framework of the non-dominated sorting genetic algorithm II (NSGA-II) to solve NMOP, called E-NSGA-II. In this method, the Elman neural network is employed for modeling, estimating the fitness value of each individual. Simultaneously, a hybrid selection solution strategy is implemented to select individuals for modeling, providing more diverse solutions in the modeling process to improve the convergence. Furthermore, E-NSGA-II incorporates a noise-driven sampling mechanism that intelligently adapts the sampling frequency based on the noise intensity. This feature could not only enhance the accuracy of neural network modeling, but also minimize the amount of calculation. Notably, the embedding of neural network does not impose much additional evaluation overheads, thereby bolstering the overall efficiency of E-NSGA-II. Experimental results prove the superiority of E-NSGA-II over four state-of-the-art noisy multi-objective optimization algorithms on dealing with NMOPs.},
  archive      = {J_ASOC},
  author       = {Jianxia Li and Ruochen Liu and Wanfeng Chen and Weibin Li},
  doi          = {10.1016/j.asoc.2025.113842},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113842},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A noisy multi-objective evolutionary optimization algorithm based on elman neural network},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective detection algorithm for small UAV based on lightweight you-only-look-once (YOLOv4-l) approach. <em>ASOC</em>, <em>184</em>, 113841. (<a href='https://doi.org/10.1016/j.asoc.2025.113841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The control and monitoring of small Unmanned Aerial Vehicles (UAV) plays a crucial role in national defense and security. However, due to their compact size and high mobility, the detection of small UAV across diverse scenarios remains a significant challenge. To address this issue, this study proposes an improved detection algorithm tailored for small UAV. The model is initially trained on a virtual dataset, and the learned parameters are transferred to real-world data through a transfer learning framework. To optimize anchor box generation, clustering analysis is performed on bounding box dimensions, resulting in anchor boxes with appropriate scales and aspect ratios. Furthermore, the Ghost module is introduced to replace conventional convolutions in CSPDarknet53, enhancing feature extraction efficiency. An Efficient Channel Attention (ECA) mechanism is also incorporated to strengthen output feature representations and improve the capture of texture details critical for small target detection. Through experiments, the proposed algorithm can achieve the mAP0.5 of 82.2 %. Experimental results demonstrate the effectiveness of the proposed small UAV detection method.},
  archive      = {J_ASOC},
  author       = {Guoning Li and Jianghao Cheng and Yanyan Liu and Jin Li and Zengming Lv and Qiang Li},
  doi          = {10.1016/j.asoc.2025.113841},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113841},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective detection algorithm for small UAV based on lightweight you-only-look-once (YOLOv4-l) approach},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TPLLM: A traffic prediction framework based on pretrained large language models. <em>ASOC</em>, <em>184</em>, 113840. (<a href='https://doi.org/10.1016/j.asoc.2025.113840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction constitutes a critical component in sustainable urban data analysis, playing a pivotal role in optimizing transportation systems for reduced carbon emissions and improved energy efficiency. The precision of prevailing deep learning-driven traffic prediction models typically improves as the volume of training data increases. However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention. This limitation severely hinders the deployment of models in regions with insufficient historical data. Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem. It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years demonstrate exceptional proficiency in cross-modality knowledge transfer and few-shot learning. Recognizing the sequential nature of traffic data, similar to language, we introduce TPLLM, a novel traffic prediction framework leveraging LLMs. In this framework, we construct a sequence embedding layer based on Convolutional Neural Networks (CNNs) and a graph embedding layer based on Graph Convolutional Networks (GCNs) to extract sequence features and spatial features, respectively. These are subsequently integrated to form inputs that are suitable for LLMs. A Low-Rank Adaptation (LoRA) fine-tuning approach is applied to TPLLM, thereby facilitating efficient learning and minimizing computational demands. Experiments on two real-world datasets demonstrate that TPLLM exhibits commendable performance in both full-sample and few-shot prediction scenarios.},
  archive      = {J_ASOC},
  author       = {Tian Ma and Yixuan Zhao and Minda Li and Yue Chen and Fangshu Lei and Yanan Zhao and Maazen Alsabaan},
  doi          = {10.1016/j.asoc.2025.113840},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113840},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TPLLM: A traffic prediction framework based on pretrained large language models},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic-linguistic fusion for brain tumor classification: A cross-modal attention framework with clinical interpretability. <em>ASOC</em>, <em>184</em>, 113839. (<a href='https://doi.org/10.1016/j.asoc.2025.113839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of brain tumors is a key challenge in medical image analysis, and existing methods mainly rely on static MRI images, which are difficult to capture the dynamic evolutionary features of tumors. In addition, the lack of descriptive clinical text and efficient multimodal feature fusion limits the classification accuracy and generalization. To overcome these limitations, this paper proposes a new dynamic language fusion (DLF) framework. The framework (1) utilizes ResNet18 in conjunction with LSTM for time series modeling to capture the temporal evolution of tumor morphology, (2) uses BioGPT and BERT for clinical text processing for semantic understanding, and (3) applies an interpretable cross-modal attentional mechanism for feature fusion to optimize dynamic perception and semantic alignment. Experiments on 10,287 images (from four publicly available datasets) show that the proposed framework achieves an overall accuracy of 98.96 %, a precision of 99.58 %, and an AUC higher than 0.998 for all categories on the test set, which is significantly better than the existing SOTA models, and especially exhibits stronger robustness and discriminative ability in boundary ambiguity and feature overlap samples. This study validates the synergistic effect of temporal modeling and semantic understanding in brain tumor diagnosis, providing clinicians with interpretable classification outputs to assist in decision-making for complex cases, while establishing a scalable framework for medical AI systems based on large language models.},
  archive      = {J_ASOC},
  author       = {Jiancong Fan and Fangyuan Chen and Yang Li and Jiehan Zhou},
  doi          = {10.1016/j.asoc.2025.113839},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113839},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic-linguistic fusion for brain tumor classification: A cross-modal attention framework with clinical interpretability},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path planning for flexible needle puncture based on multi-objective particle swarm optimization. <em>ASOC</em>, <em>184</em>, 113838. (<a href='https://doi.org/10.1016/j.asoc.2025.113838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a mixed-parameter MOPSO algorithm designed to address the puncture problem of flexible needles in obstacle environments. The algorithm incorporates the damage caused by the pivotal angle to soft tissues as an objective function, marking the first time this has been applied to the MOPSO algorithm. In comparison with four classical algorithms through simulation experiments, the path deviation is reduced to just 0.1 mm, significantly lower than the CPSO algorithm. The final path score achieves 35 points, surpassing the performance of other algorithms. A self-built FPAA hybrid control platform was employed for puncture experiments using a gelatin prosthesis. The experimental results confirm that the flexible needle successfully avoids obstacles and reaches the target, demonstrating the feasibility of the proposed puncture algorithm.},
  archive      = {J_ASOC},
  author       = {Ting Yang and Beibei Liu and Ru Sun and Bi Chen and Guijuan Ji},
  doi          = {10.1016/j.asoc.2025.113838},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113838},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Path planning for flexible needle puncture based on multi-objective particle swarm optimization},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-goal reinforcement learning framework for motion planning of a quadrotor UAV in 3D cluttered environment with unseen random goals. <em>ASOC</em>, <em>184</em>, 113836. (<a href='https://doi.org/10.1016/j.asoc.2025.113836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard reinforcement learning (RL) approaches render an optimal solution for a specific task by learning an optimal policy. Therefore, these RL approaches are suitable for single goal problems only. However, there exist numerous practical challenges that involve multiple goals to be achieved in a specific environment. For example, navigation of an unmanned aerial vehicle (UAV) to achieve multiple random targets in a specific three-dimensional (3D) cluttered space is a multi-goal problem. Using the standard RL approaches, the UAV agent needs to learn a separate optimal policy for each target. Thus, a multi-goal problem with random unseen goal allocations, especially in the 3D space of UAVs, increases computational effort substantially. To solve this issue, this paper presents a relatively generalized approach that learns the optimal state values and related optimal policies considering various regions of the robot environment. The proposed approach transforms the 3D robot environment into a Markov decision process (MDP) and further divides it into various virtual sub-spaces. For every initial goal position in a sub-space, value iteration algorithm enables the aerial agent to learn the optimal state values and the relevant policy. These optimal state values and policies are then stored in a replay buffer for later use. Once the initial state values for every sub-space are learned, the proposed approach allows the agent to use them from replay buffer and run a local search algorithm to connect every new goal position to any feasible state in the existing state values. Our proposed framework significantly reduces the computational effort for multiple unseen goal targets by restricting the re-computation of state values for each new goal. To validate the proposed method, a simulator is designed and an RL-based motion planning approach for a quadrotor UAV is presented. The results under various scenarios confer the superior performance of our anticipated multi-goal RL framework.},
  archive      = {J_ASOC},
  author       = {Ghulam Farid and Lanyong Zhang and Talha Younas and Muhammad Ilyas and Asma Iqbal},
  doi          = {10.1016/j.asoc.2025.113836},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113836},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-goal reinforcement learning framework for motion planning of a quadrotor UAV in 3D cluttered environment with unseen random goals},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wind speed prediction based on U-shaped 2D multi-scale model. <em>ASOC</em>, <em>184</em>, 113835. (<a href='https://doi.org/10.1016/j.asoc.2025.113835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate wind speed forecasting is essential for enhancing energy efficiency and reducing maintenance costs in wind power systems. However, existing CNN-LSTM and Transformer-based models face limitations in capturing periodic features, modeling long-term dependencies, and preserving information during multi-layer encoding. To overcome these challenges, this paper proposes a novel U-shaped 2D multi-scale model that integrates Bi-RLSTM encoding, wavelet transform, and multi-scale 2D convolution. The proposed model first encodes input sequences into a high-dimensional space via feature embedding and Bi-RLSTM, effectively capturing long-term dependencies. Subsequently, a wavelet transform extracts primary fluctuation patterns and their periods, converting 1D sequences into 2D feature maps to enhance periodic feature representation while preserving temporal information. Multi-scale convolutional layers are then employed to extract fine-grained spatial–temporal features from these maps. Finally, a Bi-RLSTM decoding layer, augmented with skip connections, mitigates information loss during deep encoding and reinforces long-range correlation modeling. Extensive experiments on two real-world wind speed datasets demonstrate that the proposed model significantly outperforms CNN-LSTM and attention-based approaches achieving superior performance in terms of MAE, RMSE, and R 2 , confirming its effectiveness and application potential.},
  archive      = {J_ASOC},
  author       = {Yue Gao and Zhongda Tian},
  doi          = {10.1016/j.asoc.2025.113835},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113835},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Wind speed prediction based on U-shaped 2D multi-scale model},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing multi-task learning-based tornado identification using spatial and temporal information from weather radar images. <em>ASOC</em>, <em>184</em>, 113834. (<a href='https://doi.org/10.1016/j.asoc.2025.113834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tornadoes, as dynamic weather phenomena, exhibit unique spatial and temporal evolution characteristics that reflect their formation and development. Existing tornado detection algorithms often struggle with high false alarm rates, primarily due to insufficient capture of temporal correlations in tornado development. As an improvement, we propose a multi-task tornado identification network with three-dimensional temporal and spatial information (TS-MTINet). Taking continuous three-frame radar data as input, the Multi-frame Temporal Interaction Block (MTIB) utilizes multi-head attention to model the dynamic interaction information between the radar data, thus exploring in-depth the temporal features during tornado development. Further, we design a Spatial-Temporal Enhancement Module (STEM), which analyzes the difference information between continuous data to extract local and global spatial and temporal feature variations about tornadoes. Based on this architecture, TS-MTINet incorporates a multi-task learning framework to perform tornado detection and number estimation tasks simultaneously, thus extracting comprehensive information related to tornadoes. To validate the performance of the proposed model, we construct the first Chinese tornado identification dataset with fine radar features. The experimental results show that the proposed method shows significant advantages in several evaluation metrics, especially in reducing false alarms. In practical case studies, compared to the traditional TVS method, TS-MTINet achieves an increase in POD of approximately 30% and a decrease in FAR of about 20% in several typical tornado events. Particularly in environments with strong interference, TS-MTINet demonstrates higher detection accuracy, reflecting greater robustness and practical value.},
  archive      = {J_ASOC},
  author       = {Jinyang Xie and Kanghui Zhou and Lei Han and Liang Guan and Maoyu Wang and Yongguang Zheng and Hongjin Chen and Jiaqi Mao},
  doi          = {10.1016/j.asoc.2025.113834},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113834},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing multi-task learning-based tornado identification using spatial and temporal information from weather radar images},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating regional green industry competitiveness in china: A CoSOGR-MABAC-sort framework. <em>ASOC</em>, <em>184</em>, 113831. (<a href='https://doi.org/10.1016/j.asoc.2025.113831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of concepts such as the green economy and sustainable development, the development of green industry has become increasingly urgent. Regional green industry competitiveness, as a key indicator reflecting the development status and competitive advantages of regional green industry, has not yet received sufficient attention and research. To fill this research gap, this paper proposes an evaluation framework, CoSOGR-MABAC-Sort (Combined Subjective-Objective, Grey Relational analysis-Multi-Attributive Border Approximation area Comparison-Sort), to assess the green industry competitiveness in 31 regions of China. In the proposed framework, the MABAC-Sort method is a novel multi-criteria decision sorting method with six classification rules. Compared to other MCDS methods, this method does not require predefined profile boundaries and can provide more detailed classification results. Furthermore, we propose an optimal model , called CoSOGR, to determine the weights of each indicator. Finally, by collecting subjective and objective data, we use the proposed framework to assess the green industry competitiveness in the 31 provinces of China. The main findings are as follows: 1) The CoSOGR demonstrates the highest consistency (99.33 %) in regions ranking compared to existing weight methods (CRITIC, EWM, ROCOSD, and MEREC). 2) The CoSOGR addresses the weight determination issues that ROCOSD (a mixed-integer linear programming model) cannot resolve. 3) The CoSOGR-MABAC-Sort method exhibit strong robustness and stability. 4) The green industry competitiveness in each region shows a positive correlation with the sustainable development of that region.},
  archive      = {J_ASOC},
  author       = {Jiafu Su and Baojian Xu and Lvcheng Li and Yijun Chen and Hongyu Liu and Na Zhang},
  doi          = {10.1016/j.asoc.2025.113831},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113831},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating regional green industry competitiveness in china: A CoSOGR-MABAC-sort framework},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoTandemML: Active learning enhanced tandem neural networks for inverse design problems. <em>ASOC</em>, <em>184</em>, 113828. (<a href='https://doi.org/10.1016/j.asoc.2025.113828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse design in science and engineering involves determining optimal design parameters that achieve desired performance outcomes, a process often hindered by the complexity and high dimensionality of design spaces, leading to significant computational costs. To tackle this challenge, we propose a novel hybrid approach that combines active learning with Tandem Neural Networks to enhance the efficiency and effectiveness of solving inverse design problems. Active learning allows to selectively sample the most informative data points, reducing the required dataset size without compromising accuracy. We investigate this approach using three benchmark problems: airfoil inverse design, photonic surface inverse design, and scalar boundary condition reconstruction in diffusion partial differential equations. We demonstrate that integrating active learning with Tandem Neural Networks outperforms standard approaches across the benchmark suite, achieving better accuracy with fewer training samples.},
  archive      = {J_ASOC},
  author       = {Luka Grbcic and Juliane Müller and Wibe Albert de Jong},
  doi          = {10.1016/j.asoc.2025.113828},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113828},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AutoTandemML: Active learning enhanced tandem neural networks for inverse design problems},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized cross-domain recommendation with meta networks and contrastive learning. <em>ASOC</em>, <em>184</em>, 113827. (<a href='https://doi.org/10.1016/j.asoc.2025.113827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel cross-domain recommendation (CDR) model that integrates matrix factorization (MF), attention networks, meta-networks, and contrastive learning (CL) to address the challenges of data sparsity and cold-start problems in recommender systems. Our model consists of five modules: the MF module extracts user and item embeddings from rating matrices in both information-rich and information-scarce domains; the transferable feature extraction module uses an attention network to identify and extract transferable features from users’ interactions in the information-rich domain; the cross-domain meta-knowledge transfer module employs a meta-network to transfer these features across domains while preserving users’ personalized preferences; the intra-domain CL module ensures temporal consistency of users’ preferences by learning from their interaction sequences in the information-rich domain; and the inter-domain CL module leverages feedback from users’ interactions in the information-scarce domain to refine the transferable features. We conduct extensive experiments on the Amazon and Douban datasets, evaluating our model across three different CDR tasks. The results demonstrate that our proposed model outperforms other prevalent CDR models in terms of MAE and RMSE. Additionally, our model shows the robust performance in cold-start scenarios, effectively utilizing both intradomain and interdomain knowledge to enhance the recommendation accuracy.},
  archive      = {J_ASOC},
  author       = {Shudong Liu and Xiping Hao and Xu Chen and Wenming Ma and Feng Gu},
  doi          = {10.1016/j.asoc.2025.113827},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113827},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Personalized cross-domain recommendation with meta networks and contrastive learning},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient real-time visual anomaly detection via frequency-aware diffusion model and information fusion. <em>ASOC</em>, <em>184</em>, 113826. (<a href='https://doi.org/10.1016/j.asoc.2025.113826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time visual anomaly detection is always a big challenge since advanced techniques usually suffer from intensive computation, while simple methods cannot obtain desirable performance. To address this issue, an Efficient Real-time Anomaly Detection method via frequency-aware diffusion model and information fusion (ERAD) is proposed, which uses the diffusion model to achieve a nice reconstruction for the frequency information obtained by Discrete Wavelet Transform (DWT) and utilizes the information fusion technique to take full consideration of local and global various features. First, the input image is fed into the DWT module to produce one low-frequency and three high-frequency coefficient images, in this way, data dimensions can be reduced by 75% but the important information can be greatly retained. Then, a diffusion model with one-step denoising is developed rather than the traditional iterative denoising to accelerate the reconstruction speed. As well, information fusion in the framework of selective fusion is embedded into the reconstruction process to improve the model performance. Furthermore, during the segmentation, a wavelet-based upsampling module is put forward to seamlessly combine global semantic context with detailed edge information, achieving both consistent semantics and high-resolution feature reconstruction. Finally, extensive experiments conducted on a variety of benchmark datasets demonstrate the remarkable superiority of our method in both model performance and time efficiency. Specifically, the proposed method achieves an Image AUROC of 99.7 and a Pixel AUROC of 99.5, with an inference time of only 0.04 s.},
  archive      = {J_ASOC},
  author       = {Xianzhe Yao and Ping Kong and Quanquan Li and Yan Song},
  doi          = {10.1016/j.asoc.2025.113826},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113826},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient real-time visual anomaly detection via frequency-aware diffusion model and information fusion},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-splitting conformal prediction for multi-step time series forecasting. <em>ASOC</em>, <em>184</em>, 113825. (<a href='https://doi.org/10.1016/j.asoc.2025.113825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is crucial for applications like resource scheduling and risk management, where multi-step predictions provide a comprehensive view of future trends. Uncertainty Quantification (UQ) is a mainstream approach for addressing forecasting uncertainties, with Conformal Prediction (CP) gaining attention due to its model-agnostic nature and statistical guarantees. However, most variants of CP are designed for single-step predictions and face challenges in multi-step scenarios, such as reliance on real-time data and limited scalability. This highlights the need for CP methods specifically tailored to multi-step forecasting. We propose the Dual-Splitting Conformal Prediction (DSCP) method, a novel CP approach designed to capture inherent dependencies within time-series data for multi-step forecasting. Experimental results on real-world datasets from four different domains demonstrate that DSCP significantly outperforms existing CP variants in terms of the Winkler Score, improving performance by up to 23.59% compared to state-of-the-art methods. Furthermore, the deployment of DSCP for renewable energy generation and IT load forecasting in the power management of a real-world trajectory-based application achieves an 11.25% reduction in carbon emissions through predictive optimization of data center operations and control strategies.},
  archive      = {J_ASOC},
  author       = {Qingdi Yu and Zhiwei Cao and Ruihang Wang and Zhen Yang and Lijun Deng and Min Hu and Yong Luo and Xin Zhou},
  doi          = {10.1016/j.asoc.2025.113825},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113825},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-splitting conformal prediction for multi-step time series forecasting},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FDP-net: Fourier transform guided lightweight depthwise and pointwise dynamic pooling based neural network for medical image classification. <em>ASOC</em>, <em>184</em>, 113824. (<a href='https://doi.org/10.1016/j.asoc.2025.113824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, deep learning-based medical image classification has become essential, especially in developing countries because of the high volume of patients with less medical professionals as well as required infrastructures. Deep learning models often help in the early detection of diseases; however, it require a high amount of processing power, and sometimes it becomes less scalable for various computer-aided diagnosis. To this end, in this paper, a lightweight Fourier Transform guided Depth and Pointwise Dynamic Pooling based Neural Network (FDP-Net), has been proposed for medical image classification. This paper introduces a Depth and Pointwise Feature Fusion (DPFF) block for learning the important features with less computation and without increasing the model parameters. It also proposes a dynamic pooling technique, an alternative to traditional max-pooling, which dynamically selects the important features. The proposed FDP-Net model is trained to classify medical images with the guidance of Fourier Transformation and multitask loss function, which makes the model converge faster and reduces overfitting. The proposed model has been tested on Acute Lymphoblastic Leukemia (ALL) dataset, Peripheral Blood Cell (PBC) dataset, and Raabin White blood Cell (Raabin-WBC) dataset, and it outperforms the state-of-the-art models with 100%, 98.13% and 96.79% classification accuracies, respectively. Additionally, the proposed model is made with only 0.349 million parameters, thereby enabling faster processing. Code will be avilabe at https://github.com/asfakali/FDP-Net .},
  archive      = {J_ASOC},
  author       = {Asfak Ali and Rajdeep Pal and Aishik Paul and Ram Sarkar},
  doi          = {10.1016/j.asoc.2025.113824},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113824},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FDP-net: Fourier transform guided lightweight depthwise and pointwise dynamic pooling based neural network for medical image classification},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explicit cooperation mechanism and multimodal fusion prediction model empower DouDizhu agents. <em>ASOC</em>, <em>184</em>, 113823. (<a href='https://doi.org/10.1016/j.asoc.2025.113823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imperfect information games require agents to make decisions under uncertainty, presenting significant challenges yet offering broad applicability in real-world scenarios. DouDizhu, a representative example, features complex cooperative-competitive dynamics, imperfect information, and a large strategy space. To address these challenges, a strategic decision-making algorithm is developed by integrating multi-agent credit allocation with information prediction. Specifically, an explicit credit allocation mechanism based on team reward decomposition improves cooperative behavior among peasant agents by enabling more stable and targeted policy updates. Meanwhile, combining long short-term memory networks and multi-head attention enhances the prediction of hidden opponent information through multimodal feature fusion. Moreover, convolutional neural networks are incorporated into the DouZero framework to extract high-level features and reduce the policy solution space. The resulting CAPRE_DMC significantly improves agent performance in adversarial settings. Extensive evaluations demonstrate that CAPRE_DMC outperforms baseline DouDizhu agents, achieving substantial gains in point margin and win percentage. Additionally, evaluations on benchmark multi-agent cooperative tasks demonstrate the framework’s scalability and generality in large-scale imperfect-information environments.},
  archive      = {J_ASOC},
  author       = {Jiao Wang and Longyue Fu and Xiang Li and Hongchen Luo and Zhifen Guo},
  doi          = {10.1016/j.asoc.2025.113823},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113823},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explicit cooperation mechanism and multimodal fusion prediction model empower DouDizhu agents},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight detection method for X-ray security inspection based on YOLOV8. <em>ASOC</em>, <em>184</em>, 113822. (<a href='https://doi.org/10.1016/j.asoc.2025.113822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of public safety, the prohibited items detection during X-ray security inspection is crucial for preventing potential threats. The recent advances in artificial intelligence, particularly deep learning, have achieved success in this area. However, the high number of parameters and computational load of these deep learning-based object detection methods result in significant hardware requirements limiting their practical application. To address these issues, a lightweight method for detecting prohibited items in X-ray security inspections, utilizing the YOLOV8 framework, is proposed in this paper. The key innovations of this method are threefold. First, a lightweight convolution module is designed to optimize the YOLOV8 model. This optimization significantly reduces both the number of parameters and the computational load, making the model more efficient. Second, an adaptive spatial-and-channel attention module is designed to enhance feature extraction capabilities. This module enhances feature extraction capabilities without compromising detection accuracy. Third, the Wise-IOU loss function is incorporated to enhance the overall performance of the detection method during training. Finally, the method is evaluated on our real X-ray pseudo-color image dataset, PIDRAY and CLCXRAY with the existing methods. Our proposed method attains a detection accuracy of 98.83 % on the self-constructed dataset, demonstrating a 0.6 % improvement, while concomitantly reducing the parameters by 64.2 % and computational load by 66.3 %. Furthermore, it achieves accuracy enhancements of 0.7 % and 0.82 % on the two public datasets, respectively. The experimental results indicate that this method not only reduces hardware requirements due to a lower number of parameters and computational load but also broadens the model's applicability. This work underscores the importance of balancing model complexity with performance and sets the stage for future research in AI-driven security inspection technologies.},
  archive      = {J_ASOC},
  author       = {Xizhuo Yu and Chunyang Chen and Shu Cheng and Jingming Li},
  doi          = {10.1016/j.asoc.2025.113822},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113822},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight detection method for X-ray security inspection based on YOLOV8},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic transformation of basic probability assignment based on weighted visibility graph networks. <em>ASOC</em>, <em>184</em>, 113821. (<a href='https://doi.org/10.1016/j.asoc.2025.113821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dempster-Shafer evidence theory (DSET) provides a powerful framework for uncertain reasoning, offering a theoretical basis for decision-making under ambiguity. However, its core representation, the basic probability assignment (BPA), cannot be directly applied to probabilistic decision-making, prompting the need for effective probability transformation methods. A key challenge lies in quantifying the uncertainty inherent in BPAs to guide this transformation process. To address this, we propose an improved probabilistic transformation method that integrates belief entropy and weighted visibility graph networks, which yields more accurate and interpretable probability distributions than existing approaches. Specifically, given a frame of discernment and mass function, we first apply two refined belief entropy measures to evaluate the informational content of each focal element. Based on these entropy-derived orderings, we construct a weighted visibility graph that captures the structural relationships among focal elements. The weights from this graph are then used to compute a proportional belief transformation. Experimental validation across benchmark cases on classical BPA scenarios demonstrates that our method outperforms traditional approaches in terms of entropy consistency and decision quality, as evidenced by lower Kullback–Leibler (KL) divergence, higher probability information capacity (PIC), and reduced Shannon entropy. These results highlight the method’s dual advantage in balancing decisiveness (via PIC maximization) and fidelity (via entropy minimization), making it a robust tool for uncertainty-aware decision support systems.},
  archive      = {J_ASOC},
  author       = {Yongchuan Tang and Kangkang Wu and Rongfei Li and He Guan and Deyun Zhou and Yubo Huang},
  doi          = {10.1016/j.asoc.2025.113821},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113821},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Probabilistic transformation of basic probability assignment based on weighted visibility graph networks},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Electric network frequency-based digital audio tampering event identification using multimodal feature interaction network. <em>ASOC</em>, <em>184</em>, 113820. (<a href='https://doi.org/10.1016/j.asoc.2025.113820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of digital Audio Tampering Event (ATE) detection based on Electric Network Frequency (ENF), accurately extracting ENF signals is crucial for tampering event identification. However, ENF signals are susceptible to noise interference, and it is difficult to establish an effective matching relationship with the reference frequency database, which poses a challenge to the effectiveness of existing ATE detection methods. To solve these problems, a Multimodal Feature Interactive Network (MFIN) is proposed for ATE identification under low-SNR conditions. First, a Modified Kaiser-window-based S-Transform (MKST) method is proposed to extract the estimated frequency, phase, rate of change of frequency, and rate of change of phase of the ENF component in digital audio. The frequency and time resolution are increased through the improved control function, thereby enhancing the accuracy of ENF estimation. Subsequently, a Multi-Branch Multi-Scale Attention Convolutional (MBSAC) neural network is further proposed to extract and classify ENF features. In MBSAC, the Multi-Branch Multi-Scale Temporal (MBST) attention fusion block, serving as the network’s primary structure, increases feature diversity and enhances classification performance. Finally, the ATE data acquisition hardware platform is built. Experimental results on a dataset comprising six types of ATEs demonstrate that the proposed MFIN outperforms several state-of-the-art ATE detection methods in both ENF extraction and tampering type classification.},
  archive      = {J_ASOC},
  author       = {Bing Li and Junfeng Duan and Yao Zheng and Xinxin Cai and Wei Qiu and He Yin and Wenxuan Yao},
  doi          = {10.1016/j.asoc.2025.113820},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113820},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Electric network frequency-based digital audio tampering event identification using multimodal feature interaction network},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing financial resilience in manufacturing SMEs: A q-rung picture fuzzy set-based decision framework for digital transformation adoption. <em>ASOC</em>, <em>184</em>, 113819. (<a href='https://doi.org/10.1016/j.asoc.2025.113819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital transformation (DT) has emerged as a crucial strategy for enhancing the financial resilience of small and medium-sized enterprises (SMEs) in the manufacturing sector. Despite its importance, there is a significant gap in the quantitative evaluation of the factors influencing SMEs’ DT adoption. This study addresses this gap by developing a decision framework using q-rung picture fuzzy sets (q-RPF) to identify and analyze the drivers of DT implementation aimed at boosting financial resilience in manufacturing SMEs. The proposed framework incorporates the q-RPF-weighted Heronian mean aggregation operator to aggregate expert evaluations and capture the interrelationships among input decision data. A novel weighting approach, based on the q-RPF deviation measure, is introduced to assess the significance of various enablers. Furthermore, the q-RPF-MARCOS’H model is employed to evaluate the effectiveness of DT in enhancing financial resilience across different SMEs by integrating the methodologies mentioned above. A numerical example involving manufacturing SMEs from a specific city demonstrates the practical application of the q-RPF-MARCOS’H model-based framework. The results reveal that “concurrent operations” (0.102) is a significant enabler of DT adoption in promoting financial resilience. The framework’s validity is confirmed through both sensitivity and comparative analyses. These outcomes offer recommendations for policymakers and industry leaders to design effective incentives for DT adoption and provide practical guidance for SMEs seeking to leverage DT for improved financial resilience.},
  archive      = {J_ASOC},
  author       = {Zhengyan Yang and Wei Zhong Wang and Zelin Wang and Muhammet Deveci and Dursun Delen},
  doi          = {10.1016/j.asoc.2025.113819},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113819},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing financial resilience in manufacturing SMEs: A q-rung picture fuzzy set-based decision framework for digital transformation adoption},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A regret theory-based three-way decision model under comparative linguistic expressions. <em>ASOC</em>, <em>184</em>, 113816. (<a href='https://doi.org/10.1016/j.asoc.2025.113816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real world, experts often prefer to utilize linguistic expressions over numerical data when evaluating alternatives. However, due to the complexity of actual decision-making, single linguistic terms are insufficient for experts to express their judgments accurately. Thus, it is necessary to employ a richer form of linguistic expression, known as comparative linguistic expressions (CLEs). Furthermore, existing multi-attribute decision-making (MADM) models under CLE suffer from the following limitations: (1) they fail to provide decision-making references for alternatives; (2) they do not incorporate the psychological factors of experts. In order to address the aforementioned challenges, this paper proposes a novel regret theory (RT)-based three-way decision (TWD) model under CLE. First, the attribute weights are calculated by an improved optimization model. This model combines index variability with comprehensive entropy, enabling a more objective conclusion to be drawn regarding the relative importance of attributes. Second, an enhanced neighborhood relationship is introduced, which is shown to fulfill the properties of Symmetry, Reflexivity, and Non-transitivity. Building on this foundation, this study integrates RT with the neighborhood relationship to construct a wide TWD framework. This framework incorporates decision-making references for the alternatives and accounts for the influence of experts’ psychological factors. Subsequently, the ranking of alternatives is determined using the technique for order preference by similarity to ideal solution (TOPSIS) method. Finally, the feasibility of the proposed method is demonstrated through a real-case study. Comparative experiments and parameter sensitivity analysis are designed to demonstrate the superiority and effectiveness of the model.},
  archive      = {J_ASOC},
  author       = {Zhanhao Liu and Huangjian Yi and Yushan Yao and Jiajia Wang},
  doi          = {10.1016/j.asoc.2025.113816},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113816},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A regret theory-based three-way decision model under comparative linguistic expressions},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A Q-learning based estimation of distribution algorithm for automated guided vehicle scheduling in disassembly workshop. <em>ASOC</em>, <em>184</em>, 113815. (<a href='https://doi.org/10.1016/j.asoc.2025.113815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid upgrade of electronic products, the disassembly process is becoming increasingly crucial in facilitating resources recycling. In practical disassembly workshops, the automated guided vehicle (AGV) plays an important role in improving efficiency of product transportation between different disassembly machines. In this paper, a Q-learning based estimation of distribution algorithm (QEDA) is proposed to solve the AGV scheduling problem in disassembly workshops with minimization of makespan for all transportation tasks. Firstly, a mathematical model for the problem is established, and specific encoding and decoding rules are designed for solution representation. Secondly, a novel update mechanism of EDA is designed by fully information utilization of both elite and poor solutions to accelerate convergence. Thirdly, a local search strategy based on Q-learning with three time-related states is designed for the elite solutions to enhance exploitation. Finally, comparative experiments on 120 benchmark test sets demonstrate that the QEDA outperforms the state-of-the-art algorithms in both solution quality and convergence speed, confirming its superior effectiveness for AGV scheduling in disassembly workshops.},
  archive      = {J_ASOC},
  author       = {Honggui Han and Teng Wang and Jingjing Wang},
  doi          = {10.1016/j.asoc.2025.113815},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113815},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A Q-learning based estimation of distribution algorithm for automated guided vehicle scheduling in disassembly workshop},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Criterion-guided polypharmacy side effects prediction with dual-view contrastive learning. <em>ASOC</em>, <em>184</em>, 113814. (<a href='https://doi.org/10.1016/j.asoc.2025.113814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting side effects from taking multiple drugs (polypharmacy) is a critical challenge in healthcare, with significant implications for patient safety. Traditional machine learning methods often overlook the multi-scale nature of drug information and the interdependencies among side effects, limiting their predictive power. To address these gaps, we propose CASE (Criterion-guided polyphArmacy Side Effects prediction with dual-view contrastive learning). CASE captures both microscopic (molecular graph) and macroscopic (biochemical knowledge) drug features via an adaptive substructure encoder and a biochemical feature aggregator, respectively, under a dual-view framework. Then, contrastive learning is employed to jointly balance and integrate structural and biochemical representations, enabling a more comprehensive and synergistic understanding of drug interactions. Moreover, a criterion-guided decoding strategy is designed to model complex side effect relationships. This effective design enables the proposed CASE model to achieve superior performance, with an accuracy of 88.70 %, precision of 83.62 %, recall of 96.45 %, F1-score of 89.56 %, AUROC of 94.75 %, and AUPRC of 91.16 %, respectively, on the benchmark dataset. Ablation experiments and case studies further confirm the robustness and practical utility of the model. These results demonstrate that CASE can effectively assist clinical decision-making by identifying high-risk drug combinations and delivering reliable polypharmacy risk assessments, thereby supporting safer and more personalized treatment strategies.},
  archive      = {J_ASOC},
  author       = {Yike Wang and Huifang Ma and Zihao Gao and Zhixin Li and Liang Chang},
  doi          = {10.1016/j.asoc.2025.113814},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113814},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Criterion-guided polypharmacy side effects prediction with dual-view contrastive learning},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying suicidal ideations from social media posts using deep learning and explainable AI-driven approach. <em>ASOC</em>, <em>184</em>, 113813. (<a href='https://doi.org/10.1016/j.asoc.2025.113813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background At present, suicide has become one of the leading causes of unnatural deaths worldwide. Individuals having suicidal urges often express their self-harming ideas through social media posts. Early identification of such ideations is critical for timely intervention and prevention. Besides, continuous assessment of the texts containing suicidal thoughts can uncover the hidden triggers of suicidal urges. This study presents a comprehensive approach to analyze the user-generated textual contents on social media that reflect suicidal ideas. Methodology For identifying the underlying topics that express suicidal ideations, this study has employed the Latent Dirichlet Allocation (LDA) model. Semantic Network Analysis (SNA) is used to gain a deeper quantitative and qualitative insight into these texts. Besides, an exploratory investigation of different deep learning (DL) models has been performed to identify the posts speculating suicidal ideations. Furthermore, this study has integrated Explainable AI (XAI) techniques like Local Interpretable Model-agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP) to enhance interpretability of the decisions taken by the DL models. Techniques like LDA and SNA offer a better understanding of the linguistic features of the suicidal posts, while the integration of the XAI techniques with the DL models elevates the transparency of their decisions. Contributions This study has developed an end-to-end web application, that can perform real-time classification of posts for suicidal ideation. Moreover, this application can provide insights into the rationale behind the taken decisions. This study aims to contribute to suicide prevention efforts through an innovative combination of computational techniques and AI-driven tools.},
  archive      = {J_ASOC},
  author       = {Md. Sabab Zulfiker and Nasrin Kabir and Al Amin Biswas and Md. Mashih Ibn Yasin Adan and Mohammad Shorif Uddin},
  doi          = {10.1016/j.asoc.2025.113813},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113813},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identifying suicidal ideations from social media posts using deep learning and explainable AI-driven approach},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight network enhanced by attention-guided cross-scale interaction for underwater object detection. <em>ASOC</em>, <em>184</em>, 113811. (<a href='https://doi.org/10.1016/j.asoc.2025.113811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the image quality degradation caused by multipath effects and scattering in underwater environments, we propose a lightweight neural network architecture, PRCII-Net, optimized for small target detection under complex underwater conditions. First, a Progressive Re-parameterized Attention-based Intra-scale Feature Interaction module (PR-AIFI) is proposed, which improves the network performance while reducing the difficulty of training and maintaining training stability. Second, a feature pyramid network named as the Cross-scale Information Interaction Feature Pyramid Network (CII-FPN) is proposed, including the two main fusion structures. The CII-FPN not only fully utilizes shallow and deep information, but also enhances the network’s spatial representation and the interaction between deep feature layers, thereby boosting the detection capability for small targets. Meanwhile, to reduce the model’s size and resource consumption, 1x1 convolutions are introduced into the backbone network for efficient channel compression and cross-channel feature fusion, significantly lowering computational complexity. Experiments on the Detecting Underwater Objects (DUO) and Real-time Underwater Object Detection (RUOD) datasets demonstrate that PRCII-Net outperforms existing real-time neural network models in mAP and F1 scores while maintaining efficient inference on resource-constrained devices.},
  archive      = {J_ASOC},
  author       = {Dehua Zhang and Changcheng Yu and Zhen Li and Chunbin Qin and Ruixue Xia},
  doi          = {10.1016/j.asoc.2025.113811},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113811},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight network enhanced by attention-guided cross-scale interaction for underwater object detection},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preferences relative ordering by frequency inclusion technique (PROFIT) for optimizing performance evaluation. <em>ASOC</em>, <em>184</em>, 113807. (<a href='https://doi.org/10.1016/j.asoc.2025.113807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this investigation is to introduce the novel concept of the neutro-cardinal family of inclusion measures, a four-parameter family of fuzzy inclusion measures tailored for single-valued neutrosophic sets (SVNSs). These measures are built using a new scalar cardinality measure for SVNSs, termed weighted average cardinality, and incorporate logical operators such as single-valued neutrosophic Frank t-norms and their corresponding dual t-conorms. The versatility offered by this four-parameter family allows the variation of t-norms and t-conorms within each measure based on a fixed combination of parameters. This flexibly supports modeling various layers of human cognitive behavior, particularly the outermost states: optimism (via fuzzy Lukasiewicz t-norms and t-conorms), neutrality (via product and probabilistic sum operators), and pessimism (via min and max operators). Moreover, capturing these distinct mental states, the family allows combinations of them, enabling the representation of more complex and deeper cognitive processes. Consequently, the proposed inclusion measures serve as robust and versatile mathematical tools for designing artificial intelligence (AI) systems in multi-criteria decision making (MCDM) environments. Building upon this family, we present a new MCDM method named PROFIT (Preferences Relative Ordering by Frequency Inclusion Technique). PROFIT is designed as a simple yet effective decision-making approach that leverages the neutro-cardinal measures to support AI-based evaluation processes. It is investigated in the context of organizational management, in which it enables diligent performance evaluation. The PROFIT model’s data-driven and adaptive structure is consistent with resilience-focused initiatives like Resilient Manufacturing, ManuChain II, and Towards Resilience in Industry 5.0. As such, it serves as a strategic tool for optimizing decision-making, enhancing performance, and strengthening organizational resilience in dynamic and complex environments.},
  archive      = {J_ASOC},
  author       = {Madiha Qayyum and Dania Farooq and Muhammad Riaz and Muhammad Aslam and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2025.113807},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113807},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Preferences relative ordering by frequency inclusion technique (PROFIT) for optimizing performance evaluation},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A state alignment-centric approach to federated system identification: The FedAlign framework. <em>ASOC</em>, <em>184</em>, 113800. (<a href='https://doi.org/10.1016/j.asoc.2025.113800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents FedAlign, a Federated Learning (FL) framework, designed for System Identification (SYSID) of linear State-Space Models (SSMs) by aligning state representations. Local workers can learn linear SSMs with equivalent representations but different parameter basins. We demonstrate that directly aggregating these local SSMs via FedAvg results in a global model with altered system dynamics. FedAlign overcomes this problem by employing similarity transformation matrices to align state representations of local SSMs, thereby establishing a common parameter basin that retains the dynamics of local SSMs. FedAlign computes similarity transformation matrices via two distinct approaches. In FedAlign-A, we represent the global SSM in controllable canonical form (CCF). We use control theory to analytically derive similarity transformation matrices that convert each local SSM into this form. Yet, establishing global SSM in CCF brings additional alignment challenges in multi-input multi-output SYSID, as CCF representation is not unique, unlike in single-input single-output SYSID. In FedAlign-O, we address the alignment challenges by reformulating the local parameter basin alignment problem as an optimization task. We set the parameter basin of a local worker as the common parameter basin and solve least square problems to obtain the transformation matrices needed to align the remaining local SSMs. The experiments conducted on synthetic and real-world datasets show that FedAlign outperforms FedAvg, converges faster, and provides improved global SSM stability thanks to local parameter basins’ alignment.},
  archive      = {J_ASOC},
  author       = {Ertuğrul Keçeci and Müjde Güzelkaya and Tufan Kumbasar},
  doi          = {10.1016/j.asoc.2025.113800},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113800},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A state alignment-centric approach to federated system identification: The FedAlign framework},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal harmonics prediction for distribution systems powered by multi-energy sources using bidirectional long-short term memory combined with data sequence. <em>ASOC</em>, <em>184</em>, 113799. (<a href='https://doi.org/10.1016/j.asoc.2025.113799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-energy resource aims to maintain a balance between energy output and load consumption and to ensure power continuity during different operating conditions. The harmonic distortions can be estimated from the output current of a harmonic source, which may not fully reflect its true harmonic distortions due to the interactions between the state changes at the power network level and the harmonic sources. System operators monitor each system's harmonic performance under different conditions of operation to find the actual contribution of grid-connected systems to harmonic-related issues. Development of machine learning algorithms leads to effective progress in the harmonic prediction and computation. In this paper, the combined data sequencing, and Bidirectional Long-Short Term Memory (Bi-LSTM) network has been exploited for the real-time harmonic prediction of future events in multi-energy sources. The validity of the proposed Model including the applications of ANFIS, ANNs, MLRA and LSTM is conducted on the two standard systems as IEEE 9-bus and IEEE 34-bus multi energy resources system that is associated with PV systems. The simulation results, based on climate changes of solar irradiance and ambient temperature in PV systems, demonstrate that the proposed methods can accurately forecast changes in total harmonic distortion (THD) as well as the voltage profile at the point of common coupling. The performance of Bi-LSTM, original LSTM, Machine Linear Regression (MLR), and Artificial Neural Networks (ANNs) techniques were assessed. These findings provide valuable insights. Four performance validation indices, RMSE, R-squared and MSE are considered to assess the performance of the competitive learning algorithms. The results showed that in the model IEEE 9-bus, Bi-LSTM outperformed all the applied methods as its RMSE value was 0.000019 while its MSE value was 3.61e-10 and finally, the Bi-LSTM had a higher value squared error (R 2 ) was equal 1 which indicates the effectiveness of Bi-LSTM for predicting sequential total harmonic distortion. On the other hand, in case study of IEEE 34-bus, the RMSE, MSE and R 2 are 0, 3.276e-30 and 1 using Bi-LSTM which means that the Bi-LSTM leads to the best performance validation indices compared to other competitive algorithms for the tested multi-energy systems.},
  archive      = {J_ASOC},
  author       = {Hasnaa M. El-Arwash and Almoataz Y. Abdelaziz and Ragab A. El-Sehiemy},
  doi          = {10.1016/j.asoc.2025.113799},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113799},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal harmonics prediction for distribution systems powered by multi-energy sources using bidirectional long-short term memory combined with data sequence},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced robot path planning on rough terrain: A Q-learning-based multi-objective PSO algorithm. <em>ASOC</em>, <em>184</em>, 113798. (<a href='https://doi.org/10.1016/j.asoc.2025.113798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning remains one of the most extensively studied problems in mobile robotics. While particle swarm optimization (PSO) has been widely adopted for this task, conventional implementations exhibit critical limitations including being prone to local optima, lacking population diversity, and having low accuracy — all of which compromise both the quality and efficiency of path planning solutions. To address these challenges, this paper proposes a novel hybrid algorithm called MOQLCPSO that synergistically integrates Q-learning (QL) with crossover operators into multi-objective particle swarm optimization (MOPSO) for car-like mobile robots operating in known static rough terrain environments. The proposed framework aims to generate collision-free optimal paths characterized by minimal length and terrain roughness through three key innovations: Firstly, we implement QL-based dynamic parameter adaptation to autonomously adjust PSO’s inertia weight and learning factors during optimization, effectively enhancing convergence towards the Pareto front. Secondly, a strategic crossover operator is introduced to augment exploration capabilities and maintain population diversity throughout the optimization process. Finally, comprehensive comparative simulations against state-of-the-art alternatives demonstrate MOQLCPSO’s superior performance metrics. The experimental results reveal statistically significant improvements in search accuracy, population diversity, and optimization efficiency across multiple terrain complexity levels. Notably, when benchmarked against the most competitive contemporary algorithm, MOQLCPSO achieves path length reduction of 0.64%, 2.42%, and 3.87%, terrain roughness reduction of 6.68%, 5.24%, and 4.72% in simple, moderately complex, and highly complex terrains, respectively. These advancements highlight the algorithm’s strong potential for deployment in multi-objective optimization scenarios ranging from autonomous mobile robot navigation and smart manufacturing resource allocation to power grid dispatch operations.},
  archive      = {J_ASOC},
  author       = {Zhaoxia Duan and Yi Zhang and Zhen Shao and Zhen Xu and Zhengrong Xiang},
  doi          = {10.1016/j.asoc.2025.113798},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113798},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advanced robot path planning on rough terrain: A Q-learning-based multi-objective PSO algorithm},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combined variable precision fuzzy rough set and its application in medical diagnosis. <em>ASOC</em>, <em>184</em>, 113797. (<a href='https://doi.org/10.1016/j.asoc.2025.113797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data processing, variable precision fuzzy rough set plays a crucial role in removing noisy data. Different variable precision ideas have been used to solve various problems and achieve different goals. Among them, the variable precision ideas by Zhao and Yao are widely recognized by researchers. However, a single method is often insufficient to solve complex problems. In this study, we combined these two variable precision ideas and introduced a new variable precision fuzzy rough set model ( O - C-VPFR ), which includes the advantages and properties of the aforementioned two models. Next, using the concept of attribute importance in O - C-VPFR , we developed a simple, objective method for calculating attribute weights. Based on this method, we further developed a comprehensive decision-making method integrating Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) and Preference Ranking Organization Method for Enrichment Evaluations (PROMETHEE). The new method was applied to the problem of health risk assessment of pregnant women (HRPW, https://archive.ics.uci.edu/dataset/863/maternal+health+risk ). We conducted parameter and comparative analyses by randomly selecting 10 data points from HRPW, proving the stability and reliability of our method through the Pearson correlation coefficient. Further, we used the entire dataset and performed an ordered similarity experiment and a hypothesis testing experiment to verify the stability, effectiveness, and robustness of the proposed method. In all experiments, our method comprehensively ranked pregnant women, identified high-risk individuals, and enabled timely treatment.},
  archive      = {J_ASOC},
  author       = {Jingwen Xie and Lingqiang Li},
  doi          = {10.1016/j.asoc.2025.113797},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113797},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combined variable precision fuzzy rough set and its application in medical diagnosis},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing evacuation path planning in high-rise building fires using an improved ant colony algorithm and dynamic window approach under smoke control scenarios. <em>ASOC</em>, <em>184</em>, 113796. (<a href='https://doi.org/10.1016/j.asoc.2025.113796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional emergency evacuation strategies are constrained by limited adaptability to dynamic conditions and inefficient allocation of rescue resources. To assess the impact of smoke control facilities during high-rise fire evacuations, Building EXODUS software was used to simulate occupant behavior under the influence of air curtains and mechanical smoke exhaust systems. Furthermore, to address real-time obstacle avoidance in dynamic environments and during group movement in fire emergencies, a multi-person evacuation path planning method was developed by integrating an improved ant colony optimization algorithm with the dynamic window approach. This method incorporates a heuristic function that considers fire conditions and potential fields, along with a dynamic pheromone update strategy featuring reward and punishment mechanisms. The results indicate that the effectiveness of smoke control facilities in improving evacuation outcomes ranks as follows: combined operation of mechanical smoke extraction and air curtains, mechanical smoke extraction alone, and air curtains alone. Compared to the standard ant colony algorithm, the improved algorithm reduced planned path length by 7.71 %, decreased turn times by 73.30 %, and improved computational efficiency by 6.56 %. Furthermore, under scenarios with and without operational smoke control facilities, the improved algorithm combined with the dynamic window approach reduced path overlap areas by 11.76 % and 18.19 %, respectively, and shortened path lengths by 13.49 % and 14.19 %, respectively, compared to the Building EXODUS. The proposed method effectively addresses dynamic fire environments in high-rise buildings and provides a theoretical foundation for intelligent occupant evacuation.},
  archive      = {J_ASOC},
  author       = {Yaping Yu and Qinghe Wang and Lu Jin and Ji-nan Ding and Faqi Liu},
  doi          = {10.1016/j.asoc.2025.113796},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113796},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing evacuation path planning in high-rise building fires using an improved ant colony algorithm and dynamic window approach under smoke control scenarios},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ribonucleic-acid protein interaction prediction based on deep learning: A comprehensive survey. <em>ASOC</em>, <em>184</em>, 113795. (<a href='https://doi.org/10.1016/j.asoc.2025.113795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interaction between Ribonucleic Acids (RNAs) and proteins, also called RNA Protein Interaction (RPI), governs biological processes, including gene regulation and disease pathogenesis. This comprehensive survey examines Artificial Intelligence (AI) applications in Deep Learning-based RPI Prediction (DL-based RPIP) through eight Research Questions (RQs), analyzing 179 studies (2014–2023). The key findings include: sustained technical evolution through embryonic (2014–2017), accelerated (2018–2022), and expansion phases (2023) (RQ1); hybrid models integrating Graph Neural Networks (GNNs) (for topological interface modeling) and Transformers (for long-range dependencies) achieve state-of-the-art performance (RQ4); pretrained language models enhance small-sample learning, but the cross-species generalization declines sharply with evolutionary distance (RQ5). Critical challenges persist, including data heterogeneity across databases, the scarcity of standardized benchmarks (RQ2), and balancing the trade-off between feature encoding and information preservation (RQ3). Future advancements require biologically informed DL architectures, multi-feature fusion, and rigorous cross-validation to bridge the generalization-interpretability gap (RQ8): This would accelerate the clinical translation of predictive tools (RQ6/RQ7). As the first comprehensive analysis spanning feature encoding, modeling, evaluation, applications, and tools, this work fills a critical gap in the DL-based RPIP literature.},
  archive      = {J_ASOC},
  author       = {Danyu Li and Rubing Huang and Chenhui Cui and Dave Towey and Ling Zhou and Jinyu Tian and Bin Zou},
  doi          = {10.1016/j.asoc.2025.113795},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113795},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ribonucleic-acid protein interaction prediction based on deep learning: A comprehensive survey},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level pooling self-adaptive evolutionary multi-objective gene selection algorithm for microarray data classification. <em>ASOC</em>, <em>184</em>, 113794. (<a href='https://doi.org/10.1016/j.asoc.2025.113794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene selection is a vital preprocessing technique for cancer classification using microarray data, focusing on identifying a subset of genes that achieve high classification accuracy while minimizing the gene selection rate. Despite the prevalence of multi-objective optimization algorithms in microarray data classification, balancing high classification accuracy and effective guidance in microarray classification remains challenging for most existing algorithms. This study proposes a novel multi-objective gene selection method, MOGS-MLPSAE, which utilizes multi-level pooling and self-adaptive evolutionary techniques to enhance classification accuracy. MOGS-MLPSAE employs a Pareto-based ranking pool division strategy to facilitate cross-level learning among individuals and introduces a population-biased evolutionary mechanism with five rules to drive the population toward higher classification accuracy. Compared with seven state-of-the-art multi-objective algorithms across 14 microarray datasets, MOGS-MLPSAE achieves superior performance, with classification accuracy 1.56–8.04 % higher than other algorithms and the lowest gene selection rate (average 1 %, minimum 0.01 %). This study demonstrates MOGS-MLPSAE's effectiveness in balancing classification accuracy and gene selection rate, offering a robust solution for microarray-based cancer classification.},
  archive      = {J_ASOC},
  author       = {Min Li and Rutun Cao and Chen Jin and Junke Wang and Shaobo Deng and Xiang Yu},
  doi          = {10.1016/j.asoc.2025.113794},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113794},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-level pooling self-adaptive evolutionary multi-objective gene selection algorithm for microarray data classification},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Activity transitions for semi-supervised federated learning in sensor-based human activity recognition. <em>ASOC</em>, <em>184</em>, 113793. (<a href='https://doi.org/10.1016/j.asoc.2025.113793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensor-based Human Activity Recognition (HAR) is a core component in many real-world applications such as healthcare, fitness tracking, and smart environments. However, training effective HAR models is often constrained by limited labeled data and growing privacy concerns. Federated Learning (FL) offers a privacy-preserving solution by enabling collaborative model training without sharing raw sensor data. To further address the label scarcity challenge, we propose ATCoFed, a Semi-Supervised Federated Learning (SSFL) framework that introduces a novel pseudo-label filtering method based on activity transition patterns. Unlike existing approaches that rely solely on confidence thresholds, ATCoFed incorporates temporal context by using Long Short-Term Memory (LSTM) networks and Large Language Models (LLMs) as data-driven evaluators to validate the consistency of predicted activity sequences. This dual-evaluator mechanism improves the quality of pseudo-labels and enhances model robustness. Experimental results on benchmark HAR datasets demonstrate that ATCoFed consistently outperforms SSFL baselines, achieving better accuracy while maintaining computational and communication efficiency. These findings highlight the potential of activity-aware filtering to improve semi-supervised learning in privacy-preserving HAR applications.},
  archive      = {J_ASOC},
  author       = {Tori Andika Bukit and Ericka Pamela Bermudez Pillado and Bernardo Nugroho Yahya and Seok-Lyong Lee},
  doi          = {10.1016/j.asoc.2025.113793},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113793},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Activity transitions for semi-supervised federated learning in sensor-based human activity recognition},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TPCMEA: A tri-population evolutionary algorithm with adaptive stage-switching for complex CMOPs. <em>ASOC</em>, <em>184</em>, 113792. (<a href='https://doi.org/10.1016/j.asoc.2025.113792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained multi-objective optimization problems (CMOPs) involve the optimization of objective functions along with the satisfaction of constraints, presenting significant challenges in obtaining optimal solutions. To address difficulties such as conflicting objectives, complex constraints, disconnected and narrow feasible regions, this paper proposes a novel tri-population constrained multi-objective evolutionary algorithm (TPCMEA). This algorithm innovatively introduces an adaptive diversity-convergence stage-switching method (DCSSM), based on dual indicators of diversity and convergence, by dynamically monitoring the Euclidean distance distribution of the populations and convergence towards the optimal objective vector, overcoming the issues of premature convergence or resource wastage in traditional algorithms. Moreover, TPCMEA adopts a collaborative framework with three populations, each utilizing different search strategies to effectively solve complex CMOPs. These strategies encompass achieving rapid convergence under constraints, exploring unconstrained fronts to enhance solution diversity, and integrating information from the first two populations through knowledge transfer and compressive sampling strategies. Experimental results demonstrate that TPCMEA exhibits significant competitive advantages across the MW, LIRCMOP, CF benchmark test suites and real-world engineering problems, not only in terms of superior convergence and diversity but also displaying stronger robustness and adaptability in addressing issues related to narrow feasible regions and disconnected fronts.},
  archive      = {J_ASOC},
  author       = {Jun Chen and Xiaobo Li and Yuxin Zhao and Zhendi Ma and Zhongmei Han and Yanxia Bao},
  doi          = {10.1016/j.asoc.2025.113792},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113792},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TPCMEA: A tri-population evolutionary algorithm with adaptive stage-switching for complex CMOPs},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective differential evolution algorithm integrating a directional generation mechanism for multi-objective optimization problems. <em>ASOC</em>, <em>184</em>, 113791. (<a href='https://doi.org/10.1016/j.asoc.2025.113791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective Evolutionary Algorithms (MOEAs) have gained significant attention due to their effectiveness in solving multi-objective optimization problems. However, when dealing with complex problems, they often face challenges such as low convergence accuracy and poor diversity. To address these issues, we propose a novel multi-objective differential evolution algorithm, MODE-FDGM, which integrates a directional generation mechanism. The key contributions are: (1) A directional-generation method leverages current and past information to rapidly build feasible solutions, boosting both speed and quality in exploring Pareto non-dominated space; (2) An update mechanism that combines crowding distance evaluation, iterating the population and incorporating historical information to enhance diversity and improve the ability to escape local optima; and (3) The introduction of an ecological niche radius concept along with a dual-mutation ecological niche selection evolution strategy, which improves exploration of unexplored spaces and preserves population diversity. Comparative experiments against 7 algorithms, including both classical and contemporary ones, on 24 benchmark functions demonstrate that the proposed algorithm markedly enhances the exploration of Pareto non-dominated solutions, exhibiting superior performance and advanced capabilities.},
  archive      = {J_ASOC},
  author       = {Zhuoxuan Yuan and Haibin Ouyang and Steven Li and Essam H. Houssein and Nagwan Abdel Samee},
  doi          = {10.1016/j.asoc.2025.113791},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113791},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective differential evolution algorithm integrating a directional generation mechanism for multi-objective optimization problems},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relaxation-based exploration and clustering-based exploitation for multimodal multi-objective optimization. <em>ASOC</em>, <em>184</em>, 113790. (<a href='https://doi.org/10.1016/j.asoc.2025.113790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal multi-objective optimization problems represent a specific type of multi-objective optimization problem where multiple distinct Pareto optimal solution sets (PSs) correspond to the same Pareto optimal front. Although the local PSs may not be as effective as the global PSs, obtaining the latter can be more costly. Therefore, the local PSs still hold some value for decision makers. However, many multimodal multi-objective evolutionary algorithms (MMOEAs) adopt the convergence-first criterion, which makes it challenging to obtain both the global and local PSs. To address the above issue, this paper proposes a two-stage dual-population coevolutionary algorithm with relaxation-based exploration and clustering-based exploitation (RCEA). Specifically, the relaxation-based exploration strategy defines an adaptive maximum Pareto rank, and relaxes the selection threshold for candidates from strictly non-dominated solutions to valuable solutions within this rank. Moreover, the clustering-based exploitation strategy performs a density-based decomposition on the population, separating different global and local PSs to facilitate more targeted evolution. During exploitation, a novel offspring generation operator is employed for intra-subpopulation crossover, preventing the waste of computational resources. Additionally, an improved convergence indicator is proposed to measure the convergence of solutions, which effectively handles the isolated solutions that may be erroneously identified as local Pareto optimal solutions by adaptively adjusting the neighborhood radius. Experimental results demonstrate that RCEA outperforms seven state-of-the-art MMOEAs on 70 benchmark problems.},
  archive      = {J_ASOC},
  author       = {Qianlong Dang and Zhiyang Zhang and Xiaochuan Gao and Tingting Wang},
  doi          = {10.1016/j.asoc.2025.113790},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113790},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Relaxation-based exploration and clustering-based exploitation for multimodal multi-objective optimization},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compute-efficient and backpropagation-free pseudoinverse learning for neural networks: A comprehensive survey. <em>ASOC</em>, <em>184</em>, 113789. (<a href='https://doi.org/10.1016/j.asoc.2025.113789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pseudoinverse learning algorithm is a non-gradient, efficient learning scheme originally designed for training single hidden layer feedforward neural networks. It has been developed into various variants and successfully applied across numerous fields. This paper provides a systematic review of the fundamental theories of the pseudoinverse learning algorithm and its major variants, outlining different types of neural networks and learning system architectures based on the pseudoinverse learning scheme. Furthermore, we summarize the fundamental ideas and methodologies of applying the pseudoinverse learning scheme to various learning tasks such as classification, representation learning, time series forecasting, incremental learning, automated machine learning, and content generation. We also summarize and compare the performance of pseudoinverse learning with representative competing baselines on several commonly used data sets based on existing literature reports. The results demonstrate that PIL exhibits significant efficiency advantages over gradient-based approaches (training time was reduced by 72.73% to 99.37%), aligning with its inherent gradient-free nature. Notably, recent PIL variants maintain this computational superiority while achieving enhanced performance compared to other gradient-free algorithms. In addition, we briefly introduce the representative applications of pseudoinverse learning in various fields. To the best of our knowledge, this is the first comprehensive review in this field to encompass all aforementioned aspects. It facilitates the synthesis and integration of existing knowledge from disparate studies. By highlighting limitations in prior works including the computational complexity in large-scale pseudoinverse computation, potential numerical instability for ill-conditioned matrices, risk of overfitting, and constraints in modeling multidimensional patterns, this paper also recommends directions for future research in this area.},
  archive      = {J_ASOC},
  author       = {Ke Wang and Pandi Liu and Mohammed A.B. Mahmoud and Ping Guo and Yafei Li},
  doi          = {10.1016/j.asoc.2025.113789},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113789},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Compute-efficient and backpropagation-free pseudoinverse learning for neural networks: A comprehensive survey},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient constrained multi-population cooperation search algorithm for multi-objective optimization of multireservoir operation. <em>ASOC</em>, <em>184</em>, 113788. (<a href='https://doi.org/10.1016/j.asoc.2025.113788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-objective optimization of multireservoir operation, complex nonlinear constraints often result in highly fragmented and discontinuous feasible regions, making it difficult for traditional algorithms to simultaneously achieve rapid convergence and maintain solution diversity. To effectively address this challenge, this paper proposes a novel Constrained Multi-Population Cooperation Search Algorithm (cMPCSA), which establishes an adaptive balance between global exploration and local exploitation through a cooperative evolutionary mechanism. The proposed algorithm achieves key advancements by constructing a modular multi-population framework with distinct subpopulation roles, integrating a constraint-independent environmental selection mechanism, and introducing a multi-factor learning strategy that combines edge, mean, and stochastic solutions to guide the search process more effectively. Experimental results on four mainstream multi-objective benchmark test suites demonstrate that cMPCSA significantly outperforms existing algorithms in terms of convergence performance and solution quality. Further application to a real-world multireservoir operation case confirms its robustness and superiority in coordinating multiple objectives such as hydropower generation, water supply reliability, and ecological flow protection. Overall, this study provides an efficient and scalable algorithmic tool for addressing complex multi-objective optimization problems in modern water resource management.},
  archive      = {J_ASOC},
  author       = {Zhong-kai Feng and Li Zhang and Xia-yu Wang and Fang Yang and Yi-hong Jiang and Sen Wang and Wen-jing Niu},
  doi          = {10.1016/j.asoc.2025.113788},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113788},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient constrained multi-population cooperation search algorithm for multi-objective optimization of multireservoir operation},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep insights into cognitive decline: A survey of leveraging non-intrusive modalities with deep learning techniques. <em>ASOC</em>, <em>184</em>, 113787. (<a href='https://doi.org/10.1016/j.asoc.2025.113787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive decline is a natural part of aging. However, under some circumstances, this decline is more pronounced than expected, typically due to disorders such as Alzheimer’s disease. Early detection of an anomalous decline is crucial, as it can facilitate timely professional intervention. While medical data can help, it often involves invasive procedures. An alternative approach is to employ non-intrusive techniques such as speech or handwriting analysis, which do not disturb daily activities. This survey reviews the most relevant non-intrusive methodologies that use deep learning techniques to automate the cognitive decline detection task, including audio, text, and visual processing. We discuss the key features and advantages of each modality and methodology, including state-of-the-art approaches like Transformer architecture and foundation models. In addition, we present studies that integrate different modalities to develop multimodal models. We also highlight the most significant datasets and the quantitative results from studies using these resources. From this review, several conclusions emerge. In most cases, text-based approaches consistently outperform other modalities. Furthermore, combining various approaches from individual modalities into a multimodal model consistently enhances performance across nearly all scenarios.},
  archive      = {J_ASOC},
  author       = {David Ortiz-Perez and Manuel Benavent-Lledo and Jose Garcia-Rodriguez and David Tomás and M. Flores Vizcaya-Moreno},
  doi          = {10.1016/j.asoc.2025.113787},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113787},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep insights into cognitive decline: A survey of leveraging non-intrusive modalities with deep learning techniques},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving adversarial transferability with neighborhood gradient information. <em>ASOC</em>, <em>184</em>, 113786. (<a href='https://doi.org/10.1016/j.asoc.2025.113786'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are known to be susceptible to adversarial examples, leading to significant performance degradation. In black-box attack scenarios, a considerable attack performance gap between the surrogate model and the target model persists. This work focuses on enhancing the transferability of adversarial examples to narrow this performance gap. We observe that the gradient information around the clean image, i.e., Neighborhood Gradient Information (NGI) , can offer high transferability. Based on this insight, we introduce NGI-Attack, incorporating Example Backtracking and Multiplex Mask strategies to exploit this gradient information and enhance transferability. Specifically, we first adopt Example Backtracking to accumulate Neighborhood Gradient Information as the initial momentum term. Then, we utilize Multiplex Mask to form a multi-way attack strategy that forces the network to focus on non-discriminative regions, which can obtain richer gradient information during only a few iterations. Extensive experiments demonstrate that our approach significantly enhances adversarial transferability. Especially, when attacking numerous defense models, we achieve an average attack success rate of 95.2%. Notably, our method can seamlessly integrate with any off-the-shelf algorithm, enhancing their attack performance without incurring extra time costs.},
  archive      = {J_ASOC},
  author       = {Haijing Guo and Jiafeng Wang and Zhaoyu Chen and Kaixun Jiang and Lingyi Hong and Pinxue Guo and Jinglun Li and Wenqiang Zhang},
  doi          = {10.1016/j.asoc.2025.113786},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113786},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving adversarial transferability with neighborhood gradient information},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated semi-supervised learning for in-domain/Cross-domain person re-identification. <em>ASOC</em>, <em>184</em>, 113785. (<a href='https://doi.org/10.1016/j.asoc.2025.113785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Person Re-identification (Re-ID) task refers to retrieving images of the same person from different camera views. However, the task faces three major challenges: building a person dataset may infringe on personal privacy, collecting large-scale datasets for identity annotation requires high costs, and low quality local datasets can lead to performance bottlenecks. To address the above challenges, we propose Federated semi-supervised frameworks for the first time to solve in-domain/Cross-domain Re-ID problems, which can reduce annotation costs while protecting privacy. Firstly, we propose two different weighted aggregation federated learning strategies. Specifically, under the in-domain setting, labeled clients are utilized to determine the quality of unlabeled clients. Under the cross-domain setting, the model’s recognition capability is extended to each unlabeled domain by measuring inter-domain differences. In addition, a cascaded Global-personalized Model System and Personalized Model Knowledge Transfer Module are proposed to further address the domain-gap issue by sufficiently decoupling the global shared knowledge and the personalized knowledge unique to each domain. Finally, a Classifier Checking Module is proposed to significantly reduce computational costs by dynamically adjusting the pseudo-labels of unlabeled samples. Extensive experiments on multiple public datasets of varying scales and quality verify that the proposed frameworks can achieve acceptable performance at a lower cost while ensuring the security of personal data. Meanwhile, the effectiveness of the proposed training strategies and modules in other tasks is also verified, which proves their inspirational significance for research and applications in the fields of weakly supervised learning, user privacy protection and intelligent video surveillance.},
  archive      = {J_ASOC},
  author       = {Xinyuan Chen and Mingwen Shao and Yi Niu and Qiao Zhang},
  doi          = {10.1016/j.asoc.2025.113785},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113785},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated semi-supervised learning for in-domain/Cross-domain person re-identification},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-guided triplet framework for synthetic data generation and semantic segmentation of railway fasteners under data scarcity and disparity. <em>ASOC</em>, <em>184</em>, 113784. (<a href='https://doi.org/10.1016/j.asoc.2025.113784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Railway infrastructure monitoring faces significant challenges due to limited data availability, labor-intensive annotations, and imbalanced datasets—factors that hinder efficient fastener maintenance. To address these issues, this study proposes a novel three-phase framework that leverages attention-guided deep learning for the generation and analysis of synthetic railway fastener images. In the first phase, an attention-based Deep Convolutional Generative Adversarial Network (DCGAN) is introduced to generate high-fidelity synthetic images that closely mimic real-world conditions. Unlike conventional GANs, the attention mechanism enables the model to focus on critical structural features of fasteners, enhancing the realism and diversity of the generated data. The second phase applies advanced denoising techniques, with the DnCNN model outperforming traditional methods like Median Filtering in preserving fine details. The final phase employs a Convolutional Autoencoder (CAE) for accurate semantic segmentation, achieving 88.8 % accuracy on the synthetic dataset. This end-to-end methodology improves model generalizability, reduces reliance on manual labeling, and provides a cost-effective solution for automated railway inspection. By bridging the gap between real and synthetic data, it also lays the groundwork for scalable, intelligent infrastructure monitoring systems, supporting the advancement of safer and more efficient railway operations.},
  archive      = {J_ASOC},
  author       = {Qasim Zaheer and Momina Malik and S.Muhammad Ahmed Hassan Shah and Chengbo Ai and Hongzhi Wang and Zhiyu Liang and Shi Qiu},
  doi          = {10.1016/j.asoc.2025.113784},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113784},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-guided triplet framework for synthetic data generation and semantic segmentation of railway fasteners under data scarcity and disparity},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An Actor–Critic-based adapted deep reinforcement learning model for multi-step traffic state prediction. <em>ASOC</em>, <em>184</em>, 113783. (<a href='https://doi.org/10.1016/j.asoc.2025.113783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic state prediction is critical to decision-making in various traffic management applications. Despite significant advancements in Deep Learning (DL) models, such as Long Short-Term Memory (LSTM), Graph Neural Networks (GNN), and attention-based transformer models, multi-step predictions remain challenging. The state-of-the-art models face a common limitation: the predictions’ accuracy decreases as the prediction horizon increases, a phenomenon known as error accumulation. In addition, with the arrival of non-recurrent events and external noise, the models fail to maintain good prediction accuracy. Deep Reinforcement Learning (DRL) has been widely applied to diverse tasks, including optimising intersection traffic signal control. However, its potential to address multi-step traffic prediction challenges remains underexplored. This study introduces an Actor–Critic-based adapted DRL method to explore the solution to the challenges associated with multi-step prediction. The Actor network makes predictions by capturing the temporal correlations of the data sequence, and the Critic network optimises the Actor by evaluating the prediction quality using Q-values. This novel combination of Supervised Learning and Reinforcement Learning (RL) paradigms, along with non-autoregressive modelling, helps the model to mitigate the error accumulation problem and increase its robustness to the arrival of non-recurrent events. It also introduces a Denoising Autoencoder to deal with external noise effectively. The proposed model was trained and evaluated on three benchmark traffic flow and speed datasets. Baseline multi-step prediction models were implemented for comparison based on performance metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). The results reveal that the proposed method outperforms the baselines by achieving average improvements of 0.26 to 21.29% in terms of MAE and RMSE for up to 24 time steps of prediction length on the three used datasets, at the expense of relatively higher computational costs. On top of that, this adapted DRL approach outperforms traditional DRL models, such as Deep Deterministic Policy Gradient (DDPG), in accuracy and computational efficiency.},
  archive      = {J_ASOC},
  author       = {Selim Reza and Marta Campos Ferreira and J.J.M. Machado and João Manuel R.S. Tavares},
  doi          = {10.1016/j.asoc.2025.113783},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113783},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An Actor–Critic-based adapted deep reinforcement learning model for multi-step traffic state prediction},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grammatical evolution for automatic design of actuated traffic signal control plans. <em>ASOC</em>, <em>184</em>, 113782. (<a href='https://doi.org/10.1016/j.asoc.2025.113782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traffic networks, proper signal control design is essential to ensure a reasonable level of service. Signal control designs are becoming increasingly complex, with numerous settings that must be calibrated and set. This paper introduces a novel approach for handling this complexity by automatically generating optimal actuated signal control plans using Grammatical Evolution (GE). GE has proven its effectiveness in automating the design of different complex systems, such as neural networks and analog electronic circuits. GE’s distinctive mapping and representation capabilities make it a powerful candidate for optimizing various systems. In contrast to traditional optimization methods for actuated signal plans, which focus on specific parameters, such as green times and cycle length, the GE-based approach evolves complete plans, including phases, detector placements, and transit priority strategies. As a result, it eliminates the need for human intervention in the design process, making it more efficient and less time-consuming. The proposed approach was tested with an application to an isolated intersection in Haifa, Israel. The results showed that the automatically generated signal plan outperformed the existing plan by reducing delay times and queue lengths. Moreover, this method demonstrated its efficiency in generating reliable traffic signal plans under challenging traffic conditions.},
  archive      = {J_ASOC},
  author       = {Mahmud Keblawi and Tomer Toledo},
  doi          = {10.1016/j.asoc.2025.113782},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113782},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grammatical evolution for automatic design of actuated traffic signal control plans},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized resource management in containerized clouds via hierarchical autoregressive network based workload prediction. <em>ASOC</em>, <em>184</em>, 113781. (<a href='https://doi.org/10.1016/j.asoc.2025.113781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The swift growth of containerized applications calls for highly precise and real-time prediction of workloads to make resource allocation in cloud environments efficient. Current methods are deficient either by not being able to catch the complex long-term workload dependencies in patterns or by introducing too much computational overhead that degrades real-time performance. We propose a new integrated model where a transformer-based Hierarchical Autoregressive Network (HARN) is dynamically fused with TES. With the use of self-attention mechanisms in modeling complex temporal dynamics and long-range dependencies, on the one hand, HARN captures the detailed seasonal variation but robustly deals with the smoother short-term oscillations, giving TES ample opportunity to function. The model dynamically adjusts their contributions to fit the best weighting in real-time, hence leading to optimal predictability even for changing workloads. Our analysis on the publicly released Alibaba v2018 dataset shows an average improvement in Mean Absolute Error (MAE) of 15.50% and an improvement in Root Mean Square Error (RMSE) of 14.80%. Furthermore, experiments on a specially designed container dataset show even more significant improvements, with improvements of up to 25% in CPU metrics and 40% in memory metrics. These findings and minimal computational overhead emphasize the model’s ability to facilitate real-time proactive resource provisioning in dynamic cloud settings.},
  archive      = {J_ASOC},
  author       = {Shivani Tripathi and Angelina Shibu and Priyadarshni and Rajiv Misra and T.N. Singh},
  doi          = {10.1016/j.asoc.2025.113781},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113781},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized resource management in containerized clouds via hierarchical autoregressive network based workload prediction},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pervasive multifaceted process based generative adversarial network for image quality enhancement. <em>ASOC</em>, <em>184</em>, 113780. (<a href='https://doi.org/10.1016/j.asoc.2025.113780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The practice of Generative Adversarial Networks (GAN) has extended a lot of consideration in recent times. In most of the GAN methods are problem-specific related that are personalized to report numerous trials of individual application rather than performing other image improvement tasks. Furthermore, the basic GAN generators influence their boundaries in numerous image restoration and development use cases. Therefore, in this paper, we propose a generic GAN referred to as Pervasive Multifaceted Process based Generative Adversarial Network (PMPGAN). In this generator, we introduced multiple Convolutional Neural Networks (CNN) followed by Multi-dimensional Pyramid Pooling Module (MPPM) and Attention Module (AM), of which the input for the AM is given as low-level features and it produces output with enhanced feature map. Meanwhile, we enhanced the improvement outcome of the generated image with discriminator loss function. Finally, we tested the efficiency of the proposed system through extensive experiments on five challenging applications for image enhancement, image restoration, and infrared image translation to determine the dominance and efficiency in eliminating image degradation and producing visually interesting fake images. Our PMPGAN quantitatively outperforms several latest models. The results show that the PMPGAN model is superior to the existing models.},
  archive      = {J_ASOC},
  author       = {K. Balaji},
  doi          = {10.1016/j.asoc.2025.113780},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113780},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pervasive multifaceted process based generative adversarial network for image quality enhancement},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-term time series forecasting by a frequency-domain enhanced temporal convolutional network with the stationary residual regularization. <em>ASOC</em>, <em>184</em>, 113779. (<a href='https://doi.org/10.1016/j.asoc.2025.113779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current time series models can resolve the long-term time series forecasting problem by leveraging artificial neural networks (ANNs) to capture complex temporal dependencies. However, the state-of-the-art ANNs for long-term prediction, Transformer-based models, have two inherent shortcomings that undermine both credibility and accuracy: losing temporal order information and ignoring information in prediction residuals. To fill these gaps, this study proposes a frequency-domain enhanced multi-scale temporal convolutional network (FMTCN) and a hybrid loss function regularized by stationary residuals called QM. Firstly, to preserve temporal order information, a multi-scale block with dilated causal convolution is developed as the core component of FMTCN, which can ensure the dependencies of predictions on sequentially ordered data. Secondly, an adaptive fusion mechanism for time and frequency domain patterns is designed to bridge the performance gap between convolution-based and Transformer-based models. Thirdly, to reduce residual autocorrelation, the proposed hybrid loss function integrates the Ljung–Box statistic into the mean squared error loss function as the regularization. Finally, extensive experiments across five real-world datasets are conducted to validate the proposed methods. Compared with the state-of-the-art method, the proposal improves the accuracy by 8.6% in the univariate long-term prediction and 6.1% in the multivariate long-term prediction.},
  archive      = {J_ASOC},
  author       = {Jing Wang and Yanbing Ju and Peiwu Dong and Tian Ju},
  doi          = {10.1016/j.asoc.2025.113779},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113779},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Long-term time series forecasting by a frequency-domain enhanced temporal convolutional network with the stationary residual regularization},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Loss functions in classification: An comprehensive overview and comparative study. <em>ASOC</em>, <em>184</em>, 113778. (<a href='https://doi.org/10.1016/j.asoc.2025.113778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loss functions are among the most fundamental components of a classifier’s learning processes and consequently can significantly affect the performance of classification methods. However, despite the substantial potential impact of loss functions on the classification accuracy, they have been incomprehensively investigated in the literature. For this reason, this paper tries to comprehensively evaluate the potency and impact of loss functions on the performance and accuracy in classification problems. For this purpose, fifty-five different loss functions in twelve categories—Linear Continuous Distance, Nonlinear Continuous Distance, Linear Semi-Continuous Distance, Nonlinear Semi-Continuous Distance, Linear Discrete Distance, Nonlinear Discrete Distance, Linear Continuous Direction, Nonlinear Continuous Direction, Linear Semi-Continuous Direction, Nonlinear Semi-Continuous Direction, Linear Discrete Direction, and Nonlinear Discrete Direction, are considered. In addition, in this paper, six distinct environmental pollutants/pollution benchmark data sets, three classifier types—statistical, shallow intelligence, and deep intelligence, are exemplary considered. Furthermore, in this paper, the most important measure of regular classification problems, the classification rate, has been chosen in order to compare these loss functions. The experimental results have unequivocally validated that the choice of loss functions has a significant impact on accuracy and classification rates. Numerical results of loss functions indicate that the difference between the lowest and the highest classification rate in the statistical, shallow intelligent, and deep learning classifiers, is averagely equal to 6.41 %, 5.16 %, and 2.24 %, respectively. It means that the model designer, by choosing the appropriate loss function, can averagely improve more than 4 % the obtained classification rate. Empirical results show that in the general perspective, the lowest performances of loss functions are overall related to the linear, continuous, and distance-based categories, in contrast to the highest ones, which are nonlinear, discrete, and direction-based. The exception to this outcome is the Zero-One family, in which all their loss functions, i.e., linear/nonlinear, distance/direction, are equivalent and yield the same accuracy. The best classification rate is also related to this family, that significantly better than other loss functions in all cases, as well as all classifiers. The Zero-One loss functions can averagely achieve a 98.62 % classification rate that 17.65 % is averagely higher than others. These evidences illustrate that, in addition to other effective features and factors, the loss function type should also be considered by environmental model designers in order to yield a more desired classification rate.},
  archive      = {J_ASOC},
  author       = {Fatemeh Chahkoutahi and Mehdi Khashei and Naser Molaverdi},
  doi          = {10.1016/j.asoc.2025.113778},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113778},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Loss functions in classification: An comprehensive overview and comparative study},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ultra-low memory spatiotemporal decomposition recurrent neural networks for edge structural fault monitoring. <em>ASOC</em>, <em>184</em>, 113777. (<a href='https://doi.org/10.1016/j.asoc.2025.113777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hardware memory resources of microcontrollers in wireless sensor network nodes are limited, currently only capable of data acquisition and simple computing, making it difficult to perform neural network inference for edge fault monitoring. Running neural networks on microcontrollers can enhance the edge data processing capabilities of the nodes. Recurrent neural networks with short and medium term memory excel at processing sequential data. However, inference on microcontrollers consumes a significant amount of memory, necessitating solutions to the memory constraints. This study proposes a spatiotemporal decomposition method for recurrent neural networks to address the memory constraint issue when performing inference on resource-constrained nodes, thus enabling edge fault monitoring. The method decomposes recurrent neural networks in spatiotemporal dimensions, significantly reducing memory usage during operation. Additionally, it proposes model parameter storage and addressing techniques to ensure accurate reading of Flash data. Experiments have verified that this method can achieve 99.7 % accuracy in edge fault classification, consuming only 332 bytes of RAM and 768 bytes of Flash. The spatiotemporal decomposition method also provides a solution for running other time-series models on resource-constrained edge devices.},
  archive      = {J_ASOC},
  author       = {Hao Fu and Lei Deng and Baoping Tang and Shuaiwen Cui and Yuguang Fu},
  doi          = {10.1016/j.asoc.2025.113777},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113777},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ultra-low memory spatiotemporal decomposition recurrent neural networks for edge structural fault monitoring},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid SpinalNet-fuzzy-shufflenet for brain tumor detection using MRI images. <em>ASOC</em>, <em>184</em>, 113775. (<a href='https://doi.org/10.1016/j.asoc.2025.113775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, a significant portion of the global population is affected by the serious medical condition known as Brain Tumor (BT). Damage to healthy brain tissue is suspected, as it is currently the most important cause of a huge quantity of mortality. To prevent patients from dying, early detection is very essential. Despite various notable efforts and hopeful results in this area, accurate segmentation and categorization remain a difficult task. To address these gaps, a SpinalNet Fuzzy Shufflenet (SFShuffleNet) is proposed for the detection of BT. First, the input image is fed into the preprocessing stage, utilizing ROI extraction. Subsequently, the preprocessed image undergoes enhancement using histogram equalization techniques. Then, the enhanced image undergoes segmentation with Fuzzy Local Information C-Means (FLICM). Following segmentation, the image is augmented through sharpening, translation, random erasing, and resizing techniques. Features such as mean, variance, standard deviation, average, contrast, skewness, kurtosis, and entropy with the Local Frequency Descriptor (LFD) are extracted. Finally, the proposed SFShuffleNet, which combines SpinalNet and Shufflenet with fuzzy concept modifications, detects BT. SFShuffleNet achieved the highest accuracy, sensitivity, specificity, and F1-score of 91.46 %, 90.96 %, 92.78 %, and 90.79 %, respectively.},
  archive      = {J_ASOC},
  author       = {P Srinivasa Rao and Swathi Sowmya Bavirthi and G. Sharada and Ponnaboyina Ranganath and Vemuri Sailaja and G Vimala Kumari},
  doi          = {10.1016/j.asoc.2025.113775},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113775},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid SpinalNet-fuzzy-shufflenet for brain tumor detection using MRI images},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enzyme classification integrating LSTM and prot-BERT sequence encoding. <em>ASOC</em>, <em>184</em>, 113774. (<a href='https://doi.org/10.1016/j.asoc.2025.113774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enzyme classification is essential for deciphering cellular and biological processes, driving targeted research, and influencing domains such as drug discovery and bioengineering. While various automated tools exist for enzyme classification, many are limited in scope or no longer operational. This study utilizes advanced artificial intelligence (AI) algorithms, including SVM, RF, simpleRNN, LSTM, ConvLSTM, and bidirectional LSTM, combined with numeric and Prot-BERT-based protein sequence encodings, to classify 1991 enzymes across 7 main classes, 69 subclasses, 216 sub-subclasses, and 1333 substrates. Among all trained models, the bidirectional LSTM model integrated with Prot-BERT sequence encoding (ECiLPSE) demonstrated exceptional accuracy of 99.14 % on the training set and 98.41 % on the test set, effectively capturing intricate sequence details. In addition to high accuracy, ECiLPSE achieved an F1-score of 0.99 (training set) and 0.98 (test set), with AU-ROC scores of 0.98 and 0.97, respectively. Precision and recall were both 0.99 on the training set and 0.98 on the test set. The 95 % confidence interval for test set accuracy (98.00 % - 99.97 %) further supports the model’s robustness and discriminative capability across classes. To benchmark performance, ECiLPSE was evaluated against four EC prediction tools (ECPred, CLEAN, EZYPred, EzyDeep), demonstrating superior accuracy and computational efficiency. The results from three case studies (using New-1277, Price-149, and halogenases datasets) highlight the limitations of the existing tools, underscoring the predictive accuracy, reliability, and applicability of ECiLPSE. ECiLPSE is available as a standalone tool for large datasets and as a web server for small datasets, offering a robust resource for enzyme classification, advancing computational approaches in enzyme research, and promising applications in drug discovery.},
  archive      = {J_ASOC},
  author       = {Anju Sharma and Vineet Diwakar and Rajnish Kumar and Prabha Garg},
  doi          = {10.1016/j.asoc.2025.113774},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113774},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enzyme classification integrating LSTM and prot-BERT sequence encoding},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent urban GNSS measurement uncertainty prediction by exploring the spatial characteristics with transformer. <em>ASOC</em>, <em>184</em>, 113773. (<a href='https://doi.org/10.1016/j.asoc.2025.113773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global navigation satellite system (GNSS) positioning accuracy is essential for civil applications. Unfortunately, in urban areas, GNSS signals are easily blocked and reflected by tall buildings, namely non-line-of-sight (NLOS) receptions. Thus, it is essential to classify these contaminated receptions and mitigate their pseudorange error for positioning improvement. This paper designed a Transformer-based network that utilizes satellite spatial characteristics to predict GNSS measurement uncertainty. The proposed method accurately classifies 89 % of satellites’ visibility and around 45 % compensation in NLOS pseudorange error, outperforming the state-of-the-art algorithms. By analyzing the attention matrix generated by Transformer, we explore how Transformer utilizes spatial characteristics for satellite measurement uncertainty prediction, which regards satellite visibility and pseudorange as different tasks for training.},
  archive      = {J_ASOC},
  author       = {Zekun Zhang and Penghui Xu and Guohao Zhang},
  doi          = {10.1016/j.asoc.2025.113773},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113773},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent urban GNSS measurement uncertainty prediction by exploring the spatial characteristics with transformer},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling self-supervised learning for handling the concept drift with ternary-adaptive ensemble. <em>ASOC</em>, <em>184</em>, 113772. (<a href='https://doi.org/10.1016/j.asoc.2025.113772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last several decades, drift detection and adaptation models have been trained on historical drift labels struggle with real-world data streams due to evolving distributions, impacting accurate future predictions. Existing methods update predictive models but often fail to retain previously learned patterns from streaming data, hindered by insufficient predefined labels for categorizing drift detection. To address these constraints, the proposed work introduces a two-task framework comprising a Self-Supervised Learning (SSL) model with a pretext task and a downstream task to efficiently adapt the model to evolving data distributions. In the pretext task, the proposed approach learns patterns from the unlabeled offline data in a self-supervised manner and utilizes the knowledge from the performance-based drift detector. In a subsequent downstream task, the model that has been enhanced with a forgetting-aware Ternary-Adaptive Ensemble (TAE) learner without requiring the drift labels. In the proposed approach, the TAE algorithm comprises drift-aware model updation methods for the Long Short-Term Memory (LSTM) to resolve forgetting in sequential data streams, and the three updation methods involve the pretext model’s weight transfer, Bayesian Optimization-based hyperparameter tuning, and Elastic Weight Consolidation (EWC). The proposed approach effectively handles drifts and outperformed while testing on the real-world weather and synthetic, mixed drift datasets with 82.6% and 92.36% accuracy, respectively.},
  archive      = {J_ASOC},
  author       = {Shubhangi Suryawanshi and Anurag Goswami and Pramod Patil},
  doi          = {10.1016/j.asoc.2025.113772},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113772},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling self-supervised learning for handling the concept drift with ternary-adaptive ensemble},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain knowledge distillation for domain adaptation with GCN-driven MLP generalization. <em>ASOC</em>, <em>184</em>, 113771. (<a href='https://doi.org/10.1016/j.asoc.2025.113771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) and domain adaptation (DA) represent potential research directions for reducing costs associated with deploying deep neural networks (DNN) in real-world applications. KD focuses on model compression, exploring methods to transfer informative representations from a complex model to a lighter one without incurring additional costs. Conversely, DA emphasizes the data distribution perspective, aiming to decrease labeling expenses by leveraging knowledge extracted from a labeled source domain to minimize classification errors in an unlabeled target domain. In this paper, we introduce a novel knowledge distillation (KD) approach with a teacher–student paradigm for domain adaptation (DA) tasks, termed Improved Cross-domain Knowledge Distillation (ICDKD). Specifically, we employ a graph convolutional network (GCN) classifier as the teacher model and a multilayer perceptron (MLP) classifier as the student model. During training, the teacher model utilizes a message-passing mechanism to capture the topology of the training data through neighbor information, thus explicitly enhancing semantic representations in each category to improve classification accuracy. Subsequently, the extracted knowledge from the GCN teacher model is distilled to the MLP student model. Finally, in the inference stage, only the MLP student model is utilized to meet the latency constraints of applications. Our proposed method effectively combines the strengths of both GCN and MLP classifiers to improve the classification performance and satisfy the real-world application requirements. We implemented our method on various DA benchmark datasets under unsupervised and semi-supervised domain adaptation settings, including ImageCLEF-DA, Office-31, Office-Home, VisDA2017, and DomainNet. The experimental results demonstrate the effectiveness of our proposed method on both CNN-based and ViT-based architectures, achieving outstanding classification performance compared to prior state-of-the-art domain adaptation methods.},
  archive      = {J_ASOC},
  author       = {Ba Hung Ngo and Tae Jong Choi},
  doi          = {10.1016/j.asoc.2025.113771},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113771},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-domain knowledge distillation for domain adaptation with GCN-driven MLP generalization},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy delphi-SERVQUAL model using degree of belief structure for assessing customer satisfaction in automotive after-sales services. <em>ASOC</em>, <em>184</em>, 113770. (<a href='https://doi.org/10.1016/j.asoc.2025.113770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel two-phase fuzzy multi-attribute decision-making (MADM) framework for assessing customer satisfaction in automotive after-sales services under uncertainty. The proposed model integrates a modified Fuzzy Delphi Method (FDM) with an enhanced fuzzy SERVQUAL approach, both embedded within a Degree of Belief (DoB) structure to capture evaluative ambiguity and confidence levels in expert and customer judgments. In the first phase, a belief-based FDM enables the efficient screening and prioritization of 58 service quality criteria using a single-round process that incorporates both optimistic and pessimistic expert viewpoints. In the second phase, a belief-driven SERVQUAL model evaluates customer perceptions and expectations from both stringent and lenient perspectives across six dimensions, including a newly introduced Digital Technology dimension that reflects emerging service delivery mechanisms. The model also features a δ-based sensitivity analysis to examine the impact of decision-making attitudes and a seven-zone classification system to categorize service quality gaps with high diagnostic precision. Application of the proposed framework in the Iranian automotive after-sales sector enabled the prioritization of 29 key service quality criteria from an initial pool of 58 indicators, with the most critical factors identified in the ‘Reliability’ and ‘Responsiveness’ dimensions. The resulting insights support evidence-based managerial actions, including the reallocation of resources toward high-impact service areas, targeted digital transformation initiatives, and the formulation of differentiated improvement strategies based on gap severity and belief-based customer expectations .},
  archive      = {J_ASOC},
  author       = {Mojtaba Elahi and Ramin Enayati and Mehdi Keramatpour},
  doi          = {10.1016/j.asoc.2025.113770},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113770},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy delphi-SERVQUAL model using degree of belief structure for assessing customer satisfaction in automotive after-sales services},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing a hybrid circular intuitionistic fuzzy framework to assess tourism 5.0 challenges. <em>ASOC</em>, <em>184</em>, 113769. (<a href='https://doi.org/10.1016/j.asoc.2025.113769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of Industry 5.0 (I5.0) in tourism is still in its early stages, and its complex implementation presents challenges that are not fully understood. Existing studies often focus on context-specific challenges, limiting the exploration of broader obstacles. This research aims to identify a comprehensive set of challenges, rank them, and evaluate their interdependencies in I5.0 adoption within tourism. A systematic literature review was conducted to identify key challenges, and expert input was gathered to validate and refine the findings. To evaluate the significance of the identified challenges, a novel extension of the Stepwise Weight Assessment Ratio Analysis (SWARA) method into the Circular Intuitionistic Fuzzy (CIF) environment, referred to as CIF-SWARA, was formulated to determine the relative weights of each challenge under uncertainty with more precision. Additionally, the Decision-Making Trial and Evaluation Laboratory (DEMATEL) method was extended using CIF sets, termed CIF-DEMATEL, to evaluate the causal relationships and relative influence among the challenges. The results from both methods were combined to derive final weights. Furthermore, a conceptual model was constructed using Total Interpretive Structural Modeling (TISM) based on the CIF-DEMATEL output, providing a detailed qualitative assessment of the interactions among the challenges. Findings highlight that “infrastructure issues” are the most urgent challenge requiring attention. Additionally, “technical and technological issues,” “acceptance and adaptability issues,” and “financial issues” emerged as the three most significant challenges for transitioning to “Tourism 5.0.” The model’s robustness was confirmed via sensitivity analysis, and comparative evaluations demonstrated consistent performance against established fuzzy environments. This study advances I5.0 research in tourism by identifying and modeling key challenges, offering valuable insights for policymakers.},
  archive      = {J_ASOC},
  author       = {Vahideh Shahin and Dragan Pamucar and Moslem Alimohammadlou},
  doi          = {10.1016/j.asoc.2025.113769},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113769},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Developing a hybrid circular intuitionistic fuzzy framework to assess tourism 5.0 challenges},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-adaptive discrete artificial bee colony algorithm based on block swap for steelmaking and continuous casting scheduling problem. <em>ASOC</em>, <em>184</em>, 113768. (<a href='https://doi.org/10.1016/j.asoc.2025.113768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high temperature characteristics of the steelmaking continuous casting production, the temperature decreasing energy consumption (TDEC) in the processing process cannot be ignored. Therefore, this paper establishes a mixed integer mathematical model for the steelmaking continuous casting scheduling problem with TDEC (SCCSP TDEC ). The SCCSP TDEC chooses to minimize the objective functions of the total weighted earliness and tardiness, the TDEC and the makespan. To solve the SCCSP TDEC , an improved heuristic based on problem-specific features is proposed in this paper, which takes into account the completion time of the third stage’s cast, and the effect of the cast’s holistic nature. Then, a self-adaptive discrete artificial bee colony algorithm based on block swap (SDABC bs ) is proposed to solve the SCCSP TDEC . This strategy can adaptively select an optimal structure suitable for the current population, and effectively expand the search space of neighborhood solutions. Finally, this paper tests the SDABC bs algorithm based on the extensive instances generated according to the actual production process, and the performance of the SDABC bs algorithm is verified by comparing with other efficient algorithms. The experimental results show that the proposed SDABC bs algorithm is more effective for the SCCSP TDEC among all the algorithms in comparison.},
  archive      = {J_ASOC},
  author       = {Yang Yu and Guo-Dong Yang and Qichun Zhang and Liangliang Sun and Xinfu Pang and Yefeng Liu},
  doi          = {10.1016/j.asoc.2025.113768},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113768},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-adaptive discrete artificial bee colony algorithm based on block swap for steelmaking and continuous casting scheduling problem},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained mutation operators for community hiding using genetic algorithms. <em>ASOC</em>, <em>184</em>, 113767. (<a href='https://doi.org/10.1016/j.asoc.2025.113767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection plays a key role in uncovering group structures in networks, but its misuse can lead to privacy risks by exposing sensitive relationships. As a proactive defense, community hiding seeks to perturb the network structure to reduce the effectiveness of community detection algorithms. Given the NP-hard nature of this task, genetic algorithms (GAs) widely used due to their robust global search capabilities. However, existing methods lack effective guidance when dealing with vast solution spaces, resulting in inefficient exploration and suboptimal obfuscation outcomes. To address this, we propose Network- T opology- C ombined Community Information H iding A lgorithm (TCHA), a novel GA-based method that leverages node similarity information via node embedding to guide perturbations more effectively. TCHA introduces a multidimensional mutation operator that combines coarse-grained and fine-grained mutation strategies. These fine-grained mutations are performed in the embedding space and decoded back to graph edits, enabling more precise and topologically-aware perturbations. To evaluate the efficiency of this process, we introduce a novel metric based on expected path length within a mutation transition graph, offering deeper insight into evolutionary search dynamics. Experiments on six real-world networks demonstrate that TCHA achieves an average modularity reduction of 31.92% and an average normalized mutual information of 0.6447, outperforming baselines such as Q-Attack, NEURAL, and DICE. These results confirm the superiority of the embedding-guided fine-grained mutation strategy in enhancing community hiding effectiveness.},
  archive      = {J_ASOC},
  author       = {Shanqing Yu and Jintao Zhou and Meng Zhou and Yidan Song and Jiaxiang Li and Zeyu Wang and Qi Xuan and Silu Mu and Xiaolei Qian},
  doi          = {10.1016/j.asoc.2025.113767},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113767},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fine-grained mutation operators for community hiding using genetic algorithms},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting scour depth in the presence of aprons using XGBoost-optuna. <em>ASOC</em>, <em>184</em>, 113766. (<a href='https://doi.org/10.1016/j.asoc.2025.113766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the impact of an apron in mitigating scour downstream of a trapezoidal PK weir through an experimental study. Three apron lengths were tested with two sediment types under varying hydraulic conditions to address local scouring. Results show that longer aprons reduce scouring, especially at lower densimetric Froude numbers, affecting the location of the maximum scour depth and its volume. On average, apron lengths of 1 P , 1.5 P , and 2 P ( P is weir height) decrease the scour hole areas and volumes by approximately 69–77 %. Scour indices decrease by 73–90 % for corresponding apron lengths. New empirical equations have been proposed to aid in apron design, and the estimation of various scour hole geometries . Bayesian Optimized Neural Network (BONN), Extreme Gradient Boosting model tuned by Optuna algorithm (XGBoost-Optuna), and Random Forest were also developed for forecasting scour hole characteristics in the presence of apron. Various regression tests, including residual plots and uncertainty quantification, were imposed to compare the models. The results demonstrated that the XGBoost-Optuna model outperformed the other models, achieving a correlation coefficient ranging from 0.924 to 0.985, a root mean squared error between 0.055 and 5.072, and a mean relative percentage error of 7.14–11.71 %. Most forecasts generated by the XGBoost-Optuna model fell within ±20 % error margins, highlighting its superiority in predicting scour hole characteristics in the presence of the apron for PK weirs.},
  archive      = {J_ASOC},
  author       = {Chonoor Abdi Chooplou and Saeed Balahang and Masoud Ghodsian and Mohammad Vaghefi},
  doi          = {10.1016/j.asoc.2025.113766},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113766},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting scour depth in the presence of aprons using XGBoost-optuna},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LanPPT: Enhancing landslide crack detection through pyramid pooling transformers. <em>ASOC</em>, <em>184</em>, 113765. (<a href='https://doi.org/10.1016/j.asoc.2025.113765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landslide cracks, also known as tension cracks, are a major part of landslides. Based on the distribution of landslide fractures, the location of a landslide can be broadly described, and the stress distribution of the sliding mass can be inferred. Cracks at a landslide’s head, which are a key indicator of the displacement of the landslide body, can provide early warning signs for landslide hazards. The complete knowledge of the crack development features is still lacking because of many influencing elements and intricate reasons for fracture creation. Even though some early interventions reported models for automated crack identification utilizing advanced machine learning techniques, the problem still has not been solved to its full potential. Thus, an effective deep architecture for landslide crack segmentation is suggested to address these issues, utilizing a synergistic blend of vision transformers and the pyramid pooling concept. In this work, we use the universal vision transformer backbone called the Pyramid Pooling Transformer, and plug it into our pooling-based multi-head spatial attention to build a deep architecture that identifies and segments the landslide cracks, namely LanPPT. Experiments revealed that when a pyramid pooling transformer is used as the backbone network, it performs significantly better than many earlier convolutional neural networks and normal transformer-based networks in various vision tasks. Systematic experiments show that the proposed model achieved superior performance in terms of mIoU and FPS when compared with the chosen state-of-the-art baselines in the landslide crack detection task.},
  archive      = {J_ASOC},
  author       = {S. Sreelakshmi and S.S. Vinod Chandra},
  doi          = {10.1016/j.asoc.2025.113765},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113765},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LanPPT: Enhancing landslide crack detection through pyramid pooling transformers},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential evolution algorithm with cosine similarity-based individual reduction and symmetric uncertainty-based attribute recovery for feature selection. <em>ASOC</em>, <em>184</em>, 113764. (<a href='https://doi.org/10.1016/j.asoc.2025.113764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the increase in data scaling, effectively handling high-dimensional datasets has become a focal point of attention. Feature selection (FS), a method for dealing with datasets containing features, has emerged as a crucial technique in fields such as machine learning and data mining with the objective of selecting features that contain richer information while eliminating redundancy. Due to their remarkable performance in global search, evolutionary computation techniques hold substantial potential in the application of FS. However, many existing FS methods overlook the relationships between features. In the context of classification problems, this study presents a novel wrapper FS algorithm based on the differential evolution algorithm. The proposed method reduces redundant features among individuals based on cosine similarity and selectively and recovers certain features in the crossover phase according to the uncertainty similarity. In addition, a probabilistic-based initialization method is designed. The proposed algorithm significantly outperforms five other algorithms in terms of classification error rates over 18 experimental datasets. The experimental results demonstrate a significant enhancement in the performance of the proposed algorithm attributed to these two components.},
  archive      = {J_ASOC},
  author       = {Chunzhi Hou and Ziqian Wang and Yu Zhang and Yuki Todo and Jun Tang and Zhenyu Lei and Shangce Gao},
  doi          = {10.1016/j.asoc.2025.113764},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113764},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution algorithm with cosine similarity-based individual reduction and symmetric uncertainty-based attribute recovery for feature selection},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion model based on stochastic differential equation with transformer for stock price prediction. <em>ASOC</em>, <em>184</em>, 113763. (<a href='https://doi.org/10.1016/j.asoc.2025.113763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of stock prices is an exemplary interdisciplinary problem, straddling the domains of finance, computer science, econometrics, and mathematics. The fundamental characteristics of stock price data, notably its non-linearity, non-stationarity, and considerable complexity, make the prediction of stock prices an exceptionally challenging task. In recent years, the domain of deep neural networks has shown substantial promise in terms of learning capacities, leading to significant advancements in the field of stock price prediction. However, most existing methods are still limited to predicting the closing price of the next day, rather than the future trend of stock prices, leaving investors with insufficient information for trading decisions. We propose an innovative model, DiffVT, which incorporates a Volatility Transformer for feature extraction from historical data. We introduced a de-stationary attention mechanism that integrates non-stationary information into the model to capture the dependencies in highly volatile stock price sequences. The output is then fed into an improved diffusion model based on stochastic differential equation (SDE). By iteratively solving the reverse-time SDE, our model generates a probabilistic distribution. This approach not only predicts the single-point stock price for the next day but also forecasts the future trend of stock prices over a period, providing a possible range of stock price movements. To our knowledge, DiffVT is the first model to combine a diffusion model with Transformer for stock price prediction. Extensive experiments on multiple stock indices and individual stock datasets demonstrate that DiffVT significantly outperforms state-of-the-art baseline methods, exhibiting excellent performance across various prediction window lengths. Specifically, compared to the second-best models in each domain, our approach achieves reductions of up to 11.73% in Mean Absolute Error (MAE) for point predictions, 17.23% in Continuous Ranked Probability Score (CRPS) for probabilistic predictions, and an improvement of 12.93% in the Sharpe ratio, clearly establishing its superior performance. Our code is available at https://github.com/SoraKsgn/DiffVT/ .},
  archive      = {J_ASOC},
  author       = {Chaoyang Wang and Guangyu Liu and Ling Zhu},
  doi          = {10.1016/j.asoc.2025.113763},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113763},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Diffusion model based on stochastic differential equation with transformer for stock price prediction},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobjective coevolutionary bayesian learning for hyperspectral sparse unmixing. <em>ASOC</em>, <em>184</em>, 113762. (<a href='https://doi.org/10.1016/j.asoc.2025.113762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capability of multiobjective evolutionary algorithms (MOEAs) to directly address ℓ 0 norm minimization has introduced innovative perspectives for solving sparse problems. However, enhancing the search efficiency of MOEAs remains a formidable challenge, especially in high-dimensional sparse problems. To alleviate the above problem, we propose a novel multiobjective coevolutionary Bayesian learning framework for a classical sparse problem—sparse unmixing. Leveraging the spatial similarity inherent in hyperspectral image patches, the proposed framework alleviates the complexity of the sparse unmixing task by segmenting the original image into homogeneous regions using superpixel segmentation. These regions are then demixed independently and jointly optimized under a cooperative evolutionary paradigm. During the optimization, the row-sparsity parameter inferred through cooperative Bayesian learning is embedded into a specially designed image-level genetic strategy. This parameter can guide the evolutionary direction of the population, encourages the solution to conform to the structural characteristic and significantly improves search efficiency. Experimental results on synthetic and real datasets demonstrated the effectiveness of the proposed algorithm as compared with several sparse unmixing methods.},
  archive      = {J_ASOC},
  author       = {Yiting Liu and Maoguo Gong and Xiangming Jiang and Jianzhao Li and Yue Zhao and Yan Pu and Ziqi Di},
  doi          = {10.1016/j.asoc.2025.113762},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113762},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiobjective coevolutionary bayesian learning for hyperspectral sparse unmixing},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time anomaly detection in seasonal time series with conditional variational autoencoder. <em>ASOC</em>, <em>184</em>, 113761. (<a href='https://doi.org/10.1016/j.asoc.2025.113761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time anomaly detection in high-frequency seasonal time series is commonly addressed using prediction-based methods, which require waiting for new values to perform subsequent predictions and demand continuous processing over time. This work introduces a novel framework for real-time anomaly detection in seasonal time series, with a practical implementation using Conditional Variational Autoencoders based on Multilayer Perceptrons. Our approach eliminates the need for historical time series data at inference time, instead generating a one-shot long-term expected time series that enables immediate evaluation of streaming data with minimal computational resources. Empirical evaluations on real-world seasonal time series demonstrate that the proposed approach achieves state-of-the-art performance compared in both semi-supervised and unsupervised settings. The framework provides computational efficiency and low energy consumption, making it suitable for deployment in commodity hardware and offline environments.},
  archive      = {J_ASOC},
  author       = {Lorenzo Porcelli and Marcello Trovati and Francesco Palmieri},
  doi          = {10.1016/j.asoc.2025.113761},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113761},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time anomaly detection in seasonal time series with conditional variational autoencoder},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weight optimized stacked LSTM with conditional random fields using self-adaptive generalized normal distribution optimizer for crop yield forecasting. <em>ASOC</em>, <em>184</em>, 113760. (<a href='https://doi.org/10.1016/j.asoc.2025.113760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the agricultural sector, estimating crop production is a difficult task. A crucial element in recent years has forecasted the crop prediction, which is dependent on outside variables include soil, water, and agricultural characteristics. Crop feature extraction is used to predict crop yield using deep learning-based approaches. The predictive ability of the method heavily relies on the nature of the collected information because there is a nonlinear translation between the unprocessed information and crop yield data. Agricultural marketing, crop production, efficient harvest management, and effective fertilization management rely heavily on crop yield forecasts. Numerous manual analyses are used for remote sensing, which is often used for crop prediction. Here, the deep learning technique is more crucial for predicting crop yields from the remote sensing images, and more complicated approaches are needed to derive the essential spatiotemporal features of the data. Thus, this paper suggests a new weight-optimized Stacked Long Short Term Memory with Conditional Random Field (SLSTM-CRF) and a Modified Self-Adaptive Generalized Normal Distribution Optimizer (MSGNDO) for the prediction of crop yield. This research work has the following phases; data collection, data preprocessing, weighted feature selection, and crop yield forecasting. Primarily, the standard agricultural data is taken from the benchmark sources. Then, data preprocessing is carried out to improve the quality of data. Next, the features are optimally chosen using a newly recommended MSGNDO, in which the weights are tuned via MSGNDO. Further, the tuned weights are multiplied by the extracted features. The final crop yield prediction is done via weight-optimized SLSTM-CRF, where the parameter optimization is done using the same MSGNDO method. The prediction performance of the suggested framework is compared with existing techniques using diverse error-based measures to show effective performance regarding Dataset 1 and Dataset 2. While considering the Root Mean Squared Error (RMSE) analysis, the suggested framework shows 22.4571 less than 29.88774 of CNN, 29.20486 of RNN, 27.9877 of ResNet-50, and 25.38448 of LSTM. Overall, the performance analysis of the developed model while using dataset 1 achieves 3.549 and 24.46 for Mean Absolute Error (MAE) and RMSE, respectively. While comparing dataset 2, the developed model shows values of 5.196 and 29.16 regarding MAE and RMSE measures. Hence, the entire experimental evaluation of the recommended methods shows more effective outcomes than the existing conventional models.},
  archive      = {J_ASOC},
  author       = {Shaik Shameer Basha and B S Nissar Begum},
  doi          = {10.1016/j.asoc.2025.113760},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113760},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Weight optimized stacked LSTM with conditional random fields using self-adaptive generalized normal distribution optimizer for crop yield forecasting},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimized logic mining method for data processing through higher-order satisfiability representation in discrete hopfield neural network. <em>ASOC</em>, <em>184</em>, 113759. (<a href='https://doi.org/10.1016/j.asoc.2025.113759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A high performance classification tool such as logic mining has emerged as one of the future computing systems for big data processing. The collaboration between logic and neural network resulted in extracting the most suitable induced logic to represent knowledge from real-life datasets. However, there are certain limitations within the current logic mining models including a non-flexible logical structure, non-optimal computation of the best logic, and the generation of overfitting solutions. Motivated by these limitations, a novel logic mining model incorporating the non-systematic Satisfiability, namely Random 3 Satisfiability in Discrete Hopfield Neural Network is proposed as a logical structure to represent the behaviour of the dataset. The proposed logic mining models used flexible logical structures to prevent overfitting solutions and optimize synaptic weight values. A new computational approach of the best logic by considering True Positive and True Negative values of the learning system is applied in this work to preserve the significance behaviour of the dataset. Furthermore, the comparative experiments of the logic mining models by utilizing various repository real-life datasets are conducted from repositories to assess their efficiency. In accordance with the results, the proposed logic mining model dominates in all the metrics for the average rank. The average rank for each metrics are Accuracy (1.9375), Precision (1.9375), Specificity (1.8125), Mathews Correlation (1.5625), and Fowlkes Mallows Index (2.3125). Numerical results and in-depth analysis demonstrate that the proposed logic mining model consistently produces optimal induced logic that best represents the real-life dataset for all the performance metrics used in this study.},
  archive      = {J_ASOC},
  author       = {Nurul Atiqah Romli and Nur Fariha Syaqina Zulkepli and Mohd Shareduwan Mohd Kasihmuddin and Syed Anayet Karim and Siti Zulaikha Mohd Jamaludin and Nur ‘Afifah Rusdi and Gaeithry Manoharam and Mohd. Asyraf Mansor and Nur Ezlin Zamri},
  doi          = {10.1016/j.asoc.2025.113759},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113759},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimized logic mining method for data processing through higher-order satisfiability representation in discrete hopfield neural network},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing nonlinear dependencies of mamba via negative feedback for time series forecasting. <em>ASOC</em>, <em>184</em>, 113758. (<a href='https://doi.org/10.1016/j.asoc.2025.113758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mamba is a rising model designed to distill complex patterns from historical data, providing predictive capabilities for time series forecasting tasks. Mamba’s similarity to linear-based models has been criticized due to its limited ability to capture nonlinear dependencies. In this work, we propose a novel model named Embedding C hannel Attention M aclaurin E instein Mamba (CME-Mamba 1 .) based on Mamba framework, with both Embedding Channel Attention and Maclaurin mechanisms incorporated. To further address gradient vanishing issues, we integrate Einstein FFT algorithms, ensuring robust performance against abnormal behaviors of Mamba-based architectures. Extensive experiments conducted on 11 real-world datasets with different numbers of variates, domain focus and granularity, reveal that CME-Mamba achieves state-of-the-art performance in both MSE and MAE, while maintaining reasonable memory efficiency and low time cost. The robustness and credibility of all results are substantiated by a comprehensive convergence and stability analysis. Statistically, consolidated by the Friedman Nonparametric Test and the Wilcoxon Signed-Rank Test, CME-Mamba ranks the first place with significance over counterparts. In addition, in terms of time and memory analysis, CME-Mamba is among the top three models for time and memory efficiency. Despite this, our results further demonstrate that the main contributor is the Embedding Channel Attention Block, which greatly enhances nonlinear dependencies over datasets. The Einstein FFT Block effectively suppresses gradient vanishing occurrences and contributes considerably to performance improvements, driving CME-Mamba both stable and promising. Moreover, the Maclaurin Block based on negative feedback is asymptotically stable without additional gradient vanishing issues and pioneered in achieving synergies with other blocks and greatly enhances nonlinear dependencies. With enhanced nonlinear dependencies generated from the synergy effect of all the three blocks, CME-Mamba grows excellent to uncover complex paradigms and predict future states in various domains, especially improving the performance for periodic and high-variate situations, such as traffic flow management ( ≈ + 8 % ), electricity predictions( ≈ + 6 % ).},
  archive      = {J_ASOC},
  author       = {Sijie Xiong and Cheng Tang and Yuanyuan Zhang and Haoling Xiong and Youhao Xu and Atsushi Shimada},
  doi          = {10.1016/j.asoc.2025.113758},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113758},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing nonlinear dependencies of mamba via negative feedback for time series forecasting},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential unsupervised–supervised learning for clustering time-dependent patterns using ellipsoidal calculus. <em>ASOC</em>, <em>184</em>, 113757. (<a href='https://doi.org/10.1016/j.asoc.2025.113757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a sequential clustering algorithm that combines unsupervised and supervised learning methodologies using ellipsoidal calculus and recurrent neural networks (RNNs). The unsupervised clustering algorithm (ULCA) identifies ellipsoidal sets for the data by applying Lagrange multipliers. These sets are then optimized with gradient descent to adjust their volume and orientation. For time-dependent data, the optimized ellipsoidal sets are updated dynamically by an RNN, which refines their center, orientation, and axis sizes in response to changes in the data. The ULCA is compared to density-based spatial clustering (DBSCAN) and K-means algorithms, showing superior accuracy without the need for pre-determined cluster numbers. Additionally, the convergence of the ULCA and RNN algorithms, working sequentially, is formally proven using Lyapunov stability theory, ensuring continuous classification of data that evolves over time. This study demonstrates the advantages of the proposed hybrid method over traditional clustering algorithms.},
  archive      = {J_ASOC},
  author       = {Alejandro Guarneros and Mariana Ballesteros and Iván Salgado and Isaac Chairez},
  doi          = {10.1016/j.asoc.2025.113757},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113757},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sequential unsupervised–supervised learning for clustering time-dependent patterns using ellipsoidal calculus},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid quantum annealing for large-scale exam scheduling: Validation in real-world educational scenarios. <em>ASOC</em>, <em>184</em>, 113756. (<a href='https://doi.org/10.1016/j.asoc.2025.113756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study applied the hybrid quantum annealing to address complex exam scheduling challenges in large-scale educational scenarios through hybrid quantum annealing enhanced by graph-based preprocessing. By formulating the problem as a Quadratic Unconstrained Binary Optimization (QUBO) model and leveraging the D-Wave Advantage system, our method integrates quantum annealing with classical preprocessing to resolve constraints on exam room availability, student-course conflicts, and batch synchronization. The approach was applied in a university’s 2022 pandemic-era makeup exam scheduling for 1807 students and 215 courses (2749 exam instances) with zero conflicts. Experimental results show that hybrid quantum annealing consumes merely 86 ms of Quantum Processing Unit (QPU) execution time. In contrast, classical simulated annealing requires 13547 ms of Central Processing Unit (CPU) execution time for the same problem scale. This work bridges quantum computing and educational operations, offering a comparative analysis of hybrid algorithms in multi-constraint optimization domains.},
  archive      = {J_ASOC},
  author       = {Ziyu Zhou and Qing Chen and Chaojie Zhang and Mengtong Tan and Shuyan Li},
  doi          = {10.1016/j.asoc.2025.113756},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113756},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid quantum annealing for large-scale exam scheduling: Validation in real-world educational scenarios},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-phase evolutionary search space shrinking for large-scale multi-objective feature selection. <em>ASOC</em>, <em>184</em>, 113755. (<a href='https://doi.org/10.1016/j.asoc.2025.113755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is essential in machine learning, especially for high-dimensional datasets, where irrelevant and redundant features can degrade performance and increase computational cost. In such settings, the search space becomes exponentially large and sparsely populated with relevant solutions, making effective exploration highly challenging. Despite progress in evolutionary methods, many existing algorithms struggle to scale or maintain sparsity. There is an urgent need for scalable strategies that intelligently reduce the search space while retaining essential features. Using feature importance to guide search space shrinking offers a powerful, domain-specific approach tailored for feature selection. This paper proposes a novel large-scale multi-objective evolutionary algorithm, LMSSS, that addresses sparse feature selection through a multi-phase search space shrinking strategy. Features are ranked based on correlation with class labels and frequency in an initial low-cost evolutionary process. A voting-based crossover operator prioritizes parent solutions with better classification accuracy, while a guided mutation mechanism reintroduces prematurely excluded features for reevaluation. These components enable efficient exploration of sparse, high-dimensional spaces. Experiments on 15 large-scale datasets demonstrate that LMSSS achieves superior classification accuracy with smaller, more informative feature subsets compared to state-of-the-art methods, highlighting its effectiveness and scalability.},
  archive      = {J_ASOC},
  author       = {Azam Asilian Bidgoli and Shahryar Rahnamayan},
  doi          = {10.1016/j.asoc.2025.113755},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113755},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-phase evolutionary search space shrinking for large-scale multi-objective feature selection},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive information mesh for multimodal brain tumor segmentation. <em>ASOC</em>, <em>184</em>, 113754. (<a href='https://doi.org/10.1016/j.asoc.2025.113754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing advanced deep learning techniques for automatically segmenting brain tumors in multiparametric magnetic resonance imaging (mpMRI) is crucial in enhancing diagnostic processes. However, extreme data imbalance between brain tumors and non-tumorous tissues, as well as among different subregions within the tumors, poses significant challenges for precise tumor segmentation. In this study, we revisit the problem of positive and negative sample balance from the perspective of segmentation difficulty, integrating information entropy theory. We propose an Adaptive Information Mesh that resamples based on the conditional entropy between each pixel in the original data and the final segmentation target. This resampling approach standardizes the data to a uniform level of segmentation difficulty while preserving contextual information. Our methodology includes Multi-density Sparse Convolution and Topological Reconstruction Operator to process data effectively after AI mesh sampling. Multi-density Sparse Convolution exploits the advantages of mesh data to overcome the limitations of traditional sparse convolutions, which cannot exchange information over long distances due to a fixed receptive field. The Topological Reconstruction Operator rebuilds the topological relationships between mesh layers, facilitating information exchange. We conducted training on the BraTS 2018, 2019, and 2021 datasets. Comprehensive benchmark testing on the same metrics demonstrates that our performance is comparable to state-of-the-art structured networks in terms of accuracy while reasonably allocating computational resources. Results on the ISLES 2022 dataset for infarct segmentation in ischemic stroke underscore the robustness of our method.},
  archive      = {J_ASOC},
  author       = {Qingfan Hou and Yanjun Peng and Zhuofei Wang and Jian Jiang and Nan Lv},
  doi          = {10.1016/j.asoc.2025.113754},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113754},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive information mesh for multimodal brain tumor segmentation},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An attention-based multiple band interaction network for motor imagery EEG decoding. <em>ASOC</em>, <em>184</em>, 113750. (<a href='https://doi.org/10.1016/j.asoc.2025.113750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-computer interfaces (BCIs) based on motor imagery electroencephalogram (MI-EEG) have been extensively used in many applications to assist disabled people. Key frequency bands play a crucial role in MI-EEG signal decoding. However, existing researches pay insufficient attention to the interaction among different bands, especially during feature extraction process. To address this issue, in this work, a multiple band interaction network, called MBINet, is carefully constructed, which adopts band-dependent multi-branch setting. Particularly, an attention-based guide block and a multi-scale interaction block are elaborately designed. The former allows to improve the feature extraction process of μ and β bands by the lights of full band characteristics, and the latter enables to promote multi-angle and multi-scale interactive fusion of high-order significant features from multiple bands. The performance testing of MBINet is performed on two publicly available MI datasets, namely the BCI competition IV-2a dataset and the High gamma dataset. The experimental results show that MBINet outperforms the state-of-the-art methods, with an average classification accuracy of 81.66% and 95.78%, respectively. MBINet provides a new perspective for decoding nonlinear time series, especially EEG signals, by emphasizing diverse interactions between key frequency bands.},
  archive      = {J_ASOC},
  author       = {Weidong Dang and Kefa Zhang and Haoyu Li and Dongmei Lv and Wei Guo and Zhongke Gao},
  doi          = {10.1016/j.asoc.2025.113750},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113750},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An attention-based multiple band interaction network for motor imagery EEG decoding},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The neighborhood rough set based on division-mining-fusion strategy. <em>ASOC</em>, <em>184</em>, 113749. (<a href='https://doi.org/10.1016/j.asoc.2025.113749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neighborhood rough set (NRS) model exhibits significant value in numerous data mining tasks. However, most existing NRS models are only suitable for data mining problems in a single dataset, and their comprehensive performance in processing data is not ideal. To address these two challenges, we develop a novel NRS model by applying the division-mining-fusion (DMF) strategy. Specifically, we first decompose the original dataset into multiple subsets. Then, a submodel is established for each subset, and finally, all submodels are integrated to develop a new NRS model. The experimental results demonstrate that the developed model can not only effectively handle the data mining tasks in scenarios with multiple datasets, but also has outstanding comprehensive performance. The NRS model developed in the paper provides a novel solution for efficiently processing large-scale complex data.},
  archive      = {J_ASOC},
  author       = {Conghao Yan and Qingzhao Kong and Wenbin Zhang},
  doi          = {10.1016/j.asoc.2025.113749},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113749},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The neighborhood rough set based on division-mining-fusion strategy},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable deep learning framework for lower limb activity classification using reconstructed sEMG signals in healthy and pathological subjects. <em>ASOC</em>, <em>184</em>, 113748. (<a href='https://doi.org/10.1016/j.asoc.2025.113748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing human activities from Surface Electromyography (sEMG) signals is fraught with challenges, including noise susceptibility and signal crosstalk. Addressing these, our study analyses sEMG data from 11 healthy and 11 pathological subjects across three distinct activities: sitting, standing, and walking. We introduce a pioneering preprocessing approach that integrates Bandpass Filtering, Wavelet Denoising, and Ensemble Empirical Mode Decomposition (EEMD) for signal enhancement. A significant aspect of our approach involves the reconstruction of signals by selecting the top 50% of Intrinsic Mode Functions (IMFs) based on entropy, Signal-to-Noise Ratio (SNR), correlation, and energy, effectively capturing the most informative features of the sEMG signals. To counterbalance dataset imbalances and facilitate robust feature extraction, we applied Adaptive Synthetic (ADASYN) sampling and segmented the data into 256 ms windows with a 25% overlap. Our Convolution Neural Network (CNN) achieves remarkable classification accuracies: for sitting, 99.4% in healthy and 99.2% in pathological subjects; for standing, 99.7% and 98.8% respectively; and for walking, 99.4% and 98.6%, respectively. Furthermore, the integration of Explainable AI (XAI) through Permutation Feature Importance (PFI) provides critical insights into the significant impact of muscle signals, particularly highlighting the Rectus Femoris (RF) muscle’s role in sitting with leg extension, BF muscles’s role in standing with flexion. This comprehensive and innovative methodology not only overcomes the inherent challenges of sEMG signal analysis but also enhances the interpretability and reliability of activity recognition, marking a significant advancement for personalized healthcare interventions.},
  archive      = {J_ASOC},
  author       = {Pratibha Tokas and Vijay Bhaskar Semwal and Sweta Jain},
  doi          = {10.1016/j.asoc.2025.113748},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113748},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable deep learning framework for lower limb activity classification using reconstructed sEMG signals in healthy and pathological subjects},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mission assignment and path planning for ground forces using permutation-based simplified swarm optimization with Q-learning adaptation. <em>ASOC</em>, <em>184</em>, 113747. (<a href='https://doi.org/10.1016/j.asoc.2025.113747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study tackles task assignment and path planning for counteroffensive operations using a bi-level programming model inspired by spatial crowdsourcing. While existing models often prioritize centralized platform decisions, they tend to overlook the autonomy and real-time judgment of on-site participants, whose local actions critically affect coordination outcomes. To address this, we propose the Defender Deployment and Routing Model (DDRM), which integrates upper-level task assignment with lower-level route planning. To solve DDRM efficiently, we develop a hybrid nested framework integrating an Adaptive Permutation-Based Simplified Swarm Optimization (APSSO) algorithm with Dijkstra’s algorithm. APSSO incorporates a task-aware initialization strategy and a lightweight Q-learning mechanism for operator selection. As the first SSO-based approach with permutation-based operators, APSSO effectively handles discrete task allocation structures. Experimental results on 36 benchmark instances show that APSSO achieves the best performance in 34 cases compared to six tailored evolutionary variants. Statistical analysis confirms its superiority in solution quality, while the added Q-learning module introduces minimal computational overhead. These results highlight APSSO’s efficiency and robustness across varying problem scales.},
  archive      = {J_ASOC},
  author       = {Jun-Lin Lin and Chyh-Ming Lai and Song-Pei Wu},
  doi          = {10.1016/j.asoc.2025.113747},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113747},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mission assignment and path planning for ground forces using permutation-based simplified swarm optimization with Q-learning adaptation},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic event-triggered adaptive fuzzy admittance control of robotic systems with uncertainties. <em>ASOC</em>, <em>184</em>, 113746. (<a href='https://doi.org/10.1016/j.asoc.2025.113746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern industrial production and daily life, the increasing complexity of application scenarios has led to higher requirements for the compliance and safety of robotic systems. However, during robot control, a large number of redundant signals are often sampled, which significantly increases the communication burden. Therefore, how to substantially reduce the communication burden while ensuring satisfactory performance has become an urgent issue to be addressed. To address this challenge, this paper proposes a dynamic event-triggered adaptive fuzzy admittance control (DETAFAC) strategy for robotic systems with uncertainties, where the more aggressive dynamic event-triggered condition can significantly reduce the communication burden and the admittance model is used to reshape the desired trajectory of the robotic systems. Additionally, a fuzzy logic system (FLS) is utilized to address the uncertainties of the robotic systems, the update law of the FLS and the stability of the control system are examined using the Lyapunov stability theorem, and the dynamic triggering condition is formulated to prevent Zeno behavior. Simulation and experimental validations are performed, and the results demonstrate that the proposed DETAFAC strategy can achieve better performances in comparison to the similar approaches.},
  archive      = {J_ASOC},
  author       = {Jinzhu Peng and Xuxin Liu and Shuai Ding and Yaqiang Liu},
  doi          = {10.1016/j.asoc.2025.113746},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113746},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic event-triggered adaptive fuzzy admittance control of robotic systems with uncertainties},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boxing action recognition using inertial data and deep learning. <em>ASOC</em>, <em>184</em>, 113745. (<a href='https://doi.org/10.1016/j.asoc.2025.113745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the ongoing shift toward automated sports performance analysis by employing Deep Convolutional Neural Networks (DCNNs) for the classification of movement data in combat sports, specifically boxing. Using data from wearable inertial measurement units (IMUs), this study addresses the limitations of traditional, expensive video analysis by providing an accessible, cost-effective alternative that can be seamlessly integrated into athletes' training routines. IMUs offer a practical solution for continuous performance monitoring and feedback, establishing a foundation for automated performance assessment during training sessions. In this study, the classification accuracy and recall of DCNNs are rigorously compared with alternative algorithms, demonstrating a high overall recall of 99 % and accuracy of 91 %. Focusing on pad work training as a key application area, this work advances the automation of boxing action recognition, with implications for both training optimization and real-time scoring automation in competitive boxing. These findings underscore the potential for wearable technology and advanced machine learning methods to transform athletic performance evaluation and bring engineering innovation to sports training.},
  archive      = {J_ASOC},
  author       = {J. Brindha and G. Nallavan and Radana Vilimkova Kahankova and Radek Martinek},
  doi          = {10.1016/j.asoc.2025.113745},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113745},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Boxing action recognition using inertial data and deep learning},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive dual-network control for modular robots in human–robot interaction. <em>ASOC</em>, <em>184</em>, 113744. (<a href='https://doi.org/10.1016/j.asoc.2025.113744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to address safety and compliance challenges in physical human–robot interaction (pHRI), this paper presents an adaptive dual-neural network (NN) control framework for modular robot manipulators (MRMs). The proposed approach enables seamless switching between autonomous trajectory tracking and compliant motion guided by human interaction. An adaptive fuzzy model uncertainty compensation method is employed to estimate the interaction torque that is utilized to detect physical contact, determining the MRM’s operation mode. Additionally, the reference trajectory tracking information is provided by human motion intention identification via NNs. This method avoids the need for external force/torque sensors and increases the intention recognition accuracy by jointly estimating human impedance parameters. The overall control scheme simultaneously incorporates adaptive fuzzy PD control and decentralized impedance control through a mode-switching structure, ensuring compliant and accurate motion in the two operation modes. A Lyapunov-based analysis is then conducted. The obtained results demonstrate that the tracking errors of the closed-loop system are uniformly ultimately bounded (UUB). An experimental validation for various tasks also demonstrates the high robustness and effectiveness of the proposed strategy. Note that all the procedures adopted in this paper are conducted following standard safety guidelines, with full consideration of ethical aspects in pHRI.},
  archive      = {J_ASOC},
  author       = {Yuexi Wang and Tianjiao An and Bo Dong and Mingchao Zhu and Yuanchun Li},
  doi          = {10.1016/j.asoc.2025.113744},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113744},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive dual-network control for modular robots in human–robot interaction},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic-informed machine learning: Data-driven reconstruction of delay differential equations models. <em>ASOC</em>, <em>184</em>, 113743. (<a href='https://doi.org/10.1016/j.asoc.2025.113743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the understanding of the dynamical mechanisms underlying complex systems, this study presents a novel data-driven framework for modeling Delay Differential Equations. The Galerkin-Koornwinder theory is combined with Neural Ordinary Differential Equations for the first time, addressing the significant challenge of reconstructing models that incorporate time-delay effects by approximating them with Ordinary Differential Equations within a rigorous mathematical framework. Leveraging Neural Ordinary Differential Equations to learn these approximate models, the approach achieves high interpretability and generalizability, effectively capturing the dynamics of various delay systems, including those with discrete delays, multi-time delays, and distributed delays. Extensive simulation experiments on models exhibiting bifurcation and chaos, as well as real-world biological data, demonstrate the superior performance of this method in long-term prediction and system dynamics reconstruction. When validated on a set of real-world biological experimental data, the long-term behavior prediction error was reduced from 11% to below 1%. The comparison with similar studies elucidates that the approach can accurately capture complex behaviors and outperforms existing methods in terms of prediction accuracy and generalization capability. All data and code are openly available, facilitating reproducibility and further research.},
  archive      = {J_ASOC},
  author       = {Xiyuan Chen and Zhong Liu and Qiubao Wang and Zikun Han},
  doi          = {10.1016/j.asoc.2025.113743},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113743},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic-informed machine learning: Data-driven reconstruction of delay differential equations models},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusion of data-driven models with a knowledge-guided loss function for flood forecasting. <em>ASOC</em>, <em>184</em>, 113742. (<a href='https://doi.org/10.1016/j.asoc.2025.113742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heavy rainfall frequently causes flooding disasters in basins of all sizes; however, small and medium-sized basins -typically defined as drainage areas less than 1000 km²- are particularly vulnerable due to their rapid runoff response, often resulting in casualties and property losses. Researchers worldwide have made excellent progress in advancing flood prediction capabilities by developing intelligent data-driven models. However, these models rely only on available data and ignore expert knowledge of floods and fail to account for the varying importance of prediction errors during flood and non-flood events in their training process. Therefore, we propose fusing data-driven models with expert knowledge frameworks by designing a custom loss function for accurate flood forecasting. The proposed model effectively interprets the complexity of flood events in small and medium-sized basins and introduces accurate results. By incorporating expert knowledge constraints, it emphasises the importance of prediction errors during critical flood events. The model was tested using hourly streamflow data from the Heihe and Tunxi basins in China. Results show that our model improves prediction accuracy by up to 80 % in root mean square error (RMSE) and 87 % in mean absolute error (MAE) in Heihe and increases the coefficient of determination (R²) by 66 % and the Kling–Gupta efficiency (KGE) by 58 % in Tunxi compared to baseline models such as INFORMER, TD-CNN-LSTM, STA-LSTM, STA-TCN, LSTM, CNN, TCN, and SVR. These findings demonstrate the model’s superior performance in providing accurate and reliable 6-hour-ahead (T + 6) flood forecasts.},
  archive      = {J_ASOC},
  author       = {Haider Malik and Jun Feng and Mohammed Abdallah and Jiru Zhang and Pingping Shao and Zaid Ameen Abduljabbar},
  doi          = {10.1016/j.asoc.2025.113742},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113742},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fusion of data-driven models with a knowledge-guided loss function for flood forecasting},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-organizing interval type-2 fuzzy neural network based on eigenvalue decomposition. <em>ASOC</em>, <em>184</em>, 113741. (<a href='https://doi.org/10.1016/j.asoc.2025.113741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an eigenvalue decomposition-based self-organizing interval type-2 fuzzy neural network (ED-SOIT2FNN) is proposed to tackle the identification problem of nonlinear systems. The network model determines both the structure and parameters of the network through online learning, which can realize the structure learning and parameter learning simultaneously. Firstly, in the structure learning process of ED-SOIT2FNN, the error criterion and the completeness criterion of fuzzy rules are used to verify whether the rules grow. Meanwhile, the eigenvalue decomposition method is adopted to find the less active rules for deletion, so that obtain a more compact network structure. Secondly, in terms of ED-SOIT2FNN parameter optimization and the characteristics of network parameters, they are divided into the linear and nonlinear ones. The algorithm of adaptive discount recursive partial least square is employed to optimize the linear parameters, which is conducive to improving the noise resistance of the network model and solving the data saturation problem. And the sliding window adaptive second-order algorithm with a forgetting factor is adopted to optimize the nonlinear parameters. Compared with the algorithm of gradient descent optimization, it can accelerate the convergence and achieve a good adaptability with stability. Finally, the proposed ED-SOIT2FNN was applied to four typical nonlinear examples for identification. The experimental results showed that compared with similar methods in the existing literature, the proposed ED-SOIT2FNN could produce a more compact network structure with higher accuracies of identification and prediction.},
  archive      = {J_ASOC},
  author       = {Panchao Wang and Taoyan Zhao and Jiangtao Cao and Ping Li},
  doi          = {10.1016/j.asoc.2025.113741},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113741},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-organizing interval type-2 fuzzy neural network based on eigenvalue decomposition},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Berth-quay crane-experiment allocation method based on improved genetic algorithm for cargo and scientific research port. <em>ASOC</em>, <em>184</em>, 113740. (<a href='https://doi.org/10.1016/j.asoc.2025.113740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Against the background of booming global shipping trade and growing demand for marine scientific research, multifunctional ports with both cargo transportation and scientific research functions, namely CSR_Ports, are facing significant operational scheduling challenges. Due to the intricate mutual influence and resource competition among cargo vessels, research vessels, and marine experiments, the existing method struggles to cope with the diversified demands of port operations. To solve the above challenges, this study considers the differences in infrastructure requirements between cargo vessels and research vessels, designs differentiated work areas, abstracts the berth-quay crane-experiment allocation problem into a two-dimensional packing problem with time window constraints, and establishes a dual-objective berth-quay crane-experiment allocation (BQCEA) model to minimize the vessels' average turnaround time and the average variation of the experiment start time. Then, given the limitations of genetic algorithms, such as insufficient adaptive ability and a tendency to fall into local optimality, an improved algorithm, namely NQRGA, is proposed based on neighborhood search, quantum multi-point crossover, and retention mechanism. Finally, the BQCEA model is combined with the NQRGA to propose a novel solution method for solving the berth-quay crane-experiment allocation problem, referred to as BQCEA_NQRGA. The effectiveness of the BQCEA_NQRGA method is evaluated using real operational data from two CSR_Ports in China. The test results show that, compared with the FCFS method, the BQCEA_NQRGA method achieves at least a 49.79 % performance improvement, and the optimization effect gradually increases to 101 % with the increase in the scheduling scale. Compared with the six advanced algorithms selected in this paper, the proposed NQRGA performs best in all six different instances. Notably, the BQCEA_NQRGA method can generate high-quality allocation schemes for 70 vessels and 14 experiments simultaneously within 100 s.},
  archive      = {J_ASOC},
  author       = {Ming-Wei Li and Xiang-Yang Li and Jing Geng and Zhong-Yi Yang and Wei-Chiang Hong},
  doi          = {10.1016/j.asoc.2025.113740},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113740},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Berth-quay crane-experiment allocation method based on improved genetic algorithm for cargo and scientific research port},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel deep reinforcement learning model based on DDPG considering attention mechanism and combined with GRU network for short-term load forecasting. <em>ASOC</em>, <em>184</em>, 113739. (<a href='https://doi.org/10.1016/j.asoc.2025.113739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-term load forecasting plays a crucial role in power system operations and energy market efficiency. This paper addresses the challenge of capturing complex nonlinear patterns and temporal dependencies in electricity load data, which traditional forecasting methods often fail to handle effectively. We propose a novel deep reinforcement learning approach that combines attention mechanisms with gated recurrent units within a deep deterministic policy gradient framework (Attention-GRU-DDPG). The key innovation lies in treating load forecasting as a decision-making problem, where the model learns to adapt its predictions based on multivariate inputs including historical load, weather conditions, and electricity prices. Our approach uniquely integrates three complementary components: attention mechanisms for feature prioritization, GRU networks for temporal pattern recognition, and reinforcement learning for adaptive strategy optimization. Extensive experiments on Australian electricity market data demonstrate that our model achieves superior forecasting accuracy compared to ten benchmark methods. The proposed framework offers a practical solution for power system operators requiring accurate 24–168 h ahead load predictions, contributing to more efficient grid management and market operations.},
  archive      = {J_ASOC},
  author       = {Xin He and Wenlu Zhao and Zhijun Gao and Licheng Zhang and Qiushi Zhang and Xinyu Li},
  doi          = {10.1016/j.asoc.2025.113739},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113739},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel deep reinforcement learning model based on DDPG considering attention mechanism and combined with GRU network for short-term load forecasting},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel blood supply management model using evolutionary neural network. <em>ASOC</em>, <em>184</em>, 113738. (<a href='https://doi.org/10.1016/j.asoc.2025.113738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing demand and supply uncertainties and the short lifetime of blood products cause wastage of the blood gathered from donors. Also, there is a high shortage of blood products because of the limited number of donors and emergency demands. Hence, there is a great significance to constructing a blood supply management model by minimizing (1) transportation cost, (2) shortage cost, (3) wastage cost, (4) ordering cost, and (5) inventory holding cost. The different types of costs with constraints make blood supply management a challenging multi-objective combinatorial optimization problem. Earlier, neural genetic algorithms mostly concentrate on single-objective optimization problems. In this paper, we propose a blood supply management procedure using a network-driven evolutionary network (D2NNEA). A set of pointer networks are utilized to solve the multi-objective problem. The performance of the proposed technique is analyzed by comparing it with existing techniques and the superiority of the proposed technique in solving multi-objective optimization problems is verified.},
  archive      = {J_ASOC},
  author       = {Harinandan Tunga and Soumyadip Dhar and Samarjit Kar and Debasis Giri},
  doi          = {10.1016/j.asoc.2025.113738},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113738},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel blood supply management model using evolutionary neural network},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial unified learning for dynamic change detection in hyperspectral images. <em>ASOC</em>, <em>184</em>, 113737. (<a href='https://doi.org/10.1016/j.asoc.2025.113737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image change detection (HSI-CD) aims to identify changes in bi-temporal hyperspectral images (HSIs) captured at different times in the same location. Existing algorithms often overlook the inherent class imbalance in HSI-CD, leading to poor generalization in detecting changes while introducing redundant computation in unchanged regions. This paper introduces a novel mechanism based on Partial Unified Learning for Dynamic Change Detection (PUL-DCD) to address these limitations. Particularly, a novel partial unified learning network is proposed, whose backbone is trained using multiple datasets, whilst the task-specific networks are trained independently with each individual dataset. In so doing, the network can maintain outstanding performance on specific datasets while having strong generalization ability. Furthermore, an innovative dynamic architecture is introduced that distinguishes between easy and hard regions for change detection, thereby optimizing parameter configuration and enhancing detection performance in challenging areas, while mitigating redundancy regarding unchanged information. Experimental results on three datasets show that PUL-DCD is competitive in both accuracy and efficiency.},
  archive      = {J_ASOC},
  author       = {Keyun Zhao and Ying Li and Qingping Zheng and Qiang Shen},
  doi          = {10.1016/j.asoc.2025.113737},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113737},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Partial unified learning for dynamic change detection in hyperspectral images},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Offline reinforcement learning for job-shop scheduling problems. <em>ASOC</em>, <em>184</em>, 113736. (<a href='https://doi.org/10.1016/j.asoc.2025.113736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning have shown significant potential for solving combinatorial optimization problems in real-time. Unlike traditional methods, deep learning can generate high-quality solutions efficiently, which is crucial for applications like routing and scheduling. However, existing approaches like deep reinforcement learning (RL) and behavioral cloning have notable limitations, with deep RL suffering from slow learning and behavioral cloning relying solely on expert actions, which can lead to generalization issues and neglect of the optimization objective. Offline RL addresses these challenges by learning from fixed datasets while leveraging reward signals, making it especially suitable for constrained combinatorial problems where online exploration is impractical. This paper introduces a novel offline RL method designed for combinatorial optimization problems with complex constraints, where the state is represented as a heterogeneous graph and the action space is variable. Our approach encodes actions in edge attributes and balances expected rewards with the imitation of expert solutions. We demonstrate the effectiveness of this method on job-shop scheduling and flexible job-shop scheduling benchmarks, achieving superior performance compared to state-of-the-art techniques.},
  archive      = {J_ASOC},
  author       = {Imanol Echeverria and Maialen Murua and Roberto Santana},
  doi          = {10.1016/j.asoc.2025.113736},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113736},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Offline reinforcement learning for job-shop scheduling problems},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrogate-assisted multi-objective covariance matrix adaptation evolution strategies. <em>ASOC</em>, <em>184</em>, 113728. (<a href='https://doi.org/10.1016/j.asoc.2025.113728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is one of the most preferred evolutionary algorithms for single-objective black-box optimization, its performance is limited in the multi-objective space due to its reliance solely on Gaussian-based mutation without any crossover for offspring generation. To address this limitation, this work proposes a surrogate-assisted multi-objective CMA-ES algorithm with an ensemble of offspring generation schemes. In the proposed algorithm, trial solutions are generated from an ensemble of the standard CMA-ES operator and a Genetic Algorithm-inspired operator. Consequently, the solutions are evaluated on a Gaussian Process-based surrogate model, and the solution with the best Expected Improvement (EI) is selected as the generated offspring. Experiments on the Walking Fish Group (WFG) test suite and 18 benchmark multi-objective Neural Architecture Search (NAS) problems demonstrate that the proposed approach is statistically superior to existing multi-objective CMA-ES variants and other state-of-the-art non-CMA-ES multi-objective algorithms. Specifically, the proposed algorithm achieves a win rate of 79.63% and 77.8% on the WFG and NAS test suites, respectively, against other CMA-ES variants, and demonstrates a 68.8% win rate against state-of-the-art algorithms on the NAS test suite.},
  archive      = {J_ASOC},
  author       = {Oladayo S. Ajani and Adeyinka Adedigba and Kalyana C. Veluvolu and Rammohan Mallipeddi},
  doi          = {10.1016/j.asoc.2025.113728},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113728},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate-assisted multi-objective covariance matrix adaptation evolution strategies},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-source domain adaptation approach with learning domain-specific representations for bearing fault diagnosis under limited samples. <em>ASOC</em>, <em>184</em>, 113727. (<a href='https://doi.org/10.1016/j.asoc.2025.113727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bearings, vital to train traction motors, directly influence train safety due to their exposure to harsh environments and alternating loads, leading to various faults. Bearing fault diagnosis is thus critical for ensuring train safety. However, the challenge of data privacy and the scarcity of data make diagnosing faults difficult. Currently, transfer models based on single-source domain adaptation are used to address the above issues. However, data from multiple related domains may exist simultaneously in some scenarios. Therefore, learning fault knowledge from multiple domains and transferring it to the target domain for faults is a difficult problem. In view of this, we propose a multi-source domain adaptation approach with learning domain-specific representations (MDA-LDDSR) for bearing fault diagnosis under limited samples. First, MDA-LDDSR adds a domain-specific feature extractor (MAS-Net) for source and target domain to align the target domain with the distribution. After purifying the features by a partial feature selection strategy, we innovatively construct an intra-class alignment strategy to keep the marginal and conditional distributions of the data in alignment. Then, we utilize the instance-to-domain Mahalanobis distance to explicitly model the similarity between different domains. Finally, this similarity is assigned in the form of weights to multiple domain hypotheses. Superiorly, experiments in real rolling bearing fault diagnosis also show that the model has better convergence while it is characterized by high efficiency and robustness. The proposed approach has a strong advantage to be used for real-time bearing fault diagnosis when noise is added to the environment. Also, it obtains the desired diagnosis results compared to other approaches of the same type.},
  archive      = {J_ASOC},
  author       = {Qiang Zhou and Wengang Ma and Yadong Zhang and Jin Guo},
  doi          = {10.1016/j.asoc.2025.113727},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113727},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-source domain adaptation approach with learning domain-specific representations for bearing fault diagnosis under limited samples},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrounding and context-aware network for individual emotion complement in social networks. <em>ASOC</em>, <em>184</em>, 113726. (<a href='https://doi.org/10.1016/j.asoc.2025.113726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most sentiment analysis focuses on identifying individual emotional shifts related to a specific topic or predicting an individual’s emotional state in the future. However, it overlooks the influence of surrounding factors and contextual emotional information within social networks. Furthermore, it is also important to address the gap in understanding the missing emotions of individuals during critical historical moments. In this paper, we propose a surrounding and context-aware network for individual emotion complement(denoted as SCAEC) to infer missing emotional states at pivotal moments in history. The SCAEC network consists of an encoder layer for emotion embedding, a surrounding-aware layer, a context-aware layer, a feature interactive learning layer and an emotion distribution layer. Additionally, we introduce a decay long short-term memory (LSTM) network (DLSTM) within the SCAEC, which effectively extracts emotional features from individuals during moments of emotional absence and allows for the decay of emotional influence over time. We use DLSTM to extract associated emotional features as global features and apply multi-head attention to capture key emotional features as local features from posts/comments before and after moments of emotional absence. This allows us to obtain surrounding and context features based on the global and local features, respectively. The feature interactive learning layer then combines these features between the surrounding and context features to learn their interaction. Additionally, the emotion distribution layer simulates the emotional interaction between the individual and their surroundings. Finally, we derive the individual’s emotion distribution. Experimental results demonstrate that SCAEC model achieves an F1 score of 67.52% on the X and 68.98% on the Microblog dataset, marking an improvement of 1%–10% compared to baseline methods.},
  archive      = {J_ASOC},
  author       = {Sai Kang and YaJun Du and Xianyong Li and Xiaoliang Chen and Chunzhi Xie and Jia Liu and Yan-li Lee},
  doi          = {10.1016/j.asoc.2025.113726},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113726},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrounding and context-aware network for individual emotion complement in social networks},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spam bot detection on twitter platform using positional attention based dense convolutional neural network. <em>ASOC</em>, <em>184</em>, 113725. (<a href='https://doi.org/10.1016/j.asoc.2025.113725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, social media platforms such as Twitter and Facebook play a major role in everyday life because of the incredible opportunities they offer users. Twitter is the most essential social media platform since it allows people to express themselves through tweets. However, because of its popularity among a vast number of users, the Twitter platform is being abused by automated accounts known as bots. Since automated accounts transmit fake news, fake ideas, and fake products, early detection of bots on the Twitter platform is critical. Previously, researchers introduced ineffective methods for identifying social media bots. Positional Attention-based Dense Convolutional Neural Network (PAtt_Dense CNN) is a new deep learning-based spam bot detection framework proposed in this paper. Pre-processing of the incoming data includes stop word removal, tokenization, stemming, n-gram identification, user mention, URL and hashtag removal, vocabulary density, and richness analysis. From the pre-processed images, significant features are extracted utilizing N-gram level vectorizer, TF-IDF vectorizer, character level vectorizer, and Extended Bidirectional Encoder Representations from Transformer (EBERT). After feature extraction, binary water wheel plant optimization is used to select the best characteristics. Finally, spam bot detection and classification are performed using a PAtt_Dense CNN. The performance of the proposed technique is then evaluated by analyzing the performance indicators and comparing them to existing procedures. According to the comparison results, the proposed technique achieved the enhanced outcome in terms of Accuracy, Precision, Recall, and F1-Score of 97.8 %, 97.2 %, 98.3 %, and 99.2 %, respectively.},
  archive      = {J_ASOC},
  author       = {Hemal Girishkumar Shah and Hiren Joshi},
  doi          = {10.1016/j.asoc.2025.113725},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113725},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spam bot detection on twitter platform using positional attention based dense convolutional neural network},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistency- and ordinal consensus-based multiple scenario model for group decision-making with incomplete probabilistic linguistic preference relations. <em>ASOC</em>, <em>184</em>, 113722. (<a href='https://doi.org/10.1016/j.asoc.2025.113722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete probabilistic linguistic preference relations (InPLPRs), as a practical expression to portray the uncertainty of things, can describe the information on decision makers’ (DMs’) evaluation of things in pairwise comparisons in group decision making with two dimensions simultaneously. This paper investigates a group decision-making (GDM) method with InPLPRs to express the preference information of DMs and establish a consensus reaching mechanism for multiple scenarios. First, to obtain incomplete information, we define the concept of opinion swing neighborhood based on referring to the opinions of others and analyzing the mental behavior of DMs. Furthermore, a missing information estimation model of InPLPRs considering the opinion swing neighborhood is developed to obtain consistent optimal estimates. Secondly, the ordinal consensus index is defined, facilitating a more accurate measure of consensus attainment. Then, we thoroughly explore the classification of decision situations using consistency, consensus, illogical rate, and distinguishing index. Based on this, the corresponding consensus optimization models are proposed for different decision scenarios. Subsequently, the DMs’ weights are presented for aggregating individual opinions with acceptable consistency and consensus into group opinions, and the collective opinions are ranked and selected. Finally, numerical examples, simulation experiments, and comparative analysis demonstrate the proposed method’s applicability, effectiveness, and advantages. The proposed method offers a comprehensive GDM approach based on InPLPRs, which can be applied to various real-world GDM scenarios, demonstrating broad applicability.},
  archive      = {J_ASOC},
  author       = {Ran Dang and Peide Liu and Peng Wang and Luis Martínez},
  doi          = {10.1016/j.asoc.2025.113722},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113722},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Consistency- and ordinal consensus-based multiple scenario model for group decision-making with incomplete probabilistic linguistic preference relations},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equidistant deep embedding-based multi-label group activity recognition with dependency-constrained training. <em>ASOC</em>, <em>184</em>, 113721. (<a href='https://doi.org/10.1016/j.asoc.2025.113721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group activities in real-world scenarios are often complex and require multiple labels for adequate representation, as they involve multifaceted social interactions among individuals within a small group setting. Existing group activity recognition (GAR) methods primarily focus on model inference, overlooking the inherent challenges of multi-label learning (MLL). One common approach to MLL is the label powerset (LP) method, which transforms multi-hot label vectors into one-hot vectors, converting the multi-label problem into a single-label multi-classification problem. However, this projection loses correlations among active elements in multi-hot vectors, hindering the capture of inter-activity dependencies. To address this limitation, we propose a novel equidistant deep embedding-based (EDE) multi-label GAR framework with dependency-constrained training, building upon the LP approach. Our framework leverages a deep embedding network with self-supervised equidistant regularization loss to project multi-hot label vectors into evenly spaced dense vectors, which not only facilitate problem transformation but also contain latent activity patterns, including label correlations. We treat these dense vectors as new labels and design a corresponding deep learning and classification (DLC) strategy that optimizes all GAR model parameters except for the classification layers. Additionally, we propose a training-only auxiliary branch, dubbed TransOvR module, to bolster the model’s capacity for inter-activity dependency reasoning. By leveraging Transformer’s context-aware capability, this module facilitates interactions between multiple binary classifiers, focusing on correlations between activities. Our extensive experiments demonstrate the effectiveness of our method, outperforming state-of-the-art multi-label GAR methods.},
  archive      = {J_ASOC},
  author       = {Lindong Li and Linbo Qing and Pingyu Wang and Yang Xiao and Wang Tang and Yonghong Peng},
  doi          = {10.1016/j.asoc.2025.113721},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113721},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Equidistant deep embedding-based multi-label group activity recognition with dependency-constrained training},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced nonlinear optimization of low- and high-resolution medical images using adaptive deep spearman correlation analysis (D-SCA) for pattern sequence recognition. <em>ASOC</em>, <em>184</em>, 113720. (<a href='https://doi.org/10.1016/j.asoc.2025.113720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the state-of-art model for those healthcare multimedia datasets that have nonlinear limitations during the processing of multi-dimensional or non-linear data that usually decrease the accuracy rate and increase computational cost as per computational resources. Such as correlation among the low-resolution images to high-resolution images, and videos, the reason is non-linearity between data, non-linear dimensions or variation and illusion in the data. The proposed state-of-the-art model uses multiple techniques with transfer learnings approach to solve these issues. This paper further explains the details of the proposed deep convolutional neural network model for visual feature maps extraction and then followed by the proposed algebraic extension of Spearman correlation analysis to create the relationship of low-resolution image to high-resolution image with adaptive optimization. Then it leads to the next component of model as Radial Basis Function Network (RBFN) for non-linear mapping among LR to HR images, for classification the hybrid Xception deep learning model is used and implemented on four benchmark datasets.},
  archive      = {J_ASOC},
  author       = {Muhammad Saddam Khokhar and Misbah Ayoub and Zakria and Abdullah Lakhan},
  doi          = {10.1016/j.asoc.2025.113720},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113720},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advanced nonlinear optimization of low- and high-resolution medical images using adaptive deep spearman correlation analysis (D-SCA) for pattern sequence recognition},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal deep interaction and modal-aware aggregation network for visible and infrared tracking. <em>ASOC</em>, <em>184</em>, 113719. (<a href='https://doi.org/10.1016/j.asoc.2025.113719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, developing a robust RGB-Thermal (RGBT) tracking method in complex environments remains challenging in effectively mining and fusing cross-modal complementary information, enhancing modal perception capabilities, and improving the representation ability of fused semantic features. To address these challenges, we propose a novel architecture called Multi-branch Cross-modal Deep Interaction Fusion and Adaptive Aggregation Integrating Hybrid Attention Semantic Enhancement Network (MCFTNet). Specifically, MCFTNet first extracts modality-specific features through dedicated branches, followed by designing a cross-modal deep interaction fusion network to achieves deep interaction and comprehensive fusion of cross-modal features through a fusion branch as the medium. Furthermore, a modal-aware adaptive aggregation module is developed to dynamically aggregates high-resolution features from different branches, while significantly enhancing the discriminative ability of multi-modal features. Finally, a hybrid attention semantic enhancement module is introduced that combines carefully designed enhanced multi-head attention and hierarchical attention to optimize the fused semantic features, thereby achieving highly accurate prediction of target position and shape. Extensive experiments on three mainstream public benchmark datasets, GTOT, RGBT234, and LasHeR, demonstrate the effectiveness and robustness of our proposed method.},
  archive      = {J_ASOC},
  author       = {Xiang Liu and Haiyan Li and Victor Sheng and Yujun Ma and Xiaoguo Liang and Guanbo Wang},
  doi          = {10.1016/j.asoc.2025.113719},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113719},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-modal deep interaction and modal-aware aggregation network for visible and infrared tracking},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embedding neural sampling and adversarial bandit into gene expression programming for symbolic regression. <em>ASOC</em>, <em>184</em>, 113718. (<a href='https://doi.org/10.1016/j.asoc.2025.113718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary computation methods for symbolic regression problems use a search method to actively explore the mathematical expression space to find a solution. However, the evolutionary computation search process lacks a clear direction due to the randomness of evolutionary operations. In contrast, deep learning methods focus on learning a clear mapping from a given dataset to a mathematical function, but the learning process does not generally involve an active search of the potential solution space. To combine the advantages of the active search of evolutionary computation and the clear mapping direction of neural networks, this paper proposes a novel algorithm called GVAE-ABGEP. GVAE-ABGEP incorporates a grammar variational autoencoder and adversarial bandit into gene expression programming to guide its search process. GVAE-ABGEP partitions the mathematical expression space into many subspaces. It then leverages an adversarial bandit — AvgHExp3 to choose a subspace. In the selected subspace, it then utilizes an autoencoder to sample individuals whose fitnesses are near the local optima, and executes crossover, mutation, and selection in evolutionary computation to explore the global optima. Experiments on 18 symbolic regression and 12 physics benchmarks show that GVAE-ABGEP outperforms three baseline gene expression programming methods and six baseline machine learning methods.},
  archive      = {J_ASOC},
  author       = {Qiang Lu and Qiuchen Yuan and Dawei Li and Congwen Xu and Jake Luo and Zhiguang Wang},
  doi          = {10.1016/j.asoc.2025.113718},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113718},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Embedding neural sampling and adversarial bandit into gene expression programming for symbolic regression},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based analysis of problem space and genetic programming classifier performance using optimal transport dataset distance. <em>ASOC</em>, <em>184</em>, 113716. (<a href='https://doi.org/10.1016/j.asoc.2025.113716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and interpreting machine learning tasks is a non-trivial endeavor. To this end, many works have proposed creative ways to organize and visualize a problem dataset, the learning process itself, or the models produced after learning has occurred. Moreover, as the scope of machine learning increases, the re-purposing or specialization of models becomes more common, or as continued automation is sought for algorithm deployment, understanding sets of problems has also become relevant. The present work proposes a graph-based representation of a set of machine learning problems, a novel characterization of the relationships and structures that are present in such a set. While similar proposals have been made before, the proposed methodology employs a recent and unique metric between problem datasets, the Optimal Transport Dataset Distance, which allows for the computation of a mathematically rigorous distance matrix for problem sets. This allows for the construction of a graph-based representation, while previous works relied on representations based on problem meta-features that were defined heuristically. Results show that the resulting graph representation of a problem set exhibits structural properties that are related to empirical indicators of problem difficulty, such as the average error, the size of the dataset, and the class imbalance. A similar analysis using meta-features shows that the structure of the resulting graphs cannot capture the same nuanced relationships between problems.},
  archive      = {J_ASOC},
  author       = {Joel Lee Nation and Daniel Fajardo and Yuliana Martínez and Arnoldo Díaz-Ramírez and Leonardo Trujillo},
  doi          = {10.1016/j.asoc.2025.113716},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113716},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph-based analysis of problem space and genetic programming classifier performance using optimal transport dataset distance},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based deep reinforcement learning for dynamic scheduling of flexible job-shop considering worker fatigue and multi-skill factors. <em>ASOC</em>, <em>184</em>, 113712. (<a href='https://doi.org/10.1016/j.asoc.2025.113712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the dynamic scheduling problem of flexible job shops (DFJSP) has garnered significant attention. However, most DFJSP studies focus solely on machine constraints, often overlooking the crucial factor of worker constraints. As an essential resource within the production process, the efficient utilization of workers can substantially enhance production efficiency. Consequently, the scheduling problem in dual-resource constrained (DRC) production systems, where operation times fluctuate according to worker skill levels and fatigue, is investigated. Considering new job arrivals as dynamic events, this study comprehensively integrates worker skill and fatigue factors, with the goal of minimizing total cost and makespan by developing a dual-resource constrained dynamic flexible job-shop scheduling (DRC-DFJSP) optimization model. To enable real-time model resolution in dynamic environments, an end-to-end deep reinforcement learning (DRL) scheduling method is introduced. The decision-making process is formulated as a Markov decision process (MDP) and guided by a reward mechanism tailored to optimization objectives. An enhanced proximal policy optimization (PPO) algorithm, combined with an adaptive clipping mechanism and a prioritized experience replay buffer, is applied to handle operation ordering, machine assignment, and worker allocation within the DRC-DFJSP framework. To further enhance decision-making capabilities, an attention-based graph neural network feature extraction method is incorporated to capture the intricate connections between operations, machines, and workers, resulting in a more precise characterization of the workshop state. Numerical experiments and case studies demonstrate that the proposed scheduling method surpasses existing strategies in dynamic environments that account for worker skill and fatigue levels.},
  archive      = {J_ASOC},
  author       = {Yiwen Hu and Zequn Zhang and Jie Chen and Dunbing Tang and Qixiang Cai},
  doi          = {10.1016/j.asoc.2025.113712},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113712},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph-based deep reinforcement learning for dynamic scheduling of flexible job-shop considering worker fatigue and multi-skill factors},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Command and control network architecture optimization method based on dual-layer weighting and TOPSIS-GRA of indicators. <em>ASOC</em>, <em>184</em>, 113711. (<a href='https://doi.org/10.1016/j.asoc.2025.113711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern warfare, the effectiveness of command and control (C2) networks plays a pivotal role in shaping operational performance and determining combat outcomes. Even under identical resource allocations, variations in command strategies and network architectures can lead to vastly different results. While many existing optimization methods rely on complex network theory to enhance system robustness, they often exhibit randomness and overlook commanders’ intent and expert knowledge during architecture design. Unlike civilian networks, military C2 networks are highly dependent on operational planning and domain expertise, emphasizing decision transparency and interpretability. Subjective evaluation methods-such as those based on rules or expert logic-are more intuitive for commanders but suffer from bias and inconsistency. Conversely, purely data-driven models like deep learning lack transparency, limiting their practical utility in high-stakes scenarios. To bridge this gap, this study proposes a dual-layer weighting optimization approach based on the TOPSIS and Grey Relational Analysis (GRA) methods. First, C2 network architectures are constructed using complex network theory. Then, an evaluation index system is developed around four key metrics: invulnerability, communication efficiency, connectivity, and network density. Subjective weights are derived via the Analytic Hierarchy Process (AHP), while objective weights are calculated using an improved entropy method. These are integrated into a variable-weight model to obtain final weights for optimization. By combining TOPSIS and GRA, the method accounts for both subjective preferences and objective measurements, ensuring a balanced evaluation. Simulation results demonstrate that the proposed method comprehensively considers both subjective and objective factors as well as the impact of state changes on weight adjustments, aligning more closely with real battlefield environments. The constructed C2 network architecture can adapt to different combat missions, guided by subjective evaluations of commander intent and expert experience while being refined through objective assessments to achieve adaptive adjustments. The ranking results account for both the distance and shape variations among indicators. Statistical analysis and comparisons with other methods further validate the effectiveness of the proposed approach, which also reduces computation time by a factor of five compared to alternative algorithms, highlighting its superiority.},
  archive      = {J_ASOC},
  author       = {Jianwei Wang and Qing Zhang and Chengsheng Pan},
  doi          = {10.1016/j.asoc.2025.113711},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113711},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Command and control network architecture optimization method based on dual-layer weighting and TOPSIS-GRA of indicators},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distributed learning framework with blockchain and privacy-preserving for IoV. <em>ASOC</em>, <em>184</em>, 113710. (<a href='https://doi.org/10.1016/j.asoc.2025.113710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles (IoV), as a critical component of the Internet of Things (IoT), constructs a distributed system comprising vehicles, roadside units (RSUs), and cloud servers. In the IoV environment, the secure sharing of data and the protection of privacy are of paramount importance, as they directly impact the decision-making processes of intelligent vehicles and overall road safety. Given the openness of IoV, it faces risks of privacy leakage and poisoning attacks during data exchange and model training, which threaten the integrity and reliability of the data. To address these challenges, this study proposes a privacy protection framework that integrates blockchain technology and differential privacy. This framework incorporates dual differential privacy techniques within federated learning to enhance data privacy protection and designs a dynamic gradient aggregation mechanism to defend against data poisoning attacks, thereby ensuring the security of the data. Experimental results demonstrate that this framework maintains high model accuracy even under attack rates of up to 30%, exhibiting remarkable resilience against such attacks. Overall, this study emphasizes the significance of data security and privacy protection in the IoV domain and illustrates the potential of blockchain and differential privacy technologies in enhancing the security of IoV data and safeguarding user privacy. This research provides robust support for the sustainable development of IoV.},
  archive      = {J_ASOC},
  author       = {Chunhai Li and Yan Long and Yong Ding and Changsong Yang and Chuan Zhang and Meng Shen and Liehuang Zhu},
  doi          = {10.1016/j.asoc.2025.113710},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113710},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A distributed learning framework with blockchain and privacy-preserving for IoV},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chaotic vortex nanoparticle swarms (CVNPS): Formation and stability analysis. <em>ASOC</em>, <em>184</em>, 113708. (<a href='https://doi.org/10.1016/j.asoc.2025.113708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vortex Nanoparticle Swarms (VNPS) are a class of self-organizing systems that exhibit vortex-like collective behavior but often suffer from dynamic instability and poor angular momentum conservation. While VNPS may achieve partial spatial stability, coherent motion is typically unsustained. To address this, the Chaotic Vortex Nanoparticle Swarm (CVNPS) model is proposed, incorporating chaotic perturbations via logistic maps within a discrete particle model governed by the Generalized Morse Potential. This integration enhances system stability by accelerating convergence, reducing energy fluctuations, and promoting sustained vortex formation. Comparative analysis using key performance indicators – average nearest neighbor distance, polarization, angular momentum, swarm diameter, total energy, energy dissipation rate, and variance – demonstrates that CVNPS significantly outperforms traditional VNPS. It achieves faster stabilization and maintains coherent structure over time, even under varied initial conditions. By addressing both configurational and dynamical stability, CVNPS establishes a foundation for more reliable nanoparticle swarm applications in nanotechnology.},
  archive      = {J_ASOC},
  author       = {Mahvish Khurshid Bijli and Prabal Verma and Amrit Pal Singh},
  doi          = {10.1016/j.asoc.2025.113708},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113708},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chaotic vortex nanoparticle swarms (CVNPS): Formation and stability analysis},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aspect-based context iterative network with hyperbolic tangent function for aspect-based sentiment classification. <em>ASOC</em>, <em>184</em>, 113707. (<a href='https://doi.org/10.1016/j.asoc.2025.113707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of natural language processing, sentiment analysis emerges as a pivotal undertaking. Especially, aspect-based sentiment classification has garnered significant attention in recent years, as it can identify and evaluate emotions related to specific aspects within sentences. Existing approaches typically achieve satisfactory results by extracting keyword information from the context to identify polarity. However, a common challenge faced by these methods is the inclusion of irrelevant words in the extracted keywords, leading to decreased classification accuracy. To tackle this issue, we propose an aspect-based context multiple iteration approach, which leverages the correlation between aspects and context and employs the multi-head attention mechanism to iteratively extract contextual keywords. By doing so, we aim to mitigate the interference of irrelevant words and enhance the accuracy of sentiment classification. Additionally, we address the peculiarities of hard samples by introducing a novel loss function that cleverly incorporates the hyperbolic tangent function and allows for improved model accuracy. To validate the effectiveness of our proposed approach, we conduct extensive experiments on four widely datasets and demonstrate the efficacy of our model in improving sentiment classification.},
  archive      = {J_ASOC},
  author       = {Chao Zhu and Benshun Yi and Laigan Luo},
  doi          = {10.1016/j.asoc.2025.113707},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113707},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Aspect-based context iterative network with hyperbolic tangent function for aspect-based sentiment classification},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MOGA: Multi-objective genetic algorithm to select stacking ensemble learning for classification. <em>ASOC</em>, <em>184</em>, 113706. (<a href='https://doi.org/10.1016/j.asoc.2025.113706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stacking Ensemble Learning (SEL) has been effectively integrated with Multi-Objective Optimisation (MOO) heuristics for classification tasks across various domains, including finance, healthcare, and cybersecurity. This study aims to address the challenge of generalising SEL to a diverse set of classification cases. Thus, the Multi-Objective Genetic Algorithm (MOGA) framework is proposed, utilising a Genetic Algorithm (GA) to evolve a population of distinct SELs, each built from a varied subset of base models. The goal is to select the subset that composes the most effective SEL for a given classification task. MOGA is designed with two main objectives—maximising p r e c i s i o n and r e c a l l —which helps to maintain independence from any specific classification case. In addition, incorporating models of varied types ensures adaptability and high performance in different situations. Comprehensive experimentation was conducted on 23 diverse datasets, where MOGA demonstrated high performance in nearly all datasets, outperforming other ensemble learning (EL) methods in 100% of the datasets in p r e c i s i o n , 78% in r e c a l l , 69.5% in f 1 − s c o r e , and 78% in a c c u r a c y . A t-test analysis yielded results of p -value < 0 . 05 , indicating a statistically significant improvement in the a c c u r a c y of the MOGA over the base models. Moreover, the framework’s application can be extended to regression tasks as well.},
  archive      = {J_ASOC},
  author       = {Abdellah Rezoug and Mohamed Bader-el-den},
  doi          = {10.1016/j.asoc.2025.113706},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113706},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MOGA: Multi-objective genetic algorithm to select stacking ensemble learning for classification},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning with evolutionary algorithm-guided imitation for capacitated vehicle routing problems. <em>ASOC</em>, <em>184</em>, 113705. (<a href='https://doi.org/10.1016/j.asoc.2025.113705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capacitated vehicle routing problem (CVRP) is a complex combinatorial optimization challenge that seeks to determine cost-effective routes for customer deliveries while adhering to specific capacity constraints. Although deep reinforcement learning (DRL) has shown promise in addressing CVRP, it often encounters issues such as slow convergence and suboptimal accuracy. This study introduces an innovative approach that enhances both convergence efficiency and solution quality by integrating DRL with imitation learning (IL), utilizing an evolutionary algorithm (EA) as an expert. The proposed methodology incorporates an attention mechanism-based neural network to effectively capture the intricate features of CVRP. It leverages IL to use EA-generated solutions as expert demonstrations, thereby guiding the DRL model toward a more efficient exploration of the solution space. The REINFORCE algorithm with baseline is employed to ensure stable and rapid training of the DRL model. Experimental results indicate that this hybrid approach significantly outperforms widely adopted baseline methods and approaches the performance levels of advanced algorithms like LKH3. Furthermore, the method demonstrates robust generalization capabilities across various CVRP instances, underscoring its potential for practical applications in diverse routing scenarios. This research contributes to the field by demonstrating how integrating EA as experts within an IL framework can effectively enhance DRL for solving CVRP.},
  archive      = {J_ASOC},
  author       = {Wenqiang Zhang and Xiaomeng Wang and Yashuan Mu and Miaolei Deng and Peng Li},
  doi          = {10.1016/j.asoc.2025.113705},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113705},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning with evolutionary algorithm-guided imitation for capacitated vehicle routing problems},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A focal quotient gradient system method for deep neural network training. <em>ASOC</em>, <em>184</em>, 113704. (<a href='https://doi.org/10.1016/j.asoc.2025.113704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel approach for training deep neural networks, leveraging a mini-batch focal quotient gradient system (QGS-Focal). The proposed method addresses critical challenges in imbalanced datasets and optimization efficiency. By introducing residual constraints into the loss function, we construct a quotient gradient system that effectively mitigates model overfitting and gradient vanishing problems. By incorporating a focal loss mechanism, it innovatively solves data imbalance issues at the algorithmic level. The adoption of a mini-batch strategy and limited memory method significantly reduces computational costs. Our comprehensive experiments on imbalanced CIFAR-10 and CIFAR-100 have demonstrated the superiority of QGS-Focal, achieving 83.4 % precision, 83.6 % recall, and 82.9 % F1-score on CIFAR-10, significantly outperforming SGD, Adam, and QGS. Moreover, our approach reduces training time by 13.3 %, enhancing computational efficiency while maintaining superior classification performance. The t-SNE visualization further confirms that QGS-Focal has superior convergence properties compared to traditional optimization approaches.},
  archive      = {J_ASOC},
  author       = {Caili Lv and Xian-long Lv and Zhiyuan Wang and Tianqi Zhao and Wei Tian and Qingqing Zhou and Lin Zeng and Min Wan and Chenghu Liu},
  doi          = {10.1016/j.asoc.2025.113704},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113704},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A focal quotient gradient system method for deep neural network training},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial differential equations and machine learning integration for transit-oriented development. <em>ASOC</em>, <em>184</em>, 113703. (<a href='https://doi.org/10.1016/j.asoc.2025.113703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate classification of rail transit stations is critical for advancing Transit-Oriented Development (TOD) and promoting sustainable urban growth. This research presents a novel hybrid framework that integrates Partial Differential Equations (PDEs) with Machine Learning (ML) techniques for the classification of rail transit stations. Unlike conventional TOD models, this study applies the heat equation to the Node, Place, Ridership-Time (NPRT) framework, offering a mathematically grounded approach to capture spatial-temporal dynamics in transit systems. This integration represents the first known application of PDE-based physical modeling combined with supervised learning for classifying transit stations within a TOD framework. This approach significantly enhances the model’s interpretability while maintaining competitive prediction accuracy. Through extensive case studies and empirical validation, the PDE-NPRT model demonstrates strong performance, with Mean Squared Error (MSE) values ranging from 0.0075 to 0.0222. Although slightly outperformed by enhanced ML models—such as K-Nearest Neighbors (KNN), Deep KNN (DKNN), and Deep Distributed Neural Networks (DDNN)—which achieve MSEs as low as 0.0034, the PDE-NPRT model offers a more interpretable and theoretically robust alternative. Additionally, the study introduces a multi-layer modeling strategy that combines regression analysis, clustering algorithms, PDEs, and neural networks, further enriching the understanding of ridership patterns and congestion mechanisms. Clustering outcomes are validated through external indices, confirming the alignment of model predictions with real-world site characteristics. This work represents a significant advancement in TOD modeling, offering a robust and explainable tool for urban planners and decision-makers. By bridging advanced mathematical modeling with machine learning, it paves the way for more intelligent, data-driven, and sustainable urban mobility strategies.},
  archive      = {J_ASOC},
  author       = {Ahad Amini Pishro and Shiquan Zhang and Alain L’Hostis and Qixiao Hu and Yuetong Liu and Zhengrui Zhang and Van Duc Nguyen and Yongguo Fu and Tianzeng Li},
  doi          = {10.1016/j.asoc.2025.113703},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113703},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Partial differential equations and machine learning integration for transit-oriented development},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic SLAM algorithm based on improved YOLOv9S. <em>ASOC</em>, <em>184</em>, 113700. (<a href='https://doi.org/10.1016/j.asoc.2025.113700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitations of conventional visual SLAM algorithms and the time-intensive process of semantic segmentation in dynamic environments, this paper proposes a SLAM algorithm that incorporates an enhanced version of YOLOv9S specifically designed for dynamic scenes. The ECASPPELAN and DRepNCSPELAN4 modules have been developed to improve the model’s detection capabilities in dynamic scenes. Secondly, the improved YOLOv9S is integrated into the ORB-SLAM2 framework, which employs bipolar geometry to reject dynamic targets and utilizes a depth separation algorithm to prevent the erroneous rejection of static features. Subsequently, the detection performance of the model was evaluated on the PASCAL VOC dataset, resulting in a 2% improvement in mAP_0.5 and a 1.3% improvement in mAP_0.5:0.95. The tracking performance of the system is examined in the TUM dynamic dataset, where the absolute trajectory root mean square error (RMSE) is reduced by 97.5%, and the relative rotational RMSE is reduced by 92.9% in comparison to ORB-SLAM2 in the high-dynamics scenario. Additionally, the average running rate surpasses 30 Hz while constructing high-quality dense maps in real indoor dynamic environments. This allows the robot to receive more detailed texture information. The experimental results demonstrate that the enhanced algorithm presented in this paper exhibits enhanced accuracy and resilience in dynamic scenarios.},
  archive      = {J_ASOC},
  author       = {Qiguang Zhu and Yuchao Zhao and Haofeng Zhang and Weidong Chen},
  doi          = {10.1016/j.asoc.2025.113700},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113700},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic SLAM algorithm based on improved YOLOv9S},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural semantic evaluation of blockchain policy tools in china. <em>ASOC</em>, <em>184</em>, 113699. (<a href='https://doi.org/10.1016/j.asoc.2025.113699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To explore the differences in policies guiding the development of the blockchain industry in different regions of mainland China, in this study, we developed a compound neural network model comprising bidirectional long short term memory and deep biaffine attention models that analyses the semantic texts of blockchain policies issued by 31 provinces in mainland China. Machine learning models — specifically, term frequency–inverse document frequency and K-means models are used to implement feature selection of the policy text matrix classification after semantic analysis. Finally, this study proposes an innovative policy tools matching approach. We construct a word-topological map for each text category based on semantic relationships. To validate the effectiveness of these tools, we conduct an empirical analysis using a multivariate linear regression model. The results demonstrate that blockchain policy tools significantly promote blockchain innovation in Mainland China.},
  archive      = {J_ASOC},
  author       = {Yuxi Zhang and Haifeng Guo and Ke Peng and Hongzhi Wang},
  doi          = {10.1016/j.asoc.2025.113699},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113699},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neural semantic evaluation of blockchain policy tools in china},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable nature-inspired optimization via virtual and actual multi-objective strategies to establish a smart earthquake early warning system. <em>ASOC</em>, <em>184</em>, 113698. (<a href='https://doi.org/10.1016/j.asoc.2025.113698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geosynthetic-reinforced soil (GRS) structures are considered for reducing displacement and providing economical reinforcement solutions. The risk assessment of these structures against earthquakes, based on the prediction of seismic sliding displacement, is a major challenge in this field. Multi-objective optimization is a powerful machine learning tool for selecting efficient features for high-performance forecasting. This research investigates two strategies based on swarm intelligence and genetic programming for a comprehensive evaluation. These frameworks integrate multiobjective optimization algorithms and Newmark methods for utilizing effective physics-informed features. The first strategy is virtual multi-objective (VMO) optimization by applying particle swarm optimization (PSO) based on minimizing one function via variations of other functions. In this approach, the error function, as a computational error object, is minimized versus the nomination of interpretable feature space as a computational cost object through the virtual Pareto front. The second strategy is actual multi-objective (AMO) optimization by exploiting nondominated sorting genetic algorithm II (NSGA-II) based on minimizing several functions simultaneously with two various approaches, including bi-objective and many-objective algorithms through actual Pareto-optimal solutions. In this approach, the computational error value, computational cost value, and computational time value are minimized at the same time. The main novelty of the first technique is low computational complexity, resulting in high speed due to definite search space dimension-based exploration and exploitation to forecast seismic sliding displacement, whereas the major achievement of the second technique is high computational accuracy due to multiobjective structure-assisted exploitation and exploitation. Through numerical validation by employing the Newmark methods, the resultant model predicts the seismic sliding displacement of these structures using two algorithms efficiently. Nevertheless, both strategies have good performance for intelligent forecasting. The actual many-objective optimization algorithm is a more effective switchable machine learning tool based on the proposed adaptable performance index for developing a smart earthquake early warning software that can precisely detect imminent natural hazards.},
  archive      = {J_ASOC},
  author       = {Milad Zarchi and Reza A. Nazari and Kong Fah Tee},
  doi          = {10.1016/j.asoc.2025.113698},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113698},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable nature-inspired optimization via virtual and actual multi-objective strategies to establish a smart earthquake early warning system},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTKD-DL: Dual-teacher knowledge distillation with dual-loops for continuous few-shot relation extraction. <em>ASOC</em>, <em>184</em>, 113696. (<a href='https://doi.org/10.1016/j.asoc.2025.113696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new model named DTKD-DL, designed to address the issue of Continuous Few-Shot Relation Extraction (CFRE) across tasks, with the goal of learning and adapting to newly emerging relations while reducing catastrophic forgetting. In this paper, we have designed a dual-teacher knowledge distillation model based on relation information to enrich knowledge representation and retain prior knowledge. We employ a dual-loops distillation approach, which facilitates knowledge transfer and optimizes the direction of parameter updates, thereby reducing the occurrence of catastrophic forgetting. Furthermore, to avoid overfitting issues caused by multiple rounds of distillation, we have innovatively integrated reinforcement learning with the model. We have validated our model on the FewRel and TACRED datasets and compared it with the large language model Llama3-8b, demonstrating the effectiveness of our model in this scenario and its advantages over the most advanced methods, achieving state-of-the-art results.},
  archive      = {J_ASOC},
  author       = {Ruifeng Xu and Yi Chen and Zhongyan Yi and Shun Huang and Liang He},
  doi          = {10.1016/j.asoc.2025.113696},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113696},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DTKD-DL: Dual-teacher knowledge distillation with dual-loops for continuous few-shot relation extraction},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage feature selection approach with fuzzy covering-based rough sets based on discernibility matrix. <em>ASOC</em>, <em>184</em>, 113695. (<a href='https://doi.org/10.1016/j.asoc.2025.113695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy β -covering has attracted considerable research attention due to its enhanced capability to accurately and comprehensively represent uncertain information. Unlike partition-based approaches, fuzzy β -covering maintains its covering properties when augmented with additional elements. This inherent flexibility necessitates rigorous redundancy analysis. Hence, the calculation of the reduct of fuzzy β -covering relative to decision attribute constitutes a fundamental challenge in this context. To address this issue, we formally define the concepts of indispensable covering elements and core of fuzzy β -covering with respect to the decision attribute. Then, to facilitate efficient computation of core and reduct, we introduce the discernibility matrix for fuzzy β -covering and establish equivalent representations of the core and reduct. Furthermore, we conduct systematic examinations to verify the indispensability of some existing concepts in fuzzy β -covering. Meanwhile, a significant limitation of current feature selection methods useing fuzzy β -covering lies in their computational complexity, primarily due to repeated recalculation of dependency functions during iterations. To overcome this limitation, we propose an efficient feature selection algorithm that identifies all reducts directly through the discernibility matrix. Furthermore, we propose a two-stage feature selection method for fuzzy β -covering, which iteratively eliminates redundant features to identify optimal feature subsets in each iteration. Finally, we verify through time complexity analysis and numerical experiments that the proposed algorithm significantly outperforms existing feature selection algorithms in computational efficiency. Comparative analysis further reveals that our method achieves superior results in terms of selected feature subset sizes and classification accuracy.},
  archive      = {J_ASOC},
  author       = {Tianyu Wang and Shuai Liu and Bin Yang},
  doi          = {10.1016/j.asoc.2025.113695},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113695},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage feature selection approach with fuzzy covering-based rough sets based on discernibility matrix},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-source remote sensing image watermarking model based on multi-domain generative adversarial networks. <em>ASOC</em>, <em>184</em>, 113694. (<a href='https://doi.org/10.1016/j.asoc.2025.113694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the heterogeneity stemming from distinct sensor types, quantized spatial resolution levels, and categorized acquisition angles within multi-source remote sensing images, their data distribution and characteristics vary significantly, leading to poor cross-domain consistency and weak resistance to domain-specific distortions in traditional watermarking approaches. Traditional watermarking methods often struggle to handle such heterogeneous datasets. To address this challenge, this paper presents a watermarking model based on a multi-domain generative adversarial network (MD-GAN). By incorporating a multi-domain adversarial training mechanism and designing multiple domain discriminators, the generator is optimized to work across various image domains, thereby enhancing model robustness. Additionally, a multi-scale generator is utilized to account for differences in spatial resolution, improving both the watermark’s concealment and extraction accuracy. Experimental results demonstrate that the MD-GAN model achieves a 7 % improvement in robustness over the state-of-the-art models in terms of resistance to noise attacks and geometric transformations. Ablation studies confirm that the multi-domain adversarial training mechanism and multi-scale generator contribute significantly to this enhancement. MD-GAN provides a powerful solution for the copyright protection of multi-source remote sensing images, with substantial potential for real-world applications such as secure transactions and data sharing.},
  archive      = {J_ASOC},
  author       = {Heyan Wang and Xingxiang Jiang and Minxuan Wang and Changqing Zhu and Na Ren and Luanyun Hu},
  doi          = {10.1016/j.asoc.2025.113694},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113694},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-source remote sensing image watermarking model based on multi-domain generative adversarial networks},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale convolution-attention model for efficient alzheimer’s disease and mild cognitive impairment diagnosis approach using sMRI. <em>ASOC</em>, <em>184</em>, 113693. (<a href='https://doi.org/10.1016/j.asoc.2025.113693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer's disease (AD) affects over 50 million people worldwide and causes a gradual decline in memory, language, and actions, with no known cure. Early identification and treatment are crucial to improve the quality of life of affected individuals. Deep learning algorithms based on brain structural magnetic resonance imaging (sMRI) show promise in predicting AD. However, relying solely on deep convolutional neural network (CNN) architecture has limitations in global modeling. Transformer, which is a feature learning technique, has excelled in computer vision applications, sparking interest in its application in brain image processing. However, pure Transformer architectures encounter challenges when trained on small sMRI datasets. Meanwhile, CNN-based methods do not consider the interdependence between voxels, which hinders the comprehensive understanding of the global characteristics of sMRI data. To address these challenges, an effecient deep learning framework was developed that combines the benefits of both the CNN and Transformer. In the preliminary phase of feature extraction, we introduce a multiscale feature fusion stem that employs convolutional kernels of varying scales to derive low-level features from the sMRI and integrate them to enhance the recognition accuracy. Furthermore, the proposed method introduces convolutional split attention with a squeeze and excitation block and additional convolution operations in the core sections of the Transformer, thereby enabling multiscale feature extraction and fusion. The model integrates multi-head light self-attention and a sandglass local feed-forward network block for classifier modeling, enhancing the extraction of sMRI features with a MobileNet cost. The proposed model, which was tested on the Alzheimer's Disease Neuroimaging Initiative dataset, achieved superior performance in AD diagnosis with fewer parameters and reduced computational costs, demonstrating its potential as a state-of-the-art solution.},
  archive      = {J_ASOC},
  author       = {Uttam Khatri and Jun-Hyung Kim and Goo-Rak Kwon},
  doi          = {10.1016/j.asoc.2025.113693},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113693},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiscale convolution-attention model for efficient alzheimer’s disease and mild cognitive impairment diagnosis approach using sMRI},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). F2CAU-net: A dual fuzzy medical image segmentation cascade method based on fuzzy feature learning. <em>ASOC</em>, <em>184</em>, 113692. (<a href='https://doi.org/10.1016/j.asoc.2025.113692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate medical image segmentation plays a pivotal role in clinical diagnosis and treatment planning. However, existing methods – particularly those based on U-Net architectures – still face considerable difficulties when dealing with ambiguous boundaries, low-contrast regions, and noise artifacts. These challenges arise from the deterministic nature of conventional convolutional and attention mechanisms, which are often inadequate in modeling the inherent uncertainty and variability present in medical images. To tackle these limitations, we propose F 2 CAU-Net, a Dual Fuzzy Medical Image Segmentation Cascade Method that integrates a fuzzy convolution module for better modeling of uncertain and imprecise features, and a fuzzy attention mechanism to suppress redundancy and enhance focus on regions of interest. The proposed method captures both local and global contextual fuzzy information to improve segmentation accuracy and robustness. Extensive experiments on multiple benchmark datasets – including COVID-19 lesions, brain tumors, and skin cancer – demonstrate that F 2 CAU-Net significantly outperforms existing state-of-the-art models, particularly in scenarios with complex boundaries and noise. This approach offers promising potential for clinical applications, providing a more reliable and uncertainty-aware solution for medical image analysis.},
  archive      = {J_ASOC},
  author       = {Tianyi Zhou and Haipeng Wang and Sheng Geng and Hengrong Ju and Jiashuang Huang and Fan Fu and Weiping Ding},
  doi          = {10.1016/j.asoc.2025.113692},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113692},
  shortjournal = {Appl. Soft. Comput.},
  title        = {F2CAU-net: A dual fuzzy medical image segmentation cascade method based on fuzzy feature learning},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A physics-informed mitigation method for DC microgrids under cyber attacks. <em>ASOC</em>, <em>184</em>, 113691. (<a href='https://doi.org/10.1016/j.asoc.2025.113691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber attacks pose serious threats to DC microgrids, making effective mitigation strategies very critical. However, most existing mitigation schemes ignore the physical characteristics of microgrids, which may lead to inaccurate attack detection and affect the mitigation performance. To address this issue, a physics-informed mitigation method for cyber attacks on DC microgrids is proposed. A cyber–physical framework of the DC microgrid is established, and the impact of cyber attacks on the microgrid is analyzed. The denoising autoencoder is utilized to improve the quality of the input data, and then the consistency characteristic of the DC microgrid is incorporated as a physical-informed constraint into the training of the state estimation model. Subsequently, anomaly detection is performed by comparing the estimated state and the real-time measured state. Once the attacks are detected, the estimated state is used to compensate the control inputs, mitigating the effect of attacks and ensuring the safe and stable operation of the DC microgrid. Simulation results show that compared with existing neural network-based estimators, the proposed physics-informed state estimation model can enhance the accuracy by 7 . 98 % ∼ 12 . 54 % , providing more precise estimated states to mitigate the effects of attacks.},
  archive      = {J_ASOC},
  author       = {Wanwan Ren and Jun Peng and Yun Zhou and Weirong Liu and Fu Jiang},
  doi          = {10.1016/j.asoc.2025.113691},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113691},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A physics-informed mitigation method for DC microgrids under cyber attacks},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acquisition of representative data sets by filtering out redundant objects and attributes with fuzzy preference-based rough sets and dominance principles. <em>ASOC</em>, <em>184</em>, 113690. (<a href='https://doi.org/10.1016/j.asoc.2025.113690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dominance principle is crucial for evaluating consistency in dominance-based rough sets, yet redundant objects or attributes impair decision consistency. While existing work focuses on modeling and attribute reduction, object-induced inconsistency remains understudied. This study proposes a novel methodology for acquiring compact datasets through representative object extraction and attribute reduction in fuzzy preference-based rough sets, with a specific focus on preserving consistency in dominance principles. Firstly, the extent of dominance relations is quantified by fuzzy preference relations, while the evaluation of consistency between conditional and decision attributes is accomplished through distance measures. Subsequently, representative objects are identified by eliminating those with lower consistency in dominance principles, as assessed by distance measures with a predefined parameter. Further, a streamlined dataset is achieved through attribute reductions in fuzzy preference-based rough sets, incorporating representative objects. In conclusion, our proposed method is validated using numerical datasets, and its effectiveness is evaluated through measures rooted in rough set theory, machine learning, and statistics. This research contributes to a more comprehensive understanding of dominance-based rough sets by addressing the often-overlooked issue of inconsistency resulting from redundant objects.},
  archive      = {J_ASOC},
  author       = {Shuyun Yang and Guang Shi and Yuchao Li},
  doi          = {10.1016/j.asoc.2025.113690},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113690},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Acquisition of representative data sets by filtering out redundant objects and attributes with fuzzy preference-based rough sets and dominance principles},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spiking neural network with time-varying weights for rail squat detection. <em>ASOC</em>, <em>184</em>, 113689. (<a href='https://doi.org/10.1016/j.asoc.2025.113689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Axle box acceleration (ABA) measurements can be used for continuously monitoring rail infrastructure and detecting rail surface defects such as squats. However, accurately detecting squats is challenging due to their short-duration responses and low occurrence in ABA signals, particularly for light squats that exhibit subtle ABA responses. To address this challenge, we propose using a spiking neural network (SNN) with time-varying weights to enhance the detection performance of rail squats based on ABA measurements. Our approach employs a simple SNN architecture without hidden layers, trained using a method that combines genetic algorithms, k-fold cross-validation, and multi-start gradient-based approach to optimise hyperparameters and weights. The proposed methodology demonstrates competitive accuracy compared to other state-of-the-art SNN-based methods on UCI benchmarks for both binary and multi-class nonlinear problems. Part of its advantages include higher efficiency with a simpler architecture and training approach that reduces computational times while achieving effective spatiotemporal pattern detection. As shown by real-field measurements from Dutch and Swedish railways in anomaly detection, it effectively captures subtle changes in light squat defect responses in ABA signals and achieves a detection performance of 100% for severe squat defects and over 93% for light squat defects. Furthermore, we show that the spike responses, postsynaptic potentials, and membrane potentials can be used as a new way to explain and analyse the ABA signals. The proposed method using time-varying weights highlights a correspondence with the physical problem and offers an ability to capture sudden and subtle changes in the responses, which is crucial, particularly for detecting defects in their early stages.},
  archive      = {J_ASOC},
  author       = {Wassamon Phusakulkajorn and Jurjen Hendriks and Zili Li and Alfredo Núñez},
  doi          = {10.1016/j.asoc.2025.113689},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113689},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spiking neural network with time-varying weights for rail squat detection},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral–spatial representation progressive learning via segmented attention for 3D skeleton-based motion prediction. <em>ASOC</em>, <em>184</em>, 113688. (<a href='https://doi.org/10.1016/j.asoc.2025.113688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, GCNs-based methods have demonstrated impressive performance in human behavior prediction tasks. We believe that human motion modeling can explained as motion correlation extraction from the combination of the active and static motion parts analysis. However, existing methodologies fail to address the issue that feature information associated with static regions may overshadow feature information from dynamic regions, ultimately affecting the extraction of network features. Moreover, the unique low-pass feature pre-retention processing mechanism of GCN on the spectrum will lead to the attitude of some sequences remaining unchanged during the prediction process and further hurt the prediction. In this paper, we propose a Spectral–Spatial Representation Progressive Learning network to solve the problem above. Firstly, we propose a segmented attention block to compare the input observation sequence with the static contrast standard to obtain the motion region and the rest region. Then, we design the Spectrum Deconstruction Recombination Factor block(SDRF) to extract the global bandpass spectrum of human bone joints. The joint features of different regions are encoded by graph convolution and high-frequency feature filter coding based on geometric algebra. Specifically, a spectral–spatial interaction block is presented in each SDRF, focusing on the diversity of motion sequence frequency domain and spatial domain map, and realizes the fine extraction of historical pose sequence features from the two levels of space and spectral domain. Experimental results demonstrate that our approach outperforms state-of-art algorithms by 2.4%, 5.3% and 4.7% in terms of 3D mean per joint position error on Human 3.6M, CMU Mocap and 3DPW datasets, respectively.},
  archive      = {J_ASOC},
  author       = {Wenming Cao and Jianhua Zhang and Jianqi Zhong},
  doi          = {10.1016/j.asoc.2025.113688},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113688},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spectral–spatial representation progressive learning via segmented attention for 3D skeleton-based motion prediction},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal multi-trip supply chain model aided by smart contract victim tracking — An innovative pathway to disaster management under uncertainty. <em>ASOC</em>, <em>184</em>, 113687. (<a href='https://doi.org/10.1016/j.asoc.2025.113687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the complex challenges of post-earthquake rescue operations, using the recent earthquake in Turkey as a reference case. It focuses on developing an optimized model for scenarios with acute vehicle shortages, aiming to minimize both operational costs and response time during the critical initial phase of disaster relief. The proposed solution is built upon a robust mathematical framework that employs aerial vehicles for post-disaster area assessment, resource allocation, and the relocation of critically injured victims. The model leverages a soft computing approach, integrating the Weighted Sum Method (WSM) and Neutrosophic Compromise Programming (NCP). To enhance decision-making under uncertainty, the framework incorporates hexagonal type-2 fuzzy defuzzification, a technique grounded in soft computing principles. Results demonstrate the effectiveness of this approach: the NCP method achieved a response time of 213 min (3.55 h) and a cost of Rs 821,026.5, compared to 217.5 min (3.62 h) and Rs 820,860.3 for the WSM method—both successfully coordinating the rescue of 1,450 victims through efficient deployment of drones and helicopters. In addition, the study introduces a decentralized Ethereum-based smart contract to securely store and retrieve critical victim information. Validated through rigorous unit testing, the contract ensures data transparency and integrity, executing at a cost of 0.00379246 Ether. This blockchain-enabled feature complements the core optimization model, supporting real-time, tamper-proof data handling. To further validate the model’s applicability, a second real-life numerical example based on the recent Sikkim cloudburst is analyzed. The findings reinforce the model’s adaptability and practical value. The managerial implications of this research highlight the importance of soft computing-driven decision support, proactive contingency planning, and the integration of intelligent technologies in disaster response. This holistic framework — combining soft computing methodologies, advanced optimization models, and blockchain technology — offers an innovative and scalable solution for enhancing the resilience and efficiency of disaster management supply chains.},
  archive      = {J_ASOC},
  author       = {Alisha Roushan and Amrit Das and Anirban Dutta and Uttam Kumar Bera},
  doi          = {10.1016/j.asoc.2025.113687},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113687},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-modal multi-trip supply chain model aided by smart contract victim tracking — An innovative pathway to disaster management under uncertainty},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive learning with hard negatives for sentence embeddings. <em>ASOC</em>, <em>184</em>, 113685. (<a href='https://doi.org/10.1016/j.asoc.2025.113685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised sentence representation learning remains a core challenge in natural language processing. Recent contrastive learning methods have shown strong potential in capturing sentence-level semantics, but their effectiveness is often constrained by the quality of positive and negative samples. In particular, constructing informative hard negatives in textual data is significantly more difficult than in vision tasks due to the ambiguity and compositionality of natural language. We propose HNCSE, a novel unsupervised contrastive learning framework that enhances sentence representations through hard negative compositional strategies. HNCSE introduces two key components: HNCSE-HNM, which synthesizes informative hard negatives via mixup within the batch, and HNCSE-PM, which generates harder positives by leveraging the most challenging negatives. This joint design improves both alignment and discrimination in embedding space without relying on external supervision. Extensive experiments on semantic textual similarity and transfer tasks demonstrate that HNCSE consistently outperforms existing unsupervised and supervised baselines.},
  archive      = {J_ASOC},
  author       = {Wenxiao Liu and Zihong Yang and Chaozhuo Li and Zijin Hong and Jianfeng Ma and Zhiquan Liu and Litian Zhang and Feiran Huang},
  doi          = {10.1016/j.asoc.2025.113685},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113685},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive learning with hard negatives for sentence embeddings},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HumanMoD: A multi-RAG collaborative LLM for inclusive urban public healthcare services. <em>ASOC</em>, <em>184</em>, 113684. (<a href='https://doi.org/10.1016/j.asoc.2025.113684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Large Language Models (LLMs) with healthcare systems offers transformative solutions for sustainable public health programs, especially in underserved urban and rural communities where access to professional medical expertise is limited. This paper introduces HumanMoD, a novel LLM framework designed to emulate collaborative clinical workflows of multi-expert medical doctors. Specifically, the proposed Mixture of Doctors module enables parallel diagnostic streams from diverse medical specialties, mimicking the collaborative decision-making of human doctors to ensure comprehensive assessments in resource-constrained environments. The Knowledge-driven Medical Assistant leverages domain-specific knowledge bases to mitigate LLM hallucinations, ensuring that recommendations are rooted in credible medical knowledge. Exquisitely, the Humanoid Health Conductor and LLM-powered Corrector further refine outputs to minimize diagnostic discrepancies, enhancing the reliability of responses for large-scale public health applications such as community health kiosks and mobile health apps serving remote or low-income areas. Unlike fine-tuned models, HumanMoD operates without parameter adjustment, enabling cost-effective deployment in regions with scarce computational resources, thus bridging the healthcare gap for socially vulnerable groups. Experimental results on MedQA and PubMedQA demonstrate that HumanMoD outperforms state-of-the-art models, highlighting its potential to drive equitable healthcare access and support data-driven public health policies in diverse urban and rural settings.},
  archive      = {J_ASOC},
  author       = {Song Sun and Zhijie Zhong and Nanlan Yu and Xinrong Gong and Kaixiang Yang},
  doi          = {10.1016/j.asoc.2025.113684},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113684},
  shortjournal = {Appl. Soft. Comput.},
  title        = {HumanMoD: A multi-RAG collaborative LLM for inclusive urban public healthcare services},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A KPI-related fault diagnosis method for multimode manufacturing processes based on supervised minimal gated unit and sparse broad learning system. <em>ASOC</em>, <em>184</em>, 113679. (<a href='https://doi.org/10.1016/j.asoc.2025.113679'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multimode manufacturing processes, the key performance indicator (KPI)-related fault diagnosis plays a critical role in ensuring product quality and enhancing economic benefits. However, variability in working modes, lack dynamic description for the KPIs, and model structure redundancy may lead to poor universality and low diagnostic accuracy of conventional deep learning-based approaches. In this work, a supervised minimal gated unit and sparse broad learning system (SMGU-SBLS) is developed for KPI-related fault diagnosis. Specifically, the KPIs and process variables are simultaneously utilized in the SMGU to learn the KPI-related dynamic features. Then, aiming at simplifying the network structure, a sparse version of broad learning system is proposed for fault diagnosis. Furthermore, the expansion capability of SMGU-SBLS has been analyzed. Finally, the proposed SMGU-SBLS network is applied to a real hot strip mill process (HSMP). Simulation results show that the proposed method has higher diagnostic performance than the other four state-of-the-art deep learning methods.},
  archive      = {J_ASOC},
  author       = {Chuanfang Zhang and Wenxiao Yin and Chi Zhang and Kaixiang Peng and Xueyi Zhang},
  doi          = {10.1016/j.asoc.2025.113679},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113679},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A KPI-related fault diagnosis method for multimode manufacturing processes based on supervised minimal gated unit and sparse broad learning system},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Green maritime transport performance analysis of G-7 countries using an interval-valued fermatean fuzzy ARLON-based decision model. <em>ASOC</em>, <em>184</em>, 113677. (<a href='https://doi.org/10.1016/j.asoc.2025.113677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maritime transport operations are witnessing heightened environmental sensitivities, prompting the implementation of environmentally focused maritime transport practices. The principal aim is to develop a decision support system for evaluating countries' green maritime transport performance. To this end, the interval-valued Fermatean fuzzy (IVFF)-simple weight calculation (SIWEC)-skewness impact through distributional evaluation (SITDE)-alternative ranking using two-step logarithmic normalization (ARLON) model is developed within this study. This method enables the analysis of countries' green maritime transport performance by integrating expert opinions with environmental and maritime transport parameters. The model determines the influence of experts using IVFF sets. It further facilitates the simultaneous use of subjective and objective criteria weighting approaches to compute the weights of green maritime transport performance criteria. ARLON is employed to assess and rank countries' green maritime transport performance levels. The applicability of the IVFF-SIWEC-SITDE-ARLON model is tested through a case study focusing on G-7 countries, and the results supported the successful implementation of the method. Furthermore, sensitivity and comparative analyses demonstrated the consistency and robustness of the model. According to the findings of the case study, the United States emerges as the country with the highest green maritime transport performance among the G-7 countries. The "linear shipping connectivity index" is identified as the most influential criterion in the decision-making process. The study offers actionable recommendations for the maritime industry, thereby contributing to the advancement of green maritime transport practices.},
  archive      = {J_ASOC},
  author       = {Galip Cihan Yalçın and Karahan Kara and Emre Kadir Özekenci and Vladimir Simic and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2025.113677},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113677},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Green maritime transport performance analysis of G-7 countries using an interval-valued fermatean fuzzy ARLON-based decision model},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive self-correction network for human motion prediction. <em>ASOC</em>, <em>184</em>, 113676. (<a href='https://doi.org/10.1016/j.asoc.2025.113676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion prediction aims to generate future poses from the observed historical human motion sequence. It is fundamental to many intelligent systems, e.g., human–robot interaction and self-driving. Though the existing encoder–decoder methods obtain good performance in some scenarios, there is still a gap between their prediction results and ground truth in many cases. In this work, we propose to estimate the deviation between the decoding results (which will be referred to as the conventional human motion prediction ) and the groundtruth, and integrate this estimation with the conventional prediction results to derive the corrected human motion prediction . In this way, our method can self-correct the conventional prediction results based on a preliminary estimated deviation from it to the groundtruth, and thus enhance the performance. In our work, we adopt five independent lightweight branches rather than a global estimator to estimate the deviation of the five human body components ( i.e., left arm, right arm, torso, left leg, and right leg). Based on this component-wise deviation estimation strategy, we propose a Fixed Self-Correction Network (FSCNet) for human motion prediction to obtain enhanced performance. Recognizing that not all joints exhibit the same motion dynamics inside one given body component, we further propose the Adaptive Self-Correction Network (ASCNet) to let these five estimators adaptively capture the correlated deviations and thus enhance human motion prediction performance. Extensive experiments on three large datasets (Human3.6M, CMU-Mocap, and 3DPW) validate the superiority of our proposed FSCNet and ASCNet over the established works.},
  archive      = {J_ASOC},
  author       = {Jinkai Li and Jinghua Wang and Xin Wang and Liang Yan and Xiaoling Luo and Yong Xu},
  doi          = {10.1016/j.asoc.2025.113676},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113676},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive self-correction network for human motion prediction},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RGB-D indoor scene parsing via wavelet sub-band guided transformer. <em>ASOC</em>, <em>184</em>, 113675. (<a href='https://doi.org/10.1016/j.asoc.2025.113675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth information has been shown to be complementary to RGB scene parsing. However, inherent disparities between RGB and depth modalities create challenges for effective feature fusion. Furthermore, current methods often lose high-frequency information during downsampling, limiting the full utilization of RGB and depth image information. To address these issues, we propose the Wavelet Sub-band Guided Transformer (WSGFormer), which utilizes wavelet sub-band to enhance feature correction, fusion, and refinement. The WSGFormer contains three important modules. Firstly, the Wavelet Cross-attention Rectification Module employs Haar wavelet transforms to decompose features into wavelet sub-band, and adaptively aligns RGB and depth features by extracting their mapping relationships. Secondly, the Multi-scale Fusion Module combines RGB and depth branches, utilizing vertical bar-shaped convolutions to enable cross-modal feature selection and enhance the sensitivity to high-frequency components through frequency-aware techniques. Finally, the Discrepancy Compensation Module starts with high-level semantic information and progressively guides the fusion of adjacent layers downwards, reducing disparities between them through subtraction operations. The evaluation conducted on the NYUv2, SUN-RGBD and ScanNetV2 datasets highlights the superior performance of the proposed WSGFormer.},
  archive      = {J_ASOC},
  author       = {Wen Xie and Heng Liu and JiaHao Li},
  doi          = {10.1016/j.asoc.2025.113675},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113675},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RGB-D indoor scene parsing via wavelet sub-band guided transformer},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language model driven multiple operating conditions identification and predictive control. <em>ASOC</em>, <em>184</em>, 113674. (<a href='https://doi.org/10.1016/j.asoc.2025.113674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic changes in operating conditions are common in industrial process control. Efficient and accurate detection of condition transitions and identification of operating states are critical for achieving precise control across multiple operating conditions. Traditional condition identification methods primarily rely on the numerical similarity of input–output sequences, often neglecting the dynamic semantic information embedded within the data, such as variation rates and overshoot magnitudes, which can lead to frequent misclassifications. Recently, large language models (LLMs) have exhibited remarkable capabilities in semantic understanding of complex sequences, offering a new perspective for the identification of operating conditions. However, their application in industrial control still faces two major challenges. First, the enormous parameter scale of LLMs results in high computational costs, making it difficult to achieve real time comparisons between online sequences and large historical datasets, thereby compromising the timeliness of condition identification. Second, LLMs are designed primarily for textual input, leading to a significant modality gap when processing numerical input–output sequence data, which limits their full semantic understanding potential in industrial scenarios. To address these challenges, this paper proposes a novel large language model driven multiple operating conditions identification and predictive control method. The proposed method fully leverages the semantic understanding capabilities of LLMs by mining the underlying dynamic characteristics of input–output sequences, enabling rapid identification of operating conditions under limited sample scenarios and achieving precise control across multiple conditions. Specifically, first, to accurately detect condition changes in multiple operating condition industrial processes, an Approximate Entropy based operating condition change detection method is proposed. Considering that condition transitions often cause a prediction model mismatch, leading to increased complexity and irregularity in control sequences, approximate entropy is employed to quantify the sequence complexity. A data driven adaptive thresholding mechanism based on kernel density estimation is further developed to achieve robust detection of condition changes. Second, to address the issues of limited samples and low identification accuracy arising from solely relying on numerical features, a novel LLM driven multimodal condition identification method is proposed. This method constructs a sparse representation based prediction model for historical operating conditions, forming a high information density knowledge base consisting of prediction models and representative sequences. To bridge the modality gap between numerical and textual data, a numerical to textual sequence description method enriched with dynamic semantics is innovatively introduced, enabling effective alignment between the two modalities. Furthermore, a dynamic semantics enhanced prompt engineering strategy is developed to fully exploit the LLM’s semantic understanding capabilities, facilitating accurate condition identification even under limited sample conditions. Finally, to achieve rapid response and precise control following condition changes, the proposed method quickly matches a suitable prediction model based on the LLM driven condition identification results and dynamically adjusts control parameters using a rolling horizon optimization strategy, thereby significantly improving control accuracy across multiple operating conditions. Notably, the proposed approach eliminates the need for reconstructing condition models after changes, ensuring smooth and continuous control under dynamic conditions. Extensive experiments verified the superiority of the proposed method in terms of both condition identification and multiple operating conditions control performance.},
  archive      = {J_ASOC},
  author       = {Zhongyu Zhang and Minzhi Mao and Keke Huang and Dehao Wu and Chunhua Yang},
  doi          = {10.1016/j.asoc.2025.113674},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113674},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Large language model driven multiple operating conditions identification and predictive control},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MetaIndux-PLC: A control logic-guided LLM for PLC code generation in industrial control systems. <em>ASOC</em>, <em>184</em>, 113673. (<a href='https://doi.org/10.1016/j.asoc.2025.113673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programmable Logic Controllers (PLCs) are widely used for automation control, and they are well-suited for industrial systems control tasks. LLMs can assist engineers in streamlining the programming process and reducing development costs, and one of the key issues is the construction of the PLC code dataset. However, the lack of an open-source PLC code dataset in the research community, combined with the high complexity of industrial systems control logic, has caused most LLMs to struggle with generating accurate control code. This complexity arises from the need to manage real-time sensor data fusion, integrate various communication protocols, and ensure compliance with stringent safety and regulatory standards. In this study, we construct ST4Indux, a PLC code dataset specifically for industrial systems control. And we propose the Control Logic-Guided Iterative Fine-Tuning (CLIFT) method, which iteratively optimizes the model’s generation capability. Based on these, we train a large language model named MetaIndux-PLC, to enable the generation of complex motion control code. Additionally, we propose a multi-dimensional evaluation and optimization method to systematically assess the model’s performance in terms of task completion quality, efficiency, and user experience. The experimental results demonstrate that the proposed approach significantly enhances MetaIndux-PLC’s performance and reliability in real-world engineering environments, providing a foundation for the future development of intelligent programming assistance systems.},
  archive      = {J_ASOC},
  author       = {Lei Ren and Haotian Wang and Jiabao Dong and Haiteng Wang and Shuai Liu and Yuanjun Laili and Lin Zhang},
  doi          = {10.1016/j.asoc.2025.113673},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113673},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MetaIndux-PLC: A control logic-guided LLM for PLC code generation in industrial control systems},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage identification of false data injection attacks in power systems via semi-supervised deep learning. <em>ASOC</em>, <em>184</em>, 113672. (<a href='https://doi.org/10.1016/j.asoc.2025.113672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies find that malicious data manipulations against the state estimation, such as the false data injection attack (FDIA), can evade the detection of a conventional bad data detector equipped with a measurement meter. This calls for advanced FDIA identification approaches urgently. However, existing data-driven efforts are designed under the assumption that attacks are frequent and the amount of compromised data is comparable to benign data, which may not be realistic. Thus, these approaches deliver unsatisfactory performance under highly imbalanced data in the real world. To overcome this issue, we propose a novel two-stage FDIA identification pipeline, which formulates the problem as global detection and fine-grained localization. Following this framework, we leverage deep support vector data description to distinguish attacks from benign measurements in an unsupervised manner and employ a modified one-dimensional ResNet to locate the attacking aims upon detecting an FDIA. Our approach can overcome existing limitations induced by data-driven methods under infrequent FDIAs, leading to effective and robust FDIA identification. Case studies on IEEE standard 14-bus and 118-bus systems demonstrate the effectiveness and superiority of our approach and validate our findings.},
  archive      = {J_ASOC},
  author       = {Fengrui Liu and Keng-Weng Lao and Yida Xu and Yang Li and Haotian Guo and Xiaorui Hu and Yikun Yin},
  doi          = {10.1016/j.asoc.2025.113672},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113672},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage identification of false data injection attacks in power systems via semi-supervised deep learning},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic target continuous assignment method for unmanned clusters in fragmented information environments. <em>ASOC</em>, <em>184</em>, 113671. (<a href='https://doi.org/10.1016/j.asoc.2025.113671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target assignment stands as a pivotal resource management technique in collaborative clustering. During far-sea search and rescue (SAR) missions, challenges arise in formation control under fragmented information, stemming from unstable communication and incomplete data. Additionally, the dynamic nature of targets poses continuous assignment challenges. Addressing these issues in cluster collaboration represents a significant challenge. This paper proposes a method for dynamic target successive allocation tailored to fragmented information environments. First, inspired by biological cluster behaviors and integrating graph theory and complex network theory, the approach achieves decentered cluster formation by managing subgroup separation and aggregation. This solution effectively addresses the formation control challenge in fragmented information settings during cluster collaboration. Second, leveraging reinforcement learning principles, the method determines device behavioral strategies based on maximizing the device’s interaction rewards with the environment. This approach resolves the dynamic continuous target assignment challenge within clustered environments. This paper proposes a Broken Info-Driven Target Assignment (BI-DTA) method to address these research challenges. Experimental results demonstrate that the method achieves effective decentered formation control and dynamic continuous target assignment rapidly, exhibiting robustness and stability in fragmented information environments.},
  archive      = {J_ASOC},
  author       = {Rui Ding and Yuhan Zhu and Baojie Chai},
  doi          = {10.1016/j.asoc.2025.113671},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113671},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic target continuous assignment method for unmanned clusters in fragmented information environments},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A surrogate-assisted evolutionary algorithm based on the two-round selection strategy incorporating local search for expensive high-dimensional multi/many-objective optimization. <em>ASOC</em>, <em>184</em>, 113670. (<a href='https://doi.org/10.1016/j.asoc.2025.113670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-assisted multi-objective optimization has exhibited excellent performance for solving optimization problems that involve time-intensive computer simulations or resource-intensive physical experiments. However, the majority of existing research has focused on low-dimensional problems. In this paper, a surrogate-assisted evolutionary algorithm based on the two-round selection strategy incorporating local search (TRLS) is proposed for expensive high-dimensional multi/many-objective optimization. Specifically, the convergence and diversity of the trial solutions are assessed based on the estimation of Pareto fronts, and the uncertainty is quantified by analyzing the distribution of relevant points in the decision space. The infill sampling task is guided by the above three performance indicators. Firstly, a preliminary screening is conducted by considering convergence and diversity performance. Subsequently, the quality of candidates is further refined through the implementation of a local search strategy. Finally, a comprehensive fitness is constructed to select the sampling individual. In addition, a dynamic termination criterion is devised for the surrogate-assisted evolution phase. Empirical studies, conducted on two classical benchmark suites and two real-world tasks, reveal the effectiveness and applicability of the proposed TRLS.},
  archive      = {J_ASOC},
  author       = {Yang Li and Weigang Li and Songtao Li and Qifeng Wang and Junwei Hu},
  doi          = {10.1016/j.asoc.2025.113670},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113670},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A surrogate-assisted evolutionary algorithm based on the two-round selection strategy incorporating local search for expensive high-dimensional multi/many-objective optimization},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study of the hesitant fuzzy aggregation operators for the transportation of perishable goods under real life scenarios. <em>ASOC</em>, <em>184</em>, 113668. (<a href='https://doi.org/10.1016/j.asoc.2025.113668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transportation of the perishable goods has always been a critical challenge in the market. The inherent susceptibility of the perishable items to spoilage and deterioration during transit necessitates the development and implementation of the preservation technologies that enhance the shelf life and maintain the quality of perishable products during transportation. Also, the carbon emissions stemming from activities associated with transportation present a significant and progressively urgent challenge, markedly contributing to environmental degradation. Consequently, the escalating carbon emissions linked to the transportation-related activities pose a threat to the sustainability of the environment. Recalling these facts, this paper explores the study of an interval valued multi-objective fixed charge solid transportation problem under hesitant fuzzy aggregation operators. Furthermore, this paper introduces a practical mathematical framework designed to represent the decision-making process inherited in the transportation scenarios. The proposed method leverages the hesitant fuzzy aggregation operators to offer a specific approach for the decision-making across such operators. Additionally, it introduces the notion of hesitant degrees for different objectives through the utilization of membership functions. Three conflicting objective functions: time minimization for customer satisfaction, profit maximization for the economic sustainability and minimization of the carbon emissions for the environmental sustainability have been addressed in the suggested model. The transportation time and the fuel consumption have been managed by introducing a variable called the vehicle speed coefficient. An analysis of the deterioration rates of the perishable products has been conducted considering the presence of a preservation value. Moreover, the proposed model has been formulated by employing the diverse approaches and have been solved using multi-objective techniques. A real-life-based numerical problem is presented and solved to validate the proposed concept. The results are compared with respect to different vehicle speeds and preservation values.},
  archive      = {J_ASOC},
  author       = {Awdhesh Kumar Bind and Deepika Rani and Ali Ebrahimnejad},
  doi          = {10.1016/j.asoc.2025.113668},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113668},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A study of the hesitant fuzzy aggregation operators for the transportation of perishable goods under real life scenarios},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAGE-net: Single-layer augmented gated encoder network for efficient multimodal sentiment analysis. <em>ASOC</em>, <em>184</em>, 113665. (<a href='https://doi.org/10.1016/j.asoc.2025.113665'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis has achieved remarkable progress. However, the increasing computational complexity of existing models poses significant challenges in resource-constrained scenarios. To address these challenges, this study introduces a single-layer augmented gated encoder network (SAGE-Net), a novel lightweight multimodal sentiment analysis architecture. In contrast to conventional multilayer, deeply stacked architectures, SAGE-Net employs only a single-layer encoder to preserve multimodal feature comprehension, significantly reducing computational complexity. To enable effective inter-modal interaction, a single-layer cross-attention mechanism is integrated. We extensively evaluate multiple feature fusion strategies and data augmentation strategies to enhance model effectiveness. Experimental results on the CMU-MOSI and CMU-MOSEI, and CH-SIMS datasets demonstrate that SAGE-Net achieves competitive performance and significantly lowers model size and tuning costs. These results validate SAGE-Net as a viable lightweight solution for multimodal sentiment analysis in resource-constrained scenarios.},
  archive      = {J_ASOC},
  author       = {Jiazheng Zhou and Xin Kang and Weiping Ding and Linhuang Wang and Fei Ding and Kazuyuki Matsumoto and Chenmeng Zhang and Huiwen Chi},
  doi          = {10.1016/j.asoc.2025.113665},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113665},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SAGE-net: Single-layer augmented gated encoder network for efficient multimodal sentiment analysis},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust weakly supervised product surface defect segmentation based on guided cropping and inpainting extension. <em>ASOC</em>, <em>184</em>, 113661. (<a href='https://doi.org/10.1016/j.asoc.2025.113661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of industrial manufacturing, automatic detection and segmentation of surface defects in products is vital to enhance both product quality and efficiency. However, a large number of existing deep learning methods require a substantial amount of manually labeled data for training, and the high-cost labeling process hampers the practical application of such methods. Towards this end, we present a weakly supervised defect segmentation algorithm without any segment labels. First, a training enhancement method based on a contrastive learning module (CLM) coupled with a guided cropping module (GCM) is proposed to improve the network’s attention to defects in the Defect Focus Classifier (DFC) training phase. Subsequently, a novel inpainting extension module (IEM) generates a final class activation map (CAM) to obtain a pseudo label automatically for segment network training. Finally, conditional random field (CRF) and an additional training round refine the segmentation results. Moreover, the whole process is distilled into a fully supervised segmentation network to improve the inference efficiency. Conducting extensive experimentation, we have achieved 100% and 93.39% average precision (AP) and 41.73% and 53.14% average intersection-over-union (IOU) on the public datasets KolektorSDD and KolektorSDD2, respectively. Furthermore, we verified the generalizability of our method by conducting experiments on several industrial product classes in the MVTec AD, MTD, and DAGM datasets. In these experiments, we achieved favorable classification and segmentation results using solely image classification labels.},
  archive      = {J_ASOC},
  author       = {Rui Yan and Xiaojun Wu and Qixun Yang and Michael Yu Wang},
  doi          = {10.1016/j.asoc.2025.113661},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113661},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust weakly supervised product surface defect segmentation based on guided cropping and inpainting extension},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised lip-tongue segmentation with boundary region contrast sampling. <em>ASOC</em>, <em>184</em>, 113653. (<a href='https://doi.org/10.1016/j.asoc.2025.113653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Traditional Chinese Medicine, the accurate segmentation mask of the tongue and lip is the key of inspection. Although deep learning has made remarkable progress in medical image segmentation, a lot of manual annotations are still required for training. Semi-supervised learning (SSL) is used to reduce annotation work, but its performance often suffers when applied to tongue and lip segmentation, which is because tongue and lip images have noisy background information and unique boundary regions. To alleviate the problem, we propose a semi-supervised framework named Lip-Tongue segmentation with Boundary Region Contrast Sampling (Lip-Tongue-BReCoSample). We first preprocess the data, roughly locating the target and filtering out noisy background information. Then we generate the key boundary regions and sample to carry out contrast learning, which alleviates the problem that SSL cannot make fine modeling of the boundary regions of the target with limited information. After a lot of experiments, our method has achieved good results in SSL, and makes it reach or even exceed the performance of many traditional supervised methods, which can improve MIOU performance from 88.09 to 90.43 (+2.34) in SSL specifically. Our method is also better than the latest large-dataset pre-trained model (e.g., SegGPT). To the best of our knowledge, it is the first application of SSL in tongue and lip semantic segmentation.},
  archive      = {J_ASOC},
  author       = {Tao Jiang and Lechao Zhang and Wang Yuan and Liping Tu and Ji Cui and Xiaojuan Hu and Xin Tan and Lizhuang Ma and Jiatuo Xu},
  doi          = {10.1016/j.asoc.2025.113653},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113653},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semi-supervised lip-tongue segmentation with boundary region contrast sampling},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Short-term multistep wind direction prediction of unmanned sailboats based on OVMD and optimized deep learning model. <em>ASOC</em>, <em>184</em>, 113651. (<a href='https://doi.org/10.1016/j.asoc.2025.113651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term multistep wind direction prediction is crucial for enhancing the sailing performance and operational safety of unmanned sailboats. Existing methods often face challenges due to the complexity, non-stationarity, and diverse frequency characteristics of wind direction data. In this study, a novel approach is proposed that combines optimal variational mode decomposition (OVMD) with optimized deep learning model for wind direction prediction. First, OVMD is applied to decompose the wind data into stable modal signals, effectively reducing the impact of data complexity and non-stationarity on prediction performance. Considering that different subsequences exhibit distinct frequency patterns, five deep learning models, including Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks, are employed to predict each subsequence separately. The most suitable model for each subsequence is selected based on the root mean square error (RMSE) metric. Additionally, hyperparameter optimization is conducted to enhance prediction accuracy, reduce training time, and eliminate the need for subjective parameter settings. Experimental results demonstrate that the proposed method can accurately capture wind direction variations and achieves superior performance compared to baseline models across all evaluation metrics, ensuring high accuracy and stability in short-term multistep wind direction prediction.},
  archive      = {J_ASOC},
  author       = {Zhipeng Shen and Shaoqing Zhang and Yang Yang and Zhaoyang Wu and Haomiao Yu},
  doi          = {10.1016/j.asoc.2025.113651},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113651},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term multistep wind direction prediction of unmanned sailboats based on OVMD and optimized deep learning model},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-inspired text-based recommender system with explanatory capabilities. <em>ASOC</em>, <em>184</em>, 113650. (<a href='https://doi.org/10.1016/j.asoc.2025.113650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are widely used to help users find relevant and personalized items in various domains. However, providing accurate recommendations is not enough to ensure user satisfaction, trust, and engagement. Nowadays, users demand transparency from these systems typically in the form of an explanation of the recommendation given. This paper presents a novel explainable Recommender System designed to generate recommendations from natural language queries while providing model-intrinsic explanations inspired by attention mechanisms. The system adds transparency, interpretability, and new user cold-start capabilities. We evaluate our approach on twelve datasets from diverse domains and languages, demonstrating its effectiveness and robustness. Results show that our proposal achieves competitive accuracy with respect to strong baselines, while consistently outperforming a prior interpretable model developed for the same task.},
  archive      = {J_ASOC},
  author       = {Pablo Pérez-Núñez and Paul Buitelaar and Jorge Díez and Oscar Luaces and Antonio Bahamonde},
  doi          = {10.1016/j.asoc.2025.113650},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113650},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-inspired text-based recommender system with explanatory capabilities},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MHGCL: A multi-modal hypergraph contrastive learning framework for molecular property prediction. <em>ASOC</em>, <em>184</em>, 113645. (<a href='https://doi.org/10.1016/j.asoc.2025.113645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate molecular property prediction (MPP) is pivotal in drug discovery. Although current models integrate multi-modal (1D, 2D, and 3D) molecular features, they often suboptimally leverage pharmacophoric information and inadequately capture higher-order intramolecular relationships, such as conjugated systems. This study introduces a novel multi-modal hypergraph contrastive learning framework (MHGCL) to generate enriched molecular representations for enhanced MPP. MHGCL uniquely employs hypergraphs to model complex, many-to-many interactions within molecules. It incorporates a dual-channel architecture, featuring a hypergraph transformer and an equivariant graph neural network, to distinctly process 2D and 3D molecular information. Crucially, functional group and chemical element-oriented knowledge graphs are integrated to explicitly imbue the model with pharmacophoric knowledge. The contrastive learning strategy effectively aligns these diverse representations. Extensive experiments across ten benchmark datasets demonstrate that MHGCL consistently outperforms existing state-of-the-art methods. Further ablation studies confirm that the proposed hypergraph-based molecular representation captures structural motifs of molecular functional groups more effectively, thereby affirming the design efficacy of the model’s constituent modules.},
  archive      = {J_ASOC},
  author       = {Rui Han and Qun Liu and Xu Gong and Guoyin Wang and Li Liu and Xingping Xian},
  doi          = {10.1016/j.asoc.2025.113645},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113645},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MHGCL: A multi-modal hypergraph contrastive learning framework for molecular property prediction},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-hop shapley-based framework for graph convolutional network node classification explanation. <em>ASOC</em>, <em>184</em>, 113615. (<a href='https://doi.org/10.1016/j.asoc.2025.113615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have recently demonstrated superior performance across various graph machine learning tasks. However, they are often regarded as “black-box” models, which has sparked significant interest in developing methods to interpret their predictions. Among various GCN explanation techniques, game-theoretic Shapley value approaches stand out for their ability to identify important nodes, edges, and features. Nonetheless, most Shapley-based explanation models tend to concentrate on dependencies within a fixed radius, resulting in a constrained perceptual field. Additionally, existing explanation methods primarily focus on the total marginal contributions of either very small or very large coalitions during sampling, which limits their effectiveness in capturing the joint marginal contributions inherent in mid-sized subgraphs. To address these challenges, we propose a novel Shapley-based GCN explanation model called MixHopShap, which incorporates multi-hop information and a balanced sampling strategy. MixHopShap employs a multi-hop computational graph construction process to generate an explanation domain, enabling it to capture both local and long-range dependencies. Moreover, MixHopShap introduces a new sampling strategy that promotes balanced coalition coverage, allowing for efficient sampling of mid-sized subgraphs and facilitating learning of marginal contributions for each edge. We conduct experiments on six real world datasets to evaluate the performance of MixHopShap in terms of Fidelity and ROAR metrics. The experimental results demonstrate the superiority of MixHopShap over state-of-the-art methods. Additionally, the explainability of MixHopShap can significantly improve the confidence of GCN’s prediction in node classification tasks.},
  archive      = {J_ASOC},
  author       = {Yifan Zheng and Xibei Yang and Qiguo Sun and Keyu Liu and Qihang Guo},
  doi          = {10.1016/j.asoc.2025.113615},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113615},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-hop shapley-based framework for graph convolutional network node classification explanation},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural network-based interactive clustering enhanced by human knowledge. <em>ASOC</em>, <em>184</em>, 113595. (<a href='https://doi.org/10.1016/j.asoc.2025.113595'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering techniques face persistent challenges in balancing automation with human interpretability. Traditional methods require laborious parameter tuning and domain expertise to define similarity measures and validate results, while deep learning approaches trade transparency for performance. To bridge this gap, we propose a human-in-the-loop framework that synergizes domain knowledge with graph-based semi-supervised learning. Our system enables users to iteratively refine clusters through intuitive visual adjustments on a subset of data, guided by real-time quality metrics to reduce errors and decision fatigue. These sparse annotations propagate to unlabeled instances via a graph neural network (GNN) that models latent relationships through modularity-driven structural learning. By translating cluster adjustments into semi-supervised classification tasks, our method eliminates manual feature engineering and scales to large datasets without retraining. Evaluations on two subsets of the MNIST dataset demonstrated that the NMI (Normalized Mutual Information) of our method improved by 50.44% and 64.77% relative to baseline clustering method, respectively.},
  archive      = {J_ASOC},
  author       = {Yunzhe Wang and Yushi Li and Qiming Fu and Chengtao Ji and You Lu and Jianping Chen},
  doi          = {10.1016/j.asoc.2025.113595},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113595},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph neural network-based interactive clustering enhanced by human knowledge},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced deep learning approaches for fault detection in solar PV systems: A comparative study of SPDA and AIFD-SolDL. <em>ASOC</em>, <em>184</em>, 113592. (<a href='https://doi.org/10.1016/j.asoc.2025.113592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient operation of solar photovoltaic (PV) systems is critical for maximizing power generation and ensuring optimal energy conversion. However, faults in PV modules can significantly impact system performance and reduce energy output. Therefore, accurate identification and diagnosis of these malfunctions are essential. To address the challenge of fault detection in solar PV systems, this study presents two distinct approaches. The first, called Solar Panel Degradation Assessment (SPDA), evaluates faults in solar panels by analyzing degradation effects while considering environmental factors like radiation and temperature. The second approach, named Efficient Ensemble Deep Learning Model for Enhancing Fault Detection in Solar PV Systems (AIFD-SolDL), utilizes advanced deep learning techniques, including DenseNet201, Inception-ResNet-v2, and Inception-v3, to enhance fault detection accuracy. In the AIFD-SolDL approach, PV module data undergo deep feature extraction followed by dimensionality reduction using principal component analysis (PCA). The reduced feature set is then used to train classifiers such as support vector machines (SVM), Gaussian Naive Bayes (GaussianNB), and random forests (RF) to differentiate between normal and fault conditions. Performance metrics, including precision, accuracy, recall, and F1-score, are computed for each combination of feature extractor and classifier. Extensive experiments with both the Solar Panel Images Dataset and the Infrared Solar Module Dataset show that the proposed approaches outperform state-of-the-art methods. For instance, the AIFD-SolDL approach utilizing SVM achieved perfect accuracy, precision, recall, and F1-score of 100% on the Solar Panel Images Dataset. Overall, the SPDA approach effectively detects faults, while deep learning techniques demonstrate high accuracy in fault classification, thereby enhancing the reliability of PV system maintenance and optimization.},
  archive      = {J_ASOC},
  author       = {Mohamed R. Shoaib and Heba M. Emara and Jun Zhao and Milad Taleby Ahvanooey and Essam Nabil},
  doi          = {10.1016/j.asoc.2025.113592},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113592},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advanced deep learning approaches for fault detection in solar PV systems: A comparative study of SPDA and AIFD-SolDL},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neuroevolution-based multiobjective algorithm for feature selection and binary classification of DNA microarrays. <em>ASOC</em>, <em>184</em>, 113520. (<a href='https://doi.org/10.1016/j.asoc.2025.113520'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The genomes of organisms have been sequenced for many years, leading to the discovery of thousands of genes. DNA microarrays are widely used tools for simultaneously analysing numerous genes, commonly employed in detecting and identifying various diseases, including cancer. However, microarray datasets have non-relevant and redundant information, hindering their analysis. This problem is further exacerbated considering the datasets’ high dimensionality and imbalanced classes. Consequently, standard practice involves incorporating a feature selection process to identify the most relevant genes and their associations with diseases. Various methods have been employed to address this task. However, none have taken a more holistic approach that effectively handles feature selection, automatically identifies the optimal classifier configuration, and manages potential conflicting objectives simultaneously. In response, this study introduces the S -metric selection - multiobjective neuroevolution of augmenting topologies (SMS-MONEAT) algorithm, which combines the multiobjective optimisation framework from S -metric selection - evolutionary multiobjective algorithm (SMS-EMOA) and the evolutionary operators from the neuroevolution algorithm N3O, a variation from NEAT which stands for ‘3 new operators’. SMS-MONEAT algorithm was designed to perform both feature selection and optimise the configuration of artificial neural networks for classification tasks. SMS-MONEAT was evaluated against classic and state-of-the-art methods for feature selection and microarray classification. The experiments were conducted on 20 highly challenging cancer-type datasets primarily sourced from the Curated Microarray Database, and the results were investigated for statistical significance. The findings suggest that SMS-MONEAT either outperforms or achieves competitive results in terms of classification compared to the mentioned methods, while at the same time, it selects a smaller subset of features.},
  archive      = {J_ASOC},
  author       = {Daniel García-Núñez and Katya Rodrígez-Vázquez and Carlos Hernández and Edgar Galván},
  doi          = {10.1016/j.asoc.2025.113520},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {113520},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neuroevolution-based multiobjective algorithm for feature selection and binary classification of DNA microarrays},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based bangla text normalization with emotion classification for expressive text-to-speech synthesizer. <em>ASOC</em>, <em>184</em>, 112899. (<a href='https://doi.org/10.1016/j.asoc.2025.112899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method for text normalization and emotion classification specifically designed for the preprocessing steps of Expressive Text-to-Speech (ETTS) synthesizers. Text normalization, which converts non-standard words into standardized forms, is essential for generating clear and coherent output in ETTS synthesizers. Previous studies have encountered difficulties with Bangla homographic words, prompting the development of our proposed method. Our approach begins with the creation of a tokenized and categorized dataset consisting of 26 unique semiotic classes, utilizing a rule-based method with regular expressions. This dataset is derived from the Bangla Text Normalization Corpus (BTN Corpus), which contains 2 million Bangla sentences. Initially, Bangla written texts are tokenized, and each token is classified using a Temporal Convolutional Network (TCN) algorithm trained on the BTN Corpus to identify its semiotic class. This classification aids in generating normalized text, where the resulting tokens are reassembled into coherent sentences for final output. In addition to text normalization, we implement emotion classification for each normalized sentence using a Hierarchical Attention Network (HAN) model. The HAN model was trained on 67,277 normalized texts from the Bangla Normalized Emotion Text Corpus (BNET Corpus), categorizing each text into one of six emotion classes through a rule-based method with regular expressions. The proposed method demonstrates high accuracy rates, achieving a token classification accuracy of 99.977 % with the TCN and an emotion classification accuracy of 99.735 % with the HAN model.},
  archive      = {J_ASOC},
  author       = {Md. Rezaul Islam and Mohammad Shahidur Rahman},
  doi          = {10.1016/j.asoc.2025.112899},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112899},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning-based bangla text normalization with emotion classification for expressive text-to-speech synthesizer},
  volume       = {184},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GhostNet and pair-wise similarity module for cross-domain few-shot classification of hyperspectral images. <em>ASOC</em>, <em>183</em>, 113717. (<a href='https://doi.org/10.1016/j.asoc.2025.113717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the labeling quality of existing hyperspectral datasets is uneven, and it is challenging to acquire labeled samples, which often leads to cross-domain problems and few-shot problems. At the same time, the existing cross-domain few-shot methods generally have some defects in feature extraction, embedding feature processing, cross-domain methods, or other aspects, resulting in low classification accuracy, poor generalization performance, insufficient robustness, and other problems. To address the above issues, a novel cross-domain few-shot learning method is proposed by this paper. First, an improved feature extractor based on GhostNet is introduced, which shows excellent learning ability with only a few parameters and significantly improves the quality of feature representation. Then, the adaptive subspace is used for feature processing to improve the utilization of embedding features, and the Pair-wise Similarity Module (PSM) is combined to augment the discriminative region of the support set which is achieved by examining the similarity between the support set and the query set. Finally, an improved memory bank module is used to retain the key features of both the source domain and the target domain to assist cross-domain alignment, which significantly enhances the effect of domain adaptation. Experimental results from four publicly available datasets demonstrate that the proposed Cross-domain Few-shot Learning method based on GhostNet and PSM Subspace (GPS-CFSL) can enhance the cross-domain few-shot classification accuracy of hyperspectral images effectively while ensuring good generalization and robustness.},
  archive      = {J_ASOC},
  author       = {Caihong Mu and Fugui Zhang and Jiajie Feng and Mosa Haidarh and Yi Liu},
  doi          = {10.1016/j.asoc.2025.113717},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113717},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GhostNet and pair-wise similarity module for cross-domain few-shot classification of hyperspectral images},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating generative artificial intelligence products using fuzzy social network multi-attribute decision-making model: User perspective. <em>ASOC</em>, <em>183</em>, 113715. (<a href='https://doi.org/10.1016/j.asoc.2025.113715'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of Generative Artificial Intelligence (GAI) has transformed various industries, giving rise to an array of GAI products. However, inconsistent product quality complicates user decisions, jeopardizing both the sustainability of GAI technologies and their broader adoption. To address these challenges, this study proposes a user-centered evaluation framework that integrates fuzzy social networks with advanced multi-attribute decision-making (MADM) approaches. Grounded theory is first employed to establish a ''marketing-information-product-individual'' system of factors influencing GAI product adoption. Next, fuzzy social networks reduce semantic ambiguity and mitigate expert bias, while the Decision-Making Trial and Evaluation Laboratory (DEMATEL) method uncovers causal relationships among these factors. The DEMATEL-based Analytic Network Process (DANP) then quantifies the relative importance of each factor, followed by a modified VlseKriterijumska Optimizacija I Kompromisno Resenje (VIKOR) method to comprehensively evaluate representative GAI products. The findings reveal that product maturity exerts the strongest driving force, whereas perceived effect experiences the highest overall impact. Information reliability and the individual dimension carry the greatest weights. Moreover, current products display notable deficiencies in risk management, user service, and ease of use, all of which warrant developers' attention to enhance user satisfaction and adoption. In light of these results, the study proposes targeted optimization strategies for four distinct GAI products. By integrating fuzzy social networks and MADM methodologies, this framework offers a rigorous, systematic evaluation tool that significantly improves decision-making accuracy and promotes the sustainable development of GAI applications.},
  archive      = {J_ASOC},
  author       = {Minglong Han and Yupeng Liu},
  doi          = {10.1016/j.asoc.2025.113715},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113715},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating generative artificial intelligence products using fuzzy social network multi-attribute decision-making model: User perspective},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A synergistic expert-machine bayesian belief network approach for innovation strategy selection. <em>ASOC</em>, <em>183</em>, 113714. (<a href='https://doi.org/10.1016/j.asoc.2025.113714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entrepreneurs often face challenges related to incomplete information and subjective biases when formulating innovation strategies to enhance firm performance through innovation. Striking a delicate balance between the returns and risks inherent in innovation strategies, and adapting to dynamic market forces is crucial for success. However, the strategic formulation methods proposed by current research exhibit a relatively high degree of subjectivity and insufficiently consider the resources possessed by firms. Recognizing this, this study introduces an expert-machine integrated decision support approach that utilizes a robust Bayesian Belief Network (BBN) model to assist decision-makers in managing the complexities of innovation strategy formulation, thereby enhancing firm competitiveness in uncertain business environments. Drawing from theoretical research of existing literature and integrating expert interviews, the structure of the BBN model was developed using the Fuzzy-DEMATEL approach. The parameters of the BBN model were then learned through the EM algorithm, employing 23,107 panel data from Chinese listed firms spanning from 2010 to 2020. This study proposes a data-driven decision support approach with predictive and diagnostic functionalities to assist decision-makers in formulating innovation strategies aimed at enhancing firm competitiveness. The predictive function identifies that difficult-to-imitate resources are more valuable in formulating firms’ innovation strategies. Meanwhile, the diagnostic function suggests that listed private industrial firms are better suited to adopt an exploitative innovation strategy for initial financial accumulation and exploratory innovation to establish market competitiveness. Emerging economies can achieve financial accumulation through technological innovation while also emphasizing the cultivation of innovative thinking for sustainable economic growth.},
  archive      = {J_ASOC},
  author       = {Hongjuan Wu and Shuai Feng and Kaijian Li and Taozhi Zhuang and Ming Luo},
  doi          = {10.1016/j.asoc.2025.113714},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113714},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A synergistic expert-machine bayesian belief network approach for innovation strategy selection},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A feature selection framework using an elitist quasi-opposition self-adaptive partial reinforcement optimization algorithm for breast cancer detection in mammograms. <em>ASOC</em>, <em>183</em>, 113713. (<a href='https://doi.org/10.1016/j.asoc.2025.113713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is one of the most lethal diseases that severely affects women and has a high fatality rate. Radiologists interpret mammography images to promptly diagnose breast cancer. Radiologists examine mammograms to find suspicious regions of the breast. Identifying the breast lumps, changes, or asymmetry in the mammogram is one of the challenging tasks. In this study, we introduce a two-stage breast cancer diagnosis framework to classify breast mass using mammograms. Initially, the suggested model extracts deep features from the input mammograms. After that, we employ an elitist quasi-opposition self-adaptive partial reinforcement optimizer to identify the significant features. Finally, we construct the classification model with an optimal feature subset to categorize the mammogram as a benign or malignant tumor. We examine the suggested system using four benchmark mammogram datasets. We compare the suggested scheme with seven state-of-the-art rival schemes. The final findings highlight that the suggested model outperforms other state-of-the-art methods in terms of success rate, attaining an average success rate of 95%. At a 5% significance threshold, the Nemenyi post-test findings highlight a notable disparity in performance between the proposed scheme and the compared alternatives.},
  archive      = {J_ASOC},
  author       = {Karpagalingam Thirumoorthy and Jerold John Britto and Rajendra Raj Venitta Raj},
  doi          = {10.1016/j.asoc.2025.113713},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113713},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A feature selection framework using an elitist quasi-opposition self-adaptive partial reinforcement optimization algorithm for breast cancer detection in mammograms},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain commercial site selection from changsha to wuhan in china using transfer learning. <em>ASOC</em>, <em>183</em>, 113709. (<a href='https://doi.org/10.1016/j.asoc.2025.113709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning-based research on commercial site selection involved constructing multidimensional geographical features for training according to the distribution of existing stores and performing probability prediction to determine the candidate locations of potential stores. However, this model fails when a new store opens in a city without sufficient labels. To address this issue, we propose a generalized cross-domain site selection transfer learning framework that predicts site selection without training labels in the target domain. Using the Modern China Tea Shop (MCTS) as the research object, we used the Joint Distribution Adaptation (JDA) transfer learning method to synchronously adapt the marginal and conditional distribution of the source (Changsha, China) and target (Wuhan, China) domain by integrating multisource geospatial data. We achieved good transfer results in Wuhan, which lacks labels. The accuracy of Random Forest, Support Vector Machine, Extreme Gradient Boosting, and Multilayer Perceptron classifiers on the target domain test data was improved by 12.00 %, 7.73 %, 6.76 %, and 4.44 %, respectively, through the JDA algorithm iteration. Furthermore, the JDA method outperformed the Domain Adversarial Neural Networks approach, with its superiority being further corroborated through empirical validation in Zhangjiajie. Our analytical framework and empirical results can also be used to evaluate and plan specific locations for different store brands in different cities.},
  archive      = {J_ASOC},
  author       = {Qin Huang and Yu Ma and Manchun Li and Song Hua and Chen Zhou},
  doi          = {10.1016/j.asoc.2025.113709},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113709},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-domain commercial site selection from changsha to wuhan in china using transfer learning},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-guided multi-objective operation optimization strategy for grinding process considering uncertain disturbances. <em>ASOC</em>, <em>183</em>, 113702. (<a href='https://doi.org/10.1016/j.asoc.2025.113702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The grinding process faces significant challenges in dynamic multi-objective optimization due to multi-source uncertain disturbances (e.g., raw material fluctuations, equipment wear) and the conflict between optimization objectives (energy consumption, output, product quality). Existing methods struggle with static modeling assumptions, inefficient utilization of historical operational knowledge, and slow convergence under dynamic conditions. To address these issues, this paper proposes a knowledge-guided dynamic multi-objective optimization strategy (KG-MOOS). The core innovation lies in integrating historical knowledge and real-time disturbance modeling into a dynamic optimization framework. First, an Apriori-based association rule mining framework extracts reusable process control knowledge from historical data, constructing a dynamic knowledge base. Subsequently, a Bi-LSTM network models the temporal coupling effects of disturbances and generates low-dimensional dynamic state features. Finally, a modified Dynamic Multi-Objective Particle Swarm Algorithm (DMPSO) integrates the extracted rules as soft constraints and initial population guidance to achieve fast Pareto frontier tracking. Validation against actual data from a cement grinding plant and uncertainty injection in Aspen Plus simulation shows that the KG-MOOS optimization solution significantly reduces energy consumption and equipment vibration and improves productivity compared to manual operation and other optimization methods, while maintaining real-time performance.},
  archive      = {J_ASOC},
  author       = {Mingrui Zhu and Yangjian Ji and Tao Peng and Linjin Sun},
  doi          = {10.1016/j.asoc.2025.113702},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113702},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge-guided multi-objective operation optimization strategy for grinding process considering uncertain disturbances},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification-wise and cluster-wise contrastive learning for graph neural recommendation. <em>ASOC</em>, <em>183</em>, 113701. (<a href='https://doi.org/10.1016/j.asoc.2025.113701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem statement: Graph neural network-based recommendations mine user preferences by leveraging the structural connections among users, among items, and the interactive linkages between users and items. With the structural connections needed to identify neighbors of users and items, existing works introduce contrastive learning to explore the disparities between neighboring and non-neighboring entities, thereby enriching the representations of users and items. However, they did not distinguish between the personalized behavioral patterns of different genders, ages, or occupations. Methodology: To address this issue, we propose a CCCL: Classification-wise and Cluster-wise Contrastive Learning for graph neural recommendation. First, we incorporate demographic information to derive personalized semantic neighbors via classification. To generate comparison pairs for contrastive learning, the personalized semantic neighbors of the target user are used as positive samples, while the non-neighbors are employed as negative ones. Second, we incorporate the representation of users and items to derive common semantic neighbors via clustering. To generate comparison pairs for contrastive learning, the center of the cluster containing the target user or item is used as a positive sample, while the other centers are used as negative samples. Third, we introduce a bipartite graph of the user-item interactive data to derive structural neighbors via multiple-hop walk. To generate comparison pairs for contrastive learning, the structural neighbors of the target user are used as positive samples. Finally, a new loss function for graph neural recommendation is designed, which includes the three types of contrastive learning mentioned above. Results: The experiments are compared to ten state-of-the-art algorithms on three real-world datasets. Results show that the proposed algorithm performs well in terms of four metrics. Implications: We propose a personalized semantic neighbor to broaden the meaning of neighbors. Personalized semantic neighbors are derived by classifying demographic data. Common semantic neighbors are derived by clustering user and item representations.},
  archive      = {J_ASOC},
  author       = {Heng-Ru Zhang and Xin-Yu Liu and Yuan-Yuan Xu and Yi-Fan Yu and Fan Min},
  doi          = {10.1016/j.asoc.2025.113701},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113701},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification-wise and cluster-wise contrastive learning for graph neural recommendation},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Query-efficient hard-label black-box attack against vision transformers. <em>ASOC</em>, <em>183</em>, 113686. (<a href='https://doi.org/10.1016/j.asoc.2025.113686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have revealed that vision transformers (ViTs) face similar security risks from adversarial attacks as deep convolutional neural networks (CNNs). However, directly applying attack methodology on CNNs to ViTs has been demonstrated to be ineffective since the ViTs typically work on patch-wise encoding. This article explores the vulnerability of ViTs against adversarial attacks under a black-box scenario, and proposes a novel query-efficient hard-label adversarial attack method called AdvViT. Specifically, considering that ViTs are highly sensitive to patch modification, we propose to optimize the adversarial perturbation on the individual patches. To reduce the dimension of perturbation search space, we modify only a handful of low-frequency components of each patch. Moreover, we design a weight mask matrix for all patches to further optimize the perturbation on different regions of a whole image. We test six mainstream ViT backbones on the ImageNet-1k dataset. Experimental results show that compared with the state-of-the-art attacks on CNNs, our AdvViT achieves much lower L 2 -norm distortion under the same query budget, sufficiently validating the vulnerability of ViTs against adversarial attacks. The code is available at https://github.com/GZHU-DVL/AdvViT .},
  archive      = {J_ASOC},
  author       = {Chao Zhou and Xiaowen Shi and Yuan-Gen Wang},
  doi          = {10.1016/j.asoc.2025.113686},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113686},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Query-efficient hard-label black-box attack against vision transformers},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid-domain parallel attention and collaborative interaction transformer for multi-focus image fusion. <em>ASOC</em>, <em>183</em>, 113683. (<a href='https://doi.org/10.1016/j.asoc.2025.113683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion aims to merge multiple source images with different focal positions into a single all-focus image that contains complete information, thereby achieving an extended depth of field effect. This paper proposes a hybrid domain parallel attention and collaborative interaction transformer architecture based on deep learning, which effectively addresses the information loss problem caused by information bottlenecks in the fusion layer of traditional methods through a two-stage reconstruction decoder structure. To enhance the deep interaction of features between source images, a collaborative interaction module is designed, which combines the powerful feature correlation capabilities of depthwise separable convolution and cross-attention, significantly broadening the network model’s global perspective on image information. Experimental results demonstrate that this method exhibits excellent fusion performance across multiple datasets, and both qualitative and quantitative analyses show that it outperforms comparative methods.},
  archive      = {J_ASOC},
  author       = {Hao Zhai and Lianhua Chen and Bo Lin and Minyu Deng and You Yang},
  doi          = {10.1016/j.asoc.2025.113683},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113683},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid-domain parallel attention and collaborative interaction transformer for multi-focus image fusion},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving explainable AI in attributing hydrological responses to climate variabilities in snow-dominated watersheds. <em>ASOC</em>, <em>183</em>, 113682. (<a href='https://doi.org/10.1016/j.asoc.2025.113682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explaining the decision-making of machine learning (ML) models, known as interpretation, connects data-driven results to real-world hydrological processes, representing the next major challenge in ML applications for attribution, beyond accurate simulation. To improve ML interpretability in watershed-scale hydrological attribution, this study develops an eXplainable Artificial Intelligence (XAI) framework that incorporates a novel interpretation algorithm, Lagrange Multipliers-Support Vectors (L-SV), within a feature-based, multi-criteria constraint ML framework termed Climate Feature-Bootstrapped Support Vector Regression (CF-BootSVR). SVR simulations have been conducted in two snow-dominated watersheds, showing satisfactory simulation accuracy (average R² and NSE ≥ 0.88). The aggregated features enhance model interpretability with physically meaningful inputs and reduce computational costs by up to 30 times. The multi-criteria-layer design improves robustness and generalizability (declines in R² and NSE ≤ 0.11) while reducing uncertainties compared to single-run models. L-SV ranks feature importance similarly to model-agnostic algorithms, Permutation Feature Importance (Perm) and SHapley Additive exPlanations (SHAP), particularly in identifying the most sensitive features. L-SV also provides additional directions for feature contribution and enhances computational efficiency, being over 2513 and 2023 times faster than SHAP in the watersheds of Greata and 240, respectively. Furthermore, from a physical-based perspective, the XAI-derived attributions align with general hydrological expertise. Consequently, we conclude that CF-BootSVR offers an efficient approach to enhance predictive capabilities and deepen our understanding of climate-runoff relationships. Beyond hydrology, this CF-BootSVR framework establishes a generalizable paradigm for addressing issues related to climate seasonality. Moreover, L-SV demonstrates significant potential for broader applications in interpreting SVR models across diverse research domains.},
  archive      = {J_ASOC},
  author       = {Jinyu Hui and Xiaohua Wei and Yiping Hou},
  doi          = {10.1016/j.asoc.2025.113682},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113682},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving explainable AI in attributing hydrological responses to climate variabilities in snow-dominated watersheds},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel convolutional and transformer encoder method for cancer related t cell receptor sequences prediction. <em>ASOC</em>, <em>183</em>, 113681. (<a href='https://doi.org/10.1016/j.asoc.2025.113681'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of cancer related T cell receptor sequences (TCR-seq) is of great significance for early diagnosis, treatment guidance. We propose a parallel convolutional and transformer encoder (PCTE) approach for predicting cancer related TCR-seq. This method includes TCR-seq preprocessing techniques and a deep architecture that combines parallel convolutional neural network (CNN) and Transformer encoders. First, we use word vectors to represent amino acids, which enhances feature extraction compared to other methods. The deep architecture employs a parallel Transformer encoder with different attention heads to capture multi-dimensional TCR-seq features, and the integration with CNN further strengthens feature extraction. Additionally, we trim unnecessary amino acids from the TCR-seq and address the issue of low dataset utilization by padding the sequences to a uniform length. The low utilization refers to previous methods, which divide the dataset into multiple smaller subsets, weakening the deep architecture’s generalization ability. Finally, through interpretable analysis of PCTE, we find that glutamate, proline, and tyrosine play a key role in identifying cancer related TCR-seq. In the experiment, we validate and discuss the effectiveness of the PCTE preprocessing method, the impact of trimming unnecessary amino acids on PCTE, and the roles played by the various components of PCTE. Our deep architecture achieves an average AUC of 0.86, an average accuracy of 0.803, an average AUPRC of 0.828, and an average F1 Score of 0.718 on the test dataset, outperforming other most advanced methods.},
  archive      = {J_ASOC},
  author       = {Junjiang Liu and Shusen Zhou and Mujun Zang and Chanjuan Liu and Tong Liu and Qingjun Wang},
  doi          = {10.1016/j.asoc.2025.113681},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113681},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parallel convolutional and transformer encoder method for cancer related t cell receptor sequences prediction},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling the knowledge spillover risk in high-tech industry supply chain: A multi-criteria classification assessment method. <em>ASOC</em>, <em>183</em>, 113680. (<a href='https://doi.org/10.1016/j.asoc.2025.113680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous sweep of globalization, the competition stage of high-tech industry has expanded from the single domestic market to the global market. This change has a profound impact on the construction and operation of the supply chain, and makes the supply chain of high-tech industry face greater risk of knowledge spillover. Therefore, this paper proposes an improved interval Pythagorean fuzzy multi-criteria classification assessment method to classify and assess the knowledge spillover risk in the supply chain of high-tech industry. Firstly, the weights of experts are calculated through social network analysis, and the Pythagorean intuitive fuzzy assessment matrix for each expert is established according to the standard value table of language assessment. The best and worst indexes are determined according to the scores of each index, the weight of each index is calculated by BWM method, and the approximate measure based on generalized possibility is used to sort. Then, the TOPSIS-Sort-L algorithm is improved to effectively classify the candidate solutions in the presence of feature contour information. At the same time, hierarchical clustering and K-means clustering methods based on different distance measures and proximity degrees are proposed. This study provides a systematic supply chain knowledge spillover risk assessment model for high-tech industry, effectively assesses the potential knowledge spillover risk in its supply chain, and helps to improve its competitive advantage in global competition.},
  archive      = {J_ASOC},
  author       = {Jiafu Su and Yijun Chen and Hongyu Liu and Na Zhang and Dongrong Wu},
  doi          = {10.1016/j.asoc.2025.113680},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113680},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unveiling the knowledge spillover risk in high-tech industry supply chain: A multi-criteria classification assessment method},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Particle packing theory-guided multi-fidelity deep learning for discovering low-carbon cost-effective high-performance concrete. <em>ASOC</em>, <em>183</em>, 113678. (<a href='https://doi.org/10.1016/j.asoc.2025.113678'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven methods for concrete design offer time and cost efficiencies but lack integration of concrete-specific knowledge, compromising model reliability and generalizability. This paper presents a framework to integrate data and concrete theories into the design of low-carbon, cost-effective, high-performance concrete. This research has three primary novelties: (1) Morphology of concrete ingredients, such as solid wastes, which largely affect the fresh and hardened properties of concrete, is considered based on a particle packing theory. (2) Data scarce challenge is addressed using a multi-fidelity strategy, which integrates a Compressible Packing Model, Discrete Element Model, and experimental data. (3) Compliance with concrete theories is enforced using a Physics-Informed Neural Network. The proposed approach achieves prediction accuracy of 98 %, and reduces cost and carbon emission by 29 % and 50 %, respectively, compared with traditional methods. These results demonstrate the framework’s potential to accelerate sustainable concrete design in real-world applications.},
  archive      = {J_ASOC},
  author       = {Boyuan Cheng and Yi Bao and Weina Meng and Liu Mei and Wu-Jian Long},
  doi          = {10.1016/j.asoc.2025.113678},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113678},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Particle packing theory-guided multi-fidelity deep learning for discovering low-carbon cost-effective high-performance concrete},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A state information driven evolutionary algorithm considering dimension influences for many-objective optimization. <em>ASOC</em>, <em>183</em>, 113669. (<a href='https://doi.org/10.1016/j.asoc.2025.113669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many-objective evolutionary algorithms (MaOEAs) have been studied widely in recent years. However, most of them do not utilize the state information of the evolution process to drive the population evolution, while ignoring the dimension influences on the algorithm performance. These make most MaOEAs still cannot balance convergence and diversity of the population well. For this purpose, a state information driven evolutionary algorithm considering dimension influences for many-objective optimization (SIEA) is proposed. In SIEA, an adaptive collaboration mechanism based on angle selection and multi-criterion deletion is developed, which aims at deleting individuals with poor performance one by one according to the population state information, and further better balancing convergence and diversity of the population. Meanwhile, a dimensional ranking based convergence measure is proposed to better evaluate the convergence of individuals, and further enhance the convergence. In addition, a dimensional difference based diversity measure is designed, which can not only overcome the influence of dimension abnormality but also adapt to different Pareto front shapes. Based on these, SIEA can effectively strike a good balance between convergence and diversity of the population. Extensive experimental studies have been done on four benchmark test suites, two combination optimization problems, and two real-world problems. The corresponding experimental results have demonstrated that SIEA has higher competitiveness in comparison with some state-of-the-art methods for many-objective optimization.},
  archive      = {J_ASOC},
  author       = {Wei Zhang and Jianchang Liu and Yuanchao Liu and Honghai Wang and Shubin Tan},
  doi          = {10.1016/j.asoc.2025.113669},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113669},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A state information driven evolutionary algorithm considering dimension influences for many-objective optimization},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shelf design and layout of a multi-floor vertical farming tower under uncertainty: A robust-hesitant fuzzy approach. <em>ASOC</em>, <em>183</em>, 113667. (<a href='https://doi.org/10.1016/j.asoc.2025.113667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although vertical farming has become a viable urban agriculture strategy for promoting sustainable food production to support the increasing demands of a burgeoning global population, this study is the first effort to provide a three-dimensional configuration of shelves and layouts in a multi-floor vertical farming tower, addressing a range of practical concerns. To achieve this goal, two novel multi-objective optimization models are developed, where the first objective function aims to minimize overall costs, the second focuses on reducing energy consumption, and the third seeks to maximize the cultivated area and job creation. On this matter, an extensive range of decisions and characteristics are examined, encompassing the strategic arrangement of positions on different floors to maximize the utilization of both natural and artificial lighting, the configuration of sprinkler pipes, the assignment of crop family sections across different positions of floors, the assignment of shelves to family sections, the specification of dimensions and the configuration of shelves and their layouts, the assignment of crops onto the shelves, and the determination of the area available for cultivation. Furthermore, due to the uncertainty surrounding costs, energy consumption, and the dimensions necessary for crop cultivation, a new approach combining set-induced robust optimization (SIRO) and the hesitant fuzzy set (HFS) is introduced to mitigate these uncertainties. In this regard, HFS is applied to address the uncertainties of costs and energy consumption, whereas SIRO is responsible for managing the uncertainties associated with the dimensions required for crop cultivation. Finally, to assess the validation and applicability of the proposed models and the uncertainty approach, several numerical problems and a case study are examined. The results demonstrate that there is potential for a reduction in total costs by roughly 6 %, alongside a decrease in cultivated area by about 23 %, provided that each shelf is designated for various crops within the same family.},
  archive      = {J_ASOC},
  author       = {S. Toranian and Behnam Vahdani and H.R. Gholami and A. Alinezhad},
  doi          = {10.1016/j.asoc.2025.113667},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113667},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Shelf design and layout of a multi-floor vertical farming tower under uncertainty: A robust-hesitant fuzzy approach},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The graph model with preference-approval structures based on third-generation prospect theory and its application to the e-commerce platform conflict. <em>ASOC</em>, <em>183</em>, 113666. (<a href='https://doi.org/10.1016/j.asoc.2025.113666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the graph model for conflict resolution (GMCR), the preference information of decision makers (DMs) is crucial for conflict modeling and stability analysis. In uncertain environments, DMs’ preferences are influenced by DMs’ psychological behaviors. This paper addresses DMs’ preference modeling as a multi-attribute decision-making (MADM) problem, and each DM’s preference over feasible states is expressed as a preference-approval structure (PAS), which is obtained based on third-generation prospect theory (TGPT). The TGPT comprehensively integrates DMs’ psychological expectations and can more accurately reflect DMs’ actual behavior when facing different risk. Specifically, uncertain reference points are introduced and the prospect values of states are computed across diverse situations. The prospect value reflects a DM’s perceived value of the state and forms the basis for ranking and classification of feasible states. A higher prospect value indicates a greater alignment between DMs’ evaluations of a state and DMs’ expectations. Furthermore, under the framework of GMCR, five new stability definitions are developed to adapt PASs. Finally, the effectiveness of proposed model is validated by applying it to a conflict of e-commerce platforms.},
  archive      = {J_ASOC},
  author       = {Nannan Wu and Ruyi Huang and Ming Situ and Dayong Wang},
  doi          = {10.1016/j.asoc.2025.113666},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113666},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The graph model with preference-approval structures based on third-generation prospect theory and its application to the e-commerce platform conflict},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source graph contrastive learning with dual-level dynamic fusion of structure and feature for inductive semi-supervised short text classification. <em>ASOC</em>, <em>183</em>, 113664. (<a href='https://doi.org/10.1016/j.asoc.2025.113664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using small amounts of labeled data to achieve short text classification has become a trend due to the difficulty of obtaining labeled data. With the wide application of graph neural networks, some inductive semi-supervised short text classification methods based on graph neural network are proposed to effectively predict new short text nodes or short text nodes in new graphs. The embedding ability of short text nodes included in most existing inductive semi-supervised short text classification methods based on graph neural network methods is insufficient. Also, it fails to measure the semantic correlation between short texts effectively. To overcome the above limitations, this paper proposes a new Multi-source Graph Contrastive Learning Method with Dual-level Dynamic Fusion of Structure and Feature (MSGCL-D2FSF) to efficiently implement inductive semi-supervised short text classification. Specifically, we perform the fusion of two types of multi-source graphs constructed from different perspectives, including multi-structure fusion used to fuse the structures of different information source graphs and cross-graph feature fusion to effectively capture the global correlation between short text nodes and short text nodes in different types of multi-source graphs. Further, for the multi-source graphs before and after fusion, two contrastive modes, semi-supervised multi-source graph contrastive learning, and self-supervised multi-source graph contrastive learning, are designed for the types of multi-source graphs to enhance further the short text node embeddings in the new multi-source graphs. Extensive experiments on five benchmark datasets show that the proposed MSGCL-D2FSF method outperforms the current state-of-the-art inductive semi-supervised short text classification methods based on graph neural network in several metrics. For example, on the SST2 dataset, compared to the Text-FCG model, these metrics are improved by 4.27%, 2.04%, 1.83%, and 1.74%, respectively. This achievement highlights the synergistic effects of graph fusion techniques and multi-source graph contrastive learning, providing a novel method for classifying new short text samples unseen during training.},
  archive      = {J_ASOC},
  author       = {Mingqiang Wu},
  doi          = {10.1016/j.asoc.2025.113664},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113664},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-source graph contrastive learning with dual-level dynamic fusion of structure and feature for inductive semi-supervised short text classification},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring building information modeling user satisfaction by using active interpretable machine learning. <em>ASOC</em>, <em>183</em>, 113663. (<a href='https://doi.org/10.1016/j.asoc.2025.113663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting building information modeling (BIM) user satisfaction (US) is essential for proactively addressing implementation challenges, ensuring effective adoption, and maximizing return on investment in BIM technologies in construction projects. Accordingly, this study developed advanced, interpretable boosting ensemble models to predict BIM US by integrating the forensic-based investigation (FBI) algorithm with gradient boosting machine, light gradient boosting machine, adaptive boosting (AdaBoost), extreme gradient boosting, and random forest algorithms. To validate the proposed models and establish a dataset, a comprehensive survey was conducted on 70 construction projects in Taiwan that used BIM technologies to support design work. Subsequently, the synthetic minority oversampling technique (SMOTE) was integrated into the proposed models to address the data imbalance problem. The results indicated that among all models, the FBI-AdaBoost-SMOTE model exhibited the highest performance, achieving accuracy, precision, recall, and F1 scores of 88.6 %, 90.6 %, 88.6 %, and 87.8 %, respectively. The FBI-AdaBoost model based on Shapley additive explanations identified contextual analysis and visualization, project scale, and cost estimates as key determinants of BIM US. Overall, this study presents an advanced machine learning framework for predicting BIM US and identifying key influencing factors for BIM US. It also provides actionable insights for stakeholders to enhance BIM implementation and user experience. In addition, this study highlights the potential of predictive modeling for optimizing the adoption of BIM in the architecture, engineering, and construction industry.},
  archive      = {J_ASOC},
  author       = {Wei-Chih Wang and Shyn-Chang Huang and Hsu-Pin Wang and Minh-Tu Cao},
  doi          = {10.1016/j.asoc.2025.113663},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113663},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Measuring building information modeling user satisfaction by using active interpretable machine learning},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-based semi-supervised crop and weed semantic segmentation. <em>ASOC</em>, <em>183</em>, 113662. (<a href='https://doi.org/10.1016/j.asoc.2025.113662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of crop and weed by farming robot camera can increase crop production and reduce unnecessary herbicide, which is a fundamental task in the field of sustainable and precision agriculture. However, obtaining the pixel-wise annotation of training data manually is expensive. As a solution to address this limitation, semi-supervised learning leverages a small amount of labeled data and a large amount of unlabeled data for learning. In this context, we propose a network based on vector quantization and prototype loss for semi-supervised crop and weed semantic segmentation (VQP-Net). VQP-Net achieves a strong performance in terms of consistency regularization through the implementation of a vector quantization module and prototype loss, and is capable of extracting discriminative features of crops and weeds, which are often indistinguishable. We conducted experiments using the proposed method with three open datasets: BoniRob, crop/weed field image, and rice seedling and weed datasets. The crop and weed segmentation accuracies based on mean intersection over union ( mIOU ) for the three datasets were 0.8643, 0.8329, and 0.7623, respectively, demonstrating that this method outperformed the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Chaeyeong Yun and Yu Hwan Kim and Sung Jae Lee and Su Jin Im and Kang Ryoung Park},
  doi          = {10.1016/j.asoc.2025.113662},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113662},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Artificial intelligence-based semi-supervised crop and weed semantic segmentation},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing EEG-based individual-generic emotion recognition through invariant sparse patterns extracted from ongoing affective processes. <em>ASOC</em>, <em>183</em>, 113659. (<a href='https://doi.org/10.1016/j.asoc.2025.113659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional responses to stimuli produce distinct brain activity patterns that are often sparse in time and spatial distribution across the cortex. These neural signals also contain individual-specific features, complicating emotion recognition across diverse populations. Current approaches rarely address the dual challenge of capturing sparse emotional patterns while minimizing identity-related biases in individual-generic emotion analysis. To bridge this gap, we propose a graph-based emotion-enhancing network framework that isolates emotion-specific neural signatures by amplifying sparse temporal-spatial features and suppressing person-specific biomarkers. Evaluated on two benchmark databases for binary emotion classification, our model achieved state-of-the-art performance in individual-dependent scenarios with accuracies of 65.76 % and 65.39 % for the arousal scale, and 57.75 % and 66.74 % for the valence scale. In the individual-generic condition, the accuracies were 56.11 % and 61.02 % for arousal, and 55.21 % and 66.17 % for valence. Notably, the model’s temporal and spatial enhancement modules provide interpretable insights into emotion-related neural sparsity through learned feature weights. This framework advances emotion recognition systems by reliably identifying universal emotional patterns across individuals while improving computational generalizability.},
  archive      = {J_ASOC},
  author       = {Yiwen Zhu and Jiehao Tang and Hongjuan Wei and Kaiyu Gan and Jianhua Zhang and Zhong Yin},
  doi          = {10.1016/j.asoc.2025.113659},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113659},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing EEG-based individual-generic emotion recognition through invariant sparse patterns extracted from ongoing affective processes},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cooperation and competition-based tri-population evolutionary algorithm for constrained multi-objective optimization. <em>ASOC</em>, <em>183</em>, 113658. (<a href='https://doi.org/10.1016/j.asoc.2025.113658'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When using evolutionary algorithms to solve constrained multi-objective optimization problems (CMOPs), it is essential to simultaneously satisfy various constraints and optimize multiple conflicting objective functions, which poses a serious challenge for solvers. Despite the numerous constrained multi-objective evolutionary algorithms have been proposed, most of them exhibit bad convergence or diversity performance on different types of CMOPs maybe as they fail to fully exploit promising infeasible solutions. As a remedy for this limitation, this paper proposes a cooperation and competition-based tri-population evolutionary algorithm (CCTPEA) to solve various kinds of CMOPs effectively. Specifically, the proposed method evolves three populations that employ different strategies to handle constraints. The first population considers constraints to find Pareto optimal solutions to ensure feasibility. The second population utilizes infeasible solutions with superior objective function values by ignoring constraints to pass through large infeasible regions, which ensures the convergence of algorithm. The third population adopts the constraint-relaxed method to continuously leverage promising infeasible solutions between unconstrained Pareto front (UPF) and constrained Pareto front (CPF), which is beneficial to address problems whose UPF and CPF are separate. To select an appropriate evolutionary population based on the type of problem, a dynamic competition mechanism is designed, in which it adjusts the allocation of computational resources consumed by two populations (i.e., the second population and the third population) by assessing the validity of them. The effectiveness of the proposed method is demonstrated on 37 benchmark CMOPs and 19 real-world CMOPs compared with eleven state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Kunjie Yu and Lianhe Duan and Jing Liang and Kangjia Qiao and Boyang Qu and Quan Sui and Xiangyang Ren},
  doi          = {10.1016/j.asoc.2025.113658},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113658},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A cooperation and competition-based tri-population evolutionary algorithm for constrained multi-objective optimization},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic lightweight siamese-prototypical network for few-shot relation classification. <em>ASOC</em>, <em>183</em>, 113657. (<a href='https://doi.org/10.1016/j.asoc.2025.113657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot relation classification aims to identify the relation between entity pairs in the sentence through few labeled instances. Most existing approaches based on prototypical networks and their related variants have achieved promising results. However, these methods still face several challenges: (1) They disregard the latent information present in unlabeled query instances and fail to consider the categorical information between different classes. Consequently, the prototypes computed solely from limited support instances tend to be inaccurate. (2) Some instances contain multiple entity pairs that correspond to different relations, causing significant confusion during the classification process. To address above problems, we propose a D ynamic L ightweight S iamese- P rototypical network (DLSP) to learn better prototypical representations. Specifically, we introduce the information siamese and fusion mechanism that dynamically adjusts the contribution of each support instance based on similarity, fully integrating the categorical information implied in different classes. Moreover, we further design the confusing-adaptive loss function to mitigate the impact of challenging and ambiguous relations caused by multi-entity instances on classification outcomes. Extensive experiments on the FewRel dataset demonstrate that our proposed lightweight model outperforms strong baselines with fewer parameters, requiring fewer training iterations and achieving faster convergence.},
  archive      = {J_ASOC},
  author       = {Haijia Bi and Lu Liu and Hai Cui and Ridong Han and Jiayu Han and Tao Peng},
  doi          = {10.1016/j.asoc.2025.113657},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113657},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic lightweight siamese-prototypical network for few-shot relation classification},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of artificial intelligence in electrocardiogram diagnostics: Integrating knowledge map and meta-analysis approaches. <em>ASOC</em>, <em>183</em>, 113655. (<a href='https://doi.org/10.1016/j.asoc.2025.113655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiogram (ECG) record the heart’s electrical activity and is vital for continuous cardiac monitoring. While dozens of reviews have surveyed deep learning approaches to ECG analysis, they rarely address the field’s full scope. Here, we systematically review 2,990 ECG studies published over the past decade and perform a meta-analysis on 58 articles evaluating algorithmic performance for atrial fibrillation (AF), myocardial infarction (MI), and coronary artery disease (CAD). A literature-based knowledge map highlights machine learning and deep learning as dominant research trends. Our meta-analysis reveals that convolutional neural networks (CNNs) deliver the highest diagnostic accuracy for AF, MI, and CAD, though efficacy diminishes across those conditions. We also explore emerging methods, including large language models, and conclude by discussing outstanding challenges and future directions in data quality and diversity, model generalizability, clinical integration, and novel technology adoption.},
  archive      = {J_ASOC},
  author       = {Yang Yang and Fenglin Zhu and Yuchao Gao and Zhihao Chen and Xi’an Li and Shangce Gao and Jinran Wu},
  doi          = {10.1016/j.asoc.2025.113655},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113655},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive review of artificial intelligence in electrocardiogram diagnostics: Integrating knowledge map and meta-analysis approaches},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive strategy quantum particle swarm optimization method based on intuitionistic fuzzy entropy and evolutionary game theory. <em>ASOC</em>, <em>183</em>, 113654. (<a href='https://doi.org/10.1016/j.asoc.2025.113654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since premature decline in population diversity is a vital problem in heuristic algorithm optimization, numerous methods enhancing global search ability have been developed to avoid local optimum. However, global exploration strategy diverts resources from exploitation, reducing optimization accuracy. To maintain exploration ability while ensuring accuracy, an adaptive strategy quantum particle swarm optimization method (ASQPSO) based on intuitionistic fuzzy entropy (IFE) and evolutionary game theory (EGT) is proposed in this paper. Firstly, IFE is introduced to quantify algorithm population diversity. Next, this paper proposes several strategies and develops an algorithm structure based on EGT to improve exploration and exploitation performance. Finally, comparison experiments are conducted to verify the performance of ASQPSO. Test results on 23 benchmark functions indicate that the proposed method has better comprehensive performance than the comparison algorithms. This paper researches a feasible way to adjust the diversity of the QPSO method quantitatively and provides a reference for its application in the heuristic algorithms.},
  archive      = {J_ASOC},
  author       = {Guan Zhou and Zihao Fang and Yingxin Hu and Jintao Chen and Jinyu Ren},
  doi          = {10.1016/j.asoc.2025.113654},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113654},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive strategy quantum particle swarm optimization method based on intuitionistic fuzzy entropy and evolutionary game theory},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Argumentation meets matrix factorization: A dual perspective for explainable recommendations. <em>ASOC</em>, <em>183</em>, 113652. (<a href='https://doi.org/10.1016/j.asoc.2025.113652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factorization-based models gained prominence during the Netflix Challenge (2007) and have since demonstrated strong performance in predicting user ratings. However, their limited interpretability often hinders users from understanding the rationale behind recommendations. In contrast, argumentation-based methods offer a different perspective: they model human-like reasoning by structuring information as arguments and counterarguments. They excel in explainability but typically fall short in accuracy To address this trade-off, we propose a novel framework, Context-Aware Feature-Attribution Through Argumentation (CA-FATA), which combines the predictive power of matrix factorization with the interpretability of argumentation frameworks. In CA-FATA, each user–item interaction is modeled using an argumentation framework. Items’ features are represented as arguments, and users’ ratings determine the arguments’ strengths. Additionally, the model incorporates users’ contextual information (e.g., time, location) to further improve predictive performance. Empirical evaluations on real-world datasets show that CA-FATA excels in predictive accuracy and interpretability. It outperforms existing argumentation-based methods and achieves comparable results with state-of-the-art context-free and context-aware models. CA-FATA also supports multiple explanation formats, including template-based explanations, interactive feedback, and contrastive reasoning. Furthermore, it alleviates the cold-start problem by clustering users based on feature preferences.},
  archive      = {J_ASOC},
  author       = {Jinfeng Zhong and Elsa Negre},
  doi          = {10.1016/j.asoc.2025.113652},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113652},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Argumentation meets matrix factorization: A dual perspective for explainable recommendations},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-driven multimodal alignment for long-term action quality assessment. <em>ASOC</em>, <em>183</em>, 113649. (<a href='https://doi.org/10.1016/j.asoc.2025.113649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term action quality assessment (AQA) focuses on evaluating the quality of human activities in videos lasting up to several minutes. This task plays an important role in the automated evaluation of artistic sports such as rhythmic gymnastics and figure skating, where both accurate motion execution and temporal synchronization with background music are essential for performance assessment. However, existing methods predominantly fall into two categories: unimodal approaches that rely solely on visual features, which are inadequate for modeling multimodal cues like music; and multimodal approaches that typically employ simple feature-level contrastive fusion, overlooking deep cross-modal collaboration and temporal dynamics. As a result, they struggle to capture complex interactions between modalities and fail to accurately track critical performance changes throughout extended sequences. To address these challenges, we propose the Long-term Multimodal Attention Consistency Network (LMAC-Net). LMAC-Net introduces a multimodal attention consistency mechanism that explicitly aligns features across different modalities, enabling stable integration of complementary multimodal information and significantly enhancing feature representation capabilities. Specifically, a multimodal local query encoder module with learnable queries is designed to automatically capture temporal semantics within each modality while dynamically modeling complementary relationships across modalities. To ensure interpretable evaluation results, we adopt a two-level score evaluation module, where stage-wise scores are first calculated to generate a final overall score. Additionally, we apply attention-based feature-level and regression-based result-level loss to jointly optimize multimodal alignment and decision-layer fusion. Experiments conducted on the RG and Fis-V datasets demonstrate that LMAC-Net significantly outperforms existing methods, validating the effectiveness of our proposed approach.},
  archive      = {J_ASOC},
  author       = {Xin Wang and Peng-Jie Li and Yuan-Yuan Shen},
  doi          = {10.1016/j.asoc.2025.113649},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113649},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-driven multimodal alignment for long-term action quality assessment},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unpaired to paired data synthesis and generative ensemble network for smudged image restoration. <em>ASOC</em>, <em>183</em>, 113648. (<a href='https://doi.org/10.1016/j.asoc.2025.113648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In digital photography, the clarity of photos is a critical concern. However, camera lenses can inadvertently become dirty with fingerprints or dust, leading to smudged images. When this occurs, image restoration techniques are sought to recover the clarity of blurred photos. Unfortunately, no dataset or method has been specifically designed for this image restoration task. A similar task is image dehazing. However, we observe a significant domain gap between smudged and hazy images, resulting in poor performance of dehazing methods on this challenging problem. To explain the reasons for the domain gap, we conduct an atmospheric scattering analysis. Additionally, we conduct mathematical analysis to analyze impact of different kinds of smudge to imaging. In view of the absence of solutions for this task, we introduce the first dataset designed for this endeavor. Given the challenges of capturing paired smudged and clear images in real-world scenarios, especially with moving objects, we propose an unpaired to paired data synthesis pipeline. This pipeline enables the generation of a large amount of training data to facilitate model development. Furthermore, we propose Generative Ensemble Network (GENet), an ensemble learning network with a two-branch architecture for smudged image restoration. In this model, one branch focuses on learning general features with pretrained models, while the other branch adapts to the data’s specific characteristics. Additionally, GENet enhances visual quality by incorporating adversarial training. Experimental results emphasize the significance of our proposed dataset. Additionally, extensive comparative experiments on both synthetic and real-world data demonstrate the superior performance of our GENet in smudged image restoration compared to other restoration techniques. Datasets are avaliable at .},
  archive      = {J_ASOC},
  author       = {Chen-Bin Feng and Kangdao Liu and Jian Sun and Qi Lai and Ji-Ping Jin and Houcheng Su and Guangtai Wang and Chi-Man Vong},
  doi          = {10.1016/j.asoc.2025.113648},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113648},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unpaired to paired data synthesis and generative ensemble network for smudged image restoration},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plastic distillation and local class augment for federated class incremental learning. <em>ASOC</em>, <em>183</em>, 113647. (<a href='https://doi.org/10.1016/j.asoc.2025.113647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data heterogeneity and knowledge forgetting represent two major challenges in federated class incremental learning. To mitigate these issues, this paper proposes a novel method, termed Plastic Distillation and Local Class Augmentation based federated class incremental learning (PDLCA). The proposed method comprises two key modules. Firstly, Plastic Distillation (PD) is designed to improve traditional knowledge distillation by decomposing the model probability output into labeled items and non-labeled items. By reducing the proportion of distilled labeled items, PD eases the rigid constraint of between-class discrimination information on the model during knowledge distillation, thus enhancing the model plasticity. By increasing the proportion of non-labeled items, PD improves knowledge transfer efficiency. Secondly, Local Class Augmentation (LCA) is designed to generate new classes by combining original data from each client in the first round of incremental tasks and uses an independent class increment classifier for discrimination. These synthesized classes serve dual purposes: mitigating local model over-fitting to original classes and preserving model capacity for future adaptations, thus further enhancing model plasticity. The integration of PD and LCA effectively mitigates the challenges posed by data heterogeneity and knowledge forgetting. Experimental results on CIFAR-200 and Tiny-imagenet show that PDLCA outperforms the state-of-the-art federated class incremental learning methods.},
  archive      = {J_ASOC},
  author       = {Wenyi Feng and Jianqiang Huang and Wandong Xue},
  doi          = {10.1016/j.asoc.2025.113647},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113647},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Plastic distillation and local class augment for federated class incremental learning},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-enhanced multivariate ensemble model for PV power spatio-temporal forecasting and scenario generation. <em>ASOC</em>, <em>183</em>, 113646. (<a href='https://doi.org/10.1016/j.asoc.2025.113646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate photovoltaic (PV) power forecasting is pivotal for integrating variable energy into the grid. Existing research predominantly focuses on individual models or simple combinations, often overlooking the comprehensive exploitation of spatio-temporal features. Furthermore, most studies struggle to accurately characterize typical output scenarios due to limitations in handling uncertainties. To tackle these issues, this paper proposes a novel spatio-temporal forecasting framework featuring a feature-enhanced multivariate ensemble model. First, it presents a spatio-temporal feature enhancement method for solar position and time period encoding, decomposing the power sequence into multiple intrinsic mode functions via sequential variational mode decomposition and recombining them based on approximate entropy. Then, the temporal convolutional networks (TCN), gate recurrent unit (GRU) and eXtreme gradient boosting (XGBoost) optimized by the enhanced adaptive butterfly optimization algorithm (EABOA) are used to form parallel forecasting models (parallel EABOA-optimized TCN-GRU and XGBoost ensemble model, namely PETGX), and a dynamic error-weighted ensemble method is proposed to realize the multivariate model ensemble to achieve deterministic and interval forecasting. Moreover, historical PV power data is clustered via autoencoder, and Gaussian copula parameters estimated from diverse samples within each cluster are then combined with marginal distributions derived from the proposed PETGX quantile regression (QR) and kernel density estimation (KDE) to offer effective spatio-temporal probability forecasting for multiple PV stations, and typical PV output scenarios are generated through fast forward reduction. Finally, based on real-world data from 12 power stations of Australian Yulara (5 min resolution) and Hebei, China (15 min resolution), the proposed method verifies its superiority by comparing with multiple peer approaches and advanced models using various evaluation criteria. Empirical results demonstrate that the proposed method can achieve high-precision PV power forecasting and generate typical output scenarios effectively.},
  archive      = {J_ASOC},
  author       = {Kai He and Yong Zhang and Yukun Wang and Ronghe Zhou and Hao Liu},
  doi          = {10.1016/j.asoc.2025.113646},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113646},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature-enhanced multivariate ensemble model for PV power spatio-temporal forecasting and scenario generation},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online knowledge distillation enabled multi-hop graph attention networks for anonymous transaction regulation in blockchain. <em>ASOC</em>, <em>183</em>, 113644. (<a href='https://doi.org/10.1016/j.asoc.2025.113644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology holds immense significance as it revolutionizes trust mechanisms and enhances value transfer efficiency, exerting a disruptive influence on various industries. However, security incidents such as fraud and hacking have become primary obstacles to the healthy development of blockchain. Despite the admirable designs of numerous existing blockchain platforms, which prioritize anonymity for transactional privacy, the ever-expanding scale of transaction data poses challenges to the feasibility and effectiveness of current regulatory approaches. In this paper, we examine blockchain transactions, treating them as graph data, and present a framework dubbed BTGuard , with the primary objective of exposing anomalies in anonymous transactions. Specifically, we extract hierarchical k-hop node features from the transaction graph to train an ensemble model of graph attention networks (GATs), which provides richer contextual information. An online knowledge distillation method is then seamlessly integrated into the training pipeline to facilitate mutual learning by passing soft labels among the GATs, thus building the underlying GAT into a powerful anomaly detection model and eliminating the use of sophisticated network architectures. Furthermore, a community detection technique is employed to meticulously analyze the propagation paths and associated nodes of detected anomalous transactions, facilitating penetrating transaction regulation. Extensive experiments demonstrate that our approach achieves superior performance compared to several representative methods, improving the F1-score in two real-world datasets by 2.3% and 2.0%, respectively.},
  archive      = {J_ASOC},
  author       = {Tong Gu and Min Han and Songlin He and Xiaotong Chen and Hongchun Lu and Zhizhou Wang},
  doi          = {10.1016/j.asoc.2025.113644},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113644},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online knowledge distillation enabled multi-hop graph attention networks for anonymous transaction regulation in blockchain},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). APA-boosted segmentation neural network with flexible set expansion strategy for industrial fault detection. <em>ASOC</em>, <em>183</em>, 113643. (<a href='https://doi.org/10.1016/j.asoc.2025.113643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial aspect of the industrial installation operation is proper maintenance. To ensure this, appropriate control and monitoring of the condition of the fitting are necessary. For this purpose, we propose new encoder–decoder architecture model for industrial fault segmentation. The neural network architecture is based on a three-block encoder/decoder module and a bottleneck. Information concatenation between the encoder and the decoder was also introduced to transfer information about low-level features from the encoder. Each decoder block was enriched with the Atrous Pixel Attention (APA) module, which allows for the enhancement of specific features, such as features of different scales, or spatial context and maintaining a global representation of the processed data. Additionally, we propose an algorithm for expanding the training set, which flexibly adapts to the current training progress. Technique extends the training set by adding modified samples with the highest Shannon entropy, which allows for reducing the impact of data imbalance in the original set. Moreover, the set can also be reduced in case the desired learning efficiency is achieved to prevent overfitting. We test the fault detection potential of this approach on a set of welding joint faults and a set of steel strips with patches, inclusions, and scratches using GDXray Welds and SD-saliency-900 datasets. The experiments performed showed that the proposed algorithm allows for high-accuracy detection and improves existing approaches reaching a Dice coefficient of 0.9347 and 0.8890 on GDXray Welds and SD-saliency-900, respectively.},
  archive      = {J_ASOC},
  author       = {Katarzyna Prokop and Dawid Połap},
  doi          = {10.1016/j.asoc.2025.113643},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113643},
  shortjournal = {Appl. Soft. Comput.},
  title        = {APA-boosted segmentation neural network with flexible set expansion strategy for industrial fault detection},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expansion-trajectory optimization (ETO): A dual-operator metaheuristic for balanced global and local search. <em>ASOC</em>, <em>183</em>, 113642. (<a href='https://doi.org/10.1016/j.asoc.2025.113642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Premature convergence remains a critical limitation in many metaheuristic algorithms, and is often caused by a rapid loss of population diversity as individuals become overly similar early in the search process. To address this challenge, this paper proposes a new metaphor-free algorithm called Expansion-Trajectory Optimization (ETO), which introduces a dual-operator framework designed to maintain diversity and enhance search performance. The ETO algorithm combines two complementary mechanisms: the expansion operator, which leverages collective information from multiple individuals to identify and explore promising regions in the search space; and the trajectory operator, which conducts a guided search following a Fibonacci spiral. This spiral-based path enables a smooth transition from broad exploration to focused exploitation, thereby ensuring a balanced and adaptive search process. The proposed approach was rigorously evaluated against several state-of-the-art metaheuristic algorithms, using a diverse set of benchmark functions. The experimental results confirm that ETO achieves superior performance in terms of both accuracy and robustness, demonstrating its effectiveness in overcoming early convergence and enhancing optimization outcomes.},
  archive      = {J_ASOC},
  author       = {Erik Cuevas and Oscar A. González-Sánchez and Francisco Orozco- Jiménez and Daniel Zaldívar and Alma Rodríguez-Vazquez and Ram Sarkar},
  doi          = {10.1016/j.asoc.2025.113642},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113642},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Expansion-trajectory optimization (ETO): A dual-operator metaheuristic for balanced global and local search},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kidney image segmentation from CT for disease diagnosis based on deep extreme cut and NASNet-bi-LSTM model using generative AI for improved resolution. <em>ASOC</em>, <em>183</em>, 113641. (<a href='https://doi.org/10.1016/j.asoc.2025.113641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kidney disease technically referred to as nephropathy, which is a broad term used to describe a variety of disorders that affect the structure and function of the kidneys. Even a slight deviation in kidney function and structure measurements are linked to a higher chance of death more frequent than kidney failure. The patient's kidney condition doesn't appear severe in its initial stages, but recovery becomes difficult as the illness advances. To preserve the patient's life, doctors must be able to diagnose the illness early. Several machine learning algorithms are some of the commonly used automated models to predict for diagnosing various diseases. But achieving accurate illness prediction with a low error probability is difficult due to inadequate data training, poor image quality, and incorrect segmentation. So, a hybrid deep learning system is created to detect kidney illness based on CT scans in order to allay these worries. The input images of the kidney stone, cysts, normal and tumor are collected and pre-processed using a modified Gen AI enabled super resolution conversion algorithm to replace the distorted pixels in the input image. Then for enhancing the contrast level of the super resolution image, Dandelion based CLAHE algorithm is developed. At last, hybrid NASNet-BiLSTM is utilized for detecting the kidney disease whether it is normal, stone, cysts and tumor. The suggested method provides 94 % precision, 93 % specificity, and 96 % accuracy. Consequently, by employing this automated approach for detecting the kidney disease diagnosis can be facilitated and treatment can be started early to reduce the death rate.},
  archive      = {J_ASOC},
  author       = {C. Girija and P. Ganesh Kumar},
  doi          = {10.1016/j.asoc.2025.113641},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113641},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Kidney image segmentation from CT for disease diagnosis based on deep extreme cut and NASNet-bi-LSTM model using generative AI for improved resolution},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Type-2 picture fuzzy VIKOR approach based on TODIM for evaluating last-leg delivery in metro cities in india. <em>ASOC</em>, <em>183</em>, 113640. (<a href='https://doi.org/10.1016/j.asoc.2025.113640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the logistics industry in metro cities, last-leg delivery (LLD) quality evaluation is the essential core of the customer, which is regarded as an MCDM (multi-criteria decision-making) problem. Research on sustainable last-leg delivery (LLD) is particularly popular and active because of the growing significance of LLD and the related high expenses, contaminants in the air, and logistics challenges. This research focuses on selecting the best LLD mode from various possibilities while accounting for various factors, such as inaccurate, unclear, and uncertain sustainability-related data. This work aims to present an advanced approach to sustainable LLD decision-making. A novel approach is then used based on the suggested entropy and divergence measure. Subjective and objective weights are used to create the final criteria weights, which provide the evaluation criteria a more precise weighting. The fuzzy method for sustainable LLD that has been provided is related to the real-world decision-making process. The ”e-cargo bike” is the most beneficial option in South Delhi, according to the results. As per our knowledge, no researcher has yet suggested a combination of Tsallis’ entropy and type-2 picture fuzzy divergence measure. The present manuscript proposes a fresh type-2 picture fuzzy (T2PF) measure. This paper’s goal is to use novel operations on T2PF numbers to study T2PF environments. Cuong’s picture fuzzy set is expanded upon by the T2PF set, which takes into account the neutral and invalid degrees during the analysis in addition to the degree of acceptance or rejection. The great dependability of the suggested strategy was validated by a comparison study with T2PF based TODIM–VIKOR techniques. The strong robustness of the suggested method was validated by the relative sensitivity studies of the balancing factor and trade-off parameter. The decision-makers (DMs) in the transportation sector may find the presented method useful in clarifying the sustainable LLD mode. It can resolve additional MCDM issues in the fuzzy picture scenario in addition to the emphasized one.},
  archive      = {J_ASOC},
  author       = {Vanita Rani and Satish Kumar},
  doi          = {10.1016/j.asoc.2025.113640},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113640},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Type-2 picture fuzzy VIKOR approach based on TODIM for evaluating last-leg delivery in metro cities in india},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decomposition-based multi-objective evolutionary algorithm with customized evolution strategy according to population state. <em>ASOC</em>, <em>183</em>, 113639. (<a href='https://doi.org/10.1016/j.asoc.2025.113639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary algorithms (MOEAs) are designed to obtain a set of solutions that closely approximate the Pareto front and maintain uniform distribution along it. However, most MOEAs apply the same strategy throughout the whole evolutionary process, which may introduce certain limitations. To address this issue, an MOEA based on decomposition (MOEA/D) where population can adaptively evolve based on its state is proposed to improve the convergence or diversity of solutions at the most appropriate time. First, for robustly identifying the state, a flexible sliding window is used to record the historical convergence information of the population. Then, with the support of a state identification procedure, the convergence emphasize procedure and the diversity emphasize procedure are designed to emphasize the convergence and diversity of solutions respectively. Experimental results on 39 different test problems and 3 real-world problems demonstrate that the proposed algorithm outperforms the compared algorithms including three dominance-based MOEAs, six decomposition-based MOEAs and one indicator-based MOEA.},
  archive      = {J_ASOC},
  author       = {Lexing Chen and Taiyong Li and Donglin Zhu and Wu Deng},
  doi          = {10.1016/j.asoc.2025.113639},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113639},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decomposition-based multi-objective evolutionary algorithm with customized evolution strategy according to population state},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer spectral optimization: From gradient frequency analysis to adaptive spectral integration. <em>ASOC</em>, <em>183</em>, 113637. (<a href='https://doi.org/10.1016/j.asoc.2025.113637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores a novel perspective on Transformer optimization by analyzing gradient characteristics in the frequency domain. First, we systematically quantify spectral differences between attention and MLP layer gradients, revealing that attention gradients consistently exhibit higher frequency content (23% higher mean frequency, 37% more prominent high-frequency components) compared to MLP gradients. Second, we demonstrate the potential of using spectral features for monitoring training dynamics, finding a strong correlation (r=-0.82) between early-stage spectral entropy and final validation loss. Third, building on these insights, we introduce Adaptive Spectral Integration (ASI), an optimization framework that selectively filters gradient spectra during training. Our experiments on GPT2-small with standard datasets (Penn Treebank and WikiText-2) show that ASI achieves notable inference speed improvements (6.3%-9.1%) and training time reductions (13.2%-18.8%) while maintaining comparable model quality. However, cross-architecture validation with BERT-style models reveals that ASI’s efficiency benefits are architecture-dependent, showing limited improvements on bidirectional models. These findings provide evidence that frequency domain analysis offers valuable insights for optimizing autoregressive Transformer models, while highlighting the need for architecture-aware spectral optimization strategies.},
  archive      = {J_ASOC},
  author       = {Zhigao Huang and Musheng Chen and Shiyan Zheng},
  doi          = {10.1016/j.asoc.2025.113637},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113637},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transformer spectral optimization: From gradient frequency analysis to adaptive spectral integration},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A soft sensor net based on the symplectic decomposition-global attention reconstruction architecture for biopharmaceutical industry. <em>ASOC</em>, <em>183</em>, 113636. (<a href='https://doi.org/10.1016/j.asoc.2025.113636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-linearity, time-varying properties, and high noise levels in biopharmaceutical process data have been recognized as critical factors affecting the accuracy of data-driven soft sensors. To address these issues and enhance prediction precision, we introduce BPSN, an innovative soft sensor framework grounded in the symplectic decomposition-global attention reconstruction architecture. Symplectic geometry mode decomposition effectively adapts to data complexity and reduces noise. A reconstruction module combines global attention mechanism and reversible instance normalization to enhance sharp signal features via Manhattan distance while addressing internal drift. Experiments show that the proposed soft sensor model outperforms state-of-the-art models in predicting key indicators: bacterial concentration, viscosity, and reducing sugar content in the erythromycin fermentation process. This illustrates its practical applicability and exceptional performance in biopharmaceutical industry. The source code is available at: https://github.com/Joss0623/BioPharmaSoftNet.git .},
  archive      = {J_ASOC},
  author       = {Simengxu Qiao and Yichen Song and Qunshan He and Shifan Chen and He Zhang and Xinggao Liu},
  doi          = {10.1016/j.asoc.2025.113636},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113636},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A soft sensor net based on the symplectic decomposition-global attention reconstruction architecture for biopharmaceutical industry},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive evolutionary algorithms with linear average convergence rate for optimization problems with lipschitz continuous functions. <em>ASOC</em>, <em>183</em>, 113635. (<a href='https://doi.org/10.1016/j.asoc.2025.113635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary methods, such as genetic algorithm, evolutionary strategy and differential evolution, are widely employed in the optimization of complex problems across various domains. After convergence is achieved, analyzing the convergence rate becomes essential for assessing the efficiency of the optimization process. This work investigates strategies aimed at improving the average convergence rate in optimization problems characterized by Lipschitz continuous objective functions. Strategies for evolutionary algorithms are proposed, designed to promote a positive-adaptive mutation process and ensure a linear average convergence rate, thereby providing efficient solutions to complex optimization problems. Lower bounds for the average convergence rate are also derived, considering the Lipschitz constant of the objective function and the problem’s dimensionality. To validate the theoretical results, the proposed positive-adaptive mutation strategies are applied to Genetic Algorithm, Evolutionary Strategy, and Differential Evolution in solving various benchmark optimization problems and the practical Economic Dispatch Problem. In all tests, the proposed mutation strategy demonstrated a superior average convergence rate compared to the other strategies used in the comparisons. Furthermore, the results of the Wilcoxon test provide statistical evidence to confirm a significant difference at a significance level of 0,05. When comparing the global mean of the average convergence rates, our strategy achieved an improvement of at least 12% compared to other relevant adaptations from the literature.},
  archive      = {J_ASOC},
  author       = {Fabiano Borges da Silva and Lívia Teresa Minami Borges and Leonardo Nepomuceno and Edilaine Martins Soler},
  doi          = {10.1016/j.asoc.2025.113635},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113635},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive evolutionary algorithms with linear average convergence rate for optimization problems with lipschitz continuous functions},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning with three-way decisions for privacy-preserving multicloud resource scheduling. <em>ASOC</em>, <em>183</em>, 113634. (<a href='https://doi.org/10.1016/j.asoc.2025.113634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the Federated Three-Way Decision System (F3WDS), a novel framework for multicloud resource scheduling that integrates federated learning with the three-way decision theory to address the challenges of resource heterogeneity, decision uncertainty, and data privacy. By combining privacy-preserving collaborative learning with nuanced decision-making (positive, boundary, and negative regions), the F3WDS optimizes resource allocation across multiple cloud providers while adhering to strict data sovereignty requirements. We provide rigorous theoretical guarantees, including convergence analysis, privacy bounds, and performance bounds, to demonstrate the reliability of the system. Extensive experiments on synthetic and real-world datasets demonstrate that F3WDS achieves significant improvements over state-of-the-art methods: 5%–14% higher resource utilization, 60% lower privacy loss, and 30% reduced cross-cloud latency. The framework’s scalability, robustness to stragglers, and favorable privacy-utility trade-off make it a solution for privacy-sensitive multicloud environments, with implications for future research on distributed computing and privacy-aware resource management.},
  archive      = {J_ASOC},
  author       = {Chunmao Jiang and Lirun Su},
  doi          = {10.1016/j.asoc.2025.113634},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113634},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated learning with three-way decisions for privacy-preserving multicloud resource scheduling},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-criteria decision-making approach using bipolar fuzzy numbers for optimizing two-sided matching in technology transfer decisions. <em>ASOC</em>, <em>183</em>, 113633. (<a href='https://doi.org/10.1016/j.asoc.2025.113633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To effectively compete in the technology market, users must select the right suppliers, and suppliers must align with suitable users. However, existing studies often fall short in addressing the inherent uncertainty and conflicting objectives involved in such two-sided decision-making environments, particularly in the context of technology partnerships. This paper explores the complexity of two-sided matching between technological suppliers and users, focusing on multi-criteria evaluations under uncertainty. To overcome the limitations of previous models, particularly their inability to account for both positive and negative aspects of expert evaluations—we introduce a decision-making framework based on bipolar fuzzy numbers (BFNs). This framework captures incomplete and conflicting information by representing degrees of both positive and negative belongingness. The weights of unknown criteria are objectively determined using the MEREC (method based on the removal effects of criteria) technique within the bipolar fuzzy environment. Individual assessments are aggregated through BFN-based operators to form satisfaction degree matrices for both suppliers and users. A broker then facilitates the matching process, aiming to maximize expected profit (brokerage value). To ensure stable and mutually beneficial outcomes, a multi-objective 0 – 1 optimization model is developed, aligning the interests of both sides. The applicability and effectiveness of the proposed approach are demonstrated through a real-world-inspired scenario involving diverse technology suppliers and users, each with varying investment priorities. Results confirm the robustness and relevance of the method in handling two-sided decision-making under uncertainty.},
  archive      = {J_ASOC},
  author       = {Faizan Ahemad and Pankaj Gupta and Mukesh Kumar Mehlawat and Muhammet Deveci and LeSheng Jin},
  doi          = {10.1016/j.asoc.2025.113633},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113633},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-criteria decision-making approach using bipolar fuzzy numbers for optimizing two-sided matching in technology transfer decisions},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multiscale feature fusion spectral transformer for the generation of medical hyperspectral image. <em>ASOC</em>, <em>183</em>, 113632. (<a href='https://doi.org/10.1016/j.asoc.2025.113632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to conventional three-band RGB images commonly used in pathological diagnosis, hyperspectral imaging (HSI) can capture more biochemical characteristics of both stained and unstained tissue specimens, providing richer information for distinguishing pathological tissues. However, hyperspectral imaging systems are significantly more expensive than conventional color cameras, substantially increasing the financial burden on patients. Moreover, existing hyperspectral image generation models rarely focus on medical image synthesis. To address this challenge, we propose a Hierarchical Multi-scale Feature Fusion Spectral Transformer (HMFF-ST) model to generate 3D hyperspectral images from 3-spectral medical images. First, we utilize a camera-sensitive function to convert 3D HSI images into 3-spectral measurement images as a replacement for RGB images. The HMFF-ST model incorporates two improved Swin Transformer-based feature extraction modules: Hierarchical Pre-mask Swin Transformer (HPM-Swintran), which employs a pre-masked window attention mechanism, and Multi-Shift Enhanced Swin Transformer (MSE-Swintran), which enhances feature extraction capability through a multi-shift mechanism. Additionally, we adopt a hierarchical structure and multi-scale feature fusion strategy to generate high-quality medical hyperspectral images. To evaluate the performance of the proposed HMFF-ST model, we conducted experiments on a publicly available multi-dimensional choledoch hyperspectral dataset. Compared to GAN-based models for medical image synthesis, the proposed HMFF-ST model achieved a PSNR improvement of 7.16 and an SSIM improvement of 0.042. Furthermore, when combining the HMFF-ST-generated HSI images with the original 3-spectral measurement images as input, the segmentation performance of cancer areas was enhanced, with a Dice coefficient improvement of 2.24% and an IoU increase of 1.19%. These results demonstrate that generating medical hyperspectral images contributes to improved segmentation performance.},
  archive      = {J_ASOC},
  author       = {Quanyu E and Huiyan Jiang and Guoyu Tong and Xianhua Han},
  doi          = {10.1016/j.asoc.2025.113632},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113632},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical multiscale feature fusion spectral transformer for the generation of medical hyperspectral image},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ResQ: A hybrid classical-quantum model for efficient breast cancer image classification. <em>ASOC</em>, <em>183</em>, 113631. (<a href='https://doi.org/10.1016/j.asoc.2025.113631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer remains one of the leading causes of mortality worldwide, necessitating accurate and efficient diagnostic systems to improve treatment outcomes. While deep learning-based Computer-Aided Diagnostic (CAD) tools have demonstrated promise in analyzing histopathological images, they face challenges in handling high-dimensional data and computational inefficiencies. Simultaneously, quantum computing has emerged as a transformative technology, offering unparalleled capabilities in modeling complex data distributions and accelerating computations. This paper introduces ResQ, a hybrid classical-quantum framework designed for breast cancer classification. ResQ integrates a ResNet-based feature extraction module with a Variational Quantum Circuit (VQC) classifier, leveraging the complementary strengths of classical deep learning and quantum computing. Evaluations on two publicly available datasets, BreakHis and Bioimaging (BI), reveal significant performance improvements, achieving accuracies of 98.49% and 88.96%, respectively, compared to 97.36% and 87.96% achieved by its classical counterparts. Quantum circuit evaluations have been conducted on a quantum simulator (Aer simulator) as well as a noise-affected real quantum processor ( i b m _ b r i s b a n e ). Additionally, to gain deeper insights into the effectiveness of the ResQ model over classical models, a non-parametric statistical test, viz., the Friedman test, followed by the Nemenyi test for post hoc analysis, is performed. Furthermore, a detailed circuit-level analysis explores critical trade-offs, such as circuit depth, gate count, and qubit usage, providing unique insights into the practical deployment of hybrid classical-quantum models in medical imaging. The one-qubit shallow architecture of ResQ renders lower circuit complexities, making it amenable to Noisy-Intermediate Scale Quantum (NISQ) devices. These findings underscore the potential of quantum computing to revolutionize cancer diagnostics by enhancing both accuracy and computational efficiency. The relevant codes of the proposed architecture, ResQ, are publicly available on https://github.com/DVLP-CMATERJU/ResQ-Hybrid-Classical-Quantum .},
  archive      = {J_ASOC},
  author       = {Dibyasree Guha and Somenath Kuiry and Shyamali Mitra and Siddhartha Bhattacharyya and Nibaran Das},
  doi          = {10.1016/j.asoc.2025.113631},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113631},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ResQ: A hybrid classical-quantum model for efficient breast cancer image classification},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MambaCFFuse: Infrared and visible image fusion based on VMamba with complementary feature aggregation. <em>ASOC</em>, <em>183</em>, 113629. (<a href='https://doi.org/10.1016/j.asoc.2025.113629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of infrared and visible image fusion, the Convolutional Neural Network (CNN) and Transformer have achieved remarkable results with their local feature extraction capabilities and global attention mechanisms, respectively. However, CNN are limited by their receptive fields when acquiring global feature information, while Transformer suffers from computational challenges caused by the quadratic complexity of sequence length. To address these issues, this study proposes a Mamba architecture based on a state-space model, which can effectively overcome the shortcomings of CNN and Transformer. The proposed network employs an advanced Mamba as the encoder and specifically designs a Fusion State Space Block (FSSB) to fully learn the global spatial context information of the source images. Meanwhile, due to the deficiencies of the Mamba network in content perception, a novel Complementary Feature Aggregation Module (CFM) is innovatively presented to achieve the aggregation of scene and detail features. In addition, a new loss function is designed to constrain the network, which contains structural fidelity, intensity fidelity, texture loss, and perceptual loss. Experiment on public datasets show that our approach outperforms other fusion methods in both quantitative and qualitative results. Furthermore, ablation experiments also robustly confirm the superiority of our approach.},
  archive      = {J_ASOC},
  author       = {Bicao Li and Xingguang Li and Bei Wang and Yifan Du and Zhuhong Shao and Jie Huang and Jiaxi Lu and Jinbo Yang},
  doi          = {10.1016/j.asoc.2025.113629},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113629},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MambaCFFuse: Infrared and visible image fusion based on VMamba with complementary feature aggregation},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new glass segmentation network with indoor and outdoor panoramic images. <em>ASOC</em>, <em>183</em>, 113626. (<a href='https://doi.org/10.1016/j.asoc.2025.113626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glass is ubiquitous in everyday life, but its transparent and reflective properties hinder smart devices’ ability to understand their surroundings. For instance, autonomous vehicles may accidentally collide with glass, drones may experience interference when detecting the ground, and robot navigation systems may run into glass walls. These examples highlight the importance of effectively identifying and segmenting glass. Compared to pinhole images, panoramic images offer a wider field of view and richer spatial data, which enhance scene understanding. This makes panoramic images widely used across various applications. Therefore, research on glass segmentation based on panoramic images is particularly important. However, current models are not fully suited for glass segmentation in panoramic images. To address this issue, we propose a new panoramic glass image segmentation network, PGSNet. Additionally, we develop a high-resolution panoramic image glass segmentation dataset, PGSD, which includes both indoor and outdoor scenes. Experimental results show that PGSNet outperforms existing models while using only 15% of the parameters and computational effort. For mixed indoor and outdoor data, its evaluation metrics achieve an IoU of 90.0%, an MAE of 0.0063, and an F-score of 94.74%, all of which surpass those of other models. This technology is implemented in Siwei’s products, generating annual economic benefits of 50 million yuan over the past three years.},
  archive      = {J_ASOC},
  author       = {Qingling Chang and Xiaofei Meng and Yan Cui and Chi-Man Vong and Chuangquan Chen},
  doi          = {10.1016/j.asoc.2025.113626},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113626},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new glass segmentation network with indoor and outdoor panoramic images},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAPlanner: Dual-agent framework with multi-modal large language model for autonomous driving motion planning. <em>ASOC</em>, <em>183</em>, 113625. (<a href='https://doi.org/10.1016/j.asoc.2025.113625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In autonomous driving, motion planning is a key task for enabling vehicles to make autonomous decisions and take action. The goal of Motion Planning is to design an optimal trajectory from the current to the target state, considering other traffic participants, road conditions, and driving tasks, ensuring both safety and comfort. Current planning methods are ineffective in integrating multiple features from the perception layer and increase the risk of collisions due to the lack of trajectory constraints. Therefore, we propose a Dual-Agent Framework with a Multi-modal Large Language Model for Autonomous Driving Motion Planning. Specifically, we convert the surrounding traffic environment into Frenet Space, establishing spatial relationships between traffic participants and the map. The transformed information is represented in a bird’s-eye view (BEV), supplemented by textual prompts as inputs. Finally, we use trajectory generation agents and trajectory discrimination agents based on large language models to impose constraints on the generated trajectories. Additionally, we propose a data augmentation method for motion planning to further enhance system performance. Through evaluation on the large-scale nuScenes dataset, the proposed dual-agent framework based on large language models outperforms existing methods on various metrics. Our code is publicly available at https://github.com/dacilab/DAPlanner .},
  archive      = {J_ASOC},
  author       = {Pin Zhang and Ke Lin and Duantengchuan Li and Zixun Fu and Yuefeng Cai and Bing Li and Huan Yu and Ming Li},
  doi          = {10.1016/j.asoc.2025.113625},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113625},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DAPlanner: Dual-agent framework with multi-modal large language model for autonomous driving motion planning},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective evolutionary algorithm with constraint-compliant initialization for energy transport and urban logistics in electric vehicle routing. <em>ASOC</em>, <em>183</em>, 113624. (<a href='https://doi.org/10.1016/j.asoc.2025.113624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric vehicles (EVs) offer a new opportunity to enhance the efficiency of both transportation logistics and energy distribution. Integrating these dual objectives introduces complex optimization challenges due to interdependent constraints. This paper addresses the Vehicle Routing Problem with Time Windows integrated with Energy Transport (VRPTW-ET), where a fleet of EVs is used to serve customer demands while simultaneously transporting energy to (dis)charging facilities. We formulate the problem as a multi-objective optimization problem and design an evolutionary algorithm based on NSGA-II, featuring constraint-aware initialization and problem-specific operators for routing, time windows, and energy logistics. Our approach operates under realistic simplification, including static energy demands and travel costs, which help isolate the core challenges of the problem. Experimental results on modified benchmarks show that the proposed integrated approach consistently outperforms decoupled baselines, achieving up to 30% reduction in energy costs and 20% fewer vehicles used. These findings demonstrate the effectiveness of coordinated logistics-energy strategies in promoting cost-efficient and sustainable urban mobility.},
  archive      = {J_ASOC},
  author       = {Yue Xie and Kai-Fung Chu and Albert Y.S. Lam and Fumiya Iida},
  doi          = {10.1016/j.asoc.2025.113624},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113624},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective evolutionary algorithm with constraint-compliant initialization for energy transport and urban logistics in electric vehicle routing},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A double-convolution-double-attention transformer network for aircraft cargo hold fire detection. <em>ASOC</em>, <em>183</em>, 113622. (<a href='https://doi.org/10.1016/j.asoc.2025.113622'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional smoke and gas detection systems in aircraft cargo compartments tend to have high false-alarm rates, and deep learning models reliant on video imagery tend to entail substantial computation. This paper introduces a transfer learning approach, FE-DCDA-Transformer-TL. Color features are used to enhance fire images, so as to improve the recognition of fire smoke and flame targets. The Transformer network is simplified and combined with dual convolution and dual attention mechanism modules. Dual convolution reduces the number of structural parameters of the Transformer network, and dual attention enhances the features of fire smoke and flame. FE-DCDA-Transformer-TL is trained and evaluated on a custom aircraft cargo compartment fire dataset, and tested on a similar dataset. In experiments, the proposed model achieves 97.69% accuracy, 98% precision, 96.7% recall, an F1-score of 97.34%, 0.98 AUC, 3.44G FLOPS, 21.54M Params, and 0.61 FPS. Compared with state-of-the-art methods, the proposed model improves accuracy, precision, and recall by at least 32.91%, 28.60%, and 16.94%, respectively. FE-DCDA-Transformer-TL effectively solves the accuracy problem of aircraft cargo hold fire detection, providing strong support for fire detection.},
  archive      = {J_ASOC},
  author       = {Hai Li and Zhen-Song Chen and Sheng-Hua Xiong and Peng Sun and Hai-Ming Zhang},
  doi          = {10.1016/j.asoc.2025.113622},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113622},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A double-convolution-double-attention transformer network for aircraft cargo hold fire detection},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy minimizing capacitated covering vehicle routing problem. <em>ASOC</em>, <em>183</em>, 113620. (<a href='https://doi.org/10.1016/j.asoc.2025.113620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the Energy Minimizing Covering Capacitated Vehicle Routing Problem, which aims to minimize total energy consumed during delivery routes while satisfying customers’ demands. The problem considers a homogeneous fleet of identical vehicles stationed at a central depot and utilizes a concept of flexible delivery. Customers can receive parcels either directly during a vehicle visit or indirectly through designated neighboring customers. The flexible delivery can be particularly relevant for densely populated urban areas where parking is limited or situations where customers have restricted mobility or limited home presence. We formulate the studied problem as a mixed-integer programming problem. For large-scale instances, skewed and standard general variable neighborhood search heuristics are developed to tackle the problem efficiently. Extensive testing validates the model and assesses the effectiveness and efficiency of the proposed heuristics. The study also compares the skewed GVNS with metaheuristics of similar design, including GRASP and Iterated Local Search, to benchmark its performance. The results reveal that a skewed general variable neighborhood search heuristic is a viable approach for solving the problem, particularly for large-scale instances. Additionally, the study explores the trade-off between energy consumption and total travel distance. Results suggest that slight increases in travel distance can lead to significant energy consumption and CO2 emissions savings.},
  archive      = {J_ASOC},
  author       = {Milena Vukićević and Mustapha Ratli and Atika Rivenq and Raca Todosijević and Bassem Jarboui},
  doi          = {10.1016/j.asoc.2025.113620},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113620},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy minimizing capacitated covering vehicle routing problem},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrogate-assisted differential evolutionary algorithm with dynamic region exploration for expensive optimization problems. <em>ASOC</em>, <em>183</em>, 113619. (<a href='https://doi.org/10.1016/j.asoc.2025.113619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, surrogate-assisted evolutionary algorithms (SAEAs) are widely used to solve computationally expensive optimization problems. However, in complex multimodal scenarios, there exists a notable likelihood that the algorithm might not converge to the optimal solution, highlighting the need to enhance the exploration capability of SAEAs. Therefore, exploring or developing different regions at different stages of the algorithm is crucial. This study designs a surrogate-assisted differential evolutionary algorithm with dynamic region exploration (DREDE) for expensive optimization problems. To filter out current suspected regions where better solutions might exist, a new criterion is proposed that combines the fitness level of a sample and its distance from the current optimal solution. By constructing surrogate models, DREDE dynamically performs global, local, and suspected region searches. A method for improving model accuracy by calculating the mean of selected individuals is introduced. These strategies synergistically enhance the performance of DREDE, which is comprehensively compared with several advanced SAEAs on seven benchmark functions with varying dimensions and was used in the reducer design problem. Simulation results showed that DREDE has a promising future in addressing costly practical issues.},
  archive      = {J_ASOC},
  author       = {Shu-Chuan Chu and Libin Fu and Qingwei Liang and Lingping Kong and Jeng-Shyang Pan},
  doi          = {10.1016/j.asoc.2025.113619},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113619},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate-assisted differential evolutionary algorithm with dynamic region exploration for expensive optimization problems},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GLformer: Global–local spatio-temporal graph fusion transformer network for traffic flow prediction. <em>ASOC</em>, <em>183</em>, 113618. (<a href='https://doi.org/10.1016/j.asoc.2025.113618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the noteworthy advancements of traffic flow prediction methods based on GNN and attention mechanisms in recent years, there remains a deficiency in their capacity to concurrently contemplate the potential local–global spatial attributes and the pronounced long-term temporal periodicity of traffic data. Additionally, the propagation of information in global attention is predominantly conducted through weight calculation, which is deficient in its integration of multi-scale features. To address this challenge, we propose a model based on a global–local spatio-temporal graph fusion mechanism and an optimized transformer, named GLformer, which incorporates a spatio-temporal encoder and a temporal decoder. The input data is initially expanded into three parts to comprehensively and flexibly characterize the spatio-temporal features. In the spatio-temporal encoder, the dissemination of information is integrated into the attention computation process through global–local spatial attentional to capture the global dependence of the remote nodes with respect to the local fine-grained relationships. While temporal information in the preceding, current, and subsequent locations is considered through bidirectional graph neural network, and the feature representations are fused based on trainable weights. Subsequently, the temporal decoder executes a series of temporal feature extraction tasks by utilizing three spatio-temporal multi-attention mechanisms to integrate prior knowledge for prediction tasks. Extensive experiments are conducted on four open access datasets (PeMS03, PeMS04, PeMS07, PeMS08). Compared to other representative models, the results showed that the proposed approach achieved an average improvement of 6.83% in prediction accuracy. Additionally, significance of the introduced components is clearly demonstrated.},
  archive      = {J_ASOC},
  author       = {Kunxiang Deng and Xinyu Zhang and Haoxuan Kuang and Kemal Polat and Fayadh Alenezi and Jun Li},
  doi          = {10.1016/j.asoc.2025.113618},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113618},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GLformer: Global–local spatio-temporal graph fusion transformer network for traffic flow prediction},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neutrosophic c-means and pre-trained AlexNet FC8 deep learning for classification of carbon emissions levels in forest biomass. <em>ASOC</em>, <em>183</em>, 113613. (<a href='https://doi.org/10.1016/j.asoc.2025.113613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel hybrid model that integrates Neutrosophic c-Means (NCMs) and Convolutional Neural Networks (CNNs) to effectively classify the emission levels derived from Sentinel-2 images of forest biomass. Sentinel-2 data is primarily optimized for European and African regions, which may not capture the full variability of global forest ecosystems. So, Landsat-8 and Hyperion hyperspectral image data sets were implemented to provide additional spectral and temporal coverage across other regions. The model categorizes segmented regions into six distinct emission levels including no carbon, low, moderate, high, and very high. This paper presents a novel approach to image classification by combining Neutrosophic c-Means clustering with the FC8 architecture of AlexNet, a deep learning model. This innovative integration demonstrates superior performance compared to other combinations, such as k-means clustering with AlexNet and Neutrosophic c-Means clustering with VGGNet. The proposed method improves classification accuracy while significantly enhancing robustness against uncertainty inherent in multispectral imagery. The proposed model has four key phases which are data augmentation, segmenting carbon-related features, feature extraction, and classification. In the initial phase, Generative Adversarial Networks (GANs) are employed for data augmentation to enhance the robustness of the dataset. In the second phase, NCMs is utilized to segment carbon-related features in Sentinel-2 images accurately. In the third phase, feature extraction is carried out using a pre-trained AlexNet FC8 deep learning architecture from which significant features were selected. The last phase is classification, where several machine learning algorithms were used to classify carbon emission levels in forest biomass. The complexity time, inference time, and statistical validation via paired t-tests have been calculated, and the results indicate that the proposed model markedly enhances classification accuracy with the AlexNet FC8 deep learning architecture, demonstrating better performance when paired with the Random Forest (RF) machine learning classifier. In addition, the results validate the model's efficacy, achieving an average accuracy rate of 97.8 %. It further indicates that the proposed model exhibits high specificity, sensitivity, and a substantial Youden Index, affirming its potential for real-time and automated detection of carbon emission levels.},
  archive      = {J_ASOC},
  author       = {Sameh H. Basha and Mohammed Mostafa Ahmed and Noha MM. Abdelnapi and Rania Ahmed and Ashraf Darwish and Aboul Ella Hassanien},
  doi          = {10.1016/j.asoc.2025.113613},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113613},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neutrosophic c-means and pre-trained AlexNet FC8 deep learning for classification of carbon emissions levels in forest biomass},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source manifold space domain adaptation with a full-thresholding residual network for machinery fault diagnosis. <em>ASOC</em>, <em>183</em>, 113611. (<a href='https://doi.org/10.1016/j.asoc.2025.113611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised multi-source domain adaptation, which has been intensively investigated in recent years, is promising in handling fault diagnosis tasks when no labeling is available on the target datasets. Most approaches aim to learn the domain-invariant features of all domains in common feature spaces. However, addressing practical scenarios in which data comes from multiple domains with large shifts remains challenging. Hence, a new multi-source manifold space domain adaptation method (MMSDA) with a full-thresholding residual network is proposed for machinery fault diagnosis, in which specific domain-invariant features of the source and target domains are learned. First, a full-thresholding residual convolutional neural network (FTRCNN) is designed to extract useful features from both source and target domains, which are then projected into a specific domain feature space. Then, the proposed manifold neighbor consistency (MNC) domain alignment algorithm maps the feature space to a manifold space, ensuring that the samples maintain local neighbor geometric relations. Additionally, multi-kernel maximum mean discrepancy is used to reduce the inter-domain differences. Thus, the specific domain-invariant features of each source and target domain pair in the manifold feature space are extracted. Finally, the domain-specific classifier consistency (DSCC) loss is designed to minimize the shifts in all classifiers. Through experiments on three benchmarks, the proposed method demonstrates promising results on popular rotating machinery datasets for fault diagnosis.},
  archive      = {J_ASOC},
  author       = {Wenhua Chen and Jianbin Li and Sixing Wu},
  doi          = {10.1016/j.asoc.2025.113611},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113611},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-source manifold space domain adaptation with a full-thresholding residual network for machinery fault diagnosis},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized federated learning with fairness, robustness, and collaboration incentives. <em>ASOC</em>, <em>183</em>, 113610. (<a href='https://doi.org/10.1016/j.asoc.2025.113610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research work introduces RPFed, a personalized federated learning framework designed to enhance fairness, robustness, and collaboration. It utilizes secure model aggregation through blockchain technology and multifactor trust-based client selection. By integrating encryption methods, RPFed addresses communication security challenges during model exchanges between clients and the global server. Trustworthiness is assessed through various factors, including model performance, data quality, reliability, and communication efficiency. The blockchain ensures immutability of updates, providing tamper-proof records for each training round. The framework employs a convolutional neural network for image classification on MNIST and FashionMNIST datasets, effectively managing non-IID data distributions using Dirichlet allocation. Models are secured with symmetric encryption before aggregation. The framework evaluates robustness against adversarial attacks, such as the fast gradient sign method, and tests defense mechanisms to analyze the impact of adversarial training. Additionally, an incentive mechanism rewards clients based on their contributions, including model accuracy, data quality, and trustworthiness. Dynamic client selection is informed by trust scores, prioritizing reliable participants in future training rounds. The results demonstrate improved model performance and security, validating the framework with visualizations of data distributions and client performance.},
  archive      = {J_ASOC},
  author       = {Fahad Sabah and Yuwen Chen and Zhen Yang and Muhammad Azam and Nadeem Ahmad and Raheem Sarwar},
  doi          = {10.1016/j.asoc.2025.113610},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113610},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Personalized federated learning with fairness, robustness, and collaboration incentives},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Change detection in remote sensing images based on multi-tree genetic programming. <em>ASOC</em>, <em>183</em>, 113609. (<a href='https://doi.org/10.1016/j.asoc.2025.113609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection in remote sensing images plays a crucial role in applications such as environmental monitoring, urban planning, and disaster management. Accurately identifying and distinguishing changed areas within complex image data poses significant challenges. Existing methods often struggle with high false-positive rates and limited adaptability. This paper introduces a novel approach using multi-tree genetic programming (GP) to automate the construction of ensembles for change detection in remote sensing images. The method employs a unique multi-tree GP representation comprising three distinct trees that utilize difference, spectral, and texture features to identify changes. These trees are combined into an ensemble using a majority voting strategy to make predictions. The approach integrates multi-tree crossover and mutation strategies to generate new individuals, which are evaluated based on a fitness function derived from classification accuracy. To validate its effectiveness, the proposed multi-tree GP approach is evaluated on four benchmark datasets (SZTAKI, EGY_BCD, LEVIR_CD+, and S2Looking) and compared with eight methods. In most cases, the proposed approach achieves higher maximum change detection accuracy. Notably, on the SZTAKI dataset (Img_10), it achieves an accuracy of 96.11%, representing a 5.55% improvement over the worst baseline (KNN) and a 0.55% gain over the best baseline (SpectralFormer). Experimental results demonstrate that the proposed approach outperforms standard GP, as well as several classic classifiers and neural network based methods, establishing it as an effective tool for remote sensing change detection. The method’s capability of to leverage diverse features and integrate them through ensemble learning underscores its potential in enhancing change detection accuracy using remote sensing imagery.},
  archive      = {J_ASOC},
  author       = {Ying Bi and Tuo Zhang and Jintao Lian and Yaxin Chang and Jing Liang},
  doi          = {10.1016/j.asoc.2025.113609},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113609},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Change detection in remote sensing images based on multi-tree genetic programming},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangled multi-view graph neural network for multilingual knowledge graph completion. <em>ASOC</em>, <em>183</em>, 113605. (<a href='https://doi.org/10.1016/j.asoc.2025.113605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilingual knowledge graph completion (MKGC) uses limited seed pairs from diverse knowledge graphs (KGs) to enrich and complete a target KG. Unlike traditional knowledge graph completion (KGC) tasks that focus on a single KG, MKGC deals with multiple KGs described by diverse languages, imposing a higher level of heterogeneity due to the varying semantic meanings, syntactic structures, and regular expressions across different languages. Existing MKGC methods mainly rely on an end-to-end embedding function that maps multiple KGs into a shared latent space, using relation-aware graph neural networks (GNNs) to unify the contents of entities and relations with respect to their topological structures. However, such methods might not fully exploit the heterogeneity of multilingual KGs, as they overlook inherent details related to neighborhood entities and relations. To address these limitations, we propose a novel D isentangled M ulti-view G raph N eural N etwork (DMGNN) for MKGC. Specifically, our approach consists of two multi-view GNN modules: MKGC and multilingual KG alignment (MKGA) to facilitate knowledge transfer. Notably, DMGNN effectively captures the heterogeneity of multilingual KGs by learning graph features from three distinct views: entities, relations, and triples. Moreover, we introduce a disentangling mechanism wherein separate GNNs are employed to learn features from different views, mitigating feature interference. In addition, we incorporate an attention mechanism on each view GNN to distinguish the importance of neighborhood features. Extensive experiments on public multilingual datasets demonstrate the superiority of our proposed model over existing competitive baselines.},
  archive      = {J_ASOC},
  author       = {Bingbing Dong and Chenyang Bu and Ye Wang and Yi Zhu and Xindong Wu},
  doi          = {10.1016/j.asoc.2025.113605},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113605},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Disentangled multi-view graph neural network for multilingual knowledge graph completion},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial purification using random encoding networks. <em>ASOC</em>, <em>183</em>, 113604. (<a href='https://doi.org/10.1016/j.asoc.2025.113604'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have revealed vulnerabilities to adversarial examples, which can deceive models with high confidence. This has given rise to serious threats in security-critical domains. Adversarial defense methods have been extensively studied to counter adversarial attacks. Adversarial purification, as a major defense strategy, attempts to recover adversarial examples to clean counterparts by filtering out perturbations. However, many purification defenses struggle against white-box attacks where the target and defense models are known. Additionally, the training processes against specific attacks can compromise models’ adaptability to unknown attacks, and purification operations may destroy key features of inputs. In this paper, we propose the random encoding network (REN), which consists of a random encoding denoiser and a diverse classifier to enhance the robustness of adversarial purification defense models. The internal part of the denoiser leverages adversarial sparse coding to purify examples by filtering out perturbations and noise as much as possible while preserving critical features of inputs. The external part of the denoiser employs a dynamic random mechanism to implement random encoding, thereby enhancing the models’ uncertainty. Moreover, the classifier is subjected to a diversity constraint to promote variation among random sub-models. Experimental results demonstrate that REN exhibits strong defensive generalization capabilities, effectively countering adversarial examples across diverse attack types and settings. For the CIFAR-10 and SVHN datasets, the clean-trained REN achieves average adversarial accuracies of 63.26% and 59.78% against white-box attacks, while the adversarial-trained REN achieves 68.27% and 72.39%, respectively. When faced with unknown attack scenarios, REN is more effective than state-of-the-art defense methods.},
  archive      = {J_ASOC},
  author       = {Yuxin Gong and Shen Wang and Xunzhi Jiang and Tingyue Yu and Fanghui Sun},
  doi          = {10.1016/j.asoc.2025.113604},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113604},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adversarial purification using random encoding networks},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph representation learning based on fuzzy attention network. <em>ASOC</em>, <em>183</em>, 113602. (<a href='https://doi.org/10.1016/j.asoc.2025.113602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since different nodes in hypergraph have different contributions to hyperedge learning, it is a hot topic in the field of graph representation learning to introduce attention mechanism into hypergraph neural network to represent the importance of different nodes or hyperedges. However, the existing hypergraph attention network methods are mostly based on the assumption that node attributes and topology are perfectly given. In practice, node links and attributes are usually fuzzy concepts, which will lead to the uncertainty of feature learning and then affect the effectiveness of node representation. Based on this, inspired by the attention mechanism and fuzzy logic, this paper proposes a new fuzzy attention hypergraph neural network (HFATN), which is used to quantify the contribution ability of different vertices and hyperedges to better learn the vector representation of nodes in the hypergraph. HFATN consists of two modules: fuzzy attention vertex convolution and fuzzy attention hyperedge convolution. In the process of node convolution and hyperedge convolution, the attention mechanism based on hyperedge and node uncertainty characteristics is introduced respectively. By fuzzing the node set and hyperedge set on the hypergraph, the membership degree of nodes and hyperedges is calculated, which is used to extract effective features to deal with the uncertainty in the hypergraph data. Finally, we conduct experiments on three benchmark datasets for hypergraph node classification. The results show that compared with the latest TDHGNN model, the classification accuracy of FHATN on the three datasets is improved by 2.28 %, 8.99 % and 1.85 % respectively.},
  archive      = {J_ASOC},
  author       = {Liyan Zhang and Yifan Li and Xinghui Hao and Qinyu Zhang and Aimin Yang and Jingfeng Guo and Lei Zhang},
  doi          = {10.1016/j.asoc.2025.113602},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113602},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hypergraph representation learning based on fuzzy attention network},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale convolutional attention knowledge tracing from accumulative and structural evolution perspectives. <em>ASOC</em>, <em>183</em>, 113601. (<a href='https://doi.org/10.1016/j.asoc.2025.113601'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) aims to model and predict students’ evolving knowledge states based on their historical interaction data, serving as a core component in intelligent tutoring systems. However, existing KT models often overlook the diverse knowledge absorption and forgetting patterns that unfold over multiple time scales, limiting their ability to reflect real cognitive processes. According to cognitive development theory, knowledge undergoes accumulative and structural evolutions simultaneously during the learning process. To address these challenges, we propose the novel multi-scale convolutional attention knowledge tracing (MCAKT) method that integrates both the temporal and structural features of interaction sequences. Specifically, we utilize memory networks with adding and forgetting gates to create an initial knowledge state. Subsequently, we design a structural evolution module that forms a spatiotemporal graph for knowledge representation by applying aggregation operations to simulate spatial dependencies. A multi-scale convolutional attention network is presented to capture skill gains across different time scales to address varying knowledge absorption rates. Moreover, we introduce a filter-gated neural network to simulate knowledge decay, and employ an attention mechanisms to integrate the results from the evolution module for accurate next-question prediction. Experiments on three datasets validate the effectiveness of MCAKT, with an AUC improvement of 2.7% over state-of-the-art models on the ASSIST2017 dataset.},
  archive      = {J_ASOC},
  author       = {Tao Huang and Zhuoran Xu and Xinjia Ou and Huali Yang and Shengze Hu and Junjie Hu and Jing Geng},
  doi          = {10.1016/j.asoc.2025.113601},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113601},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale convolutional attention knowledge tracing from accumulative and structural evolution perspectives},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affective audio feature-based multimodal learning model for extracting video highlights. <em>ASOC</em>, <em>183</em>, 113600. (<a href='https://doi.org/10.1016/j.asoc.2025.113600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel multimodal learning model for video highlight detection that uniquely integrates visual features from video frames with affective audio features, including emotion labels, arousal, and valence. To our knowledge, this is the first approach to leverage all three affective components in audio for highlight detection. The model employs a Long Short-Term Memory (LSTM) network to fuse features extracted from Vision Transformer (ViT) for video and Wav2Vec 2.0 for audio. For the evaluation, we constructed a combined dataset with YouTube clips and KBS public broadcast videos. Experimental results show that our model significantly outperforms audio emotion-based and video-only baselines, achieving F1 Score improvements of approximately 27.35% and 66.15%, respectively. An ablation study further validates the contribution of affective audio and visual fusion.},
  archive      = {J_ASOC},
  author       = {Yujung Hwang and Chanhyeok Kim and Gangmin Park and Hyuk-Yoon Kwon},
  doi          = {10.1016/j.asoc.2025.113600},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113600},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Affective audio feature-based multimodal learning model for extracting video highlights},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing renewable transportation framework by an extended spherical fuzzy rough multi-criteria group decision making method. <em>ASOC</em>, <em>183</em>, 113599. (<a href='https://doi.org/10.1016/j.asoc.2025.113599'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regional transportation infrastructure plays a crucial role in driving economic growth by facilitating the smooth movement of goods, services, and people. It strengthens trade connections, links markets, attracts investment, and promotes job creation and business development. A well-developed transportation network enhances accessibility, reduces transportation costs, and increases overall productivity. Additionally, it supports tourism, fosters regional integration, and ensures the efficient distribution of resources. Collectively, these factors contribute to sustainable economic progress and enhance regional competitiveness. However, major challenges in developing efficient transportation infrastructure include insufficient funding and a lack of effective coordination among stakeholders. Additionally, political instability and regulatory challenges can delay or hinder project implementation. To address the issue of sustainable regional transport architecture, this research study aims to introduce a novel multi-criteria group decision making method based on various dominating and preference relations among criteria of available options for optimization of renewable transportation infrastructure. The presented outranking approach involves elimination of certain inferior options and then comparing the remaining alternatives through outranking relationships. This process helps to identify the most suitable choices while accounting for conflicting criteria and their relative significance. The Step-wise Weight Assessment Ratio Analysis method is used to assess the relative importance of criteria weights through sequential computation. The proposed method is further integrated with spherical fuzzy rough numbers to manage uncertainty by incorporating both lower and upper approximations. To illustrate the effectiveness of the approach, it is applied to a case study aimed at optimizing the regional transportation framework in Africa, with detailed computational steps provided. The alternative that consistently outranks others through both downward and upward distillation processes is identified as the optimal solution. The methods validity and reliability are confirmed by comparing its results with those obtained from other well-established techniques, where the consistent identification of the same optimal choice demonstrates the robustness of our approach. Finally, the advantages, limitations, and potential future applications of the proposed method are discussed.},
  archive      = {J_ASOC},
  author       = {Maheen Sultan and Muhammad Akram and Cengiz Kahraman},
  doi          = {10.1016/j.asoc.2025.113599},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113599},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing renewable transportation framework by an extended spherical fuzzy rough multi-criteria group decision making method},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic self-attention network based opinion formation over dynamic social networks with application to live-streaming. <em>ASOC</em>, <em>183</em>, 113598. (<a href='https://doi.org/10.1016/j.asoc.2025.113598'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opinion Dynamics (OD) is a widely used framework for studying the evolution of group opinions in complex social networks. However, most existing models primarily focus on static networks or small-scale dynamic networks, leaving large-scale dynamic networks largely underexplored. To address this gap, this paper investigates the evolution of group opinions and behavior prediction in large-scale dynamic social networks, using live-streaming data as a case study. Specifically, we propose two novel models: (1) a Dynamic Community Network (DCN) model, which constructs dynamic networks based on real-world big data, and (2) the Real-Time Dynamic Self-Attention Network Hegselmann–Krause (RT-DySAT-HK) model, which integrates Dynamic Graph Neural Networks (DGNNs) with OD to model the evolution of group opinions and predict behaviors. Through empirical analysis and simulations, we demonstrate that user behaviors in dynamic live-streaming networks are significantly influenced by community stability. Notably, during the early and middle stages of live-streaming, community size plays a critical role in attracting and retaining users. Moreover, the RT-DySAT-HK model proves highly effective in real-time group behavior prediction, particularly in large-scale dynamic networks. Compared to baseline models, it excels in extracting high-quality node representations and achieving accurate behavior predictions. Additionally, our findings reveal that the evolution of group opinions is influenced by multiple factors, including the contradictory effects of opinion weights and update speeds, which can lead to opinion polarization. Excessively slow update speeds may also result in opinion fragmentation. These insights contribute to a deeper understanding of OD in large-scale, dynamic environments and offer practical implications for predicting and managing group behaviors in real-world scenarios.},
  archive      = {J_ASOC},
  author       = {Haixia Mao and Yiyi Zhao and Min Xu and Jianglin Dong and Jiangping Hu},
  doi          = {10.1016/j.asoc.2025.113598},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113598},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic self-attention network based opinion formation over dynamic social networks with application to live-streaming},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel variable-precision granular-ball fuzzy rough set and its application in feature subset selection. <em>ASOC</em>, <em>183</em>, 113597. (<a href='https://doi.org/10.1016/j.asoc.2025.113597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Granular-ball computing is an efficient and interpretable theoretical method for multi-granularity data processing. Traditional fuzzy rough set models have some limitations in multi-granularity generation and dynamic characterization. On one hand, most of them lack effective multi-granularity generation methods, and artificially constructed approach may lead to information loss. On the other hand, when the attribute set changes, most fuzzy rough set models exhibit limitations in accurately characterize granularity information changes, and fails to capture correlational changes between attributes. Additionally, these models often lack noise resistance capabilities and flexibility in handling various fuzzy decision scenarios. To overcome these limitations, this paper combines granular-ball computing with fuzzy rough sets, and proposes a new variable-precision granular-ball fuzzy rough set model (VPGBFRS). First, granular-ball fuzzy similarity relations and granular-ball fuzzy neighborhood are used to characterize the relationship between samples. On this basis, a pair of variable-precision granular-ball approximate operators are presented. Second, we construct a variable-precision multi-granularity dependency function to obtain richer classification information, and enhance the model’s ability to capture intrinsic data structures. Finally, we design a forward attribute reduction algorithm based on the variable-precision significance in the sense of remain the classification ability unchanged. Numerical experiments conducted on 12 datasets demonstrate that, compared with four state-of-the-art attribute reduction algorithms, the proposed model exhibits superior performance, achieving significant improvements in both classification accuracy and the size of selected attribute set.},
  archive      = {J_ASOC},
  author       = {Yongxi Chen and Zhehuang Huang and Anhui Tan and Jinjin Li},
  doi          = {10.1016/j.asoc.2025.113597},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113597},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel variable-precision granular-ball fuzzy rough set and its application in feature subset selection},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked heterogeneous graph attention network for robust recommendation. <em>ASOC</em>, <em>183</em>, 113596. (<a href='https://doi.org/10.1016/j.asoc.2025.113596'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous Graph Neural Networks (HGNNs) have gained significant attraction in recommendation due to their proficiency in capturing and utilizing the diverse data types inherent in social network. Nevertheless, HGNNs are susceptible to noise and subtle adversarial attacks, as disturbances from connected nodes can cumulatively impact a target user/item node. To address this challenge, we propose the Masked Heterogeneous Graph Attention Network for Robust Recommendation (MHGAN), which aims to enhance the resilience of recommendation against adversarial attacks. Specifically, we achieve robust recommendation through two primary strategies: de-weighting and pruning. (1) De-weighting : We introduced meta-path based propagation constraint probability that effectively reduces the weights of perturbed edges, thereby enhancing the recommendation’s robustness. (2) Pruning : We design an innovative attention-based masking mechanism that selectively prunes malicious neighboring nodes using topology and node features to defend against adversarial attacks. Extensive experiments on three benchmark datasets show that MHGAN achieves up to a 12% improvement in robustness under different levels of random noise compared to state-of-the-art methods HGCL and GSLRrec. Codes are available at https://github.com/lxw106/MHGAN .},
  archive      = {J_ASOC},
  author       = {Lei Sang and Xingwang Li and Yu Wang and Yi Zhang and Shun Lian and Yiwen Zhang},
  doi          = {10.1016/j.asoc.2025.113596},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113596},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Masked heterogeneous graph attention network for robust recommendation},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method to detect the unpopular but valuable alternatives in large-scale linguistic group decision making. <em>ASOC</em>, <em>183</em>, 113594. (<a href='https://doi.org/10.1016/j.asoc.2025.113594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whether truth is in the hands of the minority deserves attention. This phenomenon is particularly prominent in large-scale group decision-making (LSGDM) scenarios, which is manifested as the neglect of alternatives that have potential value but are buried due to lack of widespread attention, that is, unpopular but valuable alternatives. To address this issue, this paper proposes a novel method based on social network analysis, which can enhance the priority recommendation of unpopular but valuable alternatives. Initially, to reveal potential connections between decision makers (DMs) which are dominated by unpopular alternatives, a new trust network construction algorithm is proposed with the sentiment measurement. And the impact of alternatives on the trust relations is discussed from the perspectives of DM’s sentiment and the popularity of alternatives, based on which the benchmark contribution and the substantive contribution of alternatives are proposed respectively. Subsequently, this paper introduces a sentiment similarity operator to improve the traditional modularity index, and proposes an improved Louvain community detection algorithm considering the DM’s sentiment, so as to promote the effective identification of minority communities. Furthermore, based on linguistic distribution assessment and network topology parameters, a multi-objective optimization model is constructed from both expectation and probability information loss to achieve opinion aggregation and alternative ranking. Finally, an experiment with the MovieLens dataset is given to illustrate the proposed LSGDM method. And a comparison analysis with the traditional common method is provided to verify the validity and effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Jiaqi Cai and Xiao Tan and Zaiwu Gong and Tong Wu},
  doi          = {10.1016/j.asoc.2025.113594},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113594},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A method to detect the unpopular but valuable alternatives in large-scale linguistic group decision making},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bringing balance to hand shape classification: Mitigating data imbalance through generative models. <em>ASOC</em>, <em>183</em>, 113586. (<a href='https://doi.org/10.1016/j.asoc.2025.113586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most sign language handshape datasets are severely limited and unbalanced, posing significant challenges to effective model training. In this paper, we explore the effectiveness of augmenting the training data of a handshape classifier by generating synthetic data. We use an EfficientNet classifier trained on the RWTH German sign language handshape dataset, which is small and heavily unbalanced, applying different strategies to combine generated and real images. We compare two Generative Adversarial Networks (GAN) architectures for data generation: ReACGAN, which uses label information to condition the data generation process through an auxiliary classifier, and SPADE, which utilizes spatially-adaptive normalization to condition the generation on pose information. ReACGAN allows for the generation of realistic images that align with specific handshape labels, while SPADE focuses on generating images with accurate spatial handshape configurations. Our proposed techniques improve the current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the limitations of small and unbalanced datasets. Additionally, our method demonstrates the capability to generalize across different sign language datasets by leveraging pose-based generation trained on the extensive HaGRID dataset. We achieve comparable performance to single-source trained classifiers without the need for retraining the generator.},
  archive      = {J_ASOC},
  author       = {Gaston Gustavo Rios and Pedro Dal Bianco and Franco Ronchetti and Facundo Quiroga and Oscar Stanchi and Santiago Ponte Ahón and Waldo Hasperué},
  doi          = {10.1016/j.asoc.2025.113586},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113586},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bringing balance to hand shape classification: Mitigating data imbalance through generative models},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised pre-training for feature extraction in cell type annotation of single-cell multi-omics data. <em>ASOC</em>, <em>183</em>, 113585. (<a href='https://doi.org/10.1016/j.asoc.2025.113585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell type annotation is paramount for subsequent biomedical research endeavors. Recently, supervised classification methods based on deep learning have demonstrated significant advancements in cell type annotation. However, challenges persist regarding how to efficiently leverage single-cell multi-omics data for cell type annotation and interpretation and how to endow models with adaptive generalization capabilities for efficient identification of rare cell types. We introduce scMoAnno, a methodology for cell type annotation based on single-cell multi-omics data. Leveraging pre-trained cross-attention networks, scMoAnno facilitates efficient mutual learning and fusion of genetic and epigenetic distributions in single-cell multi-omics data, enabling precise cell type annotation. Predictions conducted by scMoAnno on single-cell RNA sequencing (scRNA-seq) and single-cell assay for transposase-accessible chromatin sequencing (scATAC-seq) data demonstrate enhanced generalization capacity for the recognition of rare cell types through pre-trained feature fusion extraction and training of classifiers based on fused data inputs. Rigorously conducted experiments on four meticulously curated benchmark datasets illustrate that scMoAnno surpasses nine other state-of-the-art baseline models in overall performance. Furthermore, the increase in clustering scores, fused with multi-omics data and extracted by scMoAnno, is elucidated from the perspective of promoting downstream analysis of single cells, explaining how our model’s enhancement of cell type annotation performance yields positive effects. The source code and data are publicly available at https://github.com/ZhangLab312/scMoAnno .},
  archive      = {J_ASOC},
  author       = {Yongqing Zhang and Tianhao Li and Yuhang Liu and Hong Luo and Yugui Xu and Zixuan Wang and Quan Zou},
  doi          = {10.1016/j.asoc.2025.113585},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113585},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Supervised pre-training for feature extraction in cell type annotation of single-cell multi-omics data},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overlapping community detection based on graph attention autoencoder and self-trained clustering. <em>ASOC</em>, <em>183</em>, 113584. (<a href='https://doi.org/10.1016/j.asoc.2025.113584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for detecting overlapping communities often rely solely on the attributes of the nodes and the network structure, but fail to make full use of the similarity relationship between nodes and their neighbors. Additionally, these methods lack effective utilization of a priori information, making it challenging to extract information about community structure and nonlinear data information in overlapping communities. To address these issues, a method for detecting overlapping communities based on a graph attention autoencoder and self-training clustering (GASTC) is proposed. Firstly, GASTC utilizes the graph attention autoencoder for overlapping community detection. The fuzzy modularity maximization method is embedded into the graph attention autoencoder to perform soft allocation of network nodes. Simultaneously, targeted learning is conducted based on the weights assigned to nodes and their neighboring nodes to capture the interactions between overlapping nodes and different communities. GASTC also designed a structural similarity function suitable for detecting overlapping communities. The community structure within overlapping communities is extracted through a semi-supervised learning approach that not only utilizes label information to enhance the prior, but also introduces connection probabilities between nodes. This enables the calculation of the structural similarity between the known network structure and unlabeled nodes. Finally, subspace clustering is used for self-training, where the cluster labels is used to supervise the learning of potential node features and self-expression coefficient matrices. The obtained self-expression coefficient matrix is used to guide the division of clusters, to capture the non-linear data information in overlapping communities. Experimental results on six datasets demonstrate that GASTC can achieve higher accuracy in overlapping community detection tasks, especially in networks with more complex structures.},
  archive      = {J_ASOC},
  author       = {Weitong Zhang and Wenxu Wang and Ronghua Shang and Songhua Xu},
  doi          = {10.1016/j.asoc.2025.113584},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113584},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Overlapping community detection based on graph attention autoencoder and self-trained clustering},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMAFD: An interpretable multi-stage approach to flood detection from time series multispectral data. <em>ASOC</em>, <em>183</em>, 113582. (<a href='https://doi.org/10.1016/j.asoc.2025.113582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address two critical challenges in the domain of flood detection: the absence of a comprehensive flood detection framework and the lack of interpretable decision-making processes in explainable AI (XAI). To overcome these challenges, an Interpretable Multi-stage Approach to Flood Detection, IMAFD, has been proposed. The proposed IMAFD provides a comprehensive, efficient and interpretable solution suitable for large-scale remote sensing tasks and offers insight into decision-making. The proposed IMAFD approach combines the analysis of the dynamic time series image sequences to identify images with possible flooding with the static, within-image semantic segmentation. It combines anomaly detection (at both image and pixel level) with semantic segmentation. The flood detection problem is addressed through four stages: (1) at a sequence level: identifying the suspected images (2) at a multi-image level: detecting change within suspected images (3) at an image level: semantic segmentation of images into Land, Water or Cloud class (4) decision making. Our contributions are twofold. First, we provide a multi-stage holistic approach to flood detection, which efficiently reduces the number of images to be processed for semantic change detection in the later stage and reduces the processing time, which is critical for rapid disaster management such as flood. Secondly, the proposed semantic change detection method (stage 3) offers human users an interpretable decision-making process, while most explainable AI (XAI) methods provide post hoc explanations. The evaluation of the proposed IMAFD framework was performed on two datasets, WorldFloods and RavAEn. For both datasets, the proposed framework demonstrates a competitive performance compared to other methods while also providing interpretability. Specifically, the proposed IDSS+ outperformed U-Net on the Worldfloods dataset by 1.57% for IoU water and 0.19% for mIoU. On the Raven dataset, the proposed IMAFD first stage outperformed DINO by 0.16 for average precision, 0.25 for average recall and 0.21 for average F1. For the flood detection task, the proposed IMAFD achieved 100% accuracy, precision–recall and F1 score by setting threshold at 4. Furthermore, our framework significantly reduces the time required to process the image sequences on the Raven dataset by 212.66 s in total compared to the state-of-the-art framework.},
  archive      = {J_ASOC},
  author       = {Ziyang Zhang and Plamen Angelov and Dmitry Kangin and Nicolas Longépé},
  doi          = {10.1016/j.asoc.2025.113582},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113582},
  shortjournal = {Appl. Soft. Comput.},
  title        = {IMAFD: An interpretable multi-stage approach to flood detection from time series multispectral data},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-modal fusion architecture search for stock market forecasting. <em>ASOC</em>, <em>183</em>, 113581. (<a href='https://doi.org/10.1016/j.asoc.2025.113581'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock market forecasting can leverage various data sources, and recent deep-learning-based multi-modal methods have improved predictions by integrating them. However, there remains a need for an optimal fusion neural architecture and an efficient method to integrate essential features while reducing inter-modal redundancy. To address this, we propose a novel neural architecture search framework (Fin-MNAS) that effectively utilizes multi-source financial data, including chart images, news text, and time-series stock prices. Fin-MNAS is a hierarchical framework consisting of a backbone architecture search and a stepwise feature fusion search to reflect the intra-modal characteristics and emphasize the most important data modality. In addition, we introduce dynamic sparse tensor decomposition fusion modules that dynamically extract key intra-modal features and integrate features by considering their inter-modal relationships, thereby removing redundant and noisy information. Based on experiments involving three major stock indices, we demonstrate that Fin-MNAS outperforms all baseline models. In particular, the mean absolute error and mean absolute percentage error of our proposed framework are 7.74% and 10.17% lower, respectively, than that for the state-of-the-art multi-modal model BM-NAS.},
  archive      = {J_ASOC},
  author       = {Kisu Lee and Young Jin Kim and Minkyoung Kim and Ha Young Kim},
  doi          = {10.1016/j.asoc.2025.113581},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113581},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical multi-modal fusion architecture search for stock market forecasting},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A selective memory attention mechanism for chaotic wind speed time series prediction with auxiliary variable. <em>ASOC</em>, <em>183</em>, 113579. (<a href='https://doi.org/10.1016/j.asoc.2025.113579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind speed prediction is crucial for enhancing wind energy utilization and optimizing grid integration of wind power. Its chaotic nature and the lack of correlated variables make accurate prediction difficult. Most studies rely solely on past wind speed, limiting accuracy improvements. While wind power is highly correlated with wind speed, this correlation is reversely causal. The key challenge is effectively leveraging this reverse causality between wind power and wind speed to enhance prediction precision. This study proposed SMAMnet to address the challenge mentioned, a model that establishes its backbone network via proposed new attention mechanism. The convolution operation is employed to restructure features, besides, the frequency-domain transformation and selective state space model (SSM) serves for attention weights. The novelty of SMAMnet is characterized by the development of an adaptive frequency-domain selected attention weight operator to adaptively parse meaningful information in different frequency domain intervals. Taking 15 min and 1-hour mean absolute error as the standard, the actual wind speed prediction error is reduced by 68% and 49% compared with the classic LSTM algorithm. The feasibility of mining reverse causality to improve prediction accuracy was verified.},
  archive      = {J_ASOC},
  author       = {Ke Fu and Shengli Chen and Zhengru Ren},
  doi          = {10.1016/j.asoc.2025.113579},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113579},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A selective memory attention mechanism for chaotic wind speed time series prediction with auxiliary variable},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PortfolioZero: A stock portfolio model based on deep reinforcement learning. <em>ASOC</em>, <em>183</em>, 113578. (<a href='https://doi.org/10.1016/j.asoc.2025.113578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current studies of portfolio mainly use reinforcement learning methods to build models aimed at achieving high investment returns while minimizing risks from market uncertainties. Two main issues will be considered: First, the complexity of financial markets makes it challenging to capture asset price change patterns. Second, current research assumes stock prices accurately show all asset information, and historical prices alone can predict future trends. However, numerous external factors can influence future judgments. We introduce PortfolioZero, a novel model to address these problems. PortfolioZero utilizes three connected deep neural networks combined with a Monte Carlo Tree to discover patterns of financial assets. In the representation network, a Transformer-based model is used to embed financial price data to capture temporal dynamics and potential correlations, providing richer feature representations; the prediction network and Monte Carlo Tree Search are redesigned to handle the continuous action space. Furthermore, we use the StructBERT model to process financial text data, extracting market information into sentiment scores, which are used to reconstruct two reward functions to capture dynamic changes of the financial market. In experiments conducted on the China A-share market, we compared our model with traditional portfolio methods and cutting-edge deep reinforcement learning algorithms. PortfolioZero achieved an average annualized return rate of 21.21% across three portfolio types, outperforming SARL by 20.64% and DDPG by 41.97%, while sentiment-enhanced reward functions improved average annualized return rate by 35% compared to basic reward.},
  archive      = {J_ASOC},
  author       = {Haifeng Li and Mo Hai},
  doi          = {10.1016/j.asoc.2025.113578},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113578},
  shortjournal = {Appl. Soft. Comput.},
  title        = {PortfolioZero: A stock portfolio model based on deep reinforcement learning},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GHCW: A novel guarded high-fidelity compression-based watermarking scheme for AI model protection and self-recovery. <em>ASOC</em>, <em>183</em>, 113576. (<a href='https://doi.org/10.1016/j.asoc.2025.113576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) models are valuable and frequently face malicious tampering attacks, which require significant data and time to retrain when compromised. To address this issue, we propose a Guarded High-fidelity Compression-based Watermarking (GHCW) scheme for detecting and further recovering the tampered parameters, aiming to protect the model’s functional performance without the necessity for retraining. In GHCW, the watermark consists of an authentication probe for tamper detection, recovery bits, and ciphertext for model recovery. The authentication probe is generated by computing inherent characteristics of the parameters using hash algorithms and eigenvalue calculations. Different from existing works, the recovery bits are produced through a high-fidelity model compression technique, which concentrates key model information into fewer high-priority bits. This design enables the recovery mechanism to remain effective even under large-scale parameter tampering. To protect these bits, they are linearly encrypted with a key, generating a ciphertext that serves as a guard. To the best of our knowledge, we are one of the first to propose and implement this idea in the area of AI model protection. Experimental results demonstrate that GHCW excels in recovering models from tampering, especially large-scale parameter disturbance. Compared to existing methods, GHCW shows superiority in both model recovery and tolerance to tampering rate, tolerating tampering rates up to 70%, while existing methods recover at most 50%.},
  archive      = {J_ASOC},
  author       = {Tong Liu and Xiaochen Yuan and Wei Ke and Chan-Tong Lam and Sio-Kei Im and Xiuli Bi},
  doi          = {10.1016/j.asoc.2025.113576},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113576},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GHCW: A novel guarded high-fidelity compression-based watermarking scheme for AI model protection and self-recovery},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of EEG-based imagined speech decoding. <em>ASOC</em>, <em>183</em>, 113563. (<a href='https://doi.org/10.1016/j.asoc.2025.113563'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introducing a speech neuroprosthesis for individuals who have lost their ability to speak represents a breakthrough in the field of brain–computer interfaces. Central to this advancement is the concept of Imagined Speech, which attempts to decipher imagined speech from neural signals in the brain. Despite remarkable progress in multiple relevant fields such as neuroscience and artificial intelligence, integrating theories, data, and techniques into a clinically applicable framework remains challenging. Given the existing gap in comprehensive literature that synthesizes these multifaceted elements, this review examines a total of 180 papers published between 2014 and 2024. Electroencephalography (EEG) has emerged as the predominant brain imaging modality within this framework due to its practical advantages, and is therefore the focus of this review. By exploring key aspects including the neural basis, data acquisition, experimental paradigms, signal preprocessing, feature extraction, and decoding models, we provide a thorough analysis of the current literature and outline directions for future research based on potential challenges. The goal of this systematic review is to establish a cohesive and informative resource that serves as a reference for both newcomers and seasoned researchers entering the field of Imagined Speech.},
  archive      = {J_ASOC},
  author       = {Zhishuo Jin and Dongdong Li and Shengyao Huang},
  doi          = {10.1016/j.asoc.2025.113563},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113563},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A systematic review of EEG-based imagined speech decoding},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ant colony optimization of hyper-parameters in multi-head attention layer for time series forecasting. <em>ASOC</em>, <em>183</em>, 113562. (<a href='https://doi.org/10.1016/j.asoc.2025.113562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based models excel in capturing complex temporal dependencies in time-series forecasting, yet their permutation-invariant self-attention mechanism overlooks the critical role of sequence order. To address this limitation, we propose “ACOFormer”, a novel multi-head attention layer optimized through Ant Colony Optimization (ACO) for time-series prediction. Tuning Transformer hyper-parameters for power load forecasting is challenging due to the vast configuration space, which grows exponentially with the number of tunable parameters, rendering exhaustive searches impractical and resource-intensive. ACOFormer overcomes this challenge by integrating a dual-phase ACO algorithm with K-means clustering, achieving an 20.59% reduction in Mean Absolute Error (MAE) compared to a baseline Transformer model. Facing a configuration space exceeding 82 million permutations, ACOFormer employs a dual-phase iterative approach. Initially, cluster-based exploration leverages local pheromone updates to guide probabilistic hyper-parameter selection. Subsequently, global pheromone updates expand the search across the most promising hyper-parameter regions, informed by aggregated insights. This balanced approach enhances tuning efficiency and optimizes computational resource utilization, enabling ACOFormer to capture temporal nuances effectively. Empirical results demonstrate ACOFormer’s superior performance, with a 12.62% MAE reduction and a 10.54% improvement in Mean Squared Error (MSE) compared to Informer, alongside 27.33%–29.4% MAE reductions and 38.58%–47.51% MSE improvements against MICN, Reformer, and Autoformer over a two-hour forecast horizon. These results underscore ACOFormer’s effectiveness in improving the precision of Transformer-based models for time-series forecasting and highlight its potential for resource-efficient applications across diverse industries.},
  archive      = {J_ASOC},
  author       = {Mostafa Aliyari and Enrique Lopez Droguett and Javad Barabady and Yonas Zewdu Ayele},
  doi          = {10.1016/j.asoc.2025.113562},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113562},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ant colony optimization of hyper-parameters in multi-head attention layer for time series forecasting},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-fused construction portfolio investment system with risk hedging using machine learning and long-short strategies. <em>ASOC</em>, <em>183</em>, 113555. (<a href='https://doi.org/10.1016/j.asoc.2025.113555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing consistently profitable investment strategies presents a considerable challenge within the intricate and continuously evolving financial landscape. This manuscript introduces an automated investment model meticulously designed to optimize returns through dynamic portfolio management, with a focus on comprehensive short-term portfolio decision-making. Leveraging advanced methodologies, including machine learning, natural language processing (NLP), and deep learning techniques, this study develops a robust system capable of integrating various data sources, such as extensive financial indicators, technical analysis metrics, and sentiment analysis derived from NLP-based models. We delineate essential financial factors using extreme gradient boosting and are trained on historical transaction data, financial indices, and detailed technical indicators. Furthermore, the model incorporates transformer-based NLP techniques to extract sentiment and market insights from textual data. The system autonomously identifies optimal long-short portfolio combinations and trading opportunities, employing dynamic weight adjustments informed by predictive analytics and technical indicators. Simulation results demonstrate that dynamically weighted portfolios can effectively respond to diverse economic conditions, yielding stable returns and reduced volatility, regardless of market direction. Although the scope of this study is confined to the listed construction sector in Taiwan, backtesting substantiates the robustness and potential scalability of the proposed methodology. Future research may seek to explore broader market applications to further validate the generalizability of the approach; nonetheless, the current findings already indicate significant promise for practical implementation in medium-frequency trading strategies.},
  archive      = {J_ASOC},
  author       = {Jui-Sheng Chou and Kai-Chun Lin and Tran-Bao-Quyen Pham},
  doi          = {10.1016/j.asoc.2025.113555},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113555},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AI-fused construction portfolio investment system with risk hedging using machine learning and long-short strategies},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal multi-agent debate-based dialectical knowledge transfer learning. <em>ASOC</em>, <em>183</em>, 113551. (<a href='https://doi.org/10.1016/j.asoc.2025.113551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferring knowledge from large to smaller models can enhance their reasoning abilities and expedite deployment. However, for multi-modal complex reasoning tasks, existing large models often produce knowledge with factual errors. Transferring this flawed multi-modal knowledge to smaller models can lead to error accumulation, thereby degrading their performance. To address this challenge, we propose a novel Multi-modal Multi-agent Debate-based Dialectical Knowledge Transfer Learning framework ( M2D-DKTL ). Initially, we introduce a multi-modal multi-agent debate framework to uncover deep logical relationships and enhance knowledge quality. Then, we propose using chain-of-debate to condense dialectical knowledge and transfer it to smaller models, thereby improving their complex reasoning ability. Experimental results show that debates can enhance the model’s understanding and reasoning in complex scenarios. Moreover, our method achieved average improvements of +4.3%, +3.8%, and +2.6% on the MMMU, MathVista, and CMMMU datasets, respectively, validating its effectiveness and generalizability. Further analysis reveals that our method enhances the logical reasoning and deep analysis capabilities of smaller models.},
  archive      = {J_ASOC},
  author       = {Haotian Wang and Weijiang Yu and Xiyuan Du and Qianglong Chen and Zheng Chu and Lian Yan and Jingchi Jiang and Yi Guan},
  doi          = {10.1016/j.asoc.2025.113551},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113551},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-modal multi-agent debate-based dialectical knowledge transfer learning},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label repopulation through evolutionary computation for addressing challenging multilabel problems. <em>ASOC</em>, <em>183</em>, 113523. (<a href='https://doi.org/10.1016/j.asoc.2025.113523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel classification has recently attracted considerable attention from the research community in data mining. Multilabel classification is concerned with learning where each instance can be associated with multiple classes (or labels). One of the characteristics of many multilabel problems is the low density of relevant labels. This fact makes the classification problem challenging, as there is only sparse evidence to predict most labels. In this paper, we propose a new method for improving the performance of any multilabel method using a label repopulation strategy. We assume that denser datasets may improve the performance of the algorithms. This assumption is based on the fact that adding new labels may make learning the separation surfaces easier; we do not assume that the added labels correspond to actual relevant labels absent from the dataset due to erroneous labeling. Given the uncertainty of which new relevant labels could enhance the learned models, we approach the task as an optimization problem, utilizing evolutionary algorithms to identify the optimal set of new labels. An extensive comparison using 45 datasets and nine different classification models demonstrates the advantageous performance of our approach. The method is applicable to any multilabel classification model.},
  archive      = {J_ASOC},
  author       = {Nicolás García-Pedrajas and Juan A. Romero del Castillo and Aida de Haro-García},
  doi          = {10.1016/j.asoc.2025.113523},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113523},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Label repopulation through evolutionary computation for addressing challenging multilabel problems},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale niching based differential evolution for feature selection on high-dimensional data. <em>ASOC</em>, <em>183</em>, 113510. (<a href='https://doi.org/10.1016/j.asoc.2025.113510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is regarded as a combinatorial optimization problem that aims to obtain the suitable feature subsets from large number of features and improve their quality. Evolutionary algorithm (EA) is an effective way to address FS problem. However, most of EA-based FS methods face well-known challenges, such as insufficient information exchange and significant redundancy among features that easily fall into the local optima. Especially in high-dimensional data, the presence of numerous feature combinations expands the search range of solution space, making it more difficult to search for the optimal feature subsets. As a result, a multi-scale niching based differential evolution (MNDE) method is proposed for FS on high-dimensional data. Specifically, the population is initially divided into several parts from the perspective of individual and dimension using multi-scale niching technique. Then, mutation operator with global–local optima and crossover operator with fragment incorporation are employed to enhance the information exchange within each niche, while also facilitating effective information exchange between different niches. In addition, the ranking-based selection strategy is used to choose the elite individuals based on their fitness rankings, so that superior solutions are remained while eliminating poorly performing ones. The experimental results show that the proposed MNDE achieves higher classification accuracy while selecting fewer number of features compared with other state-of-the-art methods on 13 benchmark high-dimensional datasets.},
  archive      = {J_ASOC},
  author       = {Biyu Yin and Mingwei Wang and Wei Han and Kaiyuan Yang and Zhiwei Ye and Maolin Chen},
  doi          = {10.1016/j.asoc.2025.113510},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113510},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale niching based differential evolution for feature selection on high-dimensional data},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised image stitching based on generative adversarial networks and feature frequency awareness algorithm. <em>ASOC</em>, <em>183</em>, 113466. (<a href='https://doi.org/10.1016/j.asoc.2025.113466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel unsupervised approach to image stitching that integrates Generative Adversarial Networks (GAN) and a Feature Frequency-Aware Network (FFAN) combined with a Convolutional Block Attention Module (CBAM) for robust image alignment and fusion. The proposed method specifically addresses common issues in traditional and existing deep learning-based image stitching techniques, such as challenges posed by weak textures, low-light conditions, and significant parallax disparities. The approach employs GANs for initial image enhancement, significantly improving image quality and detail representation. Subsequently, the FFAN module extracts multi-scale features, utilizing frequency-aware decomposition to effectively process both low-frequency global structures and high-frequency fine details separately. These features are further refined through a CBAM with shared weights, enabling precise attention to critical areas during the homography estimation and image warping stages. Moreover, a Transformer-based Deep Homography Estimation (TDHE) module is introduced to enhance global contextual understanding, thus improving the alignment accuracy in scenes with large disparities. Comprehensive experimental results on both real-world and synthetic datasets demonstrate superior performance compared to several classical and contemporary stitching algorithms. Quantitative assessments based on PSNR and SSIM metrics confirm significant improvements in stitching accuracy and robustness, particularly in complex scenarios involving wide baselines and considerable parallax. Ablation studies further underscore the individual contributions and synergistic effectiveness of the proposed modules.},
  archive      = {J_ASOC},
  author       = {Chunbin Qin and Xiaotian Ran and Dehua Zhang},
  doi          = {10.1016/j.asoc.2025.113466},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113466},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised image stitching based on generative adversarial networks and feature frequency awareness algorithm},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-layer state space network for multivariate time series forecasting with dimensional interdependency. <em>ASOC</em>, <em>183</em>, 113452. (<a href='https://doi.org/10.1016/j.asoc.2025.113452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate Time Series Forecasting (MTSF) presents significant challenges due to the need to capture intricate inter-dimensional dependencies and temporal dynamics. Existing Transformer-based models often suffer from quadratic computational complexity and limited efficacy in modeling long-range temporal dependencies. To overcome these limitations, we propose Mixed Spatio-Temporal Mamba (MST-Mamba), a novel architecture that integrates mixed-dimensional dependency encoding, sliding-window patch processing, and hierarchical state-space modeling. Specifically, a spectral correlation-based mixed-dimensional encoder that effectively captures cross-dimensional dependencies by analyzing amplitude spectrum similarities. Intra-patch temporal features are modeled using a Bidirectional Structured State Space architecture, enabling efficient representation of local temporal patterns. For inter-patch temporal modeling, the Mamba module is employed, facilitating robust and scalable long-range dependency extraction. We evaluate the MST-Mamba architecture on several real-world datasets, Experimental results demonstrate that our model achieves an improvement in AVG MSE of 12.9%, 4.6%, 3.1%, and 3.3% respectively across PEMS07, Traffic, Weather and Electricity datasets. Moreover, our model demonstrates superior performance in handling long sequences with lookback information and exhibits enhanced robustness in noisy environments.},
  archive      = {J_ASOC},
  author       = {Ning Zhou and Linfu Sun},
  doi          = {10.1016/j.asoc.2025.113452},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {113452},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual-layer state space network for multivariate time series forecasting with dimensional interdependency},
  volume       = {183},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safety guardian of intelligent transportation: Fisheye image based blind zone detection for super large articulated bus (SLAB). <em>ASOC</em>, <em>182</em>, 113660. (<a href='https://doi.org/10.1016/j.asoc.2025.113660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Super Large Articulated Buses (SLAB), as a complement to road traffic in big cities, brings great convenience to residents. However, due to its body length of more than 30 m and its unique driving characteristic on the left side of the road, blind zone detection during right turns has garnered significant attention. This paper proposes an intelligent method using fisheye images to address such issues. The proposed strategy is primarily divided into three steps. Firstly, fisheye cameras are mounted on the side of the SLAB’s body to capture fisheye images, and the dual longitude method is employed for distortion correction. Secondly, a vehicle detection method based on Single Shot Multibox Detector (SSD) is proposed, which combines Squeeze-and-Excitation (SE) attention mechanism, Feature Pyramid Network (FPN) and Multi-branch Dilation Block (MDB), called MDB-SSD. Through ablation experiments, the mean average precision ( mAP ) of this model is observed to increase by 5.31 % on the BDD100k dataset and 7.68 % on the VOC dataset when compared to the baselines. Specifically, the mAP of the MDB-SSD model reaches 40.13 % on the BDD100k dataset and 83.42 % on the VOC dataset, demonstrating significant improvement in detection accuracy. The detection of fisheye images exhibits good robustness, enhancing vehicle detection performance for the blind zone during right turns in SLAB. Finally, based on fisheye images, the proposed cross-longitude distance measurement method demonstrates an average detection error of 3 % for forward distance and 9.8 % for lateral distance, providing convenience for SLAB’s assisted driving. The main focus of this paper provides a solution for the safe operation of SLAB.},
  archive      = {J_ASOC},
  author       = {Fujian Liang and Yongzhao Han and Jirui Wang and Xin Wang and Hongjie Tang and Jiaoyi Wu and Zutao Zhang},
  doi          = {10.1016/j.asoc.2025.113660},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113660},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Safety guardian of intelligent transportation: Fisheye image based blind zone detection for super large articulated bus (SLAB)},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Denoising diffusion spatial-temporal residual multi-graph convolutional network for traffic flow prediction. <em>ASOC</em>, <em>182</em>, 113656. (<a href='https://doi.org/10.1016/j.asoc.2025.113656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world traffic flow prediction remains a significant challenge in intelligent transportation systems. Current methods rely heavily on preprocessed noisy data for model training, yet models trained on such smoothed data often fall short in real-world prediction accuracy. This limitation stems from the overly smoothing effect of interpolation and other preprocessing techniques, which lead to the loss of crucial historical features in the noisy data. To address this challenge, a novel D enoising d iff usion S patial- T emporal residual M ulti- G raph convolutional network (DiffSTMG) is proposed for traffic flow prediction. Specifically, DiffSTMG consists of Information Encoding (IE) module, Gated Causal Convolution (GCC) module, Multi-Graph Convolution (MGC) module, Fully Connected (FC) layer and residual connection. The core of the IE module consists of the Denoising Diffusion (DeDiff) module, which is designed to recover the latent features of the noisy data based on diffusion convolution, and the GCC module, which is designed to enhance the temporal features of the noiseless data based on the gating mechanism and the causal convolution. The output of the IE module, after the further enhancement of the temporal features by the GCC module, is passed through the MGC module to obtain the dynamic spatial features. The MGC module is based on the graph convolution network of static and dynamic graphs to further enhance the dynamic spatial features under the influence of historical patterns, locations, and holidays. Extensive experiments on six real-world datasets validate that the DiffSTMG prediction accuracy outperforms state-of-the-art models.},
  archive      = {J_ASOC},
  author       = {Yinxin Bao and Qinqin Shen and Jinghan Xue and Weiping Ding and Quan Shi},
  doi          = {10.1016/j.asoc.2025.113656},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113656},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Denoising diffusion spatial-temporal residual multi-graph convolutional network for traffic flow prediction},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-branch fusion network with mutual learning for 12-lead electrocardiogram signal classification. <em>ASOC</em>, <em>182</em>, 113638. (<a href='https://doi.org/10.1016/j.asoc.2025.113638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the vast majority of methods use 12-lead electrocardiograms as a two-dimensional array as network input, and use deep neural networks to extract inter-lead correlation features. However, extracting intra-lead differential features is particularly important, as not every lead’s feature carries equal significance for classification. In this paper, we propose a dual-branch fusion network with 12-lead separation and combination, integrating the idea of mutual learning. The dual-branch network extracts differentiated and correlated features respectively and fuse them for classification. Each branch network is not only supervised by the ground truth but also referenced the learning experience of another branch network to further improve its classification ability. To address data imbalance, we introduced a category weighted binary focal loss to increase the attention of the network to the samples with few classes. We validated the proposed method on two publicly available multi-label datasets. Compared to the baseline model, our model has significantly improved in performance, demonstrating strong competitiveness and validating the effectiveness of our method. The experimental results show that our proposed method surpasses existing methods and achieves state-of-the-art performance. The method enables lightweight deployment on wearable devices, such as 12-lead ECG garments and smartwatches, facilitating real-time arrhythmia monitoring.},
  archive      = {J_ASOC},
  author       = {Ke Ma and Tao Zhang and Hengyuan Zhang and Wu Huang},
  doi          = {10.1016/j.asoc.2025.113638},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113638},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-branch fusion network with mutual learning for 12-lead electrocardiogram signal classification},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted classification of deep and traditional histogram-based features with kernel representation for robust facial expression recognition. <em>ASOC</em>, <em>182</em>, 113630. (<a href='https://doi.org/10.1016/j.asoc.2025.113630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extraction is crucial in facial expression recognition (FER) systems. This paper introduces a novel descriptor called Local Edge-based Decoded Binary Pattern (LEDB) and a lightweight 1D-CNN named Statistical Local Feature-based Network (SLFNet) to overcome limitations of deep learning approaches, such as the need for complex deep networks, high computational demands, and large training datasets. To enhance feature extraction stability, derivative-Gaussian filters are applied across four directions, yielding more robust representations. In the resulting gradient space, inter-pixel relationships are extracted to generate LEDB micropatterns, which are moderately sized yet highly discriminative, effectively capturing low-level features. Additionally, a compact 1D-CNN with 208k parameters learns high-level features from emotion-related facial regions, enhancing robustness against variations in resolution, noise, and occlusion. High-level and low-level features are fused through a weighted kernel representation strategy to increase resilience to outliers. Extensive experiments on six FER datasets—CK+ , FACES, KDEF, MMI, JAFFE, and RAF-DB—show that the proposed LEDB, SLFNet, and their combination outperform traditional handcrafted descriptors and recent deep learning techniques across various evaluation protocols. Furthermore, the system remains robust in challenging scenarios, such as those with low resolution, noise, or occlusion, which are common hurdles in FER. Code will be made available at: https://github.com/Morteza-Najm/TDF-WKR-FER},
  archive      = {J_ASOC},
  author       = {Morteza Najmabadi and Mina Masoudifar and Ahmad Hajipour},
  doi          = {10.1016/j.asoc.2025.113630},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113630},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Weighted classification of deep and traditional histogram-based features with kernel representation for robust facial expression recognition},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of imitation learning fusion algorithm for mobile robotic arm control. <em>ASOC</em>, <em>182</em>, 113628. (<a href='https://doi.org/10.1016/j.asoc.2025.113628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the considerable potential of artificial intelligence technology in industrial applications, it still faces challenges such as high data requirements, limited generalization capabilities, and concerns with safety and stability. These issues become particularly prominent in the execution of precision operations such as opening button-lock cabinet doors during robotic inspection tasks. The tasks involve complex environmental perception, dynamically changing operational settings, high stability requirements, and the challenge of generalization, all of which necessitate the integration of multiple advanced algorithms into a fusion system design. This research focuses on the system design and algorithm integration challenges for mobile robotic arms in inspection tasks, proposing a Deep Meta-Imitation Learning (DMIL) algorithm that combines deep meta-learning with imitation learning (IL) to enhance the adaptability and efficiency of mobile robotic arms in such tasks. Expert trajectories were generated and analyzed in the CoppeliaSim simulation environment, and actual interactions were evaluated. The study employs a deep learning-based 6D pose estimation method to determine the position and orientation of button locks, with visual recognition of key operational reference points. In the imitation learning phase, the operational strategies of the robotic arm are enhanced by combining Adversarial Inverse Reinforcement Learning (AIRL) and Variable Impedance Control (VIC) technologies, supported by expert-guided trajectories and force feedback data from real-world environments. Additionally, a Latent Embedding Optimization (LEO) module is introduced into the deep meta-learning framework, enabling the model to quickly adapt to new tasks, significantly improving its generalization ability. Experiments were conducted in the CoppeliaSim simulation environment and on a mobile robotic arm platform, focusing on the recognition process, trajectory planning, and compliance control management. To assess the algorithm's effectiveness, three button-lock cabinet door-opening tasks of varying difficulty were executed. The experimental results demonstrate that the mobile robotic arm was able to accurately locate and compliantly open multiple button locks, showcasing the practicality and feasibility of this approach in advancing robotic precision operations in inspection tasks.},
  archive      = {J_ASOC},
  author       = {Yang Wang and Xiaoling Yan and Liming Wang and Wei Pan and Fei Song and Xinlei Zhou},
  doi          = {10.1016/j.asoc.2025.113628},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113628},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design of imitation learning fusion algorithm for mobile robotic arm control},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D CAD model dynamic clustering based on inertial feature encoder. <em>ASOC</em>, <em>182</em>, 113627. (<a href='https://doi.org/10.1016/j.asoc.2025.113627'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of three-dimensional (3D) computer-aided design (CAD) models of mechanical parts in cyber manufacturing has experienced explosive growth. Classified CAD model shape knowledge based on induction is conducive to model retrieval, design reuse, and machining reuse. However, 3D CAD feature extraction primarily utilizes projected views, point clouds, voxels, and meshes for dimensionality reduction. Nonetheless, complex processes and high computational costs impede effective shape analysis. Traditional distance measures in data spaces or shallow linear embedded spaces are susceptible to errors when assessing similarity in data clusters. Furthermore, as the size of the database increases, data distribution may change in dynamic clustering, leading to data drift. This paper proposes an automatic unsupervised learning shape classification method based on deep embedding for 3D mechanical part CAD models. First, an inertial feature descriptor that effectively represents shape characteristics was established to extract the multidimensional moment of inertia of the 3D CAD model. Second, the inertial feature data space was nonlinearly mapped to a low-dimensional feature space, and the clustering accuracy was improved through the joint training of the encoder and clustering layers. Simultaneously, we revealed the influence of eps and min samples of Density-based Spatial Clustering of Applications with Noise (DBSCAN) algorithm on the clustering distribution of the CAD models. Third, adding new data can effectively achieve dynamic clustering based on the original clustering results. This paper explains the potential problems of fuzzy clustering boundaries that may arise from adding new data. Experimental data showed that the silhouette coefficient calculated by the proposed method is 0.78, and the normalized mutual information is 0.82, which has an excellent automatic classification effect.},
  archive      = {J_ASOC},
  author       = {Fangwei Ning and Zirui Li and Jiaxing Lu and Yixuan Wang and Yanxia Niu and Yan Shi},
  doi          = {10.1016/j.asoc.2025.113627},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113627},
  shortjournal = {Appl. Soft. Comput.},
  title        = {3D CAD model dynamic clustering based on inertial feature encoder},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based feature fusion and forecasting approach for stock market prediction. <em>ASOC</em>, <em>182</em>, 113623. (<a href='https://doi.org/10.1016/j.asoc.2025.113623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, stock market prediction is a current renowned research topic. The existing prediction approaches are on the basis of the econometric and statistical approaches. Nevertheless, these approaches are complex to pact with non-stationary time series data. Thus, this study develops a new approach for stock market prediction utilizing a hybrid deep learning approach. In this work, the pre-processing stage is done initially with the assistance of yeo-jhonson transformation and padding-based Fourier transform (FT) denoising model. After that, technical indicators, like Williams’s %R, Rate of Change, Triple Exponential Moving Average (TRIX), Average Directional Index (ADX), Average True Range (ATR), and Relative Strength Index (RSI), are extracted. Subsequently, the feature fusion procedure is done by utilizing Morisita's overlap index and Deep Belief Network (DBN) model. Lastly, stock market forecasting is done by a hybrid approach integrating Deep Long Short-Term Memory (Deep LSTM) and Multi-Layer Perceptron (MLP). Moreover, the proposed model is compared with the conventional models, such as Padding-based Fourier Transform Denoising (P-FTD)+Recurrent Neural Network (RNN), Feedforward Neural Network (FNN)+Back-propagation Neural Network (BPNN), Residual-CNN-Seq2Seq (RCSNet), Long short-term memory (LSTM), Competitive Feedback Particle Swarm Optimization-based Deep Recurrent Neural Network (CFPSO-based Deep RNN) and Rider Deep LSTM & Deep RNN. Finally, the experimentation analysis states that the performance of Deep LSTM-MLP is superior to conventional approaches regarding the MSE, RMSE and MAE with the values of 0.113, 0.337, and 0.169.},
  archive      = {J_ASOC},
  author       = {Tzu-Chia Chen},
  doi          = {10.1016/j.asoc.2025.113623},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113623},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning-based feature fusion and forecasting approach for stock market prediction},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Game theory-based electricity pricing and microgrids management using online deep reinforcement learning. <em>ASOC</em>, <em>182</em>, 113621. (<a href='https://doi.org/10.1016/j.asoc.2025.113621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses a bi-level problem involving a retailer and multiple residential microgrids. The retailer, at the upper level, disseminates selling and buying electricity price signals to maximize profit, while microgrid agents, at the lower level, manage their resources based on these signals to minimize costs. Additionally, a distribution system operator oversees network constraints. The interaction between microgrids and the retailer is modeled as a Stackelberg game, allowing for double-sided trading. To deal with uncertainties related to sustainable resources, loads, and wholesale market prices, a hybrid fuzzy/stochastic optimization (HFSO) approach is employed. This method combines fuzzy chance-constrained programming at the upper level with risk-neutral programming at the lower level. Due to privacy-preserving concerns, the deep reinforcement learning approach is used to solve this problem. This approach is evolved to online learning to prevent data drift, especially when the load profile changes, and attain an acceptable answer quickly. To prove this claim, the ability to predict profit over a relatively long period is investigated for both the offline learning method and the proposed online learning method. The results show that the offline learning method has a prediction error of 15.54 %, while the online learning method has only a 1.8 % error. Specifically, the online learning method can predict the profit that the retailer will obtain with 96.75 % accuracy, while the offline learning method's prediction fails with −150.64 % accuracy. Also, the online learning method can predict each microgrid’s power transactions with more than 89.6 % accuracy.},
  archive      = {J_ASOC},
  author       = {Mahdi Shademan and Ali Azizi and Shahram Jadid},
  doi          = {10.1016/j.asoc.2025.113621},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113621},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Game theory-based electricity pricing and microgrids management using online deep reinforcement learning},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimized convolutional neural network with a novel spherical triangular fuzzy pooling layer for an algorithmic trading model. <em>ASOC</em>, <em>182</em>, 113617. (<a href='https://doi.org/10.1016/j.asoc.2025.113617'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pooling layers of Convolutional Neural Networks (CNNs) are applied to reduce the dimensionality of input features while overcoming the overfitting issue. Typically, average pooling assigns the same weight to each activation in the pooling region. However, each region in an input image may not be equally crucial in financial data. To overcome this drawback, we propose a novel pooling layer that incorporates spherical fuzzy logic to account for the inherent uncertainty of financial data in a novel algorithmic trading model. To that end, we propose two minor models for prioritizing activations based on technical indicators of extreme points and time positions, respectively. Based on these two models, we propose an Indicator-Time-based Spherical Triangular Fuzzy pooling (ITSFP) model to capture critical market signals with greater accuracy and adapt more effectively to market conditions. To optimize the algorithmic trading model, a Genetic Algorithm (GA) has been designed to fine-tune the architecture of the proposed CNN, improving its adaptability and performance. The results demonstrate that the optimized ITSFP outperformed the others, achieving an accuracy of 80.08 %, while the optimized Fuzzy Pooling (FP), Max Pooling (MP), and Average Pooling (AP) achieved accuracies of 69.84 %, 63.60 %, and 59.77 %, respectively. The algorithmic trading based on the ITSFP model achieved the highest compound return (2.5866), Sharpe ratio (0.3632), and Sortino ratio (0.5242), reflecting its superior risk-adjusted returns and recorded the lowest Maximum Drawdown (2.8632), indicating superior resilience during market downturns. The Wilcoxon signed-rank test has been applied to show significant outperformance of the proposed ITSFP against others.},
  archive      = {J_ASOC},
  author       = {Ehsan Mohammadian Amiri and Akbar Esfahanipour},
  doi          = {10.1016/j.asoc.2025.113617},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113617},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimized convolutional neural network with a novel spherical triangular fuzzy pooling layer for an algorithmic trading model},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A first-order meta learning method for remaining useful life prediction of rotating machinery under limited samples. <em>ASOC</em>, <em>182</em>, 113616. (<a href='https://doi.org/10.1016/j.asoc.2025.113616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting the remaining useful life (RUL) of rotating machinery is a challenging task in the field of Prognostics and Health Management (PHM). In practical applications, the number of samples in the target domain is often insufficient. To address this issue, we propose a First-Order Meta-Learning Network (FOMLN) to tackle the problem of equipment RUL prediction under limited samples. First, a meta-learner is constructed based on Conformer, combining the advantages of the self-attention mechanism and convolutional neural networks, enhancing the model's ability to capture both local and global features. Then, a dual-loop meta-learning strategy is designed: the inner loop learns at the sample level, modeling and updating parameters for specific tasks, while the outer loop updates the meta-parameters through task-level learning, improving the model's generalization across different tasks and its adaptability to new tasks under limited sample conditions. Extensive experimental results on the C-MAPSS dataset validate the effectiveness of the proposed method. Moreover, a practical application case study is introduced, demonstrating the model’s ability to predict the RUL of slurry pumps in an industrial site under few-shot scenarios, highlighting its potential for real-world applications.},
  archive      = {J_ASOC},
  author       = {Yu Wang and Shujie Liu and Shuai Lv and Gengshuo Liu},
  doi          = {10.1016/j.asoc.2025.113616},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113616},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A first-order meta learning method for remaining useful life prediction of rotating machinery under limited samples},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Common neighbor-aware link weight prediction with simplified graph transformer. <em>ASOC</em>, <em>182</em>, 113614. (<a href='https://doi.org/10.1016/j.asoc.2025.113614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The link weight prediction holds significant importance in various fields, yet it has been less explored. Building a superior model faces two major challenges. First, the classic graph neural network can only propagate information along the adjacency connections due to the message-passing paradigm. When some edges are unobserved, learning better node representations is hindered. Second, existing methods often condense the local topological patterns into link representations by either graph pooling on enclosing subgraphs or handcrafted feature indices. The former incurs a heavy computational burden while the latter lacks flexibility. To address these challenges, we present a novel link weight prediction algorithm named CoNe. We design a simplified graph Transformer with linear complexity to simultaneously capture local and global topological structure information. Specifically, CoNe leverages a novel simplified global attention mechanism, allowing interactions to no longer be hardwired in static edges but to be flexibly and efficiently extended to arbitrary nodes. Furthermore, we propose self-attentive common neighbor aggregation to embed link heuristics into learnable pairwise representations. Experiments on real-world datasets demonstrate that CoNe outperforms state-of-the-art methods with 0.51%–14.67% improvements.},
  archive      = {J_ASOC},
  author       = {Lizhi Liu},
  doi          = {10.1016/j.asoc.2025.113614},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113614},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Common neighbor-aware link weight prediction with simplified graph transformer},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing long-tailed software vulnerability type classification via adaptive data augmentation and prompt tuning. <em>ASOC</em>, <em>182</em>, 113612. (<a href='https://doi.org/10.1016/j.asoc.2025.113612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software vulnerability type classification (SVTC) is essential for efficient and targeted remediation of vulnerabilities. With the rapid increase in software vulnerabilities, the demand for automated SVTC approaches is becoming increasingly critical. However, the SVTC is significantly affected by the long-tailed issues, where the distribution of vulnerability types is highly unbalanced. Specifically, a small number of head classes contain a large volume of samples, while a substantial portion of tail classes consists of only a limited number of samples. This imbalance poses a significant challenge to the classification accuracy of existing approaches. To alleviate these challenges, we propose an innovative approach VulTC-LTPF, which integrates prompt tuning with long-tailed learning to enhance the effectiveness of SVTC. Within VulTC-LTPF, an adaptive error-rate-based data augmentation strategy is developed. This strategy allows the SVTC model to dynamically augment data for tail classes types with limited sample size during training, thereby mitigating the impact of the long-tailed problem. Furthermore, VulTC-LTPF employs a hybrid prompt tuning strategy, aligning the training process more closely with pre-training, which enhances adaptability to downstream tasks. Unlike existing approaches that rely solely on either vulnerability description or source code, VulTC-LTPF leverages both sources of information. By incorporating a combination of hard and soft prompts, it facilitates a more comprehensive and effective classification strategy. Experimental results demonstrate that VulTC-LTPF achieves substantial performance improvements over four state-of-the-art SVTC baselines, with gains ranging from 26.1% to 55.1% in MCC. Ablation studies further validate the effectiveness of the adaptive data augmentation, prompt tuning, the integration of two types of vulnerability information, and the use of hybrid prompts. These findings highlight that VulTC-LTPF represents a promising advancement in the field of SVTC, offering significant potential for further progress in addressing software vulnerability type classification challenges.},
  archive      = {J_ASOC},
  author       = {Long Zhang and Xiaolin Ju and Lina Gong and Jiyu Wang and Zilong Ren},
  doi          = {10.1016/j.asoc.2025.113612},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113612},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing long-tailed software vulnerability type classification via adaptive data augmentation and prompt tuning},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale sub-graph view generation and siamese contrastive learning for graph representations. <em>ASOC</em>, <em>182</em>, 113608. (<a href='https://doi.org/10.1016/j.asoc.2025.113608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) is an essential technique in extracting structural and node-related information in graph representation learning. Most existing GCL methods rely on data augmentation to generate multiple views of a graph, aiming to maintain consistency across them via contrastive learning. However, these approaches usually have two limitations: (1) the views generated by the random perturb strategy often disrupt the critical information of the graph, and (2) the graph contrastive strategy is challenging to comprehensively construct contrastive samples between views. To address the challenges mentioned above, we propose an innovative GCL method called the Multi-Scale Sub-graph View Generation and Siamese Contrastive Learning for Graph Representations method (M3SGCL), which consists of three modules. First, the view generation module generates two novel augmented views by introducing multiple structure views and sampled sub-graph sets, which prevents the original graph structure from being damaged, providing a deep understanding of global graph information. Second, the Siamese Network module processes multiple sub-graph views using an online encoder and a target encoder, generating multi-scale representations that enrich the selection of high-quality positive and negative sample pairs for contrastive learning. Third, to further reduce the risk of the information loss and incomplete sample construction, the contrastive learning module establishes multiple contrastive paths through the Siamese Network and employs a multi-scale loss function to learn robust and informative representations. In addition, we perform comprehensive experiments on five real-world datasets, and the results show that M3SGCL significantly outperforms ten state-of-the-art baselines, especially achieving an improvement of 19.76% compared to the second-best method on the Wisconsin dataset. These results demonstrate that our method effectively captures more nuanced and informative graph information by constructing subgraph views and introducing an enhanced multi-scale comparison strategy.},
  archive      = {J_ASOC},
  author       = {Rende Hong and Kaibiao Lin and Binsheng Hong and Zhaori Guo and Fan Yang},
  doi          = {10.1016/j.asoc.2025.113608},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113608},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale sub-graph view generation and siamese contrastive learning for graph representations},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable GMDH-type neural networks for decision making: Case of medical diagnostics. <em>ASOC</em>, <em>182</em>, 113607. (<a href='https://doi.org/10.1016/j.asoc.2025.113607'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical diagnostics, the use of interpretable artificial neural networks (ANN) is crucial to enabling healthcare professionals to make informed decisions that consider risks, especially when faced with uncertainties in patient data and expert opinions. Despite advances, conventional ANNs often produce complex, not transparent models that limit interpretability, particularly in medical contexts where transparency is essential. Existing methods, such as decision trees and random forests, provide some interpretability but struggle with inconsistent medical data and fail to adequately quantify decision uncertainty. This paper introduces a novel Group Method of Data Handling (GMDH)-type neural network approach that addresses these gaps by generating concise, interpretable decision models based on the self-organizing concept. The proposed method builds multilayer networks using two-argument logical functions, ensuring explainability and minimizing the negative impact of human intervention. The method employs a selection criterion to incrementally grow networks, optimizing complexity while reducing validation errors. The algorithm’s convergence is proven through a bounded, monotonically decreasing error sequence, ensuring reliable solutions. Having been tested in complex diagnostic cases, including infectious endocarditis, systemic red lupus, and postoperative outcomes in acute appendicitis, the method achieved high expert agreement scores (Fleiss’s kappa of 0.98 (95% CI 0.97-0.99) and 0.86 (95% CI 0.83-0.89), respectively) compared to random forests (0.84 and 0.71). These results demonstrate statistically significant improvements ( p < 0 . 05 ), highlighting the method’s ability to produce interpretable rules that reflect uncertainties and improve the reliability of decisions. Having demonstrated a transparent and robust framework for medical decision-making, the proposed approach bridges the gap between model accuracy and interpretability, providing practitioners with reliable insights and confidence estimates required for making risk-aware decisions.},
  archive      = {J_ASOC},
  author       = {L. Jakaite and V. Schetinin},
  doi          = {10.1016/j.asoc.2025.113607},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113607},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable GMDH-type neural networks for decision making: Case of medical diagnostics},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation and prioritization of software packages for remote education assistance using picture fuzzy comprehensive distance-based ranking approach. <em>ASOC</em>, <em>182</em>, 113606. (<a href='https://doi.org/10.1016/j.asoc.2025.113606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Growing competition for online education has motivated the need for software packages assessment. Selecting the most suitable software package depends on several criteria, therefore, the decision-making approaches can be more appropriate to deal with such types of problems. To this aim, this work introduces a picture fuzzy information-based ranking approach for evaluating and prioritizing the software packages by means of several criteria. This approach firstly derives the weights of invited decision-making experts using the picture fuzzy distance measure-based procedure. To this aim, novel distance measure is proposed for describing the difference between picture fuzzy sets with its usefulness in comparison with the prior developed distance measures. To aggregate the opinions of decision-making experts about the performance of each software with respect to considered criteria, the picture fuzzy weighted averaging operator is utilized and created the aggregated decision matrix. Next, the criteria weights are calculated with the amalgamation of objective and subjective weights through picture fuzzy standard deviation model and picture fuzzy ranking comparison model, respectively. Finally, a picture fuzzy extension of comprehensive distance-based ranking model is presented to solve the multi-criteria software packages evaluation problem, which validates its rationality and feasibility. The acquired results prove that the software package “Google Workspace for Education Plus” is the most suitable choice due to its lowest comprehensive degree among the others. The stability and robustness of the outcomes are further verified through sensitivity and comparative analyses.},
  archive      = {J_ASOC},
  author       = {Ahmad M. Alshamrani and Edmundas Kazimieras Zavadskas and Pratibha Rani and Jurgita Antucheviciene and Adel Fahad Alrasheedi},
  doi          = {10.1016/j.asoc.2025.113606},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113606},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation and prioritization of software packages for remote education assistance using picture fuzzy comprehensive distance-based ranking approach},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protein-peptide interaction region residues prediction using a generative sampling technique and ensemble deep learning-based models. <em>ASOC</em>, <em>182</em>, 113603. (<a href='https://doi.org/10.1016/j.asoc.2025.113603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivation Predicting protein-peptide interactions advances the understanding of drug design, protein biological functions, and cellular processes. Researchers have proposed various experimental and computational methods to identify interactions between proteins and peptides. However, traditional experimental approaches are laborious, time-consuming, and inefficient. Motivated by these challenges, a novel computational method is developed to detect protein-peptide interaction region residues from protein data, providing a complementary approach to experimental techniques. Method We designed a computational method for identifying protein-peptide interaction region residues, by incorporating a generative sampling technique with ensemble deep learning (DL) model using various features derived from protein sequences and structures. The proposed method relied on three pipelines: pre-processing, processing, and post-processing. The pre-processing pipeline converted the amino acid sequence into an image-like input representation to capture vital residue interactions. Also to overcome class imbalance challenge and non-binding over-predicting drawback, it employs a generative sampling technique for balancing the training data. Afterwards, to achieve more reliable prediction of protein-peptide interaction, a processing pipeline is designed that incorporates three independent DL sub-models. Subsequently, in the post-processing pipeline to obtain final prediction results, the outputs of ensemble DL modules are applied to three layers convolutional neural network. Results Compared to state-of-the-art sequence- and structure-based methods, the proposed method achieved the highest performance in F-measures (improved by 22.1 %), precision (improved by 3.9 %), and better balance between sensitivity and specificity. Eventually, our various experiments validated the effectiveness of the proposed method as a reliable computational assistant for predicting protein-peptide interaction region residues.},
  archive      = {J_ASOC},
  author       = {Shima Shafiee and Abdolhossein Fathi and Ghazaleh Taherzadeh},
  doi          = {10.1016/j.asoc.2025.113603},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113603},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Protein-peptide interaction region residues prediction using a generative sampling technique and ensemble deep learning-based models},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UL-Phys:Ultra-lightweight remote physiological measurement in facial videos based on unsupervised learning. <em>ASOC</em>, <em>182</em>, 113593. (<a href='https://doi.org/10.1016/j.asoc.2025.113593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG) enables non-contact monitoring of vital signs using facial videos, but current supervised learning methods often rely on complex architectures and large annotated datasets, limiting their practicality in real-time and resource-constrained scenarios. This paper addresses these limitations by proposing UL-Phys, an ultra-lightweight self-supervised framework for rPPG signal estimation. From a research standpoint, we reformulate the rPPG task as a linear self-supervised reconstruction problem, introducing a novel frequency-constrained objective to extract inherent periodic information without requiring ground truth labels. The framework integrates a lightweight 3D spatiotemporal encoder-decoder network, and a neuroscience-inspired hybrid attention module to enhance pulsatile signal regions while suppressing noise. Experimental evaluations on PURE and UBFC-rPPG datasets demonstrate that UL-Phys achieves superior performance compared to existing supervised and self-supervised baselines, while significantly reducing model complexity and inference latency. Our method also shows strong generalization across datasets, highlighting the value of embedding physiological priors into lightweight, self-supervised architectures. These findings offer a promising direction for scalable and deployable rPPG systems in real-world settings.},
  archive      = {J_ASOC},
  author       = {Haibo Zhang and Xu Wang and Pan Dang and Chaohui Ma and Shuai Liu and Zhuang Xiong and Cheng Liu},
  doi          = {10.1016/j.asoc.2025.113593},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113593},
  shortjournal = {Appl. Soft. Comput.},
  title        = {UL-Phys:Ultra-lightweight remote physiological measurement in facial videos based on unsupervised learning},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-pruning-robot and multi-fertilizing-drone collaborative task assignment using multi-class teaching-learning-based optimization. <em>ASOC</em>, <em>182</em>, 113591. (<a href='https://doi.org/10.1016/j.asoc.2025.113591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of intelligent agricultural robots and drones has greatly advanced smart agriculture. This paper investigates a multi-pruning-robot and multi-fertilizing-drone task assignment problem (MRMDTA) in a smart orchard, aiming to minimize the makespan of the whole robot-drone system. To address this issue, a mathematical model is formulated, and an innovative multi-class teaching-learning-based optimization (MTLBO) algorithm is proposed. The MTLBO algorithm integrates a multi-class teaching approach, where each class is led by both a teacher and a teaching assistant, enhancing learning efficiency across diverse groups. The algorithm operates through a well-structured, six-stage optimization process. Firstly, in the initialization stage, two heuristics based on greedy insertion are introduced. Subsequently, in the class division stage, a teacher and a teaching assistant are assigned to each class. Then, in the training stage, five heuristic search operators are designed. Following this, in the learning stage, a recombine crossover operator is presented for students to learn from teachers. Next, in the collaboration stage, a temporary class is formed. Finally, in the graduation stage, individuals with little search potential are eliminated. Extensive experimental results demonstrate that the proposed MTLBO algorithm outperforms state-of-the-art algorithms in terms of efficiency and solution quality.},
  archive      = {J_ASOC},
  author       = {Cun-Hai Wang and Wei Zhang and Quan-Ke Pan and Zhong-Hua Miao and Bing Wang},
  doi          = {10.1016/j.asoc.2025.113591},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113591},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-pruning-robot and multi-fertilizing-drone collaborative task assignment using multi-class teaching-learning-based optimization},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust adaptive support vector machine based on multi-sensor information fusion for human behavior recognition. <em>ASOC</em>, <em>182</em>, 113590. (<a href='https://doi.org/10.1016/j.asoc.2025.113590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issues of redundant high-dimensional action features, few action behavior classifications, and weak generalization ability of recognition models in traditional human behavior recognition (HBR), this paper proposes an HBR method based on Nonlinear Shannon's Principal Component Analysis (NSPCA) and Multi-Strategy Improved Remora Optimization Algorithm (MSIROA)-Adaptive Bi-kernel Support Vector Machine (ABKSVM). First, the NSPCA is employed to extract features from multiple sources of information and address the issue of nonlinear features within the data. Then, the selected principal component fusion features are input into the MSIROA-ABKSVM model, to achieve recognition of human behaviors. By utilizing an improved SVM, the extracted features are recognized to enhance the ability to identify behavior accurately. The experimental results indicate that the cumulative variance contribution rate of the six principal components selected by the NSPCA method reaches 85 % to simplify the data structure. Using the analysis of performance indicators, the classification method achieved an accuracy of 99.3 % and 99.5 % on the self-collected HBR datasets and the Chinese PLA General Hospital (PLAGH)-HBR dataset, respectively, outperforming other state-of-the-art methods. The results show that the HBR model can identify 33 different human behaviors, providing a new method for improving the recognition rate and effectiveness of daily activity monitoring for the elderly.},
  archive      = {J_ASOC},
  author       = {Lei Wu and Shuli Guo and Lina Han and Jiaoyu Jia},
  doi          = {10.1016/j.asoc.2025.113590},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113590},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust adaptive support vector machine based on multi-sensor information fusion for human behavior recognition},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pressure-sensitive paint images inpainting network via region convolution and pyramid attention. <em>ASOC</em>, <em>182</em>, 113589. (<a href='https://doi.org/10.1016/j.asoc.2025.113589'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting is crucial for restoring missing pressure data and obtaining full-field pressure distribution on the surface of wind tunnel models for pressure-sensitive paint (PSP) technology. Traditional methods are computationally intensive and have low accuracy. Deep learning, with its advanced feature learning capabilities, provides a novel approach for inpainting PSP images. However, PSP images have a unique texture distribution, where color changes directly reflect the surface pressure distribution. The inpainted data must ensure the continuity and accuracy of the full-field flow distribution while adhering to physical constraints. Standard convolution methods abuse invalid pixel values in hole regions and generate unreliable data. To address this issue, an efficient region convolution (ERC) module is proposed, which can distinguish between known and damaged regions and eliminate the interference of invalid pixels through feature migration. In addition, a non-local pyramid of self-attention (PSA) module is designed to model the long-range dependencies between cross-scale related features. Finally, the ERC and PSA modules are integrated into an encoder-decoder framework to propose a novel inpainting network. Extensive experiments on specialized PSP image datasets and two public datasets (Paris StreetView and CelebA) demonstrate the model's superiority. At a mask rate of 30–40 % on the PSP image dataset, the model achieves a PSNR of 38.04, SSIM of 0.9797, and LPIPS of 0.0185. A set of real wind tunnel experiments demonstrates that our method can notably improve the accuracy of luminescent intensity data at the PSP image holes, with the mean relative errors of 0.0492 % and 0.0532 % at pressure taps, indicating that it calculates the full-field pressure data more accurately. This is expected to play an important role in practical applications.},
  archive      = {J_ASOC},
  author       = {Jinrong Li and Chunhua Wei and Zhisheng Gao and Lei Liang and Bin Zhou},
  doi          = {10.1016/j.asoc.2025.113589},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113589},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pressure-sensitive paint images inpainting network via region convolution and pyramid attention},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A topic-specific representation learning framework for acoustic scene classification. <em>ASOC</em>, <em>182</em>, 113588. (<a href='https://doi.org/10.1016/j.asoc.2025.113588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic scene classification (ASC) involves using sound to identify the surrounding environment. It faces significant challenges from intra-scene heterogeneity and shared acoustic characteristics across scenes. To address these challenges, we propose TD-GGNN, a novel framework combining topic modeling with graph neural networks to enhance both intra- and inter-semantic dynamics. Unlike conventional topic-based methods that rely on coarse-grained topic distributions and isolated topics, our approach introduces two key innovations: a topic decoupling (TD) module leveraging topic contents for learning fine-grained representations, and a gated graph neural network (GGNN) enabling explicit semantic interactions between topics. Evaluations on two urban environmental acoustic scene datasets demonstrate significant improvements: our system achieves classification accuracies of 79.3 % and 78.1 % on the respective datasets, outperforming the baseline by 5.6 % and 5.0 %. Additionally, it records the lowest Log loss values of 0.603 and 0.629, reflecting reductions of 49.5 % and 42.0 % compared to the baseline. Key findings reveal that: topic contents provide more discriminative representations than traditional topic distributions; GGNN-based semantic interactions effectively capture inter-scene relationships; and the combined effect of TD and GGNN exhibits strong robustness to both scene heterogeneity and shared acoustic characteristics across scenes.},
  archive      = {J_ASOC},
  author       = {Yan Leng and Enze Zhang and Jian Zhuang and Can Shen and Chengli Sun and Qi Yuan and Jie Pan},
  doi          = {10.1016/j.asoc.2025.113588},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113588},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A topic-specific representation learning framework for acoustic scene classification},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary multi-objective fishing routing with decision maker’s preferences. <em>ASOC</em>, <em>182</em>, 113587. (<a href='https://doi.org/10.1016/j.asoc.2025.113587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to enhance economic and environmental sustainability of fisheries through fishing routing methods that can reduce operational costs, emission footprints, and incidental fishing risks. To achieve this, a novel problem definition is introduced, the time-dependent multi-objective orienteering problem with time windows and moving targets (TDMOOP-TWMT). Unlike existing fishing routing problems, the TDMOOP-TWMT allows users to define their fishing trips by setting a maximum time at sea rather than a predefined number of fishing sets. This multi-objective problem includes three goals: fuel-oil consumption, catches of tuna species, and incidental catches of non-target species (bycatch). To address this problem, the w-MOEA/D algorithm is employed, which incorporates decision-makers’ preferences using wide weight intervals for each objective, eliminating the need for precise weight values. Compared to the classical MOEA/D, the w-MOEA/D method achieves solutions closer to the true Pareto front while reducing the final solution set based on users’ preferences. To demonstrate the potential application and benefits in a real context, 12 historical routes are employed across different fishing scenarios, each defined by varying the weight intervals of the objectives. The results show that w-MOEA/D routes allow for consuming less fuel and catching more tuna, though with a higher risk of bycatch when compared to historical trips. However, prioritizing bycatch avoidance reduces this risk while maintaining similar fuel efficiency, although with a lower increase in catches. In summary, this study highlights the effectiveness of the proposed solution method in supporting fishers' decision-making by incorporating their preferences when planning fishing routes.},
  archive      = {J_ASOC},
  author       = {Igor Granado and Joanna Szlapczynska and Rafal Szlapczynski and Leticia Hernando and Jose A. Fernandes-Salvador},
  doi          = {10.1016/j.asoc.2025.113587},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113587},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary multi-objective fishing routing with decision maker’s preferences},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AttenHideNet: A novel deep learning-based image steganography method using a lightweight U-net with soft attention. <em>ASOC</em>, <em>182</em>, 113583. (<a href='https://doi.org/10.1016/j.asoc.2025.113583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-image steganography, embedding secret information within images while preserving visual quality, has become essential due to growing demands for secure and efficient digital communication. Traditional methods often struggle to achieve high embedding capacity without sacrificing imperceptibility. Recent advancements in deep learning have offered promising solutions by enabling more complex data embedding strategies. In this paper, we propose AttenHideNet, a novel deep learning-based steganography method leveraging a lightweight U-Net architecture (<1.2 million parameters) combined with soft attention mechanisms. By utilizing the YUV color space instead of RGB, our method significantly improves embedding efficiency, capacity, and visual imperceptibility. AttenHideNet achieves an embedding capacity of up to 24 bits per pixel (bpp) while maintaining high visual quality. The soft attention mechanism dynamically identifies and prioritizes embedding in less perceptually sensitive image regions. Experimental results on benchmark datasets demonstrate that AttenHideNet achieves superior visual quality (PSNR up to 52.67 dB) compared to state-of-the-art methods, with low latency (18 ms/image) and minimal memory usage (4.11 MB), making it suitable for real-time applications. Despite these advantages, the method shows limited robustness under firm JPEG compression and geometric transformations, highlighting essential directions for future research.},
  archive      = {J_ASOC},
  author       = {Younis M. Younis and Ramadhan J. Mstafa and Shamal AL-Dohuki},
  doi          = {10.1016/j.asoc.2025.113583},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113583},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AttenHideNet: A novel deep learning-based image steganography method using a lightweight U-net with soft attention},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human-AI interaction: Machine learning-based geostatistical hybrid models. <em>ASOC</em>, <em>182</em>, 113580. (<a href='https://doi.org/10.1016/j.asoc.2025.113580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent methods for estimating mineral grades have been developed but some tasks cannot be completely automated through artificial intelligence. Human-in-the-loop (HITL) approaches are being increasingly utilized, where the strengths of both human expertise and artificial intelligence are combined to improve outcomes. This study integrates HITL models with machine learning (ML) based geostatistical hybrid modeling and ensembling approaches for mineral grade estimation and ore sorting. In the hybrid modeling approach, ML models such as an elliptical radial basis neural network (ERBFN), locally weighted support vector regression (LWSVR), kernel density estimated trend (KDET), and a convolutional neural network (CNN) are incorporated as secondary variables within intrinsic collocated cokriging (ICCK). Additionally, the study utilizes two types of ensemble models—global (GWE) and local weighting-based (LWE) ensembles. These ensembles integrate outputs from hybrid models, applying global and local weights based on each model’s cross-validation performance. Depending on their level of expertise, humans are integrated as either (1) novice practitioners considered as human-as-feedback (HAF) systems where they act as model checks and key parameter validators, without the ability to influence ML training or (2) expert practitioners considered as systems where model parameters are actively adjusted, model structures are tuned, and the learning process is guided by human experts. The effectiveness of the HAF and HAC systems is evaluated using data from multiple blast areas obtained from an open-pit copper mine. Compared to fully automated modeling, the HAF system improved estimation accuracy in terms of R 2 values by between 3.6 % ( ICCK CNN ) and 5.9 % (GWE) across hybrid and ensemble models. Meanwhile, the HAC system demonstrated more significant enhancements, with R 2 values increase ranging from 5.0 % ( ICCK CNN ) to 16.5 % (GWE) for these same models. This advancement suggests the potential for more precise and effective decision-making in mining operations using HITL systems.},
  archive      = {J_ASOC},
  author       = {Gamze Erdogan Erten and Karim Mokdad and Jed Nisenson and Gabriela Brandao and Jeff Boisvert},
  doi          = {10.1016/j.asoc.2025.113580},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113580},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human-AI interaction: Machine learning-based geostatistical hybrid models},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable evolutionary broad learning system for damage identification in aircraft structures using lamb waves. <em>ASOC</em>, <em>182</em>, 113577. (<a href='https://doi.org/10.1016/j.asoc.2025.113577'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has gained significant attention in Lamb wave-based structural health monitoring (SHM). However, existing DL approaches for damage identification in aircraft structures require manually designed network architectures tailored to specific tasks, resulting in substantial computational overhead and hindering real-time monitoring applications. To overcome these limitations, this study proposes a novel damage identification method for aircraft structures based on Lamb waves and an interpretable evolutionary broad learning system (EBLS), which can automatically learn the complex nonlinear relationship between damage features in Lamb wave signals and structural health conditions. The proposed method incorporates a novel particle swarm optimization with square wave switching mechanism (SWSPSO) to systematically explore and optimize the complex hyperparameter space of the broad learning system (BLS). This intelligent optimization enables automated generation of task-specific BLS architectures for damage identification without manual intervention. The interpretability of EBLS is rigorously investigated through locally interpretable model-agnostic explanations (LIME), revealing physically meaningful correlations between critical feature contributions and fundamental Lamb wave propagation characteristics. Experimental validation employs a comprehensive Lamb wave dataset acquired through lead zirconate titanate (PZT) sensors mounted on aircraft structural components, encompassing diverse damage scenarios with varying locations and severity levels. Experimental results demonstrate that EBLS significantly outperforms traditional deep learning models, achieving over 0.95 accuracy in damage identification tasks while reducing computational efficiency by an order of magnitude and enhancing interpretability.},
  archive      = {J_ASOC},
  author       = {Gang Chen and Weihan Shao and Fudong Tang and Hu Sun},
  doi          = {10.1016/j.asoc.2025.113577},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113577},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interpretable evolutionary broad learning system for damage identification in aircraft structures using lamb waves},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced N-BEATS for mid-term electricity demand forecasting. <em>ASOC</em>, <em>182</em>, 113575. (<a href='https://doi.org/10.1016/j.asoc.2025.113575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an enhanced N-BEATS model, N-BEATS*, for improved mid-term electricity load forecasting (MTLF). Building on the strengths of the original N-BEATS architecture, which excels in handling complex time series data without requiring preprocessing or domain-specific knowledge, N-BEATS* introduces two key modifications. (1) A novel loss function — combining pinball loss based on MAPE with normalized MSE, the new loss function allows for a more balanced approach by capturing both L 1 and L 2 loss terms. (2) A modified block architecture — the internal structure of the N-BEATS blocks is adjusted by introducing a destandardization component to harmonize the processing of different time series, leading to more efficient and less complex forecasting tasks. Evaluated on real-world monthly electricity consumption data from 35 European countries, N-BEATS* demonstrates superior performance compared to its predecessor and other established forecasting methods, including statistical, machine learning, and hybrid models. N-BEATS* achieves the lowest MAPE and RMSE, while also exhibiting the lowest dispersion in forecast errors. The source code is publicly available at Kasprzyk (2025).},
  archive      = {J_ASOC},
  author       = {Mateusz Kasprzyk and Paweł Pełka and Boris N. Oreshkin and Grzegorz Dudek},
  doi          = {10.1016/j.asoc.2025.113575},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113575},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhanced N-BEATS for mid-term electricity demand forecasting},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid genetic tabu search algorithm for metro crew scheduling based on a space-time-state network. <em>ASOC</em>, <em>182</em>, 113574. (<a href='https://doi.org/10.1016/j.asoc.2025.113574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crew scheduling problem is highly important for the operation and management of urban rail transit. It is essential to reasonably design an approach for optimizing the crew schedule within the constraints of a provided train diagram so that the schedule is highly versatile and can meet the actual operational demand. Additionally, better results can be achieved by using an optimization method, which can reduce operating costs and satisfy crew members’ working preferences to the greatest extent possible to achieve a more rational distribution of tasks. Unlike traditional space-time networks that merely describe spatiotemporal movement trajectories, this study innovatively introduces state attributes to ensure solution feasibility during search. Using these attributes, we establish a space-time-state network for crew scheduling modeling. This model has the objective of reducing task connection time and personnel costs. To solve the provided model, a hybrid genetic tabu search (HGTS) algorithm is created by considering the distinctive characteristics of two methods: tabu search (TS) and genetic algorithm (GA), where TS handles local search and GA performs global optimization. The HGTS algorithm can efficiently address the complex metro crew scheduling problem and obtain an improved crew scheduling plan. The proposed method is validated against data from Chengdu Metro Line 5. Results demonstrate that our constructed methodology can effectively reduce the personnel costs and connection time of crew scheduling over the manual scheduling plan: a total of 148 crew duties were obtained, with an optimization rate of 10.30 % and a total connection time of 198 h 44 min 49 s, with an optimization rate of 7.71 %. Furthermore, the proposed method has a higher computational speed and enhanced stability than the shortest-path faster algorithm based on the greedy approach (G-SPFA) method, especially for large-scale data. Additionally, as a hybrid algorithm, HGTS delivers superior solutions compared to standalone GA and TS. This advantage is evidenced by key metrics: HGTS achieved a total duty duration of 725 h 31 min 51 s versus GA's 778 h 38 min 10 s and TS's 749 h 11 min 31 s, while also demonstrating tighter crew efficiency with standard deviations of 0.067, 0.077, and 0.085 for HGTS, GA, and TS respectively.},
  archive      = {J_ASOC},
  author       = {Feng Xue and Peng Liang and Ying Yang and Jincheng Wang},
  doi          = {10.1016/j.asoc.2025.113574},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113574},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid genetic tabu search algorithm for metro crew scheduling based on a space-time-state network},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic multi-objective optimization for uncertain order insertion green shop production scheduling. <em>ASOC</em>, <em>182</em>, 113573. (<a href='https://doi.org/10.1016/j.asoc.2025.113573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient production scheduling of forgings is critical to manufacturing. However, production orders in manufacturing can change dynamically, making it challenging to quickly track changes between orders and ensure productivity and environmentally friendly production in the shop. This paper presents a dynamic multi-objective green shop production scheduling optimization problem that addresses uncertain order insertion, considering the objectives of total completion time, energy consumption, and carbon emission. When a new batch of orders arrives, changes in the dimensionality of decision variables for production scheduling lead to environmental alterations. By integrating the workpiece completion rates of orders from historical environments with the workpiece quantities of orders in the new environment, the scheduling plan is dynamically adjusted and promptly responded to. Therefore, we design a discrete matrix-based dynamic multi-objective optimization algorithm (DM-DMOEA), which can measure the similarity of the orders before and after the dynamic changes, and reconstruct a high-quality scheduling solution under the new environment, which solves the problem of variable dimensionality changes due to the dynamic environment. Finally, experiments were conducted in a real case of a flange manufacturing company, and the results proved the validity of the proposed model and the superior performance of the algorithm.},
  archive      = {J_ASOC},
  author       = {Linjie Wu and Tianhao Zhao and Xingjuan Cai and Zhihua Cui and Jinjun Chen},
  doi          = {10.1016/j.asoc.2025.113573},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113573},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic multi-objective optimization for uncertain order insertion green shop production scheduling},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-branch global transformer‐assisted network for fault diagnosis. <em>ASOC</em>, <em>182</em>, 113572. (<a href='https://doi.org/10.1016/j.asoc.2025.113572'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault Diagnosis (FD) is critical in smart manufacturing, enabling predictive maintenance, reducing operational costs, and enhancing system reliability. To deal with this task more accurately, this paper proposes a generative, effective, and novel framework, a multi-branch global Transformer-assisted network (MBGTNet), for accurate FD. First, a multi-branch global-wide one-dimension convolution operation (MBG-WideConv1D) is proposed to obtain global features in different views. Meanwhile, a Transformer assist scheme (TAS) is designed to leverage the Transformer's global feature extraction capacity. The features extracted by the Transformer are fused with those extracted with MBG-WideConv1D by minimizing their pairwise correlation alignment (CORAL) distances. Benefiting from the well-designed MBG-WideConv1D and TAS, the global features hidden in the raw signals are fully extracted from multiple viewpoints. Each branch of global features is then fed into a one-dimension convolutional neural network (1DCNN) to extract local features in a multi-supervised scheme (MSS) that helps each branch learn thoroughly. Furthermore, the proposed method employs a local feature correlation enhancement scheme (LFCS) to reduce distribution differences and increase robustness among the local features of each branch. As a result, the final features used for FD are a fusion of multi-view global and local features with strong robustness, enabling accurate FD in noisy environments. Comparative experiments on four datasets, including CWRU, MFPT, SU Bearing, and SU Gear, validate the proposed method's effectiveness, achieving over 99.6 % accuracy across four datasets. Moreover, the TAS and LFCS's generalities have been demonstrated on two 1DCNNs and hybrid CNN-LSTMs with four subsets. Also, the effectiveness of each component in the proposed framework has been thoroughly analyzed.},
  archive      = {J_ASOC},
  author       = {Xiaorui Shao and Chang-Soo Kim},
  doi          = {10.1016/j.asoc.2025.113572},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113572},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-branch global transformer‐assisted network for fault diagnosis},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of a two-step committee machine approach for estimation of bitumen content from petrophysical data: Reaping the benefits of evolutionary, probabilistic and stochastic optimisation. <em>ASOC</em>, <em>182</em>, 113571. (<a href='https://doi.org/10.1016/j.asoc.2025.113571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir bitumen content is a challenging issue, affecting porosity, permeability and hydrocarbon flow rate. The current study addresses the challenge of accurate bitumen prediction by proposing a two-step committee machine (CM) model, leveraging stand-alone, ensemble, optimisation and multi-variable linear regression (MVLR) algorithms. The model employs well-logging data and bitumen content derived from rock-eval pyrolysis for six wells of the Dezful Embayment, Iran. Six stand-alone algorithms were utilised for the initial estimation of bitumen content from well logging data. The light gradient boosting machine and the adaptive neuro-fuzzy inference system outperformed the others considering the mean squared error (MSE) and correlation coefficient (R). Subsequently, five optimisation algorithms were utilised to construct the CMs using weight coefficients, improving the efficacy of the stand-alone algorithms. The covariance matrix adaptation evolution strategy (CMA-ES) and the grey wolf optimisation outperformed the others considering the mean squared error (MSE) and correlation coefficient (R). The CM with CMA-ES (CMCMA-ES) significantly reduced MSE by 66.0459 % and increased R by 9.718 % compared to the average outcomes of stand-alone algorithms. The superiority of the CMCMA-ES was validated through convergence and stability analyses along with the Friedman test. The further improvement of CMs resulted from using MVLR, that reduced MSE by 60.3481 % and increased R by 2.902 % compared to the CMCMA-ES outcomes. The effectiveness of the proposed model was confirmed by performing benchmark analysis and determining the bitumen values of other wells. This model contributes to the industry from two perspectives: detecting bitumen-rich zones to prevent production problems and mapping valuable natural bitumen resources for exploration purposes.},
  archive      = {J_ASOC},
  author       = {Ali Gholami Vijouyeh and Maha Raoof Hamoudi and Dyana Aziz Bayz and Ali Kadkhodaie},
  doi          = {10.1016/j.asoc.2025.113571},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113571},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of a two-step committee machine approach for estimation of bitumen content from petrophysical data: Reaping the benefits of evolutionary, probabilistic and stochastic optimisation},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-dynamic population multi-objective evolutionary algorithm for large-scale crude oil scheduling optimization. <em>ASOC</em>, <em>182</em>, 113570. (<a href='https://doi.org/10.1016/j.asoc.2025.113570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As refinery production scales and equipment complexity increase, refineries are setting stricter requirements for crude oil scheduling. Consequently, large-scale multi-objective crude oil scheduling problems (LSMCOSPs) involve a vast quantity of binary variables, nonlinear restrictions, and many multiple optimization objectives, making it challenging for conventional algorithms to efficiently explore the solution space and often resulting in suboptimal outcomes. This paper addresses this challenge by constructing a discrete-time mixed-integer nonlinear programming (MINLP) model for offshore refinery crude oil scheduling, covering stages such as unloading, transportation, processing in crude distillation units (CDUs), and intermediate product inventory management. Based on this model, we propose a dual-dynamic population co-evolutionary algorithm (denoted by DDPCEA) to solve the problem. The experiment consists of three scheduling cases, involving multiple crude oil types, storage tanks, and processing equipment, with a total of thousands of binary variables and dozens of nonlinear constraints. During the algorithm’s execution, the initially fixed mutation factor, crossover factor, and nonlinear learning factor dynamically evolve with the number of iterations. Additionally, a repair strategy is introduced to further optimize local continuous variables, moving infeasible solutions toward the feasible region. Experimental results demonstrate that, compared to commonly used multi-objective algorithms for LSMCOSPs, the proposed DDPCEA significantly improves both the number of changeovers and runtime efficiency, while also achieving superior performance in terms of HV and IGD metrics.},
  archive      = {J_ASOC},
  author       = {Xianyu Hou and Renchu He and Wei Du},
  doi          = {10.1016/j.asoc.2025.113570},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113570},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual-dynamic population multi-objective evolutionary algorithm for large-scale crude oil scheduling optimization},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Face recognition with morphing attack removal using deep learning models. <em>ASOC</em>, <em>182</em>, 113569. (<a href='https://doi.org/10.1016/j.asoc.2025.113569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated that face recognition systems are vulnerable to attacks involving altered or morphed face images, which pose critical security threats. While existing morphing attack detection methods show promising accuracy, challenges such as occlusions, varying illumination, facial expressions, and identity variations remain insufficiently addressed. To overcome these limitations, we propose a comprehensive deep learning framework for robust face attack detection and recognition. The framework operates in sequential phases. First, face images are collected from standard datasets and processed by a Transformer-based Visual Geometry Group 16 model enhanced with an Attention Mechanism (TVGG16-AM) to classify images as either genuine or manipulated. Real images proceed directly to face recognition, while manipulated images undergo an occlusion and morphing removal process. Occlusions are addressed using a face inpainting technique based on Adaptive Auxiliary Generative Adversarial Network (GAN) Inversion, with parameters optimized by an Improved Random Parameter-based Chameleon Swarm Algorithm (IRP-CSA). Morphing attacks are detected by analyzing facial images using deep features extracted by the TVGG16-AM model, enabling the identification of manipulated or composite biometric images. After these correction stages, face recognition is performed by feeding deep features from TVGG16-AM into an Adaptive Deep Temporal Convolution Network (ADTCN), where hyperparameters are further tuned using the improved CSA optimizer. Comprehensive evaluation using multiple metrics shows the proposed method significantly outperforms traditional approaches, achieving an accuracy of 97 % in detecting attacks and recognizing faces.},
  archive      = {J_ASOC},
  author       = {Sutraye Maruthirao and Dr. Pinjari Abdul Khayum},
  doi          = {10.1016/j.asoc.2025.113569},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113569},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Face recognition with morphing attack removal using deep learning models},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantitative risk assessment of cruise ship turbochargers using type-2 fuzzy-FMECA and dynamic bayesian network approach. <em>ASOC</em>, <em>182</em>, 113568. (<a href='https://doi.org/10.1016/j.asoc.2025.113568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marine propulsion systems, both traditional and modern electric, face significant risks associated with turbocharger and lubrication system failures. The failure outcomes can be severe, with accidents leading to deaths onboard, damage to machinery causing operational disruption, environmental pollution, and financial losses. While traditional Failure mode, effect, and criticality analysis (FMECA) methods excel in identifying system failures, their reliance on single-point estimates for severity, occurrence, and non-detection may prove limiting. Moreover, employing multiple experts in assessments can introduce biases. Integrating type-2 Fuzzy-FMECA with the linear opinion pool method is a robust approach to address these limitations. Leveraging the collective expertise of multiple experts, this framework enhances risk assessment comprehensiveness and accuracy. Focusing on the Carnival Freedom cruise ship incident near the Cayman Islands in October 2019, this study aims to develop a comprehensive risk assessment framework for assessing marine engine turbocharger and lubrication system risks. This study showed a strong positive correlation of 0.99 between the traditional risk prioritization number and the proposed type-2 fuzzy logic method, demonstrating its validity as a reliable alternative. This method effectively identified critical machinery failures, such as low-pressure switch and pressure control valve malfunctions, consistently aligning with the results of Traditional methods. It combines a dynamic Bayesian network for handling uncertainty with an interval type-2 fuzzy expert system and a bow-tie model. This framework enables both qualitative hazard identification and quantitative risk assessment. This risk analysis approach holds practical applicability in real-world scenarios, and its outcomes significantly provide actionable insights to mitigate and eliminate potential failures. Ultimately, it reduces the risk and improves the safety and reliability of cruise ship operations, providing a tangible solution to a pressing problem in the field.},
  archive      = {J_ASOC},
  author       = {Shoaib Ahmed and Tie Li and Xinyi Zhou and Shuai Huang and Run Chen},
  doi          = {10.1016/j.asoc.2025.113568},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113568},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantitative risk assessment of cruise ship turbochargers using type-2 fuzzy-FMECA and dynamic bayesian network approach},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An offshore photovoltaic risk assessment framework based on probabilistic linguistic multi-criteria decision-making method and consensus-maximizing group information aggregation model. <em>ASOC</em>, <em>182</em>, 113567. (<a href='https://doi.org/10.1016/j.asoc.2025.113567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk assessment is a necessary prerequisite for investment decisions in offshore photovoltaic power generation (OPPG) projects. However, existing risk evaluation methods for OPPG face several challenges, including insufficient rationality of evaluation indicators, distortion of original assessment information, and low computational efficiency in handling indicator correlations. To address these issues, this study constructs a comprehensive OPPG risk assessment framework. First, a criteria system is established, incorporating sixteen risk factors from economic, environmental, technical, and social sustainability dimensions. To prevent original information distortion, multi-granular probabilistic linguistic term sets (M-PLTSs) are introduced to express experts’ subjective judgments, while improved hesitancy and distance measures are developed to quantify individual reliability. A group information aggregation model is then built with the dual objectives of maximizing group consensus and minimizing the maximum individual compromise. For enhanced computational efficiency in processing indicator correlations, an optimization model integrating Pearson correlation coefficients and analytic hierarchy process is proposed to determine criteria weights. A case study of a project in Ninghai, China, demonstrates the framework’s effectiveness, revealing a medium-level overall risk with economic risk identified as the primary concern. This research provides a practical foundation for investment decision-making and risk mitigation strategy development in the OPPG industry.},
  archive      = {J_ASOC},
  author       = {Fengjia Guo and Jianwei Gao},
  doi          = {10.1016/j.asoc.2025.113567},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113567},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An offshore photovoltaic risk assessment framework based on probabilistic linguistic multi-criteria decision-making method and consensus-maximizing group information aggregation model},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing detectability of process variability in statistical process control under three-dimensional uncertainty: Evidence from textile industry. <em>ASOC</em>, <em>182</em>, 113566. (<a href='https://doi.org/10.1016/j.asoc.2025.113566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although traditional statistical quality control (SQC) techniques, such as variable control charts (VCCs), are frequently used to monitor production processes, these methods may not be effective in dealing with the uncertainty and imprecision of data encountered in real-world production environments. For example, in textile manufacturing, maintaining high-quality standards is crucial to ensure consistent and reliable fabric products. This study presents an innovative approach by integrating picture fuzzy sets (PFSs) with one of well-known VCCs types named X ̅ − s charts to improve their sensitivity and robustness in identifying variations. As PFSs provide a more comprehensive way to represent uncertainty by incorporating positive, neutral, and negative membership degrees, they enable the creation of a more sensitive quality control method that accounts for both numerical data and subjective assessments. The proposed methodology first checks whether the data is normally distributed, which is an essential requirement for the reliability of the subsequent results. If normality is confirmed, the fuzzy center line (CL) and fuzzy control limits (CLs) are calculated, along with the distances from the center line to ± 1 σ and ± 2 σ values. These distances are used as inputs in a rule-based system that is suggested for VCCs in this paper, which enables categorizations such as "low degree in control" or "high degree out of control" providing a more detailed classification than traditional control charts. The proposed picture fuzzy X ̅ − s control charts is used to monitor variations in the shoulder-to-shoulder length in shirt manufacturing, demonstrating its effectiveness in distinguishing between in-control and out-of-control conditions, thereby offering a more flexible and insightful approach to process monitoring and decision-making in the textile industry.},
  archive      = {J_ASOC},
  author       = {İhsan Kaya and Ali Karaşan and Esra İlbahar and Fatma Kutlu Gündoğdu and Kübra Yazır and Elifnaz Olgaç},
  doi          = {10.1016/j.asoc.2025.113566},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113566},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing detectability of process variability in statistical process control under three-dimensional uncertainty: Evidence from textile industry},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing public–private partnerships in waste management facilities: A risk assessment framework based on a trapezoidal cloud model and bayesian network. <em>ASOC</em>, <em>182</em>, 113565. (<a href='https://doi.org/10.1016/j.asoc.2025.113565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To effectively mitigate the risks associated with urban municipal solid waste public–private partnership (PPP) projects, a comprehensive risk assessment framework intergating a trapezoidal cloud model (TCM) and a Bayesian network (BN) is proposed. The TCM is used to transform these factors, discretize the continuous nodes, and improve the accuracy of the prior probability. Prior risk reasoning, strength analysis, sensitivity analysis and diagnosis analysis are carried out for the event via the BN, and corresponding risk control measures are taken to realize the comprehensive evaluation and control of the dynamic risk of urban municipal solid waste PPP projects. The results indicate that (1) the proposed risk assessment framework can effectively address issues such as data noise and subjective arbitrariness, demonstrating stronger performance and robustness than traditional evaluation methods. (2) Financial risk ( V 1 ) and construction risk ( V 2 ) are primary indicators that significantly impact the final risk level of a project and require strict control. Financing availability risk ( v 1 ), increased construction costs ( v 5 ), and project completion risk ( v 6 ) are key risk factors leading to V 1 and V 2 ; notably, v 1 has the greatest impact throughout the project development trajectory. (3) Social, legal, and market risks are high-risk indicators currently faced by the case project and are the core issues of risk management. Public acceptance ( v 15 ), default risk ( v 17 ), and fee changes ( v 18 ) are the secondary indicators with the greatest impact. This study provides multidimensional management insights and policy insights for stakeholders, supporting the sustainable development of PPP projects for municipal solid waste disposal.},
  archive      = {J_ASOC},
  author       = {Zongbao Feng and Xingchen Zhou and Jiarui Dong and Hongyu Chen and Yang Liu},
  doi          = {10.1016/j.asoc.2025.113565},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113565},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advancing public–private partnerships in waste management facilities: A risk assessment framework based on a trapezoidal cloud model and bayesian network},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-bipolar fuzzy graph and its centrality measure based decision making system for healthcare waste treatment method selection. <em>ASOC</em>, <em>182</em>, 113564. (<a href='https://doi.org/10.1016/j.asoc.2025.113564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy graphs help to handle various real-life uncertain problems and fuzzy preference relations have widely been utilized to deal with various decision-making problems. In reality, the multi-dimensional and counter investigations of a problem always provide an efficient outcome. However, these conventional approaches often lack consideration of counter-views and multi-dimensional perspectives in problem-solving. Therefore, the present study defines the notion of multi-bipolar fuzzy preference relation to fuse the multi-dimensional and bipolar views in handling uncertain relations in real-time problems. The notion of multi-bipolar fuzzy preference relation graph is explored to effectively study the pairwise importance in terms of multi-bipolar fuzzy relations between the multi-bipolar fuzzy sets. Additionally, the concepts of degree, in-degree and out-degree centrality measures are explored within the multi-bipolar fuzzy graph context. On the other hand, these concepts are fused to design a multi-criteria decision-making technique, where the preference relation graph helps to analyze the pairwise importance among the criteria and in-degree centrality aids to consider the importance of a criteria from other criteria. Furthermore, this fusion is implemented to address the healthcare waste treatment selection problem, where steam sterilization and chemical disinfection resulted in first and last rank, respectively. For this problem, five alternatives and their ten essential criteria are considered from the view of technical, social, environmental and economic aspects. To demonstrate the superiority and validity of the proposed technique, a comparative study is performed with existing techniques. Finally, the stability of the results is analyzed through a sensitivity performance.},
  archive      = {J_ASOC},
  author       = {Deva Nithyanandham and Felix Augustin},
  doi          = {10.1016/j.asoc.2025.113564},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113564},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-bipolar fuzzy graph and its centrality measure based decision making system for healthcare waste treatment method selection},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative trade-off analysis on accuracy and efficiency for federated learning in demand forecasting. <em>ASOC</em>, <em>182</em>, 113561. (<a href='https://doi.org/10.1016/j.asoc.2025.113561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is an emerging learning mechanism that can achieve accuracy, efficiency, and privacy concurrently, which could be particularly useful in scenarios where large volumes of sensitive data are involved, such as demand forecasting in inventory management. In this paper, we consider a dataset composing of sales records of multiple products across ten different Walmart stores in the USA, and conduct a comparative study of centralized learning, distributed learning, and FL, focusing on the accuracy of predicting future demand of certain products and the required volume of data for transmission by the multi-layer perceptron model. Our results demonstrate that, with the same number of training rounds, FL achieves competitive accuracy in predicting future demands across the selected product categories while significantly reducing data transmission compared to other learning approaches, highlighting the efficiency and practicality of FL. In addition, we compare the performances of FL approaches with different combinations of store-level data from various regions, and examine the trade-off analysis in terms of accuracy and transmission efficiency under different scenarios.},
  archive      = {J_ASOC},
  author       = {Hang Qi and Jieping Luo and Qiyue Li and Jingjin Wu},
  doi          = {10.1016/j.asoc.2025.113561},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113561},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comparative trade-off analysis on accuracy and efficiency for federated learning in demand forecasting},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on metaheuristics for solving home health care routing and scheduling problems. <em>ASOC</em>, <em>182</em>, 113560. (<a href='https://doi.org/10.1016/j.asoc.2025.113560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the healthcare of elderly people catches wide attention since the increase of aging population puts significant pressure on public medical resources. The population aging and scarce care resources are likely to result in a substantial increase in demand for home health care (HHC) services. In order to improve operation efficiency and reduce service cost, home health care routing and scheduling problems (HHCRSPs) have received many concerns from academia and industry in recent years. HHC provides care services to customers and elderly at their homes by assigning proper caregivers and resources as their requirements. Due to the large scale and strong coupling features, as well as various practical requirements in concrete scenarios, there exist a great many of challenges in addressing the HHCRSPs effectively. Recently, metaheuristics have been employed and made breakthroughs in solving such difficult HHCRSPs. This article aims to provide a comprehensive literature review of metaheuristics for handling HHCRSPs. The existing studies regarding HHCRSPs are firstly summarized and analyzed from the perspectives of optimization objectives, number of objectives, uncertainties, constraint conditions, and number of care centers. Next, the metaheuristic algorithms for handling the HHCRSPs are summed up and dissected from the views of classifications, solution encoding and decoding, ensemble of metaheuristics and local search strategies, test instances, performance metrics, statistical analysis, and stopping criteria. Subsequently, challenges of addressing the HHCRSPs are analyzed and discussed. Afterwards, we point out future research directions and significant research contents. Finally, we conclude this research.},
  archive      = {J_ASOC},
  author       = {Yaping Fu and Jie Dong and Kaizhou Gao and Ponnuthurai Nagaratnam Suganthan and Min Huang and Lihua Zhu},
  doi          = {10.1016/j.asoc.2025.113560},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113560},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A review on metaheuristics for solving home health care routing and scheduling problems},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-criteria selection of data clustering methods for e-commerce personalization. <em>ASOC</em>, <em>182</em>, 113559. (<a href='https://doi.org/10.1016/j.asoc.2025.113559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {E-commerce platforms increasingly rely on personalization to improve the user experience and drive sales, requiring efficient data clustering methods to segment users based on their behavior and preferences. However, due to the many data clustering techniques available, the key decision problem is to choose the optimal grouping method. Decision making based on single decision factors, although widely used, can lead to wrong decisions, so it is worth considering multi-criteria analysis tailored to the specifics of e-commerce customer clustering. Through extensive experiments on real e-commerce datasets, the study demonstrates the strengths and limitations of selected data clustering techniques (including the Approximated Gaussian Mixture Model, which was found to be superior to the classical Gaussian Mixture Model), considering different decision criteria related to various aspects of quality. The results provide valuable insights for e-commerce practitioners seeking to optimize their personalization strategies and ultimately suggest that a novel adaptation of the PROMETHEE II method can provide a robust framework for making informed decisions about selection of data clustering algorithms.},
  archive      = {J_ASOC},
  author       = {Elżbieta Pawełek-Lubera and Mateusz Przyborowski and Dominik Ślęzak and Adam Wasilewski},
  doi          = {10.1016/j.asoc.2025.113559},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113559},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-criteria selection of data clustering methods for e-commerce personalization},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-reconstruction self-rectification framework with momentum memory-augmented network for multivariate time series anomaly detection. <em>ASOC</em>, <em>182</em>, 113558. (<a href='https://doi.org/10.1016/j.asoc.2025.113558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrepancy between the actual contaminated data and the normality assumption poses a serious challenge to existing methods that rely on clean training data. For methods that consider contamination, the mainstream model-level methods with memory-augmented structures struggle with biased similarity measures and fail to utilize historical information, leading to inaccurate reconstruction of latent variables. Most training-level methods may confuse contaminated data with hard-to-learn normal data, affecting the model’s ability to learn normal patterns. Moreover, there is a lack of using adjustment loss to effectively constrain model-level methods to learn normal data while suppressing contaminated data, which limits the further improvement of anomaly detection performance. This paper proposes a D ual-Reconstruction Self- R ectification framework with Mo mentum Me mory-augmented network based on Transformer ( DRMoMe ) for multivariate time series anomaly detection. At the model level, a momentum memory module based on Transformer is proposed, which employs the momentum-updated framework to align the representation space and designs the multihead-attention mechanism with the similarity-based update strategy to ensure the accuracy and diversity of the memory vectors. At the training level, this paper designs a self-rectification framework, which uses the difference between dual-reconstruction paths as the loss adjustment weights to adjust the model’s learning dynamically. Additionally, the method uses the characteristic of the memory module to amplify the weight difference between the contaminated and normal data, effectively integrating the model-level and training-level approach to help the model focus on learning the normal pattern. The DRMoMe outperforms 21 state-of-the-art baselines in experiments conducted on five benchmark datasets from different domains.},
  archive      = {J_ASOC},
  author       = {Bing Xue and Xin Gao and Heping Lu and Baofeng Li and Feng Zhai and Meng Xu and Taizhi Wang and Jiawen Lu},
  doi          = {10.1016/j.asoc.2025.113558},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113558},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual-reconstruction self-rectification framework with momentum memory-augmented network for multivariate time series anomaly detection},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart contract vulnerability detection method based on multi-view fusion. <em>ASOC</em>, <em>182</em>, 113557. (<a href='https://doi.org/10.1016/j.asoc.2025.113557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of blockchain technology, smart contracts have become a major target for attackers due to their wide application and enormous economic value. The potential vulnerabilities in these contracts can lead to serious economic losses. Existing smart contract vulnerabilities detection methods typically rely on extracting control flow graphs, function call graphs, or abstract syntax trees from source code or bytecode to characterize the semantic and syntactic features of the contracts. However, these methods still have deficiencies in the comprehensiveness of feature representation and the allocation of importance weights, which affect the accuracy of vulnerability detection. To address this, we propose a method to detect smart contract vulnerabilities based on the multi-view fusion, called MV-SCVD (Multi-View Smart Contract Vulnerability Detection). This method parses the source code of smart contracts, constructs heterogeneous abstract syntax trees, control flow graphs, and function call graphs, and uses Multi-Level Graph Neural Networks to independently learn the features of the three types of graphs. Finally, it uses an attention network to fuse these features and a classifier to detect vulnerabilities and corresponding types in the contract. Experimental results on a public dataset containing 1269 smart contracts show that compared with existing methods, MV-SCVD significantly improves the detection performance on seven common types of vulnerabilities. The average values of A c c u r a c y , P r e c i s i o n , R e c a l l , and F 1 are increased by 13.67% to 31.25%, 6.92% to 20.09%, 20.78% to 58.90%, and 15.67% to 46.47%, respectively.},
  archive      = {J_ASOC},
  author       = {Zhanqi Cui and Xiguo Gu and Xinhong Duan and Xinian Gu and Zhiwei Wang and Jiale Zhang and Senlin Ren},
  doi          = {10.1016/j.asoc.2025.113557},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113557},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Smart contract vulnerability detection method based on multi-view fusion},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective multi-optima ensemble binary optimization algorithm for identifying optimal set of features for ECG-based identification. <em>ASOC</em>, <em>182</em>, 113556. (<a href='https://doi.org/10.1016/j.asoc.2025.113556'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing the number of input features for a machine learning model decreases its complexity and computation time. However, it is crucial to choose the best set of features without compromising the model's performance. There could be several subsets of features with optimal behavior. Evolutionary algorithms are great for feature optimization. However, different evolutionary algorithms may produce different solutions, and their performance is influenced by the size of the data and the types of features. To address these issues, three popular algorithms, Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Binary Differential Evolution (BDE) have been adapted to accommodate multiple populations for achieving multiple optima. The BDE algorithm applied here is a novel variant with modified mutation and crossover operators. Then they are combined to create a novel 'Multi-Objective Multi-Optima Ensemble Binary Optimization Algorithm. The algorithm has been tested on 71 fiducial ECG features including temporal, amplitude, distance, slope, angular, and HRV features for ECG-based identification. These 71 features can identify individuals using the SVM classifier with 98 % accuracy. With 71 features, there could be a maximum of 2 71 subsets. The optimization objective is to find all feature subsets that maximize classifier accuracy while minimizing the number of features. The ensemble optimizer has found 190 unique optimized subsets. These subsets have been analyzed to identify critical features for identification. The most optimal subset with the minimum number of features and maximum accuracy has been identified. The practical implementation of an ECG-based identification system requires an efficient system that can process incoming signals, extract features from the signal, and identify individuals in the shortest time possible. To speed up the processing of the input signal, a novel DFA-based algorithm has been proposed to identify fiducial points P, Q, R, S, and T from an ECG signal. The proposed algorithm applies to both recorded and live ECG signals.},
  archive      = {J_ASOC},
  author       = {Mamata Pandey and Anup Kumar Keshri},
  doi          = {10.1016/j.asoc.2025.113556},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113556},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective multi-optima ensemble binary optimization algorithm for identifying optimal set of features for ECG-based identification},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud detection network based on context feature enhancement for remote sensing images. <em>ASOC</em>, <em>182</em>, 113553. (<a href='https://doi.org/10.1016/j.asoc.2025.113553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud detection aims to identify cloud regions in satellite remote sensing images. This task remains particularly challenging due to the diverse morphological characteristics of clouds and their spectral similarity to bright ground surfaces, such as snow or sand. Most existing methods still suffer from unsatisfactory performance, primarily due to their inability to effectively model global information, which restricts the model’s understanding of the differences between cloud regions and the background. To address these issues, we propose a novel framework called the Contextual Feature Enhancement Network (CFEnet), which enhances contextual representations by incorporating global and multi-scale information. Specifically, we first introduce the global information modeling module (GIMM), which captures long-range dependencies and global context at multiple scales, enabling the model to comprehensively perceive image features. Subsequently, we design a multi-scale feature pyramid (MSFP) to gradually integrate high-resolution low-level features with low-resolution high-level features. Finally, a feature fusion module (FFM) is employed to fuse feature maps from different modules. Our method’s effectiveness was evaluated on three public datasets: GF-1 WFV, LandSat8, and SPARCS. Experimental results demonstrated that CFEnet significantly outperforms several state-of-the-art methods, achieving an average improvement of 1.77% in detection precision.},
  archive      = {J_ASOC},
  author       = {Baotong Su and Yao Chen and Wenguang Zheng},
  doi          = {10.1016/j.asoc.2025.113553},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113553},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cloud detection network based on context feature enhancement for remote sensing images},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discriminative distance metric learning via class-center guidance. <em>ASOC</em>, <em>182</em>, 113552. (<a href='https://doi.org/10.1016/j.asoc.2025.113552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance metric learning is a technique of great importance to machine learning and data processing, which can effectively improve the generalization performance of algorithms related to distance metrics. The method projects the original data to the metric space through a transformation to realize the automatic adjustment of the distance between samples, so as to achieve the increase of the between-class distance and the decrease of the within-class distance. To better achieve this goal, we propose a discriminative distance metric learning via class-center guidance (DML-CG). The proposed DML-CG learns a novel discriminative distance metric by maximizing the trace ratio of between-class covariance to within-class covariance, and at the same time transforms the trace ratio problem into a ratio-trace problem to find the global optimal solution. In addition, this method selects k nearest neighbors for each training sample to generate sample pairs, and jointly uses local metrics learned from multiple class-center guidance and a global metric to guide samples of the same class closer to the class center, and samples of different class farther away from the sample class center. This achieves both the distance metric and captures the discriminative structure of the data. Meanwhile, global regularization is introduced to improve the generalization performance and control overfitting. We design an alternating iteration algorithm to optimally solve the proposed method and theoretically analyze the convergence and complexity. Finally, the effectiveness of the proposed algorithm is demonstrated on structured artificial datasets and UCI datasets as well as unstructured image recognition datasets. Most of the results show that the proposed algorithm outperforms other state-of-the-art distance metric learning methods.},
  archive      = {J_ASOC},
  author       = {Shijie Zhao and Liang Cai and Fanshuai Meng and RongHua Yang},
  doi          = {10.1016/j.asoc.2025.113552},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113552},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discriminative distance metric learning via class-center guidance},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Striking a better balance between segmentation performance and computational costs with a minimalistic network design. <em>ASOC</em>, <em>182</em>, 113549. (<a href='https://doi.org/10.1016/j.asoc.2025.113549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While many modern segmentation models achieve appealing performance, they often come with high computational costs, such as massive parameter counts, intensive calculations, excessive memory consumption, and prolonged runtime. These burdens make them impractical for resource-limited medical devices. To address this challenge, we strategically integrate several established modules, including asymmetric convolution, dilated convolution, and attention modules, to develop a novel medical image segmentation backbone named BMIS. Specifically, unlike traditional U-Nets, BMIS adopts a shallower yet wider structure. This design not only enables effective learning of high-level semantic features but also markedly mitigates the loss of fine-grained details caused by frequent downsampling. Consequently, the decoder in BMIS can focus primarily on spatial reconstruction rather than compensating for lost details, which greatly reduces its workload. Leveraging these benefits, the decoder of BMIS does not need to be as complex as its encoder, and skip connections between the encoder and decoder are no longer necessary. These two factors collectively reduce model complexity without compromising segmentation performance. Extensive segmentation experiments across five medical imaging modalities demonstrate that BMIS achieves a better balance between segmentation performance and computational costs compared to twenty-nine competing methods. In short, compared with the best-performing comparative method DSU-Net, BMIS improves IoU by ∼ 0.32%, while substantially reducing model parameters by 97.54%, FLOPs by 68.49%, GPU memory usage by 54.48%, training time by 58.82%, and inference time by 80.00%. These impressive results highlight BMIS’s great potential for deployment in medical devices with limited computational resources.},
  archive      = {J_ASOC},
  author       = {Duwei Dai and Caixia Dong and Xu Yang and Zongfang Li and Songhua Xu},
  doi          = {10.1016/j.asoc.2025.113549},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113549},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Striking a better balance between segmentation performance and computational costs with a minimalistic network design},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A long-term precise prediction model for water quality in aquaculture based on an adaptive decomposition transformer. <em>ASOC</em>, <em>182</em>, 113548. (<a href='https://doi.org/10.1016/j.asoc.2025.113548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of water quality parameters is crucial for the sustainable development of the aquaculture industry. Dissolved oxygen (DO) is an important indicator of water quality. Its fluctuations directly affect the living conditions of aquatic organisms and significantly affect the stability of aquaculture systems. However, accurate predictions still encounter challenges owing to the nonstationary nature of DO data, the complexity of trend changes, and the dynamics of local features. Therefore, this study proposed a DO time-series prediction model based on Transformer. Firstly, an adaptive sequence decomposition method is proposed to overcome the limitations of using a single raw sequence as input and to more effectively extract different feature patterns across multiple temporal scales. Secondly, a multi-resolution causal attention mechanism is designed to enhance temporal feature representation and improve the capture of local patterns, thereby strengthening the model’s ability to model causal relationships at different scales. Lastly, to address the challenge that traditional decomposition-based methods rely heavily on attention mechanisms for trend prediction and often fail to model trends effectively, we propose a trend perception module to improve the extraction of key trend information and detailed features. The experiments are based on six marine ranch datasets collected from diverse geographic and climatic regions, including Australia, the United States, and China. These datasets span from 2013 to 2022, covering a variety of typical scenarios such as natural waters, intensive aquaculture zones, and complex coastal bays. Using the Baffle Creek dataset as an example, the proposed model achieves an average reduction of 24.72% in MAE, 19.68% in RMSE, and 24.93% in MAPE, while improving R 2 by 38.04% compared to other advanced models. These results demonstrate that the proposed model offers superior prediction accuracy and generalization capability, providing reliable support for the sustainable management of aquaculture systems.},
  archive      = {J_ASOC},
  author       = {Dashe Li and Lu Liu and Xiaodong Ji},
  doi          = {10.1016/j.asoc.2025.113548},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113548},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A long-term precise prediction model for water quality in aquaculture based on an adaptive decomposition transformer},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analytical-heuristic modeling and optimization for low-light image enhancement. <em>ASOC</em>, <em>182</em>, 113546. (<a href='https://doi.org/10.1016/j.asoc.2025.113546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement remains an open problem, and the new wave of artificial intelligence is at the center of this problem. This work describes the use of genetic algorithms for optimizing analytical models that can improve the visualization of images with poor light. The main goal of low-light image enhancement is to produce an image with features similar to those of a well-taken photograph under optimal lighting conditions. We propose a balanced analytical-heuristic method combined with optimization reasoning to approach a solution to the physical and computational aspects of transforming dark images into visible ones. The experiments demonstrate that the dichotomy-tuned approach ranks at the top among 26 state-of-the-art algorithms in the LOL (LOw-Light) benchmark, reaching a staggering 27.1717 in the synthetic version LOLv2. Moreover, the proposed dichotomy-tuned algorithm provides a pleasant visual appearance with room for improvement. The results show evidence that a simple genetic algorithm combined with analytical reasoning can defeat the current mainstream in a challenging computer vision task through controlled experiments and objective comparisons. This work opens interesting new research avenues for the soft computing community and others interested in analytical and heuristic reasoning.},
  archive      = {J_ASOC},
  author       = {Axel Martinez and Emilio Hernandez and Matthieu Olague and Gustavo Olague},
  doi          = {10.1016/j.asoc.2025.113546},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113546},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analytical-heuristic modeling and optimization for low-light image enhancement},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A memetic algorithm based on feasible solutions with local search for fuzzy unrelated parallel machine scheduling with fuzzy cost constraint. <em>ASOC</em>, <em>182</em>, 113545. (<a href='https://doi.org/10.1016/j.asoc.2025.113545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new collaborative manufacturing mode based on the industrial internet platform is driving the intelligent transformation of the manufacturing industry. In this paper, we investigate the scheduling problem of minimizing total weighted tardiness under the fuzzy cost constraint on unrelated parallel machines for jobs with fuzzy processing times and due dates in the new manufacturing environment of shared manufacturing resources on the industrial internet platform. The total weighted tardiness represents the sum of the products of each job’s tardiness (i.e., the amount of time by which each job’s completion time exceeds its due date) and its corresponding weight. To address this problem, a method for determining the fuzzy cost budget is proposed first. Subsequently, machine candidate lists of jobs under the respective cost constraints are proposed. Based on this, the initial population generation rule, crossover operator, and mutation operator are designed. Furthermore, by analyzing the property between two adjacent jobs, the job-swapping property within the machine is proposed, and the local search operator is designed to improve the quality of the solution. Finally, based on the above modules, a memetic algorithm based on feasible solutions (MAFS) is designed to solve the problem. Through extensive simulation experiments, the MAFS algorithm is compared with the commercial solver Gurobi and several meta-heuristic algorithms. The experimental results show that, for all instances, the average relative error of the MAFS algorithm is -0.2%, which is respectively 42.77%, 95.73%, 111.13%, 108.20% lower than that of ant colony optimization (ACO), genetic algorithm (GA), simplified swarm optimization (SSO) and Jaya algorithms, thus verifying the effectiveness of the MAFS algorithm.},
  archive      = {J_ASOC},
  author       = {Kai Li and Liping Xu and Han Zhang and Jianfu Chen and Tao Zhou},
  doi          = {10.1016/j.asoc.2025.113545},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113545},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A memetic algorithm based on feasible solutions with local search for fuzzy unrelated parallel machine scheduling with fuzzy cost constraint},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cold chain delivery route modeling and optimizing based on the clustered whale optimization algorithm. <em>ASOC</em>, <em>182</em>, 113544. (<a href='https://doi.org/10.1016/j.asoc.2025.113544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In addressing the optimization problem of cold chain logistics distribution paths under multiple constraints, comprehensive consideration is given to refrigeration parameters, cargo damage rates, and carbon emission factors. Systematic analysis is performed to quantify the combined effects of load capacity and ambient temperature on total operational costs. A traffic condition monitoring mechanism is subsequently integrated to dynamically evaluate roadway statuses, thereby enabling the acquisition of empirically validated transportation durations. Based on these operational parameters, a traffic-responsive optimization model for cold chain logistics (CCL) distribution routes is formulated. To address the complex multimodal characteristics of the model, the Clustering Whale Optimization Algorithm (CWOA) is proposed. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering is employed to achieve dynamic population reorganization. An innovative path encoding rule based on search agent addresses is developed, with a sine-cosine oscillation operator introduced to replace linear search strategies during stochastic search processes, thereby enhancing the flexibility of individual search movements. Comparative testing on 23 benchmark functions from the IEEE Congress on Evolutionary Computation (CEC) effectively verifies CWOA's high precision and rapid convergence performance. The model and algorithm are subsequently applied to simulation experiments for cold chain logistics distribution in the Yangtze River Delta region, demonstrating CWOA's superior capability in solving CCL distribution path planning problems.},
  archive      = {J_ASOC},
  author       = {Zhe Sun and Shengnan Ma and Yongbo Jian and Yubin Lu and Zhixin Sun},
  doi          = {10.1016/j.asoc.2025.113544},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113544},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cold chain delivery route modeling and optimizing based on the clustered whale optimization algorithm},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imbalanced broad learning system with label relaxation and sample weight adaptation. <em>ASOC</em>, <em>182</em>, 113543. (<a href='https://doi.org/10.1016/j.asoc.2025.113543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Broad Learning System (BLS), as a lightweight network architecture, has been extensively applied to various classification and regression tasks. However, BLS and its variants remain suboptimal for addressing imbalanced classification problems. These models often pay little attention to the quality of original features. Their supervision mechanisms typically rely on strict binary label matrices, which impose limitations on approximation and fail to align with the underlying data distribution. Additionally, they generally do not differentiate the contributions of majority and minority classes, leading to a bias towards majority classes in predictions. In this paper, we propose a novel imbalanced BLS framework that integrates label relaxation and sample weight adaptation to address challenges in imbalanced classification tasks. First, genetic programming is employed to optimize the original features, improving data representation capability. Then, a latent label space is constructed based on pairwise label relationships, which serves to achieve flexible label relaxation. Furthermore, a dynamic weighting mechanism is proposed based on intra-class and inter-class distributions to balance the influence of majority and minority classes. Extensive experiments conducted on 30 benchmark datasets demonstrate that the proposed method significantly outperforms various state-of-the-art approaches, with average G-mean and AUC scores of 89.6% and 90.0%, respectively. These results validate the effectiveness and superiority of the proposed model in addressing imbalanced classification tasks.},
  archive      = {J_ASOC},
  author       = {Yanting Li and Yusha Wang and Junwei Jin and Weiwei Zhang and Hongwei Tao and Huaiguang Wu and C.L. Philip Chen},
  doi          = {10.1016/j.asoc.2025.113543},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113543},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Imbalanced broad learning system with label relaxation and sample weight adaptation},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative emulation and uncertainty quantification of geological CO2 storage with conditional diffusion models. <em>ASOC</em>, <em>182</em>, 113542. (<a href='https://doi.org/10.1016/j.asoc.2025.113542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carbon capture and storage (CCS) has emerged as a pivotal technology for reaching climate-neutrality targets. Safe and effective deployment of CCS requires reliable predictions of pressure buildup and CO 2 plume migration under geological uncertainties. However, traditional numerical simulations are limited by computational inefficiency, while machine learning methods face bottlenecks in predictive accuracy and uncertainty. Here we introduce a generative emulation framework named DiffMF for efficient prediction of multiphase flows in geological CO 2 storage. The framework treats flow prediction as conditional generation processes and employs cutting-edge diffusion models to produce the temporal–spatial evolution of pressure and CO 2 saturation fields under varying geological property conditions. Unlike existing approaches that focus primarily on point estimation, the probabilistic nature of DiffMF allows for generating multiple predictions that align with the statistics of the underlying dynamics, thereby facilitating effective quantification of predictive uncertainty. Comprehensive evaluations on diverse CO 2 storage cases show that DiffMF achieves up to 52.6% lower CO 2 saturation error compared to leading baseline models while maintaining high accuracy even under increased geological heterogeneity. Furthermore, we interpret the black-box model via visual analysis, providing insights into the generation process of DiffMF. Finally, the application to uncertainty quantification and propagation task for a field-scale storage system demonstrates that DiffMF yields statistics of the system responses in close agreement with those derived from high-fidelity simulations while executing 100 times faster, underscoring its promising potential in practical applications. The proposed generative emulation paradigm enables real-time prediction and probabilistic modeling that can foster informed decision-making for CCS deployment.},
  archive      = {J_ASOC},
  author       = {Zhongzheng Wang and Yuntian Chen and Guodong Chen and Qiang Zheng and Tianhao Wu and Dongxiao Zhang},
  doi          = {10.1016/j.asoc.2025.113542},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113542},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generative emulation and uncertainty quantification of geological CO2 storage with conditional diffusion models},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cooperative co-evolutionary search for meta multigraph and graph neural architecture on heterogeneous information networks. <em>ASOC</em>, <em>182</em>, 113541. (<a href='https://doi.org/10.1016/j.asoc.2025.113541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To model the rich semantic information on heterogeneous information networks (HINs), heterogeneous graph neural architecture search (HGNAS) has become a research hotspot, as it offers a promising automatic search technique for heterogeneous graph neural networks (HGNNs). However, there is no method that can simultaneously solve the meta multigraph and neural architecture search, which are the two core problems of HGNAS. In addition, existing HGNAS methods can only search for the meta graph or determine the number of edge types by setting a threshold hyperparameter, which has limited expression or is difficult to determine and significantly affects performance. In this paper, a cooperative co-evolutionary meta multigraph and graph neural architecture search method (called CCMG) on HINs is proposed. Specifically, CCMG first represents the meta multigraph and neural architecture by discrete encodings, and the number of network layers is variable. Second, whether an encoding of the architecture is meaningful or not is affected by the value of the encoding taken at the corresponding meta multigraph position and their search space sizes are not imbalanced. To cope with these situations, they are cooperatively and collaboratively optimized in the form of subproblems, facilitating group collaboration and information sharing. Finally, the effectiveness and superiority of the CCMG are verified on six datasets for node classification and recommendation tasks. Over the comparison HGNAS method, CCMG improves its performance on the two tasks by an average of 2.29% and 1.21%, respectively.},
  archive      = {J_ASOC},
  author       = {Yang Liu and Xiangyi Teng and Jing Liu},
  doi          = {10.1016/j.asoc.2025.113541},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113541},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cooperative co-evolutionary search for meta multigraph and graph neural architecture on heterogeneous information networks},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN-based approach for data imputation and handling class imbalance using one class ensemble. <em>ASOC</em>, <em>182</em>, 113540. (<a href='https://doi.org/10.1016/j.asoc.2025.113540'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance in real-world datasets is a significant issue that results in bias in the machine learning model and may result in incorrect predictions. In this paper, a GAN-based Multiple Imputation One-Class Ensemble (GMI-OCE) is presented for imbalanced classification in scenarios with missing values in the dataset. The approach uses a hybrid OCC ensemble, incorporating a GAN architecture for imputing missing values and boosting the number of minority class instances without modifying the observed values directly. A two-step bootstrap aggregation is applied using a novel weighting algorithm that considers the accuracy of individual classifiers and their performance on synthetic data. The approach is evaluated on various imbalanced datasets and compared against seven baseline methods. The results indicate that GMI-OCE outperforms in most of the datasets compared to other methods based on various evaluation metrics.},
  archive      = {J_ASOC},
  author       = {Pranita Baro and Malaya Dutta Borah},
  doi          = {10.1016/j.asoc.2025.113540},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113540},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GAN-based approach for data imputation and handling class imbalance using one class ensemble},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-branch encoder context-aware fusion network for ultrasound image segmentation. <em>ASOC</em>, <em>182</em>, 113538. (<a href='https://doi.org/10.1016/j.asoc.2025.113538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of lesion regions in ultrasound images remains a challenging task. Recent research has focused on integrating Transformers and CNNs to leverage their complementary strengths. However, most existing methods employ coarse fusion strategies that often lead to the loss of critical local details, such as lesion boundaries. Additionally, these methods fail to fully leverage the Transformer’s capability for global context modeling, thereby limiting their effectiveness in enhancing comprehensive feature representation. To this end, we propose a dual-branch encoder context-aware fusion network (DECF-Net) for automatic and robust lesion segmentation. The network introduces a parallel dual-branch encoder architecture to simultaneously capture global information and maintain sensitivity to the low-level context. We present a progressive feature extraction (PFE) module suitable for the Transformer branch, which aims to effectively suppress clutter noise and emphasize local features. In order to facilitate the interaction and fusion of feature information between different branches, we further introduce a supplementary feature fusion (SFF) module. In addition, we present a spatial channel attention bridge (SCAB) module to enhance the features of skip connections, which can extract multi-stage and multi-scale context information. Experimental results show that DECF-Net exhibits competitive segmentation performance in both qualitative and quantitative evaluation.},
  archive      = {J_ASOC},
  author       = {Ning Yang and Xinhui Jia and Chunyu Hu and Yuang Zhang and Lei Lyu},
  doi          = {10.1016/j.asoc.2025.113538},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113538},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual-branch encoder context-aware fusion network for ultrasound image segmentation},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling evasive portable documents with explainable kolmogorov-arnold networks resilient to generative adversarial attacks. <em>ASOC</em>, <em>182</em>, 113537. (<a href='https://doi.org/10.1016/j.asoc.2025.113537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portable Document Format (PDFs) files have become a serious threat to organizational security, as adversaries exploit their popularity and rich JavaScript environment to launch cyberattacks. Although Machine Learning (ML) methods have been developed for PDF malware detection, they remain vulnerable to adversarial attacks. To address this issue, we propose an efficient, explainable, and robust PDF malware detector that is resilient to generative adversarial attacks and effective against evasive malware using a 4-Layered 5-Fold Kolmogorov-Arnold Network (4L5FKAN). Our approach leverages Kolmogorov-Arnold Networks (KAN), a novel neural network architecture that has emerged as a strong alternative to traditional Multi-Layer Perceptron (MLP) models. To train our model, we constructed a comprehensive dataset by collecting over 100,000 raw PDFs from various sources, ensuring the inclusion of evasive malware samples through an extensive PDF mining process. The proposed 4L5FKAN model is designed to be exploit-agnostic to specific exploit patterns, making it resilient to Generative Adversarial Network (GAN) based attacks, enhancing interpretability using custom-built Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP). These explanation techniques provide privacy-preserved conservative explanations for model predictions, ensuring transparency. Our experimental results demonstrate that the proposed 4L5FKAN model achieves an outstanding detection accuracy of 98.7%–99.8% on unseen samples, outperforming existing state-of-the-art methods. Furthermore, it exhibits more than 25% reduction in false positives compared to conventional MLP-based approaches and shows a 30% increase in adversarial robustness against GAN-generated malware samples. These results highlight the effectiveness of our model in detecting evasive PDF malware while maintaining high interpretability and resilience to adversarial attacks.},
  archive      = {J_ASOC},
  author       = {S.P. Sharmila and Shubham Gupta and Aruna Tiwari and Narendra S. Chaudhari},
  doi          = {10.1016/j.asoc.2025.113537},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113537},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unveiling evasive portable documents with explainable kolmogorov-arnold networks resilient to generative adversarial attacks},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight unmanned aerial vehicles anomaly detection model based on synaptic evolution mechanism and layer-adaptive neural network. <em>ASOC</em>, <em>182</em>, 113536. (<a href='https://doi.org/10.1016/j.asoc.2025.113536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide application of unmanned aerial vehicles (UAVs) puts strict requirements on reliable operation, and anomaly detection is a crucial method to ensure the reliability of UAVs. Existing anomaly detection models are highly dependent on time-series log data, and models based on Long Short-Term Memory (LSTM) are widely used due to their effectiveness in processing time-series data. However, the complex internal structure of LSTM involves many learning parameters. In addition, the traditional static parameter pruning methods fail to balance the conflict between performance and parameter scale dynamically. To address the above problems, this paper proposes a lightweight anomaly detection model based on the synaptic evolutionary mechanism and layer-adaptive neural network (LUV-DSA), which can be deployed in resource-constrained UAV application scenarios. Firstly, LUV-DSA simplifies the internal structure of LSTM by optimising the cell state update process with a new linearly weighted computational method. Secondly, inspired by the evolution of biological synapses, a method for intra-layer parameter pruning and inter-layer structured pruning is designed. For intra-layer parameters, LUV-DSA achieves dynamic model parameter competition by simulating the self-optimisation of synapses, minimising parameter scale while ensuring performance. For inter-layer structures, LUV-DSA enables inter-layer adaptation by calculating plasticity factors to assess the contribution of each layer. The experimental results show on seven UAV datasets that the model significantly reduces the number of parameters and inference time while ensuring accuracy. For example, on the ALFA dataset, LUV-DSA achieves 99.51 % accuracy with 96.14 % fewer parameters than MobileNetV4.},
  archive      = {J_ASOC},
  author       = {Rong Zeng and Hongli Deng and Bochuan Zheng and Yu Lu},
  doi          = {10.1016/j.asoc.2025.113536},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113536},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lightweight unmanned aerial vehicles anomaly detection model based on synaptic evolution mechanism and layer-adaptive neural network},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating feature selection and fuzzy decision-making: A spherical triangular fuzzy number based framework for large-scale decision-making. <em>ASOC</em>, <em>182</em>, 113535. (<a href='https://doi.org/10.1016/j.asoc.2025.113535'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel Fuzzy Large-Scale Decision-Making (FLSDM) framework designed to address the complexities of managing a large number of criteria in fuzzy decision-making contexts. While Multi-criteria decision-making (MCDM) methods are widely used across disciplines, traditional approaches often struggle when confronted with high-dimensional decision parameters. To overcome this, we propose an integrated feature selection algorithm that integrates machine learning (ML) algorithms, namely Extreme Gradient Boosting (XGBoost), Support Vector Machine-Recursive Feature Elimination (SVM-RFE), and ReliefF, within a triangular spherical fuzzy (STFN) environment to select the core criteria from a large dataset. Additionally, we extend the Integrated Determination of Objective Criteria Weights (IDOCRIW) and Additive Ratio Assessment (ARAS) methods for the STFN environment to calculate criteria weights and rank alternatives under fuzziness, respectively. The application of the proposed framework is demonstrated through a case study of ranking 10 sustainable energy sources based on a comprehensive set of sustainability indicators, including economic, technical, social, environmental, and political dimensions. Extensive robustness and sensitivity analyses validate the model’s effectiveness in managing complex, large-scale decision scenarios.},
  archive      = {J_ASOC},
  author       = {Priya Sharma and Mukesh Kumar Mehlawat and Pankaj Gupta and Weiping Ding},
  doi          = {10.1016/j.asoc.2025.113535},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113535},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating feature selection and fuzzy decision-making: A spherical triangular fuzzy number based framework for large-scale decision-making},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable reasoning schema for fuzzy knowledge graph under interval type-2 fuzzy model. <em>ASOC</em>, <em>182</em>, 113534. (<a href='https://doi.org/10.1016/j.asoc.2025.113534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) require the ability to model the prevalent fuzzy semantics of the real world to better align with human cognition and maximize their value. However, the standard RDF model inadequately expresses deep fuzzy semantics between entities and lacks interpretability, especially for deep learning-based reasoning. To address these limitations, this work proposes an interpretable reasoning schema by introducing the Interval Type-2 fuzzy (IT2_F) model. Specifically, we employ IT2_F theory to describe predicate fuzzy semantics, propose new fuzzy semantic extension principles, and design a corresponding interpretation mechanism for credible fuzzy semantic discovery. Evaluations demonstrate that our approach uncovers richer deep fuzzy semantic information and provides users with more effective and reasonable traceability services.},
  archive      = {J_ASOC},
  author       = {Pu Li and Guopeng Cheng and Yongqi Zhao and Luyang Liu and Meifang Chen and Zhengang Ma and Xiaoyu Chen},
  doi          = {10.1016/j.asoc.2025.113534},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113534},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interpretable reasoning schema for fuzzy knowledge graph under interval type-2 fuzzy model},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A matheuristic method for the automated guided vehicle scheduling problem with flexible charging and job release. <em>ASOC</em>, <em>182</em>, 113531. (<a href='https://doi.org/10.1016/j.asoc.2025.113531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated guided vehicles (AGVs) are vital in modern manufacturing for efficient material transport, requiring optimized scheduling to enhance system performance. This study introduces a new AGV scheduling problem that integrates task assignments, processing sequences, and flexible charging operations while accounting for job release times. The problem is NP-hard since it combines parallel machine scheduling and bin packing. We first formulate it as a mixed-integer linear program (MILP), which is strengthened by a set of valid inequalities. To address practical-sized instances, a matheuristic approach combining MILPs and an adaptive large neighborhood search is proposed. Key innovations include a three-step initialization algorithm for high-quality solutions, tailored destroy-and-repair operators, and a specialized evaluation function for efficient makespan approximation. Extensive experiments on 360 instances show that the matheuristic outperforms the commercial solver CPLEX in both solution quality and efficiency. Sensitivity analyses offer managerial insights into factors like charging strategies and energy management, supporting decision-making in AGV scheduling. A performance profit plot and a time-to-target plot are drawn to further validate the performance of the proposed matheuristic. The method also achieves 230 new best solutions for benchmark problems with slight modifications, demonstrating its versatility and effectiveness.},
  archive      = {J_ASOC},
  author       = {Shanshan Zhou and Zheng Wang and Yantong Li and Xin Wen},
  doi          = {10.1016/j.asoc.2025.113531},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113531},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A matheuristic method for the automated guided vehicle scheduling problem with flexible charging and job release},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Texture-aware 3D gaussian splatting for sparse view reconstructions. <em>ASOC</em>, <em>182</em>, 113530. (<a href='https://doi.org/10.1016/j.asoc.2025.113530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 3D Gaussian Splatting (3DGS) has achieved high rendering quality in Novel View Synthesis (NVS). However, as the number of input views decreases, 3DGS fails to recover the details of the captured 3D scene due to insufficient constraints. We find that the difficulty of Gaussian primitives to concentrate on texture-rich areas leads to this reconstruction degradation. To this end, we propose TA-GS, a texture-aware framework for sparse-view NVS tasks. Specifically, TA-GS introduces a Texture-Based Gaussian Migration strategy, which detects low-opacity Gaussian primitives and migrates them to texture-rich regions, improving the fidelity of texture representation. Additionally, we utilize the texture of depth maps and introduce a Depth Texture Alignment method to constrain the geometric structures. To prevent overfitting to sparse input views, TA-GS employs Phantom View Regularization to enrich texture information from additional phantom views. Extensive experiments demonstrate that our approach outperforms previous methods across a variety of datasets, including LLFF, Mip-NeRF360, DTU, and Blender.},
  archive      = {J_ASOC},
  author       = {Xinyuan Hu and Changyue Shi and Chuxiao Yang and Minghao Chen and Xiaoling Gu and Jiajun Ding and Jifa He and Jianping Fan},
  doi          = {10.1016/j.asoc.2025.113530},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113530},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Texture-aware 3D gaussian splatting for sparse view reconstructions},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chaos-DFT-MLP: Chaos transform and DFT combination MLP-like architecture for the body constitution assisted recognition using medical images. <em>ASOC</em>, <em>182</em>, 113528. (<a href='https://doi.org/10.1016/j.asoc.2025.113528'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Body constitution recognition (BCR) is a key point of the Traditional Chinese Medicine (TCM). Intelligent BCR not only assists Chinese physicians in making a quick diagnosis but also helps people gain an initial understanding of their physical state. Multi-label BCR is an important task, which combines the TCM body constitution theory and artificial intelligence (AI) methods. There is a lack of MLP-like architectures for the multi-label BCR task with tongue images database. Thus, a new Multi-label Tongue Body Constitution (MTBC) dataset is collected. Inspired by the Chaos transform theory, an MLP-like module designed using the Chaos transform is also challenging. For the multi-label BCR task, we propose a novel MLP-like architecture, called Chaos-DFT-MLP, which integrates medical image features fused Chaos transform and discrete Fourier transform (DFT) by the channel fork module (CFM). Moreover, three typical chaos mappings are discussed in the designed Chaos-DFT-MLP architecture. Finally, several Convolutional neural network (CNN)-based models, Transformer-based models, and six state-of-the-art (SOTA) MLP-like models, including Wave-MLP, Vip, Cycle-MLP, Active-MLP, Strip-MLP, and DFT-MLP are employed to assess the performance of the Chaos-DFT-MLP in comparison experiments. The results demonstrate that the proposed Chaos-DFT-MLP outperforms the multi-label BCR task, with mAP, AUC, and Acc values of 51.30%, 73.43%, and 83.23%, which are respectively 1.84, 3.07, and 0.78 percentage points higher than Strip-MLP superiority over published MLP-like methods. Besides, the performance of the Chaos-DFT-MLP has also been verified on the multi-label tongue disease location (MTDL) dataset and NIH Chest X-ray dataset.},
  archive      = {J_ASOC},
  author       = {Mengjian Zhang and Guihua Wen and Pei Yang and Changjun Wang and Xuhui Huang and Chuyun Chen},
  doi          = {10.1016/j.asoc.2025.113528},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113528},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chaos-DFT-MLP: Chaos transform and DFT combination MLP-like architecture for the body constitution assisted recognition using medical images},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MetaQuant: A framework for metaheuristic based quantification. <em>ASOC</em>, <em>182</em>, 113527. (<a href='https://doi.org/10.1016/j.asoc.2025.113527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristics are widely employed in various domains; however, their application in quantification learning remains largely unexplored. This study introduces a pioneering approach where the quantification task is treated as a global optimization problem and formulated as a linear regression model solvable by any metaheuristic algorithm. This novel methodology addresses the limitations of traditional quantifiers solved using deterministic methods, such as being trapped in local optima, encountering the curse of dimensionality, and necessitating non-convex cost functions. The paper presents MetaQuant, a novel framework for metaheuristic-based quantification algorithms, and conducts an extensive evaluation comparing 48 nature-inspired metaheuristic algorithms implementing quantifiers with 20 traditional quantifiers. The evaluation encompasses 31 multi-class datasets and reveals compelling results. The findings demonstrate the robustness and exceptional performance of metaheuristic quantifiers, with all 48 MetaQuantifiers outperforming 19 state-of-the-art quantifiers and 18 MetaQuantifiers surpassing the best traditional quantifier. This study contributes towards a promising new line of research in quantification learning.},
  archive      = {J_ASOC},
  author       = {Adriane B.S. Serapião and Zahra Donyavi and Gustavo Batista},
  doi          = {10.1016/j.asoc.2025.113527},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113527},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MetaQuant: A framework for metaheuristic based quantification},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rapid detection and identification of road manhole cover safety hazards in complex scenarios. <em>ASOC</em>, <em>182</em>, 113525. (<a href='https://doi.org/10.1016/j.asoc.2025.113525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road manhole covers are vital for urban transportation. This paper proposes MCPH-YOLO (Manhole Cover Potential Hazard-YOLO), a novel detection algorithm based on YOLOv8n, to address the inefficiency and high cost of manual inspection of road manhole cover hidden dangers. It can quickly and accurately detect the hidden danger states of road manhole covers. Firstly, due to the limited public datasets for road manhole covers, a new dataset was created. Secondly, to solve the problem of the SPPF module in YOLOv8n losing some spatial details during feature extraction, an Adaptive Multi-scale Feature Fusion module (AMS2F) was designed to replace the original SPPF module. Finally, considering the key features in road manhole cover hidden danger states, an Edge Information and Spatial Feature Fusion module (EIS2F) was developed and integrated into the C2f modules of YOLOv8n's feature fusion layer to form a new C2fEISF module, enabling the extraction and fusion of low-level and high-level features of road manhole covers. Experimental results show that MCPH-YOLO has high accuracy for detecting different hidden danger states: 85.7 % for manhole cover intact (Good), 95.6 % for manhole cover lost (Lose), 95.8 % for manhole cover broken (Broken), 94.4 % for manhole cover covered improperly (Uncovered), and 91.4 % for manhole cover periphery problems (Periphery). The mAP is 92.6 %, with a detection speed of 6.5 ms/frame (53 FPS), 5.0 M parameters, and 8.7 G floating-point operations. Compared to eleven models like Faster RCNN, SSD, RT-DETR-l, YOLOv9t, and YOLOv10n, it has the highest mAP, is easy to deploy, and offers faster detection while maintaining high accuracy. This makes it suitable for real-time detection of road manhole cover hidden danger states in complex scenarios and provides a new method for such detection.},
  archive      = {J_ASOC},
  author       = {Guoan Zeng and Feng Jiang and Meng Wang and Shoushou Zhang and Xiaotong Wei and Guoqing Wu},
  doi          = {10.1016/j.asoc.2025.113525},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113525},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Rapid detection and identification of road manhole cover safety hazards in complex scenarios},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep wavelet self-attention non-negative tensor factorization for non-linear analysis and classification of fMRI data. <em>ASOC</em>, <em>182</em>, 113522. (<a href='https://doi.org/10.1016/j.asoc.2025.113522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: This study presents Deep Wavelet Self-Attention Non-negative Tensor Factorization (Deep WSANTF), an innovative framework tailored to address the challenges of dimensionality reduction and classification in multidimensional, highly non-linear fMRI data for Autism Spectrum Disorder (ASD) and Attention-Deficit/Hyperactivity Disorder (ADHD). Methods: The proposed framework integrates wavelet self-attention mechanisms to focus on intrinsic time–frequency features, combines Non-negative Tensor Factorization (NTF) for interpretability with deep learning for non-linear data modeling, incorporates a multi-branch CNN for robust classification of neuropsychiatric disorders, and includes a formal proof of stability theory to ensure the reliability of the model under varying data distributions and perturbations. Results: Deep WSANTF achieves a classification accuracy improvement of up to 15% over state-of-the-art methods, maintains signal-to-noise ratio (SNR) under up to 4.3% noise perturbation, and provides superior feature reconstruction quality, as demonstrated in evaluations on fMRI datasets for ASD and ADHD. Conclusions: The framework effectively integrates interpretability and advanced modeling capabilities, making it an effective tool for analyzing complex fMRI data with the potential to facilitate early diagnosis of neurodevelopmental and neuropsychiatric disorders.},
  archive      = {J_ASOC},
  author       = {Fengqin Wang and Hengjin Ke and Cang Cai},
  doi          = {10.1016/j.asoc.2025.113522},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113522},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep wavelet self-attention non-negative tensor factorization for non-linear analysis and classification of fMRI data},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic network compression via evolutionary search-guided channel pruning of deep convolutional neural networks. <em>ASOC</em>, <em>182</em>, 113521. (<a href='https://doi.org/10.1016/j.asoc.2025.113521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks often exhibit parametric and computational redundancy, a common issue in various applications. Channel pruning is widely used to cut down model redundancy while maintaining the network’s core structure. However, searching for the optimal compressed network is challenging due to the vast search space and significant computational costs. In this paper, we introduce a novel channel pruning approach that utilizes surrogate-assisted neural architecture search to automatically identify an effective compressed network while adhering to predefined computational constraints. Our method introduces accuracy and efficiency coefficients into the objective function, providing fine-grained control over the trade-off between network accuracy and computational efficiency while searching for the optimal compressed network. The core of our method initiates with the training of a surrogate model employed to approximate the performance outcomes of a candidate network via generating corresponding weight assignments. These candidate networks are subjected to an evolutionary search guided by the help of the weights assigned via the surrogate model, and their interactions are regulated with the objective function. Extensive experimental evaluations showcase the adequate performance of our method in comparison to state-of-the-art techniques when applying channel pruning to ResNet-50, MobileNetV2, and VGG-16 networks. Our method, SAESA, demonstrates strong performance across various benchmarks. On the CIFAR-10 dataset with VGG-16, it achieves 94.06% accuracy using 92 million FLOPs, surpassing the performance of other baseline models. SAESA also achieves leading accuracy of 73.28% on ImageNet with MobileNetV2, while requiring fewer computational resources than recent approaches. Finally, using ResNet-50 on ImageNet, SAESA attains a competitive 76.54% accuracy with improved efficiency, operating at 1980M FLOPs. These findings demonstrate the method’s consistent performance and effective trade-off between accuracy and computational cost across different datasets and network architectures.},
  archive      = {J_ASOC},
  author       = {Abhishek Kumar and Athul Shibu and Dong-Gyu Lee},
  doi          = {10.1016/j.asoc.2025.113521},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113521},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic network compression via evolutionary search-guided channel pruning of deep convolutional neural networks},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-guided stock movements prediction with Homogeneous–Heterogeneous pattern learning. <em>ASOC</em>, <em>182</em>, 113519. (<a href='https://doi.org/10.1016/j.asoc.2025.113519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock movement prediction is a difficult task in the field of financial technology due to non-stationary dynamics and complex market interdependencies. Most of the existing research is based on deep neural networks, which lack interpretability. An interpretable prediction method helps uncover the mystery of the underlying operating mechanism of the securities market. In this work, we propose a model-guided method with interpretable homogeneous–heterogeneous processing for stock movement prediction. Specifically, based on that the correlations among the entities in the market are homogeneous within a short period, we unroll the iterative algorithm for solving the tensor robust principal component analysis (TRPCA) to separate the homogeneous and heterogeneous patterns from multiview data. Then, a specialized tensor-based attention for homogeneous and heterogeneous feature extraction is designed, and embedded in long short-term memory (LSTM) for better prediction. Experiments on real datasets show our model’s superiority over state-of-the-art stock forecast methods.},
  archive      = {J_ASOC},
  author       = {Yi Zhou and Tai-Xiang Jiang and Jun Wang and Jinghua Tan},
  doi          = {10.1016/j.asoc.2025.113519},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113519},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Model-guided stock movements prediction with Homogeneous–Heterogeneous pattern learning},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-time scale augmented neural ODEs graph neural for traffic flow prediction with elastic channel variation. <em>ASOC</em>, <em>182</em>, 113513. (<a href='https://doi.org/10.1016/j.asoc.2025.113513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is a critical task in traffic management due to the complex and dynamic spatio-temporal correlations inherent in traffic data. While existing methods often employ graph convolutional networks (GNNs) and temporal extraction modules to model spatial and temporal dependencies, respectively, deep GNNs suffer from oversmoothing, which impairs their ability to capture long-term spatial relationships. Augmented Neural Ordinary Differential Equations (ANODEs) offer a solution to this issue by enabling deeper models without oversmoothing, but they struggle with the complexity and variability of traffic data, leading to poor prediction performance. In this study, we propose the Multi-time Scale Augmented Neural ODEs Graph Neural Network (MTEC-AODE) for Traffic Flow Prediction. To address the challenges of complex information processing, we introduce the Elastic Channel Variation strategy, which adjusts the number of channels dynamically. Furthermore, we construct a traffic semantic neighborhood matrix using a Gaussian kernel similarity matrix, which captures semantic relationships across regions, aiding in the construction of a global dynamic traffic model. To handle the variability of traffic data, we develop the Multi-time Scale Augmented Neural ODEs Solver, allowing the model to adapt to different time scales and respond to dynamic changes in traffic patterns. We evaluate our model on several real-world traffic datasets, achieving Mean Absolute Errors (MAE) of 2.01, 3.52, 2.96, 3.20, 15.59, 19.75, 22.14, and 16.22, and Mean Absolute Percentage Errors (MAPE) of 4.48%, 10.17%, 7.22%, 7.66%, 15.21%, 13.98%, 9.72%, and 10.2%. Experimental results show that our method outperformed state-of-the-art benchmarks.},
  archive      = {J_ASOC},
  author       = {Zihao Chu and Wenming Ma and Mingqi Li and Hao Chen},
  doi          = {10.1016/j.asoc.2025.113513},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113513},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-time scale augmented neural ODEs graph neural for traffic flow prediction with elastic channel variation},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG-based emotion detection using long short-term memory network and reinforcement learning for enhanced feature selection. <em>ASOC</em>, <em>182</em>, 113512. (<a href='https://doi.org/10.1016/j.asoc.2025.113512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–computer interface systems can recognize users’ emotions through electroencephalography (EEG). EEG-based human emotion recognition is an emerging field that is gaining significant traction within the realm of brain–computer interfaces. However, due to the complexity and diversity inherent in EEG signals, emotion recognition remains a challenge in pattern recognition. The critical task of selecting salient features from EEG and achieving high recognition accuracy warrants further exploration. In this paper, a hybrid emotion detection system is proposed by incorporating the reinforcement learning mechanism into a deep learning framework. Reinforcement learning is used to recursively select informative features, while a Long Short-Term Memory Network (LSTM) and a deep neural network are employed for enhanced feature selection and emotion recognition. Specifically, the LSTM, based on input features, determines and generates the current state, thereby aiding the policy model in making action decisions. This process successively retains or removes features to improve emotion recognition in the next state. The neural net-based policy model generates the policy actions based on the current state and the corresponding reward signal from the classification result, to control the feature selections for the subsequent states. A public EEG emotion dataset of SEED is used in the experiments. Results show that the proposed network model is effective in feature selections and emotion classifications, which reduces feature dimensions by 11.3% on average, and achieves a higher recognition accuracy of 92.65% compared to other approaches. The proposed system can use the current state info for prediction and adaptive feature selection, which can accommodate the data pattern differences of individual participants and leverage the model for a good performance.},
  archive      = {J_ASOC},
  author       = {Junxiu Liu and Guopei Wu and Qiang Fu and Yuling Luo and Su Yang and Senhui Qiu and Yi Cao and Wei Li},
  doi          = {10.1016/j.asoc.2025.113512},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113512},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EEG-based emotion detection using long short-term memory network and reinforcement learning for enhanced feature selection},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view nonnegative matrix factorization via orthogonal and adversarial graph regularization. <em>ASOC</em>, <em>182</em>, 113508. (<a href='https://doi.org/10.1016/j.asoc.2025.113508'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative matrix factorization (NMF) obtained concern in the field of multi-view because it effectively handles nonlinear data. Traditional multi-view NMF methods are capable of dimensionality reduction, however, they still exhibit several limitations: (1) Existing multi-view NMF only comprises the first-order similarity of data distribution and do not take into account the contiguity relation of data distribution. This restricts the feasible set of the Laplacian matrix. (2) They only consider the correlation of similar samples in the original space and do not deal with samples of different categories. To overcome these challenges, this paper provides a multi-view NMF method that introduces orthogonal and adversarial graph constraints. Specifically, orthogonality is applied to the basis matrix to achieve better clustering representation. By leveraging higher-order similarities, the feasible set of the original Laplacian matrix is expanded. Additionally, adversarial graph constraints are introduced: an optimal graph is used to maintain the correlation of similar samples, while a penalty graph helps to separate dissimilar sample points. Finally, an update algorithm is proposed to get solution of the model. Comparing nine experimental methods shows that our approach has superior clustering performance.},
  archive      = {J_ASOC},
  author       = {Ning Li and Chengcai Leng and Jinye Peng and Irene Cheng and Anup Basu and Licheng Jiao},
  doi          = {10.1016/j.asoc.2025.113508},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113508},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-view nonnegative matrix factorization via orthogonal and adversarial graph regularization},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trend-aware mechanism for metaheuristic algorithms. <em>ASOC</em>, <em>182</em>, 113505. (<a href='https://doi.org/10.1016/j.asoc.2025.113505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In metaheuristic algorithms, historical search-position data often remain underutilized despite their potential to reveal valuable movement trends and promising search directions. To address this limitation, we propose the Trend-Aware Mechanism (TAM), which leverages historical position information to enhance the position updating process. TAM identifies the primary direction of movement by deriving a trend line from the population’s positions over the two most recent iterations. It evaluates candidate optimal positions by assessing the fitness of the K nearest points along this trend line. To effectively balance exploration and exploitation, TAM employs an adaptive covariance mechanism to generate high-dimensional random vectors, dynamically adjusting the update strategies. We integrate TAM with four prominent metaheuristic algorithms – PSO, SHADE, JaDE, and CMA-ES – and conduct an extensive parameter sensitivity analysis to ensure robustness. Comparative evaluations across five performance metrics demonstrate that TAM significantly improves search efficiency and consistently achieves superior results on standard benchmark functions. Moreover, TAM’s practical applicability is validated through real-world problems in engineering design, feature selection, and photovoltaic model parameter extraction. The open-source implementation of TAM will be publicly available at https://github.com/junbolian/Trend-Aware-Mechanism .},
  archive      = {J_ASOC},
  author       = {Junbo Jacob Lian and Kaichen Ouyang and Rui Zhong and Yujun Zhang and Shipeng Luo and Ling Ma and Xincan Wu and Huiling Chen},
  doi          = {10.1016/j.asoc.2025.113505},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113505},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Trend-aware mechanism for metaheuristic algorithms},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust wireless key agreement based on channel state information and fuzzy commitment. <em>ASOC</em>, <em>182</em>, 113502. (<a href='https://doi.org/10.1016/j.asoc.2025.113502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional wireless key agreement schemes using Channel State Information (CSI) face challenges such as inconsistency in channel measurements and vulnerability to interference, which lead to low key generation rates and security risks during reconciliation. To address these issues, we propose a technically novel and robust wireless key agreement protocol that integrates CSI with fuzzy commitment in a fundamentally new way. Unlike prior works that passively combine fuzzy commitment with raw CSI measurements, our method introduces a Bidirectional Differential Quantization (BDQ) mechanism tailored for channel trend encoding, which stabilizes feature extraction and enhances resilience to environmental noise without discarding large portions of data. Moreover, we decouple CSI usage from direct key derivation and instead use it to bind an independently generated random seed through fuzzy commitment, thereby eliminating key leakage risks during reconciliation and enabling consistent key derivation under imperfect reciprocity. This binding approach reflects a departure from traditional error correction-based reconciliation by embedding a privacy-preserving fuzzy matching structure into the physical layer key negotiation pipeline. Experimental results on a GNURadio/USRP testbed show that the proposed scheme achieves a false rejection rate (FRR) and a false acceptance rate (FAR) below 4.7% in various interference scenarios, while consistently passing the NIST randomness tests. These results confirm the scheme’s robustness, security, and adaptability, and demonstrate the effectiveness of our novel CSI quantization and fuzzy commitment integration strategy in real-world wireless environments.},
  archive      = {J_ASOC},
  author       = {Yubo Song and Yi Gong and Ding Li and Wenchang Liu and Yang Li and Hongyu Zhu},
  doi          = {10.1016/j.asoc.2025.113502},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113502},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust wireless key agreement based on channel state information and fuzzy commitment},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hexagonal-hogel holographic stereograms based on neural graphics primitives. <em>ASOC</em>, <em>182</em>, 113500. (<a href='https://doi.org/10.1016/j.asoc.2025.113500'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a method for generating full-parallax synthetic holographic stereograms using enhanced hexagonal hogels and neural graphics primitives is proposed and implemented. The diffraction effect and defocus aberration are introduced to analyze and compare the reconstruction performance between the proposed hexagonal pupil and the traditional square pupil, indicating that the hexagonal pupil offers better performance for floating-out holographic stereograms. Then, the extraction method for effective perspective image segments, along with the formation strategy for the synthetic effective perspective image (SEPI), is analyzed in detail. Torch-ngp is adopted to generate the SEPI from a limited number of perspective images of the 3D scene, where the direct extraction and rendering mechanism significantly improves the generation efficiency of the SEPI. Experimental results demonstrate that the proposed method can efficiently fabricate full-parallax synthetic holographic stereograms with enhanced reconstruction quality.},
  archive      = {J_ASOC},
  author       = {Xingpeng Yan and Hebin Chang and Yanan Zhang and Hairong Hu and Tao Jing and Xiaoyu Jiang and Weifeng Wang},
  doi          = {10.1016/j.asoc.2025.113500},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113500},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hexagonal-hogel holographic stereograms based on neural graphics primitives},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multispectral pansharpening method based on CNN-DI network with mixture of experts. <em>ASOC</em>, <em>182</em>, 113499. (<a href='https://doi.org/10.1016/j.asoc.2025.113499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of fusing two complementary data, panchromatic and multispectral images, to create high-resolution multispectral (HRMS) images is known as pansharpening. Combining detail injection (DI) methods with convolutional neural networks (CNN) for improved HRMS image fusion quality is a research hotspot due to their interpretability and large-scale data processing capabilities, respectively. Nevertheless, the current hybrid models typically concatenate CNN and traditional techniques, limiting the ability to utilize the benefits of both approaches. This paper presents a new hybrid network, multispectral pansharpening method based on CNN-DI network with mixture of experts (CDN-MoE), using detail injection theory to design a deep learning framework. Specifically, we first create the mixture of detail inject experts network (MoDIE-Net) that mixes training pairs of full- and reduced-resolution images to enhance model generalization. Next, the adaptive correlation residual network (ACR-Net) is suggested to find the correlation between the spectral and spatial features of the source images. Finally, the global information injection network (GII-Net) is established to strengthen the accuracy of fusion results by integrating the context of input images. Additionally, to reduce the loss of spectral features during the upsampling process, the spectral reconstruction network (SR-Net) is proposed. We perform both qualitative and quantitative experiments on the GaoFen-2, IKONOS, and WorldView-2 datasets at various resolutions. Our approach has advantages over other SOTA pansharpening methods currently available in terms of visual effects and objective metrics.},
  archive      = {J_ASOC},
  author       = {Zhongyuan Guo and Jia Lei and Shihua Zhou and Bin Wang and Nikola K. Kasabov},
  doi          = {10.1016/j.asoc.2025.113499},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113499},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multispectral pansharpening method based on CNN-DI network with mixture of experts},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symbolic regression-aided hyperparameter relationship for developing ANN for fragility prediction. <em>ASOC</em>, <em>182</em>, 113485. (<a href='https://doi.org/10.1016/j.asoc.2025.113485'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of seismic fragility parameters is crucial for assessing earthquake risks and developing effective mitigation strategies. Traditional methods, such as Incremental Dynamic Analysis (IDA), impose high computational costs, limiting their practical applicability for large-scale fragility evaluations. To address this challenge, this study proposes an optimized Artificial Neural Network (ANN) architecture for predicting fragility functions of low-rise steel moment frames. Three metaheuristic optimization algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), and Bayesian Optimization (BO), were employed to optimize the number of hidden layers, the number of neurons per layer, and the learning rate of the neural network. A comparative analysis of these methods indicated that PSO outperformed the others, yielding a lower cost function value and demonstrating more excellent stability in model tuning. Additionally, the optimal learning rate in PSO was lower than in the other two methods, suggesting a slower training process but enhanced stability of the final model. Symbolic Regression (SR) was utilized to enhance prediction accuracy and derive mathematical relationships for estimating the optimal number of neurons in hidden layers using the results of optimized network architectures. As a result, based on the proposed formula, the average prediction error was reduced by approximately 23 %, demonstrating the effectiveness of the developed approach. ANN models trained based on these relationships significantly reduced computational costs while enhancing fragility prediction accuracy. Furthermore, sensitivity analysis using the Shapley Additive explanations (SHAP) algorithm was conducted to quantify the influence of input parameters on model outputs. The results indicated that structural ductility and soil type had the most significant impact on fragility estimates, whereas seismic hazard level and importance factor exhibited the least influence. These findings highlight the effectiveness of integrating ANN, metaheuristic optimization, and sensitivity analysis in developing an efficient and computationally cost-effective fragility assessment framework. The proposed methodology enhances the accuracy and efficiency of fragility models while providing a viable alternative to traditional numerical approaches. Moreover, its applicability extends to diverse structural systems and seismic vulnerability assessments. It offers a valuable tool for earthquake engineering and risk-informed decision-making in seismic-prone regions. However, as with all data-driven models, the framework's performance depends on the quality and diversity of training data, necessitating potential hyperparameter adjustments for structures with significantly different characteristics. Addressing these limitations can provide valuable insights for future research in seismic risk analysis.},
  archive      = {J_ASOC},
  author       = {Mohammadreza Parvizi and Kiarash Nasserasadi and Ehsan Tafakori},
  doi          = {10.1016/j.asoc.2025.113485},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113485},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Symbolic regression-aided hyperparameter relationship for developing ANN for fragility prediction},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional and ℓ21-norm neural network for bone age estimation. <em>ASOC</em>, <em>182</em>, 113456. (<a href='https://doi.org/10.1016/j.asoc.2025.113456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bone age (BA) assessment is critical for evaluating children for potential endocrine, genetic and growth disorders. The evaluation of BA reading may vary among the readers. We use an Inception-v3 convolutional neural network to extract features and propose the novel ℓ 21 -norm random vector functional link neural network (LR21-RVFL) for the automatic assessment of bone age. Random vector functional link neural network (RVFL) suffers in the presence of noise and outliers due to the squared loss function. To overcome these challenges, we incorporate an ℓ 21 -norm-based loss function in the RVFL model to improve the robustness of the model. Moreover, we used ℓ 21 -based regularization to suppress the redundant/irrelevant features and hence, generate a less complex model. The proposed LR21-RVFL model achieves better performance compared to baseline models (except R21-RVFL) in bone age prediction. Moreover, we evaluate the models on the classification of UCI and KEEL datasets.},
  archive      = {J_ASOC},
  author       = {M.A. Ganaie and Jha Rohan and Krish Agrawal and Rupal Shah and Anouck Girard and Joséphine Kasa-Vubu and M. Tanveer},
  doi          = {10.1016/j.asoc.2025.113456},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113456},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Convolutional and ℓ21-norm neural network for bone age estimation},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient perturbation-aware distinguishing score for zero-shot neural architecture search. <em>ASOC</em>, <em>182</em>, 113447. (<a href='https://doi.org/10.1016/j.asoc.2025.113447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-cost proxies are under the spotlight of neural architecture search (NAS) lately, thanks to their low computational cost in predicting architecture performance in a training-free manner. The NASWOT score is one of the representative proxies that measures the architecture’s ability to distinguish inputs at the activation layers. However, obtaining such a score still requires considerable calculations on a large kernel matrix about input similarity. Moreover, the NASWOT score is relatively coarse-grained and provides a rough estimation of the architecture’s ability to distinguish general inputs. In this paper, to further reduce the computational complexity, we first propose a simplified NASWOT scoring term by relaxing its original matrix-based calculation into a vector-based one. More importantly, we develop a fine-grained perturbation-aware term to measure how well the architecture can distinguish between inputs and their perturbed counterparts. We propose a layer-wise score multiplication approach to combine these two scoring terms, deriving a new proxy, named efficient perturbation-aware distinguishing score (ePADS). Experiments on various NAS spaces and datasets show that ePADS consistently outperforms other zero-cost proxies in terms of both predictive reliability and efficiency. Particularly, ePADS achieves the highest ranking correlation among the advanced competitors (e.g., Kendall’s coefficient of 0.620 on NAS-Bench-201 with ImageNet-16-120 and 0.485 on NDS-ENAS), and ePADS-based random architecture search spends only 0.018 GPU days on DARTS-CIFAR to find networks with an average error rate of 2.64%.},
  archive      = {J_ASOC},
  author       = {Junhao Huang and Bing Xue and Yanan Sun and Mengjie Zhang and Gary G. Yen},
  doi          = {10.1016/j.asoc.2025.113447},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113447},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient perturbation-aware distinguishing score for zero-shot neural architecture search},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A surrogate-assisted evolutionary algorithm with gaussian process regression and diversity search for large-scale expensive optimization. <em>ASOC</em>, <em>182</em>, 113440. (<a href='https://doi.org/10.1016/j.asoc.2025.113440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expensive optimization problems (EOPs) need time-consuming simulations or expensive physical experiments to evaluate candidate solutions, posing a challenge for optimization. Surrogate-assisted evolutionary algorithms (SAEAs) have shown desirable performance in solving EOPs. However, most existing SAEAs are initially designed for low-dimensional EOPs and rarely consider handling large-scale EOPs. To fill this gap, this work proposes an ensemble surrogate-assisted evolutionary algorithm with a diversity search (DS-SAEA) and a lower-confidence-bound (LCB)-based Gaussian process regression (GPR) for large-scale EOPs. To handle large-scale complex optimization, the proposed DS-SAEA combines a surrogate-based global search and a surrogate-based local search taking into account both exploration and exploitation. On one hand, the proposed surrogate-based global search combines LCB-based GPR and radial basis function (RBF)-based ensemble surrogate to realize a more accurate and effective approximation to the entire landscape of EOPs. On the other hand, the local RBF surrogate-based search is used to search the local region finely. The RBF-based ensemble surrogate is composed of three different RBF surrogates with different features to make the search more diverse and robust. Further, an improved JADE is proposed with a more uniform initial population to improve the search capability. Experimental results on many well-known benchmark problems have shown the superior performance of the proposed algorithm over seven state-of-the-art SAEAs on most problems.},
  archive      = {J_ASOC},
  author       = {Xiaoliang Ma and Yueyue Li and Yongqi Zhuang and Yanhui Li and Jihua Fan and Lei Wang and Yutao Qi and Jian Xiong},
  doi          = {10.1016/j.asoc.2025.113440},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113440},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A surrogate-assisted evolutionary algorithm with gaussian process regression and diversity search for large-scale expensive optimization},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph grammars and physics informed neural networks for simulating of pollution propagation on spitzbergen. <em>ASOC</em>, <em>182</em>, 113394. (<a href='https://doi.org/10.1016/j.asoc.2025.113394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present two computational methods for performing simulations of pollution propagation described by advection-diffusion equations. The first method employs graph grammars to describe the generation process of the computational mesh used in simulations with the meshless solver of the three-dimensional finite element method. The graph transformation rules express the three-dimensional Rivara longest-edge refinement algorithm. This solver is used for an exemplary application: performing three-dimensional simulations of pollution generation by the recently closed coal-burning power plant and the new diesel power plant, the capital of Spitzbergen. The second computational code is based on the Physics Informed Neural Networks method. It is used to calculate the dissipation of the pollution along the valley in which the city of Longyearbyen is located. We discuss the instantiation and execution of the PINN method using Google Colab implementation. There are four novelties of our paper. First, we show a lower computational cost of the proposed graph grammar model in comparison with the mesh transformations over the computational mesh. Second, we discuss the benefits and limitations of the PINN implementation of the non-stationary advection-diffusion model with respect to finite element method solvers. Third, we introduce the PINN code for non-stationary thermal inversion simulations. Fourth, using our computer simulations, we estimate the influence of the pollution from power plants on the Spitzbergen inhabitants.},
  archive      = {J_ASOC},
  author       = {Maciej Sikora and Albert Oliver-Serra and Leszek Siwik and Natalia Leszczyńska and Tomasz Maciej Ciesielski and Eirik Valseth and Jacek Leszczyński and Anna Paszyńska and Maciej Paszyński},
  doi          = {10.1016/j.asoc.2025.113394},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {113394},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph grammars and physics informed neural networks for simulating of pollution propagation on spitzbergen},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated intelligent-agent optimisation of per-lane variable speed limits. <em>ASOC</em>, <em>181</em>, 113554. (<a href='https://doi.org/10.1016/j.asoc.2025.113554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in intelligent transportation systems and data analytics within transportation systems present a significant opportunity to enhance operational efficiency. In this context, the pivotal role of intelligent agents in achieving real-time optimisation for traffic management is highlighted. Such agents can predict and decide autonomously and can be trained to understand the underlying complexities of the traffic in real-time. In this paper, an innovative framework to perform real-time traffic optimal management decisions is proposed. Its rationale uses a fusion of data observations and simulation to enable an autonomous agent capable of accurate adaptive traffic management. A Case Study of application is developed using the M50 motorway in Dublin, where the speed limits are applied as adaptive parameters for optimal traffic management. Results show that the intelligent agent can autonomously predict travel times and decide in real-time the optimal speed limits to impose on a motorway when signs of congestion are found. The agent can reduce the mean travel time of a time interval by up to 55 % and the mean waiting time by up to 69 % in a situation of congestion. The average travel times of the studied M50 junction have significantly improved, showing the potential of autonomous agents in enhancing real-time optimal traffic management.},
  archive      = {J_ASOC},
  author       = {Amirreza Kandiri and Maria Nogal and Beatriz Martinez-Pastor and Rui Teixeira},
  doi          = {10.1016/j.asoc.2025.113554},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113554},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated intelligent-agent optimisation of per-lane variable speed limits},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGNSA: Adaptive graph learning-based unsupervised feature selection with non-convex sparse autoencoder. <em>ASOC</em>, <em>181</em>, 113550. (<a href='https://doi.org/10.1016/j.asoc.2025.113550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some unsupervised feature selection methodologies cannot consider the two local structures for samples and features, and there are unreasonable local structures that cannot control the feature redundancy well. So, we study an adaptive graph learning-based unsupervised feature selection with a non-convex sparse autoencoder. Firstly, a single-layer autoencoder is used to construct a reconstruction loss function to reconstruct the original features, and a new Mish activation function is studied to optimize the autoencoder structure. In the autoencoder, a feature similarity matrix is established by integrating Gaussian kernel function and Euclidean distance for reflecting the similarity of features to learn the local structure of feature graph. Particularly, a non-convex regularization term is applied into a weight matrix between the input layer and hidden layer of autoencoder, and then a feature weight matrix with sparser rows can be obtained to realize feature selection. Secondly, the Gaussian kernel function and Euclidean distance are combined to establish a sample similarity matrix. In the process of auto-encoder optimization, this sample local structure is learned by updating the sample similarity matrix adaptively, and the learned local structure is constrained near the original sample similarity matrix to avoid unreasonable local structure. Then, cosine similarity is employed to consider the feature correlation and learn redundancy matrix to control the redundancy of selected features. Finally, a new objective function is constructed, and an alternating iteration scheme is designed to optimize and compute the objective function to obtain an optimal solution for parameters, where the importance of features is judged according to the obtained feature weight matrix, and the representative feature subset is selected. Experimental results illustrate this developed methodology will be better than other comparative schemes on eight high-dimensional datasets for benchmark classification.},
  archive      = {J_ASOC},
  author       = {Lin Sun and Mengqing Li and Weiping Ding and Jiucheng Xu},
  doi          = {10.1016/j.asoc.2025.113550},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113550},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AGNSA: Adaptive graph learning-based unsupervised feature selection with non-convex sparse autoencoder},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangled reflectance-ambient feature learning for day-night vehicle re-identification. <em>ASOC</em>, <em>181</em>, 113539. (<a href='https://doi.org/10.1016/j.asoc.2025.113539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle re-identification across different time domains is a critical task in intelligent surveillance systems, aiming to match the same vehicle across multiple non-overlapping cameras under varying lighting conditions. Existing methods often struggle to handle the domain discrepancy between daytime and nighttime images, mainly due to lighting variation and glare. To address this, a novel framework named Reflectance-Ambient Feature Learning (RAFL) is proposed, which disentangles structural reflectance features from ambient lighting effects using offline reflectance decomposition. By integrating separated batch normalization and a domain alleviation module, the framework effectively minimizes the domain gap while preserving identity-discriminative features. Experimental results on benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance, with up to 4.0 % improvement in Rank-1 accuracy and over 1.5 % gain in mean Average Precision compared to existing methods. This highlights the effectiveness of feature disentanglement for robust cross-domain vehicle re-identification in real-world surveillance.},
  archive      = {J_ASOC},
  author       = {Tae-Moon Seo and Dong-Joong Kang},
  doi          = {10.1016/j.asoc.2025.113539},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113539},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Disentangled reflectance-ambient feature learning for day-night vehicle re-identification},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FFNN architecture implementation on FPGA for seizure detection using EEG signals. <em>ASOC</em>, <em>181</em>, 113533. (<a href='https://doi.org/10.1016/j.asoc.2025.113533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a chronic neurological condition characterized by recurrent seizures, which occur due to uncontrolled electrical activity among brain cells, leading to temporary changes in behavior, consciousness, and involuntary movements. Untreated epilepsy can result in irreversible brain damage and physical impairments. Recently, there has been a growing interest in using electroencephalograms (EEGs) to diagnose and treat epilepsy patients. Timely detection of epileptic activity aids in diagnosis and minimizes the impact of seizures on patients' lives. Deep learning-based seizure detection methods have shown increased efficiency. However, the challenge of hardware-based solutions remains for effective seizure detection. To address this challenge, a system is developed that integrates discrete wavelet transforms (DWT), statistical features, and a feed-forward neural network (FFNN) classifier to create a computer-aided diagnosis (CAD) method for classifying seizure signals, and implemented on a Field Programmable Gate Array (FPGA). The FFNN is trained, validated, and tested on three publicly available datasets: such as Bonn University Dataset, the Delhi Dataset, and the Multichannel CHB-MIT dataset. The process involves transforming EEG data using DWT to generate sixteen statistical features as inputs for the model. The dataset is divided into 80 % for training and development using the Tensorflow and Keras framework, with the remaining 20 % reserved for FPGA testing. The ANN model's appropriate parameters are determined using k-fold Cross-Validation during training. The most accurate FFNN model, with a structure of 16–32–16–2 neurons in its layers, is then implemented on the FPGA using the Verilog Hardware Description Language (Verilog HDL). Further, the model's effectiveness and generalizability are evaluated across three datasets using various experiments, which is crucial for appropriate intervention and treatment for different epilepsy patient.},
  archive      = {J_ASOC},
  author       = {Shalini Shanmugam and Selvathi Dharmar},
  doi          = {10.1016/j.asoc.2025.113533},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113533},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FFNN architecture implementation on FPGA for seizure detection using EEG signals},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distributed training method with intergenerational accumulation and cross-node random drop for mechanical fault diagnosis. <em>ASOC</em>, <em>181</em>, 113532. (<a href='https://doi.org/10.1016/j.asoc.2025.113532'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing complexity of deep neural networks and continuous expansion of training datasets, the computational cost of model training grows exponentially. To reduce training time, distributed training systems leveraging multiple computing devices have been developed for computational acceleration. However, compared with the rapidly increasing computing power, the communication bandwidth between devices increases slowly and becomes a bottleneck restricting the efficiency of distributed training. In this paper, an efficient distributed training method called gradient transfer compression (GTC) is proposed to reduce communication overhead and improve training efficiency. The methodology involves three key techniques: (1) Intergenerational accumulation, where gradients generated over multiple iterations are stored and accumulated, reducing the frequency of communication between computing devices; (2) Cross-node random drop, which synchronizes gradients with a specified ratio to decrease network traffic while ensuring model convergence; and (3) Mixed precision training, which reduces the bandwidth required for gradient communication. The effectiveness of GTC is demonstrated through experiments on two rolling bearing datasets. Compared with the conventional PyTorch distributed training method, the proposed method reduces the GPU memory usage by 97.10 % and 14.02 %, increases the training efficiency by 24.74 % and 8.03 % respectively in two cases, while maintaining the diagnostic performance of the model.},
  archive      = {J_ASOC},
  author       = {Zongliang Xie and Kaiyu Zhang and Jinglong Chen and Chi-Guhn Lee and Shuilong He},
  doi          = {10.1016/j.asoc.2025.113532},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113532},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A distributed training method with intergenerational accumulation and cross-node random drop for mechanical fault diagnosis},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VHTformer: A joint query perception method for visual-haptic-textual information based on transformer. <em>ASOC</em>, <em>181</em>, 113529. (<a href='https://doi.org/10.1016/j.asoc.2025.113529'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal information fusion research struggles with aligning heterogeneous modalities and addressing data imbalance, especially when integrating visual, haptic, and text—three modalities offering complementary perceptual and semantic features. Current research focuses on Transformers for unimodal and vision-haptics bimodal tasks, neglecting tri-modal integration. Leveraging text's semantic bridging capacity could address this limitation in cross-sensory learning. We propose VHTformer, a Transformer-based framework designed to unify visual, haptic, and textual modalities via joint query learning. The model leverages hierarchical attention mechanisms: self-attention refines intra-modal features (e.g., extracting texture from haptic signals or contextual semantics from text). Meanwhile, cross-attention aligns spatial-semantic patterns across modalities through learnable joint queries. This enables synergistic fusion of geometric shapes (vision), material properties (haptics), and descriptive attributes (text). Experiments were conducted on three multimodal datasets—ObjectFolder 2.0, Touch and Go, and ObjectFolder Real—covering a total of 100 + object categories with diverse material and shape properties. To mitigate class imbalance and ensure statistical reliability, we adopted stratified 5-fold cross-validation. In addition, we conducted robustness evaluations under Gaussian noise injection to verify the model's robustness. VHTformer achieves up to 99.55 % recognition accuracy and demonstrates strong robustness, highlighting the value of tri-modal integration for comprehensive object understanding.},
  archive      = {J_ASOC},
  author       = {Liang Li and Guochu Chen and Haiyan Wang and Baojiang Li and Bin Wang and Zizhen Yi and Chunbo Zhao},
  doi          = {10.1016/j.asoc.2025.113529},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113529},
  shortjournal = {Appl. Soft. Comput.},
  title        = {VHTformer: A joint query perception method for visual-haptic-textual information based on transformer},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random forest-based small training dataset complementation and features selection for capacity estimation of lithium-ion batteries in electric-powered application. <em>ASOC</em>, <em>181</em>, 113526. (<a href='https://doi.org/10.1016/j.asoc.2025.113526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, data-driven methods based on large amounts of data have gained attention for their use in estimating battery capacity. Data-driven methods usually utilize machine learning, whose performance depends on the quality and quantity of training datasets used for model learning. Owing to the irregular charging and discharging patterns in real-life battery operation, it is challenging to obtain sufficient quantity of battery capacity data, such as degradation experimental data. Therefore, it is difficult to construct datasets with sufficient quantity to train the machine learning model. The capacity estimation performance of machine learning models learned with small datasets is degraded. This paper proposes a complementary method of a small dataset using random forest (RF) to enhance the performance of machine learning for battery capacity estimation. The method complementing small dataset includes feature selection for quality improvement and estimation of unmeasured capacity data for quantity improvement. The battery capacity data estimated using RF were combined with features to form a new dataset. The new dataset was combined with a dataset with small quantity obtained from real-life operations. In addition, to improve the quality of the dataset, the permutation importance (PI) of the RF was utilized. The PI can determine the importance of a feature to the model’s performance, and the feature with the highest PI is selected to construct dataset. Dataset complemented by RF was used as the training dataset for a long short-term memory (LSTM) model. When the proposed method was applied, the root mean square error (RMSE) of the LSTM model improved by reducing the root mean square error by about 71.3 %.},
  archive      = {J_ASOC},
  author       = {Seunghwa Sin and Eunjin Kang and Jonghoon Kim and Sein Oh and Jongbok Baek},
  doi          = {10.1016/j.asoc.2025.113526},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113526},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Random forest-based small training dataset complementation and features selection for capacity estimation of lithium-ion batteries in electric-powered application},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive survey of large models-driving intelligent decision making. <em>ASOC</em>, <em>181</em>, 113524. (<a href='https://doi.org/10.1016/j.asoc.2025.113524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, intelligent decision making has become a national strategic level to attach great importance to the focus of the fields, in medicine, business, manufacturing, agriculture, etc. Exemplified by breakthroughs such as GPT-4, LLaMA, Deepseek, and multimodal systems like AlphaFold, large models have catalyzed a paradigm shift in artificial intelligence—from perceptual tasks to complex decision-making scenarios. This paper presents a comprehensive survey on large models (LMs)-driving intelligent decision making, where the main contributions are as follows: 1) innovatively give a definition of LM-driving intelligent decision making, and establish a novel multi-role functional framework distinguishing LMs as data synthesizers (preparers), contextual reasoners (facilitators), and ethical validators (reflectors). 2) cross-disciplinary bibliometric analysis reveals the current research landscape in this field, demonstrating a wide range of interests within the community. 3) conduct a profound and comprehensive analysis of key components and applications of LMs-driving intelligent decision making, and deeply analyze the advantages, limitations, and challenges associated with this approach while also suggesting future research directions. Furthermore, we recommend three priority research directions: 1) Develop reasoning-enhanced LMs overcoming parameter limits via non-parametric activation. 2) Create interpretable LMs mirroring human decision processes (intuition vs. systematic thought) through problem-solving transparency. 3) Combine fuzzy/probabilistic methods with Transformers for real-world adaptability and adaptive evaluation frameworks. This work advances both theoretical understanding through its interdisciplinary perspective and offers concrete implementation blueprints for industry practitioners navigating LM adoption in decision-critical contexts.},
  archive      = {J_ASOC},
  author       = {Yuanhang Zheng and Tong Wu and Xiangyu Xiao and Zeshui Xu},
  doi          = {10.1016/j.asoc.2025.113524},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113524},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comprehensive survey of large models-driving intelligent decision making},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight decision-making decisive feature enhancement network for medical image analysis. <em>ASOC</em>, <em>181</em>, 113518. (<a href='https://doi.org/10.1016/j.asoc.2025.113518'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is crucial in diagnosing and treating various diseases. Most existing medical segmentation methods often overlook the importance of selecting decision features, resulting in the extraction of redundant target features, which often leads to a large number of model parameters and poor deployability. Therefore, to reduce the parameter count of medical image segmentation models and improve their deployability, we propose a two-phase detection network based on enhancing decision-making decisive (DMD) features, termed the Decision-Making Decisive Feature Enhancement Network (DDFE-Net). The core idea of DDFE-net is to reduce the number of parameters required for model fitting and redundant target features by screening and enhancing the features that are important for decision-making. Specifically, in the DDFE-net, we first propose a decision network (DE-net) for initially screening and extracting DMD features through dense multi-level feature fusion and deep supervision. The DMD features of medical targets are effectively extracted through dense multi-level feature extraction and fusion. Subsequently, we introduced a DMD feature enhancement network (DEE-net) into the DDFE network to enhance the feature representation of medical targets. The DEE-net integrates DMD features of different scales and levels in the DE-net by performing secondary encoding and decoding on the extracted DMD features, thereby achieving DMD feature enhancement and further eliminating redundant features, reducing the number of model parameters, and improving the network's feature expression ability. Extensive experimental results on several medical segmentation benchmark datasets, prove that the proposed DDFE-net outperforms other state-of-the-art (SOTA) methods by 8 % in accuracy and achieves a 49 % reduction in model size, greatly improving the deployability of medical image segmentation methods.},
  archive      = {J_ASOC},
  author       = {Xiangyang Ren and Boyang Jiao and Jianbo Gao and Yazheng Chen and Na Xiao and Ying Bi and Gangqiong Liu},
  doi          = {10.1016/j.asoc.2025.113518},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113518},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lightweight decision-making decisive feature enhancement network for medical image analysis},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCC-integrals: A generalization of CC-integrals by restricted dissimilarity functions with applications on two different contexts. <em>ASOC</em>, <em>181</em>, 113517. (<a href='https://doi.org/10.1016/j.asoc.2025.113517'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the literature, it is possible to observe that the Choquet integral (CI) has been largely applied as an averaging aggregation operator in different areas, such as decision-making and classification systems. Recent advances have introduced several generalizations of the CI, for example, CC-integral (by copulae), C F 1 F 2 -integral (by two fusion functions F 1 and F 2 presenting some particular properties), and d-Choquet integral (by restricted dissimilarity functions). These generalizations showed excellent performance not only in the fields mentioned above, but also in image processing and brain–computer interface. In this paper, we introduce the concept of dCC-integral, a new generalization of the CC-integral based on restricted dissimilarity functions. We explore the theoretical properties of dCC-integrals and provide two different applications: (i) a classical application of Choquet-based aggregation operators, namely, fuzzy rule-based classification systems, and (ii) a novel kind of application on network attack detection. Our experimental study showed that our new aggregation approach outperforms existing methods, proving to be a robust tool for different tasks.},
  archive      = {J_ASOC},
  author       = {Giancarlo Lucca and Bruno L. Dalmazo and Denner G. Ayres and Tiago Asmus and Joelson Sartori and Helida Santos and Eduardo Borges and Benjamin Bedregal and Humberto Bustince and Graçaliz Pereira Dimuro},
  doi          = {10.1016/j.asoc.2025.113517},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113517},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DCC-integrals: A generalization of CC-integrals by restricted dissimilarity functions with applications on two different contexts},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining variable-length anomalies in road network time series: A two-stage optimization framework. <em>ASOC</em>, <em>181</em>, 113516. (<a href='https://doi.org/10.1016/j.asoc.2025.113516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting variable-length anomalous subsequences in network traffic is challenging due to the absence of fixed temporal patterns. Anomalies may begin at any point, last for unpredictable durations, and exhibit diverse behaviors depending on the context. Without prior knowledge of where or how long an anomaly may occur, any motif in the time series could be considered anomalous. This uncertainty increases the search complexity, as the method must explore many possible subsequences with different lengths and timings. Since labeled anomalies are often unavailable, the problem is framed as an unsupervised discovery task. It also means the methods do the search and validate anomalies without prior training. This issue makes the problem not only computationally challenging but also conceptually difficult. Existing methods often struggle because they rely on exhaustive searches that require heavy computation. Moreover, when spatial–temporal dynamics are considered, such as in road network traffic where anomalies can propagate across different locations with variable delays, the problem becomes even more complex, as the detection method must account for both when and where anomalies occur. To address these challenges, we propose a two–stage optimization framework called M P O P T . In the first stage, the matrix profile is applied to signal potential anomaly locations. In the second stage, a metaheuristic optimizer refines the starting point and length of each detected signal. During refinement, Latin hypercube sampling is used to reduce the number of comparisons between candidate signals and neighboring patterns without sacrificing generalization. We validate the proposed framework using network traffic flow data from Taiwan’s freeway system. Experimental results show that M P O P T is at least 26 times faster than benchmarking methods while achieving up to 28.5% higher search accuracy, measured based on relative anomaly scores. These results demonstrate the practical applicability and efficiency of our work for detecting complex anomalies in network time series.},
  archive      = {J_ASOC},
  author       = {Hendri Sutrisno and Frederick Kin Hing Phoa},
  doi          = {10.1016/j.asoc.2025.113516},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113516},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mining variable-length anomalies in road network time series: A two-stage optimization framework},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explainable risk classification model by integrating fuzzy multiple criteria sorting and fuzzy linguistic summarization. <em>ASOC</em>, <em>181</em>, 113515. (<a href='https://doi.org/10.1016/j.asoc.2025.113515'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel explainable risk assessment framework that integrates the fuzzy Full Consistency Method (FUCOM), fuzzy VlseKriterijumska Optimizacija I Kompromisno Resenje (VIKOR)-Sort, and fuzzy linguistic summarization to address the challenges of uncertainty, prioritization, and interpretability in risk evaluation. The model enables multi-criteria analysis under linguistic uncertainty, classifies risks into predefined categories, and generates human-readable linguistic summaries to support decision-makers. The proposed methodology is applied to a real-world construction case involving 32 risk items, which are categorized into high, medium, and low risk groups. Comparative analysis with six established multiple criteria sorting algorithms reveals strong alignment in classification outcomes. The novelty of the proposed model lies in enhancing the explainability of multiple criteria sorting algorithms by integrating fuzzy linguistic summarization into the classification process—an aspect largely overlooked in the existing literature. The results demonstrate that the proposed model not only produces consistent risk classifications across alternative methods but also enhances decision-makers’ understanding through interpretable linguistic outputs.},
  archive      = {J_ASOC},
  author       = {Esra Duygu Durmaz and Sena Aydoğan and İlker Gölcük},
  doi          = {10.1016/j.asoc.2025.113515},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113515},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An explainable risk classification model by integrating fuzzy multiple criteria sorting and fuzzy linguistic summarization},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Twin-layer attention graph convolutional network (TLA-GCN): Enhancing abnormality detection in chest x-rays. <em>ASOC</em>, <em>181</em>, 113514. (<a href='https://doi.org/10.1016/j.asoc.2025.113514'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research presents a new method called the Twin-Layer Attention Graph Convolutional Network (TLA-GCN). It aims to improve the automatic detection of abnormalities in chest X-ray images. The TLA-GCN model focuses on identifying serious conditions such as Pneumonia, Nodules, Cardimegaly, Mass, and Covid-19. The key feature of the model is the Twin-Layer Attention Mechanism (TLA), which helps the model focus on important regions within the X-ray images. The first layer of TLA identifies potential regions of interest. The second layer refines the focus by analyzing lesion-specific characteristics. This two-step attention process optimizes the model's ability to detect precise pathologies. Additionally, the model uses a Lesion-Integrating Feature Fusion strategy. This strategy combines information from different image scales. It captures both detailed and broader contextual information. By integrating these multi-scale features, the model gains a better understanding of the abnormalities. The TLA-GCN model also includes Graph Convolutional Networks (GCNs). GCNs help the model understand relationships between different regions in the X-ray image. This helps in understanding how abnormalities in one region affect others. The NIH Chest X-ray and COVID-19 dataset are used for training and testing the TLA-GCN model. The results show that the TLA-GCN model achieves an accuracy of 96.25 % in Pneumonia detection, which is a 7.75 % improvement compared to existing methods. For Nodule detection, the model achieves 95.00 % accuracy, representing a 6.25 % improvement. The model achieves a 96.00 % accuracy in detecting Cardiomegaly, an 8.00 % improvement. Mass detection accuracy is 94.50 %, which is a 5.50 % improvement, and for COVID-19, the accuracy is 96.75 %, a 7.25 % improvement. Additionally, the TLA-GCN model significantly enhances computational efficiency, reducing training time to 20 h and classification time to 0.50 s per image.},
  archive      = {J_ASOC},
  author       = {R. Rajeshwari and R. Geetha},
  doi          = {10.1016/j.asoc.2025.113514},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113514},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Twin-layer attention graph convolutional network (TLA-GCN): Enhancing abnormality detection in chest x-rays},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A grid-based MOEA guided by synchronous self-adaptive local diversity-preserving for constrained optimization of an extended-range small self-defense missile. <em>ASOC</em>, <em>181</em>, 113511. (<a href='https://doi.org/10.1016/j.asoc.2025.113511'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Miniaturized airborne antimissile interception systems can increase a fighter’s payload capacity and enhance active protection capability. However, conventional small missiles have a large aspect ratio, which reduces propulsion system reliability and aerodynamic maneuverability. This study proposes a two-stage separable extended-range missile design to solve this problem. The multidisciplinary design optimization (MDO) model of the extended-range small self-defense missile (ERSSDM) with a nonlinear design objective space and multiple constraints produces an unwonted diversity loss problem, restricting the application of heuristic multi-objective algorithms to missile MDO problems. Therefore, two concepts-grid crowding degree and relaxation factor-are introduced, and a grid-based multi-objective evolutionary algorithm (MOEA), GMOEA-SSLD, is proposed and coupled to the MDO model of this weapon system to obtain Pareto optimal designs with well-preserved diversity. This algorithm, which uses grid-based techniques and synchronous diversity-preserving approaches, eliminates the necessity for coordinate specification and improves the design efficiency in the multidisciplinary optimization of the small missile as compared to another coordinate-based MOEA. The MDO model is decoupled to some extent based on an efficient global sensitivity analysis (GSA) approach.},
  archive      = {J_ASOC},
  author       = {Hao Yan and Xiaobing Zhang},
  doi          = {10.1016/j.asoc.2025.113511},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113511},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A grid-based MOEA guided by synchronous self-adaptive local diversity-preserving for constrained optimization of an extended-range small self-defense missile},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single image and video generation using a receptive diffusion model with convolutional spatiotemporal blocks. <em>ASOC</em>, <em>181</em>, 113509. (<a href='https://doi.org/10.1016/j.asoc.2025.113509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generation of images from a single natural image/video has garnered significant attention due to its broad applications. However, existing methods training on a single input image or video face two key limitations. First, GAN-based approaches, relying on multiple models trained at progressively increasing scales, often lead to error accumulation and artifacts in generated results. Second, while diffusion models offer superior quality and diversity, they require extensive training time for a single input and are limited to generation tasks without the ability to edit existing images or videos. To address these challenges, we propose a Uni fied Diffusi on Model for Single Image/Video Training, named Union, achieving a balanced trade-off between computational efficiency and visual quality. Specifically, we introduce: (1) a unified model trained at a single scale, avoiding the error accumulation seen in multi-scale models; and (2) a novel Receptive DDPM framework with convolutional spatiotemporal blocks (CS-Block) that learns patch distribution of a natural image rather than simple image replication. The CS-Block uses ConvNext and spatiotemporal attention mechanisms to capture local and global relationships in temporal and frequency domains, enabling efficient adaptation to the patch-level receptive field of natural images and videos. Extensive experiments across image and video tasks demonstrate that Union outperforms other methods, achieving the best LPIPS score on the public Places50 dataset and excelling in high-resolution video generation, providing an optimal balance between computational cost and performance. The training and generated images/videos are available at: https://github.com/hylneu/union.git .},
  archive      = {J_ASOC},
  author       = {Yingli Hou and Wei Zhang and Zhiliang Zhu and Hai Yu},
  doi          = {10.1016/j.asoc.2025.113509},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113509},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Single image and video generation using a receptive diffusion model with convolutional spatiotemporal blocks},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing multi-focus image fusion through convolutional attention vision transformers and spatial consistency models. <em>ASOC</em>, <em>181</em>, 113507. (<a href='https://doi.org/10.1016/j.asoc.2025.113507'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Focus Image Fusion (MFIF) aims to integrate all pixels in the images to avoid defocused pixels simultaneously. The removal of defocused pixels in the image is more challenging because the traditional approach makes it difficult to accurately detect defocused regions in the image. In this paper, the Convolutional Attention based Vision Transformer based Iterative Multi-Scale Fusion Network (CAViT-IMSFN) model is proposed for MFIF. The collected input images are fed into preprocessing approaches like normalization, data imputation, and augmentation for improving the generalization ability of the model and also for reducing overfitting issues. The convolutional-based model named MobileNetV2 is used to extract local features and the AViT model is introduced to extract global features present in the preprocessed images. The spatial attention model is used in the integration stage to integrate both local and global features that preserve spatial consistency. For enhancing image quality and spatial consistency, the iterative refinement model is implemented according to the feedback mechanism that helps to update the fused output iteratively. The gradient boosting optimization algorithm is used for weight adjustment and the multi-scale fusion model is applied for the identification of focused and defocused portions in the images. This proposed model improves network adaptability by capturing multi-scale features and also has the ability to handle various levels of detail and complexity. The experimental evaluation is performed in terms of using diverse performance evaluation measures and quantitative analyses. The proposed model attained performances of 1.435 from Normalized Mutual Information (NMI) and 0.78 s from computational time. The result showed that the proposed model achieved superior outcomes rather than other MFIF-related existing approaches.},
  archive      = {J_ASOC},
  author       = {Shengchuan Jiang and Shanchuan Yu},
  doi          = {10.1016/j.asoc.2025.113507},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113507},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing multi-focus image fusion through convolutional attention vision transformers and spatial consistency models},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A single image deraining algorithm guided by text generation based on depth information conditions. <em>ASOC</em>, <em>181</em>, 113506. (<a href='https://doi.org/10.1016/j.asoc.2025.113506'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, image denoising algorithms based on text-to-image diffusion models often encounter issues with disordered internal structure layouts and discrepancies in detail when generating high-resolution images. To address these issues, we proposed a single image deraining algorithm guided by text generation based on depth information conditions. We designed a depth information encoder aimed at leveraging the depth information in rainy images to enhance the spatial mapping between text-to-image and image-to-text, thereby improving the internal structural layout of the generated images. To make the texture details of the generated image domain more similar to those of the original image domain, we designed a Cross Attention module that uses difference information to make the images in both domains more similar, thereby enhancing the guidance of existing deraining algorithms. Experimental results on multiple benchmark datasets demonstrate that the proposed algorithm outperforms state-of-the-art image deraining methods in both visual quality and quantitative performance. On average, it achieves an improvement of 0.46 in SSIM and 0.79 dB in PSNR, effectively removing rain streaks while preserving fine image details and maintaining structural consistency. We will release our code on Github .},
  archive      = {J_ASOC},
  author       = {Xing Wei and Xiufen Ye and Xinkui Mei and Junting Wang and Heming Ma},
  doi          = {10.1016/j.asoc.2025.113506},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113506},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A single image deraining algorithm guided by text generation based on depth information conditions},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated decision-making framework for evaluating industry 5.0 and circular economy in supply chain management using Z-numbers. <em>ASOC</em>, <em>181</em>, 113504. (<a href='https://doi.org/10.1016/j.asoc.2025.113504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to competitive pressures and awareness of environmental issues, companies focus on improving resilience and sustainability in the Supply Chain (SC). Industry 5.0 (I5.0) and artificial intelligence are two examples of new technologies that can improve SCs' resilience, efficiency, and transparency. Additionally, the Circular Economy (CE) supports sustainability by promoting reuse, recycling, and reduction of waste and resource consumption. This research proposes integrating I5.0 and CE in SC to achieve sustainability. A decision approach using Z-numbers is developed to evaluate the solutions. A novel integrated framework including the Simplified Best-Worst Method (SBWM), and Combined Compromise Solution (CoCoSo) approaches, is extended using Z-numbers to evaluate the implementation solutions of I5.0 and CE in SC. This approach offers flexible answers and reliable findings based on different decision-making situations. Also, comparative analysis based on different techniques and sensitivity analysis are investigated. Paying attention to appropriate investment cost, suitable investment risk, and green factors and the growth of environmentally affable procedures were the most significant sub-criteria with the significance of 0.232, 0.150, and 0.139, respectively. The results indicated that the solutions for developing digital infrastructure and suitable information systems, providing financial resources and development and attraction of investment, and creating the information system for tracking products in the circular SC are the most appropriate implementation solutions in the Z-CoCoSo method with scores of 4.703, 4.335, and 3.864, respectively. The findings of comparative analysis and examination of various scenarios also confirmed the priority of the solutions. Digital infrastructure and information systems enhance coordination and speed in SCs. Investing in advanced technologies, upgrading equipment, and attracting investors can mitigate financial risks. Additionally, using product tracking systems boosts transparency, supports sustainability, and ensures compliance with environmental regulations.},
  archive      = {J_ASOC},
  author       = {Seyyed Jalaladdin Hosseini Dehshiri},
  doi          = {10.1016/j.asoc.2025.113504},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113504},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated decision-making framework for evaluating industry 5.0 and circular economy in supply chain management using Z-numbers},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-valued inclusion–exclusion integral to aggregate interval-valued data based on multiple admissible orders. <em>ASOC</em>, <em>181</em>, 113501. (<a href='https://doi.org/10.1016/j.asoc.2025.113501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a nonlinear fuzzy aggregation function, the interval-valued Choquet integral is widely used in decision analysis, rule-based classification, and information fusion. However, its linear order between intervals is usually provided via human intervention, and different settings significantly affect calculation results. As an extended form of the Choquet integral, the Inclusion–Exclusion (IE) integral does not require sorting input variables, a property that demonstrates remarkable potential in information fusion. Nevertheless, this concept has not been extended to the interval-valued domain. Therefore, an interesting challenge arises: how to utilize the IE integral’s feature of not requiring input sorting to address the sensitivity of interval-valued Choquet integrals to order relation selection. Aiming at this sensitivity, this study proposes an interval-valued IE integral and constructs a novel aggregation model to mitigate order dependency. The research includes: (1) defining the interval-valued IE integral based on IE integral theory and analyzing its properties; (2) constructing an aggregation model by integrating interval-valued IE and Choquet integrals; (3) deriving gradient formulas for model parameters and providing the model’s computational algorithm; (4) verifying the effectiveness of the proposed method through numerical experiments and benchmark public datasets. The results provide a new methodology for addressing the order relation sensitivity of fuzzy integrals and expand the theoretical application scope of IE integrals.},
  archive      = {J_ASOC},
  author       = {Si Xu Zhu and Bo Wen Fang},
  doi          = {10.1016/j.asoc.2025.113501},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113501},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval-valued inclusion–exclusion integral to aggregate interval-valued data based on multiple admissible orders},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mask-based self-supervised network intrusion detection system. <em>ASOC</em>, <em>181</em>, 113498. (<a href='https://doi.org/10.1016/j.asoc.2025.113498'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A significant number of network intrusion detection systems utilize unsupervised anomaly detection methodologies, the majority of which fail to account for the potential for contamination in the data, resulting in suboptimal detection outcomes.This paper proposes an unsupervised method, designated MS-IDS (Mask-based Self-supervised Network Intrusion Detection System), which employs the techniques of mask shielding and Stacked Sparse Autoencoder (SSAE).MS-IDS is trained on data that has been contaminated in some way, generating a variety of masks through the process of learning. These masked inputs are subsequently reconstructed by SSAE. A composite loss function is devised, encompassing losses from both the mask unit and the SSAE. During the training phase, the combined loss function is optimized with the objective of identifying the optimal parameters and transformations for the SSAE. In the testing phase, the loss function assigns a score to each sample, which is used to classify outliers based on their scores. The performance of MS-IDS was evaluated across four intrusion datasets: The datasets used for evaluation were NSL-KDD, CIC-IDS2017, ToN-IoT, and CIC-DDOS2019. The results demonstrate that even when varying levels of contamination are introduced into the benign traffic, MS-IDS maintains robust performance with minimal decline. Notably, MS-IDS outperforms other models in terms of accuracy, AUC-ROC, and F1 scores, and its ability to detect attacks in contaminated data undergoes significant enhancement.},
  archive      = {J_ASOC},
  author       = {Xiaoya Lu and Yifan Liu and Fan Feng and Yi Liu and Zhenpeng Liu},
  doi          = {10.1016/j.asoc.2025.113498},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113498},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mask-based self-supervised network intrusion detection system},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hyperparameter-space clustering methodology of residential electricity loads. <em>ASOC</em>, <em>181</em>, 113497. (<a href='https://doi.org/10.1016/j.asoc.2025.113497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering residential electricity consumption patterns is a crucial step toward scalable and interpretable energy forecasting. Traditionally, clustering relies on direct analysis of load time series, which may obscure deeper behavioral or structural similarities between households. This study introduces a novel approach that shifts the focus from using raw consumption data to using the hyperparameters of stacked hour-ahead deep learning forecasting models—specifically based on LSTM, Bi-LSTM, and GRU architectures. By optimizing models independently for each household and clustering based on the resulting hyperparameter configurations, we reveal functional similarities in forecasting behavior that are not always evident in the original data. This method enables the formation of new, interpretable consumer segments, which can support the development of tailored forecasting models per cluster. Utilizing over three years of data from 200 households located in London, we evaluate the proposed hyperparameter-based clustering against traditional time-series clustering, applying standard metrics and explainable AI techniques (SHAP and LIME) to interpret the results. Findings demonstrate a strong alignment between the mean and median consumption patterns of clusters for both approaches, validating the effectiveness of the proposed method. Moreover, the analysis highlights that the feature transformation layers play the most critical role in shaping cluster formation, underscoring its significance in capturing underlying consumption behaviors. Beyond its analytical value, the method also offers a privacy-preserving advantage by requiring only the exchange of model hyperparameters rather than raw consumption data.},
  archive      = {J_ASOC},
  author       = {Vasilis Michalakopoulos and Ioannis Papias and Efstathios Sarantinopoulos and Elissaios Sarmas and Vangelis Marinakis and Dimitris Askounis},
  doi          = {10.1016/j.asoc.2025.113497},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113497},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hyperparameter-space clustering methodology of residential electricity loads},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auto-configured explainable graph neural networks for multi-site pollution prediction. <em>ASOC</em>, <em>181</em>, 113496. (<a href='https://doi.org/10.1016/j.asoc.2025.113496'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate particulate matter (PM) prediction is crucial for mitigating air pollution. Graph Neural Networks (GNNs) effectively model spatiotemporal dependencies, but predefined graphs limit adaptability, and some datasets complicate learning. This study introduces a graph construction method based on a confusion matrix from a supervised learning process to dynamically capture inter-class relationships. Additionally, a hybrid loss function that combines energy distance and Huber loss is applied to address the vanishing gradient problem and improve learning stability. The approach is evaluated using air pollution data from the University of Utah AirU Pollution Monitoring Network in Salt Lake City, UT, with five GNN models: Graph Convolutional Networks (GCNs), Simple Graph Convolutional Networks (SGConv), Graph Isomorphism Networks (GINs), Graph Attention Networks (GATs), and GraphSage. The experimental results of single- and multistep predictions confirm that GraphSage achieves the highest accuracy in predicting the concentrations of PM 1 , PM 10 , and PM 2.5 over different time horizons. Furthermore, GNNExplainer (Graph Neural Network Explainer) and PGExplainer (Probabilistic Graph Explainer) are applied to interpret feature importance and graph structure, ensuring model transparency. Results show improved prediction accuracy, with GNN models outperforming traditional machine learning and deep learning models (i.e., Prophet, Long short-term memory, Gated recurrent units in air pollution forecasting.},
  archive      = {J_ASOC},
  author       = {Abdelkader Dairi and Fouzi Harrou and Ying Sun},
  doi          = {10.1016/j.asoc.2025.113496},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113496},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Auto-configured explainable graph neural networks for multi-site pollution prediction},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial multi-source domain generalization approach for power prediction in unknown photovoltaic systems. <em>ASOC</em>, <em>181</em>, 113495. (<a href='https://doi.org/10.1016/j.asoc.2025.113495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of power output for previously unseen photovoltaic installations is of critical importance to the reliability and efficiency of renewable-energy management systems. Existing data-driven PV prediction techniques rely primarily on historical measurements from familiar systems, which constrains their applicability to new sites without prior observations. To address this limitation, we introduce a Generative Adversarial Domain enhanced Prediction Network (GADPN). GADPN employs an adversarial generator to synthesize diverse pseudo domain samples that mitigate distributional discrepancies between source and target domains. Through an alternating optimization regime, the framework enforces both semantic consistency and manifold regularization constraints to align synthesized and empirical feature representations, while a Transformer-based predictor captures local and global temporal dynamics. We evaluate the proposed approach on nine geographically and capacity-diverse PV systems (ranging from 2.16 kW to 45.78 kW) under a zero-sample setting. Experimental results demonstrate that GADPN achieves coefficients of determination exceeding 0.97 in eight of the nine cases and attains a peak coefficient of determination of 0.9993, outperforming state-of-the-art baselines. These findings confirm GADPN’s effectiveness for robust, zero-sample generalization in PV power forecasting.},
  archive      = {J_ASOC},
  author       = {Sizhe Liu and Yongsheng Qi and Dongze Li and Liqiang Liu and Shunli Wang and Carlos Fernandez and Xuejin Gao},
  doi          = {10.1016/j.asoc.2025.113495},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113495},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adversarial multi-source domain generalization approach for power prediction in unknown photovoltaic systems},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opinions-integrated consensus models for large-scale group decision-making in emergency medical capability evaluation. <em>ASOC</em>, <em>181</em>, 113494. (<a href='https://doi.org/10.1016/j.asoc.2025.113494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency medical capability is critical for community resilience and emergency response. However, existing evaluation methods mainly rely on expert insights while ignoring public perspectives. To bridge this gap, two opinions-integrated consensus models for large-scale group decision-making (LSGDM) are proposed. First, public opinions are analyzed using fuzzy-set Qualitative Comparative Analysis to determine criteria weights. An importance slider and programming model are introduced to quantify the relative importance of public opinions. A backtracking identification method is introduced to adjust expert insights and facilitate consensus. Based on these, a comprehensive consensus model and a professional consensus model are developed. Simulation and sensitivity analysis demonstrate the effectiveness of both models in consensus reaching. Overall, the professional consensus model performs better due to its stricter judgment mechanism. Additionally, the performance of both models is sensitive to parameter settings. Accordingly, the adaptability of both models is further discussed in terms of public participation and acceptance, evaluation timeliness, and expert heterogeneity. This study provides a systematic approach to integrating public opinions and expert insights in LSGDM, enhancing the credibility and applicability of evaluation results.},
  archive      = {J_ASOC},
  author       = {Xiaoting Cheng and Kai Zhang and Zeshui Xu and Xunjie Gou},
  doi          = {10.1016/j.asoc.2025.113494},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113494},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Opinions-integrated consensus models for large-scale group decision-making in emergency medical capability evaluation},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of signal control and speed planning at isolated mixed-traffic signalized intersections. <em>ASOC</em>, <em>181</em>, 113493. (<a href='https://doi.org/10.1016/j.asoc.2025.113493'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper develops a bi-level robust optimization framework for joint signal control and speed planning at isolated intersections in mixed traffic with connected automated vehicles (CAVs) and human-driven vehicles (HDVs). The upper-level model employs dynamic programming (DP) to find the optimal signal plan to minimize the total cost (e.g., delay and fuel consumption). With the specific signal plan, the lower level optimizes the speed profiles of connected automated vehicles with consideration of energy consumption and acceleration fluctuations using Pontryagin’s minimum principle (PMP). A stochastic car-following model is introduced to capture the stochastic behaviors of human drivers. Then, robust optimization techniques are adopted to deal with such uncertainties. Results from micro-simulation-based experiments demonstrate that compared to the benchmarking adaptive control, the proposed algorithm can reduce traffic delay and fuel consumption by 5.5% and 12.4% in typical traffic scenarios. Compared to the model without robust optimization, the proposed technique can diminish average delay and fuel consumption in the case of stochastic car-following, by 8.3% and 7.1%, respectively.},
  archive      = {J_ASOC},
  author       = {Zhanbo Sun and Jin Dai and Ang Ji and Ziye Qin and Wenbin Bu},
  doi          = {10.1016/j.asoc.2025.113493},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113493},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint optimization of signal control and speed planning at isolated mixed-traffic signalized intersections},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing predictive modeling of LPS-induced acute lung injury: A machine learning model for success rate prediction. <em>ASOC</em>, <em>181</em>, 113492. (<a href='https://doi.org/10.1016/j.asoc.2025.113492'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acute lung injury (ALI) is a life-threatening pulmonary disorder characterized by diffuse alveolar damage and an intense inflammatory response, leading to respiratory distress and hypoxemia. Early detection and intervention are imperative to prevent further deterioration of the condition. This study proposes a prediction framework for ALI prediction based on the integration of kernel extreme learning machine (KELM) with a multi-strategy enhanced particle swarm optimization (PSO) algorithm (TSPSO). TSPSO improves global search capability by incorporating advanced strategies into PSO, outperforming several PSO variants and rivaling top-tier algorithms such as LSHADE_cnEpSi on the IEEE CEC 2017 benchmark. Based on hematological data from a controlled murine ALI model, the developed bTSPSO-KELM achieves high predictive performance, with an accuracy of 91.43 %, sensitivity of 96.82 %, specificity of 82.49 %, precision of 90.40 %, Matthews correlation coefficient of 81.80 %, and an F-measure of 93.43 %. Notably, the method identifies hemoglobin, hematocrit, mean platelet volume, and lymphocyte percentage as key predictive features. In summary, this approach offers a robust and effective tool for enhancing ALI prediction accuracy and evaluating ALI in mice.},
  archive      = {J_ASOC},
  author       = {Xinsen Zhou and Chuyue Zhong and Yining Liu and Yangjing Lin and Yujie Fu and Ali Asghar Heidari and Yi Chen and Huiling Chen and Jinsheng Ouyang and Peiliang Wu},
  doi          = {10.1016/j.asoc.2025.113492},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113492},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advancing predictive modeling of LPS-induced acute lung injury: A machine learning model for success rate prediction},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A transformer-based self-supervised pre-training model for time series prediction. <em>ASOC</em>, <em>181</em>, 113491. (<a href='https://doi.org/10.1016/j.asoc.2025.113491'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series forecasting is ubiquitous in the real world. The performance of prediction model is determined by its representation ability. At present, self-supervised pre-training is the main method to improve the representation ability of prediction models. However, the periodic characteristics of time series are rarely considered in the existing pre-training models. Our experimental study shows that the periodic characteristics of time series have a great impact on the performance of self-supervised pre-training models. To address this issue, we propose a novel self-supervised prediction model, SMformer. SMformer has two distinctive features: (1) A new patch partition Module is innovatively introduced into backbone model transformer using the periodic property of time series. (2) Two pretext tasks, shuffle and mask, are design for the self-supervised pre-training of the model SMformer. We conducted extensive experiments on seven benchmark datasets, and the experimental results demonstrate that SMformer significantly outperforms prior comparison baselines.},
  archive      = {J_ASOC},
  author       = {Zhengrong Sun and Junhai Zhai and Yang Cao and Feng Zhang},
  doi          = {10.1016/j.asoc.2025.113491},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113491},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A transformer-based self-supervised pre-training model for time series prediction},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class-specific intuitionistic fuzzy kernel ridge regression classifier. <em>ASOC</em>, <em>181</em>, 113490. (<a href='https://doi.org/10.1016/j.asoc.2025.113490'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world data classification problems, class imbalance learning, noise and outliers are the major problems. The conventional kernel ridge regression (KRR) cannot efficiently deal with these challenges because all the samples are provided equal importance irrespective of their contribution to decision-making. Hence, to address this problem, we suggest a novel class-specific intuitionistic fuzzy KRR (CS-IFKRR) model for classification. CS-IFKRR provides appropriate weights to the samples for effective decision-making. CS-IFKRR classifier is designed to tackle the challenge of class imbalance in classification tasks, which generally leads to biased predictions and poor generalization for minority classes. Moreover, time efficiency is a secondary but significant advantage of CS-IFKRR, as it solves systems of linear equations. In addition to that intuitionistic fuzzy score values consider sample distance and heterogeneity to determine appropriate weights. The experimental investigation is carried out over a few popular datasets. The classification performance of the proposed CS-IFKRR model is contrasted with that of support vector machine (SVM), twin SVM, intuitionistic fuzzy SVM (IFSVM), IF twin SVM (IFTSVM), KRR and intuitionistic fuzzy KRR (IFKRR). The results, based on accuracy, F1 score and G-mean reveal the superiority of CS-IFKRR over other relevant models. Further statistical analysis is carried out based on Friedman test and posthoc Nemenyi analysis.},
  archive      = {J_ASOC},
  author       = {Barenya Bikash Hazarika and Deepak Gupta},
  doi          = {10.1016/j.asoc.2025.113490},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113490},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Class-specific intuitionistic fuzzy kernel ridge regression classifier},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing models for approximation errors in Takagi–Sugeno fuzzy control under imperfect matching. <em>ASOC</em>, <em>181</em>, 113489. (<a href='https://doi.org/10.1016/j.asoc.2025.113489'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to tackle the issues associated with stability analysis in Takagi–Sugeno (TS) fuzzy systems. A novel modeling technique is proposed to incorporate the approximation error information of membership functions (MFs) into the stability conditions. First, the classical piecewise linear approximation method is employed to decompose the MFs into a linear model and an associated error model. Then, a reconstruction strategy is introduced to transform the error model into a new fuzzy model, which is subsequently used to enhance the stability analysis of TS fuzzy systems. Compared with methods that consider only the extremal values of the error function, the proposed approach leads to a multiplicative enhancement in the amount of exploitable error information. Furthermore, stochastic disturbances are introduced to evaluate the robustness of the system. Finally, the effectiveness and practicality of the proposed method are validated through two simulation examples.},
  archive      = {J_ASOC},
  author       = {Jie Yang and Shao-Yan Gai and Fei-Peng Da},
  doi          = {10.1016/j.asoc.2025.113489},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113489},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reconstructing models for approximation errors in Takagi–Sugeno fuzzy control under imperfect matching},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-layer task planning algorithm based on UAVs-human cooperation for search and rescue. <em>ASOC</em>, <em>181</em>, 113488. (<a href='https://doi.org/10.1016/j.asoc.2025.113488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issues of low efficiency and difficult localization in search and rescue, a dual-layer task planning algorithm based on UAVs-human cooperation for search and rescue is proposed, which mainly includes the search layer for UAVs and the execution layer for rescuers. Firstly, in the search layer, to solve the problems of uneven task allocation and redundant coverage paths of heterogeneous UAVs, a coverage path optimization based on cluster algorithm (CPOC) is adopted. It applies the K-means algorithm with the proportional constraint to allocate the appropriate task-area for each UAV, and uses the non-redundant exact cellular decomposition method to achieve more efficient planning of the subregion coverage paths, meanwhile, those paths are connected by the Min-Max Ant System. Secondly, in the execution layer, the Rapidly-exploring Random Tree star with dynamic guidance mechanism (DG-RRT*) is introduced to improve the performance of path planning for rescuers in the indoor environment. By comparing the different levels of the target locations, this mechanism guides the RRT to explore purposefully to avoid the algorithm being trapped in the local optimum. Finally, compared with the classical algorithm, the total task time of CPOC in the two examples is reduced by 7.3 % and 27.8 % respectively. DG-RRT* can obtain the effective solution in a shorter time under the premise of ensuring the optimal path length. The results indicate that our algorithm can improve the efficiency of search and rescue route planning as well as the accuracy of the solutions.},
  archive      = {J_ASOC},
  author       = {Guang Yang and Yadong Mo and Chengyu Lv and Ying Zhang and Jian Li and Shimin Wei},
  doi          = {10.1016/j.asoc.2025.113488},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113488},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual-layer task planning algorithm based on UAVs-human cooperation for search and rescue},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistency-informed progressive learning and segment anything model for edge detection. <em>ASOC</em>, <em>181</em>, 113487. (<a href='https://doi.org/10.1016/j.asoc.2025.113487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection, a fundamental vision task, has gained increasing attention. Despite significant advances driven by deep learning, existing edge detectors still face two major challenges: Incomplete edge detection in complex scenes and ambiguous labels caused by annotator inconsistencies. To address these two issues, this paper presents a SAM-based edge detection framework that integrates label optimization and incrementally progressive learning ( i.e. , CPL_SAMED), enabling the model to effectively leverage ambiguous labels. Specifically, label optimization classifies pixels into easy and hard samples based on consistency analysis of historical predictions, refining training targets by extracting more reliable labels. Meanwhile, incrementally progressive learning dynamically shifts the model’s focus from easy to hard samples, and progressively mines hidden edge knowledge within hard labels. In this paper, the vision foundation model ( i.e. , SAM) is selected as our backbone for its powerful boundary localization capability; and simultaneously, lightweight CNN and adapter branches are introduced to reduce GPU memory requirements while enhancing local features for finer edge details. Extensive experiments on three benchmark datasets demonstrate that our method achieves competitive performance on BSDS500 (ODS 0.845), NYUDv2 (ODS 0.785), and Multicue (ODS 0.899). Code: https://github.com/wenya1994/CPL_SAMED .},
  archive      = {J_ASOC},
  author       = {Wenya Yang and Yiming Chen and Hongshuai Qin and Xiao-Diao Chen and Xiuting Tao},
  doi          = {10.1016/j.asoc.2025.113487},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113487},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Consistency-informed progressive learning and segment anything model for edge detection},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive learning-guided hashing model for artwork image retrieval. <em>ASOC</em>, <em>181</em>, 113486. (<a href='https://doi.org/10.1016/j.asoc.2025.113486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing demand for spiritual enrichment, artwork retrieval has gained increasing popularity in computer vision. While current quick response (QR) code-based and deep learning-based methods enable real-time artwork recognition, their dependence on manual annotations limits adaptability and scalability. In this paper, we propose an unsupervised hashing-based artwork retrieval framework that requires no manual labeling and adapts to new classes. The system comprises two key components: a detection module and a retrieval module. The detection module processes user-captured artwork images by predicting polygonal bounding boxes and rectifying them through perspective transformation algorithms. The retrieval module is the focus of our research. We present a novel contrastive learning framework for artwork retrieval that integrates a convolutional neural network (CNN) based feature extractor and a hashing encoder–decoder structure. This architecture processes input images into both floating-point and hashing representations while maintaining a binary memory bank for efficient similarity matching. Two specialized loss functions facilitate adaptive hashing encoding, aligning floating-point and hashing features through an unsupervised learning process. We evaluate our framework on a self-built dataset containing over 24k images spanning traditional Chinese paintings, oil paintings, and Chinese chops. The samples are unlabeled and there is only one image of each artwork. Comparative experiments with state-of-the-art methods demonstrate our system’s superior effectiveness and strong potential for practical artwork retrieval applications.},
  archive      = {J_ASOC},
  author       = {Zhenyu Wang and Yingdong Yang and Fucheng Wu and Wenjia Li},
  doi          = {10.1016/j.asoc.2025.113486},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113486},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive learning-guided hashing model for artwork image retrieval},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Post-earthquake infectious disease risk assessment approach using AHP and MULTIMOORA with decomposed fuzzy sets. <em>ASOC</em>, <em>181</em>, 113484. (<a href='https://doi.org/10.1016/j.asoc.2025.113484'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel approach to post-earthquake infectious disease risk assessment, a critical component of emergency planning for public health and safety. Focusing on the districts of the European side of Istanbul, the risk assessment is based on the opinions of three experts. The novel two-level methodology, which integrates Decomposed Fuzzy Set Analytic Hierarchy Process (DFS-AHP) with Decomposed Fuzzy Multi-objective Optimization by Ratio Analysis Plus the Full Multiplicative Form (DFS-MULTIMOORA), aims to improve decision-making under linguistic uncertainty and ambiguity. By integrating DFS-AHP with DFS-MOORA, the problems of precision and consistency in expert judgments are addressed. Criteria importance weights determined using DFS-AHP enable successful district ranking with DFS-MOORA. The results reveal that "Adequacy of housing conditions" is the most important sub-criteria, while Bağcılar district is identified as the highest risk due to its crowded living spaces and poor health conditions after the earthquake. Beşiktaş exhibits the lowest risk. Among the contributions are the introduction of the DFS-AHP integrated DFS-MOORA methodology, the development of criteria and importance weights for risk assessment, and the pioneering of the implementation Multiple Criteria Decision Making (MCDM) approaches for post-earthquake infectious disease risk assessment. This study provides decision makers with a quick and effective procedure for assessing post-earthquake infectious disease risk.},
  archive      = {J_ASOC},
  author       = {Kübra Yazici Sahin and Alev Taskin},
  doi          = {10.1016/j.asoc.2025.113484},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113484},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Post-earthquake infectious disease risk assessment approach using AHP and MULTIMOORA with decomposed fuzzy sets},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An instance-oriented multi-source information fusion technique based on neighborhood granules. <em>ASOC</em>, <em>181</em>, 113483. (<a href='https://doi.org/10.1016/j.asoc.2025.113483'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of society and technology, human beings have increasingly diverse sources of data collection. Consequently, multi-source information fusion techniques, aiming to utilize various technologies to process, integrate and analyze data from different sources to obtain valuable information, have attracted significant attention. As a structured and hierarchical manner by processing and analyzing data via “granules”, granular computing has been extensively applied to multi-source information fusion. Given that different information sources may contain redundant instances and noise at different levels, it is crucial to select representative instances from multiple information sources based on granular computing. However, there exists little research on instance-oriented fusion based on granular computing. To fill this gap, we investigate the issue of instance-oriented fusion in multi-source neighborhood decision information systems in this paper. Specifically, considering both the distribution and decision information of the neighborhood of an instance, we firstly propose the concept of internal confidence to reflect the reliable degree of an instance in an information source. Secondly, the external confidence is presented to measure the reliable degrees of information sources by employing the overlap degree of the neighborhood granules in multiple information sources. Then, by combining the internal confidence and the external confidence, we put forward a confidence index for instances within an information source to select representative instances from multiple information sources. Furthermore, we present an instance-oriented multi-source information fusion algorithm based on neighborhood granules (IoMsIF). Finally, the performance of IoMsIF is assessed by numerical experiments. The experimental results show that IoMsIF achieves satisfactory performance.},
  archive      = {J_ASOC},
  author       = {Xiao Zhang and Jingjing Shen and Jinhai Li and Xia Liu},
  doi          = {10.1016/j.asoc.2025.113483},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113483},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An instance-oriented multi-source information fusion technique based on neighborhood granules},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-optimized fuzzy comprehensive evaluation for real-world webrobot detection. <em>ASOC</em>, <em>181</em>, 113481. (<a href='https://doi.org/10.1016/j.asoc.2025.113481'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of malicious WebRobots poses a serious threat to network security. However, detecting WebRobots in real-world scenarios remains challenging due to the impact of low-quality training samples, which can significantly reduce the accuracy of machine learning and other data-driven methods. These low-quality samples often arise from labeling difficulties and high annotation costs, leading to mislabeling and degraded model performance. To address this issue, we propose a Self-Optimizing Fuzzy Comprehensive Evaluation (SO-FCE) method, which builds upon the traditional Fuzzy Comprehensive Evaluation (FCE) framework. The core innovation lies in the integration of an iterative learning strategy that dynamically adjusts and optimizes the evaluation parameters and processes, thus mitigating the adverse effects of erroneous samples on the accuracy of the decision. This study presents a case study conducted in a real campus network, demonstrating the effectiveness of SO-FCE in handling mislabeled data. Experimental results in WebRobot detection demonstrate that SO-FCE maintains high detection performance even as sample error rates increase, unlike traditional FCE and conventional machine learning approaches, which suffer substantial performance degradation. The application of SO-FCE in real-world scenarios has shown promising results, achieving accuracy rates of 90%–99% and effectively identifying previously undisclosed robots. This highlights its significant robustness and practical value.},
  archive      = {J_ASOC},
  author       = {Zhishuo Sheng and Zeshui Xu and Hong Rao and Guolin Shao},
  doi          = {10.1016/j.asoc.2025.113481},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113481},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-optimized fuzzy comprehensive evaluation for real-world webrobot detection},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A population-based simulated annealing approach with adaptive mutation operator for solving the discounted {0-1} knapsack problem. <em>ASOC</em>, <em>181</em>, 113480. (<a href='https://doi.org/10.1016/j.asoc.2025.113480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discounted {0-1} knapsack problem extends the traditional knapsack problem by incorporating unique discount relationships among item groups, adding complexity to the selection process. It finds applications in supply chain optimization, resource allocation, and financial portfolio management. The objective is to maximize profit while adhering to capacity constraints. This paper presents a novel approach that integrates population-based simulated annealing with adaptive differential evolution to efficiently solve the problem. The proposed algorithm introduces an advanced greedy randomized initialization, multi-neighborhood local optimization within simulated annealing framework, and use two differential evolution mutation operators (DE/current-to-rand/1/bin and DE/current-to-best/1/bin) to enhance exploration and exploitation. A comprehensive two-stage repair and re-optimization strategy is employed to handle infeasible solutions. Extensive testing on two groups of 80 benchmark instances highlights the algorithm’s robustness and performance, effectively tackling the complexities of the studied problem.},
  archive      = {J_ASOC},
  author       = {Juntao Zhao and Xiaochuan Luo},
  doi          = {10.1016/j.asoc.2025.113480},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113480},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A population-based simulated annealing approach with adaptive mutation operator for solving the discounted {0-1} knapsack problem},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mutual supervision multi-source-free domain adaptation of image classification. <em>ASOC</em>, <em>181</em>, 113479. (<a href='https://doi.org/10.1016/j.asoc.2025.113479'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of source-free domain adaptation (SFDA) is to adapt the source model to the target domain without accessing the source data. However, most current SFDA methods do not take into account the situations where multi-source models are available. This work focuses on extract complementary knowledge from multi-source models to enhance the models’ classification ability on target domain data. We propose a novel multi-source-free domain adaptation (MSFDA) method named as aliGnment of mUltiple Model predictions (GUM). Specifically, a reliability weight is defined for each source model. The more reliable a model is, the more significant its influence during training. Then a mutual information maximization loss is proposed that allows the training of each model to be supervised by the outputs of other models. During training, to reduce the pseudo-label noise from only one model, the pseudo-label of a sample can be replaced with a higher confidence pseudo-label from other models. Experiments on three benchmark datasets show that our method achieves the state-of-the-art results.},
  archive      = {J_ASOC},
  author       = {Liang Tian and Lihua Zhou and Shuaifeng Li and Yan Gan and Mao Ye},
  doi          = {10.1016/j.asoc.2025.113479},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113479},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mutual supervision multi-source-free domain adaptation of image classification},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple attention relation network for few-shot learning. <em>ASOC</em>, <em>181</em>, 113477. (<a href='https://doi.org/10.1016/j.asoc.2025.113477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a large amount of practical application scenarios, only small number of labeled samples could be collected for classification model training, which makes the big dataset based deep networks inapplicable. Few-shot learning methods have been developed to solve the small sample problem with only a few samples for each category. Among which, relation network has been an efficient method based on convolutional modules to compute the sample relations. Due to the local receptive field of convolutional operation, it could not obtain features on a global level and ignores the contextual structures within the samples. The discriminative salient features are not emphasized either. To address these issues, a multiple attention relation network (MARN) is proposed which combined spatial attention, channel attention, self attention and cross attention. Spatial attention and channel attention can obtain global description in space and across channels. These two attention module selectively aggregates the features from separate parts by weighted sum. Self attention and cross attention can capture the self and cross relations between deep features, which could enhance the salient features and increase the model comparison ability. Besides these attention mechanisms, atrous spatial pyramid pooling (ASPP) is employed to extract features from different scale receptive fields. Accordingly, MARN can efficiently combine local and global spatial features, intra and inter channel features, and different scale features, which can improve the model discrimination capability and classification performance. Meanwhile, the combination of these mechanisms only introduce a very small number of parameters. Experiments on few shot learning benchmarks mini-ImageNet and CUB-200 have verified the superiority of MARN over the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Na Lu and Zhiyan Cui and Huiyang Hu and Weifeng Wang},
  doi          = {10.1016/j.asoc.2025.113477},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113477},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiple attention relation network for few-shot learning},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-strategy iterated greedy algorithm with three-phase for hybrid flow shop scheduling in distributed factory. <em>ASOC</em>, <em>181</em>, 113475. (<a href='https://doi.org/10.1016/j.asoc.2025.113475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid globalization of manufacturing, industry is increasingly shifting from centralized to distributed production systems. The question of how to achieve distributed hybrid flow shop scheduling has become a key challenge in decentralized production. Most of algorithms for the problem using single scheduling strategy overlook job blocking constraints, which can lead to longer makespan. In this paper, a multi-strategy iterated greedy (MSIG) algorithm with three-phase optimization is proposed to address the problem with blocking constraints. In the first phase, in order to assign jobs to distributed factories, a multi-strategy greedy initialization method is designed to select optimal initial assignments. In the second phase, a cross-factory hybrid neighborhood search method is proposed to enlarge the search scope and find more candidate solutions in heterogeneous shops. In the third phase, a local intensification strategy for optimal solution construction is built to further reduce the job blocking time and makespan. Experimental results on scheduling instances with varying sizes demonstrate that MSIG achieves a makespan reduction 5.95% to 12.34%, and an ARPI reduction ranging from 0.85 to 0.91, compared to other state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Dandan Liu and Zhiyuan Zou and Xu Liang},
  doi          = {10.1016/j.asoc.2025.113475},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113475},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-strategy iterated greedy algorithm with three-phase for hybrid flow shop scheduling in distributed factory},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent cooperative multi-network group framework for energy-efficient distributed fuzzy flexible job shop scheduling problem. <em>ASOC</em>, <em>181</em>, 113474. (<a href='https://doi.org/10.1016/j.asoc.2025.113474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing integration of industrial intelligence and the Industrial Internet of Things (IIoT) has promoted distributed flexible manufacturing (DFM) as a fundamental component of smart manufacturing systems. However, the rising complexity in dynamic demands, production uncertainties, and the urgent need for energy efficiency pose significant challenges. To address these challenges, this study investigates the energy-efficient distributed fuzzy flexible job shop scheduling problem (EE-DFFJSP), which aims to minimize both makespan and total energy consumption (TEC) in DFM environments. To tackle fuzzy uncertainties and complex coupling characteristics inherent in EE-DFFJSP, a multi-agent cooperative multi-network group (MACMNG) framework is proposed. First, a mixed-integer linear programming (MILP) model for EE-DFFJSP is formulated, followed by an analysis of the problem’s properties. A triple Markov decision process formulation adapted to the problem's characteristics is designed, enabling problem decoupling and multi-agent decision-making through specific state representations and reward functions. Next, an innovative multi-network group framework is devised, and coupled decisions are effectively handled via interaction and collaboration among independent subnets. Based on problem decomposition method, EE-DFFJSP is decomposed into a set of subproblems represented by subnets within the network group. These subnets cooperate by sharing experience and knowledge through a domain parameter transfer strategy (DPTS) to enable efficient training. Finally, MACMNG employs a multi-objective DQN (MO-DQN) integrated with a dynamic weighting mechanism, enabling subnets to effectively balance between makespan and TEC during cooperative decision-making and network parameter updating. Experimental results show that MACMNG achieves superior performance compared with three priority dispatch rules (PDRs) across various scenarios. The MACMNG outperforms seven state-of-the-art multi-objective algorithms in terms of different metrics across 69 benchmark instances. This study contributes an efficient learning-driven and multi-agent collaborative promising paradigm for the energy-efficient scheduling in DFM, providing practical insights for advancing smart manufacturing in IIoT architectures.},
  archive      = {J_ASOC},
  author       = {Zi-Qi Zhang and Xiao-Wei Li and Bin Qian and Huai-Ping Jin and Rong Hu and Jian-Bo Yang},
  doi          = {10.1016/j.asoc.2025.113474},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113474},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-agent cooperative multi-network group framework for energy-efficient distributed fuzzy flexible job shop scheduling problem},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on optimizing binary spiking neural networks for neuromorphic computing. <em>ASOC</em>, <em>181</em>, 113471. (<a href='https://doi.org/10.1016/j.asoc.2025.113471'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks process information using asynchronous spikes between neurons, making them ideal for handling spatiotemporal data from neuromorphic sensors and have shown superior performance in low-latency and low-power computing applications. However, current neuromorphic computing faces challenges such as high synaptic memory usage and complex synapse calculations, with the training process optimization lacking a solid foundation. Here, we present a hardware-friendly weight-binarized spiking neural network to reduce storage needs, accelerate optimization, and enhance computational efficiency. During training, weight binarization is applied to reduce memory size and access drastically. Meanwhile, we employ a hybrid optimizer that combines the Adam method with stochastic gradient descent to address the convergence challenges that arise from gradient sparsity due to the use of binary weights. During inference, a simple shift-based batch normalization algorithm is introduced to achieve the effect equivalent to the computationally expensive BN with low accuracy loss. Then, we empirically identify and study the effectiveness of various ad-hoc techniques on neuromorphic recognition tasks as a case study, providing best practices for optimization. To the best of our knowledge, this is the first work using systematic comparisons to reveal how commonly employed tricks are effective for training binary spiking neural networks. The implementations will be open-sourced on GitHub.},
  archive      = {J_ASOC},
  author       = {Ping He and Rong Xiao and Chenwei Tang and Shudong Huang and Jiancheng Lv},
  doi          = {10.1016/j.asoc.2025.113471},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113471},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An empirical study on optimizing binary spiking neural networks for neuromorphic computing},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on diffusion models and their applications. <em>ASOC</em>, <em>181</em>, 113470. (<a href='https://doi.org/10.1016/j.asoc.2025.113470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion Models (DMs) are probabilistic models that create realistic samples by simulating the diffusion process, gradually adding and removing noise from data. These models have gained popularity in domains such as image processing, speech synthesis, and natural language processing due to their ability to produce high-quality samples. As DMs are being adopted in various domains, existing literature reviews that often focus on specific areas like computer vision or medical imaging may not serve a broader audience across multiple fields. Therefore, this review presents a comprehensive overview of DMs, covering their theoretical foundations and algorithmic innovations. We highlight their applications in diverse areas such as media quality, authenticity, synthesis, image transformation, healthcare, and more. Unlike prior surveys that are often domain-specific, this review integrates developments across multiple fields and proposes a unified taxonomy of diffusion models, categorizing them by architecture, conditioning strategy, and application. This cross-domain synthesis not only reveals underexplored areas but also identifies emerging interdisciplinary opportunities, offering actionable insights for future research. By consolidating current knowledge and identifying emerging trends, this review aims to facilitate a deeper understanding and broader adoption of DMs and provide guidelines for future researchers and practitioners across diverse disciplines.},
  archive      = {J_ASOC},
  author       = {Md Manjurul Ahsan and Shivakumar Raman and Yingtao Liu and Zahed Siddique},
  doi          = {10.1016/j.asoc.2025.113470},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113470},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive survey on diffusion models and their applications},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMOTE-ENN resampling technique with bayesian optimization for multi-class classification of dry bean varieties. <em>ASOC</em>, <em>181</em>, 113467. (<a href='https://doi.org/10.1016/j.asoc.2025.113467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The imbalanced classification problem poses a significant challenge in machine learning, often resulting in biased models and poor performance for minority classes. This study introduces an innovative hybrid resampling technique combining Synthetic Minority Oversampling Technique and Edited Nearest Neighbours (SMOTE-ENN), optimized using Bayesian Optimization, to address these limitations. The proposed framework integrates advanced feature pre-processing, hybrid resampling, and machine learning models to enhance classification performance. Using the publicly available dry bean dataset containing 16 geometric features of seven seed varieties, the methodology demonstrates remarkable improvements in predictive accuracy and class balance. Employing cutting-edge classifiers, the improved Light Gradient Boosting Machine (LBM) with Bayesian optimization achieved an unprecedented accuracy of 99.59 %, outperforming traditional approaches. Results reveal the potential of hybrid resampling techniques and Bayesian optimization in effectively capturing feature patterns, enhancing model diversity, and ensuring robust classification of imbalanced datasets. This research underscores the application of soft computing methods to real-world multi-class classification challenges, offering practical insights for similar domains.},
  archive      = {J_ASOC},
  author       = {Arnab Mukherjee and Mohammad Reza Chalak Qazani and B.M. Jewel Rana and Shahina Akter and Amirhossein Mohajerzadeh and Nusrat Jahan Sathi and Lasker Ershad Ali and Md. Salauddin Khan and Houshyar Asadi},
  doi          = {10.1016/j.asoc.2025.113467},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113467},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SMOTE-ENN resampling technique with bayesian optimization for multi-class classification of dry bean varieties},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end deep learning framework for the portfolio optimization with stop-loss orders. <em>ASOC</em>, <em>181</em>, 113465. (<a href='https://doi.org/10.1016/j.asoc.2025.113465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating stop-loss orders into portfolio optimization is an effective strategy for mitigating tail risk. However, traditional methods often rely on static stop-loss rules paired with separate price prediction models, leading to amplified error propagation in the application of stop-loss orders. We propose a novel end-to-end (E2E) data-driven deep learning framework that simultaneously determines portfolio allocations and stop-loss orders for risky assets, with each asset triggering sales when its prices reach the stop-loss order. The E2E approach uses a two-branch neural network architecture that integrates historical transaction data with additional inputs such as predicted returns and price volatility. A recurrent neural network extracts feature representation vectors for each risky asset from its time series data, and two fully connected layers process the aggregated information to generate the optimal joint action of portfolio allocations and corresponding stop-loss orders. The effectiveness of the E2E solution is demonstrated by numerical experiments on four stock market datasets. The results show that the E2E strategy outperforms alternative benchmarks across multiple metrics, including cumulative returns, alpha returns, and risk-adjusted returns. The strategy exhibits superior ability to maintain robust returns and control tail risk, achieving an average excess return of 0.33% and an average Calmar ratio of 4.50 over four datasets. In addition, the introduction of stop-loss orders yields significantly positive excess returns, and the strategy maintains its advantage even when a transaction cost rate of 0.3% is included.},
  archive      = {J_ASOC},
  author       = {Yong Zhang and Yue Liu and Weilong Liu and Xingyu Yang},
  doi          = {10.1016/j.asoc.2025.113465},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113465},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An end-to-end deep learning framework for the portfolio optimization with stop-loss orders},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dueling framework for multi-agent reinforcement learning. <em>ASOC</em>, <em>181</em>, 113464. (<a href='https://doi.org/10.1016/j.asoc.2025.113464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world tasks, multiple agents often need to coordinate with one another due to their individual private observations and restricted communication abilities. A representative research direction is the deep multi-agent reinforcement learning value decomposition, which decomposes the global shared joint action value function Q t o t ( τ , u ) into their respective action value functions Q i ( τ i , u i ) to guide the behavior of individuals. They all follow the IGM (Individual-Global-Max) principle, obeying the addable assumption and the monotonic assumption to support effective local decision making. However, to achieve scalability, existing MARL algorithms often compromise either the expressive power of their value function representations or the consistency of the IGM principles. This compromise can potentially result in instability or poor performance when dealing with complex tasks. In this paper, we introduce a novel algorithm called MDF—a Multi-Dueling Framework for Multi-Agent Reinforcement Learning. We innovatively propose the V-IGM constraint principle and correct the incomplete expression of the constant term c ( τ ) of the Qatten algorithm to further refine the decomposition of the joint action value function Q t o t ( τ , u ) . The MDF algorithm innovatively utilizes the Dueling Network architecture for decomposing the joint action value function Q t o t ( τ , u ) . Additionally, it incorporates the multi-attention mechanism to achieve an even more refined decomposition of Q t o t ( τ , u ) . Experiments show that MDF algorithm outperforms the most advanced MARL algorithm in StarCraft Π maps (e.g. 3 m, 8 m, 2m-vs-1z, 2m-vs-1sc, 2s3z, 3s-vs-4z, 3s-vs-5z, 3s5z, 1c3s5z, bane-vs-bane).},
  archive      = {J_ASOC},
  author       = {Baochang Ren and Mingjie Cai and Bin Yu},
  doi          = {10.1016/j.asoc.2025.113464},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113464},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-dueling framework for multi-agent reinforcement learning},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiagent reinforcement learning with evolution for multitarget tracking by unmanned aerial vehicle swarm. <em>ASOC</em>, <em>181</em>, 113463. (<a href='https://doi.org/10.1016/j.asoc.2025.113463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitarget tracking has immense potential in both military and civilian applications. For an unmanned aerial vehicle (UAV) swarm, a critical challenge in multitarget tracking is how to coordinate multiple unmanned aerial vehicles to continuously and accurately track multiple targets. This paper considers the cooperative decision-making problem for multitarget tracking by multiple unmanned aerial vehicles with limited sensing range in a dynamic environment. Meanwhile, a multiagent advantage actor critic with evolution, named MAA2CE, is proposed to learn the cooperative tracking policies for a UAV swarm. During training, each UAV is viewed as an agent with a policy network, making decisions based on its own information and policy. The prioritized experience replay is adopted to take full advantage of the valuable experience for learning. Considering the tracking performance of agents, the high performance agent can replicate its own network parameters to the other agents with a certain probability by network evolution. The experimental results demonstrate that the proposed algorithm is superior to three peer algorithms in learning efficiency, can acquire better collaborative tracking policies, and significantly improves the collaborative multitarget tracking proficiency of the UAV swarm.},
  archive      = {J_ASOC},
  author       = {Keming Jiao and Jie Chen and Bin Xin and Li Li and Yulong Ding and Zhixin Zhao and Yifan Zheng},
  doi          = {10.1016/j.asoc.2025.113463},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113463},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiagent reinforcement learning with evolution for multitarget tracking by unmanned aerial vehicle swarm},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An ensemble local search framework for population-based metaheuristic algorithms on single-objective optimization. <em>ASOC</em>, <em>181</em>, 113462. (<a href='https://doi.org/10.1016/j.asoc.2025.113462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population-based metaheuristic algorithms are recognized as potent tools for tackling complex optimization problems. However, they are often plagued by premature convergence, making them susceptible to getting trapped in local optima. To mitigate this issue, this paper introduces an Ensemble Local Search (ENLS) framework that can seamlessly integrate with various metaheuristic algorithms. In ENLS, an online detection mechanism is designed to identify the occurrence of premature convergence during the search process. Subsequently, three local search strategies are triggered to assist the following three subpopulations based on the characteristic of each one: (1) the orthogonal learning mechanism is applied to design an orthogonal crossover-based local search strategy for the superior subpopulation, focusing on refining solutions within a narrow region; (2) a dynamic Lévy flight-based local search strategy is developed for the medium subpopulation to enhance the population diversity by leveraging the long-term short-step Lévy random walking; (3) the inferior subpopulation employs an opposition-based local search, incorporating a modified opposition-based learning mechanism to explore a broader space between inferior solutions and their opposite positions. By integrating these three local search strategies, the ENLS framework can effectively balance exploration and exploitation, addressing the challenge of premature convergence. To validate the effectiveness of ENLS, comparative experiments are conducted using 30 benchmark problems from the IEEE CEC 2014 test suite and 20 real-world optimization problems. The experimental results confirm that the ENLS framework significantly enhances the optimization capabilities of the considered 12 metaheuristic algorithms without significantly increasing runtime complexity.},
  archive      = {J_ASOC},
  author       = {Chunlei Li and Libao Deng and Wenyin Gong and Liyan Qiao},
  doi          = {10.1016/j.asoc.2025.113462},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113462},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble local search framework for population-based metaheuristic algorithms on single-objective optimization},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skeleton-based robust registration framework for corrupted 3D point clouds. <em>ASOC</em>, <em>181</em>, 113461. (<a href='https://doi.org/10.1016/j.asoc.2025.113461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is fundamental in 3D vision applications, including autonomous driving, robotics, and medical imaging, where precise alignment of multiple point clouds is essential for accurate environment reconstruction. However, real-world point clouds are often affected by sensor limitations, environmental noise, and preprocessing errors, making registration challenging due to density distortions, noise contamination, and geometric deformations. Existing registration methods rely on direct point matching or surface feature extraction, which are highly susceptible to these corruptions and lead to reduced alignment accuracy. To address these challenges, a Skeleton-based Robust Registration Framework (SRRF) is presented, which introduces a corruption-resilient skeletal representation to improve registration robustness and accuracy. The framework integrates skeletal structures into the registration process and combines the transformations obtained from both the corrupted point cloud alignment and its skeleton alignment to achieve optimal registration. In addition, a distribution distance loss function is designed to enforce the consistency between the source and target skeletons, which significantly improves the registration performance. This framework ensures that the alignment considers both the original local geometric features and the global stability of the skeleton structure, resulting in robust and accurate registration results. Experimental evaluations on diverse corrupted datasets demonstrate that SRRF consistently outperforms state-of-the-art registration methods across various corruption scenarios, including density distortions, noise contamination, and geometric deformations. The results confirm the robustness of SRRF in handling corrupted point clouds, making it a potential approach for 3D perception tasks in real-world scenarios.},
  archive      = {J_ASOC},
  author       = {Yongqiang Wang and Weigang Li and Wenping Liu and Zhiqiang Tian and Jinling Li},
  doi          = {10.1016/j.asoc.2025.113461},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113461},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Skeleton-based robust registration framework for corrupted 3D point clouds},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural health monitoring of building structures using novel acceleration-based signal-to-image technique and 2D convolutional neural networks. <em>ASOC</em>, <em>181</em>, 113457. (<a href='https://doi.org/10.1016/j.asoc.2025.113457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural Health Monitoring (SHM) is essential for detecting damage in structures like buildings, bridges, aircraft, etc., as they can deteriorate over time or suffer sudden failure during extreme events. In this realm, the application of deep learning (DL) techniques has been observed to become quite an integral part of global-level SHM using acceleration-based measurements by automatic monitoring in a timely and precise manner. In the present work, a novel strategy has been proposed by considering acceleration-based measurements in the form of computer vision (through the encoding of time series into images) and proposes two novel strategies (employing DL models). Specifically, two advanced encoding strategies have been introduced: (i) compressed time series recurrence plot (CTSRP), which captures temporal dependencies and patterns in vibration data, and (ii) time delay encoding (dotTDE), which encodes time-delay features to enhance the representation of subtle damage characteristics. Transforming a time series to an image can bring contrasting features that may not be present in the raw time series. Such images are then utilized as input to the 2D convolutional neural network (CNN) for determining the possible damages in structures. The proposed strategies are finally validated using two building structures (numerically simulated) i.e., a shear frame structure and the IASC-ASCE benchmark structure (incorporating multiple damage scenarios). During the comparison of classification accuracy, it has been observed that the CTSRP method has achieved 96.1% accuracy and the dotTDE method has achieved 100% accuracy for the ten-story shear frame structure. These results have shown an improvement ranging from 0.9% to 92.9% for CTSRP and 5.0% to 100% for dotTDE over the state-of-the-art approaches. Additionally, for the IASC-ASCE benchmark structure, the CTSRP method has achieved 94.8% accuracy, with improvement ranging from 0.4% to 72.1%, while the dotTDE method has achieved 100% accuracy with improvement ranging from 5.9% to 77.3%. Moreover, the analysis of the robustness and scalability of the investigated DL models has also been reported. In the future, the investigation may be conducted by focusing on enhancing the proposed DL models to improve their ability to detect unseen damage classes and adapt to structural variations. Furthermore, the models can be applied to different real-world structures for broader validation and applicability.},
  archive      = {J_ASOC},
  author       = {Kunal Bharali and Manashi Saharia and Moumita Roy and Nirmalendu Debnath},
  doi          = {10.1016/j.asoc.2025.113457},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113457},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Structural health monitoring of building structures using novel acceleration-based signal-to-image technique and 2D convolutional neural networks},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG channel reconstruction using convolutional neural networks in limited BCIs: A proposed method for neuromarketing applications. <em>ASOC</em>, <em>181</em>, 113455. (<a href='https://doi.org/10.1016/j.asoc.2025.113455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromarketing is a discipline that combines insights from neuroscience with marketing strategies to gain a profound understanding of consumer behavior through the analysis of physiological data. Electroencephalography (EEG), which provides real-time and direct measurements of brain activity, is increasingly utilized to assess cognitive and emotional responses, offering vital insights for optimizing marketing campaigns. However, the application of EEG via Brain–Computer Interfaces (BCIs) in neuromarketing scenarios encounters specific challenges, particularly with low-cost BCIs. These devices, while expanding the accessibility of EEG technology for mass data collection, typically suffer from limited channel numbers and static electrode configurations, constraining the depth and amplitude of data that can be gathered. This study introduces a novel, a lightweight convolutional neural network model that has demonstrated good performance in reconstructing EEG signals for neuromarketing applications, particularly when employed with limited-channel, low-cost BCIs. The integration of advanced deep learning techniques directly into the EEG device firmware enables the virtual generation of additional EEG channels, effectively overcoming traditional device limitations. Furthermore, this research presents a custom loss function that combines mean squared error and Pearson’s correlation coefficient, ensuring high fidelity in signal reconstruction and alignment with the intricate temporal dynamics of brain activity. This enhancement is crucial for capturing accurate consumer responses. Validation against public EEG datasets demonstrated the superiority of the model in reconstructing EEG channels over traditional signal processing methods. Additionally, rigorous testing of over 1,820 channel configurations has proven the adaptability and precision of the model, making it ideal for diverse neuromarketing scenarios.},
  archive      = {J_ASOC},
  author       = {Mario Quiles Pérez and Sergio López Bernal and Eduardo Horna Prat and Luis Montesano Del Campo and Lorenzo Fernández Maimó and Alberto Huertas Celdrán},
  doi          = {10.1016/j.asoc.2025.113455},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113455},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EEG channel reconstruction using convolutional neural networks in limited BCIs: A proposed method for neuromarketing applications},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LAMNet: Fine-grained few-shot image classification with LLM-assisted multi-modal learning for urban biodiversity monitoring. <em>ASOC</em>, <em>181</em>, 113454. (<a href='https://doi.org/10.1016/j.asoc.2025.113454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid urbanization presents significant threats to biodiversity, especially in densely populated urban areas where ecological monitoring is both imperative and challenging. Accurate identification of urban species is essential for preserving ecosystem integrity; however, existing fine-grained classification methods often struggle with limited training data and high inter-class visual ambiguity To address these challenges, we propose LAMNet, a novel framework tailored for fine-grained few-shot image classification in the context of urban biodiversity monitoring. LAMNet integrates cross-modal learning with soft computing techniques to enhance feature discrimination under limited supervision. Specifically, the LLM-Cross module fuses semantic embeddings from large language models with visual features through a multilayer perceptron, yielding richer multimodal representations. In addition, we introduce two key modules: Salient Feature Optimization (SFO), which adaptively emphasizes informative local features, and Multi-Target Information Fusion (MTIF), a multi-target augmentation strategy designed to enhance generalization from limited data. Experimental results on the CUB-200-2011 dataset demonstrate that our method achieves substantial performance gains over the baseline, with improvements of up to 8.64% in 1-shot accuracy. Furthermore, LAMNet attains a 1-shot accuracy of 87.5%, surpassing the current state-of-the-art (SOTA) methods on this benchmark. These findings underscore the efficacy of LLM-enhanced cross-modal learning in enabling robust fine-grained classification. LAMNet offers a promising tool for the identification of urban species and the assessment of biodiversity, supporting conservation planning, and contributing to sustainable urban development.},
  archive      = {J_ASOC},
  author       = {Tao Zhang and Jiongchang Liu and Zeda Chen and Yeh-Cheng Chen and Mohammed Amoon and Saru Kumari and Xiaohan Liu},
  doi          = {10.1016/j.asoc.2025.113454},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113454},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LAMNet: Fine-grained few-shot image classification with LLM-assisted multi-modal learning for urban biodiversity monitoring},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural network based on back-propagation and cooperative co-evolution. <em>ASOC</em>, <em>181</em>, 113453. (<a href='https://doi.org/10.1016/j.asoc.2025.113453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have a powerful feature extraction capability, which allows them to be employed in various fields. However, as the number of layers and neurons in the network increases, the search space for parameter learning becomes complex. Currently, the most commonly used parameter training method is backpropagation (BP) based on gradient descent, but this method is sensitive to the initialization of the parameters and tends to get stuck in local optima in a complex search space. Therefore, a new training method for DNNs has been proposed that combines cooperative co-evolution (CC) with BP-based gradient descent, called BPCC. In the BPCC method, BP performs multiple training periods intermittently, and the CC algorithm is executed when the difference between the current loss function value and the previous loss function value is less than a given threshold (called a condition met). We found that the algorithm easily enters into CC iterations, which reduces the computational effectiveness of the algorithm. A tolerance parameter is designed to curb this phenomenon, and the CC is executed when the cumulative number of times the condition is met reaches the given value of the tolerance parameter, and the improved gray wolf optimizer (GWO) algorithm is used as the solver for the CC. In addition, in the CC iteration stage, the Chebyshev chaotic map series based on the current optimal point is used to initialize the population of GWO to ensure the diversity of the initial population. Experimental comparisons are made with modern network training methods in 7 network models, and the experimental results show that the improved algorithm in this study is competitive.},
  archive      = {J_ASOC},
  author       = {Yuelin Gao and Yuming Zhang and Xiaofeng Xie},
  doi          = {10.1016/j.asoc.2025.113453},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113453},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A neural network based on back-propagation and cooperative co-evolution},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial attacks on deep reinforcement learning applications in electric vehicle charging scheduling: A dual-stage attack framework. <em>ASOC</em>, <em>181</em>, 113450. (<a href='https://doi.org/10.1016/j.asoc.2025.113450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of deep reinforcement learning (DRL) into electric vehicle (EV) charging scheduling has shown promise in learning optimal charging strategies for users. However, existing security studies predominantly focus on image-input scenarios, leaving non-image DRL applications like EV charging vulnerable to under-explored adversarial threats. This paper addresses this critical gap by conducting the first systematic investigation of adversarial attacks on DRL-based EV charging systems. First, we formulate a realistic EV charging model with temporal dynamics and deploy four DRL algorithms (DQN, DDPG, PPO, SAC) to learn optimal policies. To overcome the limitations of existing white-box methods (e.g., FGSM) that require model knowledge and high computation, we design a novel dual-stage adversarial attack framework: (1) Adversarial policy networks are trained via behavior cloning using stolen data to approximate user decision-making; (2) RL-AdvGAN is designed to generate adversarial perturbations, maliciously modifying user states to induce suboptimal decisions. Experiments on real-world data demonstrate the effectiveness of the proposed approach. Compared with the popular FGSM method, our approach achieves more powerful attacks with less than 20% time cost. This work pioneers adversarial attack research in non-image DRL applications, providing critical insights into securing industrial DRL systems.},
  archive      = {J_ASOC},
  author       = {Zhenhua Peng and Qingyu Yang and Donghe Li and Feiye Zhang and Pengtao Song},
  doi          = {10.1016/j.asoc.2025.113450},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113450},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adversarial attacks on deep reinforcement learning applications in electric vehicle charging scheduling: A dual-stage attack framework},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature correction and semantic guidance for multimodal crowd counting. <em>ASOC</em>, <em>181</em>, 113449. (<a href='https://doi.org/10.1016/j.asoc.2025.113449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is a challenging task, especially under low-light conditions. RGB-T (RGB Thermal) technology merges thermal infrared (which can detect human body heat even in darkness) with standard RGB imaging, thereby mitigating the photosensitivity limitations of conventional RGB cameras. However, in complex scenes such as extreme illumination, adverse weather, occlusion, and high-density regions, the quality of RGB-T images may be unreliable. Directly fusing low-quality images under such conditions often leads to suboptimal crowd counting results. In addition, most existing fusion methods overlook the importance of feedback mechanisms for correcting modality-specific information. To address this limitation, we propose the Semantic Guided Multi-level Fusion Network (SGMNet), which enhances modal fusion through cross-modal feature interaction and semantic-guided multi-level fusion. Specifically, we design two modules: the Multimodal Information Exchange (MFE) module and the Semantic Guided Fusion (SGF) module. The MFE module measures the correlation between RGB and thermal images. It reweights the RGB-T features by explicitly modeling bidirectional attention weights between the modalities. This process helps correct disparities and enhances the consistent expression of information. Additionally, the SGF module leverages semantic information to aggregate multi-level hierarchical features, optimizing the use of cross-modal, multi-scale, and contextual information to produce high-quality population density maps. Experiments on two well-known crowd counting datasets, RGBT-CC and DroneRGBT, demonstrate that SGMNet significantly enhances performance. The model reduces Root Mean Square Error (RMSE) by 31.8% compared to the baseline (BL+IADM), highlighting its robustness and effectiveness.},
  archive      = {J_ASOC},
  author       = {Jin Wang and Yingchuan Zhao and Liyun Dou},
  doi          = {10.1016/j.asoc.2025.113449},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113449},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature correction and semantic guidance for multimodal crowd counting},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SANTM: A sparse access neural turing machine with local multi-head self-attention for long-term memorization. <em>ASOC</em>, <em>181</em>, 113448. (<a href='https://doi.org/10.1016/j.asoc.2025.113448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Sparse Access Neural Turing Machine (SANTM) to address long-term memorization challenges in sequence learning. The SANTM integrates a three-level neural controller with external memory: (1) a bottom layer for segmenting inputs into variable-length chunks, (2) a middle layer for short-term memory integration, and (3) a top layer that selectively accesses external memory via a locality-biased multi-head self-attention mechanism based on ChebNet spectral graph convolution. A sparse mask, trained through an ℓ 0 -constrained optimization scheme, reduces memory access rates while enabling pre-fetching. Theoretical analysis derives an optimal access rate under idealized conditions. Experiments on sequential image classification (MNIST, CIFAR10), text classification, speaker discrimination, and language modeling (WikiText-103, enwik8) demonstrate SANTM’s superiority over state-of-the-art sequential models. Key results include 95.7% accuracy on permuted MNIST (vs. NTM’s 94.0%), 85.4% on TC-Speech (vs. 79.6% for NTM), and 24.2 perplexity on WikiText-103 (vs. Transformer-XL’s 27.0). The sparse mask reduces FLOPs by 37%–54% compared to traditional NTMs, validating its efficiency.},
  archive      = {J_ASOC},
  author       = {Dongjing Shan and Jing Zhu and Yamei Luo},
  doi          = {10.1016/j.asoc.2025.113448},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113448},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SANTM: A sparse access neural turing machine with local multi-head self-attention for long-term memorization},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controllable diffusion models for hazardous construction site scene generation. <em>ASOC</em>, <em>181</em>, 113446. (<a href='https://doi.org/10.1016/j.asoc.2025.113446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazardous scene recognition is critical for construction site safety, but the low occurrence of such scenes in real environments leads to insufficient training data, limiting model development. Hazardous scene generation helps address data scarcity but involves complex background-object relationships and significant size differences, making precise layout control difficult. These characteristics make achieving precise layout control challenging for general-purpose hazardous scene generation models. To address these challenges, we propose a novel construction site hazardous scene generation framework based on large language and diffusion models, consisting of a two-stage generation process. In the first stage, we fine-tune the large language model (LLM) through context learning to serve as a text-based layout generator. In the second stage, we introduce a novel text-to-image diffusion model to guide the image generation process, ensuring that the generated image adheres to the scene layout produced in the first stage. Additionally, We propose two key modules, the Layout Enhancement Module and the Scale Fusion Module, to improve image quality and layout adherence. Comparative experiments show that our method generates superior scenes with stronger controllability and higher quality. Testing on a real-world dataset achieved a mAP score of 38.1%, improving model accuracy by 20.4% AP, 17.5% A P 50 , and 17.0% AR compared to models trained on real data, demonstrating our method’s effectiveness in hazardous construction site scene generation.},
  archive      = {J_ASOC},
  author       = {XunLong Wang and JiangTao Ren},
  doi          = {10.1016/j.asoc.2025.113446},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113446},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Controllable diffusion models for hazardous construction site scene generation},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A short-term memory clustering algorithm for evolving data streams. <em>ASOC</em>, <em>181</em>, 113442. (<a href='https://doi.org/10.1016/j.asoc.2025.113442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stream clustering is a fundamental problem in many streaming data analysis applications, which faces the following key challenges: (a) efficiently utilizing initial results to update clusters; (b) effectively managing concept drift when dealing with non-stationary data, which leads to decreased clustering accuracy over time. To address these limitations, this paper presents a new short-term memory clustering algorithm for evolving data streams, called STM-Stream. Short-term memory refers to storing the nucleus and radius of cell groups as the window slides, enabling streaming data clustering through three key steps: Firstly, the cell split method is used to obtain the initial data distribution. Then, a novel dynamic projection strategy is used to fuse the stored data distribution with newly arriving data distributions. Finally, based on the updated memory, an adaptive group radius grouping and merging method is designed to produce the final clustering result. Regarding the frequently occurring concept drift issue during clustering, the internal processes of four types of concept drift (Sudden, Gradual, Incremental, and Reoccurring) are analyzed and discussed. The article further extracted two main change processes: Gradual and Sudden drift to characterize the data migration process. Through dynamic projection and adaptive group radius methods, the algorithm can automatically correct its memory with sudden or gradual changes when concept drift occurs, which is equally effective for incremental and occurring concept drift. The experiment demonstrated that STM-Stream can effectively address concept drift, which frequently occurs in continuously generated streaming data, thereby preventing a decline in clustering accuracy over time.},
  archive      = {J_ASOC},
  author       = {Bo Liang and Jianghui Cai and Haifeng Yang and Gao Jie and Yaling Xun and Yupeng Wang and Fujiang Yuan},
  doi          = {10.1016/j.asoc.2025.113442},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113442},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A short-term memory clustering algorithm for evolving data streams},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost optimization strategy for dependent task offloading in vehicular edge computing. <em>ASOC</em>, <em>181</em>, 113441. (<a href='https://doi.org/10.1016/j.asoc.2025.113441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The geometrically increasing computational demands strain vehicular systems. Vehicular edge computing effectively alleviate this by dividing tasks into sub-modules and offloading these modules to edge servers. However, the interdependence among subtasks makes task offloading and resource allocation highly challenging. To address this issue, we propose a cost optimization strategy for dependent task offloading in vehicular edge computing networks. Specifically, the task offloading process is divided into two sub-problems: task offloading and resource allocation. First, we propose a Sequenced Quantization based on the Recurrent Neural Network (SQ-RNN) algorithm for offloading decisions. This algorithm uses environmental information as the input of the RNN to generate an optimal task offloading strategy. which is then quantified into multiple binary offloading actions through an order-preserving quantization method. Then, we propose a resource allocation method based on Computing Resource Blocks (CRBs), which divides server resources into blocks and assigns them to tasks with the principle of balancing resource allocation and reducing costs. Finally, extensive simulation experiments conducted on real-world datasets demonstrate that our approach reduces computing delay by 15.27% computing energy consumption by 9.93%, and cost by approximately 10.16% on average within the experimental bandwidth range, compared to the baseline algorithm. Moreover, as the number of subtasks increases, the optimization effect becomes more pronounced.},
  archive      = {J_ASOC},
  author       = {Tan Deng and Shixue Li and Mingfeng Huang and Xiaoyong Tang and Ronghui Cao and Wenzheng Liu and Yanping Wang},
  doi          = {10.1016/j.asoc.2025.113441},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113441},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cost optimization strategy for dependent task offloading in vehicular edge computing},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed multi-objective genetic model for parallel co-clustering ensemble. <em>ASOC</em>, <em>181</em>, 113437. (<a href='https://doi.org/10.1016/j.asoc.2025.113437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-clustering ensemble is a complex problem that can produce a robust co-clustering result, and the ensemble process can be solved by genetic algorithm. However, the high number of genetic operators and iterations in genetic algorithms results in low efficiency when using genetic algorithms for ensemble analysis. In this paper, parallel computing and genetic model are combined organically and a parallel co-clustering ensemble system based on distributed multi-objective genetic algorithm (GPCCE) is designed. First, this system combines the idea of parallel computing and improves the efficiency of the algorithm by assigning multiple chromosomes to different nodes, where all nodes simultaneously perform combination, mutation, and fitness calculation operations. Then, in the process of designing the system, a series of issues were considered, such as ensuring the diversity of the population and the robustness of the algorithm. Finally, a GPCCE algorithm is designed according to this system. To evaluate GPCCE, experiments that with many indicators are carried out, including for performance and for efficiency. The results show that GPCCE performs better than other outstanding algorithms.},
  archive      = {J_ASOC},
  author       = {Xu Li and Yuxin Zhong and Hongjun Wang and Tianrui Li},
  doi          = {10.1016/j.asoc.2025.113437},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113437},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distributed multi-objective genetic model for parallel co-clustering ensemble},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-stage restriction-handling method for the preventive maintenance of a complex machine using differential evolution algorithm. <em>ASOC</em>, <em>181</em>, 113427. (<a href='https://doi.org/10.1016/j.asoc.2025.113427'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of manufacturing equipment, reliability is facing increasingly serious challenges. To ensure its safe operation, this paper considers the preventive maintenance scheme of a complex machine considering production, variable maintenance instants, and deterioration. To begin with, a stochastic dynamical system is proposed to describe the machine’s deterioration process. Further, this problem is modeled as a stochastic dynamical system optimal control model (SDSOCM) including restrictions. It is challenging to directly achieve a high-quality solution of the SDSOCM due to its non-convexity, strong non-linearity, and randomness. To obtain a global optimal solution, the SDSOCM is analytically transformed into a deterministic dynamical system optimal control model (DDSOCM) with restrictions. Following that, a differential evolution algorithm with bi-stage restriction-handling method (DEA-BSRHM) is proposed to solve this DDSOCM via integrating the exterior point approach (EPA) and the interior point approach (IPA) into a differential evolution algorithm (DEA). In the first stage, the EPA involving a dynamic penalty parameter is proposed for comparing candidate members to drive them into the feasible domain. To enhance the search capability and decrease the calculation cost, the IPA including a dynamic penalty parameter is employed for choosing the candidate members in the second stage. Finally, the validity of the proposed method is illustrated via comprehensive experiments and comparative studies. Numerical results on a machine preventive maintenance problem and test functions from IEEE CEC 2010, IEEE CEC 2017, and IEEE CEC 2020 show that compared with other algorithms, DEA-BSRHM can obtain a better solution with smaller standard deviation and less number of function evaluations.},
  archive      = {J_ASOC},
  author       = {Xiang Wu and Jinxing Lin},
  doi          = {10.1016/j.asoc.2025.113427},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113427},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-stage restriction-handling method for the preventive maintenance of a complex machine using differential evolution algorithm},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing fairness and utility in healthcare machine learning models. <em>ASOC</em>, <em>181</em>, 113426. (<a href='https://doi.org/10.1016/j.asoc.2025.113426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demographic fairness or equity is a crucial aspect of machine learning models, particularly in critical domains such as healthcare, where errors can have severe consequences. A fair model avoids making distinctions between groups with different sensitive or protected attributes. Although several metrics are available to measure fairness and ensure equity between groups, proposed solutions must uphold fairness without compromising the utility of these models. Optimization problems that incorporate both fairness and utility information can help find the best machine learning model. Mathematical programming emerges as a valuable tool in this context. One approach is to use optimization functions with constraints, imposing a maximum difference between groups. In this sense, considering multiple constraints that encompass various sensitive attributes present in the dataset when adjusting the models is essential to ensure intersectional fairness, minimize hidden biases, and promote equitable decisions in diverse contexts. In this work, we propose to constrain the minimization of the loss function with multiple fairness-related metrics, ensuring that fairness metrics do not exceed a maximum limit concerning the impartiality of the decision boundary. We use metrics derived from Pareto fronts, a method used in multi-objective optimization, adapting it for single-objective optimization and incorporating fairness characteristics into the constraints. The points observed in this graph use different fairness thresholds. We compare our proposed model with existing literature and demonstrate the convergence of our model to logistic regression with simulated data. Furthermore, we apply this strategy to health-related datasets and other domains present in most fairness and optimization articles. As a result, we found that, using the proposed metrics, our model performs better, even with imbalanced data concerning sensitive attributes and smaller datasets.},
  archive      = {J_ASOC},
  author       = {Maíra Blumer Fatoretto and Gökhan Özbulak and André Anjos and Lilian Berton},
  doi          = {10.1016/j.asoc.2025.113426},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113426},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing fairness and utility in healthcare machine learning models},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum alternating operator ansatz with PSO optimizer for portfolio optimization problem. <em>ASOC</em>, <em>181</em>, 113419. (<a href='https://doi.org/10.1016/j.asoc.2025.113419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Quantum Approximate Optimization Algorithm (QAOA) and Quantum Alternating Operator ansatz (QAOAz) are utilized to solve the Mean Variance (MV) model for the portfolio optimization problem (QAOAz-MV), which shows performance advantages over classical algorithms on this huge search space. For the more complex and comprehensive risk parity (RP) model, a novel QAOAz solution (QAOAz-RP) is proposed. We begin by defining the RP model for the portfolio optimization problem. Next, we detail the QAOAz algorithm process, where the problem Hamiltonian with the ZZZZ term is derived, the corresponding quantum circuit is ingeniously constructed using the parity check method, and the whole quantum circuit containing the ring XY-mixer is given. Finally, to improve the optimization performance of QAOAz, a Particle Swarm Optimization (PSO) optimizer is introduced to tune the parameters of the quantum circuits, which is applicable to both QAOAz-MV and QAOAz-RP. The experiment conducted on multiple financial markets (e.g. Chinese, U.S., and European) demonstrate that PSO-QAOAz-RP is significantly better for portfolio optimization than ABC-LP, GWO, GA and QAOAz-RP on eight portfolios in all metrics. PSO-QAOAz-MV also has advantage for MV model over the quantum algorithms, including QAOA (improves approximate ratio by 54.79% on average) and QAOAz (improves approximate ratio by 15.38% on average). This study not only provides a breakthrough quantum solution for portfolio optimization, but also provides a reusable technology paradigm for the deep integration of quantum computing and financial engineering.},
  archive      = {J_ASOC},
  author       = {Wenjie Liu and Yue Ma and Yuchen Gu and Jiajun Cheng and Qingshan Wu},
  doi          = {10.1016/j.asoc.2025.113419},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113419},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum alternating operator ansatz with PSO optimizer for portfolio optimization problem},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A knowledge-driven fuzzy logic framework for supporting decision-making entities. <em>ASOC</em>, <em>181</em>, 113415. (<a href='https://doi.org/10.1016/j.asoc.2025.113415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision support systems enable decision makers (whether individuals, systems, or other agents) to select the most suitable options by integrating expert knowledge with computational intelligence. Accurate modeling of these decision makers is crucial to ensure optimal decision making in complex and uncertain environments. Embedding expert knowledge in these models is challenging, as experts often lack familiarity with the underlying techniques. Therefore, there is a need for frameworks that are intuitive for experts and enable them to seamlessly integrate their knowledge into decision support systems. This paper presents a novel framework for the automatic generation of fuzzy decision models based on expert knowledge, designed to support decision-making scenarios. The proposed approach leverages the Takagi–Sugeno–Kang Fuzzy Inference System (TSK FIS) to model qualitative human reasoning and automatically induce decision models through expert-defined parameters that model the expert knowledge. This framework represents decision variables using linguistic terms, and introduces a weighted co-occurrence mechanism that captures variable interactions, enabling the generation of cumulative fuzzy decision rules that produce robust and interpretable outcomes. It simplifies expert data input through an intuitive method for defining relationships between variables, eliminating the need for extensive knowledge of fuzzy logic. The flexibility of the proposed framework is demonstrated through two practical case studies: passenger train ticket selection, and weapon choice optimization in video games, showcasing its effectiveness across varied domains. Experimental results validate the system’s capacity to generate tailored decision models that adapt to specific user profiles and objectives, while maintaining both decision-making accuracy and interpretability.},
  archive      = {J_ASOC},
  author       = {David Muñoz-Valero and Juan Moreno-Garcia and Julio Alberto López-Gómez and Enrique Adrian Villarrubia-Martin and Luis Jimenez-Linares},
  doi          = {10.1016/j.asoc.2025.113415},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113415},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A knowledge-driven fuzzy logic framework for supporting decision-making entities},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing knowledge distillation via genetic recombination. <em>ASOC</em>, <em>181</em>, 113414. (<a href='https://doi.org/10.1016/j.asoc.2025.113414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diverging from conventional knowledge distillation methods that solely emphasize improving the utilization of the teacher’s knowledge, this paper explores the generation of stronger student models within available knowledge. We first conceptualize the knowledge distillation process as a genetic evolution model. The student model is regarded as an independent individual, with its parameters representing the genes of that individual. These genes are partitioned into several alleles according to the architecture of the student model. Following that, we propose a universal strategy to enhance existing knowledge distillation methods by introducing genetic recombination. Prior to distillation, we initialize two independent identically distributed student models with different random seeds to obtain the first generation of genes. With each epoch of distillation, these genes evolve into the next generation. At specific generations, we randomly select one exchangeable allele from each of the two students for exchange. Our focus lies in determining the alleles to exchange and their corresponding exchange frequency (i.e., crossing-over value). This approach provides more choices and possibilities for subsequent evolution. Extensive experiments confirm the effectiveness of the strategy, demonstrating improvements across 12 distillation methods and 17 teacher–student combinations.},
  archive      = {J_ASOC},
  author       = {Yangjie Cao and Chuanjin Zhou and Minglin Liu and Weiqi Luo and Xiangyang Luo},
  doi          = {10.1016/j.asoc.2025.113414},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113414},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing knowledge distillation via genetic recombination},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A swarm intelligence-based vaccination strategy for preventing epidemic spreading. <em>ASOC</em>, <em>181</em>, 113413. (<a href='https://doi.org/10.1016/j.asoc.2025.113413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Targeted vaccination of key population groups emerges as an effective intervention strategy due to vaccine availability limitation. This approach markedly improves the resistance of social networks against infectious disease transmission. Previous immunization strategies primarily focused on two technical aspects: optimization of disease transmission models and selection criteria for vaccination nodes. The node selection mechanism has traditionally utilized either local or global topological network metrics. In this study, we establish the analytical formulation for epidemic thresholds in undirected weighted networks, thereby transforming the population vaccination challenge into an optimization problem aimed at threshold maximization. We introduce a targeted vaccination strategy-DPSOLACC oriented around the epidemic threshold as the primary objective. This strategy implements a hybrid discrete particle swarm optimization approach, incorporating learning automata with local search mechanisms. The integrated framework significantly improves the algorithm’s global search and local adjustment capabilities, while effectively considering both global and local network topological characteristics. The optimal vaccination nodes selected through this strategy can significantly improve the efficacy of vaccination in the population and greatly strengthen the resistance of the social network to infectious diseases. The proposed algorithm comprises two phases: selection of the candidate vaccination nodes and selection of the final vaccination nodes. Experiments show that our algorithm surpasses conventional baseline strategies in strengthening the immunity of social networks to infectious diseases.},
  archive      = {J_ASOC},
  author       = {Laijun Zhao and Luping Chen and Pingle Yang and Huiyong Li and Lixin Zhou and Han Chang},
  doi          = {10.1016/j.asoc.2025.113413},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113413},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A swarm intelligence-based vaccination strategy for preventing epidemic spreading},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoML-based EEG signal analysis in neuro-marketing classification using biclustering method. <em>ASOC</em>, <em>181</em>, 113412. (<a href='https://doi.org/10.1016/j.asoc.2025.113412'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) is a technique used to capture electrical brain activity via non-invasive electrodes. In this study, we propose an innovative approach combining biclustering methods for noise reduction and AutoML techniques for classifying brain responses in a neuromarketing context. Using a 14-channel Emotiv Epoc+ EEG headset, we collected data from 25 participants while exposed to consumer stimuli. AutoML, implemented via the H2O.ai platform, tested multiple models including Random Forest, Gradient Boosting Machines, and Deep Neural Networks. The proposed framework achieved up to 94% classification accuracy and 0.95 F1-score. Our contributions include: (1) a denoising method using biclustering for EEG preprocessing; (2) a comprehensive AutoML-based pipeline; and (3) a comparative evaluation against baseline models.},
  archive      = {J_ASOC},
  author       = {Tiezhu Zhao and Francisco Nauber Bernardo Gois and Joao Alexandre Lobo Marques and Bruno Riccelli dos Santos Silva and Paulo Cortez and Senthil Kumar Jagatheesaperumal and Victor Hugo C. de Albuquerque},
  doi          = {10.1016/j.asoc.2025.113412},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113412},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AutoML-based EEG signal analysis in neuro-marketing classification using biclustering method},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards fair machine learning using many-objective feature selection. <em>ASOC</em>, <em>181</em>, 113411. (<a href='https://doi.org/10.1016/j.asoc.2025.113411'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Many-Objective Feature Selection (MOFS) as a novel approach for enhancing fairness in machine learning models. MOFS simultaneously optimizes model complexity, performance, and fairness using evolutionary algorithms to ensure that the selected features lead to a balanced and equitable model. We evaluate MOFS across 15 diverse fairness datasets, comparing it to three baseline methods, including IBM AI Fairness 360. Statistical analysis of our results demonstrates that MOFS consistently and significantly achieves higher accuracy and improves fairness metrics such as demographic parity and equalized odds. The study further examines the robustness and scalability of the MOFS approach across the datasets. The consistently high hypervolume metric indicator confirms that a balance between model complexity, performance, and fairness is achieved. This work highlights the potential of MOFS to develop fairer machine learning models while reducing model complexity and without compromising performance.},
  archive      = {J_ASOC},
  author       = {Uchechukwu F. Njoku and Alberto Abelló and Besim Bilalli and Gianluca Bontempi},
  doi          = {10.1016/j.asoc.2025.113411},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113411},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards fair machine learning using many-objective feature selection},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing smart tightening diagnosis: A transformer-based approach with sensor fusion, self-supervised learning and data augmentation. <em>ASOC</em>, <em>181</em>, 113409. (<a href='https://doi.org/10.1016/j.asoc.2025.113409'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing adoption of deep learning, particularly supervised learning, in the manufacturing highlights the need for large labeled datasets. However, generating domain-specific labeled data is costly. Focusing on smart tightening diagnosis in manufacturing, prior research introduced the tightening diagnosis transformer (TDT), which leverages self-supervised transformers to reduce dependency on labeled data. Despite its advancements, TDT has two key limitations: (1) reliance solely on torque sensor data, ignoring angle sensor data, and (2) added computational overhead from self-supervised learning, which is problematic in resource-limited shop-floor environments. This study presents a novel transformer-based multi-label classification method that integrates sensor fusion and reduces needs for both computation and labeled data. We enhance the state-of-the-art TDT by introducing the angle positional encoder (APE), enabling feature-level sensor fusion for supervised learning. Additionally, we propose a self-supervised learning method for APE-enhanced TDT to reduce the need for extensive labeled datasets. We also introduce the random sequence patchifier (RSP), a transformer-specific data augmentation technique that improves generalization and reduces computational cost. Finally, we adopt annealing augmentation scheduling to mitigate the risk of learning “fake” feature representations (unrealistic artifacts created by the augmentations). Compared with previous TDT, our experiment evaluation demonstrates that the these introduced techniques improve S u b s e t A c c u r a c y and F1 scores by 10% and 7%. Moreover, the RSP-based augmentation reduces the training time by 12% for supervised learning and 15% for self-supervised learning.},
  archive      = {J_ASOC},
  author       = {Lifei Tang and Dennis Wilkman and Lei Feng and Martin Törngren},
  doi          = {10.1016/j.asoc.2025.113409},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113409},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing smart tightening diagnosis: A transformer-based approach with sensor fusion, self-supervised learning and data augmentation},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interact2Vec — An efficient neural network-based model for simultaneously learning users and items embeddings in recommender systems. <em>ASOC</em>, <em>181</em>, 113408. (<a href='https://doi.org/10.1016/j.asoc.2025.113408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, recommender systems have experienced a surge in popularity. Despite notable progress, they grapple with challenging issues, such as high data dimensionality and sparseness. Representing users and items as low-dimensional embeddings learned via neural networks has become a leading solution. However, while recent studies show promising results, many approaches rely on complex architectures or require content data, which may not always be available. This paper presents Interact2Vec, a novel neural network-based model that simultaneously learns distributed embeddings for users and items while demanding only implicit feedback. The model employs state-of-the-art strategies that natural language processing models commonly use to optimize the training phase and enhance the final embeddings. Two types of experiments were conducted regarding the extrinsic and intrinsic quality of the model. In the former, we benchmarked the recommendations generated by Interact2Vec’s embeddings in a top- N ranking problem, comparing them with six other recommender algorithms. The model achieved the second or third-best results in 30% of the datasets, being competitive with other recommenders, and has proven to be very efficient with an average training time reduction of 274% compared to other embedding-based models. Later, we analyzed the intrinsic quality of the embeddings through similarity tables. Our findings suggest that Interact2Vec can achieve promising results, especially on the extrinsic task, and is an excellent embedding-generator model for scenarios of scarce computing resources, enabling the learning of item and user embeddings simultaneously and efficiently.},
  archive      = {J_ASOC},
  author       = {Pedro R. Pires and Tiago A. Almeida},
  doi          = {10.1016/j.asoc.2025.113408},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113408},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interact2Vec — An efficient neural network-based model for simultaneously learning users and items embeddings in recommender systems},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive temporal diffusion-based reconstruction model for industrial dynamic uncertain process monitoring. <em>ASOC</em>, <em>181</em>, 113407. (<a href='https://doi.org/10.1016/j.asoc.2025.113407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key property of industrial processes is that they are often related to the dynamic uncertainty behaviors of the measurement data (e.g., sensor performance degradation or environmental changes), which poses significant challenges for traditional uncertainty-based monitoring research that typically assumes the measurement data exhibits invariant uncertainty. This study addresses this challenge by enriching the corrupted state of the data. We propose a novel diffusion-based method called the Dynamic Uncertainty Process Monitoring (DUPM) method. DUPM consists of a temporal diffusion convolutional network (TDCN) module and an adaptive diffusion reconstruction (ADR) module. First, in TDCN module, the diffusion process enhances the pattern coverage by gradually corrupting the original data, enabling the model to cover data under different uncertainties. Then an unsupervised backbone is designed to extract the latent temporal features of the input data and remove the noise in generation process, in which a nonlinear autoencoder is equipped with a one-dimensional convolution operation. The monitoring threshold is determined based on the reconstruction error after the diffusion process and generation process. Finally, the ADR module determines the number of steps to add noise in the online stage by calculating the similarity between the online data and the historical diffusion states. In this way, the reconstruction error can be used as a monitoring score. Experiments conducted on numerical simulations and the real-world MetroPT-3 dataset show that DUPM reduces the false alarm rate by at least 3% compared to the comparison method while maintaining fault detection rates. The verification indicates that the proposed method has potential in monitoring dynamic uncertain industrial processes.},
  archive      = {J_ASOC},
  author       = {Jiawei Yin and Jianbo Yu and Qingchao Jiang and Xuefeng Yan},
  doi          = {10.1016/j.asoc.2025.113407},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113407},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive temporal diffusion-based reconstruction model for industrial dynamic uncertain process monitoring},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural networks based on interval-valued pseudo overlap and grouping functions with applications to fuzzy reasoning and image classification. <em>ASOC</em>, <em>181</em>, 113405. (<a href='https://doi.org/10.1016/j.asoc.2025.113405'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo overlap functions (POFs) are critical in image processing, text classification, decision-making systems etc. However, integrating interval-valued pseudo overlap functions (IPOFs) and interval-valued pseudo grouping functions (IPGFs) with neural networks remains underexplored. To address this gap, we propose a systematic framework that fundamentally redesigns neural architectures using IPOFs and IPGFs. First, some new results about IPOFs and IPGFs are introduced, including their pseudo automorphisms, multiplicative generators and additive generators. Then, ANNs and CNNs based on IPOFs (called IV-POG ANNs and IV-POG CNNs) are introduced respectively. The introduction of IPOFs substantially enhance the nonlinearity, asymmetry and bilateral approximation ability of ANNs and CNNs. Third, Takagi–Sugeno–Kang (TSK) fuzzy systems based on IPOFs are introduced and IV-POG ANNs are used to fit these systems and train their parameters. The experiments demonstrate that IPOFs significantly enhance the image classification performance of ANNs and CNNs, while simultaneously strengthening the inference capabilities of TSK fuzzy systems.},
  archive      = {J_ASOC},
  author       = {Mengyuan Li and Mei Jing and Xiaohong Zhang and Jun Liu},
  doi          = {10.1016/j.asoc.2025.113405},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113405},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neural networks based on interval-valued pseudo overlap and grouping functions with applications to fuzzy reasoning and image classification},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end 3D point cloud question answering via state space models. <em>ASOC</em>, <em>181</em>, 113404. (<a href='https://doi.org/10.1016/j.asoc.2025.113404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of 3D Visual Question Answering (3D-VQA) remains relatively unexplored due to the limited datasets and techniques developed thus far. Additionally, existing methods often rely on supplementary input data, such as 2D images or auxiliary tasks like object localization or scene graph prediction, to enhance performance with the use of multiple loss functions during training. This practice provides rich contextual feedback to the neural network, but also complicates model design and limits flexibility. To the best of our knowledge, this paper proposes the first state–space models (SSMs)-based approach in the 3D-VQA domain, specifically introducing a Mamba-based architecture designed to push the boundaries of what can be achieved with point clouds, questions, and answers alone, without relying on additional input/output information. Our architecture comprises four primary Mamba-based modules. The point cloud encoder converts point clouds into high-dimensional feature representations that capture both geometry and color. The question encoder generates contextual embeddings from input questions. The Mamba layer functions as a feature fusion module that fuses visual–visual and visual–linguistic features into a unified representation. Finally, the answer decoder produces open-ended or close-ended answers. While other methods may benefit from expanded information setups, our method is tailored for scenarios with limited information availability. Evaluated under varying point cloud resolutions, the proposed method achieves state-of-the-art accuracy on the CLEVR3D-REAL dataset and outperforms the baseline on the ScanQA dataset across all evaluation metrics under the same input/output data setups.},
  archive      = {J_ASOC},
  author       = {Hatem Qorsham and Reem El-Deeb and Ehab Essa},
  doi          = {10.1016/j.asoc.2025.113404},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113404},
  shortjournal = {Appl. Soft. Comput.},
  title        = {End-to-end 3D point cloud question answering via state space models},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient heuristics for the obnoxious planar p-median problem with variable sizes. <em>ASOC</em>, <em>181</em>, 113401. (<a href='https://doi.org/10.1016/j.asoc.2025.113401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The location of obnoxious facilities is an optimization problem with a large social impact. Specifically, the obnoxious facility location problem in the plane with variable sizes (OPPMVS) studies the location of facilities considering that the obnoxious effect is transmitted through the air and depends on the production or service of the facility. In this work, a memetic algorithm is proposed in which the generation of the initial population and the genetic operators have been specifically designed for the target problem. In this approximation of this continuous problem, competitive results have been obtained compared with the state-of-the-art. The proposal has been tested in 21 problem instances provided by the original authors, obtaining the best results in 14 of them with a total deviation of 0.07%. This performance is obtained in an average execution time of 22 s, which improves the best state-of-the-art algorithm by one order of magnitude. These results have been validated with statistical tests.},
  archive      = {J_ASOC},
  author       = {Sergio Salazar and Oscar Cordón and J. Manuel Colmenar},
  doi          = {10.1016/j.asoc.2025.113401},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113401},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient heuristics for the obnoxious planar p-median problem with variable sizes},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust detection of unknown adversarial examples in network traffic via enhanced latent spatial distribution analysis with diffusion model. <em>ASOC</em>, <em>181</em>, 113390. (<a href='https://doi.org/10.1016/j.asoc.2025.113390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning-based intrusion detection is widely applied in network defense but faces threats from adversarial attacks. The primary challenge in defending against these attacks is to pioneer the capability to counter evolving adversarial attacks while enhancing the distinguishability of adversarial behavior. Current research on adversarial example detection lacks effective heuristic methods to cope with the proliferation of unknown attacks, reducing the generality and practicality of detection. Additionally, redundant features and noise (such as functional attributes) negatively impact the training of adversarial example detectors, often leading to the misclassification of malicious samples as having a benign label. This paper proposes R obust D etection of U nknown A dversarial E xamples (RDUAE). It analyzes latent space distribution deviations using diffusion models and constructs a heuristic malignancy probability discriminator to identify adversarial examples of any paradigm. Besides, it introduces random discriminative feature activation to eliminate noise features unrelated to adversarial detection, amplifying the latent space distribution deviations caused by adversarial perturbations. Extensive experiments demonstrate that RDUAE achieves state-of-the-art performance with an average of 98.8% accuracy and an average of 98.4% in the Area Under Curve (AUC) in detecting both known and unknown attacks without prior knowledge of adversarial examples, uniquely identifying unknown adversarial paradigms by modeling solely benign samples and highlighting significant practicality.},
  archive      = {J_ASOC},
  author       = {Siyuan Shao and Senlin Luo and Zhiyang Zhao and Kun Gong and Limin Pan},
  doi          = {10.1016/j.asoc.2025.113390},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113390},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust detection of unknown adversarial examples in network traffic via enhanced latent spatial distribution analysis with diffusion model},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expensive multiobjective optimization based on sequence learning of long-short-term-memory model. <em>ASOC</em>, <em>181</em>, 113388. (<a href='https://doi.org/10.1016/j.asoc.2025.113388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the studies of expensive multi-objective evolution (EMOP), extensive achievements have been proposed by constructing surrogate models between the individual and the objective functions. However, the correlation between individual and population sequences is not well utilized. Based on this motivation, to take advantage of the correlation within the population sequence, a sequence learning model-based expensive multiobjective optimization algorithm (SLEMO) is proposed in this paper. By constructing the population sequence with the decomposition method and innovatively introducing the long short-term memory model (LSTM) as a sequence classifier, the LSTM network is applied to learn both the individual’s characteristics and the population’s correlation relationships so that the sequence information of individuals can guide the evolutionary search. The proposed algorithm is tested on several widely used benchmark test suites with up to 50 decision variables and an engineering optimization problem of blended-wing-body underwater gliders (BWBUG). The comparison results indicate that SLEMO demonstrates superior performance by achieving the best results in 47% of the 68 test problems and the 4.2% lift-drag ratio increase on the EMOP of BWBUG, outperforming state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Xinjing Wang and Junchang Liu and Chuanzhen Liu and Jiangtao Shen and Peng Wang},
  doi          = {10.1016/j.asoc.2025.113388},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113388},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Expensive multiobjective optimization based on sequence learning of long-short-term-memory model},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-knowledge-assisted sampling with variable sorting for large-scale multi-objective optimization. <em>ASOC</em>, <em>181</em>, 113386. (<a href='https://doi.org/10.1016/j.asoc.2025.113386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multi-objective optimization problems (LSMOPs), characterized by hundreds or thousands of decision variables, present significant challenges for existing multi-objective evolutionary algorithms (MOEAs), which often struggle to balance the trade-off among diversity, convergence, and scalability in the vast decision spaces within limited computational resources. To address these challenges, this paper introduces a novel large-scale MOEA that incorporates two key innovations: the adaptive sorting mutation (ASM) and meta-knowledge-assisted sampling (MKAS) strategies. The ASM adaptively perturbs variable values through ascending and descending sorting mutations to enrich the population’s diversity and enhance the exploration of the search space. Meanwhile, the MKAS employs a neural network to learn and predict promising meta-knowledge, thus to dynamically guide the evolutionary efforts towards the Pareto set. Additionally, the competitive swarm optimizer is integrated to further refine the final population, providing a robust baseline for optimization. The performance of the proposed method is evaluated through extensive experiments on 325 LSMOPs with up to 10,000 decision variables and 9 objectives. The results demonstrate the superior convergence, diversity, and scalability of the proposed method compared to existing state-of-the-art algorithms, highlighting its potential as an effective solution for large-scale and complex multi-objective optimization tasks.},
  archive      = {J_ASOC},
  author       = {Haofan Wang and Li Chen and Xingxing Hao and Tingfeng Yu and Yongkang Qian and Ruoxi Yang and Wei Liu},
  doi          = {10.1016/j.asoc.2025.113386},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113386},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Meta-knowledge-assisted sampling with variable sorting for large-scale multi-objective optimization},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constraint consensus assisted evolutionary algorithm for large-scale constrained optimization. <em>ASOC</em>, <em>181</em>, 113383. (<a href='https://doi.org/10.1016/j.asoc.2025.113383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale constrained optimization problems present significant challenges due to their large number of variables and many constraints. Improper handling of these constraints can lead to suboptimal or infeasible solutions. Many existing approaches overlook this aspect. In this paper, we integrate a constraint-objective cooperative coevolution framework with a Constraint Consensus method, known as DBmax (Maximum Direction-based Method), into differential evolution. In this framework, a problem is decomposed into a number of smaller subproblems (subcomponents) using the Recursive Differential Grouping technique, where interactive variables are allocated to one subproblem. By assessing the impact of each group on the objective function and constraint violation, the most suitable group is selected for evolution. Subsequently, the DBmax method is applied adaptively to the infeasible solutions within the chosen group for improving their feasibility. The algorithm was evaluated on 12 test problems, with the experimental results consistently demonstrating its effectiveness by outperforming existing state-of-the-art methods, in terms of the solution’s feasibility and quality.},
  archive      = {J_ASOC},
  author       = {Noha Hamza and Saber Elsayed and Ruhul Sarker and Daryl Essam},
  doi          = {10.1016/j.asoc.2025.113383},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113383},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constraint consensus assisted evolutionary algorithm for large-scale constrained optimization},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An injected multi-objective metaheuristic approach for optimizing aerial-robot swarm guidance in cluttered environments. <em>ASOC</em>, <em>181</em>, 113379. (<a href='https://doi.org/10.1016/j.asoc.2025.113379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a comprehensive Multi-Robot Guidance Framework (MRGF) designed to simultaneously optimize task allocation and trajectory generation for aerial robot swarms operating in dynamic, obstacle-rich environments. The proposed framework introduces a novel hybrid optimization approach that combines an Injected Genetic Algorithm (IGA) with Multiple Linear Assignment (MLA) techniques, offering a scalable and efficient solution for large-scale aerial robot swarms. The MRGF aims to address the NP-hard combinatorial challenges of efficiently assigning multiple goals to a limited number of robots. By ensuring collision-free and dynamically feasible trajectories with rapid replanning capabilities, the MRGF minimizes total mission completion time and travel distance, even in complex scenarios. Extensive simulation results validate the framework’s robustness and demonstrate its superiority over a couple of mature approaches in multi-objective performance metrics. Additionally, hardware-in-the-loop testing on embedded platforms confirms the MRGF’s real-time capabilities even for complex scenarios with hundreds of goals and dozens of aerial robots. The MRGF achieved a mission completion time of 328 s and a total travel distance of 596 meters in a challenging simulation scenario. These findings underscore its potential for a wide range of applications, including autonomous environmental monitoring, disaster response, precision agriculture, delivery systems, and infrastructure inspection. This study highlights the MRGF as a scalable, efficient, and adaptable solution for multi-robot systems in real-world scenarios.},
  archive      = {J_ASOC},
  author       = {Yunes Alqudsi},
  doi          = {10.1016/j.asoc.2025.113379},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113379},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An injected multi-objective metaheuristic approach for optimizing aerial-robot swarm guidance in cluttered environments},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning: Historical overview from inception to actualization, models, applications and future trends. <em>ASOC</em>, <em>181</em>, 113378. (<a href='https://doi.org/10.1016/j.asoc.2025.113378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning stands at the forefront of contemporary machine learning techniques and is well-known for its outstanding predictive accuracy, adaptability to data variability, and remarkable ability to generalize across diverse domains. These attributes have spurred rapid progress and the emergence of novel iterations within the discipline. Yet, this swift evolution often obscures the foundational breakthroughs, with even trailblazing researchers at risk of fading into obscurity despite their seminal contributions. This study aims to provide a historical narrative of deep learning, tracing its origins from the cybernetic era to its current state-of-the-art status. We critically examine the contributions of individual pioneer scholars who have profoundly influenced the development of deep neural networks under the taxonomy of supervised, unsupervised, and reinforcement learning. Furthermore, the study also discusses the trending deep neural network architectures, explaining their operational principles, confronting associated challenges, exploring real-world applications, and outlining potential future trajectories that could offer a starting point for aspiring researchers in the field.},
  archive      = {J_ASOC},
  author       = {Olufisayo S. Ekundayo and Absalom E. Ezugwu},
  doi          = {10.1016/j.asoc.2025.113378},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113378},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning: Historical overview from inception to actualization, models, applications and future trends},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularizing model predictive control for pixel-based long-horizon tasks. <em>ASOC</em>, <em>181</em>, 113377. (<a href='https://doi.org/10.1016/j.asoc.2025.113377'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning has been proven to be an effective strategy for dealing with complex tasks in environments. However, due to the constraints of computational budget and the accumulated model biases, planning for pixel-based long horizon tasks with limited samples remains a great challenge. To address this issue, a R egularized M odel P redictive C ontrol ( RMPC ) was proposed in this study. RMPC performs trajectory optimization using short-term reward estimates and long-term return estimates, which avoids the high burden of long-horizon planning. Additionally, an implicit regularization mechanism is employed to improve the robustness of the generated environment model and reliability of the value function estimation, which helps to reduce the risk of accumulated model biases. Extensive comparison experiments and ablation studies are performed on the benchmark datasets for evaluating the proposed RMPC. And empirical results show that RMPC outperforms the previous SOTA algorithms in terms of sample-efficiency (20.88% performance improvement) and model stability (56.39% standard deviation reduction) on pixel-based continuous control tasks from DMControl-100k benchmark. Our code is available at: https://github.com/Arya87/RMPC .},
  archive      = {J_ASOC},
  author       = {Yao-Hui Li and Feng Zhang and Qiang Hua and Chun-Ru Dong},
  doi          = {10.1016/j.asoc.2025.113377},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113377},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Regularizing model predictive control for pixel-based long-horizon tasks},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A historical surrogate model ensemble assisted bayesian evolutionary optimization algorithm for solving expensive many-objective problems. <em>ASOC</em>, <em>181</em>, 113367. (<a href='https://doi.org/10.1016/j.asoc.2025.113367'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-assisted evolutionary optimization has become a promising approach for solving expensive multi-objective optimization problems. However, as the dimension of the objective space increases, it becomes difficult to obtain sufficient samples to train a high-precision surrogate model, while the plethora of historical surrogate models generated during algorithm iterations remain underutilized. Addressing these issues, this research leverages the untapped potential of historical surrogate models by integrating them into a novel framework that enhances the diversity of environmental selection. Therefore, this paper proposes a historical surrogate model ensemble-assisted Bayesian evolutionary optimization algorithm (HMBEO) to address high-dimensional many-objective optimization problems. This method fully utilizes the historical information captured by the surrogate model. Specifically, it uses promising individuals selected by historical surrogate models as parents to guide the evolution of the population. The selection of the next generation of parents considers both the convergence of the objective space and the diversity of the decision space. Additionally, an improved infill criterion strategy is introduced to select samples for original expensive evaluation. This strategy uses the maximum distance criterion to select potential individuals and supplements sample points in sparse areas to ensure a good diversity of objectives. The performance of the proposed algorithm is evaluated on a set of expensive many-objective benchmark problems. Experimental results demonstrate that it outperforms four state-of-the-art surrogate-assisted evolutionary algorithms.},
  archive      = {J_ASOC},
  author       = {Hui Liu and Jie Tian and Qian Yu and Xin Liu and Gaige Wang},
  doi          = {10.1016/j.asoc.2025.113367},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113367},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A historical surrogate model ensemble assisted bayesian evolutionary optimization algorithm for solving expensive many-objective problems},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial hard negative samples for continual relation extraction. <em>ASOC</em>, <em>181</em>, 113365. (<a href='https://doi.org/10.1016/j.asoc.2025.113365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual relation extraction (CRE) is a crucial task in continuous learning, aiming to train a model continually on data of new relations to extract relations between entities from unstructured text. This process involves learning newly emerged relations while avoiding catastrophic forgetting of previously learned relations. Existing works have demonstrated that storing a few typical samples of old relations in memory and replaying them during subsequent training for new relations is helpful. It can assist the model to maintain a stable understanding of old relations, thus effectively avoiding forgetting them. However, most prior research has focused on efficiently utilizing memory samples while neglecting their learning difficulty, which refers to the challenge in mastering these samples. In this paper, we propose an adversarial hard negative samples selection mechanism to increase the diversity of memory samples and dynamically adjust the number of samples among relations according to the performance of the model. Experimental results show that our method consistently improves the performance of state-of-the-art CRE models without increasing the number of training samples on mainstream benchmarks.},
  archive      = {J_ASOC},
  author       = {Shunyu Yao and Jinyu Guo and Jijie Li and Jie Ou and Yufei Feng and Jie Hu and Dan Liu},
  doi          = {10.1016/j.asoc.2025.113365},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113365},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adversarial hard negative samples for continual relation extraction},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying large language models to sanitize self-disclosure in user-generated content. <em>ASOC</em>, <em>181</em>, 113311. (<a href='https://doi.org/10.1016/j.asoc.2025.113311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of e-commerce and social networking platforms has led to an increase in the disclosure of personal health information within user-generated content. This study investigates the application of large language models (LLMs) to detect and sanitize sensitive health data shared by users across platforms such as Amazon, patient.info , and Facebook. We propose a methodology that leverages LLMs to evaluate both the sensitivity of disclosed information and the platform-specific semantics of the content. Through prompt engineering, our method identifies sensitive information and rephrases it to minimize disclosure while preserving content similarity. ChatGPT serves as the LLM in this study due to its versatility. Empirical results suggest that ChatGPT can reliably assign sensitivity scores to user-generated text and generate sanitized versions that effectively preserve the original meaning.},
  archive      = {J_ASOC},
  author       = {Costanza Alfieri and Gian Luca Scoccia and Surya Ganesh and Norman Sadeh},
  doi          = {10.1016/j.asoc.2025.113311},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113311},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Applying large language models to sanitize self-disclosure in user-generated content},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using image conversion techniques to detect adversarial examples in machine learning models for tabular data. <em>ASOC</em>, <em>181</em>, 113288. (<a href='https://doi.org/10.1016/j.asoc.2025.113288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most adversarial detection methods are designed for deep neural networks (DNNs) trained on homogeneous data (e.g., images), which leaves machine learning (ML) models for tabular data particularly vulnerable to adversarial attacks. This paper introduces two novel approaches for the detection of adversarial examples in tabular data that transform tabular data into image representations, enabling the use of convolutional neural networks (CNNs) to improve adversarial robustness. The first is an unsupervised framework that exploits explainability techniques to compare model behavior using two distinct data representations: the original tabular form and its transformed image-based counterpart. By analyzing discrepancies in feature attributions and model predictions, this approach detects adversarial examples in a model- and attack-agnostic manner. The second is a supervised method utilizing transfer learning which leverages a large-scale pretrained image classification model to extract adversarial patterns from converted tabular data, significantly enhancing detection accuracy with minimal training overhead. The evaluation of these approaches, which assessed their ability to detect multiple types of attacks, was performed on seven structured datasets. Both approaches were shown to outperform five state-of-the-art adversarial detection methods, achieving near-perfect detection rates in most cases. This work pioneers the integration of multi-view learning, explainable AI (XAI), and transfer learning for adversarial detection in tabular data, providing a novel and highly effective paradigm for securing ML models in real-world applications.},
  archive      = {J_ASOC},
  author       = {Zahi Kapri and Edita Grolman and Ikuya Morikawa and Toshiya Shimizu and Yuval Elovici and Asaf Shabtai},
  doi          = {10.1016/j.asoc.2025.113288},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113288},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using image conversion techniques to detect adversarial examples in machine learning models for tabular data},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive feature mixing with vision transformers for clinical image analysis. <em>ASOC</em>, <em>181</em>, 113259. (<a href='https://doi.org/10.1016/j.asoc.2025.113259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vision Transformer (ViT) is an adaptation of the Transformer architecture that shows promise in image classification. However, limited training samples and the complex attributes of such images hinder its performance in identifying medical conditions from clinical images. To address this challenge, we propose a modified ViT architecture called ReMixViT by incorporating an efficient MLP-Mixer layer and reordering the residual blocks within the encoder block. This modification improves feature mixing and enhances the model’s generalization ability. We enhanced ReMixViT by incorporating an efficient MLP-Mixer layer. Additionally, we design two hybrid architectures, Res-ReMixViT and Res-ReMixViT+, by integrating a Convolutional Neural Network (ResNet50) and ReMixViT encoder blocks, considering feature maps of single and multiple scales, respectively. We evaluated the proposed architectures using six diverse medical imaging datasets with varying modalities and medical conditions. Our comparative study reveals that the ReMixViT and hybrid models outperform the vanilla ViT models and hybrid models with ViT encoder blocks, respectively, based on widely accepted performance measures. Specifically, we observe improvements of 4.62% and 3.08% in the F1-score performance metric. Moreover, when combined with data augmentation algorithms, the proposed hybrid architectures surpass other state-of-the-art hybrid networks. In addition to performance evaluation, we provide visual explanations through attention maps and the gradient flow of our model. These visual explanations contribute to the interpretability of the Artificial Intelligence (AI) system, assisting medical practitioners in drawing inferences from an explainable AI perspective. Moreover, an extended study demonstrates that the proposed modifications can be successfully adapted to other vision transformer architectures, resulting in enhanced performance.},
  archive      = {J_ASOC},
  author       = {Susmita Ghosh and Swagatam Das},
  doi          = {10.1016/j.asoc.2025.113259},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {113259},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive feature mixing with vision transformers for clinical image analysis},
  volume       = {181},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimized classifier chains‐based deep learning framework for inter-turn fault diagnosis in permanent magnet synchronous motors. <em>ASOC</em>, <em>180</em>, 113482. (<a href='https://doi.org/10.1016/j.asoc.2025.113482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inter-Turn Faults (ITF) of Permanent Magnet Synchronous Motor (PMSM) pose a major challenge. Early detection of these faults improves PMSM performance for predictive maintenance, preventing performance drops and reducing maintenance costs. This paper introduces a new model for the automatic detection of ITF, utilizing an optimized convolutional neural network (CNN). The proposed model incorporates convolutional layers for feature extraction, normalization layers to achieve better convergence, dropout layers to avoid overfitting, and bi-long short-term memory layers (LSTM) to preserve temporal dependencies. The LSTM layers of CNN aid in time series data analysis. Furthermore, Bayesian optimization is used to automatically select and optimize the CNN model’s parameters and improve its performance. This system has several outputs to identify the fault types and their exact location. The classifier chain technique is utilized to maintain independence between different outputs, thereby increasing the system’s accuracy and efficiency. The data used in this study includes the phase currents of the PMSM in healthy and faulty conditions with different intensities. Our proposed model is designed as a multi-output system and can detect both the fault type, such as the fault from phase A to ABC, and the fault locations in three phases, ranging from 10 % to 90 %. Additionally, this model’s performance, along with other models considered for comparison, has been evaluated using various criteria such as accuracy and F1-score to testify to the effectiveness of the proposed method. The results indicate that the proposed optimized CNN model can automatically detect stator ITFs with an accuracy higher than 95 %.},
  archive      = {J_ASOC},
  author       = {Amir Hossein Baharvand and Sina Hossein Beigi Fard and Amir Hossein Poursaeed and Meysam Doostizadeh},
  doi          = {10.1016/j.asoc.2025.113482},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113482},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimized classifier chains‐based deep learning framework for inter-turn fault diagnosis in permanent magnet synchronous motors},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in EEG-based emotion recognition: Challenges, methodologies, and future directions. <em>ASOC</em>, <em>180</em>, 113478. (<a href='https://doi.org/10.1016/j.asoc.2025.113478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition plays a pivotal role in affective computing and human-computer interaction, especially in the fields of mental health care, auxiliary medicine, and intelligent system design. As a non-invasive and time-sensitive neural signal, electroencephalogram (EEG) has become an important means of emotion recognition research. However, due to its susceptibility to noise and individual differences, EEG-based emotion recognition still faces major challenges. This review systematically summarizes the latest progress in EEG-based emotion recognition, sorts out the research paradigm of EEG-based emotion recognition, including public datasets, signal preprocessing techniques, feature extraction methods, and recognition models, and focuses on the end-to-end modeling advantages of deep learning methods in this field in recent years. Through a comparative analysis of representative literature, this study concludes that although deep learning models have promoted the development of this field, their generalization ability, interpretability, and applicability in real-world scenarios are still limited. In addition, current EEG datasets are often limited by small sample size, lack of diversity, and inconsistent labeling standards. In summary, future research should focus on cross-subject recognition techniques, small sample learning strategies, and the development of real-time, deployable emotion recognition systems. These directions are expected to bridge the gap between academic research and practical applications and further promote the advancement of EEG-based emotion recognition technology.},
  archive      = {J_ASOC},
  author       = {Jichi Chen and Yuguo Cui and Chunfeng Wei and Kemal Polat and Fayadh Alenezi},
  doi          = {10.1016/j.asoc.2025.113478},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113478},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advances in EEG-based emotion recognition: Challenges, methodologies, and future directions},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel wind power interval prediction method based on neural ensemble search and dynamic conformalized quantile regression. <em>ASOC</em>, <em>180</em>, 113476. (<a href='https://doi.org/10.1016/j.asoc.2025.113476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind power interval prediction plays a crucial role in delivering accurate estimations of the potential range of wind power generation, enhancing the stability and reliability of the power system. In this study, a novel method named Conformalized Quantile Regression with Neural Ensemble Search (NESCQR) for wind power interval prediction is proposed. The NESCQR algorithm combines Neural Ensemble Search (NES) and dynamic conformalized quantile regression. The NES employs a forward selection strategy to identify an optimal subset of models, aiming to minimize prediction errors and consequently produce tighter prediction intervals (PIs). Meanwhile, the dynamic conformalization process allows the model to effectively adapt to temporal variations in the data, significantly improving its robustness. Experiments on four real datasets show that the proposed NESCQR algorithm can obtain extremely narrow prediction intervals while ensuring valid coverage rate, and effectively alleviate the quantile crossing phenomenon, providing reliable and effective help for decision-makers.},
  archive      = {J_ASOC},
  author       = {Jianming Hu and Yuwen Deng and Jinxing Che},
  doi          = {10.1016/j.asoc.2025.113476},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113476},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel wind power interval prediction method based on neural ensemble search and dynamic conformalized quantile regression},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent decision support system for multi-objective 3D container loading using genetic algorithm combined with artificial bee colony. <em>ASOC</em>, <em>180</em>, 113473. (<a href='https://doi.org/10.1016/j.asoc.2025.113473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient container loading is a complex and critical logistics challenge, especially when dealing with strongly heterogeneous boxes in three dimensions. This study proposes an intelligent decision support system that addresses the 3D Single Container Loading Problem (3D-SCLP) using a hybrid meta-heuristic approach combining Genetic Algorithm (GA) and Artificial Bee Colony (ABC). The system introduces rotation constraints as a decision variable and optimizes for two objectives: maximizing profit and minimizing unused space. A mathematical model based on the bottom-left fill (BLF) method was developed to ensure feasible loading with non-overlapping placements and valid rotations. Experimental results on 15 real-world and 225 synthetic test cases demonstrate the superiority of the proposed GA+ABC method over standalone algorithms in both solution quality and robustness. The system achieves the lowest hypervolume metric (119.28), indicating better convergence to Pareto-optimal fronts, and provides practical feasibility for real-world logistics optimization.},
  archive      = {J_ASOC},
  author       = {Suriya Phongmoo and Komgrit Leksakul and Chaichana Suedumrong and Chakkrapong Kuensaen},
  doi          = {10.1016/j.asoc.2025.113473},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113473},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent decision support system for multi-objective 3D container loading using genetic algorithm combined with artificial bee colony},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fully interpretable convolutional neural network for intelligent fault diagnosis of rotating machinery. <em>ASOC</em>, <em>180</em>, 113472. (<a href='https://doi.org/10.1016/j.asoc.2025.113472'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning possesses powerful capabilities of extracting high-dimensional data features for fault diagnosis. However, the absence of interpretability of deep learning-based fault diagnosis has impeded progress in industrial applications, stemming from its black-box characteristics. Recently, relevant research primarily focuses on local and post hoc interpretability, highlighting a pressing need for a fully interpretable method. To address this issue, a fully interpretable convolutional neural network (FICNN) is proposed in this paper for intelligent fault diagnosis of rotating machinery. This FICNN consists of interpretable signal processing layer, interpretable feature extraction layer, and interpretable classification layer, which enables users to grasp its design principles and classification process. In this FICNN, the interpretable signal processing layer driven by wavelet convolutional with clear physical meanings, is specifically designed to extract fault related impact components from mechanical vibration signals. The interpretable feature extraction layer explains the intrinsic physical meaning of traditional convolution using sparse representation derived from traditional fault characteristic monitoring methods. Its interpretability holds profound importance for the practical applications of numerous existing convolutional neural networks. In addition, the interpretable classification layer visualizes the model results with attention weight and structure layer features to help users trust the diagnosis results. Finally, the effectiveness of the proposed method is verified by rotating machinery fault diagnosis experiments, which has superior performance and realizes highly accurate fault diagnosis, as compared to existing CNN-based fault diagnostic methods.},
  archive      = {J_ASOC},
  author       = {Tianhong Gao and Haiping Zhu and Jun Wu and Quan Yuan and Zirui Li},
  doi          = {10.1016/j.asoc.2025.113472},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113472},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fully interpretable convolutional neural network for intelligent fault diagnosis of rotating machinery},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ClipCap+ +: An efficient image captioning approach via image encoder optimization and LLM fine-tuning. <em>ASOC</em>, <em>180</em>, 113469. (<a href='https://doi.org/10.1016/j.asoc.2025.113469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ClipCap (CLIP prefix for image captioning), a leading image captioning model, exhibits limitations in recognizing images within specific domains. This study presents ClipCap+ +, an enhanced version of ClipCap that integrates key-value pair and residual connection modules. The key-value pair module implements a few-shot learning strategy by incorporating domain-specific knowledge, thereby improving the model's capability to recognize specialized image categories. The residual connection module optimizes the weight distribution between the pre-trained model and the key-value pair module, enhancing the model's transfer learning performance. During the inference phase, the model processes an input image through a multi-stage pipeline: (1) the visual encoder extracts image features to generate a hard visual prompt, (2) the key-value pair module dynamically constructs a domain-specific soft prompt, and (3) these complementary prompts are jointly fed into the large language model to synthesize the final image description. Extensive experiments on in-domain, near-domain, and cross-domain tasks show ClipCap+ + surpasses state-of-the-art models in accuracy, training efficiency, and generalization.},
  archive      = {J_ASOC},
  author       = {Ruiqin Wang and Ye Wu and Zhenzhen Sheng},
  doi          = {10.1016/j.asoc.2025.113469},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113469},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ClipCap+ +: An efficient image captioning approach via image encoder optimization and LLM fine-tuning},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heuristic information-rich evolutionary modelling for engine soft sensors of hybrid electric vehicles. <em>ASOC</em>, <em>180</em>, 113468. (<a href='https://doi.org/10.1016/j.asoc.2025.113468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the explosive demand of the electrified powertrain market, modelling schemes with strong robustness, low cost, and fast implementation are urgently required for hybrid vehicle engine development. This paper presents a data-driven holistic solution integrated with heuristic information-rich feature selection for engine soft sensors, i.e., fuel consumption, thermal efficiency, and volumetric efficiency, namely heuristic information-rich warm-start evolutionary modelling framework. Five filter methods are developed as heaters, and their selected features are converted to warm up the initialisation process in the evolutionary modelling, alleviating the inefficient exploration and local optimal problems caused by the pseudo-random initialisation of a single wrapper during the optimisation process. Meanwhile, a new factor of heuristic information richness is introduced to determine and adjust the proportion of the filter particles, further accelerate evolutionary convergence through the filter information guidance and avoid local optimality through free exploration of the particles without filter information, achieving a balance between computational efficiency and global search capability. Validated by the testing bench of a BYD 1.5 L naturally aspirated engine specially made for a hybrid powertrain, the Lasso method is the best heater and helps the proposed framework to reduce up to 54.9 % of mean squared error compared to that of the cold-start one. Compared to industry-used modelling frameworks, the proposed one achieves the equivalent prediction performance while reducing the database size by up to 85 %.},
  archive      = {J_ASOC},
  author       = {Ji Li and Xu He and Quan Zhou and Carl Anthony and Bo Wang and Guoxiang Lu and Hongming Xu},
  doi          = {10.1016/j.asoc.2025.113468},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113468},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heuristic information-rich evolutionary modelling for engine soft sensors of hybrid electric vehicles},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel probabilistic wind power forecasting using an adaptive informer network. <em>ASOC</em>, <em>180</em>, 113460. (<a href='https://doi.org/10.1016/j.asoc.2025.113460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective and feasible wind power forecasting is critical to the resource allocation and safe control of the power system. Nevertheless, the volatility and randomness of wind speed changing leads to deviations in actual wind power output. Therefore, a multilevel probabilistic wind power forecasting strategy using an adaptive Informer network is developed. To separate the long-term trend and periodic fluctuation of the raw series, wind power is firstly decomposed into equal-length sequences of multilevel frequencies through the maximum discrete overlapping wavelet transform (MODWT). Simultaneously, a piecewise adaptive loss function and an activation function for large range are considered in a novel Informer network, and the inherent structure and nonlinear features at each frequency are extracted with two layers of encoders and one layer of decoders. Moreover, the ensemble batch prediction intervals (EnbPI) are exploited to extend the deterministic forecasting to probabilistic information. Ultimately, a historical dataset is applied from an offshore wind power system in Belgium to verify that the forecasting performance, and quantitative analysis shows that the model achieves a mean absolute error of 2.5 % and a root mean squared error of 3.8 %. The developed strategy handles the volatility and complexity of wind data, providing reliable support for real wind power plant.},
  archive      = {J_ASOC},
  author       = {Sen Xie and Yuyang Hua and Shan Lu and Xin Jin},
  doi          = {10.1016/j.asoc.2025.113460},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113460},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multilevel probabilistic wind power forecasting using an adaptive informer network},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bilayer segmentation-recombination network for accurate segmentation of overlapping c. elegans. <em>ASOC</em>, <em>180</em>, 113459. (<a href='https://doi.org/10.1016/j.asoc.2025.113459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caenorhabditis elegans ( C. elegans ) is an excellent model organism because of its short lifespan and high degree of homology with human genes, and it has been widely used in a variety of human health and disease models. However, the segmentation of C. elegans remains challenging due to the following reasons: 1) the activity trajectory of C. elegans is uncontrollable, and multiple nematodes often overlap, resulting in blurred boundaries of C. elegans . This makes it impossible to clearly study the life trajectory of a certain nematode; and 2) in the microscope images of overlapping C. elegans , the translucent tissues at the edges obscure each other, leading to inaccurate boundary segmentation. To solve these problems, a Bilayer Segmentation-Recombination Network (BR-Net) for the segmentation of C. elegans instances is proposed. The network consists of three parts: A Coarse Mask Segmentation Module (CMSM), a Bilayer Segmentation Module (BSM), and a Semantic Consistency Recombination Module (SCRM). The CMSM is used to extract the coarse mask, and we introduce a United Attention Module (UAM) in CMSM to make CMSM better aware of nematode instances. The Bilayer Segmentation Module (BSM) segments the aggregated C. elegans into overlapping and non-overlapping regions. This is followed by integration by the SCRM, where semantic consistency regularization is introduced to segment nematode instances more accurately. Finally, the effectiveness of the method is verified on the C. elegans dataset. The experimental results show that BR-Net exhibits good competitiveness and outperforms other recently proposed segmentation methods in processing C. elegans occlusion images.},
  archive      = {J_ASOC},
  author       = {Mengqian Ding and Jun Liu and Yang Luo and Jinshan Tang},
  doi          = {10.1016/j.asoc.2025.113459},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113459},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bilayer segmentation-recombination network for accurate segmentation of overlapping c. elegans},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep convolutional generative adversarial network accelerated optimization algorithm for parameter optimization of permanent magnet synchronous generator controllers. <em>ASOC</em>, <em>180</em>, 113458. (<a href='https://doi.org/10.1016/j.asoc.2025.113458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In permanent magnet synchronous generators (PMSG), optimized rotor-side controller (RSC) parameters improve the power coefficient. Aiming at the traditional intelligent optimization algorithms since the long optimization time and insufficient global search capability, this work proposes adaptive differential evolution variants with linear population size reduction (L-SHADE) for constrained optimization with Levy flights (COLSHADE) accelerated by using deep convolutional generative adversarial network (DCGAN). The DCGAN-COLSHADE converts the parameters of the PMSG controllers into pictures and utilizes the DCGAN alternative algorithmic iterative process to speed up the COLSHADE iterative process and accomplish a broader and deeper global optimization problem. The PMSG simulation results utilizing the maximum power point tracking strategy verify the DCGAN-COLSHADE can obtain globally optimal solutions and higher system stability. The fitness function value of DCGAN-COLSHADE is 3.96 % smaller than the comparison algorithm; the average computation time is 79.28 % less than the particle swarm optimization (PSO), 80.35 % less than the moth flame optimization (MFO), 80.75 % less than the whale optimization algorithm (WOA), 80.52 % less than gray wolf optimization (GWO) and 77.96 % less than COLSHADE. In addition, the results of rapid control prototype (RCP) hardware experiments validate the feasibility and effectiveness of the algorithm.},
  archive      = {J_ASOC},
  author       = {Linfei Yin and Haomiao Li and Yongzi Ye and Fang Gao},
  doi          = {10.1016/j.asoc.2025.113458},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113458},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep convolutional generative adversarial network accelerated optimization algorithm for parameter optimization of permanent magnet synchronous generator controllers},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emergency decision-making in public health emergencies: Integrating intuitionistic fuzzy preferences with knowledge-unit case-based reasoning. <em>ASOC</em>, <em>180</em>, 113451. (<a href='https://doi.org/10.1016/j.asoc.2025.113451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public health emergencies (PHEs) are characterized by complexity, urgency, and high impact, posing significant challenges for timely and effective decision-making. Traditional emergency management systems often fail to provide prompt and precise responses under such circumstances. To address these limitations, this paper proposes a novel three-stage emergency decision-making (EDM) framework that integrates Knowledge-Unit Case-Based Reasoning (KU-CBR), Intuitionistic Fuzzy Preference Relation (IFPR), and the Differential Evolution (DE) algorithm. In the proposed method, alternative response strategies are generated using KU-CBR; expert preferences are then evaluated and refined via IFPR; finally, the DE algorithm is applied to optimize plan adaptation. Empirical validation is conducted on 32 historical PHE cases from 11 ASEAN countries between 2020 and 2022. The experimental results demonstrate that the proposed approach significantly improves decision quality, adaptability, and practical applicability compared to conventional methods, providing a robust tool for enhancing emergency preparedness and public health crisis management.},
  archive      = {J_ASOC},
  author       = {Chaoyu Zheng and Zhaoqiang Zhong and Baiyu Wu and Xuan Zhao and Mu Yue and Benhong Peng},
  doi          = {10.1016/j.asoc.2025.113451},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113451},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Emergency decision-making in public health emergencies: Integrating intuitionistic fuzzy preferences with knowledge-unit case-based reasoning},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy optimization model based on chaotic quantum AHA for offshore wind farm operation and maintenance scheduling problem. <em>ASOC</em>, <em>180</em>, 113439. (<a href='https://doi.org/10.1016/j.asoc.2025.113439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the offshore wind farm operation and maintenance (O&M) scheduling problem to minimize the total cost and carbon dioxide emission. Firstly, minimize the total cost of O&M and carbon emission from O&M vessels. Secondly, the trapezoidal fuzzy number is used to represent the uncertainty of the O&M vessel speed, the vessel carbon emission factor, and the wind speed in the sea area around a specific wind turbine. Meanwhile, constraints are determined regarding the frequency of vessel visits the turbines that need to be repaired, O&M personnel and spare parts, and a new fuzzy model for O&M scheduling of offshore wind farms is constructed. A new chaotic quantum artificial hummingbird algorithm (CQAHA) is used to improve the searching performance. In addition, the feasible integerization algorithm (FIA) is developed for determining the number of visiting turbines (M- DNVT), and eventually, a new O&M scheduling optimization method for offshore wind farms is proposed. Subsequently, the feasibility and superiority of the constructed solution method are tested based on the data from offshore wind farms in southern China. The results demonstrate that the proposed model can achieve an optimal scheduling solution in terms of cost and environmental protection. In the optimal solution, the total O&M cost and carbon emissions are reduced by 8.5 % and 4.7 %, respectively. Furthermore, the improved algorithm is more efficient, with smaller mean and standard deviation compared to the benchmark algorithm, while also achieving faster convergence.},
  archive      = {J_ASOC},
  author       = {Ming-Wei Li and Yi-Zhang Lei and Zhong-Yi Yang and Hsin-Pou Huang and Wei-Chiang Hong},
  doi          = {10.1016/j.asoc.2025.113439},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113439},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy optimization model based on chaotic quantum AHA for offshore wind farm operation and maintenance scheduling problem},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-source heterogeneous knowledge injected prompt learning method for legal charge prediction. <em>ASOC</em>, <em>180</em>, 113438. (<a href='https://doi.org/10.1016/j.asoc.2025.113438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legal charge prediction, an essential task in legal AI, seeks to assign accurate charge labels to case descriptions, attracting significant recent interest. Existing methods primarily employ diverse neural network structures for modeling case descriptions directly, failing to effectively leverage multi-source external knowledge. Therefore, we propose a prompt learning framework-based method that simultaneously leverages multi-source heterogeneous external legal knowledge. Specifically, we match knowledge snippets in case descriptions via the legal knowledge base and encapsulate them into the input through a hard prompt template. Additionally, we retrieve legal articles related to the given case description through contrastive learning, and then obtain factual elements through a conversational Large Language Model (LLM). We fuse the embedding vectors of soft prompt tokens with the encoding vector of factual elements to achieve knowledge-enhanced model forward inference. Experimental results show that our method achieved state-of-the-art results on CAIL-2018, the largest legal charge prediction dataset, and our method has lower data dependency. Case studies also demonstrate our method’s strong interpretability.},
  archive      = {J_ASOC},
  author       = {Jingyun Sun and Chi Wei},
  doi          = {10.1016/j.asoc.2025.113438},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113438},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-source heterogeneous knowledge injected prompt learning method for legal charge prediction},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-learning whale optimization algorithm based on reinforcement learning for a dual-resource flexible job shop scheduling problem. <em>ASOC</em>, <em>180</em>, 113436. (<a href='https://doi.org/10.1016/j.asoc.2025.113436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key areas in which production systems researchers are working these days is to find advanced optimization algorithms to efficiently schedule activities in manufacturing systems, which requires more sophisticated models with increased computational complexity. Therefore, there has been growing interest in this subject to improve the performance of meta-heuristics by incorporating reinforcement learning approaches. This paper deals with a dual-resource flexible job shop scheduling (DRFJSS) problem, in which each operation requires two resources (i.e., reconfigurable machine tool (RMT) and worker) to be processed. A mixed-integer linear programming (MILP) model is formulated to minimize the makespan. Since the proposed model cannot optimally solve most medium-sized instances, a self-learning whale optimization algorithm (SLWOA) is developed to deal efficiently with such a difficult problem. In the proposed SLWOA, an agent is trained by the state–action–reward–state–action (SARSA) algorithm to balance exploration and exploitation. The results show that the SLWOA has a stronger global search ability and faster convergence speed than the original whale optimization algorithm.},
  archive      = {J_ASOC},
  author       = {Ehsan Manafi and Bruno Domenech and Reza Tavakkoli-Moghaddam and Matteo Ranaboldo},
  doi          = {10.1016/j.asoc.2025.113436},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113436},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-learning whale optimization algorithm based on reinforcement learning for a dual-resource flexible job shop scheduling problem},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A smart contract vulnerability line detection method based on graph neural network and fusion of multidimensional code representation. <em>ASOC</em>, <em>180</em>, 113435. (<a href='https://doi.org/10.1016/j.asoc.2025.113435'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the limitations of existing smart contract vulnerability detection methods—particularly incomplete semantic information and insufficient detection accuracy caused by reliance on single-code representations—this study proposes MCR-VD, an interpretable vulnerability detection method based on graph neural networks (GNNs). The innovation of MCR-VD lies in three key aspects. First, a code property graph (CPG) is constructed by integrating the abstract syntax tree (AST), control flow graph (CFG), and program dependency graph (PDG) of smart contracts. This approach combines syntactic, control-flow, and data-flow semantic information across multiple dimensions, overcoming the limitations of single-representation methods. Second, a graph transformation mechanism for Graph of Vulnerability Region Candidates (GVRCs) and a GNN model are designed. By leveraging graph attention mechanisms, MCR-VD achieves line-level granularity in vulnerability localization, significantly enhancing the interpretability of detection results. Third, extensive evaluations on three benchmark datasets—Smartbugs Curated, Solidifi-Benchmark, and Clean Smart Contracts—demonstrate that MCR-VD outperforms state-of-the-art methods in critical metrics, including an F1-score of 92.7 %, accuracy of 94.1 %, precision of 93.5 %, and recall of 91.9 %. Furthermore, the method requires only 2.3 s per contract on average, achieving a two-order-of-magnitude efficiency improvement compared to traditional symbolic execution tools. This work presents a novel solution for smart contract security detection that balances high precision with computational efficiency.},
  archive      = {J_ASOC},
  author       = {Xiong Huanliang and Wu Canghai and Chen JiaXin and Wang Yinglong and Zhong yulin},
  doi          = {10.1016/j.asoc.2025.113435},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113435},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A smart contract vulnerability line detection method based on graph neural network and fusion of multidimensional code representation},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active learning deep autoencoder model with importance sampling for reliability analysis. <em>ASOC</em>, <em>180</em>, 113428. (<a href='https://doi.org/10.1016/j.asoc.2025.113428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most crucial difficulties in reliability analysis is to acquire the precise estimation of failure probability using the least amount of executing numerical models possible due to the high cost of large-scale engineering simulations. Surrogate model is an effective technique for reducing the overall computational cost. However, the increasing complexity of modern engineering problems poses a challenge for the classic surrogate model approach, particularly for high-dimensional problems. To address this issue, the surrogate model employed in this study combines a deep autoencoder with Gaussian process, providing an effective solution for high-dimensional problems. In order to address unbalanced data when using the deep autoencoder approach, the importance sampling is utilized. To ensure the precision and effectiveness of the multi-deep autoencoder models, an ensemble strategy and learning function are proposed. The selected points are compressed into several central points using the K-means method, further enhancing computational efficiency. Additionally, including more candidate points in the design of experiments results in a gradual increase in precision for the proposed model. Finally, the efficiency of the proposed algorithm is evaluated by a number of numerical examples.},
  archive      = {J_ASOC},
  author       = {Fan Yang and Qiangrong Xu and Feng Chen},
  doi          = {10.1016/j.asoc.2025.113428},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113428},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Active learning deep autoencoder model with importance sampling for reliability analysis},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial–temporal graph transformer network for traffic network flow prediction using parallel training based on cloud computing. <em>ASOC</em>, <em>180</em>, 113422. (<a href='https://doi.org/10.1016/j.asoc.2025.113422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient traffic network flow prediction remains a challenging issue, particularly when modeling dynamic spatial–temporal associations arising from complex road network topologies. In this study, we propose a spatial–temporal graph transformer network (STGTN) for traffic network flow prediction to improve forecasting accuracy. In the temporal dimension, a temporal multi-head self-attention (TMSA) mechanism is designed to capture causal correlations and evolution trends of traffic network flow over multiple time steps. In the spatial dimension, a spatial multi-head self-attention (SMSA) mechanism is utilized to extract dynamic spatial dependence of traffic network flow across multiple road segments. Additionally, a temporal multi-head cross-attention (TMCA) mechanism is established to convert encoded historical features into future representations using spatial–temporal embedding information. Further, a parallel training approach is developed for the STGTN model to increase learning efficiency in confronting traffic big data. The theoretical calculation formulations for parallel training are derived from the dataset decomposition mechanism and adaptive gradient descent criteria. The implementation algorithm for parallel training is deployed on a cloud computing platform. Experiments using two realistic road network traffic datasets demonstrate that the proposed STGTN consistently outperforms state-of-the-art baselines, with reductions ranging from 8.71% to 15.54% in MAE, 5.53% to 11.13% in RMSE, and 12.31% to 21.67% in MAPE for traffic network flow prediction. The proposed parallel training method can enhance the training efficiency and extract spatial–temporal features from parallel learning about the distributed data subsets.},
  archive      = {J_ASOC},
  author       = {Yongnan Zhang and Sirui Peng and Yonghua Zhou},
  doi          = {10.1016/j.asoc.2025.113422},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113422},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spatial–temporal graph transformer network for traffic network flow prediction using parallel training based on cloud computing},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated semi-supervised learning with contrastive representations against noisy labels. <em>ASOC</em>, <em>180</em>, 113421. (<a href='https://doi.org/10.1016/j.asoc.2025.113421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated semi-supervised learning presents a pragmatic scenario wherein a centralized model is trained utilizing a server with access to labeled data, while participating clients lack any labeled data. In this context, the inaccuracy of real-world labels on the server available for training poses a huge challenge to the federated semi-supervised learning. These inaccuracies can have a detrimental impact on the overall performance of the system and impose limitations on its use. In this paper, we propose a novel Federated Semi-supervised learning framework with Contrastive Representations, called FedCR, with the aim of addressing the aforementioned ubiquitous problems in the field of image classification tasks. Firstly, our approach employs contrastive representation learning to build memory representations of images, which can learn an image’s general features from an augmented view without relying on negative pairs and prevent the model from memorizing noise. Then we take a cautious approach during model updates to prevent any potential leakage to ensure the privacy and security of the clients’ information. Additionally, for the sake of improving robustness of the model, a contrastive regularization function is applied to preserve information connected to true labels while filtering out information associated with wrong labels. Furthermore, we mitigate the negative impact of mislabeled data during supervised learning by utilizing an improved cross-entropy loss function. Extensive experiments on prevalent datasets for image classification tasks show that the proposed method surpasses previously established state-of-the-art federated semi-supervised learning algorithms and efficiently alleviates the issue of model over-fitting to erroneous labels, especially when label noise is present.},
  archive      = {J_ASOC},
  author       = {Wenjie Mao and Bin Yu and Yihan Lv and Yu Xie and Chen Zhang},
  doi          = {10.1016/j.asoc.2025.113421},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113421},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated semi-supervised learning with contrastive representations against noisy labels},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming security in internet of medical things with advanced deep learning-based intrusion detection frameworks. <em>ASOC</em>, <em>180</em>, 113420. (<a href='https://doi.org/10.1016/j.asoc.2025.113420'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) revolutionizes industries like healthcare, agriculture, smart cities, and weather forecasting by enabling vast device networks to interact and transmit information. However, traditional network security methods are inadequate for IoT due to the limited storage and processing power of IoT devices. As IoT systems encounter numerous cyber threats, it is essential to develop innovative security approaches. Intrusion detection systems (IDS) are a popular mechanism for identifying and preventing systems from attacks. Therefore, this paper explores integrating deep learning (DL) models into IDS frameworks to enhance their capabilities. Deep learning is a subset of Artificial Intelligence (AI) that can extract complex patterns from data, adapt to new threats, and provide high accuracy and real-time detection. To provide the solution, this research presents novel deep learning models, including Embed-Net (Categorical Embedding Neural Network), the collaborative ConvNet-SVM model (combination of Convolutional Neural Network (CNN) and Support Vector Machine (SVM)), and the DeepSVM-Net (Deep Neural Network emulated by SVM), which are designed to detect network-based attacks on the Internet of Medical Things (IoMT). These models are evaluated using real-time benchmark datasets: ECU-IoHT, NF-BoT-IoT, and Wustl-HDRL-2024 by applying advanced preprocessing techniques that demonstrate superior performance by achieving an accuracy of 0.9990, 0.9959, and 0.9987with the lowest FNR of 0.0001, 0.0059, and 0.0014 by all three proposed models as compared to traditional methods. Furthermore, this paper conducted an ablation study which shows that our models have achieved outstanding performance with minimum training and validation losses of 0.0034 and 0.0008 for EmbedNet, 0.0018 and 0.0008 for ConvNet-SVM, and 0.0044 and 0.0016 for DeepSVM-Net. This research has enhanced IDS effectiveness, accuracy, and resilience by contributing significantly to cybersecurity, with the proposed models showing a remarkable improvement percentage ranging from 3.5 % to 7.5 % over the state-of-the-art.},
  archive      = {J_ASOC},
  author       = {Nikhil Sharma and Prashant Giridhar Shambharkar},
  doi          = {10.1016/j.asoc.2025.113420},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113420},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transforming security in internet of medical things with advanced deep learning-based intrusion detection frameworks},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solar power generation prediction using radial basis function neural network with mode decomposition and cooperation search algorithm. <em>ASOC</em>, <em>180</em>, 113418. (<a href='https://doi.org/10.1016/j.asoc.2025.113418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solar power is recognized as a crucial renewable energy source for mitigating environmental pollutants stemming from fossil fuels. However, its intermittent and random nature introduces instability into solar power generation, thereby increasing operational risks and posing threats to the safety of electric power system. To enhance prediction accuracy, this study develops a modified radial basis function neural network model for forecasting solar power generation. Initially, adaptive chirp mode decomposition is employed to identify multiple subcomponents with varying resolutions. Subsequently, an effective metaheuristic optimizer, the cooperation search algorithm, is utilized to fine-tune the computational parameters of the neural network for each identified subcomponent. Finally, the simulated outputs of each model are combined linearly to generate the final predictive outcomes. The feasibility of the proposed model is validated using solar power generation data obtained from a provincial power system in China. The simulations demonstrate that the developed model achieves superior prediction accuracy compared to several traditional forecasting models across different cases. Therefore, integrating evolutionary algorithm and mode decomposition proves effective in enhancing the performance of machine learning models for predicting renewable energy power generation.},
  archive      = {J_ASOC},
  author       = {Zhong-kai Feng and Pan Liu and Wen-jing Niu},
  doi          = {10.1016/j.asoc.2025.113418},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113418},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solar power generation prediction using radial basis function neural network with mode decomposition and cooperation search algorithm},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An uncertainty weighted kriging-random forest model for real-time ground property prediction during earth pressure balance shield tunneling. <em>ASOC</em>, <em>180</em>, 113417. (<a href='https://doi.org/10.1016/j.asoc.2025.113417'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An uncertainty weighted kriging - random forest model (KRF) is developed for real-time ground property prediction during earth pressure balance (EPB) shield tunneling by integrating Kriging extrapolation and random forest, which can guide shield operating parameter selection thereby mitigate construction risk. The proposed KRF algorithm synergizes two types of information: prior information and real-time information. The previously predicted ground properties with EPB operating parameters are extrapolated via the Kriging algorithm to provide prior information for the prediction of currently being excavated ground properties. The real-time information refers to the real-time operating parameters of the EPB shield, which are input into random forest to provide a real-time prediction of ground properties. The integration of these two predictions is achieved by assigning weights to each prediction according to their uncertainties, ensuring the prediction of KRF with minimum uncertainty. The performance of the KRF algorithm is assessed via a case study of the Changsha Metro Line 4 project. It reveals that the proposed KRF algorithm can predict ground properties with an accuracy of 93 %, overperforming the existing algorithms of LightGBM, AdaBoost-CART, DNN, SVC, and XGBoost by 29 %, 8 %, 12 %, 15 %, and 4 %, respectively. Another dataset from Shenzhen Metro Line 13 project is utilized to further evaluate the model’s generalization performance, revealing that the model can transfer its learned knowledge from one region to another with an accuracy of 89 %.},
  archive      = {J_ASOC},
  author       = {Ziheng Geng and Chao Zhang and Yuhao Ren and Minxiang Zhu and Renpeng Chen and Hongzhan Cheng},
  doi          = {10.1016/j.asoc.2025.113417},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113417},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An uncertainty weighted kriging-random forest model for real-time ground property prediction during earth pressure balance shield tunneling},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved Z-number based bayesian network modelling to predict cyber-attack risk for maritime autonomous surface ship (MASS). <em>ASOC</em>, <em>180</em>, 113416. (<a href='https://doi.org/10.1016/j.asoc.2025.113416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maritime Autonomous Surface Ship (MASS) will represent a transformative shift in the maritime industry, promising enhanced efficiency, and sustainability in transportation. It has advanced technologies, including artificial intelligence and sensor systems, to sail at sea without ship crew on board. Developed technology may face cyber threats as in many other areas. This situation may jeopardise not only operational continuity but also international maritime safety and the sustainability of global trade. Although some significant studies have explored cyber risks in ship navigation systems, they often lack a detailed probabilistic risk assessment tailored to fully autonomous vessels. Unlike previous studies, this research conducts a comprehensive probabilistic risk assessment focusing explicitly on cyber threats during the navigation of fully autonomous vessels. To do this, robust modelling including the Bayesian network (BN) is adopted under the improved Z-numbers theory. In the modelling, the BN is a significant tool capable of representing the cause-and-effect network between the variables, while the improved Z-numbers enable tackling uncertainties and enhancing the reliability of expert judgments. The findings of the research reveal that the occurrence probability of cyber-attack risk for MASS (degree 4- fully autonomous ship) is 7.15E-02 during navigation at open sea. Besides its robust theoretical background, the outcomes of research provide significant contributions to potential autonomous ship operators, MASS operators, ship inspectors, designers, maritime regulatory bodies and maritime security researchers for understanding and mitigating the potential cyber-attack threats and risk.},
  archive      = {J_ASOC},
  author       = {Muhammet Aydin and Sukru Ilke Sezer and Emre Akyuz and Paolo Gardoni},
  doi          = {10.1016/j.asoc.2025.113416},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113416},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improved Z-number based bayesian network modelling to predict cyber-attack risk for maritime autonomous surface ship (MASS)},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-data co-driven U-net segmentation network for multimodal lung tumor images. <em>ASOC</em>, <em>180</em>, 113410. (<a href='https://doi.org/10.1016/j.asoc.2025.113410'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are some challenges in multimodal medical image segmentation. Based on this, the Model-Data Co-driven U-Net Segmentation Network for Multimodal Lung Tumor Images is proposed. About “How to extract edge features in CT image?”, Edge-Driven U-Net is designed, which includes CT Data Stream Edge-Driven Module. The model ability to perceive lesion edge features is improved. About “How to extract position features in PET image?”, Position-Driven U-Net is designed, which includes PET Data Stream Position-Driven Module. The model ability to perceive lesion position features is improved. About “How to extract content features in PET/CT image?”, Content-Driven U-Net is designed, which includes PET/CT Data Stream Content-Driven Module. Under the guidance of semantic information in deep features, the model ability to perceive lesion content features is improved. About “How to fuse image features of PET/CT, PET and CT images?”, Model-Data Co-driven Module is designed. The main work of this model are as following: Firstly, the Shallow Model-Data Co-driven Module is designed, which realizes the interactive learning of different model data streams. Secondly, the Deep Model-Data Co-driven Module is designed, which realizes the interactive guide to learn position, edge and content features. The effectiveness of Model-Data Co-driven U-Net Segmentation Network for Multimodal Lung Tumor Images is validated on a clinical multimodal lung medical image dataset. The results for Miou, Dice, Voe, Rvd, Acc, and Recall are 91.60 %，95.94 %，95.92 %，95.73 %，97.92 and 95.28 % in lung lesion segmentation. The method proposed in this paper has positive significance for computer-aided diagnosis.},
  archive      = {J_ASOC},
  author       = {Tao Zhou and Pei Dang and Yunfeng Pan and Caiyue Peng and Qitao Liu and Huiling Lu and Fuyuan Hu},
  doi          = {10.1016/j.asoc.2025.113410},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113410},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Model-data co-driven U-net segmentation network for multimodal lung tumor images},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing soil moisture prediction with explainable AI: Integrating IoT and multi-sensor remote sensing data through soft computing. <em>ASOC</em>, <em>180</em>, 113406. (<a href='https://doi.org/10.1016/j.asoc.2025.113406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soil moisture (SM) is a crucial variable for comprehending various ecosystem processes on Earth. However, the collection of SM and other field data is often burdensome and fails to accurately represent local spatial variability. In order to address this contemporary challenge, a framework has been developed to predict SM content at a spatial scale by amalgamating Internet of Things (IoT) and geospatial computing approaches. Additionally, this study aimed to investigate the behaviour and contribution of explanatory variables in predicting SM content using explainable AI (XAI). Field data acquired through sensors and IoT, along with proxy variables derived from Landsat 9, Sentinel-1A data, and terrain data were utilized to develop models at a spatial scale. The Boruta algorithm was employed as a feature selection method, followed by the use of three ensemble machine learning models (random forest, bagging, and stacking) and a convolutional neural network (CNN) model to predict soil moisture based on the selected parameters. Statistical performance indicators such as mean square error (MSE), root mean square error (RMSE), mean absolute error (MAE), and correlation coefficient (R 2 ) were used to validate all the models. Furthermore, XAI based SHapley Additive exPlanations (SHAP) and Local Interpretable Model Explanation (LIME) method is used for global and local site-specific sensitivity analysis. The validation results indicate that the stacking model outperforms all other models, yielding an MSE of 0.330, RMSE of 0.574, MAE of 0.233, and R 2 of 0.996. The spatial map of SM content reveals a range varying between 6.45 % and 68.27 %. The outputs of XAI models highlights that humidity and atmospheric temperature have the highest influence on predicting SM content both globally and locally. Overall, the proposed framework holds promise as an alternative to traditional approaches for estimating surface SM content. The obtained spatial map can prove useful in sustainable water resources management at the field scale.},
  archive      = {J_ASOC},
  author       = {Santanu Mallik and Abhigyan Chakraborty and Krishanu Podder and Swapan Talukdar and Atiqur Rahman and Umesh Mishra},
  doi          = {10.1016/j.asoc.2025.113406},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113406},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing soil moisture prediction with explainable AI: Integrating IoT and multi-sensor remote sensing data through soft computing},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault diagnosis of helicopter accessory gearbox under multiple operating conditions based on feature mode decomposition and multi-scale convolutional neural networks. <em>ASOC</em>, <em>180</em>, 113403. (<a href='https://doi.org/10.1016/j.asoc.2025.113403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expanding global civil helicopter market has intensified the need for advanced fault diagnosis techniques in helicopter engines. Traditional fault classification methods, such as Variational Mode Decomposition (VMD), have limitations in decomposing complex signals and separating different signal components, which can impede accurate fault feature extraction and compromise diagnostic accuracy and reliability. This paper introduces a novel fault diagnosis method that combines Feature Mode Decomposition (FMD) with a Multi-Scale Convolutional Neural Network (MCNN) to address these challenges. The approach begins by collecting signals from helicopter accessory gearboxes under simulated ground operation conditions. The FMD technique is applied to decompose the gear vibration signals, and the decomposed signals from different sensors are reconstructed and normalized. This preprocessed data is then fed into the MCNN network at various scales, enabling simultaneous extraction and fusion of multi-scale features. The final fault classification is performed using a softmax classifier. Experimental results demonstrate the efficacy of the proposed method in extracting fault features. Under the given conditions, a fault diagnosis accuracy of up to 100 % was achieved for helicopter accessory gearboxes, marking a significant improvement of 3.1 % compared to VMD-based methods. This enhancement in accuracy represents a substantial advancement in aviation safety and reliability. The study showcases the superior performance of FMD in decomposing complex mechanical signals, particularly its ability better to capture both periodic and impulsive characteristics of fault signals. The integration of MCNN allows for more effective multi-sensor data processing, enhancing the model's capacity to detect and classify faults across various scales and conditions. The FMD-MCNN approach improves the accuracy and efficiency of fault diagnosis and demonstrates significant potential for practical application in aviation maintenance technology.},
  archive      = {J_ASOC},
  author       = {Anping Wan and Zengzhen Zhu and Khalil AL-Bukhaiti and Xiaomin Cheng and Xiaosheng Ji and Jinglin Wang and Tianmin Shan},
  doi          = {10.1016/j.asoc.2025.113403},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113403},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault diagnosis of helicopter accessory gearbox under multiple operating conditions based on feature mode decomposition and multi-scale convolutional neural networks},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning-based stochastic multi-objective optimizer for uncertain power system scheduling. <em>ASOC</em>, <em>180</em>, 113402. (<a href='https://doi.org/10.1016/j.asoc.2025.113402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power system scheduling with renewable energy sources poses significant challenges due to high computational complexity and uncertainty in operating conditions. Multi-period and multi-scenario modeling further escalates these issues, creating large-scale optimization problems that overwhelm traditional Stochastic Optimization Algorithm (SOA) with slow convergence and limited solution diversity. To tackle these challenges, we propose the Feature-Driven Multi-Objective Group Search Optimizer (FDMOGSO), a novel SOA for large-scale power systems scheduling. FDMOGSO employs the Self-Learning Method of Solution Space Feature (SLMSPF) to extract key features, reducing computational complexity by focusing exploration on promising regions. A Multi-Block Network (MBNet) classifier further enhances robustness by prioritizing high-quality solutions under uncertainty, while an enhanced Multi-Objective Group Search Optimizer (EMOGSO) adapts search strategies to improve convergence and solution diversity. Experimental results on IEEE 9-bus and IEEE 118-bus systems show that FDMOGSO significantly outperforms classical SOAs, including MOGSO, NSGA-II, MOPSO, and EMOGSO, on the Cumulative IGD Efficiency (CIGDE) metric, with improvements of 96.20%, 95.61%, 98.83%, and 94.68%, respectively. This demonstrates that FDMOGSO can find high-quality solutions for large-scale optimization problems with limited evaluations, enhancing the practical application potential of SOAs in complex power system scheduling.},
  archive      = {J_ASOC},
  author       = {B. Deng and M.S. Li and T.Y. Ji and Q.H. Wu},
  doi          = {10.1016/j.asoc.2025.113402},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113402},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning-based stochastic multi-objective optimizer for uncertain power system scheduling},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Economic emission dispatch of power systems considering uncertainty of wind-solar-hydro with fractional order multi-objective differential evolution. <em>ASOC</em>, <em>180</em>, 113391. (<a href='https://doi.org/10.1016/j.asoc.2025.113391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of renewable energy brings significant uncertainty to the operation of power systems. Multi-objective economic emission dispatch (MOEED) becomes an important way to reduce operating costs and emissions under the current focus on environmental protection. This paper establishes a MOEED model that incorporates uncertainty of wind, solar, and run-of-river small hydro. The underestimated and overestimated prediction scenarios of renewable energy are transformed into operating costs by spinning reserve cost and penalty cost for modeling. To effectively solve the model, a multi-objective differential evolution called FOMODE is proposed by using the fractional order idea. A mutation operation FOMODE/rand/1 based on discrete fractional order is designed to accelerate the convergence. Besides, a parameters adaptive method is developed to automatically adjust the control parameters of FOMODE according to the population evolution. Furthermore, multi-objective machining techniques are integrated for performance enhancement of FOMODE. The superiority of FOMODE is first verified on twenty CEC’2020 multi-objective optimization problems and benchmarked against ten peer methods. FOMODE is then applied to the MOEED of a tailored IEEE 30-bus system. The results indicate that FOMODE ranks first in 8 of 16 evaluation metrics, and achieves superior solutions with better economic and emission benefits.},
  archive      = {J_ASOC},
  author       = {Hanhao Guo and Guojiang Xiong and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.asoc.2025.113391},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113391},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Economic emission dispatch of power systems considering uncertainty of wind-solar-hydro with fractional order multi-objective differential evolution},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scalable monocular 3D detector with superpixel feature pyramid network. <em>ASOC</em>, <em>180</em>, 113389. (<a href='https://doi.org/10.1016/j.asoc.2025.113389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular 3D object detection plays a pivotal role in vehicle perception systems. Current methods frequently struggle to effectively extract scene-level semantic information, and the availability of monocular 3D detectors tailored to diverse embedded devices with varying computing power may still be limited. This paper introduces MonoYolo, a scalable detector designed for practicality and efficiency with varying resource constraints. In particular, we design a Superpixel Feature Pyramid Network (SFPN) that automatically groups pixels with similar attributes together. Experimental results on KITTI and nuScenes datasets showcase the advantageous performance of MonoYolo over superior monocular detectors for large models, while the lightweight model maintains real-time detection capabilities. Meanwhile, the proposed SFPN offers a seamless integration into existing image-only 3D detectors, presenting a plug-and-play solution for enhanced monocular 3D object detection performance.},
  archive      = {J_ASOC},
  author       = {Dongliang Ma and Fang Zhao and Ye Li and Xin Qu and Xin Jiang and Hao Wu and Xi Chen and Min Liu},
  doi          = {10.1016/j.asoc.2025.113389},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113389},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A scalable monocular 3D detector with superpixel feature pyramid network},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential evolution based on two-stage mutation strategy and multi-stage parameter control. <em>ASOC</em>, <em>180</em>, 113387. (<a href='https://doi.org/10.1016/j.asoc.2025.113387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Differential Evolution (DE) algorithm is an advanced evolutionary method for tackling global optimization challenges, yet designing effective parameter control generation methods and mutation strategies remains a significant challenge. In response, this paper introduces a differential evolution based on T wo- S tage Mutation Strategy and M ulti- S tage Parameter Control (TSMS-DE). Firstly, a multi-stage parameter control is proposed, in the early stage, a larger step size is used to enhance exploration, in the mid stage, the scaling factor is dynamically adjusted based on individual ranking, and in the late stage, a Cauchy distribution is applied to improve parameter adaptability. Secondly, an external archive optimization method utilizing a Two-Stage Mutation Strategy is developed to effectively eliminate individuals with suboptimal fitness values, ensuring the archive consistently retains high-quality individuals. Third, TSMS-DE employs an Opposite-Based Learning Strategy to generate sample points in the solution space, enabling more comprehensive coverage of the search space and enhancing overall search performance. We conducted comparative experiments on 100 benchmark test suites from the Congress on Evolutionary Computation (CEC) competitions, including CEC2013, CEC2014, CEC2017 and CEC2022. In order to rigorously evaluate the performance of the algorithms, statistical validation was carried out using a variety of tests. Compared to several advanced Differential Evolution variants and heuristic algorithms, the results demonstrate that our algorithm exhibits significant advantages in convergence, diversity, and accuracy.},
  archive      = {J_ASOC},
  author       = {Huarong Xu and Shengke Lin and Zhiyu Zhang and Qianwei Deng},
  doi          = {10.1016/j.asoc.2025.113387},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113387},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution based on two-stage mutation strategy and multi-stage parameter control},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rough membership c-means clustering algorithm based on composite granulation and mahalanobis distance. <em>ASOC</em>, <em>180</em>, 113385. (<a href='https://doi.org/10.1016/j.asoc.2025.113385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rough set-based c-means clustering methods has good performance for using the concepts of upper and lower approximation and granulation in the rough set theory and constructing a rough membership based on binary relation. However, the single binary relation only considers data correlation within a specific characteristic dimension. Furthermore, existing rough clustering methods employ the Euclidean distance as a similarity measure, which solely reflects discrepancies in feature values without accounting for inter-feature relationships and cannot measure the similarity of data with complex distributions. Addressing these challenges, this paper proposed a rough membership c-means clustering algorithm based on composite granulation and Mahalanobis distance(CMRMCM). First, a composite granulation model is constructed by combining neighborhood granulation and feature space granulation, which utilizes both neighborhood information and feature space information of the data. Second, the Mahalanobis distance based on global covariance matrix is proposed as a similarity measure among samples and between samples and class centers, which can effectively handle the correlation between data features while avoiding the Mahalanobis distance failure problem caused by the covariance matrix falling into the singular matrix. Finally, the proposed clustering algorithm is applied in color image segmentation based on the neighborhood distribution attributes inherent in color image data and the strong correlation among pixel features. Experimental results demonstrate the superior performance of the proposed algorithm compared to other state-of-the-art clustering algorithms on fuzzy boundary and unbalanced complex datasets.},
  archive      = {J_ASOC},
  author       = {Bo Lei and Yongjie Ma and Peiyan Guo and Songyi Shi and Yao Tang},
  doi          = {10.1016/j.asoc.2025.113385},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113385},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Rough membership c-means clustering algorithm based on composite granulation and mahalanobis distance},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated personalized decision method with q-rung orthopair fuzzy data for underground natural gas storage site decisions. <em>ASOC</em>, <em>180</em>, 113384. (<a href='https://doi.org/10.1016/j.asoc.2025.113384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location selection for underground natural gas storage is a multifaceted decision-making problem, as diverse factors are involved. Earlier studies on location selection for natural gas faced challenges such as uncertainty handling, methodical estimation of experts' reliability, capturing hesitation during factor significance calculation, and personalized location ordering. Therefore, the present work develops a novel integrated weighted aggregated sum product assessment (WASPAS) methodology with generalized (q-rung orthopair) fuzzy information, considering three dimensions of uncertainty: membership grade, hesitancy grade, and non-membership grade, with a flexible window allowing experts easy preference articulation. The reliability of experts is calculated using the Cronbach measure, and the importance of the criteria is computed based on the regret factor. A ranking algorithm is developed with a modified weighted aggregated sum product assessment formulation and choice vector to obtain personalized ordering of natural gas locations. The usefulness is illustrated using a case study of location selection for underground natural gas storage in India. Results show that, political acceptance is the most crucial indicator when selecting an optimal underground storage location for natural gas. The outcomes concluded that the introduced integrated framework (i) is robust, even after alterations are realized for the weights of the criteria and strategy values, (ii) produces rank orders that are consistent with the earlier models, and (iii) yields broader rank values, to support better discrimination of alternative locations and appropriate backup management compared to the extant model. Finally, the benefits, shortcomings, and implications are discussed. The model introduced can be a novel guide for natural gas location selection and can aid investors in planning their investments.},
  archive      = {J_ASOC},
  author       = {Raghunathan Krishankumar and Fatih Ecer and Pratibha Rani and Dragan Pamucar and Serhat Yüksel and Hasan Dinçer},
  doi          = {10.1016/j.asoc.2025.113384},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113384},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrated personalized decision method with q-rung orthopair fuzzy data for underground natural gas storage site decisions},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale feature fusion distilled attention network for efficient image super-resolution. <em>ASOC</em>, <em>180</em>, 113382. (<a href='https://doi.org/10.1016/j.asoc.2025.113382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient single image super-resolution (SISR) seeks to enhance reconstruction quality while minimizing computational complexity and model parameters for practical deployment on resource-constrained edge devices. A critical limitation of existing approaches lies in the restricted receptive fields of conventional convolutions, which hinder attention mechanisms and backbone networks from effectively capturing non-local features, ultimately degrading super-resolution performance. To solve this challenge, we propose a novel Multi-scale Feature Fusion Distilled Attention Network (MFFDAN) that synergizes multi-scale feature extraction with lightweight attention mechanisms. Our framework introduces three key innovations. First, we proposed a Multi-scale Convolution (MConv) module that dynamically extracts hierarchical features with multi-scale receptive fields while maintaining parameter efficiency, subsequently extended into a Multi-scale Feature Fusion Distillation Block (MFFDB). Second, we proposed a Distilled Spatial Attention (DSA) mechanism that enhances spatial feature interactions through multi-scale downsampling and distillation-based information refinement. Third, a lightweight Depth-wise Channel Attention (DCA) module optimized via depth-wise convolutions for efficient channel feature selection is proposed. These components are systematically integrated through residual connections to form the Multi-scale Feature Fusion Distilled Attention Block (MFFDAB), the core building block of our architecture. Extensive experiments demonstrate that MFFDAN achieves state-of-the-art performance while maintaining superior computational efficiency, achieving an optimal balance between reconstruction quality (PSNR/SSIM), model complexity (parameters/Multi-Adds), and inference speed across benchmark datasets. The proposed innovations provide a practical solution for deploying high-quality SISR models on edge computing platforms.},
  archive      = {J_ASOC},
  author       = {Yinggan Tang and Mengjie Su and Xiuli Zhang},
  doi          = {10.1016/j.asoc.2025.113382},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113382},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale feature fusion distilled attention network for efficient image super-resolution},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and privacy-preserving deep inference towards cloud–edge collaborative. <em>ASOC</em>, <em>180</em>, 113381. (<a href='https://doi.org/10.1016/j.asoc.2025.113381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud–edge collaborative inference approach splits deep neural networks (DNNs) into two parts to run collaboratively on resource-constrained edge devices(AIoT devices) and cloud servers, aiming at minimizing inference latency and protecting data privacy for AIoT computing system. However, despite not exposing the raw input data from edge devices directly to the cloud, state-of-the-art attacks can still target collaborative inference to reconstruct the raw private data from exposed local models’ intermediate outputs, introducing serious privacy risks. In this paper, we propose a secure privacy inference framework for cloud–edge collaboration system towards AIoT network, called CIS ( C ollaborative I nference S hield), which supports adaptively partitioning the network according to dynamically changing network bandwidth and fully releases the computational power of edge devices. To mitigate the influence introduced by private perturbation, CIS provides a way to achieve differential privacy protection by adding refined noise to the intermediate layer feature maps offloaded to the cloud. Meanwhile, given a total privacy budget, the budget is reasonably allocated by the size of the feature graph rank generated by different convolution filters, making cloud inference robust to the perturbed data, thus effectively trading-off between privacy and availability. Finally, we construct a real cloud–edge collaborative inference computing scenario to verify the effectiveness of inference latency and model partitioning on resource-constrained edge devices. Furthermore, the state-of-the-art cloud–edge collaborative reconstruction attack is utilized to evaluate the practical availability of the end-to-end privacy protection mechanism provided by CIS.},
  archive      = {J_ASOC},
  author       = {Yulong Wang and Guoxin Zhong and Yubing Duan and Yunchang Cheng and Mingyong Yin and Run Yang},
  doi          = {10.1016/j.asoc.2025.113381},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113381},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient and privacy-preserving deep inference towards cloud–edge collaborative},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional neural networks for construction safety: A technical review of computer vision applications. <em>ASOC</em>, <em>180</em>, 113374. (<a href='https://doi.org/10.1016/j.asoc.2025.113374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing number of construction-related accidents underscores the urgent need to enhance global construction safety (CS) management. Monitoring activities during the construction process is essential for effective CS management. Traditional computer vision (CV) techniques, which rely on handcrafted features and rule-based algorithms, often struggle to capture the complex dynamics of construction sites. In contrast, deep learning-based CV approaches, especially convolutional neural networks (CNNs), offer end-to-end solutions that overcome these limitations and are increasingly adopted in CS applications. This paper systematically categorizes the application of CNN-based CV technologies into four key stages: data collection, data preprocessing, model construction, and practical application. From a technical perspective, this paper provides a comprehensive review of relevant methods and tools at each stage. Initially, this paper analyzes the literature on CNN-based CV applications in CS via the Python library pyBibX combined with artificial intelligence (AI) tools for bibliometric analysis and effective visualization. This paper subsequently summarizes various data acquisition and preprocessing methods to save researchers time in data-related aspects. Furthermore, to provide researchers with a rapid understanding of existing methods, this paper presents various CV and CNN techniques, summarizing classical CV models based on CNNs through extensive literature, data, web, and competition searches, and compares their performance on public datasets. Finally, this paper provides a detailed analysis of CNN-based CV technologies used in CS, tailored to the technical aspects of various downstream tasks, revealing current applications of advanced technologies and research progress. Additionally, this paper discusses current research challenges, including technical and other aspects, and proposes several directions for future research.},
  archive      = {J_ASOC},
  author       = {Ruying Cai and Jingru Li and Yi Tan and Jingyuan Tang and Xiangsheng Chen},
  doi          = {10.1016/j.asoc.2025.113374},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113374},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Convolutional neural networks for construction safety: A technical review of computer vision applications},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable-granularity distance measures for interval-valued fuzzy sets. <em>ASOC</em>, <em>180</em>, 113373. (<a href='https://doi.org/10.1016/j.asoc.2025.113373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing interval-valued distance measures is an important topic in interval-valued fuzzy set theory. Some interval-valued distance measures have been proposed in the literature, but the granularity of these distance measures are different. Then, it is a difficult problem to choose which granularity of distance measure should be used in practical applications. To solve this problem, this paper proposes two methods for constructing distance measures of interval-valued fuzzy sets. One is to establish a continuous variable-granularity distance measure of interval values based on the intrinsic correlation between distance and area in a two-dimensional geometric space. Furthermore, a continuous variable-granularity distance measure for interval-valued fuzzy sets is constructed. The other is to construct a discrete interval-valued distance measure, which provides a way to refine the existing distance measures in the literature. This establishes a theoretical framework for selecting the appropriate granularity of distance measures. Data analysis shows that the variable-granularity distance measure proposed in this paper is effective and practical.},
  archive      = {J_ASOC},
  author       = {Zhaohao Wang and Yu Zhang},
  doi          = {10.1016/j.asoc.2025.113373},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113373},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Variable-granularity distance measures for interval-valued fuzzy sets},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFSMCG-net: A siamese change detection network based on differential feature selection and multi-scale guidance strategies. <em>ASOC</em>, <em>180</em>, 113372. (<a href='https://doi.org/10.1016/j.asoc.2025.113372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection technology effectively identifies surface changes but encounters significant challenges, including class imbalance between foreground and background and interference from pseudo-changes caused by factors such as illumination variations and geometric distortions. We propose a location-sensitive Differential Feature Selection and Multi-Scale Change Feature Guidance Network (DFSMCG-Net) to address these issues. The DFSMCG-Net introduces a Differential Feature Selection Module (DFSM) that leverages the spatial location information of bi-temporal features. This module captures spatiotemporal differential features at the exact location along the X-axis and Y-axis and integrates these features through cross-fusion to establish long-range pixel dependencies. The resulting multi-level differential features provide the network with a detailed temporal context for detecting changes. We develop a Multi-Scale Change Feature Guidance Module (MCFGM) based on a multi-head self-attention mechanism to further enhance the fusion of multi-level differential features and suppress interference from non-differential features. This module assigns each attention head a distinct non-overlapping window, dynamically adjusting window sizes according to the feature map dimensions. This approach facilitates the integration of multi-scale differential features, improving the network’s capacity to represent change-related features. Experimental results demonstrate that the proposed DFSMCG-Net performs significantly better than state-of-the-art methods on benchmark datasets, including LEVIR-CD, CDD, SYSU-CD and S2Looking. The model is particularly effective in mitigating pseudo-change phenomena under conditions of extreme class imbalance.},
  archive      = {J_ASOC},
  author       = {Hang Xue and Ke Liu and Caiyi Huang and Xianhong Meng},
  doi          = {10.1016/j.asoc.2025.113372},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113372},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DFSMCG-net: A siamese change detection network based on differential feature selection and multi-scale guidance strategies},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral image classification using uniform manifold approximation and projection with fusion deep learning network. <em>ASOC</em>, <em>180</em>, 113371. (<a href='https://doi.org/10.1016/j.asoc.2025.113371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSI) are crucial for remote sensing applications as they provide detailed spectral information that accurately identifies and analyzes various materials and land features. HSI classification faces challenges due to large data volumes and high dimensionality. Dimensionality reduction techniques help address these problems by simplifying model computation, reducing redundancy, and improving feature selection. However, traditional methods struggle to capture nonlinear structures and local–global relationships in hyperspectral data. We propose a new multi-feature fusion classification model called the Uniform Manifold Approximation and Projection (UMAP) and Simple Attention Module (SimAM) mechanism fusion network (UMAPSAMFN). The main workflow of the model consists of several steps. The network uses UMAP to map high-dimensional HSI data into a low-dimensional space while maintaining a local and global structure. The feature data are sent to the convolutional neural network (CNN) and graph convolutional network (GCN) modules to capture pixel-level features and contextual information. These data are separately processed through multi-head attention modules to enhance the ability to represent feature information. Finally, the processed data are jointly fed into a fusion module with an attention mechanism to boost feature information and achieve deep modeling results. The experimental results on three benchmark HSI datasets show that UMAPSAMFN consistently exhibits the highest classification accuracy, with overall classification accuracies of 93.28%, 96.13%, and 97.73% on the Indian Pines, Pavia University, and Salinas datasets, respectively.},
  archive      = {J_ASOC},
  author       = {Chen Lou and Mohammed A.A. Al-qaness and Sike Ni and Dalal Al-Alimi and Robertas Damaševičius and Saeed Hamood Alsamhi},
  doi          = {10.1016/j.asoc.2025.113371},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113371},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hyperspectral image classification using uniform manifold approximation and projection with fusion deep learning network},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLB-mamba: A vision mamba for closed and open-set student learning behavior detection. <em>ASOC</em>, <em>180</em>, 113369. (<a href='https://doi.org/10.1016/j.asoc.2025.113369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By effectively analyzing the learning behaviors of smart classroom students in the classroom, the interaction between teaching and learning can be significantly improved, thereby enhancing the quality of education. However, current traditional analysis of students’ classroom behavior mainly focuses on closed-set behavior detection in a single scenario. In the face of complex and open real classroom environments, obtaining meaningful behavior representations in small and densely populated complex scenarios while achieving good performance in both closed and open environments remains a major challenge. To address these challenges, this study introduces a new method called SLB-Mamba to detect students’ learning behaviors in both closed-set and open-set scenarios. The SLB-Mamba network offers high computational efficiency and flexibility in deployment and practical applications. Firstly, an Attention calculation method Reward-Weighted Attention (RWA) based on the concept of benefit value was designed to enhance the feature extraction ability of the backbone network. Additionally, the Vision State Space Feature Pyramid Network (VSSFPN) structure built through State Space Model (SSM) can effectively integrate cross-scale features. The effectiveness of SLB-Mamba has been validated through rigorous testing and evaluation on real classroom data of smart classrooms, and it has been compared with state-of-the-art (SOTA) methods. The experimental results show that SLB-Mamba achieved mean Average Precision (mAP) scores of 93.79% and 92.2% on the SLB-K12 and SCSB datasets, respectively, with the Absolute Open-Set Error (A-OSE) values of 163 and 289. These findings highlight the significant advantages of the proposed method in improving detection accuracy and efficiency in both closed-set and open-set scenarios, thereby extending the applicability of the educational assessment framework. The source code of this study is publicly available at https://github.com/CCNUZFW/SLB-Mamba .},
  archive      = {J_ASOC},
  author       = {Zhifeng Wang and Longlong Li and Chunyan Zeng and Shi Dong and Jianwen Sun},
  doi          = {10.1016/j.asoc.2025.113369},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113369},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SLB-mamba: A vision mamba for closed and open-set student learning behavior detection},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random-key algorithms for optimizing integrated operating room scheduling. <em>ASOC</em>, <em>180</em>, 113368. (<a href='https://doi.org/10.1016/j.asoc.2025.113368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient surgery room scheduling is essential for hospital efficiency, patient satisfaction, and resource utilization. This study addresses the challenge as a combinatorial optimization problem that incorporates multi-room scheduling, equipment scheduling, and complex availability constraints for rooms, patients, and surgeons, facilitating rescheduling and enhancing operational flexibility. To solve such a problem, we introduce multiple algorithms based on a Random-Key Optimizer (RKO), coupled with relaxed formulations to compute lower bounds efficiently, rigorously tested on literature and new, real-world-based instances. The RKO approach decouples the problem from the solving algorithms through an encoding/decoding layer, making it possible to use the same solving algorithms to multiple room scheduling problems case studies from multiple hospitals, given the particularities of each place, even other optimization problems. Among the possible RKO algorithms, we design the heuristics Biased Random-Key Genetic Algorithm with Q -Learning, Simulated Annealing, and Iterated Local Search for use within an RKO framework, employing a single decoder function. The proposed heuristics, complemented by the lower-bound formulations, provided optimal gaps for evaluating the effectiveness of the heuristic results. Our results demonstrate significant lower- and upper-bound improvements for the literature instances, notably in proving one optimal result. Our strong statistical analysis shows the effectiveness of our implemented heuristic search mechanisms. Furthermore, the best-proposed heuristic efficiently generates schedules for the newly introduced instances, even in highly constrained scenarios. This research offers valuable insights and practical solutions for improving surgery scheduling processes, delivering tangible benefits to hospitals by optimizing resource allocation, reducing patient wait times, and enhancing overall operational efficiency.},
  archive      = {J_ASOC},
  author       = {Bruno Salezze Vieira and Eduardo Machado Silva and Antônio Augusto Chaves},
  doi          = {10.1016/j.asoc.2025.113368},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113368},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Random-key algorithms for optimizing integrated operating room scheduling},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-population differential evolution algorithm with adaptive optimization dimensions for underwater glider motion planning. <em>ASOC</em>, <em>180</em>, 113366. (<a href='https://doi.org/10.1016/j.asoc.2025.113366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The underwater glider motion planning problem poses a significant optimization challenge due to its sensitivity to optimization dimensions. To address this, we propose a Multi-Population Differential Evolution algorithm with Adaptive Optimization Dimensions (AOD-MPDE). It features three heterogeneous subpopulations, each with different optimization dimensions, and employs a hybrid population initialization method to generate high-quality initial solutions. These subpopulations interact through a heterogeneous information interaction mechanism that enhances mutation strategies and facilitates the sharing of the best individual information. To determine the best optimization dimension, an optimization dimension co-evolution strategy is introduced, which incorporates both dimension-increasing and dimension-decreasing operations. This enables the algorithm to effectively adapt the optimization dimension to an optimal value. AOD-MPDE was evaluated across six mission scenarios and compared against seven advanced DE variants. Across these scenarios, AOD-MPDE achieved an average energy consumption reduction rate of 3.18%, demonstrating a notable advantage. Additionally, ablation studies and parameter sensitivity analyses were conducted. The experimental results validate the effectiveness of each proposed strategy and confirm the algorithm’s robustness to parameter settings.},
  archive      = {J_ASOC},
  author       = {Hao Hu and Tao Wang and Yongjian Zhou and Tonghao Wang and Xingguang Peng},
  doi          = {10.1016/j.asoc.2025.113366},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113366},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-population differential evolution algorithm with adaptive optimization dimensions for underwater glider motion planning},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic cooperative coevolution based on variable importance for non-separable large-scale global optimization. <em>ASOC</em>, <em>180</em>, 113363. (<a href='https://doi.org/10.1016/j.asoc.2025.113363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wide range of large-scale optimization problems have emerged across various fields. Among these problems, non-separable problems pose significant challenges in finding global optimal solutions. To tackle these challenges, cooperative coevolution has become a widely utilized algorithmic framework for solving large-scale problems by decomposing them into smaller components. However, existing methods for problem decomposition struggle with effectively handling the inherent structural characteristics of non-separable problems. To address this issue, this paper presents a dynamic grouping approach based on variable importance. Throughout the entire solution process, this approach periodically assesses the importance of each variable by quantifying the impact of their perturbations on the objective function. The variable group is then formed by selecting a small number of highly important variables and combining them with randomly selected variables. Additionally, a variable reset method is introduced to reset some of the already-converged variables, enhancing the exploration capabilities. The proposed algorithm is evaluated against five state-of-the-art algorithms using a self-established non-separable function benchmark suite. The results demonstrate that the proposed algorithm performs exhibits significant advantages on non-separable functions. Specifically, the proposed algorithm achieves a success rate of 13 out of 16 functions.},
  archive      = {J_ASOC},
  author       = {Yuning Chen and Chun Ouyang and Yi Liu and Hongda Zhang and Zhongxue Gan},
  doi          = {10.1016/j.asoc.2025.113363},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113363},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic cooperative coevolution based on variable importance for non-separable large-scale global optimization},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-body structure information enhancement GCN for 3D human pose estimation in videos. <em>ASOC</em>, <em>180</em>, 113362. (<a href='https://doi.org/10.1016/j.asoc.2025.113362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To alleviate the problem of depth ambiguity in 3D human pose estimation, we propose Multi-body Structure information enhancement Graph Convolutional Network (MS-GCN), which exploits neighborhood subgraph structure to distinguish similar nodes and reduces the risk of over-smoothing of graph convolutional networks. Specifically, MS-GCN comprises the Structure Weight information enhancement GCN (SWGCN) and the Structural Semantic information excitation GCN (SSGCN), which capture global and local interactions among nodes. In the SWGCN, we propose a data-enhanced method. Here, an adaptive distance perception method is proposed to dig the multi-body structure weight information of neighborhood subgraphs, which contains the nonlinear dynamic characteristics of human motion. Then, the proposed multi-head cross-talk graph attention explores a variety of possible associations between node neighbors based on content and structure representations. In the SSGCN, we propose a Neighborhood Subgraph Encoder (NSE) to aggregate structural semantic information, which provides sufficient feature information for pose estimation. Besides, we adopt a Gated Recurrent Unit Pose Lifting (GRUPL) layer to improve the accuracy of the final result. Finally, we conduct extensive experiments on two popular benchmarks, e.g., Human3.6M and HumanEva-I, where the proposed model outperforms a number of recent state-of-the-art approaches with fewer input frames and model size.},
  archive      = {J_ASOC},
  author       = {Hanghang Zhou and Yumei Zhang and Jinli Ma and Honghong Yang and Xiaojun Wu},
  doi          = {10.1016/j.asoc.2025.113362},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113362},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-body structure information enhancement GCN for 3D human pose estimation in videos},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven multi-phase constrained optimization based on multi-surrogate collaboration mechanism and space reduction strategy. <em>ASOC</em>, <em>180</em>, 113359. (<a href='https://doi.org/10.1016/j.asoc.2025.113359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving complex black-box constrained optimization problems (BCOPs) remains a challenge in the optimization field. This paper presents a data-driven multi-phase constrained optimization method using a multi-surrogate collaboration mechanism and space reduction strategies (DMSCSR) that can efficiently find the global optimum. The proposed method consists of three main phases: the global, transitional, and local phases. These phases are executed in the original design space and two subspaces, respectively. The global phase employs global surrogates for optimization and sampling, while the transitional phase uses a self-organizing map to create a subspace where local surrogates are constructed and optimized. The local phase utilizes the current best sample to construct another subspace, where a local search is performed. Additionally, a multi-surrogate collaboration mechanism is integrated into DMSCSR, enhancing the accuracy of the approximate optimization problems by leveraging multiple types of surrogates. To balance exploration and exploitation, DMSCSR adopts the optimal complex function–Voronoi method to generate additional samples. The effectiveness of DMSCSR is demonstrated on twenty-three mathematical and eight engineering benchmark problems. Compared with three state-of-the-art methods, DMSCSR exhibits superior performance. We also compare DMSCSR with its five variants to further investigate its properties. The results are analyzed, and the noticeable advantages of DMSCSR in solving BCOPs are further confirmed.},
  archive      = {J_ASOC},
  author       = {Xiao-Yao Han and Peng Wang and Huachao Dong and Xinjing Wang},
  doi          = {10.1016/j.asoc.2025.113359},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113359},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven multi-phase constrained optimization based on multi-surrogate collaboration mechanism and space reduction strategy},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative study of Center–Radius and Lower–Upper type interval neural network methods in uncertainty modeling. <em>ASOC</em>, <em>180</em>, 113347. (<a href='https://doi.org/10.1016/j.asoc.2025.113347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval Neural Networks (INNs) offer a novel methodology for addressing uncertainties in data by employing interval-valued inputs, outputs, and, in some cases, weights. In this study, we conduct a comparative analysis of two types of INNs. The first model, termed CR-INN, utilizes interval-valued inputs and outputs with crisp (non-interval) weights, relying on the Center–Radius (CR) approach for interval arithmetic during both forward and backward propagation. The second model, LU-INN, extends this framework by incorporating interval-valued inputs, outputs, and weights, using the Lower–Upper (LU) bound method for interval calculations. The effectiveness of these two models is evaluated through two case studies. The first involves a continuous mathematical function with one input and one output, serving as a simplified, synthetic test case. The second case applies these models to a real-world multidimensional poverty dataset, comprising 12 input dimensions and one output. However, both INN models demonstrated effective performance on both datasets. Our findings suggest that CR-INN, which has only half the number of trainable parameters compared to LU-INN, achieved lower training and validation loss while also requiring less training time. Nonetheless, the capacity of LU-INN to handle interval weights indicates that it may be better equipped to tackle more complex problems with higher-dimensional input–output relationships.},
  archive      = {J_ASOC},
  author       = {Sandeep Kumar and S. Chakraverty and Narayan Sethi},
  doi          = {10.1016/j.asoc.2025.113347},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113347},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comparative study of Center–Radius and Lower–Upper type interval neural network methods in uncertainty modeling},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep fusion for enhancing internet of vehicles security in sustainable smart vehicular communication with LSTM and GRU-guided artificial deep neural networks. <em>ASOC</em>, <em>180</em>, 113346. (<a href='https://doi.org/10.1016/j.asoc.2025.113346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles (IoV) represents a blend of smart cars and Internet of Things (IoT) networks, which boosts transportation effectiveness yet reveals significant security weaknesses. Traditional security methods, including firewalls and the signature-based Intrusion Detection System (IDS), do not protect against modern cyber threats, making IoV networks vulnerable to data breaches and malicious attacks. This research introduces a Deep Learning (DL)-based IDS designed to process attack patterns through Long-Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) that capture both immediate and extended attack dependencies. The system employs an Artificial Neural Network (ANN)-guided security approach for neural key exchange, which enables encrypted and dependable communication between interconnected vehicles. The proposed approach undergoes thorough testing using several benchmark datasets such as Car-Hacking 2020, CIC-IDS 2017, CSE-CIC-IDS 2018, and CIC DoS to ensure it can handle multiple real-world attack scenarios like DDoS attacks, spoofing attacks, and vehicle intrusions. The detection model proposed shows its outstanding performance through more than 99.8% accuracy in detecting combined DDoS attacks while achieving 99.9% accuracy for vehicle hacking detection above traditional IDS systems. The research delivers a hybrid LSTM-GRU intrusion detection system, ANN-based key synchronization for IoV communication security, and improved generalization through the use of real-world datasets. This research develops a strong security framework that protects smart vehicular networks by real-time detection of complex cyber threats and maintains secure key exchange. Research findings demonstrate the capacity of deep learning-based IDS to reduce IoV network vulnerabilities while advancing the development of secure, sustainable smart vehicular communication transportation systems.},
  archive      = {J_ASOC},
  author       = {Heng Zhang and Arindam Sarkar},
  doi          = {10.1016/j.asoc.2025.113346},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113346},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep fusion for enhancing internet of vehicles security in sustainable smart vehicular communication with LSTM and GRU-guided artificial deep neural networks},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-way decision with granular rough sets. <em>ASOC</em>, <em>180</em>, 113344. (<a href='https://doi.org/10.1016/j.asoc.2025.113344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By integrating granular computing with rough set theory, granular rough sets enhance the semantics and effectiveness of decision-making through granule-based representations. Existing research has not thoroughly explored the issues of inducing three-way decision rules with granular rough sets, partly due to the challenge of meaningfully describing granules. To address these gaps, this paper proposes a unified framework for three-way decision models based on granular rough sets. Additionally, we introduce a generalized formulation for granule descriptions. It extends traditional representations to include all possible descriptions within a given domain. Through the lens of the proposed framework and granular descriptions, we formulate a three-way decision model in generalized granular rough sets and further demonstrate its instantiation potential across three specific types of granular spaces: quotient spaces, neighborhood-induced granular spaces, and maximal-clique-induced granular spaces. The effectiveness of the proposed models is illustrated through examples using set-valued information tables and experiments on real-world datasets. The results show that the proposed models have good performance and practical applicability.},
  archive      = {J_ASOC},
  author       = {Junfang Luo and Mengjun Hu and Chengjun Shi and Yiyu Yao},
  doi          = {10.1016/j.asoc.2025.113344},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113344},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way decision with granular rough sets},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cold-start aware group recommendation method based on an augmented graph and knowledge distillation. <em>ASOC</em>, <em>180</em>, 113341. (<a href='https://doi.org/10.1016/j.asoc.2025.113341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing a group consensus for group recommendations is a highly challenging problem. In the real world, many groups are often ad-hoc, so recommender systems frequently face a cold-start problem. Recent multi-view-based approaches, however, overlook unseen groups that have no historical data on items. Their performance could degrade when there are many unseen groups, because unseen groups do not have item-level information. Though there have been several cold-start-aware methods, they do not fully utilize higher-order relationships, e.g., group memberships. Hypergraph-based approaches have been shown to be effective for modeling higher-order relationships. However, there is no recursive hypergraph-based approach to cope with unseen groups yet. We develop a unified graph for group recommendations through a novel recursive-hypergraph modeling that utilizes connected edges, e.g., group memberships and user preferences, so that we can obtain rich information for unseen groups. Since training on a unified graph alone is insufficient for generalization, we propose two regularization components based on knowledge distillation with graph augmentation for informative training of group preferences. One is to balance the information between unified and augmented graphs. The other is to distill members’ preferences into groups’ preferences. We conduct performance experiments using four real-world datasets and show that our proposed method significantly outperforms other recent group recommendation techniques on average.},
  archive      = {J_ASOC},
  author       = {Kwang Hee Lee and Hyun Ji Jeong and Myoung Ho Kim},
  doi          = {10.1016/j.asoc.2025.113341},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113341},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A cold-start aware group recommendation method based on an augmented graph and knowledge distillation},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The hierarchical and stochastic VIKOR to deal with imprecise and missing information. <em>ASOC</em>, <em>180</em>, 113340. (<a href='https://doi.org/10.1016/j.asoc.2025.113340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A great majority of Multiple Criteria Decision Aiding (MCDA) methods assume that all assessment criteria are considered at the same hierarchical level. However, decision problems often involve hierarchical structures with varying configurations. Another common challenge in real-world decision-making is the perceived subjectivity of preference elicitation. Although these challenges have been widely discussed in the literature, they have not been adequately addressed in the context of the VIKOR method. This paper proposes a new VIKOR-based method, called VIKOR-HSMAA, that deals simultaneously with indirect preference elicitation and hierarchical structure of criteria. The novelty of the methodology consists of a joint consideration of hierarchical assessments of alternatives performances for hierarchical criteria, considering imprecise criteria weights and analysis of compromise solution resulting from the VIKOR method. In addition, the conditions for the acceptance of VIKOR’s solution are adapted as new SMAA descriptive measures. A case study regarding the assessment of sustainability dimensions of Brazilian states shows the method application. VIKOR-HSMAA provided information at the sustainability level with the state of Santa Catarina (SC) as the most sustainable. Considering each macro-criterion, SC is the first in the economic dimension, Roraima and Distrito Federal (DF) are the two first in the environmental rank and SC and DF are the two first in the social rank. By enabling the consideration of missing or imprecise data, the VIKOR-HSMAA method proves to be a robust tool to reduce subjectivity in decision-making, providing a reliable support system for the assessment of institutions and governments. VIKOR-HSMAA innovates by providing stochastic measures to consider more than one alternative in the solution, thus maintaining the characteristics and advantages of the VIKOR method in a structure with imprecise data and hierarchy of criteria.},
  archive      = {J_ASOC},
  author       = {Rafaela Heloisa Carvalho Machado and Samuel Vieira Conceição and Renata Pelissari and Arthur Almeida Santos and Sarah Ben Amor},
  doi          = {10.1016/j.asoc.2025.113340},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113340},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The hierarchical and stochastic VIKOR to deal with imprecise and missing information},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Malware detection model based on stacking ensemble technique. <em>ASOC</em>, <em>180</em>, 113338. (<a href='https://doi.org/10.1016/j.asoc.2025.113338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Android malware has appeared frequently, posing a significant threat to users. The imbalance of malware samples, the redundancy of sample features, and the fluctuation of individual model performance also bring many challenges to malware detection. To address these issues, this paper propose a detection model based on a stacking ensemble technique. Firstly, to address the problem of sample imbalance, we propose a hybrid sample generation method named BS-GAN, which combines Generative Adversarial Networks with Borderline-SMOTE (Borderline-Synthetic Minority Over-Sampling Technique). BS-GAN is capable of generating malicious samples that closely resemble real ones. Secondly, to tackle the issue of feature redundancy, this paper introduces a feature processing method called IG-PCA, which integrates Information Gain and Principal Component Analysis. This approach improves detection accuracy while reducing model complexity. Finally, to address the challenges of fluctuating performance and limited generalization ability in individual models, this paper presents AM-Stacking (A Stacking Ensemble Model based on Attention Mechanisms), which dynamically adjusts the weights of the base classifiers to overcome the limitations of unstable performance and poor generalization ability of a single model when facing complex and diverse malware samples. Through model integration, AM-Stacking enhances detection performance, reduces overfitting, improves generalization, reduces feature redundancy, and increases overall detection stability. Experimental results show that the model can effectively detect known malware and identify some unknown malware.},
  archive      = {J_ASOC},
  author       = {Tun Li and Qianying Yao and Juan Li and Qian Li and Rong Wang and Chaolong Jia and Yunpeng Xiao},
  doi          = {10.1016/j.asoc.2025.113338},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113338},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Malware detection model based on stacking ensemble technique},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reference-point based non-dominated sorting for motif discovery. <em>ASOC</em>, <em>180</em>, 113337. (<a href='https://doi.org/10.1016/j.asoc.2025.113337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of biological motifs ( Motif Discovery Problem ) is a complex and significant issue in bioinformatics, for which metaheuristics are particularly well-suited. In this work, we focus on evaluating various multi-objective approaches that simultaneously optimize both the quality of discovered motifs and computational efficiency. Specifically, we examine the multi-objective evolutionary algorithm Non-Dominated Sorting Genetic Algorithm 3 (NSGA3) and propose modifications to adapt it to this problem, enhancing its convergence speed. We assess the convergence speed of NSGA3 by comparing it with other methods published in the literature, using twelve instances from four different organisms (fruit fly, Homo sapiens , mouse, and yeast). The analysis centers on the number of evaluations (objective function calculations) required by each algorithm to reach acceptable quality solutions. To achieve this, we examine the quality of the non-dominated solutions found by each algorithm throughout their execution, enabling us to determine which performs better. Additionally, we evaluate the success rates of the algorithms over 31 independent runs. Finally, we measure the biological significance of the solutions found by means of four well-known biological indicators, comparing the results with other approaches published in the literature.},
  archive      = {J_ASOC},
  author       = {Laura Escobar-Encinas and Álvaro Rubio-Largo and José M. Granado-Criado},
  doi          = {10.1016/j.asoc.2025.113337},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113337},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reference-point based non-dominated sorting for motif discovery},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new link prediction model for grain trade networks based on improved variational graph autoencoder and genetic algorithm. <em>ASOC</em>, <em>180</em>, 113336. (<a href='https://doi.org/10.1016/j.asoc.2025.113336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food security is related to the national economy and people’s livelihood, and exploring potential cooperative relationships with oneself is one of the most effective strategies to prevent and mitigate the risk of food import supply chain disruptions. How to use prior information in the grain import trade network to obtain better potential representations of nodes is a key issue in link prediction tasks. Under the theoretical framework of the variational graph autoencoder, this paper creates a new link prediction model, IVGAE-GA. Two feature extraction modules and a feature fusion module are designed to mine effective information in the trade networks. Specifically, a dynamic adaptive graph attention (DAGAN) module is proposed to extract high-order feature information from trade networks. Then, the neighborhood feature information of each node is captured through the graph convolutional neural network (GCN) to strengthen the guiding effect of the initial prior information on the prediction results. In addition, an average feature fusion (AVFF) module is designed to further refine the latent representation of nodes by mixing these non-local and local feature information. The entire IVGAE framework is optimized through cross-entropy loss and KL loss. Finally, the genetic algorithm (GA) is utilized for hyperparameter selection to help the model perform better. Extensive experimental results on two widely used publicly available datasets and four real grain trade networks illustrate that our model achieves better prediction performance compared to some existing methods. The proposed link prediction framework can be a good option for predicting potential cooperative relationships.},
  archive      = {J_ASOC},
  author       = {Yanhui Li and Yuzhi Song and Qi Yao and Xu Guan},
  doi          = {10.1016/j.asoc.2025.113336},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113336},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new link prediction model for grain trade networks based on improved variational graph autoencoder and genetic algorithm},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep knowledge tracing model based on cognitive assimilation and item response theory with better interpretability. <em>ASOC</em>, <em>180</em>, 113331. (<a href='https://doi.org/10.1016/j.asoc.2025.113331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning has been applied in knowledge tracing, it still faces various challenges. On the one hand, order-related information of items is usually overlooked in existing knowledge tracing models. On the other hand, existing models rarely considered the simulation of the knowledge state increment. Towards these issues, this paper aims to reconstruct a knowledge tracing model driven by deep learning. Particularly, based on knowledge/skill graph which consists of preparatory relationship between knowledge/skills, an approach of linearly ordering knowledge/skills is proposed through exploiting the topological hierarchy of knowledge/skills. By means of rough concept analysis, an approach of hierarchically clustering learning resources is proposed. Furthermore, a deep learning driven knowledge tracing model is developed by incorporating mechanisms of item response theory and cognitive assimilation theory within neural networks. The proposed model takes the advantage of powerful nonlinear modeling ability of neural networks, and meanwhile exploits the interpretable framework of item response theory to mine items’ features that can be explained from the viewpoint of cognitive assimilation. Finally, comparative data experiments are conducted to verify the efficiency of the proposed model.},
  archive      = {J_ASOC},
  author       = {Lankun Guo and Jingyi Zhang and Guozhi Ma and Jianhua Dai},
  doi          = {10.1016/j.asoc.2025.113331},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113331},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep knowledge tracing model based on cognitive assimilation and item response theory with better interpretability},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-way utility decision model with multi-label based on fuzzy neighborhood environments. <em>ASOC</em>, <em>180</em>, 113330. (<a href='https://doi.org/10.1016/j.asoc.2025.113330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been extensive research conducted on the Multi-Attribute Decision Making (MADM) problem, particularly focusing on the growing number of attributes involved. The importance of attribute weight has become a crucial factor in MADM scenarios. Conventional approaches for determining attribute weights are no longer sufficient for this domain. The primary obstacle is to efficiently assign attribute weights in order to get effective decision-making. This research presents a novel three-way utility decision (3WUD) model that combines multi-label and fuzzy neighborhood environments. Initially, we construct a Multi-Label Fuzzy Neighborhood Decision System (MLFNDS) utilizing an innovative method to calculate the multi-label conditional probability. Afterward, we determine the label importance and create a utility function that takes into account multi-label. Furthermore, we present a novel approach to assess the attribute importance by considering both algebraic and information-theoretic viewpoints. Ultimately, we have constructed a 3WUD model within multi-label fuzzy neighborhood environments to determine the definitive ranking of all objects. Additionally, four evaluation indicators are utilized to assess the efficiency, logic, and excellence of the suggested methodology. In summary, this paper provides a new perspective on three-way decision (3WD) and MADM research and expands its application scope. In addition, it also provides a new method and perspective for determining attribute weights in MADM.},
  archive      = {J_ASOC},
  author       = {Zhaohui Qi and Jiafeng Hu and Hui Li and Jianhua Dai},
  doi          = {10.1016/j.asoc.2025.113330},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113330},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-way utility decision model with multi-label based on fuzzy neighborhood environments},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized feature selection in high-dimensional gene expression data using weighted differential gene expression analysis. <em>ASOC</em>, <em>180</em>, 113329. (<a href='https://doi.org/10.1016/j.asoc.2025.113329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene expression datasets often contain many features/genes that do not contribute to classifying sampled tissues. This is particularly challenging in high-dimensional datasets, where the number of genes is much higher than the number of samples. Therefore, it is important to identify the most influential features. This study presents a new approach called weighted Fisher score (WFISH) for selecting features in gene expression data. WFISH uses gene expression differences between classes to assign weights to features, prioritizing informative ones and reducing the impact of less useful ones. By incorporating these weights in the traditional Fisher score, WFISH aims to select the most informative and biologically significant genes in high-dimensional classification problems. Our experiments show that WFISH outperforms other feature selection techniques in accurately classifying gene expression data. When using random forest (RF) and k nearest neighbors ( k NN) classifiers on five benchmark datasets, WFISH consistently achieved lower classification errors compared to the existing techniques.},
  archive      = {J_ASOC},
  author       = {Amjad Ali and Zardad Khan and Saeed Aldahmani},
  doi          = {10.1016/j.asoc.2025.113329},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113329},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized feature selection in high-dimensional gene expression data using weighted differential gene expression analysis},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-guided cross teaching semi-supervised framework for histopathology image segmentation with curriculum self-training. <em>ASOC</em>, <em>180</em>, 113328. (<a href='https://doi.org/10.1016/j.asoc.2025.113328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Histopathology image segmentation is crucial in disease diagnosis, therapeutic response evaluation, and prognosis. However, manually annotating pixel-level labels for histopathology images is both time-consuming and labor-demanding task. In this study, we propose a novel semi-supervised semantic segmentation framework called UTCS ( U ncertainty-guided cross T eaching and C urriculum S elf-training) to address the challenges of limited labeled data. UTCS effectively harnesses the benefits of consistency regularization and self-training in semi-supervised learning. Our approach introduces a mutual consistency network, where one network’s prediction is used as a pseudo mask to supervise the other network and vice versa. Addressing the issue of unreliable pseudo labels, we propose a dynamically re-weighted loss function that leverages uncertainty to perform pixel-level selection during the mutual teaching process, referred to as uncertainty-guided cross teaching. Furthermore, inspiring from curriculum learning, we incorporate an self-training strategy, focusing on image-level selection, that prioritizes reliable images during the re-training stage and aims to generate high-quality pseudo-labels for less reliable images. Extensive experiments on two publicly available histopathology datasets, BCSS and LUAD-HistoSeg, demonstrate the superior performance of our method compared to state-of-the-art semi-supervised semantic segmentation methods.},
  archive      = {J_ASOC},
  author       = {Rui Xu and Nan Zhou and Siyang Feng and Zhenbing Liu and Huahu Deng and Yajun An and Jian Li and Chu Han and Zaiyi Liu and Rushi Lan and Cheng Lu and Xipeng Pan},
  doi          = {10.1016/j.asoc.2025.113328},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113328},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertainty-guided cross teaching semi-supervised framework for histopathology image segmentation with curriculum self-training},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level semantic and spatial hierarchy reasoning for 3D face reconstruction and dense alignment in unconstrained environments. <em>ASOC</em>, <em>180</em>, 113327. (<a href='https://doi.org/10.1016/j.asoc.2025.113327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D face reconstruction and dense alignment tasks encounter significant challenges when the samples are in highly unconstrained environments, particularly with large poses, extreme expressions, occlusions and complex backgrounds. Although carefully designing the neural network architecture can enhance the representation ability, its performance is still unsatisfactory due to the absence of accurate facial semantic and spatial hierarchy information. To address this challenge, we propose an approach that uses neural networks to capture multi-level facial semantic and spatial hierarchy knowledge so as to guide the learning process. Specifically, our approach, referred to as Multi-Level Semantic and Spatial Hierarchy Reasoning Network (MSHRNet), leverages a point-to-space level progressive face structure loss function to precisely learn the semantic and spatial hierarchy knowledge of different facial parts. This knowledge is then injected into the backbone network through multi-level hierarchy knowledge matrices to incorporate structural reasoning knowledge, which can suppress the effects of large poses, extreme expressions, and occlusions. Moreover, we introduce a sample-to-dataset level data augmentation module that effectively yields rich and diverse semantic and spatial hierarchy information to inhibit occlusions and complex backgrounds while learning fine-grained local details. Extensive quantitative and qualitative experiments on benchmark datasets demonstrate that our MSHRNet outperforms the state-of-the-art methods in terms of both accuracy and computational complexity at the cost of little increase in the number of parameters. Codes and all data are publicly available at https://github.com/Ray-tju/MSHRNet .},
  archive      = {J_ASOC},
  author       = {Lei Li and Fuqiang Liu and Junyuan Wang and Yanni Wang and Zhitao Zhang and Jiahao Li and Qi Wang},
  doi          = {10.1016/j.asoc.2025.113327},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113327},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-level semantic and spatial hierarchy reasoning for 3D face reconstruction and dense alignment in unconstrained environments},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A control-oriented operation mode recognizing method using fuzzy evaluation and attention LSTM networks. <em>ASOC</em>, <em>180</em>, 113326. (<a href='https://doi.org/10.1016/j.asoc.2025.113326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operation mode recognition is a prerequisite for precise regulation of key performance indicators (KPIs) in industrial processes. However, system uncertainties and the complexity of the process dynamics pose significant challenges in achieving accurate mode partitioning. This study proposes a control-oriented operation mode recognition method called attention-long short-term memory-Monte Carlo simulation (AT-LSTM-MC). First, a fuzzy inference-based ‘indicator regulation potential’ evaluation framework is established to quantitatively describe the maximum control potential of each control variable on KPIs. Subsequently, considering the temporal dependencies of industrial process data, a long short-term memory (LSTM) autoencoder network is employed as the core architecture for feature extraction, where the ‘indicator regulation potential’ guides the LSTM autoencoder through attention layers to extract control-oriented deep clustering features. Finally, the K-means clustering method is utilized to determine the system operation modes based on these deep clustering features. To address uncertainty-induced challenges, multiple Monte Carlo simulations are performed on the operation mode recognition for the same period, thereby obtaining a statistically convergent operation mode. The effectiveness of the proposed method is validated through a case study of an actual industrial process.},
  archive      = {J_ASOC},
  author       = {Bei Sun and Zhixuan Peng and Juntao Dai and Yonggang Li},
  doi          = {10.1016/j.asoc.2025.113326},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113326},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A control-oriented operation mode recognizing method using fuzzy evaluation and attention LSTM networks},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated decision support system for supplier selection and performance evaluation in global supply chains. <em>ASOC</em>, <em>180</em>, 113325. (<a href='https://doi.org/10.1016/j.asoc.2025.113325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of global supply chains is increasing due to the variety of factors that must be considered, including price, quality, delivery, capabilities, and sustainability. Effective enhancement of supply chains is significantly contingent upon precise evaluation and selection of supplier performance. Traditional evaluation methods frequently result in inadequate decisions and inefficiencies. This paper presents an integrated decision support system (DSS) that utilizes advanced multicriteria decision-making (MCDM) techniques to tackle these challenges. We employ Criteria Importance Assessment (CIMAS) to evaluate the significance of criteria, integrating subjective expert judgments in the process. We compute objective weights through Logarithmic Percentage Change-Driven Objective Weighting (LOPCOW), finalizing a comprehensive, data-driven assessment process. We use Evaluation Based on Relative Utility and Nonlinear Standardization (ERUNS) to rank supplier alternatives, integrating both subjective and objective data for a thorough analysis. The application of q -rung fuzzy sets ( q -RFSs) facilitates improved management of uncertainty and imprecision in evaluating the performance of the supplier. The proposed decision support system (DSS) is validated using comprehensive sensitivity and comparative analyses. The findings indicate that our system is superior to traditional methods regarding accuracy and efficiency, providing a more systematic and comprehensive approach to managing the complexities of global supply chains. This integrated approach addresses the limitations of standard methods and enhances decision-making and operational performance.},
  archive      = {J_ASOC},
  author       = {Ziyan Xiang and Xiuzhen Zhang},
  doi          = {10.1016/j.asoc.2025.113325},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113325},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated decision support system for supplier selection and performance evaluation in global supply chains},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic univariate sampling algorithm for dynamic optimization problems. <em>ASOC</em>, <em>180</em>, 113324. (<a href='https://doi.org/10.1016/j.asoc.2025.113324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic optimization refers to the process of optimizing decisions in systems where variables change over time. The speed of dynamic environmental changes poses a significant challenge in solving dynamic optimization problems. The convergence speed and search efficiency need to be appropriately balanced due to algorithm platform limitations. This paper introduces a new platform called univariate sampling that achieves fast convergence speed and high-efficiency search performance. Based on this, this paper proposes a dynamic univariate sampling (DUS) algorithm to solve dynamic optimization problems. The algorithm preserves the best solution found so far if the sampling environment changes and restarts search if the sampling environment becomes stationary. The DUS algorithm is tested against five state-of-the-art dynamic optimization algorithms on Generalized Moving Peaks Benchmark (GMPB) and Generalized Dynamic Benchmark Generator (GDBG) problems. Experimental results verify improved performance of the DUS algorithm in both convergence efficiency and accuracy.},
  archive      = {J_ASOC},
  author       = {Hong-jian Li and Geng Zhang and Xiang-yu Sun and Jie Deng and Yun Li},
  doi          = {10.1016/j.asoc.2025.113324},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113324},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic univariate sampling algorithm for dynamic optimization problems},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent space diffusion model for image dehazing. <em>ASOC</em>, <em>180</em>, 113322. (<a href='https://doi.org/10.1016/j.asoc.2025.113322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models (DMs) achieve SOTA performance by sequentially applying denoising networks to model the image synthesis process. Image dehazing requires strict constraints to produce results that are consistent with the ground truth, unlike image synthesis. Consequently, conventional diffusion models demonstrate significant inefficiencies in the estimation of feature maps across multiple iterations when implemented within large-scale architectures. To alleviate this problem, we introduce a latent space dehazing diffusion model, consisting of a codec, a latent diffusion network, and a latent space projection strategy execution unit. In order to make dehazing tasks more suitable, we propose utilizing a differential perception constraint method and a vector alignment training approach. Specifically, our method encompasses two training phases: pre-training and DM training. During pre-training, we utilize a vector alignment strategy that enables the encoder to project the feature space of the input image (incorporating degradation information) into the latent space, while the decoder in recovering clear images from the clean latent feature space. In the second stage, we train the DM to estimate the clean latent space directly from the latent space containing degradation information. Our method achieves remarkable results, especially achieving the performance of SOTA on the non-uniform haze concentration dataset. Our code links to https://github.com/fatsotiger/DehazeDiff .},
  archive      = {J_ASOC},
  author       = {Haobo Liang and Yan Yang and Jiajie Jing},
  doi          = {10.1016/j.asoc.2025.113322},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113322},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Latent space diffusion model for image dehazing},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty estimation by boosting with gaussian process for expensive optimization problems. <em>ASOC</em>, <em>180</em>, 113320. (<a href='https://doi.org/10.1016/j.asoc.2025.113320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In surrogate-assisted evolutionary algorithms (SAEAs), uncertainty measures the confidence level of the predicted fitness and plays an important role in selecting candidate points for expensive evaluation, as evaluating those with a large degree of uncertainty can enhance the exploration ability of algorithms and significantly improve model reliability. Gaussian process (GP) gains great popularity with a predictive variance to estimate uncertainty. However, the performance of GP-assisted SAEAs is still limited with weakened predictions and underutilized uncertainty. In this paper, we propose a novel SAEA that leverages a boosting ensemble with GP as the base learner to enhance optimization performance. First, GP is leveraged to boost a weak learner that captures global trends, refining its prediction in guiding the search. Then, the uncertainty provided by GP is integrated into boosting ensemble model to estimate its overall ensemble uncertainty, enhancing the model reliability and promoting the exploration capabilities. Moreover, a mechanism study is performed through standardized residual and a distance-based metric to have deep insights into search behaviors, demonstrating that the migrated uncertainty basically covers the statistical boundary of Gaussian distribution and contributes to exploitation and exploration trade-off. Experimental results across multiple benchmark problems and an antenna design problem demonstrate that the proposed method significantly enhanced GP-assisted SAEAs in statistical results of the best fitness value and convergence profile without increasing too much running time, particularly in high-dimensional problems.},
  archive      = {J_ASOC},
  author       = {Hanhua Zou and Sanyou Zeng and Changhe Li and Ruwang Jiao and Fei Zhao},
  doi          = {10.1016/j.asoc.2025.113320},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113320},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertainty estimation by boosting with gaussian process for expensive optimization problems},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating a simplified formula graph representation into a graph neural network model for premise selection. <em>ASOC</em>, <em>180</em>, 113318. (<a href='https://doi.org/10.1016/j.asoc.2025.113318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The search space for automatic theorem proving typically experiences exponential growth when attempting to prove a conclusion with numerous axioms. Premise selection presents a novel approach to tackle this challenge. However, one major obstacle lies in enhancing the presentation of logical formula graphs and graph neural network models in existing premise selection methods to preserve potential information from the logical formulas effectively. This study proposes a novel simplified graph representation of logical formulas by eliminating repeated quantifiers, along with a new term-walk graph neural network model incorporating an attention mechanism and attention pooling (ASTGNNS). This model aims to preserve syntax and semantic information of logical formulas, particularly regarding the order of symbols and the scope of quantifiers in logical formulas, thereby improving classification accuracy in premise selection problems. Specifically, we first transform first-order logical conjectures and premise formulas into simplified logical formula graphs by removing repeated quantifiers. Next, we introduce a method based on a common path kernel function to measure graph similarity and validate the interpretability of our simplified logical formula graphs method. Then, an attention mechanism is employed to assign weights to term-walk feature information of nodes for updating node feature representations; meanwhile, attention pooling is utilized for selecting nodes that significantly contribute towards generating the final formula graph vector. Finally, combining the premise graph vector and conjecture graph vector forms a binary classifier for classification purposes. Experimental results demonstrate that our proposed method achieves an accuracy rate of 88.77% on the MPTP dataset and 85.17% on the CNF dataset, outperforming the state-of-the-art premise selection method.},
  archive      = {J_ASOC},
  author       = {Xingxing He and Zhongxu Zhao and Yongqi Lan and Yingfang Li and Li Zou and Jun Liu and Luis Martínez and Tianrui Li},
  doi          = {10.1016/j.asoc.2025.113318},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113318},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating a simplified formula graph representation into a graph neural network model for premise selection},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A kernel-free quadratic surface twin support vector machine with capped l1-norm distance metric for robust classification. <em>ASOC</em>, <em>180</em>, 113317. (<a href='https://doi.org/10.1016/j.asoc.2025.113317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a robust capped L 1 -norm kernel-free quadratic surface twin support vector machine (C L 1 QTSVM) for robust classification tasks. By incorporating the capped L 1 -norm distance metric and adopting a kernel-free approach, the C L 1 QTSVM not only enhances robustness against outliers but also eliminates the need for selecting kernel functions and tuning their parameters, significantly saving computational time. The introduction of an L 2 -norm regularization term further ensures that the model adheres to structural risk minimization, thus improving its generalization ability. The primal non-differentiable optimization problems are equivalently reformulated into a pair of differentiable problems using re-weighting techniques and vectorization operators. An efficient alternating iteration algorithm is developed based on these reformulations. Additionally, the dimensionality of the inverse matrix is effectively reduced in high-dimensional feature spaces by applying the Sherman–Morrison–Woodbury formula. Theoretical analyses are provided on the convergence properties, time complexity, and the existence of locally optimal solutions to the optimization problems. Extensive experiments on several benchmark datasets validate the classification performance and robustness of the proposed model.},
  archive      = {J_ASOC},
  author       = {Qi Si and Zhixia Yang and Jinbiao Zhao and Rongda Chen},
  doi          = {10.1016/j.asoc.2025.113317},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113317},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A kernel-free quadratic surface twin support vector machine with capped l1-norm distance metric for robust classification},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic learning-based genetic algorithm for scheduling resource-constrained projects with alternative subgraphs. <em>ASOC</em>, <em>180</em>, 113316. (<a href='https://doi.org/10.1016/j.asoc.2025.113316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic algorithms (GAs) are population-based algorithms widely applied for solving complex scheduling problems and such the resource-constrained project scheduling problem with alternative subgraphs (RCPSP-AS) in which alternatives for work packages should be selected prior to project scheduling. The objective of this research is twofold. First, we develop a dynamic GA based on a new hybridisation of initialisation procedures, local searches based on learning approaches and restart schemes for scheduling problems in general. Second, we improve existing benchmark solutions for a large artificial dataset for the RCPSP-AS in particular. Our dynamic GA leverages existing constructive heuristics and priority rules to create a pool of high-quality initial solutions. Subsequently, these solutions are further improved by means of learning approaches that are designed as weight- or population-based local searches. In order to avoid getting stuck in a local optimum, various restart schemes are implemented. Based on our results, gradual learning and learning based on the population outperform other approaches for high-complex problem instances. Since metaheuristics — such as GAs — are mainly beneficial in complex problem settings, we are convinced that these research findings can inspire researcher when solving similar or other scheduling problems.},
  archive      = {J_ASOC},
  author       = {Rojin Nekoueian and Tom Servranckx and Mario Vanhoucke},
  doi          = {10.1016/j.asoc.2025.113316},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113316},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic learning-based genetic algorithm for scheduling resource-constrained projects with alternative subgraphs},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-branch and multi-scale feature fusion underwater image enhancement algorithm for diverse water environments. <em>ASOC</em>, <em>180</em>, 113315. (<a href='https://doi.org/10.1016/j.asoc.2025.113315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, underwater image enhancement technology has attracted much attention due to its important role in ocean exploration and environmental protection. Compared with the exploration of the marine environment, the study of highland and freshwater-circumstance lakes is poorly understood. Currently, the proposed underwater image datasets are trained and tested on paired datasets from the marine environments, which are overly simplistic and some datasets even contain only the original underwater images. Therefore, it is unknown whether these methods can effectively restore highland and freshwater-circumstance datasets. To address this, we introduce a highland and freshwater-circumstance dataset (P-HFUI) with 2000 pairs of paired images and propose an underwater image enhancement method named Multi-Branch and Multi-scale Feature Fusion Underwater Image Enhancement Algorithm (MM-DUIE) that can restore both marine and highland and freshwater-circumstance. At the same time, we compare the proposed algorithm with the most advanced ones qualitatively and quantitatively. The results show that the proposed model ranks among the top three in terms of Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measurement (SSIM), Mean Squared Error (MSE) and Root Mean Square Error (RMSE), and is more in line with the visual requirements of human eyes.},
  archive      = {J_ASOC},
  author       = {Zhen Li and Kaixiang Yan and Changcheng Wang and Dongming Zhou},
  doi          = {10.1016/j.asoc.2025.113315},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113315},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-branch and multi-scale feature fusion underwater image enhancement algorithm for diverse water environments},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A global Spatial–Temporal attention method based on motion decomposition for precipitation nowcasting. <em>ASOC</em>, <em>180</em>, 113314. (<a href='https://doi.org/10.1016/j.asoc.2025.113314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precipitation, the weather phenomenon closely related to human activities, has always been a primary focus for researchers aiming to achieve accurate forecasting. Additionally, common artificial intelligence methods often exhibit insufficient performance for long-term predictions. Therefore, based on the existing ConvRNN framework, we propose a precipitation nowcasting model named GMSA, which incorporates a global feature attention mechanism. GMSA comprises four main components: the Global Feature Extractor, the Gating Mechanism, the Global Feature Focus, and the Self Attention Memory Module. By extracting global features, we aim to achieve long-term extraction of precipitation characteristics. We conduct experiments using the CIKM2017 dataset and Shanghai2020 dataset. The experimental results indicate that GMSA achieves optimal performance in both two datasets. The model can still accurately predict the shape and intensity of key rainfall areas in the latter prediction stages (5–10 steps). Ablation experiment results demonstrate that the introduction of global feature extraction significantly enhances the model’s ability to infer trends in rainfall areas. The Self Attention Memory Module ensures that the model maintains high accuracy and visual sharpness of the predicted images in long-term prediction results.},
  archive      = {J_ASOC},
  author       = {YuHao Du and YiHong Li and ZhongZe Zeng and Hui Liu and JianKai Zhang},
  doi          = {10.1016/j.asoc.2025.113314},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113314},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A global Spatial–Temporal attention method based on motion decomposition for precipitation nowcasting},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated cross-chain trust training for distributed smart grid in web 3.0. <em>ASOC</em>, <em>180</em>, 113313. (<a href='https://doi.org/10.1016/j.asoc.2025.113313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-chain federated training (FT) is a paradigm for privacy-preserving predictive scheduling models in distributed smart grids under Web 3.0. Currently, it is often operates in a mode of “off-chain training and inter-chain sharing”. However, this mode brings three major trust challenges: (1) Opaque off-chain training may lead to missing training steps; (2) Cross-chain model transmission without a verification mechanism is vulnerable to forgery and tampering attacks; (3) The isolation mechanism between different chains makes cross-chain federated aggregation unverifiable. To solve the above issues, this paper proposes a cross-chain federated trusted training scheme (CCFTTS) to ensure the trustworthiness of multi-chain FT from three aspects: off-chain training, inter-chain transmission, and cross-chain aggregation. For the first issue, we design a twin trustworthy verification algorithm (TTVA). This algorithm replicates the off-chain training process using random numbers and symbol mapping mechanisms, thereby generating evidence to verify the integrity of the training steps and ensure the trustworthiness of off-chain training. For the second issue, we use the BLS aggregate signature algorithm to ensure the trustworthiness of the model during cross-chain transmission. For the third issue, we propose a cross-chain aggregation trusted verification method (CCATV). This method combines the commitment mechanism and utilizes its homomorphic aggregation property. By comparing the aggregated commitments of the off-chain training model with the commitment of the final aggregated model, it ensures the trustworthiness of cross-chain aggregation. Experimental results show that the proposed scheme is effective in ensuring trustworthiness and is feasible in practice.},
  archive      = {J_ASOC},
  author       = {Cui Zhang and Hui Yang and Chen Zhang and Jie Zhang and Qiuyan Yao and Zhiwei Wang and Athanasios V. Vasilakos},
  doi          = {10.1016/j.asoc.2025.113313},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113313},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated cross-chain trust training for distributed smart grid in web 3.0},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An ensemble classifier combining Dempster–Shafer theory and feature selection methods aggregation strategy. <em>ASOC</em>, <em>180</em>, 113306. (<a href='https://doi.org/10.1016/j.asoc.2025.113306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several individual feature selection methods have proven to be important preprocessing techniques for dimensionality reduction in microarray gene expression datasets. However, one of the deficiencies of these methods is that their results differ from classifier to classifier and from method to method, even for a single domain. The ensemble classification method is a promising solution that can be used to alleviate this issue by combining the outputs of several classifiers and individual feature selection methods. In this paper, an ensemble of classifiers and feature selection approaches is proposed based on Dempster–Shafer evidence theory and the weighted majority vote (EC-FMDS). First, feature selectin methods from different categories are applied to generate feature subsets. Then, the Dempster–Shafer theory is used to fuse the output of the individual feature selection methods and assign a weight to each classifier. Finally, by fusing all the subsets and weights of classifiers, the weighted majority vote is applied to gain the final prediction and calculate the performance measures. To evaluate the effectiveness of the proposed EC-FMDS approach, eight feature selection methods and four classifiers are used in the ensemble method. The experimental analysis is conducted using twelve microarray datasets based on accuracy, recall, and precision measures. A comparative study with the results of the individual feature selection methods and the ensemble state-of-the-art proves the effectiveness of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Hasna Chamlal and Fatima Ezzahra Rebbah and Tayeb Ouaderhman},
  doi          = {10.1016/j.asoc.2025.113306},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113306},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble classifier combining Dempster–Shafer theory and feature selection methods aggregation strategy},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving collision perception via estimation of deviation from the looming course. <em>ASOC</em>, <em>180</em>, 113304. (<a href='https://doi.org/10.1016/j.asoc.2025.113304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve visual collision avoidance intelligence, a neural network based on the locust’s visual neural circuit for looming perception has been established and applied to intelligent robots. However, the existing locust-inspired collision avoidance neural network lacks the ability to distinguish the deviation of the looming course, hindering the measurement of risk degrees for different approaching objects. To address this issue, we introduce a deviation calculation mechanism and a risk weighting component into the traditional locust-inspired collision perception network, enabling the network to discriminate the threat level in different approaching ways. The proposed mechanism estimates the velocity of the approaching object in both looming and translating directions. Meanwhile, the risk weighting component based on the velocity estimation output guides the locust-inspired collision perception network in identifying threat levels for different approaching ways. Numerous experimental results substantiate that the proposed model exhibits a robust capability to discriminate looming threats with different approaching ways, such as passing by. This study expands the understanding of the approaching mode and the application potential of the locust-inspired collision detection network.},
  archive      = {J_ASOC},
  author       = {Yi Zheng and Hao Chen and Yusi Wang and Haiyang Li and Jigen Peng},
  doi          = {10.1016/j.asoc.2025.113304},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113304},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving collision perception via estimation of deviation from the looming course},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient unified information extraction model based on large language models. <em>ASOC</em>, <em>180</em>, 113302. (<a href='https://doi.org/10.1016/j.asoc.2025.113302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have shown underperformance in information extraction (IE) tasks compared to smaller models. This underperformance is largely attributed to the mismatch between IE’s structured output and LLMs’ natural language output and the absence of IE tasks in the pre-training corpus. Furthermore, prior research aimed to adapt LLMs to IE tasks through instructional design without updating model parameters, or by fine-tuning them with substantial computational resources, at the expense of their original performance in other tasks. Inspired by the Parameter Efficient Fine-Tuning (PEFT) technique, we designed an efficient unified information framework for LLMs (LLM-UIE), which performs domain adaptation fine-tuning with low resource requirements. Importantly, LLM-UIE introduced an additional answer selection task to improve LLMs’ ability to generate desired answers, efficiently addressing the inconsistency between LLMs’ fuzzy outputs and standard answers. Experiments on extensive information extraction datasets show that LLM-UIE not only matches but even surpasses the F1 scores of state-of-the-art models while demonstrating significant advantages in training efficiency, substantially reducing training time. Moreover, compared to previous LLM-based studies, LLM-UIE significantly lowers computational resource requirements.},
  archive      = {J_ASOC},
  author       = {Xieyun Zhang and Shimin Cai and Xiaorong Shen and Han Yang and Wenhao Hu and Yanru Zhang},
  doi          = {10.1016/j.asoc.2025.113302},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113302},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient unified information extraction model based on large language models},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metric tools for sensitivity analysis with applications to neural networks. <em>ASOC</em>, <em>180</em>, 113300. (<a href='https://doi.org/10.1016/j.asoc.2025.113300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Machine Learning models are considered for autonomous decisions with significant social impact, the need to understand how these models work rises rapidly. Explainable Artificial Intelligence (XAI) aims to provide interpretations for predictions made by Machine Learning models, in order to make the model trustworthy and more transparent for the user. For example, selecting relevant input variables for the problem directly impacts the model’s ability to learn and make accurate predictions. One of the main XAI techniques to obtain input variable importance is the sensitivity analysis based on partial derivatives. However, existing literature of this method provides no justification of the aggregation metrics used to retrieved information from the partial derivatives. In this paper, a theoretical framework is proposed to study sensitivities of ML models using metric techniques. From this metric interpretation, a complete family of new quantitative metrics called α -curves is extracted. These α -curves provide information with greater depth on the importance of the input variables for a machine learning model than existing XAI methods in the literature. We demonstrate the effectiveness of the α -curves using synthetic and real datasets, comparing the results against other XAI methods for variable importance and validating the analysis results with the ground truth or literature information.},
  archive      = {J_ASOC},
  author       = {Jaime Pizarroso and David Alfaya and José Portela and Antonio Muñoz},
  doi          = {10.1016/j.asoc.2025.113300},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113300},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metric tools for sensitivity analysis with applications to neural networks},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AEFA-FDB: A score-based artificial electric field algorithm for optimal reactive power dispatch problem with renewable and load demand uncertainties. <em>ASOC</em>, <em>180</em>, 113292. (<a href='https://doi.org/10.1016/j.asoc.2025.113292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal reactive power dispatch (ORPD) is critical to maintaining the reliability and security of electric power networks, especially with the increasing integration of renewable energy sources. This paper introduces AEFA-FDB, a physics-based metaheuristic algorithm designed to tackle the stochastic ORPD problem. AEFA-FDB incorporates a fitness distance-based learning strategy, leverages historical data, and employs a nonlinear adaptive weighting mechanism to balance exploration and exploitation during optimization. Additionally, the algorithm uses Monte Carlo simulations to model uncertainties in renewable energy sources, including solar photovoltaic, wind, and hydropower systems. In real-world case studies employing IEEE 30 bus, 57 bus, and large-scale 118 bus systems, AEFA-FDB consistently outperforms state-of-the-art methods by reducing active power losses, improving voltage profiles, and enhancing voltage stability. Specifically, AEFA-FDB achieves optimal power losses of 4.4138 mW, 18.574 mW, and 58.6 mW as well as optimal voltage deviation of 0.0884 p.u., 0.6031 p.u., and 0.811 p.u. for the IEEE 30 bus, 57 bus, and 118 bus systems, respectively. Moreover, AEFA-FDB improves the average power loss and voltage deviation by up to 18% and 43%, respectively, compared to other methods across all systems. These results demonstrate the effectiveness of the algorithm in addressing challenges related to renewable energy integration in ORPD optimization. The MATLAB code for AEFA-FDB can be found here https://github.com/ChauhanDikshit .},
  archive      = {J_ASOC},
  author       = {Dikshit Chauhan and Anupam Yadav and Sung-Bae Cho},
  doi          = {10.1016/j.asoc.2025.113292},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113292},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AEFA-FDB: A score-based artificial electric field algorithm for optimal reactive power dispatch problem with renewable and load demand uncertainties},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective closed-loop supply chain inventory model with learning and forgetting under carbon emission policies using NSGA-II, MOPSO, and TOPSIS. <em>ASOC</em>, <em>180</em>, 113291. (<a href='https://doi.org/10.1016/j.asoc.2025.113291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the impact of workers’ good practices in remanufacturing, manufacturing, and inspection processes under the learning and forgetting (LaF) framework on total cost and carbon emissions in a closed-loop supply chain (CLSC) inventory model. The investigation is conducted under three carbon emission reduction policies: carbon tax (CT), cap-and-offset (CCO), and cap-and-trade (CCT). Workers’ involvement in the continuous learn-forget-learn process across different tasks in the CLSC, including remanufacturing, manufacturing, machine operation, inspection, and correcting production errors, boosts productivity and process quality. The main focus for the CLSC participants is sustainability, emphasizing the improvement of worker experience to enhance productivity and process quality, aiming to minimize total cost and carbon emissions. First, the multi-objective optimization problems are formulated under the CT, CCO, and CCT policies while incorporating LaF effects. The total cost function serves as the first objective, while the carbon emission function constitutes the second. The non-dominated sorting genetic algorithm II (NSGA-II) and multi-objective particle swarm optimization (MOPSO) are employed to solve the optimization model, with their parameters fine-tuned using Taguchi analysis. Pareto fronts are generated to identify optimal solutions, and the best solutions are selected using multi-criteria decision analysis (MCDA) with the technique for order preference by similarity to an ideal solution (TOPSIS). A statistical analysis is conducted to compare the performance of NSGA-II and MOPSO. Numerical results reveal that learning significantly reduces total costs and carbon emissions across all three policies. A comparative analysis of the policies with and without the LaF effect indicates that the CCT policy with LaF is the most effective in reducing total costs and emissions in the CLSC. Sensitivity analysis further highlights the impact of parameter variations on total costs and carbon emissions under different policies. As the learning exponent (LE) increases from 0 to 0.415, total costs and carbon emissions steadily decline. Under the CT policy, average costs decrease by 2.65%, while carbon emissions are reduced by 4.76%. The CCO policy results in reductions of 2.58% in costs and 5.86% in emissions. In contrast, the CCT policy exhibits the most significant improvements, with cost reductions of 3.84% and emission reductions of 6.93%.},
  archive      = {J_ASOC},
  author       = {Tanmay Halder and Bijoy Krishna Debnath},
  doi          = {10.1016/j.asoc.2025.113291},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113291},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective closed-loop supply chain inventory model with learning and forgetting under carbon emission policies using NSGA-II, MOPSO, and TOPSIS},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neutrosophic hesitant fuzzy PROMETHEE and its application to location selection of migrating swarm. <em>ASOC</em>, <em>180</em>, 113290. (<a href='https://doi.org/10.1016/j.asoc.2025.113290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The desert locust ( Schistocerca gregaria ) represents a significant threat to agriculture worldwide, causing extensive damage through its unpredictable and expansive migratory swarms. Developing predictive models for these movements is crucial, given their profound implications for crop security and agricultural stability. This study presents an advanced solution for forecasting locust migratory loci by leveraging SVNHFSs (Single Valued Neutrosophic Hesitant Fuzzy Sets) synergized with the PROMETHEE (Preference Ranking Organization METHod for Enrichment Evaluations) method. This framework adeptly navigates the intrinsic uncertainties and hesitancies characteristic of locust migration data through the introduction of novel information measures, specifically the neutrosophic hesitant fuzzy Hausdorff distance and the integrated neutrosophic hesitant fuzzy entropy. Our mathematical rigor is demonstrated in the development of optimization models that calibrate criteria weights, thereby augmenting the decision-making process with enhanced precision. A comprehensive comparative analysis with existing models underscores the superior predictive capability of our methodology. The validation via a robust case study further attests to its efficacy, positioning this approach as a pivotal instrument for anticipatory locust management. Ultimately, our framework aspires to fortify agricultural resilience and secure food resources in locust-prone regions, thereby addressing a critical facet of global food security.},
  archive      = {J_ASOC},
  author       = {Sayon Bakshi and Mahatab Uddin Molla and Bibhas C. Giri},
  doi          = {10.1016/j.asoc.2025.113290},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113290},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neutrosophic hesitant fuzzy PROMETHEE and its application to location selection of migrating swarm},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view anchor graph regularization induced unbiased learning for positive and unlabeled problem. <em>ASOC</em>, <em>180</em>, 113287. (<a href='https://doi.org/10.1016/j.asoc.2025.113287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to expensive and mundane annotations, it is wise to collect data from diverse sources or present it in multiple ways. Consequently, the scarcity of labeled data and the richness of data representation foster advancements in multi-view semi-supervised learning. Nevertheless, numerous real-world scenarios often present a formidable challenge, namely the Positive and Unlabeled (PU) problem, where negative label information is absent. Most semi-supervised algorithms struggle to effectively address PU problems, and a universal model for multi-view positive and unlabeled (PU) learning remains lacking. Hence, in this paper, we propose a Multi-view Anchor Graph Regularization-induced unbiased method for PU problems (MvAGRPU). In MvAGRPU, we transform multi-view PU problems into supervised problems with label noise, learning unbiased classifiers through loss factorization and centroid approximation. Additionally, we devise a regularization term tailored for PU problems, which integrates anchor graph, local, and semantic information to facilitate interplay among diverse views. Subsequently, we devise an efficient algorithm to tackle the resultant optimization problem. Finally, we compare MvAGRPU against five PU algorithms on multiple datasets, validating its superiority and stability.},
  archive      = {J_ASOC},
  author       = {Jie Zhao and Xiao Li and Yitian Xu and Min Yuan},
  doi          = {10.1016/j.asoc.2025.113287},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113287},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-view anchor graph regularization induced unbiased learning for positive and unlabeled problem},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A communication-efficient federated learning approach via dynamic mutual distillation for image recognition. <em>ASOC</em>, <em>180</em>, 113286. (<a href='https://doi.org/10.1016/j.asoc.2025.113286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a promising approach to protect data privacy in the image recognition field, enabling collaborative model training across distributed edge participants without compromising local data. However, the privacy-preserving feature comes at the cost of significant communication overhead due to frequent model parameter exchanges. In the current edge computing environment, clients are usually deployed on edge devices with limited bandwidth, the communication delay greatly influences the training efficiency of federated learning. Hence, the paper proposes a communication-efficient federated learning approach FedDMS based on mutual distillation and dynamic client selection for image recognition. It improves the convergence efficiency through client-side dynamic distillation and increases task accuracy through fine-tuning. In addition, the server adaptively selects participation clients through periodic gradient evaluation, thereby reducing the communication overhead. FedDMS is evaluated from the aspects of performance and parameter sensitivity on two public datasets. The experimental results show that compared with other federated algorithms, FedDMS can save 73% of communication costs, significantly improving efficiency. Furthermore, FedDMS’s performance remains stable in different network structures, demonstrating its strong adaptability and optimization potential. At a cost, it requires additional computing resources on the client side.},
  archive      = {J_ASOC},
  author       = {Youhuizi Li and Yu Chen and Yuyu Yin and Haitao Yu},
  doi          = {10.1016/j.asoc.2025.113286},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113286},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A communication-efficient federated learning approach via dynamic mutual distillation for image recognition},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADNS: An adaptive dynamic neighborhood search method guided by joint learning heuristics and corresponding hyperparameters. <em>ASOC</em>, <em>180</em>, 113280. (<a href='https://doi.org/10.1016/j.asoc.2025.113280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinatorial optimization problems are prevalent in fields such as scheduling and transportation. Numerous exact and heuristic algorithms are available to obtain satisfactory solutions within a reasonable time. Despite their performance, implementing manually crafted effective heuristics and hyperparameters remains challenging. Integrating neural networks with classical algorithms has considerable potential. One widely discussed method involves employing neural networks to guide neighborhood search, in which disruption and repair operators are learned through neural networks. Notably, heuristics (search operators) and hyperparameters (search scope) jointly influence optimization during the search scope. Learning search degrees alongside operators enhances the algorithm’s capability for adequate exploration, a possibility that has yet to be thoroughly investigated. This paper proposes a method that jointly learns search heuristics and hyperparameters via neural networks. This method treats neighborhood search as a graph-based optimization problem. Each iteration models the solution as a graph, and its state is summarized using an improved graph attention network. Subsequently, the proposed adaptive exploration mechanism learns operators and search scope applicable to the current state to achieve moderate neighborhood exploration. Experimental results indicate that simultaneously learning search heuristics and hyperparameters during neighborhood search is beneficial. We found that the proposed method outperforms traditional heuristic algorithms and neural combinatorial optimization algorithms with fixed search steps in constrained vehicle routing problems on various scales.},
  archive      = {J_ASOC},
  author       = {Zhilong Wang and Xiaoxuan Hu and Haiquan Sun and Wei Xia},
  doi          = {10.1016/j.asoc.2025.113280},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113280},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ADNS: An adaptive dynamic neighborhood search method guided by joint learning heuristics and corresponding hyperparameters},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-granularity ensemble sample selection and label correction for classification with noisy labels. <em>ASOC</em>, <em>180</em>, 113266. (<a href='https://doi.org/10.1016/j.asoc.2025.113266'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample selection is crucial in classification tasks with noisy labels, yet most existing sample selection methods rely on a single criterion. These approaches often face challenges, including low purity of selected clean samples, and underfitting due to an insufficient number of selected clean training samples. To address these challenges, this paper proposes GNet-SSLC, a novel multi-granularity network framework that integrates multiple criteria ensemble sample selection (SS) and multiple views label correction (LC). In the SS phase, this paper proposes a metric learning-based dual k-Nearest Neighbor (k-NN) sample selection method. This method first uses corrected soft labels from the initial k-NN round to guide the selection of clean samples in the subsequent k-NN round. To further enhance selection accuracy, we combine this dual k-NN approach with a small loss sample selection technique through a voting mechanism. This multiple criteria ensemble method addresses the issues of low purity and instability inherent in single criterion approaches. In the LC phase, this paper designs a multiple views label correction framework that generates high-quality pseudo-labels for selected noisy samples. A key innovation of the framework is the design of a regularized contrastive learning loss, which optimizes the semi-supervised learning process by leveraging multiple views of training samples. The additional inclusion of training samples with high-quality pseudo-labels can effectively mitigate underfitting caused by a limited number of clean training samples. Experimental results on both synthetic and real-world noisy datasets indicate that GNet-SSLC enhances the purity and stability of the selected clean samples, and significantly improves classification performance. The enhancement is particularly notable with high noise rate dataset, such as CIFAR-100 dataset with 80% noise rate, achieving a 19.3% increase in classification accuracy compared to the baseline method.},
  archive      = {J_ASOC},
  author       = {Kecan Cai and Hongyun Zhang and Witold Pedrycz and Duoqian Miao and Chaofan Chen},
  doi          = {10.1016/j.asoc.2025.113266},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113266},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-granularity ensemble sample selection and label correction for classification with noisy labels},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing stock investment strategies with double deep Q-networks: Exploring the impact of oil and gold price signals. <em>ASOC</em>, <em>180</em>, 113264. (<a href='https://doi.org/10.1016/j.asoc.2025.113264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research endeavored to augment the Q-value selection mechanism within the Double Deep Q-Network (DDQN), a sophisticated deep reinforcement learning algorithm. The study incorporated data from the S&P 500 stocks and integrated the mean closing prices of international oil indices and gold futures as supplementary input parameters, thereby enriching and diversifying the dataset. This approach enabled a more holistic comprehension of the interrelated variables within the financial market. In comparison to traditional supervised learning methodologies, DDQN exhibited superior adaptability and flexibility. The integration of data from the stock market, oil, and gold futures allowed this study to elucidate the interrelationships among these economic indicators further. The introduction of decision thresholds into the trading decisions notably enhanced the stability of Q-value selection within the model. The empirical findings revealed that the integration of gold and oil futures into the DDQN model markedly enhanced the precision of investment predictions for S&P 500 stocks compared to the utilization of DQN or Dueling DQN exclusively. Notably, under conditions of elevated market pressure, gold futures demonstrated greater stability than oil futures. Analyses of long-term and short-term Return on Investment (ROI) indices across numerous experiments underscored the superiority and stability of the DDQN incorporating gold and oil indices in stock market investments. This research offers invaluable insights for participants in the financial market, assisting them in making prudent investment decisions in uncertain environments. According to our experimental model, the long-term ROI from 2010 to 2019 achieved 236.82%, while the short-term ROI for 2019 was 23.62%, outperforming existing machine learning methods and traditional quantitative approaches in both long-term and short-term ROI performance. The outcomes of this research not only further clarified stock market predictions but also broadened asset allocation strategies for conventional financial market portfolios. This innovative approach provides effective decision support for financial market investors, empowering them to navigate the rapidly evolving global economic landscape. This research’s source code and associated data files are available in a public GitHub repository at: https://github.com/NKUSTmis/DeepQNetworkIn/tree/main .},
  archive      = {J_ASOC},
  author       = {Yi-Ting Fu and Wen-Chen Huang},
  doi          = {10.1016/j.asoc.2025.113264},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113264},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing stock investment strategies with double deep Q-networks: Exploring the impact of oil and gold price signals},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable classification of epileptic EEG signals using ALIF decomposition and attention-augmented cascaded deep neural networks. <em>ASOC</em>, <em>180</em>, 113211. (<a href='https://doi.org/10.1016/j.asoc.2025.113211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a chronic neurological disorder characterized by recurrent seizures. Accurate diagnosis and effective monitoring require the precise classification of electroencephalogram (EEG) signals. In this study, we introduce a novel approach that combines Adaptive Local Iterative Filtering (ALIF) for signal decomposition with an attention-enhanced cascaded deep neural network (CDNN) architecture. The ALIF algorithm decomposes EEG signals into intrinsic mode functions (IMFs) that capture inherent oscillatory components. These IMFs are processed by the CDNN, which operates in two stages: a feature extraction module and a classification module. In the feature extraction stage, a SEblock channel attention mechanism dynamically prioritizes significant features within the IMFs. The classification stage employs a hybrid CNN-LSTM architecture that effectively captures both spatial and temporal dependencies. To enhance interpretability, the SHapley Additive exPlanations (SHAP) framework is incorporated to provide insights into the model’s decision-making process, while Gradient-weighted Class Activation Mapping (Grad-CAM) visualizes the most discriminative regions in the input data. Rigorously validated using 10-fold cross-validation on the Bonn and EEG Epilepsy databases, the proposed methodology achieved an exceptional classification accuracy of 100%, with sensitivity, specificity, and F1-scores exceeding 99% across various scenarios. The integration of SHAP and Grad-CAM not only elucidates the model’s decision processes but also contributes to a more interpretable and reliable system for epileptic EEG signal classification. This synergistic combination of advanced signal processing, deep learning, and interpretability techniques holds significant potential to enhance epilepsy diagnosis and strengthen trust in clinical decision support systems.},
  archive      = {J_ASOC},
  author       = {Wei Zeng and Minglin Zhang and Liangmin Shan and Yang Chen and Zuoyong Li and Shaoyi Du},
  doi          = {10.1016/j.asoc.2025.113211},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {113211},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interpretable classification of epileptic EEG signals using ALIF decomposition and attention-augmented cascaded deep neural networks},
  volume       = {180},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer stacked residual coordinate termite alate network for multi-class lung diseases detection from chest X-ray images. <em>ASOC</em>, <em>179</em>, 113393. (<a href='https://doi.org/10.1016/j.asoc.2025.113393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The World Health Organization identifies COVID-19, pneumonia, tuberculosis, and pneumothorax among the higher effects of death worldwide. Common symptoms include shortness of breath, fever, sneezing, and coughing. Traditional diagnostic methods such as thorough blood counts, the Mantoux skin test, antibodies, and DNA testing are often time-consuming and have a sensitivity of only about 80 %, with a 20 % error rate. As a faster and more reliable alternative, chest X-ray imaging is increasingly used for the detection of lung diseases. This research proposed a new system called the Multi-Layer Stacked Residual Coordinate Network, designed to accurately classify various lung diseases using chest X-ray images. Images are drawn from 6 datasets that are openly accessible. To enhance image quality, we develop the Gaussian Fourier Pyramid for Local Laplacian Filter technique, which combines adaptive histogram equalization with a multi-resolution Gaussian pyramid to better highlight important lung features while reducing noise. Next, the Mantis Search algorithm, a novel thresholding method, is used to segment critical regions in the X-rays, focusing the analysis on the most relevant areas. For deeper feature extraction, the model employed a Multi-Generative Adversarial Transformer, which captures complex patterns in the segmented regions. Finally, for classification, the Multi-Layer Stacked Residual Coordinate Network is optimized using Termite Alate Optimization, a metaheuristic inspired by termite foraging behavior that fine-tunes network parameters for better accuracy. According to experimental findings, the suggested method achieves 99 % accuracy, significantly outperforming existing techniques for multiclass lung disease classification.},
  archive      = {J_ASOC},
  author       = {Raju Egala and M.V.S. Sairam},
  doi          = {10.1016/j.asoc.2025.113393},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113393},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-layer stacked residual coordinate termite alate network for multi-class lung diseases detection from chest X-ray images},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The consensus and dissent model in the graph model for conflict resolution with interval fuzzy preferences with application to doctor-patient disputes. <em>ASOC</em>, <em>179</em>, 113392. (<a href='https://doi.org/10.1016/j.asoc.2025.113392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strategic conflicts, such as doctor–patient disputes, have become a major societal concern, with increased media attention exacerbating tensions among stakeholders. A key challenge in such conflicts is information asymmetry, which leads to uncertainty in the expression of decision-maker (DM) preferences. Existing consensus models in conflict analysis typically assume clear and consistent DM preferences, limiting their applicability in real-world scenarios characterized by ambiguity and complexity. Thus, within the framework of the graph model for conflict resolution (GMCR), this paper first attempts to characterize DM’s uncertainty and resolve conflicts by considering interval fuzzy consensus and dissent preference between two DMs. The innovation of this work lies in promoting the research of consensus and dissent model in GMCR in uncertain fields. Specifically, the first work reflects the DM’s real judgments over states using interval fuzzy scales and then converts them into clear values, which can be applied to conflict analysis process. More importantly, the logical stability definitions and matrix stability definitions of the consensus and dissent model in the GMCR framework under interval fuzzy preference relations (IFPRs) for two DMs are determined. In addition, we introduce the specific solving and analysis steps for resolving real life conflicts by using proposed models. Compared to existing consensus models in conflict decision-making with crisp preferences, DM’s IFPRs in new proposed GMCR provides a new way to characterize DM’s uncertainty and constructs a set of stability definitions in uncertainty decision making situations. Finally, in order to illustrate the correctness and scientificity of the new proposed GMCR model, it is applied to real-life doctor-patient disputes in China. The model’s validity and applicability are demonstrated through a case study of doctor–patient disputes in China, with the stability analysis offering practical insights for conflict resolution in uncertain environments.},
  archive      = {J_ASOC},
  author       = {Dayong Wang and Yejun Xu},
  doi          = {10.1016/j.asoc.2025.113392},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113392},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The consensus and dissent model in the graph model for conflict resolution with interval fuzzy preferences with application to doctor-patient disputes},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test-time feature caching network for cross-domain multilayer ceramic capacitors defect detection. <em>ASOC</em>, <em>179</em>, 113380. (<a href='https://doi.org/10.1016/j.asoc.2025.113380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate defect detection in Multi-Layer Ceramic Capacitors (MLCCs) is essential for maintaining the reliability of electronic devices. However, both traditional inspection methods and CNN-based models often suffer performance degradation under domain shifts, which arise from variations in image characteristics across different production lines or machines. To address this challenge, this study proposes a Test-Time Feature Caching Network (TTFCN), a novel unsupervised domain adaptation framework that enhances defect detection without requiring labeled target domain data or retraining. TTFCN introduces a feature caching mechanism and advanced augmentation strategies to learn domain-invariant features and generate accurate pseudo-labels during the adaptation phase. The proposed framework was validated on real-world datasets collected from multiple MLCC production branches. Experimental results demonstrate that TTFCN not only outperforms traditional and state-of-the-art adaptation methods in detection accuracy, but also offers faster adaptation with minimal computational cost, thereby reducing production downtime. This study presents a scalable and effective solution for defect detection and provides insights for broader applications in dynamic industrial environments.},
  archive      = {J_ASOC},
  author       = {Chia-Yu Hsu and Yi-Wei Lu},
  doi          = {10.1016/j.asoc.2025.113380},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113380},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Test-time feature caching network for cross-domain multilayer ceramic capacitors defect detection},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive embedding and structural alignment for generalized zero-shot learning. <em>ASOC</em>, <em>179</em>, 113376. (<a href='https://doi.org/10.1016/j.asoc.2025.113376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized zero-shot learning (GZSL) aims to efficiently transfer knowledge from seen to unseen classes by learning semantic attributes and visual features. However, previous works mainly suffer from two limitations. 1) Due to the lack of unseen class samples in the training process, the embedding method is faced with serious domain shift problem, which leads to the prediction bias toward the seen classes; 2) The generation method usually lacks effective constraints in the process of sample generation, and does not consider the spatial structure consistency of semantic attributes and visual features, causing the generated features lack discriminative information. In order to overcome these limitations, a generative GZSL method: Contrastive Embedding and Structural Alignment Model (CESAM) is proposed in this paper. Specifically, we firstly present the contrastive embedding module, design the contrastive loss in embedding space to perform visual-level supervision and semantic-level supervision for GZSL and promote the model to construct more accurate and discriminative embedding space. Secondly, we present the structural alignment module to bridge the correlation information lost in contrastive learning, and enable the proposed method to maintain spatial structure consistency between semantic attributes and visual features, and further optimize the learning of feature generator with reconstruction loss. Furthermore, we design the integrated GZSL module to integrate the embedding module with the constrained generative module, construct the collaborative learning between the embedding module and the constrained generative module to learn a more discriminative and generalizable model. At last, extensive experimental evaluations on four datasets demonstrate that CESAM performs state-of-the-art performance.},
  archive      = {J_ASOC},
  author       = {Chunmei He and Jing Tang and Zhengchun Ye and Kang Zhou and Shengyu Wu},
  doi          = {10.1016/j.asoc.2025.113376},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113376},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive embedding and structural alignment for generalized zero-shot learning},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sustainable circular supplier evaluation in project-driven supply chains with a fuzzy stochastic decision model under uncertainty. <em>ASOC</em>, <em>179</em>, 113370. (<a href='https://doi.org/10.1016/j.asoc.2025.113370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluation of sustainable circular suppliers in the project-driven supply chain (PDSC) is critical because project-driven companies are progressively dependent on their suppliers due to the rising trend of subcontracting and focusing on their core. Since construction operations expend 50 % of all extracted materials and generate 33 % of all waste, evaluating suppliers from a sustainable circular view is essential. Also, in many practical situations of multi-criteria decision-making (MCDM) challenges, assessments of suppliers through the sustainable circular criteria may not be fixed in some states. This kind of MCDM problem is recognized as stochastic MCDM (SMCDM). In this paper, a new scenario-based MCDM model is introduced. Also, interval-valued fuzzy soft stochastic sets (IVFSSSs) are presented to cope with SMCDM problems. The beta distribution and its median are used to tackle the uncertainty of SMCDM problems. Moreover, to use the merits of the best worst method (BWM), it extended under IVFSSS to better address the diverse possible states. The BWM method is more powerful than traditional methods, like AHP, and is used to determine criteria weights. Furthermore, the weighted distance-based approximation (WDBA) approach is extended under IVFSSS to rank suppliers. Eventually, two actual cases of the PDSC are solved to illustrate the practicality of the proposed model. Reverse logistics and energy consumption in production and recycling products have been the most important criteria in the first case study, and cost and quality in the second study. Suppliers 7 and 10 in the first case and suppliers 5 and 4 in the second case have been the best, respectively. Finally, activities 8, 10, and 12 had the lowest level of sustainable circularity in the first case and B and B1 in the second case.},
  archive      = {J_ASOC},
  author       = {Yahya Dorfeshan and Fariborz Jolai and Seyed Meysam Mousavi},
  doi          = {10.1016/j.asoc.2025.113370},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113370},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sustainable circular supplier evaluation in project-driven supply chains with a fuzzy stochastic decision model under uncertainty},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based attention model for predictive analysis in train-bridge systems. <em>ASOC</em>, <em>179</em>, 113360. (<a href='https://doi.org/10.1016/j.asoc.2025.113360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Train-Bridge Coupled Systems (TBCs) often encounter challenges such as low computational efficiency and slow processing speeds. To address these issues, deep learning methods have been introduced for rapid computation of structural responses. However, traditional deep learning approaches typically have limitations in terms of prediction length and generalization capabilities. In response to these challenges, this study presents the Graph Time Neural Network (GTN), which utilizes graph attention mechanism to effectively integrate the spatial and topological structures of complex systems. The model incorporates multi-scale time series analysis and employs multi-head attention mechanisms to capture relationships in long time series data more efficiently. Additionally, the study introduces an auxiliary learning method and a hybrid training mode, enabling the GTN model to achieve high accuracy and robust generalization in predicting long-term structural responses in TBCs. Compared to existing models, such as Graph Neural Networks (GNN), Long Short-Term Memory (LSTM), and GNBlock, the GTN model demonstrates superior performance.},
  archive      = {J_ASOC},
  author       = {Xuan Peng and Peng Zhang and Zhuo Huang and Xiaonan Xie and Zefeng Liu and Yufei Chen and Ping Xiang},
  doi          = {10.1016/j.asoc.2025.113360},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113360},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph-based attention model for predictive analysis in train-bridge systems},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement-learning-based parallel genetic algorithm for the multi-objective hot-rolling scheduling problem of wide-thick slab. <em>ASOC</em>, <em>179</em>, 113355. (<a href='https://doi.org/10.1016/j.asoc.2025.113355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hot rolling is the core process of steel production, and its scheduling problem is the key to determining the rolling rhythm. Compared with traditional hot-rolling scheduling, hot-rolling scheduling optimisation of wide-thick slabs needs to consider the characteristics of cross rolling and the influence of the reheating furnace, which significantly increase the difficulty of problem-solving. In this paper, the characteristics of cross rolling and the influence of the reheating furnace are analysed, and the hot-rolling scheduling problem of the wide-thick slab (WTS-HRSP) is mapped to a multi-objective asymmetric vehicle routing problem (MAVRP) model. Combined with the multi-objective characteristics of MAVRP, a reinforcement-learning-based parallel genetic algorithm (RPGA) was designed, which implements parallel computation of multiple populations based on the master-slave island model, designs three priority rules to initialise populations, and optimises the parameters of the genetic operation using Q-Learning. Based on the rolling data of the wide-thick slab hot-rolling mill of a Chinese iron and steel group, experiments were conducted to compare RPGA with five advanced multi-objective optimisation algorithms, and the results showed that RPGA had better convergence and stability and was more suitable for the WTS-HRSP. The findings can help optimise hot-rolling scheduling.},
  archive      = {J_ASOC},
  author       = {Zhuolun Zhang and Bailin Wang and Shuaipeng Yuan and Yiren Li and Xiqing Wang and Tieke Li},
  doi          = {10.1016/j.asoc.2025.113355},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113355},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reinforcement-learning-based parallel genetic algorithm for the multi-objective hot-rolling scheduling problem of wide-thick slab},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale attention feature deep fusion network for iris region localization and segmentation from dual-spectral iris image. <em>ASOC</em>, <em>179</em>, 113334. (<a href='https://doi.org/10.1016/j.asoc.2025.113334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precision of iris localization and segmentation is crucial for the accuracy of iris recognition. With the evolution of iris recognition technology and the diversification of application scenarios, it is imperative to improve the performance of algorithms further to address both existing and emerging challenges and issues. In response, this article introduces a lightweight multiscale attention feature deep fusion network (MA-DFNet), designed for multitask segmentation, simultaneously locating both internal and external iris boundaries and generating iris masks. The primary contributions of this work are as follows: Firstly, to provide a more robust feature representation when facing complex scenarios, a multiscale feature extractor is designed in the middle layer, and a gated attention pyramid is incorporated into the skip connections. These designs realize dynamic and adaptive feature selection and emphasis, thereby achieving precise iris region localization and segmentation. Secondly, extensive experiments were performed on iris datasets collected under two near-infrared (CASIA-Iris-M1 and CASIA.v4-distance) and two visible spectra (MICHE-I and UBIRIS.v2). These experiments demonstrate that MA-DFNet, achieves excellent performance in both segmentation and localization tasks in dual-spectrum iris images, outperforming existing state-of-the-art networks. Finally, this study raised objections to the annotations of the publicly available ground truth (GT) images, arguing that inaccurate annotations are adverse to the development of iris recognition technology. In contrast, the segmentation and localization results of MA-DFNet were more rational than GT, which gets validated by comparing recognition experiment results from both.},
  archive      = {J_ASOC},
  author       = {Shubin Guo and Ying Chen and Liang Xu and Junkang Deng and Huiling Chen and Ali Asghar Heidari},
  doi          = {10.1016/j.asoc.2025.113334},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113334},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiscale attention feature deep fusion network for iris region localization and segmentation from dual-spectral iris image},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating vision-based artificial intelligence and large language model for smart traffic light control. <em>ASOC</em>, <em>179</em>, 113333. (<a href='https://doi.org/10.1016/j.asoc.2025.113333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasingly complicated urban traffic patterns lead traffic signal control to a new trend of higher flexibility and quicker response, which becomes possible with advances in both sensor technology and artificial intelligence. Though in its early stage, existing intelligent signal controllers equipped with reinforcement learning (RL)-based feature extractor and large language model (LLM)-driven scenario understanding and decision support already demonstrate powerful data digesting ability. This study thus proposes a smart traffic light control system integrating a vision-based perception tool to extract traffic state from real-time snapshot image of the intersection, and an LLM agent controller for signal phase switching upon scenario analysis. An indicator describing the urgency for green time at phase level is defined to abstract the contextual information regarding the competition of multiple approaching traffic flows, which augments the LLM with domain-specific logical reasoning for signal control action generation, aimed at assigning green time to the flows with the most compelling needs. With a RL-based controller providing initial control decision as backup, the proposed method is able to handle both pre-trained and out-of-distribution scenarios through real-time traffic state diagnosis and knowledgeable reasoning. Simulation evaluation on different intersection layouts and vehicle compositions is conducted with horizontal comparison of five benchmarks. A decrease in average waiting time was realized by more than 5 % under normal traffic scenario and 20 % under emergency vehicle scenario, respectively. Further, comprehensive analysis was conducted to explore the applicability of the proposed method and feasibility for real-world application in unmanned aerial vehicle (UAV)-based intelligent traffic management.},
  archive      = {J_ASOC},
  author       = {Jiarong Yao and Jiangpeng Li and Xiaoyu Xu and Chaopeng Tan and Kim Hui Yap and Rong Su},
  doi          = {10.1016/j.asoc.2025.113333},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113333},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incorporating vision-based artificial intelligence and large language model for smart traffic light control},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven interval type-2 fuzzy learning controller design for tracking complex dynamical trajectories in robotic systems. <em>ASOC</em>, <em>179</em>, 113321. (<a href='https://doi.org/10.1016/j.asoc.2025.113321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking control has extensive applications in industrial robots, such as grabbing, welding, and part assembly. It is known that the tracking control of manipulator with unknown information remains a significant challenge. Thus, attaining desirable tracking performance for the manipulator in the absence of model information is a pressing issue. Furthermore, most existing control schemes feature fixed controller structures and lack learning capabilities, resulting in poor adaptability to dynamic changes in the system. To this end, a data-driven interval type-2 fuzzy learning (IT2-FL) controller is proposed in this paper. The whole learning processes consist of two phases, namely, offline pretraining and online continuous learning. An inverse dynamics model for the robotic arm is constructed based on the offline pretraining phase, and then, a fuzzy adjustment module and error compensator are integrated into the online continuous learning phase. A two-arm robotic manipulator simulation example is utilized to verify the RMSE performance metric of the proposed controller. A comparative analysis is conducted to assess the performance of the proposed controller against multiple controllers, encompassing machine-learning controller, interval type-1 fuzzy learning (IT1-FL) controller, and multilayer ensemble evolving fuzzy inference system (MEEFIS) control scheme. Finally, a manipulator platform is employed for assessing the performance of the controller.},
  archive      = {J_ASOC},
  author       = {Hainan Yang and Tao Zhao},
  doi          = {10.1016/j.asoc.2025.113321},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113321},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven interval type-2 fuzzy learning controller design for tracking complex dynamical trajectories in robotic systems},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel-evolutionary population p systems and applications in tumor segmentations. <em>ASOC</em>, <em>179</em>, 113319. (<a href='https://doi.org/10.1016/j.asoc.2025.113319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population P systems are a class of tissue-like P systems, inspired by natural computing and are applied in real-world problems. Although the model could dynamically reflect the changing state of cells in P systems, classical ones usually tree-based cells in multienvironments, ignoring the various evolutionary characteristics of elementary, sub population and population, which limit the evolutionary computing ability of population P systems. In this paper, we propose a new multilevel-evolutionary population P (MEPP) systems, containing three types of cells with corresponding new rules to describe different evolutionary mechanism. Furthermore, a special control variable is designed to accept nonimproving moves probabilistically to adaptively find the global optima. Multi-supervisions are also designed to constantly revises effective information. Besides, we applied the MEPP system to segment organs and tumors in CT/MR images. Experimental results also verify the effectiveness of MEPP system. The code of MEPP system is available at https://github.com/xuejUNC/Multilevel-evolutionary-Population-P-Systems .},
  archive      = {J_ASOC},
  author       = {Jie Xue and Qi Li and Bosheng Song and Xiyu Liu and Dengwang Li},
  doi          = {10.1016/j.asoc.2025.113319},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113319},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multilevel-evolutionary population p systems and applications in tumor segmentations},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Injecting object pose relationships into image captioning via attention capsule networks. <em>ASOC</em>, <em>179</em>, 113310. (<a href='https://doi.org/10.1016/j.asoc.2025.113310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is a fundamental bridge linking computer vision and natural language processing. State-of-the-art methods mainly focus on improving the learning of image features using visual-based attention mechanisms. However, they are limited by the immutable attention parameters and cannot capture spatial relationships of salient objects in an image adequately. To fill this gap, we propose an Attentive Capsule Network (ACN) for image captioning, which can well utilize the spatial information especially positional relationships delivered in an image to generate more accurate and detailed descriptions. In particular, the proposed ACN model is composed of a channel-wise bilinear attention block and an attentive capsule block. The channel-wise bilinear attention block helps to obtain the 2nd order correlations of each feature channel; while the attentive capsule block treats region-level image features as capsules to further capture the hierarchical pose relationships via transformation matrices. To our best knowledge, this is the first work to explore the image captioning task by utilizing capsule networks. Extensive experiments show that our ACN model can achieve remarkable performance, with the competitive CIDEr performance of 133.7% on the MS-COCO Karpathy test split.},
  archive      = {J_ASOC},
  author       = {Hong Yu and Yuanqiu Liu and Hui Li and Xin Han and Han Liu},
  doi          = {10.1016/j.asoc.2025.113310},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113310},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Injecting object pose relationships into image captioning via attention capsule networks},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selectable receptive-field graph convolution networks for skeleton-based action recognition. <em>ASOC</em>, <em>179</em>, 113305. (<a href='https://doi.org/10.1016/j.asoc.2025.113305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale spatial–temporal modeling is a mainstream approach in skeleton-based action recognition, where different order adjacency matrices are constructed to model the interrelationships between joints and thus extract multi-scale spatial features, and multiple 1D convolutions applied to consecutive frames of different time ranges to obtain rich temporal representation. However, existing methods simply concatenate the post-processing of multi-scale spatial–temporal information as output, ignoring the flexibility of this information. For certain actions (such as drinking water), joints(hand and head) that do not have physical connections in the spatial dimension usually have a stronger relationship, while certain actions (such as nodding) have directly connected joints (head and neck) that are clearly more strongly related. In the temporal dimension, the duration of keyframes for different actions (such as nodding and taking off jacket) varies. Therefore, the contribution of information at different scales on the spatial–temporal dimension should be different. In this work, we construct a selectable receptive field (SF) module that allows the network to dynamically compute and adjust the weights of various pieces of information based on multi-scale inputs. This module effectively fuses features from branches of varying importance to ensure that the most relevant receptive field information is retained at the fusion stage. We embed this module into the multi-scale spatial–temporal network separately to flexibly extract more discriminative information. Finally, extensive experiments are conducted on public datasets including NTU-RGB+D, NTU-RGB+D 120, and Northwestern-UCLA, achieving significant results.},
  archive      = {J_ASOC},
  author       = {Yinghao Gong and Jian Lu and Xiaogai Chen and Jian Zhou and Kaibing Zhang},
  doi          = {10.1016/j.asoc.2025.113305},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113305},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Selectable receptive-field graph convolution networks for skeleton-based action recognition},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pointwise fuzzy mutual information based multi-label feature selection via feature low-rank regularization. <em>ASOC</em>, <em>179</em>, 113301. (<a href='https://doi.org/10.1016/j.asoc.2025.113301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an effective solution to the dimensionality explosion of multi-label data. To assess the classification capability of features, many techniques have been developed from the view of granular computing. These methods can address the uncertainty in data but confront two primary challenges. Firstly, they rely on the predefined evaluation function and heuristic search strategies often converge to local optima. Secondly, redundant features remain difficult to be identified and eliminated. To tackle these challenges, we propose an embedded multi-label feature selection model by integrating pointwise fuzzy mutual information and feature low-rank regularization (PMILR). Redundant features tend to be highly correlated with other features, but contribute minimally to the labels. Highly correlated features exist in the same subspace, making the feature space low-rank. In this study, the low-rank structure of features is revealed to recognize potentially redundant features. Simultaneously, the pointwise fuzzy mutual information is formulated to capture the feature-label correlation. With the guidance of feature representation coefficients and feature-label correlation, a regularizer is properly designed to eliminate the effect of redundant features to labels. Theoretical analysis and experimental results validate the superiority of the developed method.},
  archive      = {J_ASOC},
  author       = {Qingwei Jia and Tingquan Deng and Ziang Zhang and Yan Wang and Changzhong Wang},
  doi          = {10.1016/j.asoc.2025.113301},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113301},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pointwise fuzzy mutual information based multi-label feature selection via feature low-rank regularization},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time series anomaly detection via temporal relationship graphs and adaptive smoothing. <em>ASOC</em>, <em>179</em>, 113298. (<a href='https://doi.org/10.1016/j.asoc.2025.113298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in time series data is crucial across diverse domains yet challenging due to complex temporal dependencies and high dimensionality. Existing methods often fail to capture a holistic view of these dependencies, overlooking subtle anomalies. This paper introduces a novel framework integrating multifaceted temporal correlation modeling with efficient dimensionality reduction for comprehensive anomaly detection. We construct three distinct Temporal Correlation Graphs (TCGs) – Similarity, Causality, and Synchronization – capturing diverse temporal dependencies beyond pairwise similarity. We seamlessly incorporate Reverse Piecewise Aggregate Approximation (RPAA) within the TCG construction, reducing dimensionality while preserving essential temporal features. Our framework uses a diverse set of statistical, graph-theoretic, and temporal metrics combined with a context-aware scoring system leveraging TCG clusters, enabling accurate detection of both point-based and event-based anomalies. Extensive evaluations on real-world and synthetic datasets demonstrate superior performance, achieving up to a 17% improvement in the F1-score compared to state-of-the-art techniques across a wide range of anomaly types. The statistical significance of these improvements is confirmed through a Wilcoxon signed-rank test.},
  archive      = {J_ASOC},
  author       = {Rongfei Ma and Yuhao Ma and Xiufeng Liu},
  doi          = {10.1016/j.asoc.2025.113298},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113298},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Time series anomaly detection via temporal relationship graphs and adaptive smoothing},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view deep support vector machines based on discriminative contrastive loss. <em>ASOC</em>, <em>179</em>, 113296. (<a href='https://doi.org/10.1016/j.asoc.2025.113296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning is a rapidly evolving direction due to its ability to learn outstanding discriminative representations. However, the two theoretically complementary models, contrastive learning and support vector machine (SVM), have never been integrated. In this paper, we propose two novel multi-view deep SVMs models based on the discriminative contrastive loss to solve the multi-view multi-class classification problem. Specifically, first, we impose the discriminative contrastive loss to learn the local structural information of each view. In addition, we propose a self-learning view-weight method to explore inter-view diversity information by assigning different view weights to each view, and explore cross-view consistency information by imposing similarity constraints on the disagreements generated by different view classifiers. Finally, two novel models take the o n e − v s − r e s t method to solve the multi-class classification problem. In the optimization problem, the back propagation method is used to implement joint learning of the entire multi-view deep model. The experimental results validate the effectiveness of the models by comparing them with state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Yanfeng Li and Junqi Lu and Xijiong Xie},
  doi          = {10.1016/j.asoc.2025.113296},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113296},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-view deep support vector machines based on discriminative contrastive loss},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI software selection for cybersecurity auditing using neutrosophic CRITIC CODAS. <em>ASOC</em>, <em>179</em>, 113295. (<a href='https://doi.org/10.1016/j.asoc.2025.113295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s companies must develop creative solutions to counter the risks of cyberattacks that make it difficult to protect their valuable information in an increasingly complex digital world. In this context, cybersecurity audits have gained importance, and companies have become especially interested in artificial intelligence (AI)-based cybersecurity audit tools. On the other hand, the selection of AI software includes multiple criteria and alternatives, and decision experts may have uncertainty in their linguistic evaluations. In this study, a new neutrosophic CRiteria Importance Through Intercriteria Correlation (CRITIC) integrated COmbinative Distance-based ASsessment (CODAS) methodology is proposed for selecting AI software for cybersecurity auditing. The importance weights of the criteria are directly calculated with the CRITIC method, and the alternatives are ranked with the CODAS approach. The uncertainty of decision experts is modeled with neutrosophic sets through truth, indeterminacy, and falsity degrees. The study includes sensitivity analyses for criterion and decision expert weights, as well as a comparative study with rank correlation analysis. Implications and discussions, limitations, and future research avenues are also given in the study.},
  archive      = {J_ASOC},
  author       = {Fatih Sahin and Akin Menekse and Selman Yilmaz},
  doi          = {10.1016/j.asoc.2025.113295},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113295},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AI software selection for cybersecurity auditing using neutrosophic CRITIC CODAS},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Popularity-aware dynamic graph neural collaborative filtering with local-global convergence. <em>ASOC</em>, <em>179</em>, 113289. (<a href='https://doi.org/10.1016/j.asoc.2025.113289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic graphs are extensively used to model evolving relationships among individuals in diverse contexts, notably in scenarios like social networking platforms, e-commerce systems, and biomolecular networks. Dynamic graph representation learning aims to derive efficient node embeddings that accurately reflect the evolving characteristics of dynamic graphs. However, current approaches commonly depend on rough time series forecasting, neglecting the dynamic imbalance in network topology and the complex collaboration between nodes. To address this issue, this paper proposes a P opularity-aware D ynamic graph neural C ollaborative F iltering with local–global convergence algorithm called PDCF. The algorithm can adaptively sample the graph and assign exponentially decaying weights to key nodes, while considering both direct and indirect relationships between nodes, effectively addressing the dynamic imbalance in the evolving graph topology and the complex collaborative relationships among nodes. We evaluated PDCF on three benchmark datasets: Wikipedia, Reddit, and LastFM. It achieves an average 8.97% improvement in Mean Reciprocal Rank (MRR) and a 2.39% increase in Recall compared to the next-best methods. Robustness and ablation studies further validate the effectiveness of our approach and highlight the positive contributions of each module.},
  archive      = {J_ASOC},
  author       = {Xiaoxue Yang and Hui Zhou and Zhongying Zhao},
  doi          = {10.1016/j.asoc.2025.113289},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113289},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Popularity-aware dynamic graph neural collaborative filtering with local-global convergence},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep domain-adversarial anomaly detection with partial gradient reversal. <em>ASOC</em>, <em>179</em>, 113285. (<a href='https://doi.org/10.1016/j.asoc.2025.113285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial settings, outliers can significantly affect equipment stability and potentially jeopardize system’s regular operation and performance. Especially when confronted with new equipment or operating conditions, it becomes imperative to establish a precise model with a limited dataset to swiftly and accurately detect and manage outliers. To tackle this issue, we introduce a transfer learning-based approach aimed at quickly adapting to novel environments and constructing efficient anomaly detection models. This strategy merges transfer learning with conventional anomaly detection techniques to create resilient models within a weakly supervised framework. In contrast to conventional methods disregarding unknown outliers, our approach incorporates a Gradient Partial Reversal technique, employing a domain adversarial mechanism to gently segregate outliers at a distinct level from the anomaly detection algorithms. This strategy yields training outcomes comparable to supervised models. To validate the efficacy of our model, we conducted experiments across three scenarios: digit image detection (utilizing the MNIST and USPS datasets), object recognition (employing the Office–Home dataset), and rolling bearing anomaly detection. Our results show that the proposed algorithm significantly outperforms existing state-of-the-art methods in terms of detection accuracy and robustness.},
  archive      = {J_ASOC},
  author       = {Jingkai Chi and Zhizhong Mao},
  doi          = {10.1016/j.asoc.2025.113285},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113285},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep domain-adversarial anomaly detection with partial gradient reversal},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent optimization of e-commerce order packing using deep reinforcement learning with heuristic strategies. <em>ASOC</em>, <em>179</em>, 113283. (<a href='https://doi.org/10.1016/j.asoc.2025.113283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of e-commerce has intensified demands for efficient logistics, particularly in optimizing three-dimensional bin packing (3D-BPP) to balance space utilization, operational costs, and sustainability. Traditional methods often fail to address the dynamic, multi-constrained nature of e-commerce orders, which involve diverse item combinations, real-time decision-making, and complex practical constraints. This study proposes a hybrid framework that integrates deep reinforcement learning (DRL) with heuristic strategies to tackle these challenges. We first formulate a comprehensive mathematical model for 3D-BPP that explicitly incorporates rotation, boundary, and non-overlapping constraints. Building on this foundation, we develop a heuristic strategy system with five operator components for bin selection, item grouping, packing sequence, position selection, and orientation determination. To enhance adaptability, we introduce two DRL algorithms: the Order Packing Optimization DRL (OPO-DRL) for dynamic item sequencing and the Packing Combination Strategy DRL (PCS-DRL) for adaptive operator selection. The hybrid framework synergizes DRL’s learning capabilities with heuristic efficiency, enabling real-time adjustments to varying order patterns and bin specifications. Experimental validation using real-world data from JD.com demonstrates significant improvements, achieving an average packing rate of 68.60% with computation times of 0.16 s per order, outperforming state-of-the-art methods. Statistical analysis confirms significant improvements in both solution quality and computational efficiency compared to existing approaches. This work bridges theoretical optimization with operational realities, providing a scalable solution for modern warehouse automation and intelligent logistics systems.},
  archive      = {J_ASOC},
  author       = {Kaibo Liang and Man Shan and Huwei Liu and Jianglong Yang and Chenxi Gu and Xiangyu Yin},
  doi          = {10.1016/j.asoc.2025.113283},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113283},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent optimization of e-commerce order packing using deep reinforcement learning with heuristic strategies},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A particle swarm optimization-based ensemble metaheuristic for long-term transmission network expansion planning. <em>ASOC</em>, <em>179</em>, 113282. (<a href='https://doi.org/10.1016/j.asoc.2025.113282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To significantly enhance the exploration and exploitation capabilities of particle swarm optimization, it is beneficial to hybridize complementary algorithms, integrate effective methods at specific stages, and trigger a ‘bail-in’ mechanism when encountering a local optimum. In this study, a Particle Swarm Optimization-Based Hybrid Framework (PSO-BHF) is proposed. The framework includes a probability calculation method that considers particle improvement after using Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to promote the convergence rate. Additionally, an opposition-based learning method is integrated to extend search regions and maintain particle diversity if the search becomes stagnant, while a Success History Intelligent Optimizer (SHIO) is employed to prevent premature convergence. Finally, the Sequential Quadratic Programming (SQP) method is used to enhance exploitation around the best particle in the later stages of the evolutionary process. The novel framework was evaluated on CEC2017 benchmark functions and compared with 15 state-of-the-art PSO-based variants and 11 non-PSO-based algorithms. Ablation tests illustrate the effectiveness of individual mechanisms and their combinations. The framework is also applied to a real-world long-term Transmission Network Expansion Planning (TNEP) problem, which was released by GECCO 2023 and IEEE CEC 2023 for their joint competition on evolutionary computation in the energy domain. Experimental results demonstrate the impressive performance of the proposed framework compared to PSO-based variants and its effectiveness in the TNEP problem. However, it still needs time to catch up with the top tier of metaheuristics, specifically Differential Evolution (DE)-based algorithms.},
  archive      = {J_ASOC},
  author       = {Libin Hong and Guodong Wang and Ruibin Bai},
  doi          = {10.1016/j.asoc.2025.113282},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113282},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A particle swarm optimization-based ensemble metaheuristic for long-term transmission network expansion planning},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective evolutionary neural architecture search for medical image analysis using transformer and large language models in advancing public health. <em>ASOC</em>, <em>179</em>, 113279. (<a href='https://doi.org/10.1016/j.asoc.2025.113279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of medical imaging data in modern healthcare networks demands sophisticated automated analysis methods that can maintain high accuracy while operating efficiently at scale. Current approaches using transformers and large language models (LLMs) face challenges balancing computational requirements with diagnostic precision across diverse healthcare settings. This paper presents TransMed-NAS (transformer medical neural architecture search), a multi-objective evolutionary neural architecture search framework that automatically discovers efficient hybrid architectures by integrating transformers and LLMs for medical image segmentation. Our approach leverages evolutionary computation to optimize segmentation accuracy and computational efficiency while incorporating medical domain knowledge through LLM guidance. The framework introduces several innovations: a hierarchical channel selection strategy that preserves clinically relevant features, a weight entanglement mechanism that accelerates architecture search through intelligent knowledge transfer, and a surrogate model acceleration technique that reduces computational overhead while maintaining reliability. Experimental results on the ISIC 2020 dataset demonstrate TransMed-NAS’s superior performance compared to state-of-the-art methods. Our small model variant achieves competitive accuracy (0.934 Dice score) with only 0.82M parameters, while our large variant establishes new benchmarks (0.947 Dice score) with significantly reduced computational requirements. Ablation studies confirm the effectiveness of each component, particularly highlighting how LLM integration enhances architecture search efficiency and clinical relevance. These results demonstrate TransMed-NAS’s potential to advance automated medical image analysis in resource-diverse healthcare settings, making sophisticated diagnostic capabilities more accessible to underserved communities.},
  archive      = {J_ASOC},
  author       = {Yu Sun and Xianglin Zhang and Liang Dong and Ning Liu},
  doi          = {10.1016/j.asoc.2025.113279},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113279},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective evolutionary neural architecture search for medical image analysis using transformer and large language models in advancing public health},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy-based multimodal approach for interpretable fake news detection. <em>ASOC</em>, <em>179</em>, 113277. (<a href='https://doi.org/10.1016/j.asoc.2025.113277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news has become a significant threat to society, exacerbated by the rapid growth of multimedia content on social media platforms. While several existing approaches focus on integrating textual and visual data for fake news detection, they often rely on traditional fusion techniques such as concatenation or addition, which suffer from two primary issues, i.e. limited cross-modal feature fusion, and inadequate detection of inconsistencies between text and images. To address these challenges, this paper proposes a novel fuzzy-based multimodal fake news detection framework that integrates text, images, and their corresponding semantic relationships for more accurate and interpretable results. The model combines a Long Short-Term Memory for text analysis, an Adaptive Neuro-Fuzzy Inference System for image classification, a similarity analysis to check the coherence between text and image, and a Fuzzy Inference System to reason about the final outcome. This multimodal fusion allows the system to detect subtle inconsistencies that may signal the presence of fake news. Furthermore, this framework produces interpretable results by describing the particular contributions of each modality (text, picture, and semantic consistency) to the final categorization. Extensive experiments on three publicly available datasets (Twitter, Buzzfeed, and PolitiFact) demonstrate that the proposed method significantly improves fake news detection performance, achieving superior accuracy, precision, recall, and F1 scores compared to state-of-the-art methods. The study is completed by a formal analysis of the Fuzzy Inference System, to support a brief discussion on its stability.},
  archive      = {J_ASOC},
  author       = {Tayasan Milinda H. Gedara and Vincenzo Loia and Stefania Tomasiello},
  doi          = {10.1016/j.asoc.2025.113277},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113277},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy-based multimodal approach for interpretable fake news detection},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty measurement for hybrid data using KNN-neighbourhood rough set model: Application to attribute reduction based on overlap degree. <em>ASOC</em>, <em>179</em>, 113276. (<a href='https://doi.org/10.1016/j.asoc.2025.113276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighbourhood rough set ( N -rough set) model is extensively applied to work with numerical data. However, most N -rough set models cannot effectively classify categories when they are used to depict classification ability. k -nearest neighbour rule ( K N N rule) is an important classification technique. This paper introduces K N N -neighbourhood rough set model by combining N -rough set model with K N N rule, investigates uncertainty measurement (UM) for hybrid data using this model, and considers its application to attribute reduction based on overlap degree. First, a specific distance function is constructed to describe the difference between two information values with respect to each attribute in a hybrid information system (HIS), the distance matrix is defined to establish the neighbourhood relation on the object set, and the neighbourhood classes where a variable parameter is selected to adjust the size of the neighbourhood classes are built. N -rough set model is introduced using the neighbourhood classes. The k -nearest neighbour in an HIS is then proposed on the basis of K N N rule, and K N N -neighbourhood rough set model is presented by combining N -rough set model with the k -nearest neighbour. On the basis of K N N -neighbourhood rough set model, the information granules in an HIS are constructed, and four UMs in an HIS are presented. Next, an experiment is carried out to select the UM with the optimal performance. Furthermore, an attribute reduction algorithm is designed using the selected UM. Finally, the designed algorithm is evaluated using a series of experiments implemented on real-world datasets. The experimental results demonstrate the statistical advantages of the designed algorithm over the other six reduction algorithms.},
  archive      = {J_ASOC},
  author       = {Bozhan Li and Qin Huang and Zhaowen Li and Yonghua Lin},
  doi          = {10.1016/j.asoc.2025.113276},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113276},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertainty measurement for hybrid data using KNN-neighbourhood rough set model: Application to attribute reduction based on overlap degree},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive inpainting transformer for semantically coherent image completion. <em>ASOC</em>, <em>179</em>, 113271. (<a href='https://doi.org/10.1016/j.asoc.2025.113271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional methods for restoring unknown regions in images often face challenges arising from ambiguous semantics and unrealistic structural reconstructions. These issues primarily stem from the rigid frameworks that struggle to extract reliable semantic information from corrupted inputs. To overcome these limitations and improve model adaptability, we propose a flexible model that leverages dynamic feature aggregation and mask-aware guidance. Our approach introduces an integrated architecture that synergizes Transformers and CNNs, endowed with dynamic learning mechanisms through image and feature-level predictive filtering. This innovative architecture is further enhanced by a mask-guided, dual-way dynamic convolution designed to address discordant decoding issues. The effectiveness of these novel components is demonstrated through comprehensive evaluations on various benchmark datasets, consistently achieving superior performance in reconstructing coherent and plausible structures, particularly in cases of severe corruption.},
  archive      = {J_ASOC},
  author       = {Shuyi Qu and Jun Wang and Huixin Chen and Yirong Ma and Shenglin Peng},
  doi          = {10.1016/j.asoc.2025.113271},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113271},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive inpainting transformer for semantically coherent image completion},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Link prediction for attribute and structure learning based on attention mechanism. <em>ASOC</em>, <em>179</em>, 113268. (<a href='https://doi.org/10.1016/j.asoc.2025.113268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The link prediction problem within complex network analysis focus on accurately inferring future relationships between unconnected node pairs in networks. In real-world scenarios, many relationships can find effective expression through network representation learning, making link prediction pivotal in solving various practical problems. However, existing link prediction methods often exhibit an over-reliance on network structure information or struggle to effectively integrate network structure with node attribute information. Moreover, the current limitations of GPU memory pose challenges for deploying graph neural network (GNN) methods at scale. To address these challenges, this paper introduces attribute and structure learning based on attention mechanism(ASLAM), a novel link prediction framework that amalgamates graph neural networks with attention mechanisms. ASLAM adeptly extracts and integrates both node attribute and structural information, constructing semantic and structural node representations through attention mechanisms and facilitating predictions via adaptive merging. Additionally, to mitigate GPU memory consumption, ASLAM innovatively incorporates the subgraph history embedding method, ensuring stable performance with either constant or linear increases in memory consumption. Experimental results show ASLAM’s superiority over baseline methods across nine real datasets in link prediction tasks. The AUC and AP metrics witness enhancements ranging from 0.11% to 3.03% and 0.15% to 3.32%, respectively, underscoring its promise and applicability.},
  archive      = {J_ASOC},
  author       = {Renjuan Nie and Guoyin Wang and Qun Liu and Chengxin Peng},
  doi          = {10.1016/j.asoc.2025.113268},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113268},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Link prediction for attribute and structure learning based on attention mechanism},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-informed multiplication convolution generalization network for interpretable equipment diagnosis under unknown speed domains. <em>ASOC</em>, <em>179</em>, 113263. (<a href='https://doi.org/10.1016/j.asoc.2025.113263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalization performance and interpretability of intelligent fault diagnosis methods under unknown speed domains are crucial concerns in real industry practice. However, existing solutions seldom address both issues simultaneously, restricting their development prospects. Motivated by these challenges, this study puts forward a signal-processing-collaborated deep learning architecture—knowledge-informed multiplication convolution generalization network (KI-MCGN), which is composed of three layers, called adaptive mode capturer (AMCer), prior knowledge pooler (PKPer) and classifier. Informed by the mode response characteristics of fault vibration signals, AMCer first tailors several speed-fused multiplication filtering kernels (SF-MFKs) for adaptive mining of fault-related modes. To improve the generalization capability, the center frequency and bandwidth coefficient of SF-MFKs are no longer defined directly, but are innovatively fitted by multiple trainable coefficients with regard to the speed information. This novel speed fusion strategy allows SF-MFK to not only learn the mapping relationship between the speed information and the distribution of fault-included modes, but also to autonomously adjust its modal filtering scale in unknown speed domains. In light of the excellent comprehensibility of prior indicators in characterizing the health status of equipment, a novel pooler named PKPer is presented subsequently. It pools each extracted mode into 12 frequency-domain modal prior indicators (MPIs). Eventually, two dense layers are adopted as the classifier to output the ultimate decision. In particular, considering the distribution difference of mode features across different speed domains, local-domain generalization is further integrated to assist the model extract generalized features. The comparison results from two experimental cases demonstrate that the proposed KI-MCGN architecture outperforms the other eight state-of-the-art approaches and three ablation models. Meanwhile, comprehensive visualization analysis not only validates the modal filtering potency of SF-MFKs under unknown speed domains, but also explores the guiding meaning of MPIs for the final diagnosis. It can be also foreseen that the proposed KI-MCGN framework is expected to provide reliable and explainable intelligent decision-making for equipment maintenance under unknown speed domains.},
  archive      = {J_ASOC},
  author       = {Rui Liu and Xiaoxi Ding and Benyuan Ye and Yuanyuan Xu and Jiahai Huang and Hongyu Lv},
  doi          = {10.1016/j.asoc.2025.113263},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113263},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge-informed multiplication convolution generalization network for interpretable equipment diagnosis under unknown speed domains},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subspace clustering with self-representation sparsification and regression via connected components. <em>ASOC</em>, <em>179</em>, 113262. (<a href='https://doi.org/10.1016/j.asoc.2025.113262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering is a well-studied extension of traditional clustering for the recent decades, aiming at uncovering cluster structures of high-dimensional data as subspaces. Spectral-based subspace clustering is one of the most widely researched subfields, which first constructs a self-representation matrix to represent each data point with a linear combination of other points. Sparsity and connectivity of the self-representation matrix are two critical properties for effective subspace clustering, and a variety of algorithms try to improve either of them, or to balance between them, which however is challenging. In this article, we propose an algorithm to post-process the self-representation matrix to increase sparsity while maintaining connectivity, with the help of a specifically designed type of connected components. Points in the same connected component indicate strong connections and those in different components suggest weak connections. By retaining strong connections but removing weak ones, the self-representation matrix is sparsified, and connectivity is also not destroyed because of the consideration of connected components. Representation coefficients are further adjusted through a Ridge Regression for each point, to better reflect the relations between points. Extensive experiments on 11 real-world datasets demonstrate the effectiveness of the proposed algorithm in sparsifying the self-representation matrix, and the noticeable superior performance of subsequent clustering against the state-of-the-art baselines (achieving the best results on 9 datasets), with only a small amount of post-processing cost.},
  archive      = {J_ASOC},
  author       = {Zhiguo Long and Rui Guan and Yi Gao and Hua Meng and Rong Luo},
  doi          = {10.1016/j.asoc.2025.113262},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113262},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Subspace clustering with self-representation sparsification and regression via connected components},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep ensemble-based framework for the prediction of oral cancer through histopathological images. <em>ASOC</em>, <em>179</em>, 113258. (<a href='https://doi.org/10.1016/j.asoc.2025.113258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early diagnosis and treatment for cancer can save lives and is a primary worldwide health concern. This is also true for Oral Cancer (OC), highlighting how crucial early intervention is. OC, specifically Oral Squamous Cell Carcinoma (OSCC), is a complex malignancy with a high mortality rate. One group of diseases that dentists can diagnose and treat is OSCC, which are found in the oral cavity. Deep Learning (DL) algorithms have shown promise in the identification and prediction of OC due to their ability to analyze large datasets and detect subtle patterns in buccal tissue. This work introduces a lightweight ensemble-based DL network for the prediction of OC. The proposed methodology incorporates various DL techniques such as Convolutional Neural Network (CNN), Bidirectional Long Short Term Memory (Bi-LSTM), and Bidirectional Gated Recurrent Unit (Bi-GRU) for extraction of deep features. Here, we have developed the CNN and Bi-LSTM blocks to extract the spatial and contextual features from the histopathological images. Also, the proposed Bi-GRU block increases classification performance by exploiting spatial and sequential features to better capture dependencies within image data when paired with CNNs. After the deep features are extracted from the proposed CNN, Bi-LSTM, and Bi-GRU blocks, they are amalgamated to form a new set of deep features. To forecast the probability of OC among unnoticeable patients during screening, this study uses a soft-voting ensemble classifier to classify after the deep features have been extracted from the proposed deep network based on five baseline classifiers including Logistic Regression (LR), Decision Tree (DT), k -Nearest Neighbors ( k -NN), Support Vector Machine (SVM), and Stochastic Gradient Descent (SGD). For experimental verification here, we have used the two publicly available OC image datasets, which are the Histopathological Oral Cancer Detection Dataset (HOCDD) and the Histopathological Image Repository of Normal Epithelium of Oral Cavity and OSCC Dataset (HRNEOCD). By effectively leveraging deep features, the proposed ensemble-based classification model achieves remarkable accuracy scores of 98.34% for the HOCDD dataset and 97.89%, 98.76% for the HRNEOCD Set-1, Set-2, dataset, showcasing its robust predictive capabilities and its potential for reliable OC prediction. This study highlights the importance of integrating DL algorithms with conventional classifiers to enhance the performance of OC prediction models.},
  archive      = {J_ASOC},
  author       = {Pradip Dhal and Debahuti Mishra and Buddhadeb Pradhan},
  doi          = {10.1016/j.asoc.2025.113258},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113258},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep ensemble-based framework for the prediction of oral cancer through histopathological images},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A niching-based nondominated sorting for multimodal multiobjective optimization with local pareto fronts. <em>ASOC</em>, <em>179</em>, 113223. (<a href='https://doi.org/10.1016/j.asoc.2025.113223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multimodal multiobjective optimization problems (MMOPs), there may be local Pareto optimal solutions (PSs). Finding both global PS and local PS simultaneously can provide additional options for decision makers. However, since individuals on the global Pareto front (PF) dominate individuals on the local PF, traditional multimodal multiobjective evolutionary algorithms (MMEAs) cannot effectively handle MMOPs with local PF (MMOPLs). In this paper, we improve our previous dynamic-niching-based Pareto domination (DNPD) approach to search for local Pareto optimal solutions. The new version is called niching-based non-dominated sorting (NNSL). There are two improvements to NNSL. First, the dynamic niche is changed to a fixed niche to increase the diversity of the population. Second, individuals with poor convergence in the non-dominated layer are deleted to ensure the convergence of the population. NNSL can be combined with Pareto dominance-based algorithms. We apply NNSL to solve MMOPLs (MMF_e and IDMP_e series). Experiments show that NNSL can help traditional MMEAs find global PF/PS and local PF/PS simultaneously.},
  archive      = {J_ASOC},
  author       = {Qi Deng and Juan Zou and Shengxiang Yang and Yuan Liu and Fan Yu and Tianbin Xie and Jinhua Zheng},
  doi          = {10.1016/j.asoc.2025.113223},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {113223},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A niching-based nondominated sorting for multimodal multiobjective optimization with local pareto fronts},
  volume       = {179},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature fusion-based hand gesture classification with time-domain descriptors and multi-level deep attention network. <em>ASOC</em>, <em>178</em>, 113375. (<a href='https://doi.org/10.1016/j.asoc.2025.113375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In conventional human-robot interaction (HRI), it is difficult to provide adaptability by located systems in the human body. Surface Electromyography (sEMG) signals have the potential to meet adaptability in HRI by directly representing movements, and classifying hand gestures with sEMG can be an effective solution to meet the increasing needs of these applications. In this paper, a hybrid and multi-scale convolutional neural network (CNN) model is proposed to obtain an efficient sEMG-based classification approach of human hand gestures. The proposed method includes an effective feature extraction process, including spectral moments, sparseness, irregularity factor, Teager–Kaiser energy, Shannon entropy, Katz fractal dimension, and Higuchi’s fractal dimension, and waveform length. The obtained features are then converted to RGB images. The designed network is built on multi-scale convolutional blocks with residual learning and convolutional blocks, including the CBAM to improve the network performance by focusing on channel and spatial features. Furthermore, a pyramid non-pooling local block is utilized at the end of the network to learn more powerful features and their correlations. Five comprehensive publicly available datasets are evaluated in the experiments, and the obtained results are compared with the benchmark CNN models and network variations with different attention mechanisms. In the comparative evaluations, the CBAM achieves a classification accuracy between 84.62 % and 97.56 % while other attention mechanism results give accuracy values between 82.88 % and 97.17 %. The experiments show that the proposed method gives more accurate and robust classification performance compared with other variations and benchmark models.},
  archive      = {J_ASOC},
  author       = {Ömer Faruk Alçin and Deniz Korkmaz and Hakan Acikgoz},
  doi          = {10.1016/j.asoc.2025.113375},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113375},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature fusion-based hand gesture classification with time-domain descriptors and multi-level deep attention network},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusion channel interaction attention network for water jet detection in firefighting robots. <em>ASOC</em>, <em>178</em>, 113364. (<a href='https://doi.org/10.1016/j.asoc.2025.113364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firefighting robots play a critical role in fire suppression. Ensuring the water stream precisely hits the target during autonomous fire extinguishing is of paramount importance. By visually detecting the landing point of the water jet, closed-loop control of the extinguishing process can be achieved. However, achieving accurate jet landing point localization in complex environments, such as changes in ambient lighting, jet end divergence, and jet breakup, presents a challenging task. To address this, we propose a novel CIA-YOLOX (Channel Interaction Attention–You Only Look Once) model for the precise identification of water jet landing points in firefighting robots using unmanned aerial vehicle (UAV) visual information. First, the model introduces the Triplet Attention (TA) mechanism to capture feature dependencies across different dimensions, enriching feature information. Second, a module named Coordinate Attention Transformer (CA-Trans) is designed to establish long-range dependencies between directional feature vectors, enabling the extraction of precise positional information critical for accurate impact point prediction. Additionally, a Dual-branch Channel Interactive Attention Fusion (DCIAF) module is proposed to enhance feature representation capabilities by facilitating feature complementation through semantic modeling of channel interactions. Experimental results indicate that the proposed model surpasses current state-of-the-art methods in performance while maintaining low computational costs, confirming its efficacy. This approach enhances the robot's ability to perceive complex environments, providing valuable insights for implementing firefighting actions in real-world scenarios.},
  archive      = {J_ASOC},
  author       = {Juxian Zhao and Wei Li and Jinsong Zhu and Zhigang Gao and Lu Pan and Zhongguan Liu},
  doi          = {10.1016/j.asoc.2025.113364},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113364},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fusion channel interaction attention network for water jet detection in firefighting robots},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrigendum to “Pigmented skin disease classification via deep learning with an attention mechanism” [Appl. soft comput. 170 (2025), 112571]. <em>ASOC</em>, <em>178</em>, 113361. (<a href='https://doi.org/10.1016/j.asoc.2025.113361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Jinbo Chen and Qian Jiang and Zhuang Ai and Qihao Wei and Sha Xu and Baohai Hao and Yaping Lu and Xuan Huang and Liuqing Chen},
  doi          = {10.1016/j.asoc.2025.113361},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113361},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Pigmented skin disease classification via deep learning with an attention mechanism” [Appl. soft comput. 170 (2025), 112571]},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Short-time photovoltaic power forecasting based on informer model integrating attention mechanism. <em>ASOC</em>, <em>178</em>, 113345. (<a href='https://doi.org/10.1016/j.asoc.2025.113345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise Photovoltaic Power Generation Forecasting (PVGF) is significant for achieving reliable power supply, optimizing energy scheduling, and responding to changing energy market demand for sustainable development. However, Photovoltaic Power (PV) is vulnerable to changes in solar radiation levels and temperature, then result in electricity generation fluctuations. To further enhance the precision of PVGF, we propose a new short-term PVGF method based on Informer model integrating attention mechanism. Firstly, Locally Weighted Scatterplot Smoothing (LOWESS) is introduced to preprocess data, enhancing the stability of the input data. Secondly, Feature Engineering (FE) is used for feature screening. Thirdly, Informer model is improved, termed as Attention-Informer-Attention (AT-Informer-AT) model. Specifically, Attention mechanism (AM) layer is added to the encoder and decoder of Informer model respectively, allowing the model to flexibly adjust the attention to different time series data and effectively capture important patterns in the PV data, thereby enhancing prediction performance and generalization ability. Eventually, the novel prediction approach’s efficiency is confirmed through analyzing the cases of two different power stations in DKASC area, Alice Springs, Australia and Xuhui District, Shanghai, China. The Experimental results demonstrate that the proposed method superiors other models, with the best prediction accuracy and generalization ability.},
  archive      = {J_ASOC},
  author       = {Weijie Yu and Yeming Dai and Tao Ren and Mingming Leng},
  doi          = {10.1016/j.asoc.2025.113345},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113345},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-time photovoltaic power forecasting based on informer model integrating attention mechanism},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Signature kernel ridge regression time series model: A novel approach for hydrological drought modeling using multi-station meteorological drought information. <em>ASOC</em>, <em>178</em>, 113343. (<a href='https://doi.org/10.1016/j.asoc.2025.113343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of growing environmental challenges and the need for sustainable water resource management, hydrological drought prediction has gained prominence as a critical issue. Existing artificial intelligence and time series-based models for hydrological drought indices have traditionally been established using streamflow data. This study gives a significant progress in hydrological drought modeling through the introduction of the Signature Kernel Ridge Regression (SKRR) time series model. Instead of directly using rainfall and runoff data to develop a rainfall-runoff (RR) model, the Standardized Precipitation Evapotranspiration Index (SPEI) values in neighbor meteorological stations serve as inputs for estimating the Streamflow Drought Index (SDI) in target hydrometric stations, considering the 3-, 6-, and 12-month moving average time windows. The objective of this study is to enhance hydrological drought modeling by integrating soft computing techniques that effectively handle multivariate and irregular time series. The efficacy of the SKRR is compared with the well-established Generalized Regression Neural Network (GRNN), Random Forest (RF), and Auto Regressive Integrated Moving Average model with eXogenous input (ARIMAX). The findings indicate that SKRR is capable of precisely estimating SDI in three hydrometric stations using meteorological drought information from 14 stations, outperforming the GRNN, RF and ARIMAX models. The enhanced performance of the SKRR time series model stems from the utilization of a new and effective signature kernel which can be utilized for the study of irregularly sampled, multivariate time series in addition to be applicable to time series of different temporal spans while being a positive-definite kernel, facilitating usage in the Hilbert space. The novel drought based-RR model established by SKRR utilized various external stations’ meteorological drought indices to compute the hydrological drought indices in target stations not only enhances the modeling capability but also progress our understanding of drought dynamics by showcasing the power of soft computing in handling environmental uncertainty. Furthermore, it offers visions for developing of adaptive and resilience strategies to lessen the hazards caused by drought phenomenon.},
  archive      = {J_ASOC},
  author       = {Mir Jafar Sadegh Safari and Shervin Rahimzadeh Arashloo and Babak Vaheddoost},
  doi          = {10.1016/j.asoc.2025.113343},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113343},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Signature kernel ridge regression time series model: A novel approach for hydrological drought modeling using multi-station meteorological drought information},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path-planning algorithm for small environmental surveillance unmanned surface vehicles. <em>ASOC</em>, <em>178</em>, 113342. (<a href='https://doi.org/10.1016/j.asoc.2025.113342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ports are essential hub facilities that provide support for economic development. However, the construction, development, and operation of ports increase the risk of environmental pollution in marine areas. Small environmental surveillance unmanned surface vehicles (ESUSVs) are being deployed to monitor port environments and prevent pollution. This study proposes a bidirectional elastic force contraction algorithm (BEFCA) and a Lévy flight weighted whale optimization (LFWWOA) and BEFCA hybrid algorithm (LFWWOA-BEFCA) to solve the path planning problem of ESUSVs. BEFCA solves the slow convergence and unsmooth path-characteristic problem of the elastic force contraction algorithm (EFCA) by employing a bidirectional search strategy and ship kinematics to smoothen the turning points in the path, respectively. LFWWOA uses a Lévy flight-based strategy in the global exploration phase of the whale optimization algorithm (WOA) to increase the solution diversity and improves the global and local search performance by modifying the coefficient calculation method and adding adaptive weighting coefficients. Thirteen benchmark functions were selected for the LFWWOA optimization performance experiments and compared with other intelligence algorithms. The results demonstrate that the proposed algorithm achieved the best global performance. Therefore, LFWWOA was used to optimize the BEFCA parameters, which resulted in higher-quality planned paths. Simulation experiments of real scenarios and complex environments showed that the path lengths and algorithm runtimes of BEFCA and LFWWOA-BEFCA outperformed those of the state prediction rapidly exploring random trees (spRRT) and spRRT-informed algorithms, respectively. The planned paths are consistent with the motion characteristics of ESUSVs, which can be used directly for tracking. The findings of this study indicate that shorter travel paths can be planned for ESUSVs in harbors for environmental monitoring, effectively solving the difficulty of tracking the paths of ESUSVs, and reducing energy consumption during the travel process.},
  archive      = {J_ASOC},
  author       = {Zhenyang Wang and Ping Yang and Diju Gao and Chunteng Bao},
  doi          = {10.1016/j.asoc.2025.113342},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113342},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Path-planning algorithm for small environmental surveillance unmanned surface vehicles},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive analysis of bitcoin volatility forecasting using time-series econometric models. <em>ASOC</em>, <em>178</em>, 113339. (<a href='https://doi.org/10.1016/j.asoc.2025.113339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world of cryptocurrency has expanded rapidly over the last ten years, with the most recent advancements witnessed in the last few years as many individuals have realized the importance of storing digital assets online. According to Twitter statistics, there are roughly 1500 tweets on Bitcoin alone per hour, which lends credence to this claim. As a consequence, investors are eager to learn how to make profitable cryptocurrency trades and investments, and the fundamental idea behind digital currencies is growing in acceptance and understanding. This study investigates the notable inefficiencies in the several research efforts that have attempted to create algorithms that can accurately forecast price fluctuations in the Bitcoin market. This work compares different econometric models based on Root Mean Squared Error (RMSE) and Root Mean Squared Percentage Error (RMSPE). The RMSE score of our proposed Threshold Autoregressive Conditional Heteroskedasticity (TARCH) model is 0.065, and an RMSPE score of 0.197, which is minimal compared to other models. The proposed Bootstrap TARCH technique appropriate for simulating the intricate and volatile characteristics of Bitcoin returns.},
  archive      = {J_ASOC},
  author       = {Nrusingha Tripathy and Sarbeswara Hota and Debabrata Singh and Biswa Mohan Acharya and Subrat Kumar Nayak},
  doi          = {10.1016/j.asoc.2025.113339},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113339},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive analysis of bitcoin volatility forecasting using time-series econometric models},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extracting hierarchical relationships of aspects from reviews. <em>ASOC</em>, <em>178</em>, 113335. (<a href='https://doi.org/10.1016/j.asoc.2025.113335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect Based Sentiment Analysis (ABSA) attracts significant attention in recent years. Three elements of ABSA including aspect term, aspect, and domain (or entity) present a hierarchical relationships in e-commerce reviews. Extracting the hierarchical relationships can significantly enhance various applications, such as creating user profiles, identifying hierarchical topics, and visualizing review data. In this study, we proposed a framework to tackle this task, consisting of two primary components: a text adversarial autoencoder that efficiently encodes review content, and a deep network that extracts the clusters of aspect terms from review dataset and organizes them to a hierarchical structure using the Student-Teacher paradigm. Our framework also addresses the challenge of acquiring labeled training data by utilizing self-supervised learning. We evaluated the proposed framework on three public datasets and observed that it outperforms baseline models, indicating the feasibility and effectiveness of our approach.},
  archive      = {J_ASOC},
  author       = {Jiangtao Qiu and Ling Lin and Siyu Wang},
  doi          = {10.1016/j.asoc.2025.113335},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113335},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extracting hierarchical relationships of aspects from reviews},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion-based road defect detection model integrating edge information and efficient multi-scale convolution. <em>ASOC</em>, <em>178</em>, 113332. (<a href='https://doi.org/10.1016/j.asoc.2025.113332'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Roads are vital infrastructure components, and the prompt detection and repair of defects are critical for their longevity and safety. This paper introduces the Edge Efficient Multi-Scale Focusing Diffusion Network (EEFNet), a precise method for road defect detection. The Edge Information Enhancement Module (EIEM) accentuates crack contours while minimizing background noise. An Efficient Multi-Scale Convolution (EMSConv) is proposed. The EMSConv captures features across multiple scales, thereby enhancing model efficiency through reduced computational demands and parameter count. The Focusing Diffusion Pyramid Network (FDPN) collects and distributes context-rich features across various scales using a diffusion mechanism, thereby improving detection capabilities. Additionally, the Task Dynamic Align Detection Head (TADDH) facilitates parameter sharing among detection heads, which enhances classification and localization accuracy. EEFNet has demonstrated a 92.7 % accuracy rate at 126 FPS (Frames Per Second) on a road defect dataset and has proven robustness on several diverse datasets including Unmanned Aerial Vehicle Asphalt Pavement Distress Dataset (UAPD), Visual Object Classes 2007 (VOC2007), Global Road Damage Detection2022 (GRDD2022), and Vision Meets Drone 2019 (Visdrone2019). In addition, by pruning the model and deploying it onto edge computing devices, practical experiments have demonstrated that the EEFNet model has substantial practical application value.},
  archive      = {J_ASOC},
  author       = {Xueqiu Wang and Huanbing Gao and Zemeng Jia},
  doi          = {10.1016/j.asoc.2025.113332},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113332},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Diffusion-based road defect detection model integrating edge information and efficient multi-scale convolution},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-individual electrophysiological signal recognition in clivia biosensors via domain adaptation. <em>ASOC</em>, <em>178</em>, 113323. (<a href='https://doi.org/10.1016/j.asoc.2025.113323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the use of Clivia plants as biosensors for environmental monitoring and ecological protection, focusing on the analysis of electrophysiological signals generated under various stress conditions. Plants’ ability to produce real-time electrophysiological signals in response to stressors such as salinity, drought, and pest infestations presents a promising method for precision agriculture and ecological surveillance. However, a key challenge is the significant variability in plant responses and signal distributions across individual plants, which limits the generalizability of models trained on specific plant samples.To address this, we introduce DA-PlantNet, a novel model that leverages domain adaptation techniques to enhance the model's adaptability and transferability across different plant individuals. By minimizing discrepancies in feature distribution between plants, DA-PlantNet effectively differentiates and classifies electrophysiological signals from various Clivia individuals, enabling robust cross-individual classification. We collected signals from Clivia plants under varying soil moisture conditions and analyzed them using DA-PlantNet. Experimental results demonstrate that DA-PlantNet significantly outperforms traditional methods, achieving an accuracy of 95.336 %, precision of 93.853 %, recall of 95.467 %, and an F1-score of 94.047 %, underscoring its robustness and generalization capability.This research introduces a novel approach to enhancing the adaptability and transferability of plant-based biosensor models, paving the way for scalable and reliable applications in precision agriculture and environmental monitoring. DA-PlantNet offers a valuable tool for ecological protection and sustainable agricultural practices, advancing the engineering of plant-based biosensors for real-world applications.},
  archive      = {J_ASOC},
  author       = {Chenrui Liu and Ji Qi and Xiuxin Xia and Yicheng Wang and Qiuping Wang and Lingfang Sun and Hong Men},
  doi          = {10.1016/j.asoc.2025.113323},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113323},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-individual electrophysiological signal recognition in clivia biosensors via domain adaptation},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting airfoil separation bubble locations using ABCP algorithms. <em>ASOC</em>, <em>178</em>, 113309. (<a href='https://doi.org/10.1016/j.asoc.2025.113309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, new equations to predict the location parameters of the laminar separation bubble that occur in the flow over the blade/wing and negatively affect the blade/wing aerodynamic performance in unmanned aerial vehicles and wind turbines were developed first in the literature by Artificial Bee Colony Programming (ABCP) and quick ABCP (qABCP) algorithms. Data from the experimental study for NACA2415 were processed using ABCP and qABCP methods. The results of the models were also compared with the results of the XFOIL code, a numerical analysis in the literature, and an Artificial Neural Network (ANN). Even though low Reynolds numbers with more viscous effects were not given in the training data, both ABCP and qABCP algorithms successfully estimated the separation (Xs) and the reattachment points (Xr). Considering the error analysis and correlation coefficient values, it was seen that both algorithms can be used for both Xs and Xr predictions. Users/designers of the aerospace and energy industry can use to estimate Xr and Xs points for the NACA 2415 airfoil using the new equations proposed in this study at Re numbers ranging from 50,000 to 300,000, without the need for expensive and time-consuming experiments or Computational Fluid Dynamics (CFD) analysis. Furthermore, it was concluded that ABCP methods not only have the advantage of flexibly building models but are also highly competitive with other machine learning methods used in the literature for prediction, such as ANN.},
  archive      = {J_ASOC},
  author       = {İlyas Karasu and Beyza Görkemli̇ Bayram and Mustafa Serdar Genç},
  doi          = {10.1016/j.asoc.2025.113309},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113309},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting airfoil separation bubble locations using ABCP algorithms},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A Q-learning based arithmetic optimization algorithm for a multi-warehouse joint replenishment and delivery problem. <em>ASOC</em>, <em>178</em>, 113307. (<a href='https://doi.org/10.1016/j.asoc.2025.113307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joint replenishment and delivery (JRD) strategy is critical for increasing operational management efficiency and reducing costs. This study introduces a new multi-warehouse JRD model for heterogeneous products. The main goal of the JRD strategy is to minimize costs by determining the optimal replenishment intervals, delivery frequency, and basic replenishment cycle time. To address the difficulties of the JRD optimization issue, we propose the Q-learning-based arithmetic optimization algorithm (QAOA). In the QAOA framework, Q-learning is the guiding principle, making decisions based on current conditions and constantly refining search strategy via feedback mechanisms. Furthermore, an escape mechanism has been implemented to reduce the possibility of algorithmic stagnation in local optima. Experiments show that QAOA exceeds eight popular benchmark algorithms. By employing the QAOA technique, the practical JRD model has been effectively handled, resulting in considerable cost reductions in supply chain management.},
  archive      = {J_ASOC},
  author       = {Lu Peng and Sirui Wang},
  doi          = {10.1016/j.asoc.2025.113307},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113307},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A Q-learning based arithmetic optimization algorithm for a multi-warehouse joint replenishment and delivery problem},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative adversarial network based on CNN classifier predicted scores for image style transfer. <em>ASOC</em>, <em>178</em>, 113303. (<a href='https://doi.org/10.1016/j.asoc.2025.113303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, image style transfer has emerged as an increasingly popular research theme in the field of computer vision. Image style transfer seeks to convert the style of an image or mix it with other styles to produce an image with stylistic features not found in the original image. In the past, image style conversion was mainly implemented using feature conversion or filters, processes which require a significant degree of manual design, thus limiting these approaches to performing at most a single style conversion. However, with the development of deep learning technologies, an increasing number of studies have begun to break through these research challenges, achieving significant progress. The study proposes generating image data attribute tags from the classification dataset, constructing a GAN architecture for multi-style conversion tasks, and proposes a Classifier Style Scores GAN (CSS-GAN) model. First, a CNN classifier is used to train on the classification dataset. Once its stability is verified, the classifier is used to make predictions and its output layer features are extracted as attribute labels. These labels are subjected to different types of pre-processing to assess the performance difference between smoothed labels and binary classification labels. Finally, the resulting labels are used to train a multi-style transfer GAN. Experiments are conducted using a facial attribute dataset to compare the labeling method with the proposed model architecture. The results indicate that using classifier-predicted features and applying feature smoothing as attribute labels for training the GAN can effectively enhance the quality and stability of images generated for the style transfer task. Additionally, this approach allows for better control over the degree of transformation and improves the overall performance of style transfer.},
  archive      = {J_ASOC},
  author       = {L.Mary Gladence and Yi-Wei Lai and Fu-Ti Lee and Mu-Yen Chen and Hsin-Te Wu},
  doi          = {10.1016/j.asoc.2025.113303},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113303},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generative adversarial network based on CNN classifier predicted scores for image style transfer},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic prediction and control of a tunnel boring machine with a particle swarm optimization–random forest algorithm and an integrated digital twin. <em>ASOC</em>, <em>178</em>, 113294. (<a href='https://doi.org/10.1016/j.asoc.2025.113294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tunnel boring machines (TBMs) often experience attitude deviation during excavation, impacting the stability and safety of tunnel construction. Traditional attitude adjustment relies on manual adjustment, which has a lagging effect. Therefore, the study combines a digital twin platform and a hybrid intelligence algorithm to enable real-time adjustment of the shield attitude deviation. The particle swarm optimization and random forest (PSO-RF) algorithm is first used to make accurate predictions of the shield attitude. Shapley additive explanations (SHAP) is subsequently employed to identify the key construction parameters. Then, based on these parameters, a control system for the shield attitude is designed in conjunction with a digital twin (DT) technique. A case study of China's Guiyang Metro Line 3 demonstrates the following: (1) The PSO-RF model achieves high accuracy, with R² values ranging from 0.916 to 0.943 for six shield attitude targets. (2) The key shield parameters are continuously optimized and adjusted within the control range to achieve shield attitude control. (3) The digital twin system provides real-time attitude warnings and parametric inference, significantly improving TBM performance and safety. In this paper, a novel method of combining predictive modeling and the DT platform is proposed. Under the proposed intelligent method, the attitude deviation of a TBM during tunneling was significantly reduced.},
  archive      = {J_ASOC},
  author       = {Tiejun Li and Jun Liu and Xianguo Wu and Feiming Su and Yang Liu},
  doi          = {10.1016/j.asoc.2025.113294},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113294},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic prediction and control of a tunnel boring machine with a particle swarm optimization–random forest algorithm and an integrated digital twin},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot outliers classification in high-speed railway track geometry based on self-supervised transformer. <em>ASOC</em>, <em>178</em>, 113281. (<a href='https://doi.org/10.1016/j.asoc.2025.113281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {External disturbances, data transmission, sensor signal offsets and weather conditions are common sources of outliers in track geometry inspection data. The infrequent occurrence of these outliers leads to a scarcity of labeled samples, making high-accuracy few-shot classification challenging with traditional supervised learning methods. To address this, we propose a method for the few-shot classification of outliers in high-speed railway track geometry inspection data based on a self-supervised transformer. First, the self-supervised transformer pre-trains on a substantial amount of unlabeled data, enabling the model to learn and extract fundamental features and patterns from the inspection data. Next, the limited labeled outliers are used to fine-tune the model, enhancing its adaptability to the task of classifying outliers. This approach allows for the automatic identification and classification of outliers even with limited labeled data and without prior knowledge. Experimental results demonstrate that the proposed method accurately identifies and classifies outliers such as local burr, turnout gauge widening, constant section of unilateral gauge data, and abnormal distribution in track geometry inspection data, achieving a classification accuracy of up to 97.8 % and an F1-score as high as 97.9 %. This performance surpasses five supervised baselines by 4–26 % in accuracy and by 5–35 % in F1-score. Moreover, the method maintains an accuracy rate exceeding 92 % across different inspection trains and lines, demonstrating excellent generalization performance.},
  archive      = {J_ASOC},
  author       = {Yu Liu and Jinzhao Liu and Wenxuan Zhang and Sen Yang and Kai Tao and Fei Yang},
  doi          = {10.1016/j.asoc.2025.113281},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113281},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Few-shot outliers classification in high-speed railway track geometry based on self-supervised transformer},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable AI-enhanced machine learning for CBR prediction in stabilized and unstabilized subgrade soils. <em>ASOC</em>, <em>178</em>, 113275. (<a href='https://doi.org/10.1016/j.asoc.2025.113275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel framework integrating explainable artificial intelligence (XAI) techniques with machine learning (ML) models to predict the California Bearing Ratio (CBR) of subgrade soils, addressing the "black-box" limitation in traditional ML applications. Five ML models, CatBoost, XGBoost, Random Forest, LightGBM, and Categorical Mixture Density Networks (CasMDN), were employed to predict the CBR of unstabilized and nano-silica stabilized soils. Quantitative results revealed that CatBoost achieved the highest predictive accuracy with an R² of 0.98, RMSE of 0.07, and IOA of 1.0 for unstabilized soils, outperforming the other models across multiple metrics. For nano-silica stabilized soils, CatBoost also led with an R² of 0.81 and an RMSE of 0.32, showing its robustness for varying soil types. Shapley Additive Explanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME) were employed to provide both global and local interpretability, illustrating the key features influencing CBR predictions. The findings from SHAP and LIME indicate that sand content and plastic limit are the most influential factors in CBR estimation, with variable feature importance across the soil types. A user-friendly application was developed by integrating these explainable ML models, thereby enabling rapid, reliable, and interpretable CBR predictions for practical geotechnical applications. This study’s insights enhance transparency and foster trust in ML models, paving the way for their wider adoption in infrastructure design and soil stability assessment.},
  archive      = {J_ASOC},
  author       = {Ishwor Thapa and Sufyan Ghani},
  doi          = {10.1016/j.asoc.2025.113275},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113275},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable AI-enhanced machine learning for CBR prediction in stabilized and unstabilized subgrade soils},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geological and engineering insights from transfer learning with fourier neural operators: A case study of CO2 storage forecasting in disparate saline aquifers. <em>ASOC</em>, <em>178</em>, 113272. (<a href='https://doi.org/10.1016/j.asoc.2025.113272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of machine learning techniques, particularly Fourier Neural Operators (FNO), offers a promising approach to predicting CO 2 saturation and pressure distributions in geological carbon storage. This study explores the application of FNO combined with transfer learning (FNO+TL) to enhance computational efficiency and accuracy in forecasting CO 2 storage under diverse geological and operational conditions. We trained FNO models on datasets from the SACROC (153 samples) geological model and applied TL to predict outcomes for the Illinois Basin - Decatur Project (IBDP). Our findings highlight the substantial computational savings without significant compromise in performance of the FNO+TL models compared to FNO, using 10 and 20 samples for pressure and saturation predictions respectively. The FNO+TL model achieved an average Mean Absolute Error (MAE) of 0.11 for CO2 saturation and 8.7 psia for pressure predictions, compared to 0.79 and 2.4 psia respectively for FNO. While saturation predictions were less precise, the model effectively captured the overall CO 2 migration trends. Notably, transfer learning significantly reduced computational costs, decreasing training time by 62.5 % and storage, RAM requirements by 90 % and 68 %, respectively. Despite some limitations in saturation prediction accuracy, the FNO+TL approach demonstrates potential for efficient and reliable CO 2 storage forecasting. This study highlights the potential of FNOs and transfer learning for efficient and accurate forecasting of CO 2 storage behavior and management of carbon sequestration projects.},
  archive      = {J_ASOC},
  author       = {Yusuf Falola and Siddharth Misra and Andres Nunez},
  doi          = {10.1016/j.asoc.2025.113272},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113272},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Geological and engineering insights from transfer learning with fourier neural operators: A case study of CO2 storage forecasting in disparate saline aquifers},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient compression and communication for prototype-based decentralized learning. <em>ASOC</em>, <em>178</em>, 113270. (<a href='https://doi.org/10.1016/j.asoc.2025.113270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In prototype-based federated learning, the exchange of model parameters between clients and the master server is replaced by transmission of prototypes or quantized versions of the data samples to the aggregation server. A fully decentralized deployment of prototype-based learning, without a central aggregator of prototypes, is more robust upon network failures and reacts faster to changes in the statistical distribution of the data, suggesting potential advantages and quick adaptation in dynamic learning tasks, e.g., when the data sources are IoT devices or when data is non-iid. In this paper, we face the challenge of designing an efficient prototype-based decentralized learning network by reducing the overheads in communication and computation. This allows enhancing the scalability of the global system, specially for IoT settings with resource-limited devices. First, we compress the prototype size by applying a clustering algorithm. After that, we filter the prototypes to be disseminate using an information-theoretic measure to share only relevant models or models that provide new knowledge to their neighbors. Then, we define a parallel gossip algorithm to disseminate these models within the learning network. Finally, we define a suitable scheduler able to manage the set of prototypes received to optimize the aggregation phase. In order to validate our proposal we present an analysis of the parallel gossip algorithm regarding the age-of-information (AoI). Our experimental results show the communications load can be substantially reduced without decreasing the convergence rate of the learning algorithm.},
  archive      = {J_ASOC},
  author       = {Pablo Fernández-Piñeiro and Manuel Fernández-Veiga and Rebeca P. Díaz-Redondo and Ana Fernández-Vilas and Martín González Soto},
  doi          = {10.1016/j.asoc.2025.113270},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113270},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards efficient compression and communication for prototype-based decentralized learning},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Macroscopic GA-based multi-objective traffic light optimization prioritizing tramways. <em>ASOC</em>, <em>178</em>, 113269. (<a href='https://doi.org/10.1016/j.asoc.2025.113269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban traffic congestion significantly impacts city efficiency, environmental quality, public safety, and overall quality of life. The primary objective of this research has been to develop traffic light optimization strategies to reduce congestion in areas where tramways or high-speed buses need priority. To this end, a novel approach is presented in this paper as a Genetic Algorithm-based Multi-Objective Traffic Light Optimization (MaMoTLO). It takes into account multiple factors such as minimizing stops, reducing travel and waiting times (private and public), and ensures the synchronizations. This approach is particularly useful in urban areas where multiple tramways intersect with regular traffic, creating potential bottlenecks for private flows. The main innovative aspects are the insertion of structural constraints on flow and queues are junctions, the insertion of penalty constraint for the tramways passage, the capability of working at macroscale of the solution, the validation against real conditions, and the comparison with a large set of solutions. The work presented compares the new optimization strategies with existing state-of-the-art methods, including those based on Non-Dominated Sorting Genetic Algorithm II (NSGA-II/III), Simulation of Urban MObility) (SUMO) Actuated solutions, and Webster’s formula for traffic light timing. The proposed solutions have been tested using real traffic data from Florence, Italy, and simulated scenarios to measure their effectiveness in reducing congestion and improving traffic flow on Snap4City open platform. The findings indicate that selected MaMoTLO solutions substantially outperform state-of-the-art methods by providing more balanced and efficient traffic light solutions (presenting improvement of 10 % or higher in real conditions), which is crucial for urban areas with high tramway traffic. The proposed optimization improves the flow of vehicular traffic and ensures that public transportation systems like tramways operate smoothly without unnecessary delays. The research has been developed for the CN MOST, national center on sustainable mobility in Italy.},
  archive      = {J_ASOC},
  author       = {Stefano Bilotta and Zahra Fereidooni and Luciano Alessandro Ipsaro Palesi and Paolo Nesi},
  doi          = {10.1016/j.asoc.2025.113269},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113269},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Macroscopic GA-based multi-objective traffic light optimization prioritizing tramways},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical transformer network with label attention for personality prediction by MBTI classification. <em>ASOC</em>, <em>178</em>, 113267. (<a href='https://doi.org/10.1016/j.asoc.2025.113267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personality prediction is one of the emerging researches with applications in multiple fields such as psychology, ai, recommendation system, job screening, education, police department (to monitor and enforce law and order). With the evolution of deep learning models, personality prediction can be formulated as a classification problem from social media texts. More recently, transformer-based models have demonstrated impressive results in personality prediction of individuals. This paper proposes a hierarchical transformer enabled with label attention mechanism for multiclass and binary classification of personality traits from the Myers-Briggs Type Indicator (MBTI) dataset, named as Hierarchical Transformer network with Label Attention for Personality Prediction (HT-LA-PP). By capturing relationships between words in sentences using a hierarchical transformer enabled with self-attention, followed by label attention to each personality trait, the proposed model surpasses the performance of similar deep learning and Transformer-based models with significant accuracy and robustness.},
  archive      = {J_ASOC},
  author       = {Bama S and Hema M S and Esakkirajan S and Nageswara Guptha M},
  doi          = {10.1016/j.asoc.2025.113267},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113267},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hierarchical transformer network with label attention for personality prediction by MBTI classification},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skin lesion segmentation by fusing local and global features using axial shift and spatial state model. <em>ASOC</em>, <em>178</em>, 113261. (<a href='https://doi.org/10.1016/j.asoc.2025.113261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin lesions present challenges to medical diagnosis due to their complex features, including shape variations, uneven color, and blurred boundaries. Currently, models based on convolutional neural networks (CNNs) and Transformers often have too many parameters, making them difficult to deploy in resource-limited medical environments while also struggling to balance local and global features. To address this, this paper proposes a Shift-Mamba structure that effectively captures local features through an axial shift mechanism and fuses global features using Mamba’s spatial state model (SSM). Notably, the new model (SM-UNet) designed based on the Shift-Mamba structure has only 0.02 million (M) parameters, making it one of the lightest models available, much lighter than those based on CNN or Transformer architectures. The SM-UNet model was validated on the ISIC 2017 and ISIC 2018 datasets, achieving IoU and Dice scores of 84.04%, 91.15% and 82.50%, 90.23%, respectively. These results surpass those of existing segmentation models, demonstrating the superiority of SM-UNet in the task of skin lesion segmentation. Code is available at https://github.com/guangguangLi/SM-UNet .},
  archive      = {J_ASOC},
  author       = {Guangju Li and Qinghua Huang and Wei Wang and Longzhong Liu},
  doi          = {10.1016/j.asoc.2025.113261},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113261},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Skin lesion segmentation by fusing local and global features using axial shift and spatial state model},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task prompt adapter for vision-language model. <em>ASOC</em>, <em>178</em>, 113254. (<a href='https://doi.org/10.1016/j.asoc.2025.113254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in tuning CLIP (Contrastive Language-Image Pre-training) have shown remarkable effectiveness across diverse downstream tasks. However, most existing methods compromise CLIP’s original multi-modal architecture by focusing on uni-modal adaptations, leading to increased model complexity and reduced generalizability as task numbers grow. To address these limitations, we propose a novel multi-modal tuning framework for multi-task learning based on CLIP, which preserves the integrity of the pre-trained model while enabling effective task adaptation. Central to our approach is a lightweight and plug-and-play multi-modal fine-tuning adapter A , which aligns and fuses task-relevant features from both text and vision modalities. Unlike previous approaches, our adapter facilitates shared learning across tasks while maintaining task-specific independence, avoiding interference and overfitting. Additionally, a learnable alignment module is introduced to dynamically adapt prompt representations in multi-task scenarios, promoting better feature generalization. Extensive experiments on standard benchmarks demonstrate that our framework consistently outperforms existing methods, especially in scenarios involving diverse and overlapping tasks. By preserving CLIP’s multi-modal strengths while introducing modular adaptability, our work presents a scalable and generalizable solution for multi-task vision-language learning, offering new insights and tools for advancing research in multi-modal foundation models.},
  archive      = {J_ASOC},
  author       = {Wei Zhong and Jiabao Han and Cheng Ding and Cheng Wang and Luwei Luo},
  doi          = {10.1016/j.asoc.2025.113254},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113254},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-task prompt adapter for vision-language model},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep q network with action retention for going long and short selling. <em>ASOC</em>, <em>178</em>, 113252. (<a href='https://doi.org/10.1016/j.asoc.2025.113252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer-simulated games, the primary objective of adopting reinforcement learning is to achieve victory by attaining the highest hand-crafted reward, considering the optimal state-value functions across the promising trajectories. However, in the context of algorithmic trading, there is no clear goal for hand-crafting an extremely high reward for the state-value function. Besides, the exploration and exploitation of the reinforcement learning could generate a high number of unexpected buy and sell actions. These actions could lead to overlapped transactions which cannot provide a fair reward function. In order to alleviate these problems, we propose a novel trading algorithm named Deep Q Network with Action Retention (DQN-AR). Firstly, the action retention mechanism is proposed to avoid the overlapped transactions. Secondly, the divide-and-conquer approach is employed to break down the profit maximization goal into several sub-goals, with the aim of optimizing the annualized returns from all transactions throughout the entire trading period. Thirdly, we evaluate the effectiveness of the proposed approach by implementing the DQN-AR model for both long and short selling in algorithmic trading. In the experiments, we compare DQN-AR with DQN, Gated-DQN (GDQN), Simple Moving Average (SMA) and Dual Moving Average Crossover (DMAC). The experimental result shows that DQN-AR is superior to DQN, GDQN, SMA and DMAC and achieves the state-of-art trading performance both for long and short positions. In summary, our DQN-AR achieves 15.4% higher profit on average than the second top competitor approach for the long position and 101.03% higher on average for the short position.},
  archive      = {J_ASOC},
  author       = {Qizhou Sun and Yain-Whar Si},
  doi          = {10.1016/j.asoc.2025.113252},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113252},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep q network with action retention for going long and short selling},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage consensus model incorporating manipulative and manipulated behaviors for large group decision-making under social network environment. <em>ASOC</em>, <em>178</em>, 113240. (<a href='https://doi.org/10.1016/j.asoc.2025.113240'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large group decision-making (LGDM), some decision-makers (DMs) may engage in manipulative behaviors driven by personal interests, while others may become susceptible to manipulation due to the complexity and uncertainty of the decision-making process. These manipulative and manipulated behaviors hinder the effective achievement of group consensus and undermine the fairness and acceptability of the decision-making process. To address this, we propose a two-stage consensus model that accounts for both manipulative and manipulated behaviors. First, the trust relationships among DMs are adjusted based on the similarity of their evaluations, and the strength of these relationships is calculated using their adjusted mutual trust degrees. Next, a clustering method based on the fracture of relationship strength is introduced to classify DMs into subgroups. By considering DMs' hesitancy, trust relationships, and preference degrees for various alternatives expressed in their evaluations, manipulators are identified and penalized with a weight penalty. The combination of hesitation degree, trust degree, and similarities in alternative ordinals, before and after subjective adjustment, is used to identify and impose penalties on manipulated DMs. Furthermore, various objective adjustment strategies are proposed to better manage the different behaviors of DMs, thereby improving decision-making efficiency and consensus. Finally, an application example and comparative analyses are presented to validate the feasibility of the proposed method. The proposed method effectively manages manipulative and manipulated behaviors, significantly enhancing consensus efficiency, fairness, and acceptability in the decision-making process.},
  archive      = {J_ASOC},
  author       = {Xiangyu Zhong and Fuhao Liu and Zhijiao Du and Qifeng Wan},
  doi          = {10.1016/j.asoc.2025.113240},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113240},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage consensus model incorporating manipulative and manipulated behaviors for large group decision-making under social network environment},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel stratified algorithms for sustainable decision-making under uncertainty: MOSDM and SSDM. <em>ASOC</em>, <em>178</em>, 113239. (<a href='https://doi.org/10.1016/j.asoc.2025.113239'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s dynamic decision-making environments, decision-makers face considerable challenges due to conflicting objectives, fluctuating criteria weights, and evolving preferences. Traditional decision-making methods often fail to maintain effectiveness in such complex scenarios. This study aims to develop and evaluate two innovative algorithms—Multi-Objective Stratified Decision-Making (MOSDM) and Sustainable Stratified Decision-Making (SSDM)—based on the Concept of Stratification (CST), to address these challenges. The primary objective is to enhance decision-making by integrating multiple objectives, improving computational efficiency, and ensuring sustainability in uncertain environments. The MOSDM exponentially reduces computational complexity from O ( 2 n ) to O ( n ) , making it feasible for large-scale applications while effectively addressing the limitations of existing stratification methods. The SSDM algorithm extends MOSDM by incorporating sustainability principles, allowing decision-makers to account for future scenarios and fluctuating criteria. The proposed algorithms were validated through real-world case studies in financial markets, where their performance was benchmarked against traditional methods, demonstrating superior decision integration, adaptability, and sustainability. Key findings underline the scalability and efficiency of the algorithms, their ability to navigate dynamic environments, and their potential for advancing sustainable decision-making practices. By providing practical solutions to industries facing multi-objective challenges, the proposed algorithms offer significant contributions to decision science and sustainability.},
  archive      = {J_ASOC},
  author       = {Shayan Sharifi},
  doi          = {10.1016/j.asoc.2025.113239},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113239},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Novel stratified algorithms for sustainable decision-making under uncertainty: MOSDM and SSDM},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy multi-objective multi-item five dimensional transportation problem: A case study of milk transportation problem. <em>ASOC</em>, <em>178</em>, 113237. (<a href='https://doi.org/10.1016/j.asoc.2025.113237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transportation problem with multiple dimensions and objectives mirrors the transportation challenges faced in real life. This research creates an intricate model for a fuzzy multi-objective, multi-item five-dimensional transportation problem (FMOMI5DTP), highlighting the intricacy of transportation challenges in the real world. Although classical optimization algorithms are commonly utilized for addressing transportation issues, they may struggle to fully grasp the knowledge and judgment of those making decisions. To overcome the problem, the non-dominated sorting genetic algorithm III (NSGA III) is utilized here to generate solutions that consider various levels of objectives without being dominated by others. However, to generate the solutions, NSGA III needs an initial feasible population. For this purpose, this paper suggests a structured method to effectively create an initial population to handle the FMOMI5DTP. Moreover, actual milk transportation data is gathered using Google Maps and Mapple Maps to validate the suggested model. The issue is resolved by utilizing NSGA III, which is then contrasted with the results of NSGA II, aspiration level-based NSGA III (AL-NSGA III), non-dominated sorting differential evolution based on reference point (NSDE-R), hybrid genetic algorithm (HGA), Goal Programming (GP), and fuzzy programming technique (FPT). The results of the study demonstrate that NSGA-III outperforms other algorithms in both the quality and quantity of solutions for the given case study. NSGA-III generates 100 Pareto-optimal solutions, significantly more than NSDE-R, which produces 70 solutions, and NSGA-II, which yields only 7 solutions for the same population size of 100. Furthermore, the solutions obtained from NSGA-III dominate many of those produced by NSGA-II and NSDE-R, highlighting its effectiveness in maintaining a diverse and high-quality solution set. Sensitivity analysis is performed to evaluate the stability of the solutions with respect to objective functions, changes in input parameters, and confidence levels. Performance metrics like hypervolume, coverage, generation distance, and ratio of non-dominated individuals (RNI) value are used to assess the effectiveness of NSGA III in managing multi-dimensional transportation problems with multiple objectives.},
  archive      = {J_ASOC},
  author       = {Ekata Jain and Jayesh M. Dhodiya},
  doi          = {10.1016/j.asoc.2025.113237},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113237},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy multi-objective multi-item five dimensional transportation problem: A case study of milk transportation problem},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalised healthy food text recommendations through fuzzy linguistic variables: A generative AI-based approach. <em>ASOC</em>, <em>178</em>, 113234. (<a href='https://doi.org/10.1016/j.asoc.2025.113234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nutrition and healthy eating habits are fundamental for the global population. Nowadays, there is an increasing tendency to consume less healthy recipes and a low general knowledge of nutrition. In these terms, generative AI arises as a potential tool for health-aware food recommendations, especially when improving communication with the user. This study presents a pipeline to enrich prompts with fuzzy modelling to increase the quality of textual recommendations. We apply our pipeline to generate a personalised frequency of food consumption, considering both nutritional and individual profiles. This is an essential task for increasing the health-conscious recommendation systems. We conducted extensive experimentation across different roles and prompt strategies. We evaluated the quality of the text and the nutritional rigour of the text responses. Our results show that enriching prompts with fuzzy modelling of the nutritional information of the foods significantly improves the quality of the prompt responses.},
  archive      = {J_ASOC},
  author       = {Andrea Morales-Garzón and Ana María Rojas-Carvajal and Roberto Morcillo-Jimenez and Maria J. Martin-Bautista and Karel Gutiérrez-Batista},
  doi          = {10.1016/j.asoc.2025.113234},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113234},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Personalised healthy food text recommendations through fuzzy linguistic variables: A generative AI-based approach},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Community detection in networks: A rough sets and consensus clustering approach. <em>ASOC</em>, <em>178</em>, 113219. (<a href='https://doi.org/10.1016/j.asoc.2025.113219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this paper is to propose a framework, called Rough Clustering-based Consensus Community Detection (RC-CCD), to effectively address the challenge of identifying community structures in complex networks from a set of different community partitions. The method uses a consensus approach based on Rough Set Theory (RST) to manage uncertainty and improve the reliability of community detection. The RC-CCD framework is tested on synthetic benchmark networks generated by the Lancichinetti–Fortunato–Radicchi (LFR) method, which simulate varying network scales, node degrees, and community sizes. Key findings demonstrate that RC-CCD outperforms established algorithms like Louvain, Greedy, and LPA in terms of normalized mutual information, showing superior accuracy and adaptability, particularly in networks with higher complexity, both in terms of size and dispersion. These results have significant implications for enhancing community detection in fields such as social and biological network analysis.},
  archive      = {J_ASOC},
  author       = {Darian H. Grass-Boada and Leandro González-Montesino and Rubén Armañanzas},
  doi          = {10.1016/j.asoc.2025.113219},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113219},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Community detection in networks: A rough sets and consensus clustering approach},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TAS-TsC: A data-driven framework for estimating time of arrival using temporal-attribute-spatial tri-space coordination of truck trajectories. <em>ASOC</em>, <em>178</em>, 113214. (<a href='https://doi.org/10.1016/j.asoc.2025.113214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately estimating the time of arrival (ETA) for trucks is crucial for optimizing transportation efficiency in logistics. GPS trajectory data provides valuable information for ETA, but challenges arise due to temporal sparsity, variable sequence lengths, and the interdependencies among multiple trucks. To address these issues, we propose the Temporal-Attribute-Spatial Tri-space Coordination (TAS-TsC) framework, which leverages three feature spaces – temporal, attribute, and spatial – to enhance ETA. Our framework consists of a Temporal Learning Module (TLM) that uses state space models to capture temporal dependencies, an Attribute Extraction Module (AEM) that transforms sequential features into structured attribute embeddings, and a Spatial Fusion Module (SFM) that models the interactions among multiple trajectories using graph representation learning. These modules collaboratively learn trajectory embeddings, which are then used by a Downstream Prediction Module (DPM) to estimate arrival times. We validate TAS-TsC on real truck trajectory datasets collected from Shenzhen, China, demonstrating its superior performance compared to existing methods.},
  archive      = {J_ASOC},
  author       = {Mengran Li and Junzhou Chen and Guanying Jiang and Fuliang Li and Ronghui Zhang and Siyuan Gong and Zhihan Lv},
  doi          = {10.1016/j.asoc.2025.113214},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113214},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TAS-TsC: A data-driven framework for estimating time of arrival using temporal-attribute-spatial tri-space coordination of truck trajectories},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight model for long-term multivariate time series forecasting via high-dimensional feature maps. <em>ASOC</em>, <em>178</em>, 113208. (<a href='https://doi.org/10.1016/j.asoc.2025.113208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical applications, both performance and computational cost are crucial for long-term time series forecasting (LTSF). Transformers excel in capturing long-sequence interactions but suffer from slow training and high computational demands, limiting timely predictions. On the other hand, lightweight linear models often lack accuracy. These challenges may arise from the limited representational capacity of time series in their one-dimensional (1D) form. To address these limitations, TSMNet, a novel lightweight model, is proposed. TSMNet decouples the time series into higher dimensions, significantly enhancing feature representation efficiency while avoiding information redundancy. By segmenting 1D time series into multiple time series maps (TSMs) and employing periodic feature mixing and sub-TSM stacking strategies, local dependencies and global correlations are captured simultaneously. Additionally, an encoder based on two-dimensional (2D) dilated convolutions is introduced, with expandable channels and progressively widening convolutional kernels, enabling the comprehensive extraction of complex temporal variations. Finally, the extracted features are efficiently decoded and predicted via a multilayer perceptron (MLP) flatten head. Experiments on ten benchmark long-term time series forecasting datasets demonstrate that TSMNet achieves relative improvements of 8.40% compared with the state-of-the-art (SOTA) method. Furthermore, TSMNet operates approximately 10–41 times faster with extremely low computational resources.},
  archive      = {J_ASOC},
  author       = {Shixiang Tang and Yepeng Guan},
  doi          = {10.1016/j.asoc.2025.113208},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113208},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight model for long-term multivariate time series forecasting via high-dimensional feature maps},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion-based dynamic super-dense candidate boxes with random center points for 3D object detection. <em>ASOC</em>, <em>178</em>, 113181. (<a href='https://doi.org/10.1016/j.asoc.2025.113181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have achieved promising results in image generation, but their applications in 3D object detection still need further exploration. In this paper, we design a novel model DiffCandiDet based on dense heads with Gaussian distributed center points for 3D object detection, which effectively integrates the anchor-based method and the Gaussian random noise-based method to leverage the powerful denoising and reconstruction capabilities of the diffusion model. To achieve the learning balance for multi-class 3D object detection, we propose a Dynamic Super-dense Candidate Boxes (DSCB) strategy. Notably, DiffCandiDet addresses the issue of traditional models struggling to detect pedestrians walking side by side. In addition to Gaussian distribution, we also propose a DSCB strategy based on discrete uniform distribution (DUCandiDet) and continuous uniform distribution (CUCandiDet), to reduce the runtime consumption and enhance the robustness of the model. Extensive experiments show that DiffCandiDet achieves competitive results on both KITTI and Waymo Open Datasets. DiffCandiDet ranks 1st on the KITTI validation set in the Car and Pedestrian detection leaderboard. Code is available at https://github.com/SiHengHeHSH/DiffCandiDet .},
  archive      = {J_ASOC},
  author       = {Si-Heng He and Zi-Jia Wang and Yuan-Gen Wang and Yicong Zhou and Sam Kwong},
  doi          = {10.1016/j.asoc.2025.113181},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113181},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Diffusion-based dynamic super-dense candidate boxes with random center points for 3D object detection},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble self-organizing recursive neural network for modeling furnace temperature in municipal solid waste incineration. <em>ASOC</em>, <em>178</em>, 113170. (<a href='https://doi.org/10.1016/j.asoc.2025.113170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modeling of furnace temperature (FT) is the foundation of optimizing and controlling in municipal solid waste incineration (MSWI) process. However, owing to the high nonlinearity and dynamicity, complex reaction mechanisms, and strong coupling phenomena of MSWI process, accurately modeling the FT remains a significant challenge. In this paper, an ensemble self-organizing recursive neural network with information fusion gain algorithm (ESORNN-IFG) is proposed for FT modeling in MSWI process. First, the AdaBoost algorithm is introduced to combine multiple base learners by weighting them to enhance the accuracy and robustness of the strong learner. Second, an IFG index is introduced to appraise the contribution of hidden neurons and their interrelationships. Third, a self-organizing strategy combined with the IFG index is designed for adjusting the structure of the base learners during model training. Finally, the merits and effectiveness of the proposed ESORNN-IFG is confirmed by comparison with other existing approaches after testing the experimental results on several benchmark problems and the practical application of FT modeling in MSWI process.},
  archive      = {J_ASOC},
  author       = {Tao Yu and Haixu Ding and Junfei Qiao},
  doi          = {10.1016/j.asoc.2025.113170},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113170},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble self-organizing recursive neural network for modeling furnace temperature in municipal solid waste incineration},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating multi-dimensional graph attention networks and transformer architecture for predicting air pollution in subway stations. <em>ASOC</em>, <em>178</em>, 113033. (<a href='https://doi.org/10.1016/j.asoc.2025.113033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of PM 2.5 concentrations in subway stations is crucial for developing effective air pollution control strategies. However, existing methods struggle to accurately predict PM 2.5 concentrations due to the challenges of multi-step ahead forecasting, modeling long time series, capturing complex spatiotemporal correlations, and handling data quality issues such as missing values. This study proposes MAGICFormer, a novel hybrid end-to-end model for predicting PM 2.5 concentrations. The model comprises key components such as data preprocessing, a multi-dimensional graph attention network (md-GAT) module, as well as an Informer encoder and a Cross Decoder based on the Transformer architecture. The data preprocessing method improves data quality by addressing missing values and correcting anomalies. MAGICFormer integrates spatiotemporal correlations to predict PM 2.5 concentrations. The md-GAT module adaptively captures complex spatial relationships among subway stations across different dimensions, with its output serving as input to the Spatio Decoder. The Informer encoder processes long sequences and extracts temporal features, which are then passed to the Spatio Decoder and Temporal Decoder within the Cross Decoder for information fusion. The Cross Decoder aggregates the outputs of the Spatio and Temporal Decoders using a cross-attention mechanism, leveraging the interdependencies between graph-structured and time-series data to enhance prediction accuracy and improve model performance by effectively fusing spatial and temporal information. Experiments on Seoul subway stations show that MAGICFormer improves prediction accuracy by over 20 % compared to existing methods, demonstrating its effectiveness in long-term PM 2.5 forecasting. The proposed model offers a practical decision support tool for enhancing air quality management strategies in subway systems, particularly for long-term monitoring and control.},
  archive      = {J_ASOC},
  author       = {Dingya Chen and Hui Liu},
  doi          = {10.1016/j.asoc.2025.113033},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113033},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating multi-dimensional graph attention networks and transformer architecture for predicting air pollution in subway stations},
  volume       = {178},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Offloading-verified framework for adversary detection and mitigation in IoT. <em>ASOC</em>, <em>177</em>, 113312. (<a href='https://doi.org/10.1016/j.asoc.2025.113312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-physical systems (CPSs) designed for the Internet of Things (IoT) enhanced security and resource infrastructures to support various applications and services, undetected adversaries in the temporarily connected IoT network impose different user and data privacy threats, this research introduces an Offloading-verified Adversary Detection and Mitigation Scheme (OADMS), this proposed scheme coexists with the IoT communication and CPS security infrastructure for adversary detection, conventional behavior-based adversary detection with partial order adversarial network training validates the infrastructure security support against cyber-attacks. The behavior is analyzed for independent and offloaded service exchanges, reducing communication failures and is recurrently analyzed in the detection process until the service termination, communication metrics of the infrastructure units are used to verify adversary and user channel behavior. The learning process recommendations are exploited to validate the channel's reliability through IoT-sharing platforms, and the performance of the proposed system is assessed using communication latency, failure rate, response ratio, and detection factor. The model achieved an excellent detection accuracy rate of 96.8 %.},
  archive      = {J_ASOC},
  author       = {Nadhem Ebrahim and Mourad Elloumi and Abdullah Mohammed Alharthi and Fahad S. Altuwaijri and Mohammed Alsaadi},
  doi          = {10.1016/j.asoc.2025.113312},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113312},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Offloading-verified framework for adversary detection and mitigation in IoT},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of U-net+ + deep learning network for segmentation and processing of asphalt mixture X-ray CT images. <em>ASOC</em>, <em>177</em>, 113308. (<a href='https://doi.org/10.1016/j.asoc.2025.113308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness and generalization ability of traditional image segmentation algorithms are poor, and their performance is further degraded by factors such as uneven lighting, photoelectric noise, and low contrast, making it challenging to segment three-phase materials in X-ray CT images of asphalt mixtures. With the advancement of computer network technology, deep learning image segmentation algorithms have garnered widespread attention and application. However, the specific implementation process of deep learning network algorithms for X-ray CT images of asphalt mixtures remains unclear, and there is a notable absence of publicly available datasets specific to asphalt mixtures. Therefore, this study explores the specific implementation process of the U-Net+ + algorithm in X-ray CT images of asphalt mixtures. Deep learning networks were constructed using Python and PyTorch, implementing U-Net, U-Net+ + networks, and AG-U-Net and AG-U-Net+ + networks, each incorporating the AG attention mechanism. A specific method for creating a deep learning dataset was proposed using threshold segmentation and manual annotation of a custom-labeled dataset. Three-phase material segmentation and 3D modeling of OGFC-16 and AC-16 Marshall specimens were successfully achieved through neural networks. The research results indicate that image cropping during the dataset production process may result in the loss of global information, leading to segmentation errors for porous targets. In the segmentation of aggregates and pores, AG-U-Net+ + and U-Net+ + exhibit different optimal evaluation metrics, highlighting the need for further optimization of the dataset creation scheme for pore targets in subsequent research.},
  archive      = {J_ASOC},
  author       = {Tengfei Nian and Shiwen Xue and Jinguo Ge and Zhao Han and Zhijie An},
  doi          = {10.1016/j.asoc.2025.113308},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113308},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of U-net+ + deep learning network for segmentation and processing of asphalt mixture X-ray CT images},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating LDA thematic model, FCE, and QFD methods for consumer-centered visual planning of the creative tourism destination: A macrosystem decision approach. <em>ASOC</em>, <em>177</em>, 113299. (<a href='https://doi.org/10.1016/j.asoc.2025.113299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satisfying the consumer needs of tourism users is the key to the success of creative tourism destinations in landscape shaping, project design, and tourism operation. However, existing research on tourist needs mostly relies on expert experience and often stops at establishing evaluation results, lacking a systematic mechanism for translating user needs into implementation design. This study, however, synthesizes the perspectives of professionals and space users and innovatively proposes a macro-systematic decision-making approach that combines the LDA(Latent Dirichlet Allocation) thematic model, FCE(Fuzzy comprehensive evaluation), and QFD(Quality-function deployment) theory to ensure that the visual quality planning of tourism landscapes is closer to the aesthetic expectations of actual users. The feasibility of the research method was tested by analyzing 1056 social media comments and 237 questionnaires from a famous Chinese creative tourism venue (798 Art District) to determine the priority of design feature planning, thus improving the satisfaction of the tourist experience. The results show that more room for improvement is needed in terms of aesthetics, fun, creativity, and comfort of creative tourism venues in China. Relevant professionals should pay attention to attracting more potential consumers and enhancing their visual experience evaluation through the construction of features such as novelty, uniqueness, history, openness, and diversity in creative tourism destinations. The results of this study not only enrich the practical application of the theory of mass-functional configuration but also provide an important reference for how art can be involved in urban public space to enrich the evaluation of the tourism experience.},
  archive      = {J_ASOC},
  author       = {Yingyi Hao and Mohd Fabian Hasna and Faziawati Abdul Aziz},
  doi          = {10.1016/j.asoc.2025.113299},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113299},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating LDA thematic model, FCE, and QFD methods for consumer-centered visual planning of the creative tourism destination: A macrosystem decision approach},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements and prospects of fuzzy-based adaptive unscented kalman filters for nonlinear systems: A review. <em>ASOC</em>, <em>177</em>, 113297. (<a href='https://doi.org/10.1016/j.asoc.2025.113297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid developments in computational technologies have recently imposed even more significant requirements for efficient and accurate state estimation methods that can be applied to nonlinear dynamic systems. Among the widely known nonlinear estimation techniques stands the unscented Kalman filter. However, in real-life applications, its performance is usually affected due to the presence of noise and model uncertainties. Such disturbances are handled by adaptation-based approaches, wherein the noise covariances are adjusted. Many researchers have been attracted to adaptive methods using fuzzy tuning with covariance matching in the last decade. Adaptation methodologies based on fuzzy logic applied to an unscented Kalman filter related to different practical applications are reviewed herein. It is performed by examining various kinds of fuzzy inference systems, other categories of membership functions, adaptation laws, or tuning relations of covariance matrices and their respective applications. Fuzzy logic control is one of the parts or components of artificial intelligence. The fuzzy inference systems, such as Mamdani and Takagi-Sugeno, implemented for adaptive estimation techniques with unscented Kalman filters in real-world applications are highlighted. Furthermore, the readers may easily refer to the highlighted future possibilities and significant challenges in the field.},
  archive      = {J_ASOC},
  author       = {Manav Kumar and Sharifuddin Mondal},
  doi          = {10.1016/j.asoc.2025.113297},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113297},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advancements and prospects of fuzzy-based adaptive unscented kalman filters for nonlinear systems: A review},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-criteria decision support model for new energy vehicle selection considering social media influencer reviews and personalized preferences. <em>ASOC</em>, <em>177</em>, 113293. (<a href='https://doi.org/10.1016/j.asoc.2025.113293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of China’s market for new-energy vehicles (NEVs) highlights significant changes in consumer behavior and automotive technology. Social media influencers (SMIs) are seen as key players in helping consumers make purchasing decisions by providing credible opinions and suggestions on online platforms. However, the personalized preferences of SMIs when writing articles can lead to misunderstandings for consumers, and the quality of SMIs varies widely. To address these challenges, this paper presents a novel decision support method for NEV selection. The method first identifies high-value SMIs based on a comprehensive measure of the quality, recency, and frequency of posts. Using this measure, the key NEV criteria are extracted through seeded latent Dirichlet allocation (LDA). A linear programming model is established to capture the individual preferences of SMIs by balancing the consistency of personalized topic expressions and the diversity among SMIs. Additionally, a weighted calculation method based on the Ebbinghaus forgetting curve is applied to prioritize recent reviews, ensuring better reflection of current preferences. A case study on the website PCauto (pcauto.com) validates the proposed method, demonstrating its effectiveness and robustness through sensitivity, comparison, and effectiveness analyses. The proposed approach enhances the relevance of NEV recommendations, providing a more reliable decision support system for NEV consumers.},
  archive      = {J_ASOC},
  author       = {Zhang-peng Tian and Chuan Wu and Le Wang and Ru-xin Nie},
  doi          = {10.1016/j.asoc.2025.113293},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113293},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-criteria decision support model for new energy vehicle selection considering social media influencer reviews and personalized preferences},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the performance of fault diagnosis network via interpretable projection information. <em>ASOC</em>, <em>177</em>, 113284. (<a href='https://doi.org/10.1016/j.asoc.2025.113284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of fault diagnosis networks for rotating machine mainly depends on the classifiers and feature extractors. To improve the performance of the two components, a novel regularization and a modified normalization module are proposed based on the interpretable projection information. Specifically, the classifier is firstly studied from the perspective of projection mechanism rather than the traditional base representation. It is found that a negative correlation is beneficial to classifiers, and a corresponding regularization is proposed for improving its ability. Meanwhile, by analyzing the convolution operations in convolution networks from the projection perspective, we find that the projection information will be affected by the traditional batch normalization block followed with a rectified linear unit, if the batch size is huge and the number of classes is small. To solve this problem, a novel normalization module, which is designed based on absolute value operation, is proposed to fully retain the projection information while effectively suppressing the noises in features. Finally, the proposed method is used to improve several typical fault diagnostic networks, and the fault diagnosis experiments on rolling bearings and planetary gearboxes demonstrate that the proposed method can effectively improve the performance of diagnosis networks.},
  archive      = {J_ASOC},
  author       = {Biao He and Pengfei Dong and Yi Qin},
  doi          = {10.1016/j.asoc.2025.113284},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113284},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving the performance of fault diagnosis network via interpretable projection information},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating large language models with cross-modal data fusion for advanced intelligent transportation systems in sustainable cities development. <em>ASOC</em>, <em>177</em>, 113278. (<a href='https://doi.org/10.1016/j.asoc.2025.113278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the 6 G network era, autonomous navigation systems, as a key driving force for sustainable cities innovation and development, have raised higher demands for navigation and perception technologies. Against this backdrop, this paper proposes a multi-level cross-modal fusion method based on large language models (LLMs) for environmental perception in autonomous navigation systems. Specifically, the system first utilizes Transformer and PointNet models to extract and encode features from multimodal data obtained from cameras and LiDAR sensors. Subsequently, the cross-modal multi-head self-attention mechanism enhances the information exchange between different data types, enabling richer feature representations. Finally, a Mixture of Experts (MoE) model is deployed as a secondary fusion module, and a pre-fetching strategy is introduced to reduce the communication latency of the model. Experimental results indicate that this method outperforms other state-of-the-art 3D object detection algorithms on the KITTI dataset. Specifically, when detecting "car" targets, the mAP value of this method exceeds that of the next best method, Fast-CLOCs, by 2.08 %. When detecting "pedestrian" targets, the mAP exceeds that of the next best method, Epnet+ +, by 1.88 %. The proposed method also demonstrates a higher convergence speed and greater throughput than other strategies in the comparison of expert pre-fetching strategies. For example, in the case of the Transformer model, the EPP strategy reduces the time required to achieve target accuracy by 40 %, 17 %, and 25 % compared to Tutel, Lina, and Janus, respectively. Additionally, the MCF model shows robustness in handling data from sensors with varying data collection frequencies and timestamps, and it performs well across different weather conditions and lighting scenarios.},
  archive      = {J_ASOC},
  author       = {Jiachen Jiang and Yang Li and Jing Nie and Jingbin Li and Baoqin Wen and Thippa Reddy Gadekallu},
  doi          = {10.1016/j.asoc.2025.113278},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113278},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating large language models with cross-modal data fusion for advanced intelligent transportation systems in sustainable cities development},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive multi-factor integrated forecasting model based on periodic reconstruction and random forest for carbon price. <em>ASOC</em>, <em>177</em>, 113274. (<a href='https://doi.org/10.1016/j.asoc.2025.113274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and appropriate carbon price prediction can provide a quantitative benchmark for the decision-making of government and investors, promoting the rapid development of carbon market. However, the inherently complexity of carbon price affected by multiple external factors poses a challenge for accurate forecasting. Thus, an adaptive multi-factor integrated hybrid model based on periodic reconstruction and random forest is developed for carbon price prediction. In the model, the improved decomposition method and periodic reconstruction are introduced to fully extract and integrate the hidden laws, which realizes the efficient and accurate prediction under multiple time scales. Considering the disparities of carbon markets, a three-stage influencing factors screening framework is proposed based on random forest, achieving the adaptive prediction by using the selected external factors to modify the forecasting of carbon prices. Four representative carbon markets in China (i.e., Shanghai, Guangdong, Shenzhen, and Hubei) are employed for empirical analysis. The results reveal that carbon price can be affected by energy and financial markets in short-term fluctuations, while its long-term trends are mainly influenced by climate and policy effects. Compared with other benchmark models, the proposed adaptive model considering multiple factors is reasonable and effective to predict carbon price with different characteristics that the average MAPE and RMSE are 0.3977 and 0.5036, respectively. Therefore, the proposed model not only provides a reliable tool for carbon price prediction, but also provides a unique perspective for governments and investors to explore the multi-time scale influencing factors of carbon price variations, which helps stakeholders understand the market rules and make appropriate decisions.},
  archive      = {J_ASOC},
  author       = {Shunyu Zhao and Yelin Wang and Jianwei Deng and Zheng Li and Gen Deng and Zhi Chen and Youjie Li},
  doi          = {10.1016/j.asoc.2025.113274},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113274},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive multi-factor integrated forecasting model based on periodic reconstruction and random forest for carbon price},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A Q-learning-oriented multi-objective teaching-learning-based optimization algorithm for balancing U-shaped disassembly lines. <em>ASOC</em>, <em>177</em>, 113273. (<a href='https://doi.org/10.1016/j.asoc.2025.113273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles a multi-objective U-shaped disassembly line balancing problem (UDLBP) to optimize the number of workstations, the line balance, the demand index, and the hazard index simultaneously. Firstly, a mathematical model is developed to formulate the problem. Additionally, a novel Q-learning-oriented multi-objective teaching-learning-based optimization algorithm (Q-MOTLBO) is developed to address the UDLBP. In this algorithm, a two-phase decoding method is proposed to handle precedence relations and cycle time constraints. Meanwhile, five new neighbourhood structures and their corresponding local search operators are developed based on the characteristics of the UDLBP. Finally, the effectiveness of Q-learning in the proposed Q-MOTLBO is verified by comparing it with the classical MOTLBO algorithm and six other classical multi-objective algorithms by solving 29 cases. The experimental results indicate that the Q-MOTLBO algorithm produces promising performance in solving the UDLBP.},
  archive      = {J_ASOC},
  author       = {Wanlin Yang and Zixiang Li and Chenyu Zheng and Tao Li and Zikai Zhang and Liping Zhang},
  doi          = {10.1016/j.asoc.2025.113273},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113273},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A Q-learning-oriented multi-objective teaching-learning-based optimization algorithm for balancing U-shaped disassembly lines},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdvancedScoreCAM: Enhancing visual explainability through hierarchical upsampling. <em>ASOC</em>, <em>177</em>, 113265. (<a href='https://doi.org/10.1016/j.asoc.2025.113265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have achieved remarkable success across various domains. However, the intricate nature of these models often hinders our understanding of their decision-making processes. Explainable AI methods such as Class Activation Mapping (CAM) become indispensable in providing intuitive explanations for these model decisions. Previous CAM-based methods often employed simple upsampling operations, resulting in the loss of contextual information. In this work, we propose a simple yet highly effective approach, AdvancedScoreCAM (ASC), which introduces a concurrent upsampling and fusion pipeline method to enhance visual explainability. Our proposed method introduces a direct and progressive upsampling pipeline, which can fully extracts contextual information during the upsampling process. This improvement is achieved by selectively integrating contextual details within the upsampled activation layers. Through extensive experiments and qualitative comparisons on two datasets, we demonstrate that ASC consistently produces clearer and more interpretable heatmaps that better reflect the model’s decision-making process compared to previous methods. Our code is available at https://github.com/jiiaozi/AdvancedScoreCAM .},
  archive      = {J_ASOC},
  author       = {HaoJun Zhao and Mohd Halim Mohd Noor},
  doi          = {10.1016/j.asoc.2025.113265},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113265},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AdvancedScoreCAM: Enhancing visual explainability through hierarchical upsampling},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight YOLOv8-based model with squeeze-and-excitation version 2 for crack detection of pipelines. <em>ASOC</em>, <em>177</em>, 113260. (<a href='https://doi.org/10.1016/j.asoc.2025.113260'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack detection is crucial to the buried pipelines that transit water, gas, oil, etc. However, the traditional detection methods may lack accuracy and robustness for the pipelines in low-light and complex backgrounds. This study proposes a YOLOv8-GhostConv-SEV2 model based on the lightweight YOLOv8n framework, which optimizes feature extraction by introducing the GhostConv module and enhances noise suppression capability with the SEV2 (Squeeze-and-Excitation Version 2) attention mechanism. Based on a dataset of 11,135 images of pipelines in a low-light environment, the proposed model achieves 98.1 % (+2.62 %) precision, 95.7 % (+3.80 %) recall, 0.969 (+3.30 %) F1 score, and 82.4 % (+11.49 %) mAP50–95 on the test set. Additionally, the improved model size is only 5.67 MB (-5.34 %), which is lightweight and highly suitable for the crack detection of buried pipelines in complex environments.},
  archive      = {J_ASOC},
  author       = {Zhaochao Li and Linxuan Xiao and Meiling Shen and Xiya Tang},
  doi          = {10.1016/j.asoc.2025.113260},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113260},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight YOLOv8-based model with squeeze-and-excitation version 2 for crack detection of pipelines},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Question answering over temporal knowledge graphs based on time sensitive graph neural network. <em>ASOC</em>, <em>177</em>, 113257. (<a href='https://doi.org/10.1016/j.asoc.2025.113257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answering query questions accurately on large-scale knowledge graph has always been the key of question answering system. Recently, answering temporal questions on temporal knowledge graphs has attracted wide attention. However, all rely on predefined temporal knowledge or structured data, which often limits their ability to generalize to more dynamic or unstructured temporal information. These models can also struggle to deal with ambiguous or complex temporal reasoning, especially when dealing with long or overlapping time frames. Therefore, we propose a new T ime S ensitive G raph N eural N etwork Q uestion A nswering model (TSGNN-QA), which not only addresses the limitations of existing knowledge graph QA models in learning and representing temporal information, but also enhances the ability to infer and predict implicit temporal information and improve multi-hop node connectivity and node ability, providing a new solution for temporal knowledge graph QA tasks. On the one hand, we use temporal graph neural network to learn temporal knowledge graph, and improve its time encoding part by hyperplane technology to improve the time sensitivity of the model. On the other hand, we use gated recurrent unit to reason and predict the hidden temporal information, so as to alleviate the problem of query accuracy degradation caused by lack of temporal information. In addition, in view of the fact that some question entities and answer entities are not directly connected in the temporal question answering process, it may take multiple entities to find the answer entity. This paper establishes and increases the multi-hop connection between nodes and carries out reasoning scoring, which improves the multi-hop query ability of the model. In the query module of TSGNN-QA model, we optimize and decodes the multi-head attention mechanism and multi-layer perceptron. Finally, experimental results show that the TSGNN-QA model has significant advantages in temporal question-answering queries. Its experimental results for Hits@1 on the CronQuestions dataset and Complex-CronQuestions dataset outperform the best baseline model by 12.03 % and 1.52 %, respectively.},
  archive      = {J_ASOC},
  author       = {Luyi Bai and Linshuo Xu and Lin Zhu},
  doi          = {10.1016/j.asoc.2025.113257},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113257},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Question answering over temporal knowledge graphs based on time sensitive graph neural network},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning fuzzy cognitive maps for prediction problems using swarm intelligence based on an input-sensitive reasoning mechanism. <em>ASOC</em>, <em>177</em>, 113256. (<a href='https://doi.org/10.1016/j.asoc.2025.113256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy cognitive maps (FCMs) are well known for their ability to learn and analyze the dynamic behavior of complex systems, but their effectiveness is often hindered by classical reasoning mechanisms, which are rooted in control systems. Regardless of the initial conditions, they tend to converge to a steady state, leading to poor predictive performance in learning FCMs. To address this issue, this study proposes a novel reasoning mechanism tailored for error-driven learning in FCMs. By incorporating indirect relationships into the reasoning mechanism, the proposed method provides a more comprehensive assessment of each concept’s influence within the system. This allows for a more effective response to initial conditions and improves prediction accuracy. The proposed method employs the Particle Swarm Optimization (PSO) algorithm to learn relationships and is evaluated through extensive experimental studies using three real-world datasets (scenarios). The experiments compare the proposed method with classical and recently developed reasoning mechanisms by systematically varying normalization techniques, fitness functions, and computational methods. The findings demonstrate that the proposed method consistently outperforms classical methods in prediction accuracy across multiple scenarios. For instance, in one scenario, the proposed method achieves up to 49 % lower out-of-sample error and correctly predicts 83 % of concepts, outperforming sigmoid and hyperbolic tangent-based methods, which achieve 51 % and 50 % accuracy, respectively. Notably, Heaviside emerges as the most effective fitness function across all experimental setups, delivering the lowest prediction errors and improving accuracy by 12–35 % using the proposed reasoning mechanism. Comparisons with exponential normalized reasoning method reveal that the proposed method achieves superior prediction accuracy in 90 % of test cases, demonstrating statistically significant improvements in most setups. Overall, the results demonstrate the superiority of the proposed method in learning FCMs, offering enhanced prediction accuracy and generalization across diverse scenarios.},
  archive      = {J_ASOC},
  author       = {Mirac Murat and Umut Asan},
  doi          = {10.1016/j.asoc.2025.113256},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113256},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning fuzzy cognitive maps for prediction problems using swarm intelligence based on an input-sensitive reasoning mechanism},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG microstates classification empowered with optimum-path forest using different distance measurements. <em>ASOC</em>, <em>177</em>, 113253. (<a href='https://doi.org/10.1016/j.asoc.2025.113253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) is considered one of the most important tests for neurological disease diagnosis, whose signals comprise microstate information regarding the spatiotemporal characteristics of human brain activity. However, interpreting and identifying such signals denotes a complex and time-consuming activity, thus motivating the use of automated approaches like machine learning techniques in many studies. Recent research has presented several techniques for detecting neurological disorders through analyzing EEG microstates. However, this area has room for advancements and improvements, using distinct methods and more in-depth scrutiny. Therefore, this paper proposes a new method for EEG signal classification through microstate analysis for diagnosing neurological diseases using a graph-based algorithm, namely the Optimum-Path Forest (OPF) classifier. Experiments were conducted over features extracted from EEG microstates obtained from the Temple University Hospital Abnormal EEG Corpus (TUAB) and the Schizophrenia EEG datasets. Such features are further processed using principal component analysis, t-distributed stochastic neighbor embedding, and uniform manifold approximation and projection dimensionality reduction techniques to improve computational efficiency and increase the classifier’s performance. Furthermore, this work evaluates 44 distance measurements in OPF’s graph modeling context. Finally, the experimental results show that with a reduced set of features extracted from the microstates and an appropriate distance measure, it is possible to obtain accuracy values equivalent to 100% with low processing time compared to raw data. In addition, the k -nearest neighbors and support vector machine classifiers were used to compare the experimental results.},
  archive      = {J_ASOC},
  author       = {Raniere R. Guimarães and Leandro A. Passos and David W. Kuster and Ani Dong and Victor Hugo C. de Albuquerque},
  doi          = {10.1016/j.asoc.2025.113253},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113253},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EEG microstates classification empowered with optimum-path forest using different distance measurements},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep hashing image retrieval based on CNN and visual transformer network. <em>ASOC</em>, <em>177</em>, 113244. (<a href='https://doi.org/10.1016/j.asoc.2025.113244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing technology can achieve unified representation of multimedia technology, which is widely used in fields such as smart agriculture, smart transportation, and public safety. However, the current deep hashing methods cannot achieve efficient fusion of global and local information, and the learning ability of hash code representation needs to be further strengthened. In this paper, we propose a deep hashing retrieval algorithm based on integrated CNN and visual Transformer(ICVT) network. Firstly, we propose a lightweight nonlinear spatial group enhancement (NSGE) module, which is integrated with the Transformer through a parallel architecture and introduces a self-attention mechanism to enhance the spatial distribution of semantic features through the similarity between local and global features. The CNN representation capability of feature maps is improved to enhance the spatial distribution of semantic features within each feature semantic group. Secondly, we propose a margin contrast loss function to optimize the hash code parameters, which is used to improve the retrieval accuracy of the algorithm for multi-label datasets. Finally, extensive experiments on CIFAR-10, NUS-WIDE, and ImageNet datasets demonstrate that ICVT has superior retrieval performance compared with the currently popular algorithms, especially on CIFAR-10, where the mAP performance reaches 97.5%.},
  archive      = {J_ASOC},
  author       = {Shuli Cheng and Xingming Xiao and Liejun Wang},
  doi          = {10.1016/j.asoc.2025.113244},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113244},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep hashing image retrieval based on CNN and visual transformer network},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tourism attraction selection driven by online tourist reviews: A novel multi-attribute decision making method based on the evidence theory and probabilistic linguistic term sets. <em>ASOC</em>, <em>177</em>, 113243. (<a href='https://doi.org/10.1016/j.asoc.2025.113243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s internet age, potential tourists often browse online tourist reviews (OTRs) before determining travelling destinations. However, the information in massive OTRs is usually fuzzy and uncertain. The probabilistic linguistic term set (PLTS) is a helpful tool to describe the ambiguous and uncertain information. This study utilizes the PLTS to depict information in OTRs and proposes a novel tourist attraction selection method with OTRs. First, a new score function of PLTSs is introduced by combining risk attitudes of decision makers (DMs) and the hesitancy of the PLTS. Subsequently, the attributes evaluating tourist attractions are determined by extracting the top 50 high frequency words from OTRs. A new sentiment analysis technique is developed for transforming OTRs into PLTSs with the five-granularity linguistic term set. According to the decision information and the number of times attributes commented, a bi-objective programming model is built to derive attribute weights. Finally, fusing alternative perceived utility values into the D-S evidence theory, a new decision method is developed to rank alternative tourist attractions. At length, a case study of selecting tourist attractions is provided to illustrate the applications of the proposed method. Furthermore, the comparative analyses are conducted to show its effectiveness and superiority.},
  archive      = {J_ASOC},
  author       = {Han Yang and Gaili Xu},
  doi          = {10.1016/j.asoc.2025.113243},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113243},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tourism attraction selection driven by online tourist reviews: A novel multi-attribute decision making method based on the evidence theory and probabilistic linguistic term sets},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Promotion of china's hierarchical diagnosis and treatment in the post epidemic era: Intelligent decision support. <em>ASOC</em>, <em>177</em>, 113242. (<a href='https://doi.org/10.1016/j.asoc.2025.113242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the post-pandemic era, China’s primary healthcare institutions face the critical tasks of normalizing COVID-19 prevention and control while managing chronic diseases in an aging society. Chronic disease management encounters challenges in efficiency, accuracy, and continuity. This paper focuses on AI-assisted intelligent decision-making for chronic disease management using reinforcement learning. This method enables an intelligent agent to learn through interactions with the chronic disease management environment, dynamically generating personalized treatment and management strategies based on patients' real-time conditions and historical data. In clinical trials, hypertension patients—the most frequently diagnosed group during the pandemic—were selected from primary hospitals. A total of 106 patients participated, with 52 in the experimental group and 54 in the control group. By leveraging reinforcement learning and interaction experiences with the environment, the system enhanced individual decision-making through evaluable feedback. After 200 rounds of simulated management strategies, the average total reward reached 154.8, effectively stabilizing patients’ conditions. Additionally, 493 chest CT scan images were randomly divided into training and validation sets. The binary classification model’s accuracy improved by 18 % compared to baseline methods. Using five-fold cross-validation for diagnostic evaluation, the deep learning algorithm achieved an accuracy of 96.1 %, sensitivity of 91.5 %, specificity of 93.6 %, and an AUC of 0.95 on an independent test set. The intergroup consistency among different doctor groups had a Kappa value ranging from 0.44 to 0.68. Experimental results demonstrate that the deep learning algorithm significantly outperforms clinical doctors in diagnostic performance. This approach not only significantly enhances the quality and effectiveness of chronic disease management but also facilitates the implementation of a tiered diagnosis and treatment system.},
  archive      = {J_ASOC},
  author       = {Yong Wang and Xiyuan Wang},
  doi          = {10.1016/j.asoc.2025.113242},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113242},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Promotion of china's hierarchical diagnosis and treatment in the post epidemic era: Intelligent decision support},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing financial time series forecasting with hybrid deep learning: CEEMDAN-informer-LSTM model. <em>ASOC</em>, <em>177</em>, 113241. (<a href='https://doi.org/10.1016/j.asoc.2025.113241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial time series forecasting is fraught with challenges due to the significant noise and uncertainty in the financial market that can bias model prediction outcomes. However, deep learning, as an important branch of artificial intelligence, has demonstrated a strong ability in dealing with large-scale nonlinear data. Therefore, in order to solve this problem, this paper mainly focuses on the closing price of the CSI 300 index as the research object and proposes a new deep learning hybrid prediction model, CEEMDAN-Informer-LSTM. The model decomposes the signals using complete ensemble empirical mode decomposition of adaptive noise (CEEMDAN) and classifies the decomposed signals by the zero-mean T-hypothesis testing method into high-frequency (H-IMF) and low-frequency components (L-IMF). In order to utilize the prediction advantages of different models in different frequency ranges, as well as to more accurately capture the intrinsic patterns and features in the signals, the combination of Informer prediction of H-IMF and Long Short Memory Model (LSTM) prediction of L-IMF is used to form a hybrid model. In the empirical study, this paper is presented by comparing with BP, RNN, LSTM, Informer, Transformer, iTransformer, CEEMDAN-BP, CEEMDAN-RNN, CEEMDAN-LSTM, CEEMDAN-Informer, CEEMDAN-Transformer, CEEMDAN-iTransformer models are compared and four loss functions, MAE, RMSE, MAPE, and R 2 , are selected to evaluate the model performance. The experimental results show that Informer has the best results in individual model prediction accuracy, followed by iTransformer, Transformer, LSTM, BP, and RNN. The prediction performance of the proposed CEEMDAN-Informer-LSTM hybrid prediction model is higher than all other models. In order to verify the robustness of the model, this paper has found out that the proposed hybrid model has good prediction accuracy and also exploited the application of the model in the stock market through multi-step prediction, MCS confidence test, dataset discussion, data leakage processing, replacing experimental data, and constructing a simple quantitative trading investment strategy from which the proposed hybrid model has good prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Jiang-Cheng Li and Li-Ping Sun and Xiao Wu and Chen Tao},
  doi          = {10.1016/j.asoc.2025.113241},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113241},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing financial time series forecasting with hybrid deep learning: CEEMDAN-informer-LSTM model},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated RBF networks for periodic extensions for solving boundary value problems. <em>ASOC</em>, <em>177</em>, 113238. (<a href='https://doi.org/10.1016/j.asoc.2025.113238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the radial basis function networks (RBFNs) are trained to extend a non-periodic function to become a periodic one in a rectangular domain, from which Cartesian-grid-based partial-differential-equation (PDE) solvers can be applied. The networks are constructed through integration instead of the usual differentiation. The presence of the integration constants results in a higher dimension space in the hidden layer, which enhances the quality of approximation of the networks. In addition, the resulting integrated basis functions are applied to sinusoidal functions, which guarantees that the approximating function in the extended domain is naturally periodic. With the RBF width, amplitude and phase shift being adjustable parameters, the equations representing the networks are nonlinear and they can be solved by using the Levenberg–Marquardt method. Numerical experiments demonstrate that the iterative convergence is fast, the residual is low and the approximating function in the extension domain possesses a low level of fluctuation. The proposed networks are then utilised in conjunction with some high-order discretisation methods based on RBFs to enable the PDE in a non-rectangular domain to be solved in a rectangular one. The task of discretising a continuous spatial domain is very economical as a simple Cartesian grid can be used to represent the computational/extended domain. Linear and nonlinear problems are considered and the IRBF results are compared with the analytic solutions and numerical results produced by the finite difference and finite element methods. Numerical experiments demonstrate that the high-order discretisation methods are still able to produce their fast rates of convergence.},
  archive      = {J_ASOC},
  author       = {N. Mai-Duy and Y.T. Gu and K. Le-Cao and C.M.T. Tien},
  doi          = {10.1016/j.asoc.2025.113238},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113238},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrated RBF networks for periodic extensions for solving boundary value problems},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrusion detection in IoT and wireless networks using image-based neural network classification. <em>ASOC</em>, <em>177</em>, 113236. (<a href='https://doi.org/10.1016/j.asoc.2025.113236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Telecommunication networks play more and more important role in our modern times, and there are significant security risks associated with both wireless and wired networks. These risks stem from various malicious actions and security threats that have emerged with the development of Fourth Generation (4G), Fifth Generation (5G), and Internet of Things (IoT) networks. Machine learning (ML) algorithms have been applied to Intrusion Detection Systems (IDSs) due to their capacity to their ability to detect complex network traffic patterns. Deep learning (DL) networks are highly effective in processing images and videos and they have potential to solve other types of data. Given the characteristics of network traffic records used for intrusion detection in wireless and wired networks, we propose a simple data preprocessing method to convert the data into a grid-structured format, making it compatible with image processing networks. To validate the proposed structure, modified LeNet networks have been used for intrusion detection based on the NSL-KDD and CICIoV2024 (Canadian Institute for Cybersecurity Internet of Vehicles 2024 dataset) benchmark datasets. The simulation results indicate that methods based on extracted features may not always guarantee improved performance. The proposed Image Classification Neural Network-based Intrusion Detection (ICNN-ID) outperforms the compared existing methods. The multiclass classification experimental results show that the proposed LeNet-based IDS achieved a test accuracy (TAC) of 89.97% for NSL-KDD and nearly 100% (99.996%) for CICIoV2024. Additionally, it offers higher accuracy and improved robustness compared to a one-dimensional CNN and a recent deep learning model that integrates deep convolutional neural networks (DCNN) and bidirectional long short-term memory (BiLSTM).},
  archive      = {J_ASOC},
  author       = {Yanxia Sun and Zenghui Wang},
  doi          = {10.1016/j.asoc.2025.113236},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113236},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intrusion detection in IoT and wireless networks using image-based neural network classification},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mutable hierarchy feature selection based on generalized fuzzy rough sets. <em>ASOC</em>, <em>177</em>, 113233. (<a href='https://doi.org/10.1016/j.asoc.2025.113233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical classification divides data into correlated sub-tasks from coarse to fine. Compared to flat classification, it is more complex and suffers from the curse of dimensionality. Existing hierarchical Fuzzy Rough Sets (FRS) methods only stay at the fine-grained to select the features, which is a fine-grained search strategy. Thus, we propose a coarse-grained search strategy and use Generalized Fuzzy Rough Sets (GFRS) to enhance its robustness. Furthermore, we introduce the concept of tree fragmentation. Though assessing the degree of fragmentation in the tree structure, it selects an appropriate granularity search strategy for hierarchical tree-structured datasets. Finally, we combine both granularity search strategies and collectively named - Mutable Hierarchy Search Strategy (MHSS); the entire proposed algorithm is named - Mutable Hierarchy Feature Selection Based on Generalized Fuzzy Rough Sets (MHFS). We compare two FRS-based feature selection algorithms, three hierarchical optimization methods, and six flat feature selection methods. Extensive experiments demonstrate the performance of our method.},
  archive      = {J_ASOC},
  author       = {Zilong Lin and Yaojin Lin and Chenxi Wang and Jinkun Chen},
  doi          = {10.1016/j.asoc.2025.113233},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113233},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mutable hierarchy feature selection based on generalized fuzzy rough sets},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scale-aware triple semantic deep network for polarimetric SAR image classification. <em>ASOC</em>, <em>177</em>, 113232. (<a href='https://doi.org/10.1016/j.asoc.2025.113232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has demonstrated outstanding performance in polarimetric synthetic aperture radar (PolSAR) image classification. However, complex PolSAR scenes typically contain terrain objects of varying scales and orientations, and existing methods that rely on a single network architecture struggle to effectively capture both multi-scale terrain objects and precise edges. To address this challenge, we propose a scale-aware triple semantic deep network for PolSAR image classification, which is a novel “divide and conquer” framework. This network divides the scene into homogeneous, heterogeneous, and boundary regions, and then customizes specific subnetwork to learn scale-aware features for each region. For the boundary regions, a simple convolutional neural network (CNN) is designed to capture local details effectively. For homogeneous regions, a superpixel-based graph convolutional network (SGCN) is utilized to extract contextual features. For heterogeneous objects, a larger-scale c -hop SGCN is developed to capture global semantic information. These subnetworks are integrated with an attention mechanism to select relevant features and minimize redundancy. Experiments on four real PolSAR datasets demonstrate that our method outperforms state-of-the-art approaches, particularly in classifying heterogeneous objects and achieving precise edges.},
  archive      = {J_ASOC},
  author       = {Haiyan Jin and Junfei Shi and Linjing Xu and Mengmeng Nie and Tiansheng He and Junhuai Li and Maoguo Gong},
  doi          = {10.1016/j.asoc.2025.113232},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113232},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Scale-aware triple semantic deep network for polarimetric SAR image classification},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From fluid relaxations to double deep Q-network for dynamic multiplicity flexible job-shop scheduling. <em>ASOC</em>, <em>177</em>, 113231. (<a href='https://doi.org/10.1016/j.asoc.2025.113231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the random arrival of orders during production, companies may face challenges that result in the execution of a scheduling plan that is not optimal. The real-time dynamic arrival of orders is a critical issue. In view of this, this paper proposes a multi-policy double deep Q-network (MPDDQN) to optimize a Dynamic Multiplicity Flexible Job-Shop Scheduling Problem (DMFJSP), in which the dynamic arrival of job orders and multiple types of jobs are considered simultaneously. Specifically, firstly, a mathematical model for DMFJSP is established, and its validity is verified by the Gurobi solver. Secondly, we propose a modified fluid model that transforms discrete scheduling into a continuous flow to simplify the complexity of the DMFJSP. Based on this fluid model, we further develop a multi-metric selection strategy to assist in production scheduling decisions. Third, a Markov Decision Process (MDP) model is constructed, we extracted 19 features related to jobs and machines as references for state features and designed 20 composite rules as candidate actions. Subsequently, the MPDDQN algorithm achieves an effective balance between exploration and exploitation across various complex scenarios by combining prioritized experience replay, a soft update mechanism, and an adaptive action selection strategy. Finally, by conducting a comparative analysis with 10 well-known heuristic rules and 3 existing deep reinforcement learning-based scheduling approaches across 81 test cases, the proposed algorithm demonstrates superior generalization, robustness, and optimization capabilities in DMFJSP across scenarios with varying levels of complexity.},
  archive      = {J_ASOC},
  author       = {Xiaoyu Yang and Yuyan Han and Yuting Wang and Huan Li and Kaizhou Gao and Yiping Liu},
  doi          = {10.1016/j.asoc.2025.113231},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113231},
  shortjournal = {Appl. Soft. Comput.},
  title        = {From fluid relaxations to double deep Q-network for dynamic multiplicity flexible job-shop scheduling},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-learning brainstorm optimization for synchronization of operations and maintenance toward dual resource-constrained flexible job shops. <em>ASOC</em>, <em>177</em>, 113230. (<a href='https://doi.org/10.1016/j.asoc.2025.113230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semi-automated flexible job shop manufacturing scenarios such as furniture customization and circuit board assembly, machine and worker resources need to be flexibly assigned to the processing of each operation, to improve the efficiency of human-machine collaboration and reduce the makespan. Driven by the practical need, the dual resource-constrained flexible job shop scheduling problem (DRCFJSP) has gradually attracted attention from the academic community. However, preventive maintenance (PM) of machines as a key constraint tends to be overlooked in previous research. In this study, a synchronization optimization of the DRCFJSP and PM scheduling is proposed and a joint decision-making model is established, to strike a balance between flexible job shop operations and maintenance. A self-learning brainstorm optimization algorithm (SLBOA) is developed to solve the model. In the SLBOA, an adaptive K-means algorithm based on the silhouette method is employed for flexible clustering, and four global update strategies are adaptively selected using the Q-learning algorithm to facilitate an effective interaction of individuals between different clusters. Furthermore, two knowledge-based local search methods are used to enhance the exploration of elite solutions within the necessary neighborhood structure. Experimental results show that the SLBOA outperforms four state-of-the-art algorithms in solving the proposed DRCFJSP with PM.},
  archive      = {J_ASOC},
  author       = {Qi Yan and Hongfeng Wang and Shengxiang Yang and Yaping Fu},
  doi          = {10.1016/j.asoc.2025.113230},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113230},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-learning brainstorm optimization for synchronization of operations and maintenance toward dual resource-constrained flexible job shops},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time series prediction for cryptocurrency markets with transformer and parallel convolutional neural networks. <em>ASOC</em>, <em>177</em>, 113229. (<a href='https://doi.org/10.1016/j.asoc.2025.113229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the application of deep learning methods in time series analysis, specifically in the context of trend prediction in financial markets. The proposed model combines the transformer model, which is widely used in natural language processing tasks, with Convolutional Neural Networks (CNNs). Additionally, it adopts a parallel processing approach inspired by the Inception model within the CNN structure. A novel vectorization method is introduced to transform raw time series data into vectors, which are then utilized by the transformer encoder model. Moreover, in this paper, the impact of using transformer blocks on the performance of the models is investigated. To evaluate the effectiveness of the proposed model, it is trained and tested on three cryptocurrency data in different time periods. The results demonstrate that incorporating transformer and parallel CNN blocks enhances the performance of neural network models in time series classification tasks. This consequence is achieved by comparing the performance results with state-of-the-art structures. Various performance metrics are used to compare different model structures in trend prediction, including their behavior within a trading strategy. The findings reveal that utilizing transformer blocks improves the prediction performance of prediction models, with the proposed model demonstrating superior results in predicting future trends, and achieving 57 percent accuracy on Bitcoin's test data. Overall, this research offers valuable insights into the application of deep learning for more accurate trend prediction in financial markets, presenting promising opportunities for further advancements in time series analysis.},
  archive      = {J_ASOC},
  author       = {Mohammad Ali Izadi and Ehsan Hajizadeh},
  doi          = {10.1016/j.asoc.2025.113229},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113229},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Time series prediction for cryptocurrency markets with transformer and parallel convolutional neural networks},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-oriented multi-domain adversarial network for fake news detection. <em>ASOC</em>, <em>177</em>, 113227. (<a href='https://doi.org/10.1016/j.asoc.2025.113227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of fake news on online social media has severely misled public perception of event authenticity. To combat this, various Fake News Detection (FND) methods have been developed for specific domains, typically requiring large amounts of labeled data for effective training. However, in some domains, labeled data remains scarce. Multi-domain Fake News Detection (MFND) provides a solution by training a model with data from multiple domains to identify news veracity across domains. Despite its potential, discrepancies in feature distributions across domains, known as domain shift, can negatively impact the effectiveness of joint training. To mitigate this, we propose the Task-oriented Multi-domain Adversarial Network ( TMDA-Net ), a novel MFND model designed to align feature distributions across domains within a unified feature space using domain adversarial learning. This approach enhances generalization in data-scarce domains. Given that FND is domain-dependent, we also propose a fusion mechanism with adjustable weights to incorporate domain-specific characteristics. Additionally, we introduce two task-oriented auxiliary classification tasks to efficiently align and integrate features, ultimately improving FND accuracy. Extensive experiments on publicly available Chinese and English datasets validate the effectiveness of our proposed model.},
  archive      = {J_ASOC},
  author       = {Guo Zeqi and Ouyang Jihong and Li Ximing and Li Changchun},
  doi          = {10.1016/j.asoc.2025.113227},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113227},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Task-oriented multi-domain adversarial network for fake news detection},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-inflated tweedie boosted trees with CatBoost for insurance loss analytics. <em>ASOC</em>, <em>177</em>, 113226. (<a href='https://doi.org/10.1016/j.asoc.2025.113226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Tweedie regression model is one of the most widely used regression models in the insurance industry. However, its effectiveness is often constrained by the complexity of underlying data dynamics, particularly in handling the large incidence of zero claims. In this paper, we explore advanced modifications to the Tweedie regression model in order to address its limitations in modeling aggregate claims for various types of insurance such as automobile, health, and liability. Our recommended approach involves a refined modeling of the zero-claim process, together with the integration of boosting methods in order to help leverage an iterative process to enhance predictive accuracy. This approach assumes that the inflated probability of zero is a function of the mean parameter, allowing for more efficient modeling of claim distributions. Although iterative boosting algorithms can introduce computational slowdowns, efficient implementations such as CatBoost facilitate parameter tuning and enhance predictive performance. We validate our methodology using an insurance telematics dataset, which presents additional complexities due to compositional feature variables, as we compare eight different Tweedie methodologies, including traditional models. Our modeling results reveal a marked improvement in model performance, showcasing its potential to deliver more accurate predictions suitable for insurance claim analytics.},
  archive      = {J_ASOC},
  author       = {Banghee So and Emiliano A. Valdez},
  doi          = {10.1016/j.asoc.2025.113226},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113226},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Zero-inflated tweedie boosted trees with CatBoost for insurance loss analytics},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sarcasm detection method based on modality inconsistencies and textual knowledge enhancement. <em>ASOC</em>, <em>177</em>, 113225. (<a href='https://doi.org/10.1016/j.asoc.2025.113225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm detection aims to identify emotional tendencies in tweets, which helps governments and enterprises monitor online public opinions. The Twitter platform can create messages, including images and texts. Existing sarcasm detection methods mainly focus on extracting high-level semantic information from images while ignoring textual information. However, previous research has demonstrated that text is more important than images in sentiment analysis tasks. Inspired by this, we reduce the involvement of image information and investigate the sarcasm detection from a textual perspective. First, we divide the text in the primary dataset into pure text and hashtags. The hashtags are fused with high-frequency words in the pure text. Then, considering the differences on the data distribution between the training corpus of Bidirectional Encoder Representation from Transformers (BERT) and the sarcasm detection corpus, we use the Twitter sentiment analysis corpus to further pre-train the BERT model, obtaining the Basic_BERT and Hash_BERT models as feature extractors for the pure text and hashtags. Furthermore, to better play the role of the text in this task, a cross-gate mechanism method is proposed by a cross-attention transformer module and a similarity constraint. The cross-attention transformer module is used to generate a representation of intra-modal and inter-modal fusion while the similarity constraint is used to achieve a balance between the original modal representation and the fused modal representation. On the sarcasm detection dataset, the proposed model achieves an F1-score of 87.22%, an improvement of 3.30% over the most advanced model.},
  archive      = {J_ASOC},
  author       = {Yuxin Han and Runtao Yang and Mingyu Zhu and Lina Zhang},
  doi          = {10.1016/j.asoc.2025.113225},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113225},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A sarcasm detection method based on modality inconsistencies and textual knowledge enhancement},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Option price prediction based on optimized decomposition-dynamic ensemble and text mining. <em>ASOC</em>, <em>177</em>, 113224. (<a href='https://doi.org/10.1016/j.asoc.2025.113224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper primarily focuses on the decomposition-ensemblehybrid model and proposes an optimized decomposition-dynamic ensemble algorithm. Additionally, it employs text mining techniques to construct the Investor Sentiment Index (ISI), which is ultimately applied to the prediction of option prices. Through empirical analysis of option data from the SSE 50 ETF, CSI 300 ETF, and CSI 500 ETF, we find that the dynamic ensmble method within the decomposition-ensemble hybrid model not only enhances predictive accuracy but also improves the stability of the model. Furthermore, the effectiveness of the dynamic hybrid model is validated through a rolling window decomposition-prediction framework. In terms of using ISI for option price prediction, the constructed ISI significantly enhances the predictive performance of both single and hybrid models. Notably, the ISI generated by FinBERT outperforms that constructed using traditional dictionary methods. Moreover, in the context of hybrid models, ISI effectively improves the low-frequency and high-frequency components across various hybrid models, while exerting relatively minor influence on the mid-frequency modal components.},
  archive      = {J_ASOC},
  author       = {Weiyi Kang and Suisui Chen},
  doi          = {10.1016/j.asoc.2025.113224},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113224},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Option price prediction based on optimized decomposition-dynamic ensemble and text mining},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating example-based explainable artificial intelligence methods for breast cancer diagnosis. <em>ASOC</em>, <em>177</em>, 113222. (<a href='https://doi.org/10.1016/j.asoc.2025.113222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, various example-based explainable artificial intelligence (EBXAI) methods have been proposed for diverse medical diagnosis contexts. However, the evaluation of their explanatory performance has been rarely explored in the literature. To address this gap, this paper develops a multicriteria evaluation framework based on the applications of three representative EBXAI methods in breast cancer diagnosis (BCD). In the proposed framework, a unified mathematical model is constructed to formulate the determination of related examples for explanations as a similarity-based general optimization problem. By solving this problem, different types of related examples are obtained to explain the black-box predictions. Building upon these examples, a TOPSIS-based multicriteria selection process is designed, incorporating five quantitative evaluation criteria to assess the explanatory performance of EBXAI methods. Using five real-world breast cancer datasets, the proposed framework’s effectiveness is validated, and there are three important findings. First, the predictive performance of different black-box models varies across datasets, emphasizing the need to carefully select prediction models for BCD. Second, EBXAI methods exhibit more stable explanatory performance on tabular datasets compared to image datasets. Third, when multiple criteria are combined using the TOPSIS method, the explanation method based on factual examples consistently outperforms those based on contrastive and counterfactual examples. These findings underscore the advantages of the proposed framework over individual-criterion evaluation approaches and provide fresh insights into the application of EBXAI methods for BCD. Furthermore, the evaluation results generated by the framework are rigorously confirmed using statistical tests to ensure their robustness and reliability.},
  archive      = {J_ASOC},
  author       = {Che Xu and Peng Zhu and Zhenhua Fan and Yajun Ma},
  doi          = {10.1016/j.asoc.2025.113222},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113222},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating example-based explainable artificial intelligence methods for breast cancer diagnosis},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-organizing interval type-2 fuzzy neural network for multi-step time series prediction. <em>ASOC</em>, <em>177</em>, 113221. (<a href='https://doi.org/10.1016/j.asoc.2025.113221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data uncertainty is inherent in many real-world applications and poses significant challenges for accurate time series predictions. The interval type 2 fuzzy neural network (IT2FNN) has shown exceptional performance in uncertainty modelling for single-step prediction tasks. However, extending it for multi-step ahead predictions introduces further issues in uncertainty handling as well as model interpretability and accuracy. To address these issues, this paper proposes a new self-organizing interval type-2 fuzzy neural network with multiple outputs (SOIT2FNN-MO). Differing from the traditional six-layer IT2FNN, a nine-layer network architecture is developed. First, a new co-antecedent layer and a modified consequent layer are devised to improve the interpretability of the fuzzy model for multi-step time series prediction problems. Second, a new link layer is created to improve the accuracy by building temporal connections between multi-step predictions. Third, a new transformation layer is designed to address the problem of the vanishing rule strength caused by high-dimensional inputs. Furthermore, a two-stage, self-organizing learning mechanism is developed to automatically extract fuzzy rules from data and optimize network parameters. Experimental results on chaotic and microgrid prediction problems demonstrate that SOIT2FNN-MO outperforms state-of-the-art methods, by achieving a better accuracy ranging from 1.6% to 30% depending on the level of noises in data. Additionally, the proposed model is more interpretable, offering deeper insights into the prediction process.},
  archive      = {J_ASOC},
  author       = {Fulong Yao and Wanqing Zhao and Matthew Forshaw and Yang Song},
  doi          = {10.1016/j.asoc.2025.113221},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113221},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-organizing interval type-2 fuzzy neural network for multi-step time series prediction},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommender system based on noise enhancement and multi-view graph contrastive learning. <em>ASOC</em>, <em>177</em>, 113220. (<a href='https://doi.org/10.1016/j.asoc.2025.113220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural network has become the mainstream model of Knowledge-aware Recommendation because of its ability to capture higher-order information. Contrastive learning, due to its self-supervised learning paradigm, has been successfully employed to alleviate the sparse supervision signal problem in Knowledge-aware Recommendation models based on graph neural network. While graph neural networks and contrastive learning have advanced knowledge-aware recommendation, two critical limitations persist: (1) indistinguishable node representations due to data sparsity exacerbate the long-tail problem, and (2) suboptimal contrastive performance caused by non-uniform node distributions in embedding space. To mitigate the aforementioned issues, we propose the novel recommender system based on Noise Enhancement and Multi-View Graph Contrastive Learning (MNCL), the first framework that jointly optimizes node distribution uniformity and multi-view semantic alignment. Specifically, we inject uncertainty-aware noise into some graphs to achieve topology-invariant uniform representations and construct structural diversity-driven view for contrastive preference disentanglement. Unlike existing GNN-based contrastive methods that rely on heuristic augmentation or single-view alignment, MNCL dynamically regulates distribution entropy via noise intensity and preserves heterogeneous graph semantics through multi-view sampling. Extensive experiments on three public datasets show that our model achieves better results compared with the mainstream methods.},
  archive      = {J_ASOC},
  author       = {Duantengchuan Li and Jiayao Lu and Zhihao Wang and Jingxiong Wang and Xiaoguang Wang and Fobo Shi and Yu Liu},
  doi          = {10.1016/j.asoc.2025.113220},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113220},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Recommender system based on noise enhancement and multi-view graph contrastive learning},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Redistribution of humanitarian items in disaster management multi-period location–allocation problem under type-2 neutrosophic environment. <em>ASOC</em>, <em>177</em>, 113217. (<a href='https://doi.org/10.1016/j.asoc.2025.113217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disasters cause significant damage to both human life and property, as well as the necessary resources required for survival, within a short period of time. The sudden disturbance to society caused by a disaster often leaves many people homeless. With the increased frequency of disasters, researchers have become increasingly concerned with proposing suitable models to handle these situations. Only a suitable humanitarian logistic operation can rescue those affected by disasters by providing life-saving items. To this end, this assay introduces a multi-period, multi-objective facility location redistribution model that is considered under some uncertain environmental conditions. The primary objectives of this study are to minimize total transportation costs and required time while maximizing the satisfaction of demand for new relief camps. The concept of redistribution is employed to facilitate rapid responses in emergency situations and reduce logistics waste. Some parameters in the model are handled using type-2 neutrosophic numbers, which are subsequently converted into crisp values through a ranking function. Subsequently, an improved neutrosophic TOPSIS approach is developed based on neutrosophic programming and fuzzy TOPSIS to obtain non-dominated solution. Three numerical examples are provided to illustrate the problem briefly. Following this, a comparative study is conducted between the proposed approach and two existing techniques: neutrosophic programming and fuzzy TOPSIS. Additionally, a stability analysis is incorporated to assess the resilience of the designed model. In conclusion, the paper offers significant insights and highlights areas for future research in this field.},
  archive      = {J_ASOC},
  author       = {Lipika Shaw and Soumen Kumar Das and Sankar Kumar Roy and Leonidas Sakalauskas and Gerhard-Wilhelm Weber and Hiroshige Dan},
  doi          = {10.1016/j.asoc.2025.113217},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113217},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Redistribution of humanitarian items in disaster management multi-period location–allocation problem under type-2 neutrosophic environment},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-guided global optimization for expensive and black-box constrained multi-objective engineering design problems. <em>ASOC</em>, <em>177</em>, 113216. (<a href='https://doi.org/10.1016/j.asoc.2025.113216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained multi-objective optimization problems (CMOPs) involving expensive and black-box objectives and constraints pose significant challenges in engineering design problems (EDPs). This paper introduces a novel knowledge-guided evolutionary multitasking (EMT)-based global optimization algorithm (SA-EMCMO) specifically designed to address such computationally expensive and black-box CMOPs. SA-EMCMO approach models the optimization of an expensive CMOP as two interrelated tasks: the main task, which focuses on solving the original expensive CMOP, and the auxiliary task, which aims to optimize the objectives while neglecting the constraints. A surrogate-assisted constrained multi-objective evolutionary algorithm (SA-CMOEA) is designed to address the main task, while the auxiliary task continuously provides valuable knowledge to guide the main task’s optimization. Notably, the genetic information carried by both parent and offspring individuals is dynamically regarded as useful knowledge due to the complementary nature of the two tasks. This knowledge is transferred between the tasks to enhance their overall performance. Additionally, a dynamic sampling strategy is proposed to efficiently select final solutions from the main or auxiliary tasks for real function evaluations within a limited number of function evaluations. The effectiveness of SA-EMCMO algorithm is demonstrated through comprehensive comparison with state-of-the-art methods on 132 benchmark mathematical problems and its application to six EDPs, confirming its superior performance in solving complex EDPs.},
  archive      = {J_ASOC},
  author       = {Wenxin Wang and Huachao Dong and Xinjing Wang and Peng Wang and Jiangtao Shen and Yichen Jiang and Zhiwen Wen and Haijia Zhu},
  doi          = {10.1016/j.asoc.2025.113216},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113216},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge-guided global optimization for expensive and black-box constrained multi-objective engineering design problems},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic real-time aiming strategy optimization of multi-horizons heliostat fields. <em>ASOC</em>, <em>177</em>, 113215. (<a href='https://doi.org/10.1016/j.asoc.2025.113215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud prediction error is the dominant factor causing uncertainty and tracking errors in the heliostat aiming strategy optimization of Solar Power Tower (SPT) plants. In this work, an effective and scalable optimization method is proposed to address these cloud prediction errors. Specifically, a Multi-step Unscented Kalman filter (Mt-UKF) is developed to predict the SPT output flux trajectory under the influence of cloud prediction errors. Additionally, an improved Grey Wolf Optimization (GWO) algorithm is proposed, which integrates a reconfigured Grey Wolf social hierarchy with a Dimensionality Extension Learning (DEL) mechanism. This improvement enables the feedback correction of optimization errors in the heliostat field caused by cloud prediction errors. A simulated heliostat field is introduced as the experimental scenario to validate the proposed method. The Dimensionality Extension Learning Grey Wolf Optimization (DEL-GWO) algorithm is compared against four other state-of-the-art swarm intelligence algorithms. Experimental results and statistical tests demonstrate that the Mt-UKF combined with DEL-GWO exhibits high competitiveness and significantly outperforms the other algorithms. This combination effectively mitigates tracking errors induced by cloud prediction errors, demonstrating its robustness and applicability for heliostat field optimization.},
  archive      = {J_ASOC},
  author       = {Yi’an Wang and Zhe Wu and Dong Ni},
  doi          = {10.1016/j.asoc.2025.113215},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113215},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic real-time aiming strategy optimization of multi-horizons heliostat fields},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bi-objective discrete flower pollination algorithm for planning the collaborative disassembly of retired power batteries by humans and robots. <em>ASOC</em>, <em>177</em>, 113213. (<a href='https://doi.org/10.1016/j.asoc.2025.113213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-robot collaboration (HRC) for the disassembly of retired power batteries is attracting attention due to the complementary advantages of humans and robots. To optimize workforce allocation and enhance scheme flexibility, a disassembly line balancing and sequencing problem in HRC (DLBSP_HRC) is formulated, aiming to minimize the total cost and disassembly time by considering variations in skill levels, workforce sizes, and salary grades. Since DLBSP_HRC is an NP-hard problem, a novel modified discrete flower pollination algorithm with Q-learning (MDFPA_QL) is proposed. The algorithm integrates a driving strategy and a preference policy based on the unique characteristics of the problem and incorporates Q-learning to intelligently balance global and local searches. Subsequently, the disassembly of the Tesla Model S is used to validate the advantage of MDFPA_QL over four other advanced meta-heuristics. Furthermore, a knowledge-based selection mechanism is introduced, examining the relationship between delay penalties from large-scale tasks and the cost of employing multi-human-robot teams with various skills. Comparative analysis across different scenarios highlights the superiority of the multi-human-robot scheme over traditional methods.},
  archive      = {J_ASOC},
  author       = {Mengling Chu and Weida Chen},
  doi          = {10.1016/j.asoc.2025.113213},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113213},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bi-objective discrete flower pollination algorithm for planning the collaborative disassembly of retired power batteries by humans and robots},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretability adversarial example detection through multi-interpretation aggregation comparison. <em>ASOC</em>, <em>177</em>, 113212. (<a href='https://doi.org/10.1016/j.asoc.2025.113212'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network adversarial attacks have extended to the domain of interpretability, resulting in interpretation manipulation. Therefore, we propose a novel approach for detecting explanatory adversarial examples based on multi-interpretation aggregation comparison, called AGGEC (Aggregate Explain Compare). Our approach employs the aggregation of multiple different interpretation results to enable a comprehensive analysis of the disparities in texture shapes before and after the interpretation aggregation and employs gray level co-occurrence matrix to extract texture features before and after the interpretation aggregation to accentuate the discernible distinctions. These dissimilarities are subsequently utilized to train an external detector. AGGEC exhibits exceptional detection performance in black-box, gray-box, and white-box scenarios, achieving detection success rates of 99.4% and 95.6% on CIFAR-10 and ImageNet, respectively. Empirical results substantiate the efficacy of our proposed method in effectively identifying malicious manipulation of interpretation results.},
  archive      = {J_ASOC},
  author       = {Zigang Chen and Zhangqi Wang and Ding Pan and Tao Leng and Yuhong Liu and Haihua Zhu},
  doi          = {10.1016/j.asoc.2025.113212},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113212},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interpretability adversarial example detection through multi-interpretation aggregation comparison},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traffic scene perception via multimodal large language model with data augmentation and efficient training strategy. <em>ASOC</em>, <em>177</em>, 113210. (<a href='https://doi.org/10.1016/j.asoc.2025.113210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent mobility, driven by advancements in deep learning and computing power, enhances transportation efficiency and societal connectivity, fostering economic and urban development. Current computer vision solutions often struggle to capture the complex details or understand the context within traffic scenes, limiting advanced intelligent mobility and raising safety concerns. Multimodal Large Language Models (MLLMs), by integrating linguistic and visual data, can aid vehicles and transportation systems in gaining a deeper understanding of the real-world traffic scenes, offering solutions to current challenges. Nevertheless, existing approaches predominantly employ MLLMs as instruments for querying and engaging with traffic infrastructure, rather than empowering these models to genuinely comprehend the traffic environment. This limitation curtails the potential of MLLMs and may even pose safety risks. In this paper, we first introduce a data augmentation framework designed to transform raw data into datasets suited for specific training objectives, thereby addressing issues related to data scarcity. Secondly, we propose a learning rate-based staged training strategy that segments the training process into distinct stages. This strategy involves deploying datasets targeted at various training objectives according to the patterns of parameter changes observed in different stages, thereby enhancing the training efficiency of the model. Utilizing these methods, we present InsightGPT, a model endowed with robust understanding and reasoning capabilities in traffic scenarios. In experiments conducted across six tasks, InsightGPT consistently outperforms baseline MLLMs in evaluating both the overall traffic scenes and individual objects within it, demonstrating its superior traffic comprehension and reasoning abilities. InsightGPT’s parameters and deployment details are available at https://github.com/JinleLiu/InsightGPT .},
  archive      = {J_ASOC},
  author       = {Shuo Liu and Lei Shi and Yucheng Shi and Yufei Gao and Xiaole Sun},
  doi          = {10.1016/j.asoc.2025.113210},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113210},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Traffic scene perception via multimodal large language model with data augmentation and efficient training strategy},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RTAD-HIS: Regulated transformer architecture based anomaly detection framework towards security in healthcare IoT systems. <em>ASOC</em>, <em>177</em>, 113209. (<a href='https://doi.org/10.1016/j.asoc.2025.113209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of Healthcare Internet of Things (HIS) devices has transformed modern medical systems by enabling real-time patient monitoring, seamless data exchange, and automated diagnostics. However, existing methods struggle with accurate anomaly detection, often facing overfitting and false positives/negatives due to vast and complex datasets. Reliable solutions are essential to ensure precise and timely anomaly identification of security threats and system vulnerabilities. This paper introduces the Regulated Transformer-Based Anomaly Detection (RTAD) model, which employs a Self-Regulated Transformer-Convolutional Residual Neural Network (SRTrans-ConvRNN) for anomaly detection in HIS data streams. The RTAD model utilizes temporal pooling layers to enable feature extraction, Stateful Long Short-Term Memory (SLSTM) layers to enable sequential learning, Multi-Head Self-Attention (MHSA) to enable parallel processing, and position-wise feed-forward neural networks to boost data processing and feature extraction. Additionally, an Anomaly Scoring Module (ASM) evaluates and ranks deviations from normal behavior patterns, thus ensuring efficient and effective anomaly detection. Experimental findings show that the RTAD realizes precision of 98.8 %, recall of 99.2 %, F1-score of 99.3 %, and overall accuracy of 99.6 % tested on three publicly available datasets such as Edith Cowan University - Internet of Health Things (ECU-IoHT) dataset, the Washington University in St. Louis - Enhanced Healthcare Monitoring System 2020 (WUSTL-EHMS 2020), and the Canadian Institute for Cybersecurity Intrusion Detection System 2017 (CICIDS2017) dataset. From these fact, it is a reliable real-time solution for anomaly detection in HIS environments.},
  archive      = {J_ASOC},
  author       = {Arun Kumar Rai and Deepak Kumar Verma and Rajendra Kumar Dwivedi},
  doi          = {10.1016/j.asoc.2025.113209},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113209},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RTAD-HIS: Regulated transformer architecture based anomaly detection framework towards security in healthcare IoT systems},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Financial asset allocation strategies using statistical and machine learning models: Evidence from comprehensive scenario testing. <em>ASOC</em>, <em>177</em>, 113193. (<a href='https://doi.org/10.1016/j.asoc.2025.113193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate return and risk forecasts are critical for asset allocation; however, traditional models such as Mean-Variance (MV) and Risk Parity (RP) suffer from significant estimation errors and sensitivity to noise. We address these challenges by comparing six asset allocation strategies—four MV configurations and two RP-based approaches—against an equally weighted benchmark, using 111 stocks from the NASDAQ-100 and NASDAQ Financial-100 indices over 2000–2019. Two of the MV strategies, one of which we introduce, combine both econometric and Machine Learning (ML) forecasts for returns (via Facebook Prophet) and volatility (via GARCH), while another established ML variation of RP uses Hierarchical Risk Parity (HRP). The proposed hybrid MV strategy combines interpretable, regulatory-compliant methods with ML methodology. Our hypothesis was that ML strategies would significantly outperform their simpler counterparts, and that our proposed MV approach would be particularly competitive. Scenario testing was performed to assess the generalizability of the strategies. Rigorous scenario testing—varying stock sets, training periods, and hyperparameter configurations—reveals that: (i) our ML-enhanced Maximum Sharpe Ratio (MSR) strategy achieves up to 1490% higher Return on Investment (ROI) than the benchmark and 1390%–1909% higher than alternative strategies; (ii) Prophet’s competitive Normalized Mean-Square Error (NMSE) values confirm its robustness in forecasting noisy data; (iii) ML approaches exhibit sensitivity to training data, with compound annual returns declining by up to 5.24% under alternative training periods, reflecting macroeconomic regime-switching effects; and (iv) while ML methods often produce higher absolute returns, they do not consistently yield improved risk-adjusted performance, with non-ML strategies sometimes matching or surpassing ML Sharpe Ratios (SR). Notably, HRP outperformed naïve RP in all scenarios, consistently delivering higher SR. Overall, while ML methods show strong potential, their effectiveness is contingent on data selection and regime stability—underscoring the need for robust scenario analyses such as the one presented.},
  archive      = {J_ASOC},
  author       = {Bautista Penayo and Vedrana Pribičević and Andrej Novak},
  doi          = {10.1016/j.asoc.2025.113193},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113193},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Financial asset allocation strategies using statistical and machine learning models: Evidence from comprehensive scenario testing},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fire classification and detection in imbalanced remote sensing images using a three-sphere model combined with YOLOv5. <em>ASOC</em>, <em>177</em>, 113192. (<a href='https://doi.org/10.1016/j.asoc.2025.113192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fires pose a threat to global ecosystems, socio-economic structures, and public safety. Relying on remote sensing satellites for timely fire detection is of great significance for maintaining social stability and reducing economic losses. In real life, the number of fire samples is far greater than that of non-fire samples, leading to data imbalance. Traditional machine learning cannot effectively handle imbalanced fire data and often fails to unify the features of forest fires and non-forest fires with significant discrepancies. In this case, we propose a three-sphere model (TriSphere Model) specifically for imbalanced fire data. Unlike binary classification, TriSphere divides fire data into two categories and pushes them out into two large spheres separately. Furthermore, we adopt the idea of first classification and then detection, integrating YOLOv5 into our model, which not only speeds up the detection process but also improves the accuracy of detecting non-fire images. To simulate the distribution of imbalanced data, we set the imbalance rates of the dataset to 15%, 10% and 5% respectively. Experimental results show that compared to traditional machine learning, TriSphere can perform excellently on datasets with different imbalance rates. Ablation experiments also indicate that TriSphere-YOLOv5 can filter out about 70% of non-fire data and increase accuracy by 8.62%, 5.17% and 3.45% respectively.},
  archive      = {J_ASOC},
  author       = {Zidong Nie and Yitian Xu and Jie Zhao and Min Yuan},
  doi          = {10.1016/j.asoc.2025.113192},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113192},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fire classification and detection in imbalanced remote sensing images using a three-sphere model combined with YOLOv5},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective latent hierarchical feature fusion in multiple instance learning for whole slide image classification. <em>ASOC</em>, <em>177</em>, 113191. (<a href='https://doi.org/10.1016/j.asoc.2025.113191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning applications in computational pathology have revolutionized cancer diagnostics through histopathology tissue analysis of Whole Slide Images (WSIs). However, the gigapixel scale of WSIs presents significant challenges for traditional approaches. While Multiple Instance Learning (MIL) frameworks address these challenges by treating WSIs as bags of patches, existing methods often focus solely on information extraction modules, neglecting effective decoding of latent features. This paper introduces LHFF-MIL, a novel framework that emphasizes latent feature decoding and fusion in MIL. Our key contribution is the Latent Feature Distribution Decoder (LFDD), which efficiently decodes diverse information from high-dimensional semantics across different WSI resolutions, enabling explicit measurement of image informativeness for tumor detection. Evaluated on three real-world datasets of breast and gastric cancer, LHFF-MIL consistently outperforms competing methods, demonstrating statistically significant diagnostic accuracy improvement from 0.27% to 1.44% with at least 95% of confidence level. These improvements advance computational pathology by enhancing classification performance, potentially enabling more reliable computer-aided diagnosis systems in clinical settings. Code will be available upon acceptance.},
  archive      = {J_ASOC},
  author       = {Qingzhi Lan and Yaozu Wu and Weiping Ding and Jingping Yuan},
  doi          = {10.1016/j.asoc.2025.113191},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113191},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effective latent hierarchical feature fusion in multiple instance learning for whole slide image classification},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An attention-based balanced variational autoencoder method for credit card fraud detection. <em>ASOC</em>, <em>177</em>, 113190. (<a href='https://doi.org/10.1016/j.asoc.2025.113190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit card fraud detection poses a significant role in both academia and industries. Conventionally, manual checking is time-consuming and not accurate. With the advent of artificial intelligence (AI), numerous machine-learning approaches have emerged. Most of them are proven effective and efficient, especially those based on attention mechanisms. However, the highly imbalanced distribution of normal and fraudulent transactions increases the hardness of further performance improvement. So far, there exist three issues in current contributions: 1) the existing oversampling methods did not fully consider the imbalanced distribution theoretically; 2) the current methods neglected the importance of features and did not mine helpful information from them; 3) the layers of current attention-based methods are too shallow to capture enough characteristics of data. To address the above issues, we propose a novel method: Balanced-Variational AutoEncoder-Attention (Bal-VAE-Attention), which can significantly boost the existing fraud detection results. In our approach, we design a novel oversampling method that considers the imbalanced data distribution. Moreover, we apply an automatic feature selection procedure. Also, we deploy a deep multi-head attention structure to depict the complex inner structure of fraudulent data. We implement abundant experiments on two open-source credit card fraud datasets. Through substantial experiments and ablation study, we prove our proposed method’s effectiveness. To the best of our knowledge, it is superior to other state-of-the-art credit card fraud detection baselines.},
  archive      = {J_ASOC},
  author       = {Si Shi and Wuman Luo and Giovanni Pau},
  doi          = {10.1016/j.asoc.2025.113190},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113190},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An attention-based balanced variational autoencoder method for credit card fraud detection},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acoustic backdoor attacks on speech recognition via frequency offset perturbation. <em>ASOC</em>, <em>177</em>, 113188. (<a href='https://doi.org/10.1016/j.asoc.2025.113188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing deployment of deep learning-based speech recognition systems, backdoor attacks have become a serious security threat, enabling adversaries to implant hidden triggers that activate malicious behaviors while preserving model performance on benign inputs. However, existing acoustic backdoor attacks, whether in the time or frequency domain, often struggle to achieve sufficient stealthiness, as poisoned samples either disrupt semantic integrity or introduce perceptible artifacts. Moreover, these methods typically fail to strike an effective balance among attack efficacy, stealthiness, and robustness. To address these limitations, we propose Shadow Frequency (SF), a novel backdoor attack that leverages psychoacoustic-guided frequency offset perturbations to inject imperceptible yet model-sensitive signals near dominant spectral components. This design ensures auditory imperceptibility while maintaining high attack effectiveness and robustness. Experimental results show that SF achieves over 96% ASR with minimal impact on clean data accuracy, and remains effective under common defenses, validating its practicality for real-world deployment.},
  archive      = {J_ASOC},
  author       = {Yu Tang and Xiaolong Xu and Lijuan Sun},
  doi          = {10.1016/j.asoc.2025.113188},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113188},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Acoustic backdoor attacks on speech recognition via frequency offset perturbation},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integration of type-2 fuzzy TOPSIS and quality function deployment to address patient satisfaction in healthcare. <em>ASOC</em>, <em>177</em>, 113187. (<a href='https://doi.org/10.1016/j.asoc.2025.113187'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The healthcare sector seeks to maintain a sustainable competitive advantage through Patient Satisfaction (PS) against frequently changing global conditions. Analyzing the Voice of Patient (VoP) is very important for understanding patient preferences and developing effective strategies. This study focuses on improving current service quality and patient satisfaction by analyzing survey data collected from outpatient, inpatient, and emergency departments of a private university hospital in Türkiye. In this study, a double-sided hybrid methodology integrating Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) and Quality Function Deployment (QFD) is proposed utilizing Interval Type-2 Fuzzy Sets (IT2FS). This approach evaluates both patient expectations and the service components that hospitals need to develop to meet these needs. The criteria evaluated and valued by patients receiving service from different departments are not the same In this context, the findings reveal that each department needs different strategies to maximize satisfaction and quality. To the best of our knowledge, this is the first study to combine TOPSIS and QFD within IT2FS in the healthcare context and offers a versatile methodology that can be applied in various healthcare institutions.},
  archive      = {J_ASOC},
  author       = {Büşra Meniz and Sezin Ozturk Usun and Sema Akin Bas and Elif Yafez and Beyza Ahlatcioglu Ozkok},
  doi          = {10.1016/j.asoc.2025.113187},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113187},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integration of type-2 fuzzy TOPSIS and quality function deployment to address patient satisfaction in healthcare},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Study of occupational hazards under the environment of interval valued q-rung picture fuzzy sets. <em>ASOC</em>, <em>177</em>, 113185. (<a href='https://doi.org/10.1016/j.asoc.2025.113185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical group decision-making (DM) issues typically involve challenging situations when trying to assign appropriate values to the facts because of the ambiguity and unpredictability of the surrounding conditions. In order to address the uncertainty and imprecision that arise in DM challenges, interval-valued q -rung picture fuzzy sets (IV q -RPFSs) are more widely constructed. This work integrates the criteria importance through intercriteria correlation (CRITIC), the decision-making trial and evaluation laboratory (DEMATEL) approaches, and the multi-attributive border approximation area comparison (MABAC) method separately. The MABAC technique determines the distance of each decision from the boundary approximation area, making it a very reliable and useful tool for real-world problem resolution. In the CRITIC technique, the correlations between attributes are taken into account when determining the criteria weights, and the DEMATEL methodology is recognized as the most efficient method for determining the interactions between many criteria or components. In the light of these factors, we develop the CRITIC-MABAC and DEMATEL-MABAC methods for IV q -RPFSs. This article’s main objective is to identify the occupational risk that most significantly affects hospital medical staff health by utilizing the recommended methodologies. First, we use the CRITIC technique to determine the criteria weights. Additionally, we calculate the weights of the criteria using the DEMATEL approach. To determine the most hazardous work environment for hospital employees, the applicability of the suggested methodologies is investigated. To confirm the accuracy of the suggested techniques, we compare our findings with previous studies.},
  archive      = {J_ASOC},
  author       = {Gulfam Shahzadi and Musavarah Sarwar and Muhammet Deveci},
  doi          = {10.1016/j.asoc.2025.113185},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113185},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Study of occupational hazards under the environment of interval valued q-rung picture fuzzy sets},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lot streaming scheduling algorithm with variable sub-lots for solving a multi-objective flexible job shop scheduling problem. <em>ASOC</em>, <em>177</em>, 113184. (<a href='https://doi.org/10.1016/j.asoc.2025.113184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To adapt to the current market change of customized production, a long-distance guided gray wolf optimization algorithm is proposed to solve the lot streaming scheduling problem. Aiming to simulate the real production situation, the problem takes into account the switching time of the jobs as well as the limitation of the number of work equipment. A multi-objective optimization model is also proposed to minimize the maximum completion time and machine idle rate. Secondly, an operation-level encoding and the corresponding adaptive segmented decoding method are designed to improve the search efficiency of the algorithm and effectively reduce the solution space. Moreover, an innovative two-segment hierarchical initialization strategy is designed to ensure the high quality of the initial population, which initializes the workpiece order and the number of machines, respectively. In addition, an improved social hierarchy is proposed to properly guide the wolf pack search direction to avoid falling into local optimization. Finally, the effectiveness of the proposed method is verified with examples of different scales, and the superiority of this method is proved by comparing it with existing methods.},
  archive      = {J_ASOC},
  author       = {Shuai Shao and Gaochao Xu and Jiaxing Li and Xianqiu Meng and Xu Xu and Haidong Wang and Zhenjun Jin},
  doi          = {10.1016/j.asoc.2025.113184},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113184},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lot streaming scheduling algorithm with variable sub-lots for solving a multi-objective flexible job shop scheduling problem},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MGC-DMF: A traffic flow forecasting method based on multi-graph spatio-temporal convolution and dynamic metric fusion with multi-source basic information. <em>ASOC</em>, <em>177</em>, 113178. (<a href='https://doi.org/10.1016/j.asoc.2025.113178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction results are of great significance for regulating road characteristics. Multi-graph spatio-temporal convolution methods currently dominate traffic flow prediction. The richness of explicit and latent spatio-temporal features can improve the interpretability and robustness of the model. However, the existing multi-graph spatio-temporal convolution studies lack the mining of potential spatio-temporal features (congestion correlation, core node correlation) derived from the basic road features. Moreover, the existing spatio-temporal feature fusion methods may result in feature loss or redundancy. To address this issue, this paper proposes MGC-DMF: A traffic flow forecasting method based on multi-graph spatio-temporal convolution and dynamic metric fusion with multi-source basic information. MGC-DMF consists of the MGC module and the DMF module, which are respectively used for processing and feature extraction of multi-source basic information and the fusion of multi-source information. In the Multi-Graph Convolutional Module (MGC), this paper proposes two novel spatio-temporal correlation matrices based on multi-source foundational information. The first matrix, named Interaction Core Node Correlation (ICNC), reflects the internal correlations within traffic core node clusters. Travel Comfort Correlation (TCC) is a correlation matrix that integrates multiple sources of information and is used to evaluate the potential correlation of road driving comfort. Additionally, a Distance Weight Correlation Adjacency Matrix (DisW) is constructed based on distance information to serve as supplementary information. Compared to the traditional method of integrating different features using basic operations, a dynamic metric fusion module (DMF) proposed in this paper evaluates the importance of spatio-temporal features before fusion. Furthermore, residual connections, convolutional neural networks (CNN), and graph convolutional networks (GCN) are utilized to facilitate feature flow and extraction. Experimental results on three real-world datasets demonstrate that MGC-DMF exhibits superior performance compared to baseline models. The effectiveness of each component is verified through multi-graph ablation experiments and structural ablation experiments. The Key source code and data are available at https://github.com/charlotte404/MGC-DMF.git .},
  archive      = {J_ASOC},
  author       = {Ji Feng and Jiashuang Huang and Weiping Ding and Chang Guo and Zhenquan Shi},
  doi          = {10.1016/j.asoc.2025.113178},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113178},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MGC-DMF: A traffic flow forecasting method based on multi-graph spatio-temporal convolution and dynamic metric fusion with multi-source basic information},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A manifold-based adversarial autoencoder with fourier convolution for hyperspectral unmixing. <em>ASOC</em>, <em>177</em>, 113176. (<a href='https://doi.org/10.1016/j.asoc.2025.113176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral unmixing aims to decompose each subpixel into their pure endmembers and the corresponding proportions. But existing deep autoencoder-based hyperspectral unmixing methods often suffer from obstacles like endmember variability, local respective fields and insufficient use of inner structure. In the manuscript, we build a manifold-based Fourier adversarial autoencoder which regards generative adversarial mechanism as a utilization of prior information. This method combines manifold learning with adversarial autoencoder in order to promote the performance of hyperspectral unmixing. Specifically, firstly, in order to preserve local manifold structure, we add a discriminator to the autoencoder which uses the covariance matrices of a superpixel as real samples while covariance matrices of the abundance as fake samples; secondly, we add a regularization term of Laplacian eigenmap at the loss of autoencoder to in-depth abbreviate autoencoder solution space; thirdly, Fast Fourier Convolution modules are used to enhance multi-scale information fusion. At last, comparative experiments are conducted on three popular datasets, including Jasper, Urban4 and Samson, to validate the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Ziyang Guo and Meixia Xiao and Fa Zhu and Xingchi Chen and Achyut Shankar and Mazdak Zamani and Sushil Kumar Singh},
  doi          = {10.1016/j.asoc.2025.113176},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113176},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A manifold-based adversarial autoencoder with fourier convolution for hyperspectral unmixing},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-TFP: Integrating large language models with spatio-temporal features for urban traffic flow prediction. <em>ASOC</em>, <em>177</em>, 113174. (<a href='https://doi.org/10.1016/j.asoc.2025.113174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate urban traffic flow prediction is essential for intelligent transportation systems, playing a significant role in urban traffic management, congestion mitigation, and public transit optimization. While deep learning techniques have shown considerable success in capturing the spatio-temporal dynamics of traffic flow data, their heavy reliance on large volumes of labeled data limits their effectiveness in scenarios with limited data availability. Large language models (LLMs), however, have demonstrated a remarkable ability to capture intricate spatio-temporal patterns in time series analysis, offering a promising alternative for urban traffic flow prediction. Therefore, we propose an urban traffic flow prediction framework that integrates large language models with spatio-temporal features (LLM-TFP). Our approach includes a spatio-temporal tokenizer that converts raw traffic flow data into LLM-compatible tokens by utilizing timestep, time-of-day, and spatial embeddings, capturing the essential spatio-temporal features of the data. Furthermore, we develop a spatio-temporal adapter that links these tokens with a pre-trained BERT model, selectively freezing and fine-tuning certain parameters to retain the model’s language processing strengths. Experiments on the two real-world datasets have demonstrated that LLM-TFP surpasses 13 benchmark models in prediction accuracy and exhibits strong generalization capabilities in few-shot and zero-shot tasks.},
  archive      = {J_ASOC},
  author       = {Haitao Cheng and Zibin Gong and Chang Wang},
  doi          = {10.1016/j.asoc.2025.113174},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113174},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LLM-TFP: Integrating large language models with spatio-temporal features for urban traffic flow prediction},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical-aware uncertainty prompt learning for real-world blind image restoration. <em>ASOC</em>, <em>177</em>, 113173. (<a href='https://doi.org/10.1016/j.asoc.2025.113173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed notable progress in universal image restoration, which tackles multiple image degradations using a single model. However, these methods struggle to handle complex real-world scenarios due to the lack of paired real data and limited adaptability to unknown corruptions. To address these challenges, we propose a novel Physical-aware Uncertainty Prompt (PUP) paradigm for real-world blind image restoration. Specifically, instead of simply employing pre-synthesized degraded images, we develop a Physical-aware Degradation Modeling scheme (PDM) that considers multiple distortion factors to generate more authentic degraded data online during training. To adaptively handle unknown corruptions, we propose an Uncertainty-Prompted Fourier Transformer (UPFomer) for unified image restoration, which comprises two collaborative designs: Spatial-Frequency Selective Interaction (SSI) and Uncertainty Prompt Alignment (UPA). The former aggregates global frequency information and local spatial context for robust feature representation, while the latter interacts learnable prompts with SSI features via uncertainty weights to compute degradation-aware knowledge. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods on real-world images while ensuring favorable model efficiency.},
  archive      = {J_ASOC},
  author       = {Yuanjian Qiao and Mingwen Shao and Lingzhuang Meng and Wangmeng Zuo},
  doi          = {10.1016/j.asoc.2025.113173},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113173},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Physical-aware uncertainty prompt learning for real-world blind image restoration},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive weighting and nearest neighbor-based area control for imbalanced data classification. <em>ASOC</em>, <em>177</em>, 113171. (<a href='https://doi.org/10.1016/j.asoc.2025.113171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data classification problem poses an important challenge in data mining. Oversampling is an effective technique to handle this problem, but most methods based on oversampling usually suffer from three shortcomings: inadequate noise filtering, inability to accurately account for the importance of samples, and exacerbated class overlap. To this end, in this paper we propose an adaptive weighting and nearest neighbor-based area control method to cope with these shortcomings. The proposed method mainly consists of three steps. Specifically, we first introduce a sample position-based noise filtering approach to adequately filtering noise instances. And then, in order to accurately represent the importance of samples across locations, we further use relative density to weight the denoised minority classes adaptively. Finally, a nearest neighbor area control strategy is used to guide the generation of samples so as to avoid newly created samples penetrating into the majority class area and causing class overlap. Experimental results on synthetic and benchmark datasets demonstrate the effectiveness of the proposed method compared to some state-of-the-art sampling methods. The source code of the proposed method will be released at https://github.com/AHUT-MILAGroup/AWNNAC .},
  archive      = {J_ASOC},
  author       = {Wei Xue and Lilong Duan and Xudong Hong and Xiao Zheng},
  doi          = {10.1016/j.asoc.2025.113171},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113171},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive weighting and nearest neighbor-based area control for imbalanced data classification},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary learning driven load forecasting and demand response management model for smart grid. <em>ASOC</em>, <em>177</em>, 113169. (<a href='https://doi.org/10.1016/j.asoc.2025.113169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The smart meter data generated from the customers’ usage help utility providers to manage and control the electricity distribution among the customers for reliable services in the smart grid. Smart meter data provide an opportunity for utility providers to address these challenges by implementing demand response management (DRM), which enables customers to reduce consumption and balance load across the grid. For the demand response management in the smart grid, a multi-objective problem (MOP) is formulated considering three objectives, i.e., power scarcity, wastage and load factor. These objectives are computed by the newly proposed load forecasting neural network that precisely predicts upcoming loads on the demand side. The weights of this neural network are optimized using a hybrid learning approach that integrates evolutionary algorithms with Adam optimization. Additionally, the Multi-Objective Programming problem is efficiently addressed through the Non-dominated Sorting Genetic Algorithm (NSGA-III). Experimental results indicate that the proposed method surpasses state-of-the-art techniques in the terms of load forecasting. The proposed approach achieves a forecasting accuracy of up to 96.30% and a root mean squared error of 0.1367, based on the Open Energy Data Initiative (OEDI) dataset provided by the National Renewable Energy Laboratory of the United States.},
  archive      = {J_ASOC},
  author       = {Jatinder Kumar and Pooja Rani and Deepika Saxena and Ashutosh Kumar Singh and Aaisha Makkar},
  doi          = {10.1016/j.asoc.2025.113169},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113169},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary learning driven load forecasting and demand response management model for smart grid},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized connections and feature interactions for more efficient single-image desnowing. <em>ASOC</em>, <em>177</em>, 113153. (<a href='https://doi.org/10.1016/j.asoc.2025.113153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of single image desnowing primarily stems from the diversity and irregular shape of snow. While existing methods can effectively remove snow particles of various shapes, they often introduce distortion to the restored images. To address the challenges posed by the diverse shapes and sizes of snow particles, as well as the issue of distortion after desnowing, we propose a novel single image desnowing network called Star-Net. Our approach designs a Star type Skip Connection (SSC), which establishes information channels for different scale features. This design allows the network to aggregate all scale features, making it easier to handle snow particles with complex shapes and varying sizes. Additionally, we design a Multi-Stage Interactive Transformer (MIT) as the foundational module of Star-Net to solve image distortion. MIT explicitly models a range of essential image recovery features (e.g., local features, multi-scale features) by combining the advantages of convolution and attention mechanisms to restore regions of image distortion and further enhance the comprehension of different snow particle shapes and sizes. Furthermore, through experimental observations, we identify the presence of snow particle residuals within the SSC. To address this, we propose a Degenerate Filter Module (DFM) that filters out snow particle residuals in the SSC across spatial and channel domains. Extensive experiments on standard snow removal datasets and real-world datasets demonstrate that Star-Net achieves state-of-the-art performance on snow removal tasks. Importantly, our approach retains the original sharpness of the images while effectively removing snow.},
  archive      = {J_ASOC},
  author       = {Jiawei Mao and Yuanqi Chang and Xuesong Yin and Binling Nie and Yigang Wang},
  doi          = {10.1016/j.asoc.2025.113153},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113153},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized connections and feature interactions for more efficient single-image desnowing},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generative framework for detection and classification of plant leaf disease using diffusion network. <em>ASOC</em>, <em>177</em>, 113152. (<a href='https://doi.org/10.1016/j.asoc.2025.113152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture is a crucial driver of global economic growth, significantly contributing to employment, Gross Domestic Product (GDP), and the provision of essential food and raw materials. However, crop diseases, which often go unnoticed and untreated, cause substantial yearly reductions in farm productivity. While effective on small-scale farms, traditional manual methods for disease detection are impractical for larger operations due to their inefficiency, subjectivity, and vulnerability to human error. In contrast, deep learning technologies offer a promising solution by automating the accurate recognition and classification of crop diseases, thus enhancing efficiency and accuracy without requiring highly specialized expertise. This work presents a novel approach by introducing a diffusion model for classifying plant leaf diseases. We propose a novel diffusion-based deep learning technique (LeafDisDiff), which employs a U-Net-style architecture with position embeddings, group normalization, attention blocks, and integrated residual blocks, centring around a denoising approach for each time step. The proposed LeafDisDiff algorithm has shown superior classification accuracy across three crop disease data benchmarks: Plant Village, Bangladeshi Crop Disease, and Apple Disease. For example, LeafDisDiff outperforms several deep learning classifiers such as ViT, Cross ViT, VGG-19, CNN, ResNet-50, and EfficientNet-B3 and improves classification accuracy by approximately 9.2, 6.2, 5.7, 5.5, 2.5, and 0.7 percentage points, respectively, achieving an average accuracy of 99.49% over the Plant Village dataset. This work is motivated by recent advancements in diffusion probabilistic algorithms for generative image modelling and demonstrates the potential of these techniques in agricultural applications. Moreover, the implementation of this model can facilitate early and accurate detection of plant diseases, enabling prompt interventions like targeted pesticide application and selective breeding of disease-resistant crops. This proactive approach reduces yield losses, lowers chemical input reliance, and promotes sustainable farming. Furthermore, its automation lowers labour costs and enhances scalability, benefiting farms globally. The code is publicly available at the Link},
  archive      = {J_ASOC},
  author       = {Aryan Das and Rajul Mahto and Wentao Wang and Ali Jamali and Pravendra Singh and Swalpa Kumar Roy},
  doi          = {10.1016/j.asoc.2025.113152},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113152},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A generative framework for detection and classification of plant leaf disease using diffusion network},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The periodic sinc kernel: Theoretical design and applications in machine learning and scientific computing. <em>ASOC</em>, <em>177</em>, 113151. (<a href='https://doi.org/10.1016/j.asoc.2025.113151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the data-dependent Sinc kernel function specifically designed for kernel-based machine learning tasks involving oscillatory and periodic data. Mercer’s theorem is proven for the proposed kernel, and its derivatives are explicitly computed. Notably, it is demonstrated that these derivatives form real symmetric positive definite Toeplitz matrices. To evaluate the effectiveness of the proposed kernel in machine learning and scientific applications, comprehensive assessments are conducted on a range of real-world and benchmark datasets, covering both periodic and non-periodic regression and classification tasks. Furthermore, the accuracy of the proposed kernel is validated through simulations involving different configurations of fractional Helmholtz, time-fractional sub-diffusion, and time-fractional Korteweg–de Vries differential equations on an unbounded domain. The results indicate that the proposed method outperforms existing periodic kernels, including Fourier and Wavelet kernels, in terms of accuracy. To facilitate the practical implementation and adoption of these findings, an open-source Python package named sinc is introduced at the end of this paper.},
  archive      = {J_ASOC},
  author       = {Alireza Afzal Aghaei},
  doi          = {10.1016/j.asoc.2025.113151},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113151},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The periodic sinc kernel: Theoretical design and applications in machine learning and scientific computing},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable fuzzy rough approximate reduct computation in hyperbox space. <em>ASOC</em>, <em>177</em>, 113148. (<a href='https://doi.org/10.1016/j.asoc.2025.113148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Rough Set Theory (FRS) is an effective method for feature subset selection (reduct computation) for numeric decision systems. Several Fuzzy Rough Set reduct computation approaches were proposed using Fuzzy Similarity Matrix (FSM) or Fuzzy Discernibility Matrix (FDM) constructed on object space. However, constructing FDM or FSM on object space increases the complexity of the algorithm. In a stand-alone scenario, a representative instance-based reduct computation approach is useful in increasing the scalability while computing an approximate reduct. The FMNN-FRS algorithm is one such approach where the given dataset is represented by a much smaller interval-valued decision system using Fuzzy Min–Max Neural Network (FMNN) pre-processing. This work aims to further increase the scalability of FMNN-FRS by designing a novel MapReduce approach (MR_FMNN_FRRC) using centroid-based FMNN. We also proposed two more representative instances-based approaches using MapReduce through FMNN pre-processing. The relevance of the proposed approaches is assessed through a comparative experimental analysis with state-of-the-art Fuzzy Discernibility Matrix (FDM)-based MapReduce algorithm(MR_IFDMFS), Distributed Fuzzy Rough Set (DFRS)-based feature selection, MapReduce based Approach for Fuzzy Decision Reduct Computation (MR_IMQRA), and Hybrid-FMNN-FRS. The results amply validate that the proposed approaches achieve higher scalability with an ability to induce generalizable classification models while achieving dependency measures almost equal to 1.},
  archive      = {J_ASOC},
  author       = {Vadlamudi Aadarsh and Anil Kumar and P.S.V.S. Sai Prasad},
  doi          = {10.1016/j.asoc.2025.113148},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113148},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Scalable fuzzy rough approximate reduct computation in hyperbox space},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A swin transformer based on multi-directional-shift window attention and inductive bias for diagnosis of pleural effusion. <em>ASOC</em>, <em>177</em>, 113146. (<a href='https://doi.org/10.1016/j.asoc.2025.113146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of healthcare, deep learning has shown promise in addressing diagnostic challenges. However, existing methods often struggle with generalization due to overfitting on non-discriminative features and limited datasets. To address these limitations, Ultra-Multi-SWIN is introduced as a novel deep learning model for pleural effusion diagnosis using ultrasound images. The model incorporates physician-inspired inductive biases into its architecture, enabling it to focus on discriminative features while avoiding overfitting to irrelevant information. Specifically, a multi-directional-shift window structure captures spatial features dependent on direction, and a MASK-based masking module suppresses redundant non-ultrasound features. A dataset comprising 50 subjects and four levels of pleural effusion severity (large, moderate, small, none) is established to evaluate the model’s performance. Experimental results demonstrate that Ultra-Multi-SWIN achieves state-of-the-art performance, with average accuracies of 0.988 (subject-dependent) and 0.952 (subject-independent). Visualization and ablation studies further confirm the model’s ability to generalize effectively by focusing on clinically relevant regions. The open-source code is released at Ultra-Multi-SWIN , promoting broader adoption and future research.},
  archive      = {J_ASOC},
  author       = {Zekun Tian and Dunlu Peng and Debby D. Wang and Linna Zhang and Zheng Zou and Hejing Huang and Shiqi Zhang},
  doi          = {10.1016/j.asoc.2025.113146},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113146},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A swin transformer based on multi-directional-shift window attention and inductive bias for diagnosis of pleural effusion},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual object tracking: Review and challenges. <em>ASOC</em>, <em>177</em>, 113140. (<a href='https://doi.org/10.1016/j.asoc.2025.113140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking is a challenging research topic in computer vision. Numerous visual tracking algorithms have been proposed to solve this problem and achieved promising results. Traditional visual tracking algorithms can be categorized into generative and discriminative algorithms. Recently, deep learning based visual tracking algorithms attracted great attention from researchers due to their excellent performance. In order to summarize the development of visual object tracking, some studys have analyzed non-deep learning and deep learning visual tracking algorithms. In this paper, the most advanced tracking algorithms are comprehensively summarized, including both non-deep learning and deep learning based algorithms. First, traditional non-deep learning based tracking algorithms are categorized into generative and discriminative methods. The generative algorithms are summarized from three perspectives: kernel series, subspace series and sparse representation series, and the discriminative algorithms are summarized from two perspectives: correlation filtering series and deep features series. Then, deep learning based algorithms are divided into Siamese network series and Transformer series. Siamese network based algorithms are summarized from different innovation directions, and Transformer based algorithms are summarized from two perspectives: CNN-Transformer and Fully-Transformer. Moreover, the commonly used datasets and evaluation indicators are introduced in visual object tracking, as well as the results and analysis of representative algorithms. Finally, the challenges faced in visual object tracking were summarized and its future development trends were pointed out.},
  archive      = {J_ASOC},
  author       = {Zeshi Chen and Caiping Peng and Shuai Liu and Weiping Ding},
  doi          = {10.1016/j.asoc.2025.113140},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113140},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Visual object tracking: Review and challenges},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bio-inspired UAV swarm operation approach towards decentralized aerial electronic defense. <em>ASOC</em>, <em>177</em>, 113136. (<a href='https://doi.org/10.1016/j.asoc.2025.113136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The protection of critical assets and infrastructure from aerial attacks by adversaries is critical to national defense strategy. In this regard, autonomous aerial electronic defense with UAV swarm have the potential to distribute tasks and coordinate their operations to provide electronic countermeasures as an extra layer of defense. However, the challenge of decentralized coordination design of the swarm is a key bottleneck in UAV swarm electronic defense operations. This paper puts forward a decentralized honey bees-inspired multi-agent-based coordination design approach for UAV swarm in aerial electronic defense. The approach abstracts the coordination and planning of each UAV in the swarm as groups of agents with a hybrid hierarchical organization. Next, based on the behavior and operations of honey bees, a task planning model for coordination of the swarm is presented. Simulation results based on interception success rate, proportion of UAV working times, and interception path lengths show the proposed approach is capable of abstracting the swarm coordination problem and achieving a generally optimized aerial electronic defense. This approach shows promising results in designing a decentralized, responsive, and lightweight UAV swarm system capable of providing electronic countermeasures.},
  archive      = {J_ASOC},
  author       = {Weizhi Ran and Sulemana Nantogma and Shangyan Zhang and Yang Xu},
  doi          = {10.1016/j.asoc.2025.113136},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113136},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bio-inspired UAV swarm operation approach towards decentralized aerial electronic defense},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing the flexible job shop scheduling problem via deep reinforcement learning with mean multichannel graph attention. <em>ASOC</em>, <em>177</em>, 113128. (<a href='https://doi.org/10.1016/j.asoc.2025.113128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Job shop scheduling plays a crucial role in manufacturing informatization. Recently, significant progress has been made in terms of optimizing flexible job shop scheduling problems (FJSPs) via deep reinforcement learning (DRL). However, the complex structures of the disjunctive graphs encountered in FJSPs introduce a large amount of redundant information, and their oversized action spaces further increase the difficulty of training. To address these issues, a mean multichannel graph attention-proximal policy optimization (MCGA-PPO) model is proposed. First, the channel graph attention (CGA) mechanism reduces the amount of redundant information, allowing the agent to focus on task-relevant critical information. Second, for the first time, the overestimation phenomenon observed in FJSPs is explored in depth, and the MCGA method is developed to address the issue of overestimation from a single direction. MCGA employs information weighted across multiple channels to balance the estimation process. Furthermore, to address large action spaces, an entropy loss is introduced to optimize the exploration and exploitation processes of the agent. The experimental results confirm that our proposed model provides performance improvements of 1.22% and 1.29% on synthetic and classic datasets, respectively, demonstrating its effectiveness in addressing complex FJSPs.},
  archive      = {J_ASOC},
  author       = {Dailin Huang and Hong Zhao and Jie Cao and Kangping Chen and Lijun Zhang},
  doi          = {10.1016/j.asoc.2025.113128},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {113128},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing the flexible job shop scheduling problem via deep reinforcement learning with mean multichannel graph attention},
  volume       = {177},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision-making method combining machine learning and expert subjective judgment and its application to typhoon-induced house damage assessment. <em>ASOC</em>, <em>176</em>, 113235. (<a href='https://doi.org/10.1016/j.asoc.2025.113235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining high-quality data often poses significant challenges in real-world scenarios, resulting in poorly performing traditional machine learning (ML) models. To address this issue, this study developed a decision-making approach that combines ML with expert subjective examination and applied it to assessing house damage caused by typhoons. First, an ML model was constructed based on similar cases, selecting data from the optimal number of similar cases as the training data, thereby significantly improving data quality. Subsequently, a decision-making method was developed based on evidential reasoning. By integrating the predictive results of multiple ML models, the advantages of various models were utilized to enhance prediction accuracy and robustness. Additionally, expert opinions were integrated to introduce domain knowledge and experience, further optimizing the prediction results. Finally, experiments verified the effectiveness of the proposed decision-making method in evaluating house damage caused by typhoons and compared it with traditional ML algorithms. The results indicate that the proposed method provides a flexible decision-making approach that combines ML and expert subjective examination, thereby effectively enhancing decision accuracy.},
  archive      = {J_ASOC},
  author       = {Sheng-Qun Chen and Hai-Liu Shi and Ying-Ming Wang and Li-Ting Chen},
  doi          = {10.1016/j.asoc.2025.113235},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113235},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decision-making method combining machine learning and expert subjective judgment and its application to typhoon-induced house damage assessment},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting high increases in stock prices using text mining and data resampling techniques. <em>ASOC</em>, <em>176</em>, 113228. (<a href='https://doi.org/10.1016/j.asoc.2025.113228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text mining techniques have been demonstrated their effectiveness in developing stock prediction models, in which most of them focus on predicting whether the future stock price will rise or fall as a binary classification problem. However, in practice, existing prediction models cannot fulfill a well-defined investment portfolio composed of high-, medium-, and low-risk target stocks for different levels of return on investment (ROI). In order to achieve this practical demand, the prediction models should be able to predict different stock rise ratios for the investment portfolio. To construct this kind of prediction models, the class imbalance problem occurs in the training datasets that the number of data examples in the high-rise class is much less than the ones in the nonhigh-rise class. Therefore, the aim of this paper is to examine the performances of text mining-based stock prediction models by different machine learning and deep learning techniques in predicting different high-stock-price ratios, including 3 %, 5 %, 7 %, and 9 %. In addition, different data resampling techniques are employed to rebalance the class imbalanced training datasets to construct the prediction models for performance comparison. The experimental results indicate that one-class classifiers, such as one-class support vector machine and isolation forest, perform very well over the class imbalanced datasets in terms of AUC rates and the type I error of misclassifying high-rise cases into the nonhigh-rise class. Furthermore, after rebalancing the training datasets using over- and hybrid sampling algorithms, most classifiers show certain performance improvement, where hybrid sampling is the better choice than oversampling.},
  archive      = {J_ASOC},
  author       = {Chih-Fong Tsai and Ming-Chang Wang and Wei-Chao Lin and Xin-Yu Zheng},
  doi          = {10.1016/j.asoc.2025.113228},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113228},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting high increases in stock prices using text mining and data resampling techniques},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tomato leaf disease recognition based on fine-grained interpretable knowledge distillation model for smart agricultural. <em>ASOC</em>, <em>176</em>, 113195. (<a href='https://doi.org/10.1016/j.asoc.2025.113195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale smart agricultural plantations, in order to utilize computer vision technology for automatic recognition of Tomato Leaf Diseases (TLD) and improve the intelligence level of Smart Agricultural Internet of Things (SAIoT), this paper designs a novel Fine-Grained Interpretable Knowledge Distillation (FGIKD) model. Firstly, based on Deformable Dilated Convolution (DDC) and Simplified Self-Attention (SSA) mechanism, a new Deformable Multi-Scale Perception (DMSP) spatial attention module is designed to integrate the irregular local perception ability of DDC with the global modeling ability of self-attention, thereby enhancing the low-level visual feature extraction capability of the model. Secondly, based on Cross-Layer Feature Fusion (CLFF) and Graph Self-Supervised Learning (GSSL), a new Fine-Grained (FG) feature extraction module is designed to alleviate the problem of "high intra-class variance and low inter-class variance" in TLD images. Thirdly, DMSP and FG distillation functions are designed to transfer the knowledges from teacher network to student network, enabling it to achieve performance close to the teacher network with a small number of parameters. Finally, combining class activation maps with regional confidence weighting technique, a new CNN model post-hoc explanation scheme is designed in the form of "saliency map". In the comparison experiments of standard dataset validation and real-world application testing, the knowledge-distilled student network achieves 98.13 % and 97.56 % TLD recognition accuracies, while the number of model parameters is only 2.921MB, which can meet the requirements of SAIoT terminal model deployment.},
  archive      = {J_ASOC},
  author       = {Daxiang Li and Cuiyun Hua and Ying Liu},
  doi          = {10.1016/j.asoc.2025.113195},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113195},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tomato leaf disease recognition based on fine-grained interpretable knowledge distillation model for smart agricultural},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing performance evaluation of green supply chain management practices with linguistic q-rung orthopair fuzzy information. <em>ASOC</em>, <em>176</em>, 113194. (<a href='https://doi.org/10.1016/j.asoc.2025.113194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Green supply chain management (GSCM) and environmentally conscious manufacturing have emerged as critical strategies for companies to enhance operational efficiency, reduce environmental impact, and improve profitability and market competitiveness. To address the inherent fuzziness and uncertainty in selecting optimal GSCM practices, this paper proposes a novel decision framework by integrating Criteria Importance Through Intercriteria Correlation (CRITIC) and Evaluation based on Distance from Average Solution (EDAS) methods under the linguistic q-rung orthopair fuzzy (Lq-ROF) environment. To support this integration, we define Hamacher operations for Lq-ROF numbers and develop two Hamacher aggregation operators. Then, the Lq-ROF-CRITIC-EDAS approach is designed to solve multi-criteria group decision-making problems by using these proposed aggregation operators. Moreover, the relative weights of the evaluation criteria for GSCM practices were determined using the Lq-ROF-CRITIC model. A real-world decision problem of evaluating the performance of GSCM practices is solved to verify our suggested approach. In addition, the sensitivity analysis is also carried out by changing the parameter’s value to check the consistency of the ranking order. Finally, the proposed model is compared with existing approaches to demonstrate its strength. The sensitivity and comparative analyses reveal that the suggested technique offers greater feasibility, reliability, and precision in ranking alternatives, outperforming traditional methods in handling uncertainty and aligning with real-world GSCM requirements.},
  archive      = {J_ASOC},
  author       = {Shahid Hussain Gurmani and Harish Garg and Huayou Chen and Zhifu Tao and Zhao Zhang},
  doi          = {10.1016/j.asoc.2025.113194},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113194},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing performance evaluation of green supply chain management practices with linguistic q-rung orthopair fuzzy information},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical deep learning for multi-label imbalanced text classification of economic literature. <em>ASOC</em>, <em>176</em>, 113189. (<a href='https://doi.org/10.1016/j.asoc.2025.113189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the vast amount of economic literature available in this day and age, efficient and accurate text classification becomes increasingly important. We propose an extended version of the Hierarchical Deep Learning for Text Classification (HDLTex) approach, called HDLTex++. HDLTex++ applies hierarchical learning using neural networks to classify documents and is adapted for the multi-label classification of class imbalanced data. We use HDLTex++ to assign to economic publications category labels from the Journal of Economic Literature classification system, which has a hierarchical tree structure with three levels. The performance of HDLTex++ is compared to two methods based on Support Vector Machines (SVMs), one where the class hierarchy is fully incorporated, and one where only the tertiary subcategories are taken into consideration. Performance is evaluated using the standard F1-score and a novel hierarchical F1-score that accounts for both class imbalance and class hierarchy. Our findings show that HDLTex++ is more effective in the prediction of primary category labels, compared to both SVM models, and in the prediction of secondary category labels, compared to the hierarchical SVM model.},
  archive      = {J_ASOC},
  author       = {Sanne Lin and Flavius Frasincar and Jasmijn Klinkhamer},
  doi          = {10.1016/j.asoc.2025.113189},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113189},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical deep learning for multi-label imbalanced text classification of economic literature},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monitoring legume nutrition with machine learning: The impact of splits in training and testing data. <em>ASOC</em>, <em>176</em>, 113186. (<a href='https://doi.org/10.1016/j.asoc.2025.113186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision agriculture, powered by the capabilities of hyperspectral remote sensing, has the potential to revolutionize crop management. This study investigates the impact of different ratios in training-testing data splits and corresponding sample sizes on predicting nutritional attributes (neutral detergent fiber [NDF], acid detergent fiber [ADF], crude protein [CP]) and biophysical/biochemical characteristics (N accumulation, biomass) of forage produced by three legumes. We used in-situ hyperspectral data and pseudo satellite-based data from a future sensor known as CHIME in conjunction with machine learning [ML] to examine attributes in soybean [Glycine max], tepary bean [Phaseolus acutifolius], mothbean [Vigna aconitifolia]. Different techniques of ML were applied, including weighted k-nearest neighbors (KKNN), support vector machines (SVM), and random forest (RF). Results indicate larger sample sizes lead to better model performance, and emphasized the importance of adequate data for training. The KKNN approach was most robust for in-situ data, while RF handles variations in train-test splits effectively for CHIME. The CHIME data regularly outperforms in-situ data with small datasets (105 P). Optimal ratios for train-test splits for peak model performance across all models of ML were 70:30–80:20. The study provides valuable insights into optimal sampling strategies and modeling techniques for using hyperspectral remote sensing as a tool in precision agriculture.},
  archive      = {J_ASOC},
  author       = {H.K. Chinmayi and K. Colton Flynn and Gurjinder S. Baath and Prasanna Gowda and Brian Northup and Amanda Ashworth},
  doi          = {10.1016/j.asoc.2025.113186},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113186},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Monitoring legume nutrition with machine learning: The impact of splits in training and testing data},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-fault diagnosis with wavelet assisted stacked image fusion and dual branch CNN. <em>ASOC</em>, <em>176</em>, 113183. (<a href='https://doi.org/10.1016/j.asoc.2025.113183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rotating machine components are interconnected. If the machines are not monitored properly, it causes damage to the connected parts, causing catastrophic failure. Dependability on a single sensor or sensors of the same modality for multi-fault diagnosis influences decision-making. Therefore, multi-modality multi-sensor fusion has been used to gather distinct information. This work proposes a Wavelet Assisted Stacked Image Fusion (WASIF) with Dual Branch Convolutional Neural Network (DBCNN) to effectively diagnose multi-faults. At first, various multi-fault conditions in a test rig are introduced, which consist of conditions like faulty motor, faulty bearing, mechanical unbalance, shaft misalignment and their combinations. Thereafter, vibration and acoustic data are acquired at a varying speed condition. The acquired signatures are pre-processed and converted into time-frequency spectrums using Fourier Synchrosqueezed Transform (FSST). The vibration and acoustic spectrums are fused into vibro-acoustic spectrums using the WASIF technique. The generated spectrums are used for DBCNN training for multi-fault classification, and 98.8 % overall classification accuracy is achieved. In this paper, a separate ablation experiment is done along with a published literature comparison to justify the effectiveness of the selected parameters. The proposed fusion-based multi-fault diagnosis strategy would be helpful to the industries for incipient fault detection, inventory management and workforce allocation.},
  archive      = {J_ASOC},
  author       = {Rismaya Kumar Mishra and Anurag Choudhary and S. Fatima and A.R. Mohanty and B.K. Panigrahi},
  doi          = {10.1016/j.asoc.2025.113183},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113183},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-fault diagnosis with wavelet assisted stacked image fusion and dual branch CNN},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wavelet-denoised graph-informer for accurate and stable wind speed prediction. <em>ASOC</em>, <em>176</em>, 113182. (<a href='https://doi.org/10.1016/j.asoc.2025.113182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As global demand for renewable energy increases, wind power has become increasingly valued as a clean energy source. Effective wind speed forecasting is crucial for wind energy production and power grid's stability. To mitigate high-frequency interference in wind speed signals and improve spatiotemporal feature extraction, we propose a novel short-term wind speed prediction model called WST-Informer. Firstly, Wavelet Decomposition (WD) is fused to erase high-frequency noise in monitored signal series. Secondly, Informer encoder is designed to capture long-term temporal dependencies efficiently, and multiple cities' spatio-temporal maps are constructed through designing Residual Graph Convolutional Network (RS-GCN). Moreover, a new Attentional Feature Fusion (AFF) method is designed to fuse temporal and spatial features. Furthermore, the decoder of the Informer predicts outcomes using fused features. Additionally, outliers are more prone to bigger errors and highly curtail in real-world application, a Kernel Mean Squared Error (KMSE) loss function is introduced to further enhance their prediction. With real datasets from weather stations across five Danish cities and seven Dutch cities, extensive experiments were conducted, and the proposed model demonstrates reduced prediction error across multiple forecasting steps in both datasets, resulting in lower prediction errors during sudden wind speed changes, outperforming current state-of-the-art time series forecasting models.},
  archive      = {J_ASOC},
  author       = {Biao Yu and Zhenyu Lu and Weiwei Qian},
  doi          = {10.1016/j.asoc.2025.113182},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113182},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Wavelet-denoised graph-informer for accurate and stable wind speed prediction},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for optimal microgrid energy management with renewable energy and electric vehicle integration. <em>ASOC</em>, <em>176</em>, 113180. (<a href='https://doi.org/10.1016/j.asoc.2025.113180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development and utilization of renewable energy sources (RES) are gaining unprecedented attention as a response to the environmental, economic, and energy security challenges posed by non-renewable fossil fuels. Nonetheless, integrating renewable energy into large-scale power grids remains a complex endeavour, which constrains the widespread adoption of these sustainable energy sources. Microgrid, which can function both autonomously and in coordination with the larger grid, provides an effective solution for integrating RES into the broader power system. To coordinate and optimize various energy resources within the microgrids to meet demand while maintaining stability and efficiency, the deployment of an Energy Management System (EMS) is crucial. This paper proposes a deep reinforcement learning (DRL)-based real-time optimal energy management method to assist the EMS for microgrids in making optimal scheduling decisions. Electric vehicles are introduced as a new controllable power source into the MG, alongside uncontrollable photovoltaic and wind power sources, resulting in an enhancement of the self-balance capability of the entire system. The efficacy of our proposed methodology is validated via a case study. Specifically, in comparison to the traditional energy scheduling approach, our method is found to enhance the self-balancing rate by a maximum of 22.97% and augment the operator’s profit by as much as 33.74%. These results unequivocally demonstrate the superiority and practical value of our methodology in the relevant domain.},
  archive      = {J_ASOC},
  author       = {Baoyin Xiong and Lili Zhang and Yang Hu and Fang Fang and Qingzhi Liu and Long Cheng},
  doi          = {10.1016/j.asoc.2025.113180},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113180},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning for optimal microgrid energy management with renewable energy and electric vehicle integration},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning fuzzy decision trees for predicting outcomes of legal cases relating to intellectual property rights. <em>ASOC</em>, <em>176</em>, 113179. (<a href='https://doi.org/10.1016/j.asoc.2025.113179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legal cases involve specific terminology, use past judgments as references, and the entire legal process is expensive, both in terms of time and money. Further, it is not clear at the outset whether the expected judgment will prevail. In the context of trademark and copyright cases, the present paper develops a rule-based system that can be useful, for both lawyers and litigants, as an assisting tool to predict outcomes. The paper proposes a forecasting framework involving TF-IDF weighting scheme, Fuzzy C-means algorithm for clustering, the construction of decision trees using Gini Impurity Measure, and using Takagi–Sugeno fuzzy controller for efficient prediction. The dependent variable is binary, and we observe that the combination of specific words and their relative importance has a bearing on the judicial outcome. The paper goes beyond predicting outcomes based on relevant features, and suggests specific rules leading to outcomes of legal proceedings. Accuracy, Balanced Accuracy, Precision, Recall, and F-beta are used as forecasting efficiency metrics and the results indicate moderate forecasting efficiency.},
  archive      = {J_ASOC},
  author       = {Somnath Mukhopadhyay and Jayati Mukherjee and Devangshu Das and Ashaawari Datta Chaudhuri and Sunita Sarkar and Tamal Datta Chaudhuri and Kunal Paul},
  doi          = {10.1016/j.asoc.2025.113179},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113179},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning fuzzy decision trees for predicting outcomes of legal cases relating to intellectual property rights},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MODE/CMA-ES: Integrated multi-operator differential evolution technique with CMA-ES. <em>ASOC</em>, <em>176</em>, 113177. (<a href='https://doi.org/10.1016/j.asoc.2025.113177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolutionary paradigm based on decomposition is highly promising for resolving multi-objective optimization issues. As the solution converges towards the Pareto front, the exploration–exploitation relationship should be preserved in decomposition-based multi-objective evolutionary algorithms (MOEA/D). Although several proposed models based on MOEA/D have encouraging outcomes, they are not without drawbacks. The parent solution is chosen randomly from the neighbors or the entire population. Because traditional methods, such as single-mutation differential evolution, could not manage the computational demands of solving multi-objective issues, robust mechanisms for multi-objective evolution are needed. By putting out a multi-operator differential evolution variant with CMA-ES (Covariance Matrix Adaptation Evolution Strategy), this study seeks to enhance the exploration–exploitation relationship in MOEA/D. While CMA-ES is supposed to converge the solutions towards the Pareto front, mutation operators are useful for exploration. Through the facilitation of the clustering-based technique, which disintegrates the initial solution set into clusters, the method explicitly preserves diversity. The next generation is anticipated to contain the solutions closest to the ideal point throughout the entire objective space. The main objective is to maximize population variety by employing multiple mutation strategies and prioritize solutions closer to the optimal location in the decomposition-based paradigm to accelerate the convergence rate. Moreover, we evaluate the performance of the proposed algorithm with the MOEA/D state-of-the-art methods on two popular test suites. The experimental results demonstrate that the suggested technique outperforms the MOEA/D alternatives that are now available. This is because the search space is covered collectively, and maximal variety is achieved by integrating the distinct solutions from DE and CMA-ES.},
  archive      = {J_ASOC},
  author       = {Sakshi Aggarwal and Sarsij Tripathi},
  doi          = {10.1016/j.asoc.2025.113177},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113177},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MODE/CMA-ES: Integrated multi-operator differential evolution technique with CMA-ES},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service defects identification by integrating fuzzy clustering and optimization model with quality function deployment. <em>ASOC</em>, <em>176</em>, 113175. (<a href='https://doi.org/10.1016/j.asoc.2025.113175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service defect is critical for the success of fresh product instant delivery (FPID) service. Therefore, the identification and prioritization of such defects is an important demand from FPID companies. Quality function deployment (QFD) can help companies identify and prioritize service defects. However, existing research based on QFD failed to consider the subjectiveness and similarity of evaluation information simultaneously while neglecting the integration of service quality characteristics and customer expectations. To fill these gaps, a novel methodology based on QFD model and optimization model is proposed. Firstly, a two-phase QFD framework for FPID service is developed to establish service quality characteristics, strengthening the relationship between customer requirements and service quality characteristics. To address the fuzziness and similarity of evaluation information, a new analysis model, named L-FCM-HOQ, by integrating Linguistic terms, Fuzzy C-Means (FCM), and House of Quality (HOQ) is proposed to determine related weights in QFD. Secondly, a decision variable, "improvement rate" is introduced to determine service defects among service quality characteristics and quantify their improvement degrees. Based on this decision variable, a multi-objective optimization model is constructed to derive optimal improvement strategies aligned with customer expectations. Finally, the proposed methodology is illustrated with a case study regarding a FPID company of China, and its efficiency and advantages are verified via comparative analysis. The proposed methodology effectively identifies service defects and assesses improvement potential, contributing to the development of FPID service quality and actionable insights to guide the FPID companyies in achieving sustainable improvement.},
  archive      = {J_ASOC},
  author       = {Xiaobing Li and Yujun Wang and Zhen He},
  doi          = {10.1016/j.asoc.2025.113175},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113175},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Service defects identification by integrating fuzzy clustering and optimization model with quality function deployment},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised feature contrast incremental learning framework for bearing fault diagnosis with limited labeled samples. <em>ASOC</em>, <em>176</em>, 113172. (<a href='https://doi.org/10.1016/j.asoc.2025.113172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, rotating machinery consistently introduces new fault classes, but intelligent fault diagnosis methods mostly rely on the closed-world assumption, expecting only known fault classes during testing. Moreover, obtaining a sufficient number of labeled samples is often challenging. These challenges constrain the application and reliability of intelligent diagnosis models in real-world scenarios. Semi-supervised incremental learning enables continuous learning of new fault classes in an open environment, relying on a small number of labeled samples and a certain number of unlabeled samples. To address the semi-supervised incremental learning problem of fault classes, semi-supervised feature contrast (SSFC) is proposed, a new approach for bearing fault diagnosis with limited labeled samples. Specifically, a feature contrastive loss incorporating enhancement strategies is designed, independent of labeled sample information. This approach enables the model to retain knowledge of old classes while learning about new ones. A label reconstruction mechanism based on class centroids is utilized, effectively leveraging the structural information inherent in the samples to support supervised training. A dynamic class prototype cosine classifier initialized by class centroids is devised to mitigate interference between knowledge of fault classes. Finally, two incremental fault diagnosis case studies are designed to evaluate the effectiveness of the proposed method. The fault diagnosis results indicate that SSFC can continuously learn knowledge of new fault classes with limited labeled samples and effectively alleviate catastrophic forgetting.},
  archive      = {J_ASOC},
  author       = {Xuyang Tao and Changqing Shen and Lin Li and Dong Wang and Juanjuan Shi and Zhongkui Zhu},
  doi          = {10.1016/j.asoc.2025.113172},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113172},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semi-supervised feature contrast incremental learning framework for bearing fault diagnosis with limited labeled samples},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A top-weighted classification method with expert ability characterization for failure mode and effect analysis. <em>ASOC</em>, <em>176</em>, 113168. (<a href='https://doi.org/10.1016/j.asoc.2025.113168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure mode and effect analysis (FMEA) is a useful tool to assess the potential risks of a system and provide necessary correction suggestions. This study proposes a top-weighted classification method with expert ability characterization for FMEA to deal with two crucial issues: few studies have attempted to characterize the ability of FMEA experts to provide accurate assessments in the risk assessment process, and the realistic feature that different risk categories have different importance was not considered. First, we develop a synergy theory-based weight iterative algorithm, by which the ability of experts is characterized and specific weight information is automatically generated at the element level of their assessment matrices. Then, we suggest a novel top-weighted distance measure that considers the importance of different risk categories, based on which the consensus-based top-weighted classification method (CTWCM) is proposed. After that, a simulation comparison experiment is designed to examine the performance of the CTWCM against the ABC analysis and ELECTRE-TRI methods. The numerical results show that the CTWCM outperforms the other two methods on all three key classification indices. In addition, a theoretical comparison is further presented to demonstrate our novelty and significance. Finally, the proposed method is illustrated through a SARS-CoV-2 management case.},
  archive      = {J_ASOC},
  author       = {Sihai Zhao and Siqi Wu and Haiming Liang and Hengjie Zhang},
  doi          = {10.1016/j.asoc.2025.113168},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113168},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A top-weighted classification method with expert ability characterization for failure mode and effect analysis},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graphics processing unit-enabled path planning based on global evolutionary dynamic programming and local genetic algorithm optimization. <em>ASOC</em>, <em>176</em>, 113167. (<a href='https://doi.org/10.1016/j.asoc.2025.113167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel path planning method for real-time robotic path planning in a dynamic environment involving moving obstacles. It combines on a holistic platform a global approach to rapidly generate initial paths of prominent diversity and a heuristic approach to enable local path refinement for enhanced computational efficiency, exploration, and robustness. The global approach innovates a formulation that treats a path planning problem with a visibility graph as a Markov decision process and decomposes the process into many subproblems. A new evolutionary dynamic programming approach (EDP) is proposed to solve these subproblems in an iterative manner using graphics processing unit (GPU) computing to allow backpropagation of state values from goal to start points. The EDP generates multiple feasible initial paths with salient state values, each initializing an independent genetic algorithm (GA) optimization on waypoints only near the mobile robot, and all GAs are run in parallel on GPU, further improving exploration and convergence speed. The strategy to fully utilize CPU/GPU resources for various components in the pipeline is also established. The proposed algorithms are then implemented on an edge computing device (Jetson AGX Xavier) onboard a mobile robot (TurtleBot 3 Waffle Pi). Optimal paths can be continuously generated at the rate of 0.1 seconds/path, enabling successful obstacle avoidance and robot navigation through dynamic environments and, hence, verifying the real-time capabilities and accuracy of the present method. Compared to other benchmarks, the present method greatly enhances path planning robustness, computing speed, and path quality.},
  archive      = {J_ASOC},
  author       = {Junlin Ou and Ge Song and Yi Wang},
  doi          = {10.1016/j.asoc.2025.113167},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113167},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graphics processing unit-enabled path planning based on global evolutionary dynamic programming and local genetic algorithm optimization},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social network group decision-making with linguistic Z-number preference relations based on personalized individual semantics and trust driven. <em>ASOC</em>, <em>176</em>, 113166. (<a href='https://doi.org/10.1016/j.asoc.2025.113166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to preference relations (PRs) based on one-dimensional data description, linguistic Z-number (LZN) preference relations (LZPRs) exhibit more advantages in expressing uncertainty information when comparing objectives. However, the extant preference group decision-making (PGDM) with LZPRs focus on traditional group decision-making (TGDM) problems. In addition, there are certain shortcomings in the consistency analysis of LZPRs proposed in the PGDM with LZPRs. Therefore, this study will focus on discussing the PGDM with LZPRs based on dynamic social networks. Firstly, A new additively consistent concept for LZPRs is presented, and the social network structure based on LZN trust relationships is constructed. Furthermore, a synthetical personalized individual semantics (PIS) determination method based on consistency driven and social network driven is developed for the credibility of evaluation values in LZPRs. Secondly, a dynamic mixed experts weights determination method based on experts’ opinions and social network trust relationships between experts has been proposed, considering multiple indicators of experts’ opinions. Thirdly, a synthetical consensus improving algorithm based on dynamic trust-based feedback adjustment mechanism is designed by integrating optimization-based consensus strategy and identification rule (IR) and direction rule (DR) strategy. Finally, the rationality and effectiveness of the proposed method are verified through a numerical example. Meanwhile its merits are illustrated by comparison analyses.},
  archive      = {J_ASOC},
  author       = {Xiao-Yun Lu and He-Cheng Li and Ze-Hui Chen},
  doi          = {10.1016/j.asoc.2025.113166},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113166},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Social network group decision-making with linguistic Z-number preference relations based on personalized individual semantics and trust driven},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrigendum to “Privacy preserving verifiable federated learning scheme using blockchain and homomorphic encryption” [Appl. soft comput. 167 (Part b) (2024) 112405]. <em>ASOC</em>, <em>176</em>, 113165. (<a href='https://doi.org/10.1016/j.asoc.2025.113165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Ganesh Kumar Mahato and Aiswaryya Banerjee and Swarnendu Kumar Chakraborty and Xiao-Zhi Gao},
  doi          = {10.1016/j.asoc.2025.113165},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113165},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Privacy preserving verifiable federated learning scheme using blockchain and homomorphic encryption” [Appl. soft comput. 167 (Part b) (2024) 112405]},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic jump diffusion process informed neural networks for accurate american option pricing under data scarcity. <em>ASOC</em>, <em>176</em>, 113164. (<a href='https://doi.org/10.1016/j.asoc.2025.113164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pricing American options under jump diffusion models, such as the Merton model, presents significant challenges due to the early exercise feature and underlying price discontinuities. This paper introduces the PINN Merton model, a Physics-Informed Neural Network (PINN) framework that embeds the Merton partial integro differential equation (PIDE) into its loss function for American options pricing. By integrating sparse market data with financial physics, the model achieves robust pricing accuracy with limited training samples. We provide a rigorous derivation of the Merton PIDE from the Lévy process and prove the convergence of PINN Merton to the true option price under standard assumptions. Empirical evaluations on SPY and AAPL option datasets from June 2023 to September 2024 demonstrate that PINN Merton significantly outperforms traditional parametric models (e.g., Binomial Tree, Merton Jump Diffusion), data-driven baselines (e.g., Transformer, Neural Network, XGBoost), and PINN under the Black–Scholes formula (PINN BS), particularly in data-scarce scenarios. With only 200 training samples, PINN Merton achieves best performance, yielding an R 2 of 0.9899 for SPY Calls (vs. 0.9654 for traditional Binomial Tree and 0.9617 for PINN BS), R 2 of 0.9933 for SPY Puts (vs. 0.9924 for transformer), and R 2 of 0.9897 for AAPL Calls (vs. 0.9822 for transformer), respectively. These results underscoring the model’s effectiveness and generalizability across option types and underlying assets.},
  archive      = {J_ASOC},
  author       = {Qiguo Sun and Hanyue Huang and Xibei Yang and Yuwei Zhang},
  doi          = {10.1016/j.asoc.2025.113164},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113164},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stochastic jump diffusion process informed neural networks for accurate american option pricing under data scarcity},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network extension planning towards resilient urban critical infrastructures using deep reinforcement learning. <em>ASOC</em>, <em>176</em>, 113163. (<a href='https://doi.org/10.1016/j.asoc.2025.113163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As cities continue to grow, expanding metro networks becomes essential for optimizing urban transportation efficiency. Therefore, scientific strategic planning of metro networks is indispensable. This study proposes a deep reinforcement learning (DRL) approach to discover the optimal planning strategy for metro network extension. The model integrates multi-source data into the reward function while customizing the state and action spaces to reflect the unique characteristics of metro networks. A policy network is developed using an Encoder-Decoder framework, with the parameters being updated by an Actor-Critic framework based on policy gradient. A comprehensive performance index is proposed to evaluate the vulnerability and service capacity of planned networks. The proposed method is validated through a case study on the Hangzhou metro system. The results demonstrate that the proposed DRL can result in optimal planned networks that outperform the actually implemented network with a maximum Performance Improvement Percentage of 19.87 %. The DRL-based optimization framework proposed for metro network extension planning is anticipated to enhance adaptability towards urban development and increase resilience.},
  archive      = {J_ASOC},
  author       = {Qiong Liu and Limao Zhang and Miroslaw J. Skibniewski},
  doi          = {10.1016/j.asoc.2025.113163},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113163},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Network extension planning towards resilient urban critical infrastructures using deep reinforcement learning},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pure convolutional neural network for fault diagnosis of rotary machinery. <em>ASOC</em>, <em>176</em>, 113162. (<a href='https://doi.org/10.1016/j.asoc.2025.113162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of artificial intelligence, deep learning has become the most popular method, particularly for fault diagnosis. Current fault diagnosis methods have achieved great progress, however their accuracy decreases dramatically if the noise is high. In this study, a ConvNeXt with Dense connection Network (CDNet) fault diagnosis method was proposed to learn features from vibration signals and reduce the impact of noise. First, we used ConvNeXt as the backbone of CDNet to extract the feature map. Second, a Joint Attention Module (JAM) was applied in CDNet to learn spatial features and channel features to alleviate the influence of noise. Finally, we set up the dense connection to improve the backpropagation of the gradient. Experimental validation was conducted on the CWRU, SEU_gear_bearing, Paderborn bearing datasets and UConn gear datasets. The proposed CDNet achieves the diagnosis accuracy of 80.29%, 77.98%, 40.86% and 51.73% when SNR= − 10 dB for four datasets respectively, which exceeds other seven methods. The encouraging findings show that the proposed CDNet can learn more effective features and obtain higher accuracy than the other seven methods when the vibration signal is submerged by noise.},
  archive      = {J_ASOC},
  author       = {Yue Zhao and Yong Yang and Wenhua Gao and Zengshou Dong},
  doi          = {10.1016/j.asoc.2025.113162},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113162},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A pure convolutional neural network for fault diagnosis of rotary machinery},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised transformer for 3D point clouds completion and morphology evaluation of granular particle. <em>ASOC</em>, <em>176</em>, 113161. (<a href='https://doi.org/10.1016/j.asoc.2025.113161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the morphology characteristics of particles using 3D point cloud is promising and crucial for the quality inspection of granular materials. However, it remains challenging due to the cumbersome process and incomplete 3D point clouds obtained from laser scanning of particles. In this study, a novel intelligent method, named self-supervised transformer-based encoder and decoder model for granular materials (SSPoinTr-GM), is developed for the automatic completion of partially occluded 3D point clouds and morphology characteristics evaluation. The complete cloud points of 100 cobble and 100 gravel particles are first scanned to establish a benchmark 3D point cloud dataset. To form partial point clouds for training, the complete point cloud is divided into global seed points by the farthest point sampling (FPS) method and the local cloud points around each seed point by the k-nearest neighbor method. Then, the seed points and their local cloud points are randomly removed to generate partial cloud points as input, training the encoder and decoder in a self-supervised way with the original complete point cloud as ground truth. Experiments are conducted to validate the effectiveness of the novel method compared with four existing completion baselines based on the 3D point cloud dataset. The results indicate that the CD1 loss of the completed particles by the proposed method is, on average, 49.05 % lower than that of existing baselines. Additionally, the error rate of the calculated morphology characteristics of the completed particles is, on average, 66.06 % lower than that of the partial point clouds.},
  archive      = {J_ASOC},
  author       = {Haoran Zhang and Zhen-Yu Yin and Ning Zhang and Xiang Wang},
  doi          = {10.1016/j.asoc.2025.113161},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113161},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-supervised transformer for 3D point clouds completion and morphology evaluation of granular particle},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transparent and bias-resilient AI framework for recidivism prediction using deep learning and clustering techniques in criminal justice. <em>ASOC</em>, <em>176</em>, 113160. (<a href='https://doi.org/10.1016/j.asoc.2025.113160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the Recidivism Clustering Network (RCN), an effective approach for predicting repeat offenses using deep learning (DL), clustering, and explainable AI (XAI). The RCN improves offender profiling for more accurate and interpretable recidivism predictions, aligning with key legal principles like fair sentencing, transparency, and non-discrimination. The RCN employs machine learning (ML) models optimized with a Keras tuner, using the Synthetic Minority Over-sampling Technique (SMOTE) to handle class imbalance. With about 75% accuracy, the model shows strong recall, identifying 10,661 recidivists but producing 4,038 false positives—indicating a trade-off between sensitivity and specificity. Beyond predictions, RCN integrates clustering methods, including k-means, principal component analysis (PCA), and t-distributed Stochastic Neighbor Embedding (t-SNE), to identify hidden patterns within offender data. Visualizations reveal distinct clusters, linking characteristics, such as age, to recidivism behaviors. SHapley Additive exPlanations (SHAP) values enhance interpretability, showing that factors like time since the last conviction and age significantly impact predictions. The RCN approach offers substantial potential for criminal justice applications by combining predictive power with actionable insights, supporting a more ethical and accountable use of ML in offender profiling and aiding in fairer recidivism prevention strategies. The code and data are publicly available on GitHub at https://github.com/cavusmuhammed68/Recidivism-Clustering-Network-RCN- .},
  archive      = {J_ASOC},
  author       = {Muhammed Cavus and Muhammed Nurullah Benli and Usame Altuntas and Mahmut Sari and Huseyin Ayan and Yusuf Furkan Ugurluoglu},
  doi          = {10.1016/j.asoc.2025.113160},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113160},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transparent and bias-resilient AI framework for recidivism prediction using deep learning and clustering techniques in criminal justice},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chinese calligraphy character generation with component-level style learning and structure-aware guidance. <em>ASOC</em>, <em>176</em>, 113159. (<a href='https://doi.org/10.1016/j.asoc.2025.113159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating Chinese calligraphy characters poses significant challenges, stemming from factors such as the vast number of characters and the intricate details and complex structures inherent in the Chinese characters. Besides, as a visual art form, Chinese calligraphy strokes often exhibit rich variations. To overcome these challenges, we employ Latent Diffusion Model (LDM) for Chinese calligraphy character generation. To adapt the LDM to this task, we introduce multiple conditions to guide the generation process. Recognizing the importance of individual components within characters, we decompose each character into a component sequence and encode it as a local condition. Combined with the global style condition, the reference styles can be learned at the component level. Moreover, to ensure the structural integrity of the generated characters, we utilize the readily available SimSun font character image as a guiding condition. Additionally, we introduce a novel loss function, termed as latent representation alignment loss, aimed at striking a delicate balance between fidelity and authenticity of the generated characters. To assess the effectiveness of our approach, extensive experiments have been conducted, demonstrating state-of-the-art performance that surpasses comparison methods both quantitatively and qualitatively. Ablation experiments further validate the efficacy of each module in our proposed method.},
  archive      = {J_ASOC},
  author       = {Li Liu and Xia Xiong and Ming Wan and Chengying Tao},
  doi          = {10.1016/j.asoc.2025.113159},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113159},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chinese calligraphy character generation with component-level style learning and structure-aware guidance},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent feature selection-based fake news detection model for pandemic situation with optimal attention based multiscale densenet with long short-term memory layer. <em>ASOC</em>, <em>176</em>, 113158. (<a href='https://doi.org/10.1016/j.asoc.2025.113158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news has recently used the strength and scope of online networking sites to efficiently propagate misinformation, eroding confidence in the press and journalism while also manipulating public perceptions and emotions. However, much information appearing on the Internet is dubious and even intended to mislead. Some fake news is so similar to the real ones that it is difficult for humans to identify them. Therefore, Fake News Detection (FND) needs to develop effectual models to overcome the existing challenges. So, in this paper, a novel deep-learning approach is developed for the recognition of fake news in pandemic situations. Initially, text data are collected from benchmark resources related to the pandemic situation and provided to the pre-processing stage. Then, the obtained pre-processed data is inputted into the feature extraction process. Here, the features are extracted using glove embedding, Bidirectional Encoder Representations from Transformers (BERT), and Term Frequency Inverse Document Frequency (TFIDF). Later, the extracted features are taken to the fused optimal weighted feature selection, and the weights are optimized using the Updated Random Variable-based Artificial Rabbits Optimization (URV-ARO), leveraging the Artificial Rabbits Optimization (ARO). The attained optimal weighted features are then given to the classification process. In the classification phase, the fake news is classified with the help of Optimal Attention-based Multiscale Densenet with Long Short-Term Memory layer (OAMDNet-LSTM). Moreover, parameters in DenseNetand LSTM are tuned by developed URV-ARO. Optimizing parameters in the DenseNetand LSTM helps fine-tune the model to achieve higher accuracy in distinguishing between genuine and fake news. The effectiveness of the proposed model is validated with conventional approaches to showcase the effectiveness of others.},
  archive      = {J_ASOC},
  author       = {V Rathinapriya and J. Kalaivani},
  doi          = {10.1016/j.asoc.2025.113158},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113158},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An intelligent feature selection-based fake news detection model for pandemic situation with optimal attention based multiscale densenet with long short-term memory layer},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A variable window multi-interval rescheduling optimization algorithm for dynamic flexible job shop problem. <em>ASOC</em>, <em>176</em>, 113157. (<a href='https://doi.org/10.1016/j.asoc.2025.113157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic flexible workshop scheduling problem (DFJSP) requires the generation of new scheduling plans after being subjected to dynamic disturbances. Due to the reconfigurability of chromosomal gene, scheduling schemes have a large search space, which poses challenges for solving scheduling schemes. Therefore, a variable window multi-interval optimization (VWMI) rescheduling algorithm is proposed to solve the DFJSP. A nonlinear adaptive crossover probability and mutation probability function is proposed to address the issue of combinatorial optimization easily getting stuck in local optima. Based on the mapping relationship between individual space and objective space, a spatial joint selection method is proposed to select diverse individuals. Compared with other algorithms in dynamic workshop test cases, the rescheduling strategy achieved 7 optimal performance values in 15 test cases, with a maximum time efficiency improvement of 30.2%. In addition, the VWMI achieved 11 good performances in test cases, outperforming other optimization methods.},
  archive      = {J_ASOC},
  author       = {Zeyin Guo and Lixin Wei and Xin Li and Shengxiang Yang and Jinlu Zhang},
  doi          = {10.1016/j.asoc.2025.113157},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113157},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A variable window multi-interval rescheduling optimization algorithm for dynamic flexible job shop problem},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention based patch mixing network for robust few-shot hyperspectral image classification. <em>ASOC</em>, <em>176</em>, 113156. (<a href='https://doi.org/10.1016/j.asoc.2025.113156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot hyperspectral image classification aims to identify the classes of each pixel in the images by only marking few of the pixels. In order to obtain the spatial–spectral joint features, current works extracts pixel features from the fixed-size patches around each pixel. From the perspective of classification tasks, these patches are the samples used for model training and classification. However, most patches belonging to the same target would be similar, with only few patches located at the target boundary having significant differences. When applying these patches for model training, the model will not have the ability to identify complex patches because the training samples lack diversity. This paper proposes an attention based patch mixing network (APMN) for robust few-shot hyperspectral image classification. By imitating the patches located at the target boundary, which are mixed with multi-class pixels, the proposed method randomly mixes up two patches online to synthesize complex patches, and directly uses the synthetic patches to train the model. While flattening each patch into a sequence of pixels, it adopts the transformer as patch feature extractor to learn the pixel-to-class attention and accordingly pay different attention to different pixels in the patches. The attention are also used to balance the losses derived from predicting the synthetic patches as different classes. Compared with existing methods, the proposed method has demonstrated better performance and robustness for few-shot hyperspectral image classification in our experiments.},
  archive      = {J_ASOC},
  author       = {Chun Liu and Longwei Yang and Dongmei Dong and Zheng Li and Wei Yang and Zhigang Han and Jiayao Wang},
  doi          = {10.1016/j.asoc.2025.113156},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113156},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention based patch mixing network for robust few-shot hyperspectral image classification},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust real-time object detection and counting system for casting foundries. <em>ASOC</em>, <em>176</em>, 113155. (<a href='https://doi.org/10.1016/j.asoc.2025.113155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite advances in automation, casting foundries rely heavily on manual labor for postproduction tasks, such as counting the number of castings. Such manual labor is time- and labor-intensive and has a high error rate (up to 15%). These disadvantages are compounded by harsh working conditions and labor shortages. Computer vision systems have thus been used for automatic counting in foundries. However, these systems are limited by the diversity of castings, the overlap between castings, environmental interference from sand and debris, and difficulties in adapting to changes in production conditions in real time. Accordingly, to address these limitations, we propose DeepMachining, a robust real-time object detection and counting system for casting foundries. DeepMachining combines pretraining with adaptive fine-tuning to enhance object detection accuracy in dynamic industrial environments. We also propose a novel dual-branch architecture comprising a frozen main branch and a trainable distillation branch; this architecture allows for effective domain adaptation while ensuring that key feature representations are preserved. Moreover, to mitigate information loss, we introduce extra pathways that enable efficient gradient flow. Experiments revealed that DeepMachining outperformed its state-of-the-art counterparts on a specialized dataset of 42,678 challenging-to-detect images of 71 casting products. It achieved an average counting accuracy rate of 97.8% through few-shot learning, even in the presence of overlapping objects and harsh environmental noise. Ablation studies validated the effectiveness of our use of domain adaptation techniques and real-time environmental data. In general, DeepMachining can generalize well to a diverse range of products without extensive retraining, achieve accurate counting in complex scenarios, and perform well in real-time monitoring.},
  archive      = {J_ASOC},
  author       = {Che-Wei Chou and Yu-Teng Hsu},
  doi          = {10.1016/j.asoc.2025.113155},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113155},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust real-time object detection and counting system for casting foundries},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection via class-specific approximate markov blanket and rough set-based mapping. <em>ASOC</em>, <em>176</em>, 113154. (<a href='https://doi.org/10.1016/j.asoc.2025.113154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov Blanket (MB) is a currently popular approach to feature selection that helps to effectively select correlated features and eliminate redundant features. However, existing MB-based approaches involve complex computations and extensive search. Therefore, we propose a novel concept, Class-specific Approximate Markov Blanket (CSAMB), to solve the above two problems from a class-specific perspective. This concept involves the transformation of decision attributes and features in the specific class using a proposed Rough Set-based Mapping (RSM) method, facilitating the selection results with high classification correlation and low inter-redundancy. The RSM not only preserves the positive, negative and boundary regions of a specific class with respect to a given feature, but also accurately quantifies the relationship between features within that class. Notably, we explore the approximate upper and lower bounds of grouping of correlation features via CSAMB. We then design a CSAMB-based algorithm, and extend it to two variants: CSAMB-min and CSAMB-max using the approximate upper and lower bounds, which demonstrates the performance range of our algorithm. Experiments shows that our algorithms outperform state-of-the-art algorithms regarding accuracy and efficiency, especially for large-scale and high-dimensional datasets.},
  archive      = {J_ASOC},
  author       = {Jie Zhao and Junchao Chen and JiaXin Wu and Ling Tan and Pei Liang and Eric W.K. See-To},
  doi          = {10.1016/j.asoc.2025.113154},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113154},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection via class-specific approximate markov blanket and rough set-based mapping},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and performant transformer private inference with heterogeneous attention mechanisms. <em>ASOC</em>, <em>176</em>, 113150. (<a href='https://doi.org/10.1016/j.asoc.2025.113150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of large-scale models, Transformer architectures have gained widespread adoption. However, privacy concerns become critical when model inference involves separate ownership of data and model parameters. Existing MPC-based methods for private inference suffer from significant overhead and high latency, where replacing traditional Softmax attention mechanisms with faster alternatives serves as a promising research direction. To achieve a better balance between Transformer model performance and inference speed, we explore the impact of attention mechanisms and attention heads on model performance. First, we found that the performance of the attention mechanism is closely related to the downstream task dataset, and the attention mechanism that is faster on specific datasets can actually achieve better model performance. Additionally, we discovered that for attention mechanisms that experience a performance decline, appropriately restoring the attention heads of the Softmax mechanism can significantly enhance performance. We further observed that the selection of key attention heads under different mechanisms is consistent, providing a basis for designing search strategies adapted to different scenarios. Based on these findings, we propose an MPC-friendly attention mechanism replacement method that enables Transformer private inference to be more efficient and performant. This method incorporates two strategies for selecting and replacing attention mechanisms to address diverse scenario requirements, and the resulting heterogeneous attention mechanism significantly improves the speed of private inference while maximizing model performance. With experiments on different downstream tasks, we demonstrated that our method improves average model performance by 1.94 % compared to standard pre-training models, with an inference speed increase of approximately 3 × . Compared to the state-of-the-art methods, our approach enhances model performance by 1.01–8.03 %, with faster inference speeds. Additionally, when evaluated using comprehensive metrics, our method shows improvements of 4.15 × to 8.97 × compared to other approaches.},
  archive      = {J_ASOC},
  author       = {Peng Hu and Lei Sun and Cuiyun Hu and Xiuqing Mao and Song Guo and Jingwen Wang and Miao Yu},
  doi          = {10.1016/j.asoc.2025.113150},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113150},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient and performant transformer private inference with heterogeneous attention mechanisms},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development and optimization of geopolymer concrete with compressive strength prediction using particle swarm-optimized extreme gradient boosting. <em>ASOC</em>, <em>176</em>, 113149. (<a href='https://doi.org/10.1016/j.asoc.2025.113149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents an experimental investigation of the workability and strength behavior of eco-friendly binary geopolymer concrete (GPC) containing ground granulated blast furnace slag (GGBS) and sugarcane bagasse ash (SBA). Initially, this study optimizes binder content (GGBS) in GGBS-based GPC by utilizing the Taguchi method by examining various variables, including alkaline activator-to-binder ratio (AAS/B), sodium hydroxide molarity (M), sodium silicate-to-sodium hydroxide ratio (SS/SH). After optimizing the binder (GGBS) content, SBA is incorporated to formulate a binary GPC by partially replacing GGBS with SBA at substitution levels of 0 %, 5 %, 10 %, 15 %, and 20 %. The effects of varying the AAS/B, M, and SS/SH ratios at different SBA additions on the workability and compressive strength of binary GPC were analyzed. Split and flexural strength were tested on optimized binary GPC samples with varying SBA replacement levels. Moreover, machine learning prediction models using the XGBoost and hyperparameter-optimized XGBoost using particle swarm optimization (PSO) were developed to predict the 28th day compressive strength of binary GPC. Finally, the cost efficiency of binary GPC at different SBA replacement levels was determined. The experimental findings demonstrate that an AAS/B ratio of 0.6, an SS/SH ratio of 2.5, and a sodium hydroxide molarity of 12 M provide an optimal balance between workability and compressive strength. Furthermore, the binary GPC incorporating GGBS and SBA demonstrated compressive strengths ranging from 57 to 79 MPa after curing for 28 days at ambient temperature. This study suggests that 15 % SBA was the optimal replacement level in GGBS-based GPC without significantly compromising its mechanical properties. The prediction outcomes demonstrate that the PSO-XGBoost model is highly effective in predicting binary GPC compressive strength, with an R² value of 0.97. According to the SHAP (Shapley Additive exPlanations) study, the compressive strength of binary GPC was substantially impacted by the quantities of GGBS and SBA.},
  archive      = {J_ASOC},
  author       = {Shimol Philip and Nidhi Marakkath},
  doi          = {10.1016/j.asoc.2025.113149},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113149},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development and optimization of geopolymer concrete with compressive strength prediction using particle swarm-optimized extreme gradient boosting},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-enhanced large language models for ideation to implementation: A new paradigm in product design. <em>ASOC</em>, <em>176</em>, 113147. (<a href='https://doi.org/10.1016/j.asoc.2025.113147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional product design processes often struggle to accurately capture complex user needs and generate market-relevant solutions due to a heavy reliance on subjective human input and limited decision support tools. While Large Language Models (LLMs) have shown proficiency in various domains, their application in product design remains limited, often resulting in generic outputs. To address this, we propose an innovative paradigm for intelligent product design generation, termed ProdGen. The core of ProdGen is the ProdGen-Agent system, which integrates LLMs with customized expert design tools, leveraging the proposed Multi-Design Task Adapter (MDT-A) method and a Dual Knowledge Enhancement Mechanism. The MDT-A method injects multimodal design task knowledge into LLMs through a unified knowledge fusion framework, enabling enhanced task decomposition and efficient interaction with custom design tools. The Dual Knowledge Enhancement Mechanism enriches LLM performance by incorporating domain-specific knowledge bases and structured graph-based data retrieval, ensuring more accurate and relevant design outputs. Demonstrated through kitchen design cases, ProdGen-Agent autonomously handles the entire design process, excelling in user need analysis, task breakdown, decision-making support, tool integration, and multidimensional design generation. Expert evaluations validate ProdGen-Agent’s effectiveness in automating complex design tasks, confirming its potential to revolutionize product design processes across various industries by leveraging LLMs in combination with domain expertise.},
  archive      = {J_ASOC},
  author       = {Zhinan Li and Zhenyu Liu and Guodong Sa and Jiacheng Sun and Mingjie Hou and Jianrong Tan and Lei Sun and Jun Wei},
  doi          = {10.1016/j.asoc.2025.113147},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113147},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge-enhanced large language models for ideation to implementation: A new paradigm in product design},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary mosaicking for high-resolution wide-field optical coherence tomography angiography. <em>ASOC</em>, <em>176</em>, 113145. (<a href='https://doi.org/10.1016/j.asoc.2025.113145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical coherence tomography angiography (OCTA) is a noninvasive imaging modality that produces retinal blood flow images. However, the limited field of view (FOV) of typical high-resolution scan poses challenges for comprehensive analysis. This work presents a fully automatic method for generating high-resolution wide-field OCTA mosaics from overlapping scans, addressing the need for wider FOVs without requiring advanced OCTA equipment or manual mosaicking. The proposed approach consists of a three-stage pipeline: an initial mosaic is constructed using correlation-based template matching, refined with an evolutionary algorithm to optimize vascular continuity at seams, and finalized with a blending stage to improve overall quality. Unlike existing methods, our approach avoids keypoint extraction or input image preprocessing, making it robust against noise and artifacts typically present in clinical OCTA images. Using a correlation-based metric that measures the degree of vascular continuity at the seams in each mosaic, we obtained a mean and standard deviation equal to 0 . 562 ± 0 . 078 (before blending) for all the mosaics analyzed. The proposed method presented robust results, producing high-resolution wide-field OCTA mosaics.},
  archive      = {J_ASOC},
  author       = {Javier Martínez-Río and Enrique J. Carmona and Daniel Cancelas and Jorge Novo and Marcos Ortega},
  doi          = {10.1016/j.asoc.2025.113145},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113145},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary mosaicking for high-resolution wide-field optical coherence tomography angiography},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly leveraging 1D and 2D convolution on diachronic entity embedding for temporal knowledge graph completion. <em>ASOC</em>, <em>176</em>, 113144. (<a href='https://doi.org/10.1016/j.asoc.2025.113144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal knowledge graphs (TKGs) model knowledge that dynamically changes over time in the real world, providing effective support for temporal-aware artificial intelligence (AI) applications. However, existing TKGs are far from complete, and their incompleteness significantly affects the performance of downstream applications. Therefore, Temporal Knowledge Graph Completion (TKGC) has become a current research hotspot, which aims to reason potential missing facts based on existing ones. In the widely studied TKGC methods with the implicit representation of temporal information, existing methods that embed temporal information into entity representations can capture the temporal evolution of entities. However, they fail to take the behavioral characteristics of entities across different time units into account, making them challenging to precisely model the fine-grained dynamics of entities. Furthermore, given the powerful expressiveness of Convolutional Neural Networks (CNNs), some TKGC methods have employed the 1D convolution operation to capture global relationships within the embedded quadruple, enabling the learning of explicit knowledge in TKGs and attaining competitive performance for TKGC. Nevertheless, the non-linear and deep features embedded in the entity-relation interaction have not been insufficiently explored. To address these challenges, this paper proposes JointDE, a TKGC model that applies both 1D and 2D convolution operations to the generated diachronic entity embedding, which simultaneously learns the explicit and implicit knowledge in TKGs. The new diachronic entity embedding method explicitly models the inherent attributes of entities and integrates temporal features across different time units, thereby possessing the ability to capture fine-grained entity evolution. More importantly, we construct feature matrices and filters using diachronic entity embeddings and relation embeddings, leveraging an internal 2D convolution mechanism to expand their interactions. This is the first work to learn implicit knowledge embedded in TKGs from a local relationship perspective for TKGC. Experimental results demonstrate that JointDE surpasses several TKGC baseline methods and achieves state-of-the-art performance on three event-based benchmark datasets: ICEWS14, ICEWS05–15, and GDELT. Specifically, JointDE improves Mean Reciprocal Rank (MRR) by 3.17 % and Hits@1 by 5.87 % over the state-of-the-art baseline for entity reasoning.},
  archive      = {J_ASOC},
  author       = {Mingsheng He and Lin Zhu and Luyi Bai},
  doi          = {10.1016/j.asoc.2025.113144},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113144},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Jointly leveraging 1D and 2D convolution on diachronic entity embedding for temporal knowledge graph completion},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A diversity enhanced tree-seed algorithm based on double search with genetic and automated learning search strategies for image segmentation. <em>ASOC</em>, <em>176</em>, 113143. (<a href='https://doi.org/10.1016/j.asoc.2025.113143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation represents a critical yet inherently complex problem in the field of image processing, with the objective of extracting significant information from visual data. Traditional methodologies often encounter difficulties in effectively retrieving pertinent information. In contrast, swarm intelligence techniques, which optimize through collaborative interaction and stochastic exploration without dependence on prior knowledge, are more adept at addressing image segmentation challenges. The Tree-Seed Algorithm (TSA), a prominent swarm intelligence optimization technique, has been extensively utilized to tackle intricate optimization issues. Nonetheless, the reliance on a singular seed generation approach may result in inadequate exploration, premature convergence, diminished diversity, and local stagnation. To address these deficiencies, a hybrid variant known as the Tree-Seed-Gene Algorithm (TSGA) is proposed, drawing inspiration from the Genetic Algorithm (GA) and incorporating a double search strategy that integrates genetic and automated learning strategies. The genetic search contains mechanisms such as elite, crossover, and mutation. Furthermore, an opposition-based learning strategy is introduced to bolster population diversity, thereby enhancing exploration capability. The efficacy of the TSGA algorithm is assessed in comparison to both classical and contemporary meta-heuristic algorithms, including their variants, utilizing benchmark functions from the IEEE CEC 2014, 2017, 2020, and 2022. The performance of the TSGA is substantiated through statistical analyses, specifically, the Wilcoxon signed-rank and Friedman tests. The findings indicate that the TSGA algorithm exhibits superior performance in resolving image segmentation issues. In conclusion, the experimental results consistently affirm the TSGA has significant potential for practical applications in the domain of image segmentation.},
  archive      = {J_ASOC},
  author       = {Xianqiu Meng and Gaochao Xu and Xu Xu and Ziqi Liu and Jiaqi Ge and Jianhua Jiang},
  doi          = {10.1016/j.asoc.2025.113143},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113143},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A diversity enhanced tree-seed algorithm based on double search with genetic and automated learning search strategies for image segmentation},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-ensemble learning with adaptive sampling for imbalanced medical raman spectroscopy data. <em>ASOC</em>, <em>176</em>, 113142. (<a href='https://doi.org/10.1016/j.asoc.2025.113142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raman spectroscopy combined with artificial intelligence algorithms for disease diagnosis has been widely used in the medical field with great potential. However, since the low prevalence of certain diseases makes it difficult to obtain disease samples, the problem of medical Raman spectroscopy data imbalance occurs in disease diagnosis, where the model tends to predict samples in most classes and has a lower diagnostic accuracy for the disease class, which may delay the patient’s treatment or lead to an increased risk of misdiagnosis. In this study, we propose the MAEL model, which utilizes meta-learning ideas to automatically learn a sampling strategy from the data and adaptively resample the query set iteratively, to tackle the problem of unbalance in medical Raman spectroscopy data. We apply the model for the first time to unbalanced medical Raman spectral data to improve the unbalanced data distribution of the spectral data, and use integration learning to integrate all sampling results during model training to improve model performance. We used three metrics, AUC-PRC, G-mean, and F1-score values, to evaluate the performance of the model and compared it with six traditional data balancing methods. The experimental results show that the MAEL model achieves significant improvements on various medical Raman spectroscopy datasets, with maximum improvements of 0.364, 0.563, and 0.587 for the AUC-PRC, G-mean, and F1-score values, respectively. This study provides an effective way to solve the data imbalance problem in medical spectroscopy and has potential applications.},
  archive      = {J_ASOC},
  author       = {Yishan Guo and Chenjie Chang and Cheng Chen and Jiahao Li and Jun Yu and Xue Wu and Yuxuan Guo and Shunzhe Mao and Wei Bi and Chen Chen and Xiaoyi Lv},
  doi          = {10.1016/j.asoc.2025.113142},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113142},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Meta-ensemble learning with adaptive sampling for imbalanced medical raman spectroscopy data},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards energy-efficient robotic mobile fulfillment system: Hybrid agent-based simulation with DEA-based surrogate machine learning. <em>ASOC</em>, <em>176</em>, 113141. (<a href='https://doi.org/10.1016/j.asoc.2025.113141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of retail e-commerce has increased demand for warehouses to handle large volumes and diverse SKUs. To meet these demands, Robotic Mobile Fulfillment System (RMFS) is widely adopted. However, the automation in RMFS significantly raises energy consumption. The challenge is that the dynamic complexity of RMFS operations poses a major challenge in improving energy efficiency. This research proposes a hybrid optimization model to optimize traffic policy, routing strategy, number of robots, and robot’s max speed for reducing energy consumption while maintaining throughput rate. We first formulated a realistic RMFS energy consumption. A new priority rule for traffic policy was then proposed to reduce unnecessary stoppages. Two routing strategies namely Aisles Only and Underneath Pod were evaluated. Agent-based model was finally developed. Simulation experiment shows that the proposed priority rule reduces energy consumption by 3.41 % and increases the throughput by 26.07 % compared to FCFS. Further, global optimization was performed by first unifying conflicting objectives into a single-efficiency objective using Data Envelopment Analysis. Surrogate-based machine learning was then fitted and optimized via metaheuristic algorithm. The near-optimal configuration for RMFS was achieved by implementing the Priority Rule as traffic policy, Underneath Pod as routing strategy, 26 as number of robots, and 1.372 m/s as max speed. ANOVA reveals that the number of robots is the most influential factors to overall RMFS performance.},
  archive      = {J_ASOC},
  author       = {Zakka Ugih Rizqi and Shuo-Yan Chou and Adi Dharma Oscar},
  doi          = {10.1016/j.asoc.2025.113141},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113141},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards energy-efficient robotic mobile fulfillment system: Hybrid agent-based simulation with DEA-based surrogate machine learning},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defect recognition network for optical fiber cables based on feature information compensation. <em>ASOC</em>, <em>176</em>, 113139. (<a href='https://doi.org/10.1016/j.asoc.2025.113139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The occurrence of electrical corrosion defects in ADSS optical fiber cables presents a significant challenge to the reliable operation of communication lines. Despite the importance of this issue, there has been limited research on accurately detecting electrical corrosion defects in recent years. Moreover, existing defect detection algorithms for industrial issues, such as electrical corrosion in ADSS optical fiber cables, are prone to feature information loss. To address this, we propose an improved Feature Compensation You Only Look Once (FC-YOLO) algorithm for effective detection of electrical corrosion defects in optical cables. First, we proposed the Feature Information Compensated Fusion Network (FICFN), which compensates for fusion features, mitigates the loss of defect information during cross-layer fusion, and enhances feature fusion. Second, an auxiliary training head is integrated into the head network, improving the information expression capability of the FICFN. Finally, an Efficient Local Attention (ELA) mechanism is incorporated into the neck network to boost the localization capabilities of the FICFN. To evaluate the efficacy of the proposed FC-YOLO, we conducted comparison experiments using different mainstream algorithms on both the ADSS electrical corrosion defects dataset and the NEU-DET dataset. Results from the ADSS dataset show that, compared to the YOLOv10s algorithm, the proposed algorithm achieves a 4.7 % increase in mean average precision (mAP@50), reaching 90.2 %, and a 4.1 % improvement in mAP@50–95. These enhancements meet the specifications required for power inspection. On the NEU-DET dataset, the algorithm improved mAP@50 and mAP@50–95 by 8.0 % and 6.1 %, respectively, demonstrating its adaptability for industrial defect detection tasks.},
  archive      = {J_ASOC},
  author       = {Shao-Kai Zheng and Sheng-Su Ni and Peng Yan and Hao Wang and Dao-Lei Wang},
  doi          = {10.1016/j.asoc.2025.113139},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113139},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Defect recognition network for optical fiber cables based on feature information compensation},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A local and global multi-head relation self-attention network for fault diagnosis of rotating machinery under noisy environments. <em>ASOC</em>, <em>176</em>, 113138. (<a href='https://doi.org/10.1016/j.asoc.2025.113138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis under noisy environments (FDUNE) for rotating machinery is a highly challenging task. In recent years, deep learning models have become research hotspots in the field of FDUNE. However, the existing FDUNE approaches suffer from a limitation that insufficient consideration of both local and global features in the feature extraction process leads to unsatisfactory diagnostic performance. In this paper, a local and global multi-head relation self-attention network (LGMHRSANet) is proposed to improve the diagnostic accuracy of rotating machinery under noisy environments, which integrates convolution and self-attention into the transformer form, enabling it to capture local features and global long-range temporal features from vibration signals. Two experimental cases on rolling bearings and gearboxes are implemented to verify the effectiveness of LGMHRSANet under noisy environments. Experimental results demonstrate that LGMHRSANet has superior diagnostic performance compared to other deep learning models, regardless of whether it is in a non-noise environment, or a strong noise environment. In addition, the adaptive performance analysis in the variable noise domain indicates that LGMHRSANet has good robustness in noisy environments.},
  archive      = {J_ASOC},
  author       = {Yiwei Cheng and Xinnuo Lin and Wenwei Liu and Ming Zeng and Pengfei Liang},
  doi          = {10.1016/j.asoc.2025.113138},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113138},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A local and global multi-head relation self-attention network for fault diagnosis of rotating machinery under noisy environments},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A periodic intervention and strategic collaboration mechanisms based differential evolution algorithm for global optimization. <em>ASOC</em>, <em>176</em>, 113137. (<a href='https://doi.org/10.1016/j.asoc.2025.113137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential Evolution (DE) algorithm is a well-known metaheuristic algorithm that features a simple structure and excellent optimization performance. However, it still suffers from premature convergence or stagnation when dealing with complex optimization problems. To avoid these dilemmas in the DE algorithm, we propose a novel DE variant, abbreviated as PISCDE, which is based on periodic intervention and strategic collaboration mechanisms. PISCDE incorporates two types of operations: routine operation and intervention operation. The routine operation employs two mutation strategies with different functional positions to drive the population toward the optimal position. In contrast, the intervention operation uses two intervention strategies with distinct functional roles to restore population diversity and is executed only when a fixed number of iterations is reached. Additionally, to achieve a better balance between global exploration and local exploitation during the optimization process, we propose several strategic collaboration mechanisms. These mechanisms are based on the positioning analysis of different strategies and the interaction analysis between strategies and their corresponding control parameters. To verify the optimization performance of PISCDE, we selected nine comparison algorithms with outstanding optimization performance that have been proposed in the last five years. We used the IEEE CEC 2014 testbed to construct comparative experiments. Based on the comparative results, three conclusions can be drawn: (1) PISCDE has the best overall optimization performance among all the algorithms. (2) PISCDE performs more significantly on complex test problems. (3) PISCDE shows more impressive optimization performance when the dimension of the test problems is increased.},
  archive      = {J_ASOC},
  author       = {Guanyu Yuan and Gaoji Sun and Libao Deng and Chunlei Li and Guoqing Yang and Lili Zhang},
  doi          = {10.1016/j.asoc.2025.113137},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113137},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A periodic intervention and strategic collaboration mechanisms based differential evolution algorithm for global optimization},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BORF: A bayesian optimized random forest for prediction of aerosol extinction coefficient from mie lidar signal. <em>ASOC</em>, <em>176</em>, 113130. (<a href='https://doi.org/10.1016/j.asoc.2025.113130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In continuous observation signals of lidar, the identification and selection of effective signals are crucial, especially for the aerosol extinction coefficient retrieval. In this study, the Bayesian Optimized Random Forest (BORF) model, a machine learning approach combining Random Forest regression with Bayesian optimization, was developed for predicting aerosol extinction coefficients. Built upon the foundation of the Random Forest (RF) regression method, this model leverages Bayesian optimization to adjust model parameters precisely, significantly enhancing the accuracy of aerosol extinction coefficient predictions. This approach offers a valuable means to identify and screen anomalous Lidar signals. We constructed a training dataset comprising continuously observed Mie Lidar signals and aerosol extinction coefficients retrieved using the Klett method. The dataset contains dimensions, including Mie Lidar signals, detection time, detection distance, pressure, and temperature. This paper provides a detailed description of the BORF model’s establishment process and the optimization of model parameters using Bayesian optimization. Through model assessments, significance tests, and comparative experiments, we demonstrate the effectiveness of the BORF model. Experimental results indicate that, compared to other relevant models, the BORF model excels in predicting aerosol extinction coefficients, closely aligning with the accuracy of the Klett method. Specifically, in datasets with better data quality, the BORF model exhibits an approximately 4% increase in R 2 compared to the RF and BP neural network optimized by genetic algorithm (BPGA), accompanied by a 41% to 47% reduction in MSE and MAE. The Mean Squared Error (MSE) and Mean Absolute Error (MAE) decrease by approximately 40% to 90% in datasets with lower data quality and less apparent data variations. This study provides a robust technical solution to ensure the reliability of Lidar data, thereby contributing to an enhanced understanding of atmospheric aerosols and environmental monitoring.},
  archive      = {J_ASOC},
  author       = {Hao Chen and Fei Gao and Zhimin Rao and Dengxin Hua},
  doi          = {10.1016/j.asoc.2025.113130},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113130},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BORF: A bayesian optimized random forest for prediction of aerosol extinction coefficient from mie lidar signal},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancement of artificial intelligence applications in hydrocarbon well drilling technology: A review. <em>ASOC</em>, <em>176</em>, 113129. (<a href='https://doi.org/10.1016/j.asoc.2025.113129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the petroleum upstream has increasingly relied on artificial intelligence (AI), with applications spanning machine/deep learning (ML/DL), hybrid models, and committee machine learning. Particularly in drilling engineering (DE), AI has become crucial for addressing complex subsurface challenges. Nevertheless, its implementation continues to be a significant obstacle owing to the technological, operational, and engineering challenges involved in real-time applications of DE approaches. This review examines AI technologies in DE, focusing on their practicality, performance, and associated challenges. It evaluates models for predicting drilling fluid properties, hole cleaning, rate of penetration, wellbore trajectory, fluid hydraulics, bit wear, borehole stability, subsurface problems, and fault diagnosis. It explores integrating AI models with downhole sensors and surface data for real-time/automated drilling control, alongside real-world AI application cases. It highlights the benefits of combining ML/DL with optimization algorithms in hybrid models and analyzes trends in AI research in DE through bibliometric and scientometric studies. Guidelines are provided for selecting and improving AI algorithms for various drilling applications and assessing their economic impacts. The review concludes by identifying future research directions to advance AI applications in the drilling industry.},
  archive      = {J_ASOC},
  author       = {Shadfar Davoodi and Mohammed Al-Shargabi and David A. Wood and Mohammad Mehrad},
  doi          = {10.1016/j.asoc.2025.113129},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113129},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advancement of artificial intelligence applications in hydrocarbon well drilling technology: A review},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Block-level index mixing and classification enhancement attention for occluded person re-identification. <em>ASOC</em>, <em>176</em>, 113127. (<a href='https://doi.org/10.1016/j.asoc.2025.113127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of occluded person re-identification (Re-ID), a major challenge lies in effectively recognizing partially occluded pedestrian images. Existing methods leveraging occlusion simulation and local feature segmentation often struggle with real-world complexity and inadvertently introduce noise from non-target regions. To address these limitations, this paper proposes the Block-level Index Mixing and Classification Enhancement Attention (BMCE) framework, which integrates data augmentation and classification enhancement strategies. For data augmentation, the Block-level Index Mixing (BLIM) module partitions images with different labels into several blocks. By sharing an index list and controlling the proportion of sampled blocks, the module simulates diverse occluded images. Additionally, adaptive weight mixing of labels enhances the discriminative ability of the simulated images. For classification enhancement, the Classification Enhancement Attention (CEA) module leverages multi-granularity features to enhance classification weights and mitigates the influence of non-target regions through an attention mechanism, improving performance in occlusion scenarios. Experimental results demonstrate that BMCE achieves competitive performance on occlusion, partial, and holistic Re-ID datasets. Notably, it attains 74.1% Rank-1 accuracy and 64.7% mAP on the Occluded-Duke dataset. Source code is available at https://github.com/aohan-del/BMCE .},
  archive      = {J_ASOC},
  author       = {Lun Zhang and Shuli Cheng and Liejun Wang},
  doi          = {10.1016/j.asoc.2025.113127},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113127},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Block-level index mixing and classification enhancement attention for occluded person re-identification},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning-based particle contribution evaluation mechanism for meta-heuristic optimization algorithms. <em>ASOC</em>, <em>176</em>, 113119. (<a href='https://doi.org/10.1016/j.asoc.2025.113119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-heuristic algorithms have been a popular research field nowadays. However, they are prone to falling into local optima, especially when applied to the Problem with Weak Influence of Global Optimal Solutions (PWIGOS), where the global optimal solution has a very small influence area in the search space. In this paper, based on the analysis of the influence of PWIGOS on meta-heuristic optimization algorithms, a novel Particle Contribution Evaluation Mechanism (PCEM) is proposed. Different from the current mechanisms in this field, PCEM is innovative in that it uses deep learning models to infer whether a particle is a high contribution particle within the influence region of the global optimum according to the feature information. This provides meta-heuristics with this additional critical information from outside the optimization process to guide the correct evolution of particle population. Additionally, a dynamic threshold setting method and a particle evolution adjustment method are designed, and three different types of classic and representative meta-heuristic algorithms, differential evolution (DE), Particle Swarm Optimization (PSO) and Gravitational Search Algorithm (GSA) are selected as application examples of PCEM. Experiments are conducted on 27 benchmark functions, CEC2017 benchmark suite and four real-word problems. According to the statistical results, PCEM not only excels in particle contribution assessment but also significantly enhances algorithm performance, especially when addressing challenging PWIGOS.},
  archive      = {J_ASOC},
  author       = {Fang Su and Ying Liu and Liquan Chen},
  doi          = {10.1016/j.asoc.2025.113119},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113119},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep learning-based particle contribution evaluation mechanism for meta-heuristic optimization algorithms},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HawkEye: A robust loss function for regression with bounded, smooth, and insensitive zone characteristics. <em>ASOC</em>, <em>176</em>, 113118. (<a href='https://doi.org/10.1016/j.asoc.2025.113118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector regression (SVR) encounters challenges when confronted with outliers and noise, primarily due to the limitations of the traditional ɛ -insensitive loss function. To address this, bounded loss functions have gained traction for their robustness and improved generalization. More recent advancements, such as BLINEX and bounded least square loss, focus on smooth bounded loss functions that enable efficient gradient-based optimization. However, these approaches lack an insensitive zone, which is crucial for mitigating minor deviations and noise. The challenge of designing a loss function that combines boundedness, smoothness, and an insensitive zone remains unresolved in the current literature. To address this issue, we develop the HawkEye loss, a novel formulation that integrates boundedness, smoothness, and the presence of an insensitive zone. This unique combination enhances the robustness and generalization capabilities of SVR models, particularly in the presence of noise and outliers. Notably, the HawkEye loss is the first in SVR literature to simultaneously incorporate boundedness, smoothness, and an insensitive zone. Leveraging this breakthrough, we integrate the HawkEye loss into the least squares framework of SVR and yield a new robust and scalable model termed HE-LSSVR. The optimization problem inherent to HE-LSSVR is addressed by harnessing the adaptive moment estimation (Adam) algorithm, known for its adaptive learning rate and efficacy in handling large-scale problems. To our knowledge, this is the first time Adam has been employed to solve an SVR problem. To empirically validate the proposed HE-LSSVR model, we evaluate it on UCI, synthetic, time series, and brain age datasets. The experimental outcomes unequivocally reveal the superiority of the HE-LSSVR model both in terms of its remarkable generalization performance and its efficiency in training time. The code of the proposed model is publicly available at https://github.com/mtanveer1/HawkEye .},
  archive      = {J_ASOC},
  author       = {Mushir Akhtar and M. Tanveer and Mohd. Arshad},
  doi          = {10.1016/j.asoc.2025.113118},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113118},
  shortjournal = {Appl. Soft. Comput.},
  title        = {HawkEye: A robust loss function for regression with bounded, smooth, and insensitive zone characteristics},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Photovoltaic parameter extraction through an adaptive differential evolution algorithm with multiple linear regression. <em>ASOC</em>, <em>176</em>, 113117. (<a href='https://doi.org/10.1016/j.asoc.2025.113117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solar cells play a crucial role in generating clean, renewable energy. Accurate modeling of photovoltaic (PV) systems is essential for their development, and simulating their behaviors requires precise estimation of their parameters. However, many optimization methods exhibit high or unstable root mean square error (RMSE) due to local optima entrapment and parameter interdependence. To address these challenges, we propose MLR-DE, a novel hybrid approach that integrates adaptive differential evolution (DE) with multiple linear regression (MLR). The main innovation is to decompose the PV model into linear coefficients and non-linear functions, the latter being iteratively estimated using DE. By treating nonlinear function outputs as independent variables and known measured currents as dependent variables, linear coefficients are analytically solved through MLR. Additionally, we introduce a data-fusion-based parameter generation scheme to improve DE’s reliability by integrating historical crossover rates with estimated crossover rates. We validate MLR-DE through experiments across 11 PV configurations: 3 standard diode models and 8 environmental variants. The results demonstrate MLR-DE’s superiority in all tests. It achieves the lowest average RMSE compared to other algorithms, with standard deviations at or below 2e−16. In the Friedman test, MLR-DE ranked first with a score of 1.94, outperforming the second-place (3.72) and last-place (7.58) competitors. The convergence curve shows that MLR-DE achieves convergence in less than 3,000 function evaluations over standard models, with an average convergence time of less than 0.6 s.},
  archive      = {J_ASOC},
  author       = {Bozhen Chen and Haibin Ouyang and Steven Li and Liqun Gao and Weiping Ding},
  doi          = {10.1016/j.asoc.2025.113117},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113117},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Photovoltaic parameter extraction through an adaptive differential evolution algorithm with multiple linear regression},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic scheme for collaborative redundant manipulators aided with neural networks in a competitive manner. <em>ASOC</em>, <em>176</em>, 113115. (<a href='https://doi.org/10.1016/j.asoc.2025.113115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Focusing on k-winners-take-all ( k -WTA) strategy, this paper considers competitive kinematic program of multiple redundant manipulators using neighbor-to-neighbor communication topology and proposed a dynamic repetitive motion planning (DRMP). Aided by a distributed neural network solver, a cooperative control law of multiple redundant manipulators is formulated for dynamic task allocation with constraint equations, communication topology among manipulators, singularity avoidance of the winner manipulator and repetitive execution of the given tasks. Theoretical analyses prove the manipulation and feasibility of the proposed DRMP among multiple redundant manipulators. Computational simulations based on PUMA560 manipulators are conducted to verified the efficacy of the proposed DRMP and the underlying neural network.},
  archive      = {J_ASOC},
  author       = {Ying Kong and Xi Chen and Jiayue Yin},
  doi          = {10.1016/j.asoc.2025.113115},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113115},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic scheme for collaborative redundant manipulators aided with neural networks in a competitive manner},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of multi-objective evolutionary algorithm based on transfer learning in sliding bearing. <em>ASOC</em>, <em>176</em>, 113111. (<a href='https://doi.org/10.1016/j.asoc.2025.113111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, decomposition-based multi-objective evolutionary algorithms have gained increasing attention for solving complex optimization problems. However, existing weight vector adaptation methods often struggle to balance diversity and convergence. To address this issue, we propose a multi-objective evolutionary algorithm based on transfer learning (MOEA/D-TL), which integrates joint distribution adaptation (JDA) to coordinate the populations generated by genetic and differential operators. The key innovations of MOEA/D-TL include: (1) a dual-operator framework that leverages JDA to integrate the strengths of both operators; (2) auxiliary population labeling using Pareto dominance, leveraging JDA’s characteristics; and (3) sparsity-driven adaptive weight vector adjustment to refine population distribution. Extensive experiments on 44 benchmark problems demonstrate that MOEA/D-TL outperforms nine state-of-the-art algorithms, achieving a 42%–60% improvement across three performance metrics. When applied to the optimization of sliding bearings with conflicting objectives (load capacity, heat generation, and friction coefficient), MOEA/D-TL yields solutions with broader distribution and improved uniformity compared to seven other algorithms. These results validate the algorithm’s capability to balance diversity and convergence effectively.},
  archive      = {J_ASOC},
  author       = {Xuepeng Ren and Maocai Wang and Guangming Dai and Lei Peng},
  doi          = {10.1016/j.asoc.2025.113111},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113111},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of multi-objective evolutionary algorithm based on transfer learning in sliding bearing},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing multi-drone patrol path planning under uncertain flight duration: A robust model and adaptive large neighborhood search with simulated annealing. <em>ASOC</em>, <em>176</em>, 113107. (<a href='https://doi.org/10.1016/j.asoc.2025.113107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When conducting drone path planning, the flight duration of drones is a critical factor influencing the planning solution. Given the characteristics of drone batteries, accurately predicting the actual flight duration is challenging. It is crucial to reduce the impact of uncertain flight duration on path feasibility. To solve this problem, this paper proposes a robust optimization method that constructs a budget uncertainty set to describe the uncertain flight duration. To facilitate the solution process of the model, the strong duality theorem is employed to transform the robust model into a mixed integer linear programming model. To efficiently handle large-scale path planning problems, a hybrid heuristic algorithm with robust feasibility check (ALSA-RFC) is proposed. This algorithm combines the advantages of adaptive large neighborhood search and simulated annealing. Furthermore, to ensure the robustness of the solution, a method for generating robust initial solutions quickly and a robust feasibility checking method for solutions are constructed. Numerical experimental results demonstrate that ALSA-RFC can quickly find high-quality robust solutions. Additionally, through Monte Carlo simulations, the impact of robust parameters on the robustness of the solution scheme is analyzed, evaluating the performance of the algorithm in different scenarios. Comparisons with chance-constrained programming methods revealed that ALSA-RFC can significantly reduce the sensitivity of path planning results to fluctuations in flight duration without substantially increasing flight costs. Finally, a case study is conducted to further validate the practicality of ALSA-RFC in real-world applications.},
  archive      = {J_ASOC},
  author       = {Xiaoduo Li and He Luo and Guoqiang Wang and Zhihong Song and Qiwen Gou and Fanhe Meng},
  doi          = {10.1016/j.asoc.2025.113107},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113107},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing multi-drone patrol path planning under uncertain flight duration: A robust model and adaptive large neighborhood search with simulated annealing},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective evolutionary algorithm based on a grid with adaptive divisions for multi-objective optimization with irregular pareto fronts. <em>ASOC</em>, <em>176</em>, 113106. (<a href='https://doi.org/10.1016/j.asoc.2025.113106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance degradation of most existing multi-objective optimization evolutionary algorithms (MOEAs) when tackling multi-objective problems (MOPs) with irregular Pareto fronts is a critical challenge in the field of multi-objective optimization. To address this issue, a novel grid-based MOEA is proposed in this paper. This algorithm dynamically adjusts the number of grid divisions during the optimization process, thereby enabling effective partitioning of the objective space and guiding solution distribution across MOPs with varying Pareto front shapes. Additionally, to enhance diversity preservation, a grid stabilization strategy is proposed to maintain a stable environment for diversity, while a boundary solution protection strategy ensures diversity by promoting exploration of the boundaries. Furthermore, a population reselection method is designed to bolster exploration capabilities within the objective space. Experimental results from benchmark test suites, which include a variety of Pareto front types, demonstrate that our proposed algorithm outperforms seven state-of-the-art MOEAs in addressing both irregular and regular Pareto front MOPs.},
  archive      = {J_ASOC},
  author       = {Zhe Liu and Fei Han and Qinghua Ling and Henry Han and Jing Jiang and Qing Liu},
  doi          = {10.1016/j.asoc.2025.113106},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113106},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective evolutionary algorithm based on a grid with adaptive divisions for multi-objective optimization with irregular pareto fronts},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic task offloading in edge computing for computer access point selection based on adaptive deep reinforcement learning with meta-heuristic optimization. <em>ASOC</em>, <em>176</em>, 113105. (<a href='https://doi.org/10.1016/j.asoc.2025.113105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computationally intensive tasks are processed by mobile devices which include data processing, virtual reality, and artificial intelligence. The computational resources of the mobile devices are very low so they are suited to perform all tasks with low latency. Mobile Edge Computing (MEC) is a cutting-edge computing model that offloads computation-intensive tasks to MEC servers to increase the capability of computing in Mobile Devices (MDs). Due to the extensive use of Wireless Local Area Networks (WLAN), each MD can use numerous Wireless Access Points (WAPs) to offload tasks to a server. In this research work, the task offloading problem is determined by considering the delay-sensitive task along with edge load dynamics to reduce the long-term cost. The distributed algorithm based on Adaptive Deep Reinforcement Learning (ADRL) is introduced, where every device is analyzed for offloading decisions without knowing the task model of other devices. The parameters in the model are optimized using the Fitness-based Piranha Foraging Optimization Algorithm (F-PFOA) to enhance the performance of the model. Finally, the evaluation is done by using the various metrics to showcase the effectiveness of the proposed model, and it gives the throughput is 93.5, which is enhanced than other existing models. Thus, the simulation outcome with a greater number of mobile devices and corresponding edge nodes showed that the developed optimization minimizes the dropped task’s ratio and average task delay respectively. The result of the designed model outperformed better than other available models.},
  archive      = {J_ASOC},
  author       = {S. Vidya and R. Gopalakrishnan},
  doi          = {10.1016/j.asoc.2025.113105},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113105},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic task offloading in edge computing for computer access point selection based on adaptive deep reinforcement learning with meta-heuristic optimization},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamically allocated interval-based generative linguistic steganography with roulette wheel. <em>ASOC</em>, <em>176</em>, 113101. (<a href='https://doi.org/10.1016/j.asoc.2025.113101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing linguistic steganography schemes often overlook the conditional probability (CP) of tokens in the candidate pool, allocating the one coding to all tokens, which results in identical selection likelihoods. This approach leads to the selection of low-CP tokens, degrading the quality of stegos and making them more detectable. This paper proposes a scheme based on the interval allocated, called DAIRstega. DAIRstega first uses a portion of the read secret to build the roulette area. Then, this scheme uses the idea of the roulette wheel and takes the CPs of tokens as the main basis for allocating the roulette area (i.e., the interval length). Thus, tokens with larger CPs are allocated more area. The secret will have an increased likelihood of selecting a token with a higher CP. During allocation, we design some allocation functions and three constraints to optimize the process. Additionally, DAIRstega supports prompt-based controllable generation of stegos. Rich experiments show that the proposed embedding way and DAIRstega perform better than the existing ways and baselines, which shows strong perceptual, statistical, and semantic concealment, as well as anti-steganalysis ability. It can also generate high-quality longer stegos, addressing the deficiencies in this task. DAIRstega is confirmed to have potential as a secure watermarking, offering insights for its development. Our codes and data are available at: https://github.com/WangYH-BUPT/DAIRstega .},
  archive      = {J_ASOC},
  author       = {Yihao Wang and Ruiqi Song and Lingxiao Li and Ru Zhang and Jianyi Liu},
  doi          = {10.1016/j.asoc.2025.113101},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113101},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamically allocated interval-based generative linguistic steganography with roulette wheel},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective sustainable multipath delivery problem in hilly regions with customer-satisfaction using TLBO. <em>ASOC</em>, <em>176</em>, 113100. (<a href='https://doi.org/10.1016/j.asoc.2025.113100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logistic delivery through road contributes substantial carbon emission (CE). In business, timely goods delivery i.e. customer satisfaction, is important. With these facts, a sustainable multi-objective 3D delivery problem with customer satisfaction (SMO3DDPwCS) in a hilly region (HR) is developed to minimize total CE and customer dissatisfaction (CDS) simultaneously. Here, one supplier’s vehicle starts from the depot with goods equal to retailers’ demands, distributes among the retailers as per their orders within their preferred times, and comes back. The retailers’ shops and depot are connected through multiple hilly tracks, which have up and down slopes and are susceptible to landslide. The cautious driving through these tracks produces extra CE and CDS. The SMO3DDPwCS is solved by a modified MOTLBO (mMOTLBO) algorithm. This algorithm incorporates self-learning concepts after both the teaching and learning phases, introduces innovative upgrading strategies, and employs a group-based learning approach. Some statistical tests are performed using mMOTLBO on the standard TSPLIB instances. The efficiency of mMOTLBO is established against NSGA-II and MOEA/D. Multiple solutions in Pareto front are ranked using TOPSIS. Some managerial decisions are drawn. The optimum routing plan for SMO3DDPwCS in a hilly region is presented and gives better results (31% total CE and 8% total CDS) than the single path formulation. mMOTLBO showed superiority over other algorithms in most cases concerning the Pareto front for the objectives. On the benchmark instances, mMOTLBO demonstrated its superiority by outperforming NSGA-II and MOEA/D, showing improvements of 0.11 in IGD and 4.12 in GD.},
  archive      = {J_ASOC},
  author       = {Somnath Maji and Samir Maity and Izabela Ewa Nielsen and Debasis Giri and Manoranjan Maiti},
  doi          = {10.1016/j.asoc.2025.113100},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113100},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective sustainable multipath delivery problem in hilly regions with customer-satisfaction using TLBO},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A modified neutrosophic fuzzy approach for managing electronic waste considering sustainability and resilience dimensions. <em>ASOC</em>, <em>176</em>, 113097. (<a href='https://doi.org/10.1016/j.asoc.2025.113097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising problem of electronic waste (e-waste) demands management strategies that minimize environmental impact and prioritize resilience and sustainability, especially amid global disruptions and pressure on manufacturers to adopt extended producer responsibility policies. Existing literature on e-waste management primarily addresses either operational efficiency or sustainability, leaving a research gap in understanding the relationship between sustainability and resilience. To bridge this gap, this study proposes a framework for building resilient and sustainable e-waste management systems in dynamic environments. This framework utilizes a multi-objective optimization model that balances cost, environmental impact, and social factors (sustainability dimensions) while incorporating non-resilience vulnerabilities for optimal decision-making. The model addresses parameter uncertainties through a fuzzy programming approach based on the Me-measure, further enhanced by proposing variants of novel neutrosophic fuzzy programming techniques. The proposed framework is validated by implementing it in a real-world case problem. Key findings show that enhancing e-waste management network resilience relies on strategically reinforcing critical facilities with redundancy. Allocating 100 % priority to resilience achieves a resilience goal of 100 % and a sustainability goal of 52 %, while prioritizing sustainability at 100 % results in a sustainability goal of 73.7 % and resilience of 71.4 %, suggesting that sustainable practices often inherently enhance resilience. Research offers valuable insights for policymakers, regulators, and stakeholders through managerial recommendations, visualizations, and sensitivity analyses.},
  archive      = {J_ASOC},
  author       = {Muhammad Salman Habib and Seung-June Hwang},
  doi          = {10.1016/j.asoc.2025.113097},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113097},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A modified neutrosophic fuzzy approach for managing electronic waste considering sustainability and resilience dimensions},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selecting a health emergency strategy through large-scale multi-criteria decision-making based on intuitionistic fuzzy self-confidence data. <em>ASOC</em>, <em>176</em>, 113085. (<a href='https://doi.org/10.1016/j.asoc.2025.113085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex decision-making scenarios involving multiple stakeholders, the uncertainty and individual confidence of decision-makers (DMs) are crucial in determining the outcomes. A novel approach is proposed in this paper to improve decision-making processes within a large group of DMs operating under an “Intuitionistic Fuzzy Self-Confidence (IFN-SC)” setting. The research presents a hybrid clustering algorithm to categorize DMs based on their numerical similarities and psychological factors. A multi-objective nonlinear optimization problem is employed to determine the criteria weights in the IFN-SC environment when the weight vector is either partially or fully unknown. Using the max operator, we derive a single-objective nonlinear optimization problem, which is solved by the “Particle Swarm Optimization (PSO)” algorithm. Furthermore, extending the “Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS)” for the IFN-SC environment significantly enhances the model’s effectiveness in ranking alternatives. The study exemplified its capability in managing a large-scale decision-making problem based on health emergency strategy selection and presented various analyses highlighting its utility, adaptability, and robustness in practical situations.},
  archive      = {J_ASOC},
  author       = {Priya Sharma and Mukesh Kumar Mehlawat and Pankaj Gupta and Shilpi Verma},
  doi          = {10.1016/j.asoc.2025.113085},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113085},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Selecting a health emergency strategy through large-scale multi-criteria decision-making based on intuitionistic fuzzy self-confidence data},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-trajectory optimization for train using distributional reinforcement learning with conditional value-at-risk. <em>ASOC</em>, <em>176</em>, 113079. (<a href='https://doi.org/10.1016/j.asoc.2025.113079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence methods like reinforcement learning (RL) have been widely studied to train trajectory optimization problems to achieve flexible driving. To meet the demand for flexible driving strategies in actual operations, N optimized trajectories for the single train are usually generated based on different scheduled times. It brings up two issues: the computational cost of N trajectories is N times that of a single trajectory, and manual intervention is required to adjust the initial conditions, such as schedule time. This paper proposes a conditional value-at-risk (CVaR) distributional Q-learning approach (CDQ) to generate trajectories with different driving styles, balancing safety and efficiency. First, analyzing the actual control deviations, the distribution of returns is modeled using the quantile of distributional RL. Then, we introduce CVaR as a risk metric to evaluate the risk of actions and develop risk-sensitive strategies based on various confidence levels, simultaneously optimizing multiple trajectories for the single train. Finally, we simulate the experiments with data from an actual line. The results demonstrate that the CDQ algorithm can simultaneously optimize multiple train trajectories without requiring human intervention. Through a two-layer selection mechanism, five trajectories with varying driving styles can be selected to fulfill scheduling flexibility requirements. Compared to standard Q-learning, distributional Deep Q-Network and other risk-sensitive RL, CDQ shows improved performance in both energy-saving and punctuality. The total computation time of CDQ is only 31.47% and 35.44% of Q-learning and risk-sensitive RL.},
  archive      = {J_ASOC},
  author       = {Yalan Chen and Jing Xun and Shibo He and Xin Wan and Yafei Liu},
  doi          = {10.1016/j.asoc.2025.113079},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113079},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-trajectory optimization for train using distributional reinforcement learning with conditional value-at-risk},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multi-region prediction strategy for dynamic multi-objective optimization. <em>ASOC</em>, <em>176</em>, 113072. (<a href='https://doi.org/10.1016/j.asoc.2025.113072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel adaptive multi-region prediction strategy for dynamic multi-objective optimization problems (DMOPs), which aims to efficiently generate diverse populations in response to environmental changes and facilitate the exploration of the new Pareto front. The strategy consists of two main phases: predictive population initialization and elite-guided resampling. In the predictive population initialization phase, the strategy integrates global exploration and local exploitation. Global exploration divides the population into N subregions based on population distribution characteristics. For each subregion, the historical information of its center point is used to predict its new position in the next environment, and then a Gaussian mixture model (GMM) is used to sample new individuals based on the position information of all new center points. Local exploitation employs the K-Medoids method to cluster historical Pareto fronts and selects individuals corresponding to the medoids in the decision space as representative individuals. These representative individuals are then used to predict their new locations, followed by Gaussian sampling to generate individuals. The initial predicted population is formed by combining the individuals from global exploration, local exploitation, and randomly generated individuals. In the elite-guided resampling phase, the initial predicted population is evaluated, and top-ranked elite individuals are selected. These elites guide the generation of the final population through Gaussian sampling and Latin Hypercube Sampling (LHS), enhancing solution quality and diversity. The proposed strategy is validated on 14 benchmark problems using MIGD, MHV, R(IGD), and DMIGD metrics. Results demonstrate its better comprehensive performance under varying environmental change intensities (mild, moderate, and severe) compared to existing approaches. Furthermore, its application to a real-world PID controller tuning problem highlights the strategy’s practical potential, showcasing superior performance.},
  archive      = {J_ASOC},
  author       = {Tao Zhang and LinJun Yu and HuiWen Yu},
  doi          = {10.1016/j.asoc.2025.113072},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113072},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive multi-region prediction strategy for dynamic multi-objective optimization},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel graph neural architecture search optimization with incomplete features. <em>ASOC</em>, <em>176</em>, 113068. (<a href='https://doi.org/10.1016/j.asoc.2025.113068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have shown remarkable success in many fields. However, the results of different model architectures for different scenarios can be very different. Designing effective neural architectures requires a great deal of specialized knowledge, which limits the application of GNNs models. In recent years, graph neural architecture search (GNAS) has attracted widespread attention. GNAS selects the GNNs structure in predefined search space using a suitable search algorithm. The search direction is constrained based on the evaluation made by the estimation strategy. Traditional GNAS methods suffer from long search times, difficulty in parameter selection, and high sensitivity to data quality. When feature information is missing, the candidate architectures explored during the search process cannot obtain complete feature information, which significantly reduces the accuracy of GNAS. To tackle these challenges, we propose a novel optimization framework for parallel graph neural architecture search, named AutoPGO. In AutoPGO, we complement the features based on a feature propagation algorithm generated by minimizing the Dirichlet energy function, improve the search algorithm using the mutation decay strategy and complete the optimization of the parameters using the Bayesian optimization method. Experimental results show that AutoPGO has good performance and some degree of robustness.},
  archive      = {J_ASOC},
  author       = {Haitao Yang and Zhaowei Liu and Dong Yang and Lihong Wang},
  doi          = {10.1016/j.asoc.2025.113068},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113068},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parallel graph neural architecture search optimization with incomplete features},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constrained multi-objective optimization via neural network and cooperative populations. <em>ASOC</em>, <em>176</em>, 113051. (<a href='https://doi.org/10.1016/j.asoc.2025.113051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained multi-objective optimization problems are widely used in practical scenarios such as intelligent manufacturing and network communication. These problems are often made intractable by constraints, and achieving a balance between convergence, diversity, and feasibility becomes increasingly challenging. To address this issue, a constrained multi-objective evolutionary algorithm named NNCP is proposed, which is based on the neural network and, three cooperative populations. Specifically, the neural network is employed to accelerate the population’s convergence by utilizing neuron weights to capture neighborhood information. Among the three populations, the first population uses self-organizing mapping and curvature estimation to approximate the Pareto front, the second population utilizes non-dominance sorting and an angle selection mechanism to identify high-quality infeasible solutions, thereby enhancing diversity, and the third population adopts an adaptive penalty mechanism to improve feasibility. These populations work cooperatively to identify promising infeasible solutions and navigate infeasible regions to approximate the Pareto front. Finally, five state-of-the-art constrained multi-objective optimization algorithms are compared with NNCP. Out of the total 47 test problems, NNCP outperforms the best-performing baseline algorithm on more than 35 problems, highlighting its superior convergence and diversity capabilities.},
  archive      = {J_ASOC},
  author       = {Jie Cao and Yiyuan Wang and Jianlin Zhang and Zuohan Chen},
  doi          = {10.1016/j.asoc.2025.113051},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113051},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constrained multi-objective optimization via neural network and cooperative populations},
  volume       = {176},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stream neural network for the prediction of multiple defects in continuous casting. <em>ASOC</em>, <em>175</em>, 113116. (<a href='https://doi.org/10.1016/j.asoc.2025.113116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The steel industry is a large and complex modern process industry, and continuous casting is one of its most important production links. It is a complex process due to the physical, mechanical, and chemical components involved. In recent years, machine learning techniques have become an indispensable part of the monitoring of complex production processes. However, the characteristic of the class imbalance problem in continuous casting industrial data has influenced the application of machine learning techniques. To overcome this limitation, a contrastive learning pretext task called similarity discrimination, and a two-stream neural network for predicting multiple defects in continuous casting have been proposed. The network effectively combines metallurgical knowledge with data-driven models. It was trained and evaluated on an industrial dataset. The two-stream neural network achieved an accuracy of 0.886 and a binary accuracy of 0.896 for predicting multiple quality defects in continuous casting. The comparative experiment results showed that it is at least 45.5 % higher than other machine learning methods in recall. To analyze the relationship between model components and model performance, ablation studies were performed. The contrastive learning pretext task and the new neural network architecture increased the accuracy and recall of the model by 7.1 % and 18.6 %, respectively. Furthermore, the interpretable machine learning technique was introduced to ensure the interpretability of the neural network. It enhanced the credibility of the machine learning systems, which helped users trust the model and predictions.},
  archive      = {J_ASOC},
  author       = {Xinyu Ning and Haijun Li and Qibo Wang and Fei Yang and Yanfeng Zhang and Guodong Wang},
  doi          = {10.1016/j.asoc.2025.113116},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113116},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stream neural network for the prediction of multiple defects in continuous casting},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-forming-process transfer enabled graph neural networks for accurate axial-forming prediction in metal tube bending. <em>ASOC</em>, <em>175</em>, 113114. (<a href='https://doi.org/10.1016/j.asoc.2025.113114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate axial forming of bent tubes is critical for securing the reliability of aeroengine systems. The growing demand for advanced, customized, and smoothly operating aeroengines has driven the adoption of metal bent tubes with spatially free axes, formed by free bending (FB). However, the complexity of the FB process limits data accessibility and diminishes the axial prediction accuracy of deep-learning approaches. To address these challenges, a novel cross-forming-process transfer (CFPT) learning framework is proposed herein. The CFPT realizes precise axis prediction by transferring knowledge from a data-rich rotary-draw bending (RDB) process to a data-limited FB process. The CFPT employs graph neural networks (GNNs) as the base learner. Process reconstruction based on sparse coding realizes effective knowledge transfer by alleviating the effects of heterogeneity among the forming processes. This step is followed by adversarial domain adaption to extract the domain-invariant features between the source and target domains. Finally, a unified learning framework simplifies the model-training steps and prevents overfitting. Evaluation results revealed that CFPT significantly enhances the predictive accuracy, achieving a coefficient of determination (R 2 ) of 0.985, a root mean squared error (RMSE) of 6.645, and a mean absolute error (MAE) of 4.778 on the validation set, along with an R 2 of 0.970, an RMSE of 8.423, and an MAE of 5.766 on the test set—reflecting 30 % improvement on the validation set and 20 % improvement on the test set compared to the conventional methods. Further experiments also indicated that each of the transfer-learning strategies in CFPT is essential for successful knowledge transfer. These results demonstrated that the proposed CFPT precisely predicted spirally bent tube axes with limited data. This capability of the CFPT is expected to enable the smart manufacturing system for tube bending to respond flexibly to complex and variable bending demands.},
  archive      = {J_ASOC},
  author       = {Caicheng Wang and Zili Wang and Shuyou Zhang and Yaochen Lin and Yongzhe Xiang and Le Wang and Jianrong Tan},
  doi          = {10.1016/j.asoc.2025.113114},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113114},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-forming-process transfer enabled graph neural networks for accurate axial-forming prediction in metal tube bending},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Twin-population multiple knowledge-guided transfer prediction framework for evolutionary dynamic multi-objective optimization. <em>ASOC</em>, <em>175</em>, 113113. (<a href='https://doi.org/10.1016/j.asoc.2025.113113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multi-objective evolutionary algorithms (DMOEAs) have been widely studied, and one of the main tasks is the need for algorithms to track Pareto optimal front (POF) under dynamic environmental changes. Existing methods integrate transfer learning (TL) techniques to predict the initial population for the new environment. However, the lack of transferred individual diversity and inaccurate moving directions lead to poor performance of DMOEAs. Therefore, this work proposes a Twin-population Multiple Knowledge-guided Transfer prediction (TMKT) framework to form an initial population for the new environment. Three strategies, i.e., Twin Populations Guided prediction (TPG), SVM-based Multi-knowledge prediction (SVM-M) and Kernel Subspace Alignment for Transfer prediction (KSA-T), are designed to mine and transfer positive historical knowledge for accurately predicting changing POFs. First, TPG is used to obtain new approximate individuals and provide potential directions of subsequent transfer, which splits the population into two twin populations based on upper and lower quartile points of the first objective and their angles. Subpopulations transmit information by different similarity methods to obtain their new positions. Secondly, to obtain solutions with better diversity and convergence, SVM-M trains a certain classifier that can discriminate between positive and negative solutions and predicts labels of noisy solutions based on useful knowledge from the first two environments. Third, KSA-T is proposed to further enhance the accuracy of the new population prediction. The kernel trick and second-order feature alignment are introduced in subspace alignment to develop a new TL technique called Kernel Subspace Alignment (KSA) for adaptively achieving homotypic distributions of the source domain and target domain. Solutions predicted by TPG as the target domain are employed to guide the evolution, and obtained-SVM-M positive solutions are transferred to the new environment via KSA. TMKT is integrated with two baseline algorithms MOEA/D and NSGA-II to construct DMOEAs. Numerical results on 14 functions of different variation types and a real parameter optimization problem of control system validate the superior dynamic optimization performance of TMKT compared with five state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Shijie Zhao and Tianran Zhang and Miao Chen and Lei Zhang},
  doi          = {10.1016/j.asoc.2025.113113},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113113},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Twin-population multiple knowledge-guided transfer prediction framework for evolutionary dynamic multi-objective optimization},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic interrelationships among crude oil, green bond, and carbon markets: Evidence from fuzzy logic autoencoders. <em>ASOC</em>, <em>175</em>, 113112. (<a href='https://doi.org/10.1016/j.asoc.2025.113112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the dynamic interrelationships among various markets covering crude oil, green bonds, and carbon emissions from January 2014 to October 2022, using a Fuzzy Logistic Autoencoder (FLAE) model, which elevates methodological sophistication and helps capturing intricate and complex relationships across the three markets. Different features of FLAE, such as identifying crossed lags and introducing a novel sigmoid-type activation function, enhance structural stability and establish the model as a reference for studying cross-temporal effects across markets. The key findings indicate that green bond returns negatively impact the returns of carbon emission allowances and Brent oil in the short and medium term. The impact of carbon emission allowance returns and oil returns on the forecast of green bond returns is comparatively trivial. Forecasting green bond returns is primarily driven by its short-term lags. These findings should be useful for portfolio managers in energy markets, environmentally conscious investors, and policy-makers concerned with financial sustainability amid the energy transition.},
  archive      = {J_ASOC},
  author       = {Nini Johana Marín-Rodríguez and Elie Bouri and Juan David González-Ruiz and Sergio Botero and Alejandro Peña},
  doi          = {10.1016/j.asoc.2025.113112},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113112},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic interrelationships among crude oil, green bond, and carbon markets: Evidence from fuzzy logic autoencoders},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Challenges with hard-to-learn data in developing machine learning models for predicting the strength of multi-recycled aggregate concrete. <em>ASOC</em>, <em>175</em>, 113110. (<a href='https://doi.org/10.1016/j.asoc.2025.113110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on multi-recycled aggregate concrete (MRAC), which involves reusing recycled concrete, has been actively pursued to promote sustainable practices. However, studying the properties of MRAC often requires significant time and resources. Machine learning (ML)-based predictive methods offer a promising solution to overcome these challenges. This study developed and evaluated ML models to predict the compressive strength of MRAC using 197 samples, 8 input features, grid search, cross-validation, and 9 algorithms. The results demonstrated that ML models could achieve high accuracy (R² > 0.9) even without the application of advanced techniques. However, certain data points consistently exhibited high error rates under different ML algorithms, cross-validation methods and data split ratios, suggesting the presence of inherently difficult-to-learn data. The high error likely result from the integration of supplementary cementitious materials, which have contradictory effects on compressive strength, as well as issues arising from small sample sizes of compressive strength at early-age. These results highlight the importance of incorporating domain knowledge when constructing the database to improve model reliability. Finally, a post-analysis was conducted using Shapley Additive Explanations to clarify the relationship between the inputs and output, and recommendations were provided for improving MRAC properties for future research. This study provides valuable insights into the application of ML for predicting concrete properties.},
  archive      = {J_ASOC},
  author       = {Jeonghyun Kim},
  doi          = {10.1016/j.asoc.2025.113110},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113110},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Challenges with hard-to-learn data in developing machine learning models for predicting the strength of multi-recycled aggregate concrete},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A biased randomised GRASP for the electric vehicle routing problem with heterogeneous supplemental infrastructures. <em>ASOC</em>, <em>175</em>, 113109. (<a href='https://doi.org/10.1016/j.asoc.2025.113109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Green logistics policies have positioned electric vehicles (EVs) as the preferred choice for logistics. Prompted by technological advancements, more companies are now adopting electric logistics vehicles equipped with both charging and battery swapping capabilities. This study addresses the electric vehicle routing problem (EVRP) by integrating various charging technologies, partial charging strategies, and different battery swapping specifications. A mixed-integer programming (MIP) model is developed to minimise total logistics costs, including vehicle operating costs, energy replenishment costs, and variable mileage costs. To solve this problem, we design a biased randomised-greedy randomised adaptive search procedure (BR-GRASP) algorithm incorporating geometric distribution. This algorithm is complemented by local search operators and energy management strategies designed for heterogeneous supplemental infrastructures (HSI). For efficient iterative optimisation, we employ a variable neighbourhood descent (VND) mechanism. Computational experiments validate the effectiveness of HSI and the proposed algorithm from multiple perspectives. Additionally, a real-world case study demonstrates the significant benefits of applying our methods to a logistics company. The research findings offer decision-making recommendations and managerial insights for logistics companies adopting EVs, as well as for relevant government agencies.},
  archive      = {J_ASOC},
  author       = {Rui Xu and Bowen Song and Wei Xiao and Xing Fan},
  doi          = {10.1016/j.asoc.2025.113109},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113109},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A biased randomised GRASP for the electric vehicle routing problem with heterogeneous supplemental infrastructures},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven oil production strategy selection under uncertainties. <em>ASOC</em>, <em>175</em>, 113108. (<a href='https://doi.org/10.1016/j.asoc.2025.113108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a user-friendly tool to assist in selecting oil production strategies when facing high levels of uncertainty in a real case. Specifically, we address the challenge of determining optimal well-bore positioning and control parameters under uncertain geological conditions, aiming to maximize production efficiency while managing computational complexity. The model deals with decision-making factors and geological data, represented by high-dimensional maps traditionally handled through intensive numerical methods. The production strategy goes through robust optimization based on decision variables in set P , such as well-bore positioning, and uncertainties associated with 3D reservoir properties in set R , such as porosity and permeability. The method combines two sets of P variables, emphasizing positioning and control guidelines. The technique employs representative scenarios to find a generally applicable strategy considering P × R mixtures. The variables R describe a real and heterogeneous reservoir in the pre-salt area in Brazil. The method focuses on critical information through dimensionality reduction while guaranteeing faster, more accurate, robust decisions and balancing efficiency with effectiveness. We rely upon machine-learning, such as Gradient Boosting Regression with few-shot training strategies. The SHapley Additive exPlanations and feature importance allow the model interpretation, enabling us to understand how the well-bore positioning impacts the response. The method is integrated into the optimization loop to work alongside the simulator, and both methods work in tandem as a fast metaheuristic system supported by a slow numerical one. The method improves the computational footprint by 76%.},
  archive      = {J_ASOC},
  author       = {Gabriel Cirac and Guilherme Daniel Avansi and Jeanfranco Farfan and Denis José Schiozer and Anderson Rocha},
  doi          = {10.1016/j.asoc.2025.113108},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113108},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven oil production strategy selection under uncertainties},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixed supervised cross-subject seizure detection with transformer and reference learning. <em>ASOC</em>, <em>175</em>, 113104. (<a href='https://doi.org/10.1016/j.asoc.2025.113104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic seizure detection aims to identify occurrences of epileptic seizures, enabling timely seizure intervention and protecting patients’ safety. In recent years, deep learning has significantly promoted the research progress in the field of seizure detection. In this paper, we propose a cross-subject seizure detection system based on a transformer encoder. A novel data fusion approach is leveraged to mitigate the imbalance issue of seizure and non-seizure data. Meanwhile, a new mapping method is employed to replace traditional feature extractors, effectively enhancing the real-time capabilities of the system. A mixed supervised and unsupervised learning approach, coupled with a specially designed loss function, is utilized to strengthen the model's ability to capture temporal and spatial features in electroencephalogram (EEG) signals. Furthermore, an innovative learning strategy named reference learning is proposed to enhance the model's generalization performance. Finally, the proposed system was evaluated on the publicly available CHB-MIT dataset using the Leave-One-Out Cross-Validation (LOOCV) strategy. The system achieved a segment-based sensitivity of 91.06 % and an event-based sensitivity of 93.59 % in the cross-subject seizure detection task.},
  archive      = {J_ASOC},
  author       = {Landi He and Dezan Ji and Xingchen Dong and Haotian Li and Guoyang Liu and Weidong Zhou},
  doi          = {10.1016/j.asoc.2025.113104},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113104},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mixed supervised cross-subject seizure detection with transformer and reference learning},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor idempotent representation with auto-weighted exponential tensor nuclear norm minimization for multi-view clustering. <em>ASOC</em>, <em>175</em>, 113103. (<a href='https://doi.org/10.1016/j.asoc.2025.113103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To comprehensively capture the high-order correlations and intrinsic structures of multi-dimensional data, tensors are widely employed in multi-view clustering. However, existing algorithms have not achieved a clear tensor block-diagonal structure, failing to fully utilize the consistency information across multiple views. Additionally, tensor nuclear norm is often used to constrain tensors, but it tends to excessively shrink larger singular values, resulting in the substantial loss of information. To solve these issues, this paper proposes the tensor idempotent representation with auto-weighted exponential tensor nuclear norm minimization for multi-view clustering (TIR/AWETNN). Specifically, first, we leverage the self-expressive properties of the sample matrices to acquire self-expressive matrices, stack them into a tensor, and design a new tensor idempotent representation to achieve a clear tensor block-diagonal structure. The main objective of maximizing the utilization of consistency information across multiple perspectives and strengthening the algorithm's robustness. Second, we have designed an auto-weighted exponential tensor nuclear norm (AWETNN) to constrain the tensor, which serves as a better alternative to tensor rank. AWETNN takes full account of the physical differences among singular values through a non-convex penalty function, thus more accurately representing the correlation among multiple perspectives at a higher order. Finally, the above two steps are unified into one framework using the augmented Lagrange multiplier method. On some datasets, the TIR/AWETNN shows a performance enhancement of up to 42.78 %, with experimental results across diverse datasets demonstrating that the TIR/AWETNN algorithm surpasses the state-of-the-art algorithms. The code is publicly available at https://github.com/TongWuahpu/TIR-AWETNN .},
  archive      = {J_ASOC},
  author       = {Tong Wu and Gui-Fu Lu},
  doi          = {10.1016/j.asoc.2025.113103},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113103},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tensor idempotent representation with auto-weighted exponential tensor nuclear norm minimization for multi-view clustering},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval type-3 fuzzy set for rank-level fusion using multiple feature vectors: Theory and application to face recognition. <em>ASOC</em>, <em>175</em>, 113102. (<a href='https://doi.org/10.1016/j.asoc.2025.113102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Very recently interval type-3 fuzzy set theory is introduced as a more effective way of addressing uncertainty in decision making than its fuzzy set counterparts. This paper suggests an approach, which utilizes interval type-3 fuzzy sets to yield fuzzy ranks for rank-level fusion using multiple feature vectors for face recognition. Use of multiple feature vectors corresponding to different feature extraction techniques unveils the auxiliary discriminatory features. This aids in resolving inter-class resemblances to a certain degree. Fuzzy ranks for each of the pertinent classes are derived by formulating interval type-3 fuzzy sets with the feature vectors of the training samples and those of the near-by neighbouring training images from all the classes. This would attenuate the uncertainty caused by inter-class resemblances and intra-class variations. With respect to a feature vector, the classifier generates confidence factors for all the classes which exhibit their possibilities for being the class of the test image. The algorithm selects top H classes to utilize the type-3 fuzzy sets. For each of these classes, corresponding training samples are utilized to express the Gaussian secondary membership functions. Further, with respect to a training sample, by utilizing the feature vectors of its near-by neighbouring training images from all classes based on a similarity measure, it generates the tertiary domain. These similarity measures are employed to express lower and upper membership functions of scaled Gaussian domain of uncertainty of interval type-3 fuzzy set. The former and later approaches mitigate the effect of within-class differences and between-class resemblances, respectively. Finally, successive direct defuzzifications result in interval type-3 induced fuzzy ranks among the top H classes. We judiciously convolve these fuzzy ranks with the confidence factors, generated by multiple feature vectors, to get the final score for that class. The class with the lowest score identifies as the class of the test image. Four publicly accessible face databases are used to evaluate the method and the outcomes of the experiments demonstrate a higher level of effectiveness compared to the related methods.},
  archive      = {J_ASOC},
  author       = {Manas Ghosh and Jamuna Kanta Sing},
  doi          = {10.1016/j.asoc.2025.113102},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113102},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval type-3 fuzzy set for rank-level fusion using multiple feature vectors: Theory and application to face recognition},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic multi-objective optimization based on knowledge prediction and density clustering strategy. <em>ASOC</em>, <em>175</em>, 113099. (<a href='https://doi.org/10.1016/j.asoc.2025.113099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multi-objective evolutionary algorithms (DMOEAs) that extract historical knowledge from the past environment to predict new solutions are known to be effective for solving dynamic multi-objective optimization problems (DMOPs). However, most of the existing methods simply reuse historical solutions without further extracting the knowledge between different historical environment solutions, which may make the algorithm ignore some important historical knowledge and limit its performance. In this paper, we propose a knowledge prediction strategy and a density clustering strategy for DMOEA, called KPDCS-DMOEA, which aim to extract historical knowledge from the past environment to build a more accurate prediction model. Firstly, the trend of change in the initial environment is obtained by predicting previous environmental changes through linear prediction methods based on knee point clusters. Secondly, a strategy was proposed to pair the solutions between adjacent environments and construct each dimensional motion vector as historical knowledge. The training set is constructed according to the motion step of the motion vector and the motion direction of each dimension, and the neural network is trained to predict the initial population in the new environment. Finally, a guided evolution strategy based on a density clustering algorithm is developed to speed up population convergence and ensure that the population is well distributed. KPDCS-DMOEA is compared with several state-of-the-art DMOEAs. Experimental results show that the performance of KPDCS-DMOEA is better than the selected comparison algorithms.},
  archive      = {J_ASOC},
  author       = {Yong Wang and Shengao Wang and Kuichao Li and Gai-Ge Wang},
  doi          = {10.1016/j.asoc.2025.113099},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113099},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic multi-objective optimization based on knowledge prediction and density clustering strategy},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bug priority prediction using deep ensemble approach. <em>ASOC</em>, <em>175</em>, 113098. (<a href='https://doi.org/10.1016/j.asoc.2025.113098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A software bug is a fault in the programming of software or an application. Bugs cause problems ranging from stability to operability and are typically the result of human error during the programming process. They could be the result of a mistake or error, as well as a fault or defect. Software bugs should be discovered during the testing stage of the software development life cycle, but some may go undetected until after deployment. When addressing a bug, it is critical to consider its priority, which is determined manually. However, it was a difficult task, and making the wrong decision could lead to major software failures. Therefore, the primary goal of this study is to propose an ensemble approach for predicting bug priority levels in bug reports. We make use of Bugzilla's dataset, which includes over 25,000 bug reports. After preprocessing the data, this study applies a variety of feature extraction techniques, including Glove, Word2Vec TF-IDF, and Doc2Vec. Then, a model that primarily employs seven architectures of Convolutional Neural Network (CNN) Algorithms, including AlexNet, LeNet, VGGNet, 1DCNN, ResNet, ZF Net, and DenseNet as the basic models. The five architectures with the highest accuracy were then used in the ensemble method, which included ResNet, DenseNet, LeNet, AlexNet, and 1DCNN, with the final results determined by the majority values. The ensemble approach performed with 79.18 % of the final accuracy result. Other architectures include AlexNet 77.1 %, ZF Net 44.50 %, VGG Net 39.30 %, 1DCNN 75.44 %, ResNet 77.34 %, DenseNet 77.32 %, and LeNet 48.58 %. It was discovered that the proposed ensemble model outperformed each algorithm. Finally, when a new bug is discovered, it can be added to the proposed model, which will then determine its priority level.},
  archive      = {J_ASOC},
  author       = {P.G.S.M. Dharmakeerthi and R.A.H.M. Rupasingha and B.T.G.S. Kumara},
  doi          = {10.1016/j.asoc.2025.113098},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113098},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bug priority prediction using deep ensemble approach},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quaternary converter based balanced graph contrastive learning for recommendation. <em>ASOC</em>, <em>175</em>, 113096. (<a href='https://doi.org/10.1016/j.asoc.2025.113096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning (GCL) has emerged as a highly effective collaborative filtering method for recommender systems in recent years. However, existing collaborative filtering methods based on GCL often excessively prioritize user-side information, leading to inadequate exploration of user–item information. Furthermore, these methods generate contrastive views through data augmentation, which is prone to noise interference. To address these issues, we propose a balanced graph contrastive learning framework (BGCL). Specifically, BGCL incorporates a quaternary converter that introduces negative user based on triples (user, positive item, negative item), to provide the GCL module with embeddings that treat users and items balancedly. Subsequently, BGCL includes a noiseless GCL module that conducts contrastive learning on the embeddings after approximately infinite layers of convolution and the embeddings after k-layer graph convolutional networks to mitigate noise interference. We conducted experiments comparing our algorithm with 15 alternative approaches using real-world datasets, and the results demonstrate that our algorithm outperforms state-of-the-art methods in terms of recommendation accuracy and convergence speed.},
  archive      = {J_ASOC},
  author       = {Fan Ye and Hongwei Li and Zhangling Duan and Zhaolong Ling},
  doi          = {10.1016/j.asoc.2025.113096},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113096},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quaternary converter based balanced graph contrastive learning for recommendation},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework to undersample and refine the synthetic minority set. <em>ASOC</em>, <em>175</em>, 113095. (<a href='https://doi.org/10.1016/j.asoc.2025.113095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oversampling the minority class is a popular strategy for coping with the imbalance of datasets. It improves the cognition of the minority points to an admissible extent. Nonetheless, the synthetic minority instances accentuate the overlap between the majority class and the augmented minority class. It is detrimental to the rightful cognition of both classes. To this end, this paper introduces a novel strategy to undersample the synthetic minority set. A multi-armed bandit (MAB) guided protocol is followed to [i] identify the synthetic minority instances that contribute to the increased overlap between the two classes and [ii] subsequently remove (undersample) them iteratively to obtain a refined synthetic minority set. Simulation on synthetic datasets shows that the proposed strategy is successful in increasing the Gromov–Wasserstein distance between the original majority class distribution and the synthetic minority points’ distribution (as compared to the regular oversampled data obtained through state-of-the-art techniques). Empirical evaluation in sixteen real-world datasets, four state-of-the-art minority oversamplers, and two refinement techniques manifest the competence of the proposed strategy over baseline results and against the two competing methods. The proposed strategy has improved the performance of the majority class without bringing down the minority class’s performance and can be incorporated in sensitive real-world domains.},
  archive      = {J_ASOC},
  author       = {Payel Sadhukhan},
  doi          = {10.1016/j.asoc.2025.113095},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113095},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A framework to undersample and refine the synthetic minority set},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-search mechanism-improved differential evolution algorithm for the no-wait flow shop scheduling problem. <em>ASOC</em>, <em>175</em>, 113094. (<a href='https://doi.org/10.1016/j.asoc.2025.113094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The no-wait flow shop scheduling problem is a crucial model in large-scale manufacturing environments. A multi-search mechanism-improved differential evolution (MMDE) algorithm is proposed in this research to solve the no-wait flow shop scheduling problem (NWFSP) with the objective of minimizing the maximum makespan. The algorithm operates as follows: First, the largest order value (LOV) rule is utilized to convert continuous individual vectors in the differential evolution algorithm into discrete job permutations for the NWFSP. Second, an initial population comprising high-quality individuals is constructed by combining the modified NEH (Nawaz–Enscore–Ham) algorithm. Third, a statistical learning method is employed to establish a probability model of estimation of the distribution algorithm (EDA), which describes the solution distribution from a macroscopic group perspective. Fourthly, weighted mutation operations, along with three crossover mechanisms, are used to obtain more promising solutions. Finally, a local search mechanism based on the NEH is designed to enhance optimization capability and expedite the convergence of the algorithm. The simulation results show that, compared with existing algorithms, the MMDE algorithm is highly effective in solving the NWFSP, exhibiting significant advantages.},
  archive      = {J_ASOC},
  author       = {Yizhen Li and Chundong Zheng and Xiaobing Pei},
  doi          = {10.1016/j.asoc.2025.113094},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113094},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-search mechanism-improved differential evolution algorithm for the no-wait flow shop scheduling problem},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Greedy mechanism-based bi-objective optimization for green scheduling in manufacturing systems considering transportation. <em>ASOC</em>, <em>175</em>, 113093. (<a href='https://doi.org/10.1016/j.asoc.2025.113093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses scheduling challenges in hybrid flow manufacturing systems with crane transportation (HFMS-CT) driven by intelligent control, mass customization, and eco-friendly manufacturing. Unlike previous studies, it considers the interdependence between machine processing and crane transport, focusing on minimizing both makespan and energy consumption. A bi-objective mixed-integer programming model is developed, and the Epsilon-constraint method is used for small-scale cases. Given the NP-hardness, a modified multi-objective Harris Hawk optimization (MMOHHO) is proposed. It adopts greedy mechanisms by integrating Laplace crossover, tent-based chaotic mapping, elite selection, and nonlinear optimization strategy to balance exploration and exploitation capabilities. The proposed algorithm is compared with the Epsilon-constraint method and benchmark metaheuristics. The experimental results reveal that the proposed algorithm outperforms other methods regarding NPS, DPO, IGD, and ES evaluation metrics. Finally, an in-depth discussion is conducted using a real-world case study, offering valuable managerial insights and practical recommendations for implementation.},
  archive      = {J_ASOC},
  author       = {Zhu Wang and Rongping Qiu and Binghai Zhou},
  doi          = {10.1016/j.asoc.2025.113093},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113093},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Greedy mechanism-based bi-objective optimization for green scheduling in manufacturing systems considering transportation},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining N-grams and graph convolution for text classification. <em>ASOC</em>, <em>175</em>, 113092. (<a href='https://doi.org/10.1016/j.asoc.2025.113092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification, a cornerstone of natural language processing (NLP), finds applications in diverse areas, from sentiment analysis to topic categorization. While deep learning models have recently dominated the field, traditional n-gram-driven approaches often struggle to achieve comparable performance, particularly on large datasets. This gap largely stems from deep learning’ s superior ability to capture contextual information through word embeddings. This paper explores a novel approach to leverage the often-overlooked power of n-gram features for enriching word representations and boosting text classification accuracy. We propose a method that transforms textual data into graph structures, utilizing discriminative n-gram series to establish long-range relationships between words. By training a graph convolution network on these graphs, we derive contextually enhanced word embeddings that encapsulate dependencies extending beyond local contexts. Our experiments demonstrate that integrating these enriched embeddings into an long-short term memory (LSTM) model for text classification leads to around 2% improvements in classification performance across diverse datasets. This achievement highlights the synergy of combining traditional n-gram features with graph-based deep learning techniques for building more powerful text classifiers.},
  archive      = {J_ASOC},
  author       = {Tarık Üveys Şen and Mehmet Can Yakit and Mehmet Semih Gümüş and Orhan Abar and Gokhan Bakal},
  doi          = {10.1016/j.asoc.2025.113092},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113092},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combining N-grams and graph convolution for text classification},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of deep learning networks for nonlinear delay differential system for stuxnet virus spread in an air gapped critical environment. <em>ASOC</em>, <em>175</em>, 113091. (<a href='https://doi.org/10.1016/j.asoc.2025.113091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the tranquil confines of air-gapped environment, the custodians of digital fortitude must recognize the limitations of a singular defense mechanism, the cornerstone of this defensive architecture lies in proactive threat detection and rapid response capabilities. In the presented study, a deep-learning based bidirectional LSTM architecture is designed to accurately capture the time-delay differential propagation dynamics of the Stuxnet virus in an air gapped environment intricately linked with a network of critical control infrastructure. To address the challenges encountered in compromising the air gapped environment, the mathematical model introduces time delay factors τ 1 , τ 2 and τ 3 , necessary for exploiting the susceptible USB media, susceptible air gapped computers utilizing infected USB media and connected susceptible computers using infected computers respectively. Removable storage media serves as a pivotal link in bridging the air gapped environment and controlling the industrial controllers connected to critical systems thereby posing a significant threat to the integrity of the entire system. Synthetic temporal simulations serve as the ground truth for dual-layer bidirectional LSTM networks exactment on various scenarios involving the infiltration of the air-gapped environment by the Stuxnet virus in a time delay differential system. A detailed comparative analysis with numerical outcomes showed minimal disparity between the predictions generated by LSTM networks, with mean squared error (MSE) values falling within the range of 1 0 − 7 underscoring the effectiveness, robustness, and stability of the proposed neural networks in predicting the complex dynamics of virus in air gapped situation.},
  archive      = {J_ASOC},
  author       = {Muhammad Junaid Ali Asif Raja and Zaheer Masood and Ijaz Hussain and Aneela Zameer and Muhammad Asif Zahoor Raja},
  doi          = {10.1016/j.asoc.2025.113091},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113091},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design of deep learning networks for nonlinear delay differential system for stuxnet virus spread in an air gapped critical environment},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fostering fairness in image classification through awareness of sensitive data. <em>ASOC</em>, <em>175</em>, 113090. (<a href='https://doi.org/10.1016/j.asoc.2025.113090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) has demonstrated remarkable ability to uncover hidden patterns in data. However, the presence of biases and discrimination originating from the data itself and, consequently, emerging in the ML outcomes, remains a pressing concern. With the exponential growth of unstructured data, such as images, fairness has become increasingly critical, as neural network (NN) models may inadvertently learn and perpetuate societal and historical biases. To address this challenge, we propose a fairness-aware loss function that iteratively prioritizes the worst-performing sensitive group during NN training. This approach aims to balance treatment quality across sensitive groups, achieving fairer image classification outcomes while incurring only a slight compromise in overall performance. Our method, evaluated on the FairFace dataset, demonstrates significant improvements in fairness metrics while maintaining comparable overall quality. These trade-offs highlight that the minor decrease in overall quality is justified by the improvement in fairness of the models.},
  archive      = {J_ASOC},
  author       = {Ivona Colakovic and Sašo Karakatič},
  doi          = {10.1016/j.asoc.2025.113090},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113090},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fostering fairness in image classification through awareness of sensitive data},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security modules of delegation methods in mobile cloud computing using probabilistic interval neutrosophic hesitant fuzzy set based decision-making model. <em>ASOC</em>, <em>175</em>, 113089. (<a href='https://doi.org/10.1016/j.asoc.2025.113089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security models of delegation methods in mobile cloud computing (MCC) play a crucial role in mitigating security threats when offloading device operations to the cloud while ensuring optimal performance. These threats include data breaches, unauthorized access, and loss of control over sensitive data. Although numerous security models have been developed for different delegation methods, categorized under layering, authentication, dynamic offloading, and encryption, none fully satisfy all development security criteria despite extensive research efforts. To address this gap, this paper proposes a decision-modeling approach to identify the most effective security modules for delegation methods in MCC. Such problem falls under multi-criteria decision-making (MCDM) due to (1) the presence of multiple development security criteria, (2) the varying importance of these criteria with inherent ambiguity and uncertainty, and (3) data variation. To achieve this, we developed a fuzzy-weighted zero-inconsistency method (FWZIC) under a probabilistic interval neutrosophic hesitant fuzzy set (PINHFS) and the evaluation based on distance from the average solution (EDAS) technique. In addition, the decision matrix in this paper is constructed by crossing 50 security modules from four delegation methods in MCC, categorized as follows: 23 encryption, 12 authentication, 9 layering, and 6 dynamic offloading modules, evaluated based on 13 development security criteria. The findings of PINHFS–FWZIC method indicate that C 7 ‘Data Security’ was the most critical and sensitive evaluation criterion, while C 1 ‘General Security Issue’ was the least sensitive. According to the EDAS results, SM_07 ranked highest for layering and encryption-based delegation methods. For dynamic offloading, SM_02 and SM_03 achieved the top ranking, while for authentication, SM_02 and SM_03 were also identified as the best security modules. To validate the robustness of the proposed approach, sensitivity analysis, Spearman’s correlation coefficient test, and comparative analysis were conducted. The results confirm that the proposed approach provides a comprehensive and reliable method for selecting optimal security modules in MCC delegation methods.},
  archive      = {J_ASOC},
  author       = {Sendeyah Al Hantoobi and A.A. Zaidan and Hassan Abdulsattar Ibrahim and Sarah Qahtan and Muhammet Deveci and Sinan Isik and Hana Tomášková},
  doi          = {10.1016/j.asoc.2025.113089},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113089},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Security modules of delegation methods in mobile cloud computing using probabilistic interval neutrosophic hesitant fuzzy set based decision-making model},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on resilient microgrid system from cybersecurity perspective. <em>ASOC</em>, <em>175</em>, 113088. (<a href='https://doi.org/10.1016/j.asoc.2025.113088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to increases in communication speed, computation, the liberalization of the electrical service business, and the environmental impact of traditional power generation technologies, Distributed Energy Resources (DERs) power systems such as microgrids are gaining in popularity. It is therefore imperative to develop resilient microgrid systems capable of withstanding cyber physical threats. The capacity to integrate Machine Learning (ML) and Deep Learning (DL) to analyze energy data has created opportunities for businesses and academia to explore the possibilities of enhancing the cybersecurity of microgrid systems. This study surveys and discusses recent developments, challenges, and opportunities in cybersecurity for microgrid systems, from both attack and defense perspectives. In this paper, we address the current state and future directions in cybersecurity in industrial communication networks, and endpoint security in distributed control systems. This paper discusses attack types including Man-In-The-Middle (MITM), False Data Injection (FDI), and Distributed Denial of Service (DDoS) attacks, alongside defensive mechanisms including AI-based detection and multi-layered security frameworks. Furthermore, this survey offers comprehensive insights into benchmark datasets and open-source tools frequently utilized in experimental research and practical applications. It includes an in-depth comparison, discussion, and opportunities for future research to guide the research community’s focus and advancing progress in the field.},
  archive      = {J_ASOC},
  author       = {Zhibo Zhang and Benjamin Turnbull and Shabnam Kasra Kermanshahi and Hemanshu Pota and Ernesto Damiani and Chan Yeob Yeun and Jiankun Hu},
  doi          = {10.1016/j.asoc.2025.113088},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113088},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A survey on resilient microgrid system from cybersecurity perspective},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-adaptive framework of reducing domain bias under distribution shift for semi-supervised domain generalization. <em>ASOC</em>, <em>175</em>, 113087. (<a href='https://doi.org/10.1016/j.asoc.2025.113087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have achieved remarkable progress, but still face significant challenges such as domain shift and the scarcity of labeled data. Existing pseudo-labeling methods are affected by domain bias when dealing with distribution shifts across domains. To address these challenges, this paper proposes a novel self-adaptive framework for semi-supervised domain generalization. The proposed framework has three key components: (1) DGMatch, a pseudo-labeling method that encourages domain fairness and uncertainty weighting to generate more reliable pseudo-labels, thus overcoming the domain bias issues of existing pseudo-labeling approaches; (2) AutoMixLayer, a plug-and-play module that automatically adjusts the parameters for domain-style mixing, thus reducing the domain shift in the source data; and (3) DCT, a method that randomly transfers the target domain-style to a prototype of the source domain-style, effectively calibrating the domain-style during the test stage. These three modules allow the proposed framework to effectively alleviate the problem of performance degradation when faced with domain bias. Extensive experiments on image classification benchmarks and a medical image segmentation dataset demonstrate that the proposed self-adaptive framework outperforms state-of-the-art semi-supervised domain generalization methods. Further analysis reveals that the framework can effectively balance the quantity and quality of pseudo-labels, enabling robust model generalization.},
  archive      = {J_ASOC},
  author       = {Liangqing Hu and Zuqiang Meng},
  doi          = {10.1016/j.asoc.2025.113087},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113087},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-adaptive framework of reducing domain bias under distribution shift for semi-supervised domain generalization},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards sustainable societies: Convolutional neural networks optimized by modified crayfish optimization algorithm aided by AdaBoost and XGBoost for waste classification tasks. <em>ASOC</em>, <em>175</em>, 113086. (<a href='https://doi.org/10.1016/j.asoc.2025.113086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The categorization of waste is playing a pivotal role in addressing and alleviating the environmental and health repercussions linked to waste. It is essential for safeguarding the environment, as improper handling of hazardous waste may result in soil, water, and air contamination, posing significant threats to ecosystems and human well-being and maintaining a sustainable society. Effective waste classification enhances the efficacy of waste management by organizing waste into distinctive groups based on characteristics that include toxicity, flammability, recyclable potential, and biodegradability. This research introduces a methodology that relies on employing convolutional neural networks and the AdaBoost and XGBoost models for the purpose of waste classification. It emphasizes the necessity of customizing every deep learning method to suit the specific problem that needs to be solved. An altered form of the latterly proposed crayfish optimization algorithm is suggested, explicitly developed to meet the requirements of the particular waste classification task in hand. The assessment of the presented method using real-world datasets consistently demonstrates that models configured by the proposed modified algorithm achieve an accuracy level that exceeds 89.6140%. This pinpoints the considerable potential of this method in effectively addressing pressing problems in waste management within real-world scenarios.},
  archive      = {J_ASOC},
  author       = {Ana Tasic and Luka Jovanovic and Nebojsa Bacanin and Miodrag Zivkovic and Vladimir Simic and Miroslav Popovic and Milos Antonijevic},
  doi          = {10.1016/j.asoc.2025.113086},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113086},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards sustainable societies: Convolutional neural networks optimized by modified crayfish optimization algorithm aided by AdaBoost and XGBoost for waste classification tasks},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient fine-tuning of small-parameter large language models for biomedical bilingual multi-task applications. <em>ASOC</em>, <em>175</em>, 113084. (<a href='https://doi.org/10.1016/j.asoc.2025.113084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating computational costs of large language models (LLMs) have catalyzed the pursuit of more efficient alternatives, particularly in specialized domains like biomedicine. In this study, we propose BioQwen, a series of small-parameter biomedical bilingual (Chinese–English) multi-task models designed to mitigate the resource demands of LLMs while achieving high performance. BioQwen is trained on carefully curated open-source biomedical datasets, employing a stringent preprocessing pipeline with thorough quality filtering and standardized formatting. Through an efficient two-stage fine-tuning strategy, BioQwen models with 0.5B, 1.5B, and 1.8B parameters attain competitive performance across a variety of comprehension and generative tasks. For comprehension tasks, BioQwen-1.8B achieves a Macro F1 score of 0.730 and a balanced accuracy of 0.802 on the BC5CDR dataset, surpassing the 7B-parameter Taiyi model’s scores of 0.685 and 0.757. In generative tasks, BioQwen delivers superior zero-shot results on the iCliniq dataset, outperforming all baselines across multiple metrics. Comparisons with established small-parameter LLMs (e.g., Llama3.2 1B) further substantiate the effectiveness of domain-specific fine-tuning. Significantly, BioQwen’s reduced iteration time highlights its computational efficiency, and its quantized version demonstrates successful deployment on mobile devices, confirming its viability in resource-constrained settings. This study demonstrates the potential of strategically fine-tuned small-parameter LLMs to deliver resource-efficient, high-performing solutions for biomedical bilingual applications, expanding accessibility and usability in the field.},
  archive      = {J_ASOC},
  author       = {Yinghong Li and Yudong Yan and Zhuohao Tong and Yu Wang and Yinqi Yang and Mingze Bai and Dan Pu and Jiazheng Xie and Chuan Liu and Bo Li and Mingwei Liu and Kunxian Shu},
  doi          = {10.1016/j.asoc.2025.113084},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113084},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient fine-tuning of small-parameter large language models for biomedical bilingual multi-task applications},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing learning paths: Course recommendations based on graph convolutional networks and learning styles. <em>ASOC</em>, <em>175</em>, 113083. (<a href='https://doi.org/10.1016/j.asoc.2025.113083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of Massive Open Online Course (MOOC) platforms and the growing popularity of self-directed learning, an increasing number of learners are utilizing online platforms to access educational resources. While these extensive course resources offer learners diverse and accessible learning experiences, they also present challenges in personalized course selection. Traditional recommendation models often lack sufficient interpretability and fail to effectively leverage the interactive data generated during curriculum learning or account for the impact of individual learning styles on recommendations. To address these limitations, this study proposes a novel model, Course Recommendations based on Graph Convolutional Networks and Learning Styles to Optimize Learning Paths. Firstly, learner-course interaction data is recursively propagated through graph convolutional networks to generate predictive scores for courses. Secondly, a matching scale between courses and learning styles is established to compute similarity scores. Finally, the predictive scores and learning style similarity scores are integrated to achieve personalized course recommendations. The experimental results on the MOOCCube dataset demonstrate that CGCNLS significantly outperforms the baseline methods across multiple evaluation metrics, and the average performance of Precision, Recall and NDCG is improved by 6.94 %, 6.63 % and 7.98 %, respectively, under different Top-K Settings (K = 5, 10, 20, and 30), which can more effectively recommend courses for learners. The findings of this research provide robust support for further advancements in recommender systems and are expected to enhance the user experience and learning outcomes on online learning platforms.},
  archive      = {J_ASOC},
  author       = {Guodao Zhang and Xiaoyun Gao and Haiyang Ye and Junyi Zhu and Wenqian Lin and Zizhao Wu and Haijun Zhou and Zi Ye and Yisu Ge and Alireza Baghban},
  doi          = {10.1016/j.asoc.2025.113083},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113083},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing learning paths: Course recommendations based on graph convolutional networks and learning styles},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transcending modularity: A memetic algorithm combining triangle motif and edge information for community detection. <em>ASOC</em>, <em>175</em>, 113082. (<a href='https://doi.org/10.1016/j.asoc.2025.113082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research towards community detection plays a crucial role in revealing the topological structure and functional characteristics of complex networks. Nowadays, modularity and its variants are the most popular community quality evaluation metric applied in the detection of community structures. However, these modularity-based methods rarely consider higher-order structural information such as motifs in a network, which may lead to an incomplete and inaccurate understanding of the network. While some motif-based methods have been proposed, they suffer from resolution limitations and completely ignore lower-order structures. To bridge this gap and address community detection problem from a more hybrid view focusing on both lower-order and higher-order structures, this paper first proposes an adaptive hybrid-order modularity optimization function termed as TE-Modularity, which harmonizes triangle motif and edge information. It transcends traditional modularity by considering both lower-order and higher-order structures and can be applicable to all types of networks. In addition, we design a memetic algorithm called TE-MA, that uses TE-Modularity as the objective function to solve the community detection problem. A novel mutation operator based on triangle motifs is proposed, which can effectively accelerate the convergence of the proposed algorithm. Furthermore, we develop a new multipoint local search strategy, striking a good balance between the efficiency and quality of the algorithm. Through experiments conducted on different optimizers and comparisons with several state-of-the-art community detection methods, our approach's effectiveness and superiority are demonstrated on both real and synthetic networks.},
  archive      = {J_ASOC},
  author       = {Xiangyi Teng and Xinyue Luo and Jing Liu},
  doi          = {10.1016/j.asoc.2025.113082},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113082},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transcending modularity: A memetic algorithm combining triangle motif and edge information for community detection},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking the multi-scale feature hierarchy in object detection transformer (DETR). <em>ASOC</em>, <em>175</em>, 113081. (<a href='https://doi.org/10.1016/j.asoc.2025.113081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Detection Transformer (DETR) has emerged as the dominant paradigm in the field of object detection due to its end-to-end architectural design. Researchers have explored various aspects of DETR, including its structure, pre-training strategies, attention mechanisms, and query embeddings, achiving significant progress. However, high computational costs limit the efficient use of multi-scale feature maps and hinder the full exploitation of complex multi-branch structures. We examine the negative impact of multi-scale features on the computational cost of DETRs and find that introducing long sequence data to the encoder is suboptimal. In this work, we aim to further push the boundaries of DETR’s performance and efficiency from the model structure perspective, thus developing the fusion detection Transformer (F-DETR) with heterogeneous scale multi-branch structure. To the best of our knowledge, this is the first explicit attempt to integrate multi-scale features into the end-to-end DETR structure. Specifically, we propose a multi-branch structure to simultaneously utilize feature maps at different levels, facilitating the interaction of local and global features. Additionally, we select certain joint latent variables from the interactive information flow to initialize the object container, a technique commonly used in query-based detectors. Experimental results show that F-DETR achieves a 43.9 % AP using 36 training epochs on the popular public COCO dataset. Furthermore, our approach demonstrates a better trade-off between accuracy and complexity compared to the original DETR.},
  archive      = {J_ASOC},
  author       = {Fanglin Liu and Qinghe Zheng and Xinyu Tian and Feng Shu and Weiwei Jiang and Miaohui Wang and Abdussalam Elhanashi and Sergio Saponara},
  doi          = {10.1016/j.asoc.2025.113081},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113081},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Rethinking the multi-scale feature hierarchy in object detection transformer (DETR)},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous space co-sparse representation: Leveraging fuzzy dependency and feature reconstruction for feature selection. <em>ASOC</em>, <em>175</em>, 113080. (<a href='https://doi.org/10.1016/j.asoc.2025.113080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an efficient approach to dimensionality reduction. There is a large number of literatures tackling this issue. Most of them prioritize classification ability of features, but often fail to fully consider the synergistic effect of local and global subspace information, thus limit the performance of feature selection in revealing the intrinsic structure of data. In this paper, a novel embedded feature selection model, called the heterogeneous space collaborative sparse representation for feature selection through leveraging fuzzy dependency and feature reconstruction (HCoSRDC), is proposed. In the proposed model, a fuzzy self-information operator is constructed to nonlinearly map samples from their feature space to a fuzzy dependency space, where the fuzzy dependency discloses classification ability of features and the local subspace structure in data is captured. Furthermore, samples are sparsely self-represented in their feature reconstruction space to extract global subspace structure while emphasizing feature distinctiveness. The consistency between local sparse representation and global sparse representation is integrated to learn weights of features for feature selection. An algorithm is designed to solve HCoSRDC. Extensive experiments on various benchmark datasets are conducted and experimental results demonstrate the superior performance of the proposed model in comparison with the state-of-the-art models for feature selection.},
  archive      = {J_ASOC},
  author       = {Yang Huang and Tingquan Deng and Ge Yang and Changzhong Wang},
  doi          = {10.1016/j.asoc.2025.113080},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113080},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heterogeneous space co-sparse representation: Leveraging fuzzy dependency and feature reconstruction for feature selection},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal sentiment analysis with text-augmented cross-modal feature interaction attention network. <em>ASOC</em>, <em>175</em>, 113078. (<a href='https://doi.org/10.1016/j.asoc.2025.113078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is the foundation of human–computer interaction and personalized services. With the rapid growth of online social platforms, people are more inclined to express their opinions through videos (text, acoustic, and visual). To analyze the sentiment tendency expressed in a video, multimodal sentiment analysis research has increasingly attracted the attention of researchers. The existing multimodal sentiment analysis methods mainly focus on the analysis of explicit sentiment. However, to avoid triggering societal sensitivity to information, people often implicitly express sentiment in videos, especially in the key modal text, which is rich in semantics and accuracy. Currently, very few studies, although considering the existence of implicit emotions, only perform simple mapping of multimodal information, which leads to insufficient information extraction and significant influence from noise. To address this, we propose an implicit sentiment analysis method based on cross-modal feature interaction attention network, which leverages label, visual, and acoustic information to assist in reasoning out implicit sentiment. Specifically, we construct a prompt sentence based on the annotated label and fuse it with the original text to alleviate the deep implicitness of text-sentiment and increase the sentiment cues in the original text. Meanwhile a text-based multi-head interactive attention mechanism is designed to learn text-related auditory and visual features. Considering the impact of noise or redundant information on the model’s robustness, we design a text-centric dual cross-modal deep fusion mechanism to weaken the noise from visual, auditory, and cross-modal interactions, and to obtain effective multimodal sentiment features. We conduct extensive experiments on four public multimodal datasets. The results show that our approach outperforms the existing methods and can explain the contributions of label, visual, and acoustic in multimodal implicit-sentiment analysis.},
  archive      = {J_ASOC},
  author       = {Huanxiang Zhang and Junjie Peng and Zesu Cai},
  doi          = {10.1016/j.asoc.2025.113078},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113078},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multimodal sentiment analysis with text-augmented cross-modal feature interaction attention network},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oscillating activation functions can improve the performance of convolutional neural networks. <em>ASOC</em>, <em>175</em>, 113077. (<a href='https://doi.org/10.1016/j.asoc.2025.113077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have been successful in solving many socially important and economically significant problems. Their ability to learn complex high-dimensional functions hierarchically can be attributed to the use of nonlinear activation functions. A key discovery that made training deep networks feasible was the adoption of the Rectified Linear Unit (ReLU) activation function to alleviate the vanishing gradient problem caused by using saturating activation functions. Since then, many improved variants of the ReLU activation have been proposed. However, a majority of activation functions used today are non-oscillatory and monotonically increasing due to their biological plausibility. This paper demonstrates that oscillatory activation functions can improve gradient flow and reduce network size. Two theorems on limits of non-oscillatory activation functions are presented. A new oscillatory activation function called Growing Cosine Unit(GCU) defined as C ( z ) = z ⋅ cos z that outperforms Sigmoids, Swish, Mish and ReLU on a variety of architectures and benchmarks is presented. The GCU activation has multiple zeros enabling single GCU neurons to have multiple hyperplanes in the decision boundary. This allows single GCU neurons to learn the XOR function without feature engineering. Extensive experimental comparison with 16 popular activation functions indicate that the GCU activation function significantly improves performance on CIFAR-10, CIFAR-100, Imagenette and the 1000 class ImageNet benchmarks.},
  archive      = {J_ASOC},
  author       = {Mathew Mithra Noel and Arunkumar L. and Advait Trivedi and Praneet Dutta},
  doi          = {10.1016/j.asoc.2025.113077},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113077},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Oscillating activation functions can improve the performance of convolutional neural networks},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting synthetic data generation to enhance pollution prediction. <em>ASOC</em>, <em>175</em>, 113076. (<a href='https://doi.org/10.1016/j.asoc.2025.113076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pollution in urban areas is turning into a primary focus for local governments in developed nations around the globe. Lots of data are currently collected for this from smart developments provided with atmospheric and climatic sensors. A hot research line is now exploiting such data to extract patterns and predict pollution levels in such a way that countermeasures can be taken beforehand and exposure to harmful concentrations is avoided. However, a key issue is the lack of significant data, due to incomplete smart infrastructures or calibration problems in sensors. Dealing with this, in this paper we propose the exploitation of synthetic data generation to enhance pollution prediction based on limited data sources, concretely extending real measurements of two weeks to up to ten extra years. We present a data generation approach based on Generative Adversarial Networks (GANs), with a particular model focused on generating artificial pollution data, which is later exploited using different Machine Learning (ML) algorithms. Results indicate that the usage of synthetic data further improves prediction when used as the basis dataset to be later finetuned using real records. For 62% of pollutants this way to proceed in data mixing (among five different approaches) provides the best results in evaluations. Such effect is due to extra model robustness due to data regularization, and better generalization capabilities by avoiding sensor limitations in real deployments.},
  archive      = {J_ASOC},
  author       = {Juan Morales-García and Emilio Ramos-Sorroche and Sara Balderas-Díaz and Gabriel Guerrero-Contreras and Andrés Muñoz and Jose Santa and Fernando Terroso-Sáenz},
  doi          = {10.1016/j.asoc.2025.113076},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113076},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exploiting synthetic data generation to enhance pollution prediction},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic parameters genetic algorithm for collaborative strike task allocation of unmanned aerial vehicle clusters towards heterogeneous targets. <em>ASOC</em>, <em>175</em>, 113075. (<a href='https://doi.org/10.1016/j.asoc.2025.113075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative strikes by unmanned aerial vehicle clusters (UAVCs) is becoming a key focus in the future air warfare, which can significantly enhance warfare effectiveness and reduce costs. To exactly describe the real battlefield scenarios, various heterogeneous strike-targets should be embedded. However, it will significantly increase the complexity of multi-constraint combinatorial optimization problem, thus the traditional genetic algorithm (GA) is difficult to solve efficiently due to its unchanged gene operator. In this paper, a dynamic parameters genetic algorithm has been proposed for UAVCs collaborative task allocation towards heterogeneous targets. Firstly, according to the differences of type, value, combat and defense, the heterogeneous strike-targets have been abstracted into strike target points and the UAVCs have been formulated into a set. Secondly, an innovative multiple unmanned aerial vehicles duplicate tasks orienteering problem (MUDTOP) model has been built to achieve multiple strikes on certain targets. Finally, the new triple-chromosome encoding and duplicate gene segments have been designed, and a novel genetic algorithm called DPGA-TEDG has been presented through dynamic gene operator. Experimental comparison results across various battlefield scales demonstrate that the outcomes of the proposed DPGA-TEDG algorithm not only meet practical requirements, but also outperform that of the other three algorithms in both optimality and robustness. Especially, in the battlefield scale environment of 180 km* 180 km, the average objective value of DPGA-TEDG is better than that of traditional genetic algorithm (GA-TEDG), simulated annealing algorithm (SA) and particle swarm optimization algorithm (PSO) about 2.71 %, 6.58 % and 20.49 %, respectively.},
  archive      = {J_ASOC},
  author       = {Chao Zhang and Jianlu Guo and Fei Wang and Boyuan Chen and Chunshi Fan and Linghui Yu and Zhiwen Wang},
  doi          = {10.1016/j.asoc.2025.113075},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113075},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic parameters genetic algorithm for collaborative strike task allocation of unmanned aerial vehicle clusters towards heterogeneous targets},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite-time stabilization of fractional-order neural networks with time-varying delays: A generalized inequality approach and controller design. <em>ASOC</em>, <em>175</em>, 113074. (<a href='https://doi.org/10.1016/j.asoc.2025.113074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores finite-time stabilization methods for a specific class of neural networks with fractional-order dynamics and time-varying delays. The first contribution involves introducing a generalized inequality, an extension of the existing one, to analyze the finite-time stabilization behavior of the addressed model. This extension has successfully addressed numerous limitations and challenges present in existing works. Additionally, an explicit formula for calculating the finite-time stabilization duration is provided. Subsequently, two types of controllers—delay-independent and delay-dependent feedback controllers—are developed to achieve finite-time stabilization for the neural networks under consideration. The conditions for stability, dependent on both the delay and the order, are formulated as linear matrix inequalities using inequality techniques, Lyapunov stability theory, and the newly proposed finite-time stability inequality. These conditions ensure that the fractional-order neural network model is stabilized in finite-time. The efficacy of the suggested design approach is demonstrated through two numerical case studies.},
  archive      = {J_ASOC},
  author       = {M. Shafiya and N. Padmaja},
  doi          = {10.1016/j.asoc.2025.113074},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113074},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Finite-time stabilization of fractional-order neural networks with time-varying delays: A generalized inequality approach and controller design},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proximal recursive generalized hyper-gradient descent method. <em>ASOC</em>, <em>175</em>, 113073. (<a href='https://doi.org/10.1016/j.asoc.2025.113073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the non-convex, non-smooth composite optimization problem. It consists of a non-convex loss function and a non-smooth regularizer function that admits a proximal mapping. However, the method is still limited in handling objective functions that involve non-smooth regularizer. How to determine the step size for solving composite optimization problems can be a challenge. To address this gap, we propose a recursive gradient descent algorithm using generalized hyper-gradient descent, named ProxSarah-GHD, which utilizes variance reduction techniques and provides update rules for adaptive step sizes. To improve its generalization in proximal gradient descent, a generalized variant of hyper-gradient descent, named G eneralized H yper-gradient D escent (GHD), is proposed in this paper. We prove that ProxSarah-GHD attains a linear convergence rate. Moreover, we provide the oracle complexity of ProxSarah-GHD as O ϵ − 3 and O n ϵ − 2 + n in the online setting and finite-sum setting, respectively. In addition, to avoid the trouble of manually adjusting the batch size, we develop a novel E xponentially I ncreasing M ini-batch scheme for ProxSarah-GHD, named ProxSarah-GHD-EIM. The theoretical analysis that shows ProxSarah-GHD-EIM achieves a linear convergence rate is also provided, and shows that its total complexity is O ϵ − 4 + ϵ − 2 and O n + ϵ − 4 + ϵ − 2 in the online setting and finite-sum setting, respectively. Numerical experiments on standard datasets verify the superiority of the ProxSarah-GHD over other methods. We further analyze the sensitivity of the ProxSarah-GHD-EIM to its hyperparameters, conducting experiments on standard datasets.},
  archive      = {J_ASOC},
  author       = {Hao Zhang and Shuxia Lu},
  doi          = {10.1016/j.asoc.2025.113073},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113073},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Proximal recursive generalized hyper-gradient descent method},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thinking innovation strategy (TIS): A novel mechanism for metaheuristic algorithm design and evolutionary update. <em>ASOC</em>, <em>175</em>, 113071. (<a href='https://doi.org/10.1016/j.asoc.2025.113071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metaheuristic optimization algorithm(MHS) is a global optimization method inspired by natural phenomena, demonstrating superior performance in specific application scenarios. Traditional optimization algorithms utilize two main concepts: exploration, to expand the search range, and exploitation, to enhance solution accuracy. However, as problem complexity and application scenarios increase, MHS struggles to balance exploration and exploitation to find the optimal solution. Therefore, this paper introduces innovative characteristics of individual thinking and proposes a new Thinking Innovation Strategy (TIS). TIS does not aim for an optimal solution but seeks global optimization based on successful individuals, enhancing algorithm performance through survival of the fittest. This paper applies TIS strategies to improve various MHS algorithms and evaluates their performance on 57 engineering problems and the IEEE CEC2020 benchmarks. Experimental results indicate that the TIS-enhanced algorithms outperform the original versions across 57 engineering problems, according to Friedman ranking and Wilcoxon rank-sum test results. Some algorithms show significant improvement, demonstrating the feasibility and practicality of TIS for optimization problems. The TIS (LSHADE_SPACMA) of the source code can be accessed through the following ways: https://github.com/LIANLIAN-Serendipity/TIS-},
  archive      = {J_ASOC},
  author       = {Heming Jia and Xuelian Zhou and Jinrui Zhang},
  doi          = {10.1016/j.asoc.2025.113071},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113071},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Thinking innovation strategy (TIS): A novel mechanism for metaheuristic algorithm design and evolutionary update},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A performance-driven multi-stage KNN approach for local adaptive classification. <em>ASOC</em>, <em>175</em>, 113070. (<a href='https://doi.org/10.1016/j.asoc.2025.113070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key issue of the K-Nearest Neighbors (KNN) algorithm is determining the optimal neighborhood size K , which limits the widespread applicability of KNN. To address this, a performance-driven multi-stage KNN (PMKNN) approach is proposed in this paper. Given a set of alternative K values, the traditional KNN algorithm is initially employed in the PMKNN approach to identify the optimal K values for all known samples. A convex optimization model is then constructed based on the least squares loss function to learn the correlation between known samples and query samples. After the learned correlation is used to evaluate the performances of all candidate K values in classifying query samples, a weighted majority voting process is designed to generate the final classification results. Unlike existing KNN approaches, the proposed PMKNN approach considers multiple optimal K values for each query sample, enhancing classification stability and reliability. The proposed approach also reduces the negative impact of inappropriate K values on classification performance. An experimental study is conducted using twenty real-world classification datasets collected from two public data repositories to assess the effectiveness of the proposed PMKNN approach. The relevant results highlight the high classification performance of the proposed PMKNN approach compared to seven state-of-the-art KNN methods and underscore its predictive stability compared to the traditional KNN algorithm using all possible K values.},
  archive      = {J_ASOC},
  author       = {Che Xu and Zhenhua Fan},
  doi          = {10.1016/j.asoc.2025.113070},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113070},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A performance-driven multi-stage KNN approach for local adaptive classification},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of the anti-disturbance capability of fMRI-based spiking neural network based on speech recognition. <em>ASOC</em>, <em>175</em>, 113069. (<a href='https://doi.org/10.1016/j.asoc.2025.113069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exterior electromagnetic noise can degrade the performance of neuromorphic hardware based on brain-inspired model. Therefore, enhancing the robustness of a brain-inspired model is a critical issue. However, the topology of a brain-inspired model lacks bio-plausibility. The purpose of this paper is to enhance the anti-disturbance capability of brain-inspired model under exterior electromagnetic noise by improving its bio-plausibility. In this paper, we propose a new spiking neural network (SNN) as a brain-inspired model called fMRI-SNN, in which the topology is constrained by functional magnetic resonance imaging (fMRI) data from the human brain, the nodes are Izhikevich neuron models, and the edges are synaptic plasticity models (SPMs) with time delay co-regulated by excitatory synapses and inhibitory synapses. Then, taken speech recognition (SR) as a case study, the recognition performance of fMRI-SNN is certified. To evaluate its anti-disturbance capability, the SR accuracy of fMRI-SNN under exterior electromagnetic noise is investigated, and is compared with SNNs with alternative topologies. To reveal its anti-disturbance mechanism, the neuroelectric characteristics, adaptive adjustment of synaptic plasticity, and dynamic topological characteristics of fMRI-SNN under exterior electromagnetic noise are discussed. The results indicate that the SR accuracy of fMRI-SNN under exterior electromagnetic noise is higher than that of SNNs with alternative topologies, and our discussion elucidates its anti-damage mechanism. Our results prompt that the brain-inspired model with bio-plausibility can enhance its robustness.},
  archive      = {J_ASOC},
  author       = {Lei Guo and Chongming Li and Youxi Wu and Menghua Man},
  doi          = {10.1016/j.asoc.2025.113069},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113069},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of the anti-disturbance capability of fMRI-based spiking neural network based on speech recognition},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic integration of adaptive sampling and ensemble techniques in federated learning for aircraft engine remaining useful life prediction. <em>ASOC</em>, <em>175</em>, 113067. (<a href='https://doi.org/10.1016/j.asoc.2025.113067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial manufacturing, predicting the remaining useful life of machines is crucial for improving operational efficiency and reducing maintenance costs. However, data privacy concerns and commercial competition make traditional centralized data collection methods insufficient to meet these needs. Federated learning offers a decentralized training approach that protects data privacy, but existing research faces challenges such as inadequate performance of single models, data quality disparities, and improper client selection strategies. To address these issues, this study proposes an adaptive sampling-based ensemble federated learning framework. By integrating the predictions of multiple models, the framework reduces model errors and enhances prediction accuracy and generalization capability. Additionally, we design an adaptive sampling method that dynamically adjusts the client selection strategy based on data quality, focusing on clients with low-quality data to ensure that their contributions are effectively utilized. Experimental results show that the proposed framework significantly outperforms existing benchmark methods on the turbofan engine dataset, with a 12% reduction in RMSE and a 35% decrease in Score. Ablation experiments and sensitivity analysis confirm that the framework maintains reliable predictive performance and efficiency in dealing with issues such as data imbalance, missing data, and scale changes. Supplementary materials for this article are available online.},
  archive      = {J_ASOC},
  author       = {Ancha Xu and Renbing Wang and Xinming Weng and Qi Wu and Liangliang Zhuang},
  doi          = {10.1016/j.asoc.2025.113067},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113067},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Strategic integration of adaptive sampling and ensemble techniques in federated learning for aircraft engine remaining useful life prediction},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Railway prioritized food logistics in developing countries using fuzzy decision making under interval-valued pythagorean fuzzy environment. <em>ASOC</em>, <em>175</em>, 113066. (<a href='https://doi.org/10.1016/j.asoc.2025.113066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current state of agricultural logistics is vulnerable to global crises and oil price fluctuations, especially in developing countries that depend heavily on highway transportation. Experts are seeking efficient and eco-friendly solutions, exploring options such as railroad transport and innovative concepts such as synchromodality for improvement. In this study, a decision-making approach for policymakers and logistics experts to improve the efficiency and resilience of agricultural logistics by using more sustainable transport modes and synchromodality is proposed. The approach is based on a new total ordering principle on the class of Interval-Valued Pythagorean Fuzzy Numbers (IVPFNs), which is compared with existing ranking methods. In this paper, we have used IVPFNs for modelling our problem. The idea of IVPFNs (generalising interval-valued intuitionistic fuzzy numbers) introduced by Yager in 2013. However, the total ordering of the class of IVPFNs has not been studied so far. The main Mathematical contribution of this work lies in defining the total order relation on the set of IVPFNs for the first time in the literature. To do this, firstly, the Four new score functions on the set of IVPFNs are introduced and various mathematical properties of them are studied. Secondly, a new total ordering principle is introduced by combining all these score functions, and their mathematical proofs are given. Thirdly, a new group decision-making algorithm based on interval-valued Pythagorean fuzzy extent analysis (IVPFEA) is proposed and applied to a real-life case study problem. Finally, the sensitivity analysis has been done properly to show the robustness of the proposed algorithm and the results. The case study involves seven experts role-playing as advisors for the Republic of Türkiye, which is a developing country, on choosing the best agricultural logistics system alternative among four alternatives. Twelve criteria, under four aspects, are presented for participants to consider. Based on the responses of the experts, the railway-prioritized food logistics system is the primary alternative. Overall, the results of this study provide a mathematical and data-driven approach to deciding on a new logistics system that policymakers and sector experts can utilize.},
  archive      = {J_ASOC},
  author       = {Ali Atilla Arisoy and S. Jeevaraj and Ilgin Gokasar and Muhammet Deveci and Seifedine Kadry and Zhe Liu},
  doi          = {10.1016/j.asoc.2025.113066},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113066},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Railway prioritized food logistics in developing countries using fuzzy decision making under interval-valued pythagorean fuzzy environment},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). USVDD: A one-class outlier detection method with imprecisely- observed data. <em>ASOC</em>, <em>175</em>, 113065. (<a href='https://doi.org/10.1016/j.asoc.2025.113065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting outliers is essential to recognizing potential faults in high-value equipment. Since such equipment typically undergoes careful maintenance to prevent breakdowns, outlier detection often involves a one-class classification problem. Current advanced methods frequently face difficulties in managing the complexities of one-class observations, particularly when dealing with uncertain and imprecise monitoring data. This study proposes a novel approach to fault detection, called uncertain support vector data description (USVDD), in contexts involving one-class uncertain data. Drawing on uncertainty theory, USVDD conceptualizes imprecise observations as uncertain variables. By integrating novel hyperparameters, particularly belief degrees α k , the method effectively addresses and quantifies data uncertainty, enhancing its ability to handle complex, uncertain datasets. Testing on 12 real-world datasets highlights that the proposed USVDD method outperforms alternative methods by achieving the highest balanced F1-scores on 10 datasets. Additionally, it demonstrates remarkable efficiency, completing computations on high-dimensional datasets up to 60 times faster than competing methods on the same hardware setup. USVDD excels in managing observational data influenced by aleatory uncertainty, making it a reliable solution for one-class diagnostic modeling in situations with scarce fault samples.},
  archive      = {J_ASOC},
  author       = {Jingyu Liang and Jie Liu},
  doi          = {10.1016/j.asoc.2025.113065},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113065},
  shortjournal = {Appl. Soft. Comput.},
  title        = {USVDD: A one-class outlier detection method with imprecisely- observed data},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A radial visualization method based on knee point information for many-objective optimization. <em>ASOC</em>, <em>175</em>, 113064. (<a href='https://doi.org/10.1016/j.asoc.2025.113064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous high-dimensional solutions for many-objective optimization problems (MaOPs) usually impose a high cognitive burden on decision makers (DMs). Pareto front (PF) of MaOPs can express the problem characteristics, and then provide prior knowledge for solving the MaOPs. However, the existing high-dimensional visualization methods usually do not establish the relationship between PF information and decision making. Therefore, a novel radial visualization (RadViz) method called KRadViz that incorporates knee point information is proposed to visualize the information of PF shape and aid decision making. The relationship between the optimized performance information and PF shape is established, and the PF shape identification method is constructed. KRadViz is constructed by combining the optimization performance and PF shape. Three preferred solution selection methods are proposed to quickly screen out a few preferred solutions in different scenarios. The proposed KRadViz is compared with three high-dimensional visualization methods. The experimental results show that KRadViz can effectively display the high-dimensional PF shape, and give the optimization performance information of different solutions. The selection preferences of the three methods are also analyzed, and the effectiveness of the assisted decision process is verified. For the DTLZ2 and real-world MaOPs, the individual hypervolume (HV) contribution of preferred solutions increased by 9.98 % and 10.95 %, respectively.},
  archive      = {J_ASOC},
  author       = {Gui Li and Renbin Xiao},
  doi          = {10.1016/j.asoc.2025.113064},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113064},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A radial visualization method based on knee point information for many-objective optimization},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An automated brain tumor segmentation and classification using adaptive bayesian fuzzy clustering. <em>ASOC</em>, <em>175</em>, 113061. (<a href='https://doi.org/10.1016/j.asoc.2025.113061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An uncontrolled growth of malignant cells in the brain is known as a brain tumor. Rapid treatment response follows an early identification of tumors in the brain that increases the chance of patient survival. Adequate tumor classification and segmentation are necessary for treatment planning and best evaluation. It would be ideal and beneficial to have regular detection and identification. The design of medical imaging systems has been greatly influenced by the introduction of deep learning in recent years. Hence, an innovative brain tumor classification model is suggested in this work that resolves the drawbacks of traditional methods such as computational complexity, and low accuracy. At first, the necessary images are garnered from the online benchmark for the subsequent process. Further, the garnered images are given to the segmentation procedure, where an Adaptive Bayesian Fuzzy Clustering (ABFC) is utilized for segmenting the abnormalities. Moreover, an Improved Eurasian Oystercatcher Optimizer (IEOO) is adopted in the segmentation process for tuning the parameters in the ABFC technique, which increases the performance. The segmented images are subjected to the Multi-scale Residual Attention Network with Long Short Term Memory (MRAN-LSTM) layer for classifying the brain tumors. Finally, the simulations are done to verify the success rate of the implemented brain tumor segmentation and classification approach by contrasting it with traditional models.},
  archive      = {J_ASOC},
  author       = {Veesam Pavan Kumar and Satya Ranjan Pattanaik and V.V. Sunil Kumar},
  doi          = {10.1016/j.asoc.2025.113061},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113061},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An automated brain tumor segmentation and classification using adaptive bayesian fuzzy clustering},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extensible markup language web data retrieval using hybrid deep learning model. <em>ASOC</em>, <em>175</em>, 113060. (<a href='https://doi.org/10.1016/j.asoc.2025.113060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectives An Extensible Markup Language (XML) document is a kind of text file that utilizes the markup language to store data in an easily readable format for both machines and humans. In the data retrieval process, the client will send a request to the database or web server, which then retrieves the requested data and returns it in XML format. However, XML documents are complex and intricate to navigate, also it is difficult to identify and access the particular data attribute or element. Therefore, the primary objective of this research is to develop a novel Deep Learning (DL) model for XML web data retrieval. Methods This research proposed the Deep Fuzzy Clustering Bidirectional Gated Random Multimodel Deep Learning (DFC-BiGRMDL) for efficient data extraction and refinement. At first, the XML web document is collected from a specific database. Then, the user query and XML web data are employed to check similarity by using a new search algorithm that combines hybrid semantic similarity measures, which are combined using Mahalanobis distance and Clark similarity measures. It enables the identification of relevant data based on the user's queries. The Deep Fuzzy Clustering (DFC) model is used to group the retrieved XML web data. At last, refinement from retrieved XML web data is performed using the proposed DFC-BiGRMDL model, which is designed by the integration of Bidirectional Gated Recurrent Unit (BiGRU) and Random Multimodel Deep Learning (RMDL). Finding The performance of the proposed DFC-BiGRMDL model is evaluated using metrics, like Clark similarity, precision, recall and F-measure that have reached maximum values of 0.908, 91.360 %, 92.367 % and 91.860 % respectively. Improvement The developed model enhances the data extraction and refinement by combining advanced techniques like Clark similarity measures and Mahalanobis distance, and also, DFC for more accurate data retrieval. Furthermore, the hybridization of BiGRU and RMDL improves the learning from sequential data and ensures robust, flexible clustering. Moreover, the proposed model customizes data refinement according to user queries, providing a highly relevant solution. Moreover, its scalability and flexibility make it well-suited for various web data sources and large-scale datasets.},
  archive      = {J_ASOC},
  author       = {Gopianand M and R. Muthumeenakshi},
  doi          = {10.1016/j.asoc.2025.113060},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113060},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extensible markup language web data retrieval using hybrid deep learning model},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep neural aggregation for recommending items to group of users. <em>ASOC</em>, <em>175</em>, 113059. (<a href='https://doi.org/10.1016/j.asoc.2025.113059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern society dedicates a significant amount of time to digital interaction, as social life is more and more related to digital life, the information of groups’ interaction with the elements of the system is increasing. One key tool for the digital society is Recommender Systems, intelligent systems that learn from our past actions to propose new ones that align with our interests. Some of these systems have specialized in learning from the behavior of user groups to make recommendations to a group of individuals who want to perform a joint task. This research presents an innovative approach to representing group user preferences using deep learning techniques, enhancing recommendations for joint tasks. The proposed aggregation model has been evaluated using two different foundational models, GMF and MLP, four different datasets, and nine group sizes. The experimental results demonstrate the improvement achieved by employing the proposed aggregation model compared to the state-of-the-art, and this aggregation strategy can be applied to upcoming models and architectures.},
  archive      = {J_ASOC},
  author       = {Jorge Dueñas-Lerín and Raúl Lara-Cabrera and Fernando Ortega and Jesús Bobadilla},
  doi          = {10.1016/j.asoc.2025.113059},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113059},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep neural aggregation for recommending items to group of users},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A memetic method for solving portfolio optimization problem under cardinality, quantity, and pre-assignment constraints. <em>ASOC</em>, <em>175</em>, 113058. (<a href='https://doi.org/10.1016/j.asoc.2025.113058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial finance, portfolio selection has emerged as a critical challenge that has received considerable attention over the past few decades. The standard approach to this problem is the Markowitz mean–variance model, which seeks to balance two inherently conflicting objectives: maximizing returns and minimizing risk. This study investigates portfolio optimization under realistic constraints, including cardinality, quantity, and pre-allocation. To address these challenges, we propose a memetic algorithm specifically designed to solve constrained portfolio optimization problems. The performance of the algorithm was evaluated using benchmark datasets from major financial markets, including the Hang Seng, DAX 100, FTSE 100, S&P 100, NASDAQ, and Nikkei indices. A comparative analysis with the Non-dominated Sorting Genetic Algorithm II (NSGA-II) and Particle Swarm Optimization (PSO) demonstrated that the memetic algorithm consistently outperformed both NSGA-II and PSO in terms of execution time and the quality of efficient solutions. Across all tested markets, the memetic algorithm achieved superior risk/return ratios and faster computation times, confirming its effectiveness in solving complex portfolio optimization problems with real-world constraints.},
  archive      = {J_ASOC},
  author       = {Sara Regaigui and Madani Bezoui and Mustapha Moulai and Saeed Mian Qaisar},
  doi          = {10.1016/j.asoc.2025.113058},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113058},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A memetic method for solving portfolio optimization problem under cardinality, quantity, and pre-assignment constraints},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semi-supervised multi-source remote sensing image classification network based on adaptive pseudo-label generation. <em>ASOC</em>, <em>175</em>, 113055. (<a href='https://doi.org/10.1016/j.asoc.2025.113055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Earth observation technology leveraging remote sensing (RS) imagery serves as a valuable non-contact detection method with broad applications in classification research. Hyperspectral (HS) image classification, while effective in various domains, faces challenges due to the unique characteristics of HS data. Fusion diverse RS data sources can mitigate redundancy and enhance classification efficiency. However, many deep learning approaches for multi-source RS classification rely heavily on abundant labeled data, which can be time-consuming and often impractical. To address the limitations in feature extraction and classification accuracy stemming from the scarcity of labeled multi-source RS image samples in complex scenes, we propose a novel semi-supervised multi-source RS image classification network based on adaptive pseudo-label generation (S2CNet-APG). This framework incorporates attention modules that effectively embed active RS features into HS features, enhancing performance through squeezing and excitation (SE) driven attention mechanisms. Our semi-supervised learning approach employs adaptive thresholds to manage the quantity of pseudo-labels derived from unlabeled samples, while maintaining the spatial consistency of the information to ensure quality. This dual strategy effectively balances the quantity and quality of pseudo-labels, enabling accurate classification with limited labeled samples and transitioning multi-source RS image classification from a supervised to a semi-supervised paradigm. We conducted extensive experiments on three real-world multi-source RS datasets, achieving superior results that validate the efficacy of the proposed method.},
  archive      = {J_ASOC},
  author       = {Yining Feng and Lu Wang and Jiarui Jin and Xianghai Wang},
  doi          = {10.1016/j.asoc.2025.113055},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113055},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A semi-supervised multi-source remote sensing image classification network based on adaptive pseudo-label generation},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective cuckoo search algorithm using generalized lèvy flight and dissimilar egg identification for multispectral image thresholding. <em>ASOC</em>, <em>175</em>, 113054. (<a href='https://doi.org/10.1016/j.asoc.2025.113054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cuckoo Search (CS) stands as a highly efficient meta-heuristic optimization algorithm. Existing literature showcases the ability of CS in multi-objective scenarios, delineated by the three fundamental rules. However, the first rule of the algorithm incurs the cost of a generation to update a single nest, while the third rule necessitates hit-and-trial methods for parameter adjustment. To address these concerns, a multi-objective cuckoo search algorithm is proposed in this paper. The algorithm builds upon a generalized concept of Lèvy Flight for generating new solutions. Problem-specific, constraint-based strategies for identifying the best nest and dissimilar eggs are also introduced. The algorithm is further applied to solve multispectral remote sensing image thresholding problem. Prior studies have underscored the efficiency of entropy and clustering-based thresholding methods over other techniques. Nevertheless, most entropy-based approaches entail converting color images to grayscale before segmentation, potentially sacrificing crucial spectral information and consequently degrading segmentation algorithm’s performance. To avoid these limitations, this research introduces an entropy-based thresholding method to segment a color image without converting it to grayscale. The experiments are carried out using very high resolution (VHR) and coarse resolution (CR) multispectral (MS) images from the satellite sensors Pl’eidas-1B and Sentinel-2b, respectively. The proposed methods undergo validation against four state-of-the-art techniques on benchmark functions and six clustering indexes, respectively.},
  archive      = {J_ASOC},
  author       = {Ramen Pal and Pritam Roy and Srijon Mallick and Somnath Mukhopadhyay and Sunita Sarkar and Mike Hinchey},
  doi          = {10.1016/j.asoc.2025.113054},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113054},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective cuckoo search algorithm using generalized lèvy flight and dissimilar egg identification for multispectral image thresholding},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and fast subspace representation learning for multi-view subspace clustering. <em>ASOC</em>, <em>175</em>, 113050. (<a href='https://doi.org/10.1016/j.asoc.2025.113050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering (MVSC) plays an indispensable role in the domains of data mining and machine learning. Compared to single-view analysis, this integration of information leads to more accurate and comprehensive clustering results, providing a solution for large-scale data clustering. Notably, various techniques have been proposed in the field. In the present context, most multi-view clustering methods mainly focus on enhancing the consistency of clustering and handling noise. Adapting multi-view subspace clustering effectively for the clustering of big data poses a significant challenge. To overcome this challenge, we propose a new method called “robust and fast subspace representation learning for multi-view subspace clustering (RFSR)”, which utilizes a unified encoder to process information from each view and integrates the information between different views. In this process, we reduce the impact of noise, employing either correntropy or ℓ 2,1 -norm for handling it. Specifically, we start by randomly sampling from each view and then process the sampled data for noise. Subsequently, we train a unified encoder for each view to leverage complementary information from multiple views, thereby enhancing the robustness of clustering. We not only consider the multi-view data features but also account for its large scale and noise structure. Furthermore, we demonstrate through experiments the efficiency and robustness of our approach in multi-view subspace clustering.},
  archive      = {J_ASOC},
  author       = {Tailong Yu and Yesong Xu and Nan Yan and Mengyang Li},
  doi          = {10.1016/j.asoc.2025.113050},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113050},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust and fast subspace representation learning for multi-view subspace clustering},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cancelable binary biometric template generation scheme based on partial walsh transformation and MinHash algorithm. <em>ASOC</em>, <em>175</em>, 113049. (<a href='https://doi.org/10.1016/j.asoc.2025.113049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread use of biometrics, biometric templates stored in biometric systems are at serious risk of security and privacy breaches. Cancelable biometric scheme is an effective remedy when many unprotected biometric templates are compromised. We propose a cancelable binary biometric template generation scheme based on the partial Walsh transformation and the MinHash algorithm to improve recognition accuracy and generation efficiency. Firstly, the partial Walsh matrix transforms the high-dimensional original biometric feature into a low-dimensional space. Then, protected cancelable binary biometric templates are generated based on the proposed sliding window grouping minimum hash algorithm SWG-MinHash. Our scheme demonstrates superior recognition accuracy and generation efficiency on fingerprint and face databases compared to existing schemes. Meanwhile, our scheme satisfies the properties of non-invertibility, revocability, and unlinkability, and is resistant to common security and privacy attacks. Therefore, our scheme effectively mitigates the problem of balancing recognition accuracy, security, and generation efficiency of cancelable biometric schemes and is more practical for biometric systems. The source code of our scheme is available at https://github.com/sscwrx/cbef .},
  archive      = {J_ASOC},
  author       = {Shuaichao Song and Yeming Yang and Miao Yu and Yuming Liao and Weilai Guo and Jiyuan Li and Songhui Guo},
  doi          = {10.1016/j.asoc.2025.113049},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113049},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cancelable binary biometric template generation scheme based on partial walsh transformation and MinHash algorithm},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved multi-objective evolutionary algorithm for the low-carbon flexible job shop scheduling with automated guided vehicles. <em>ASOC</em>, <em>175</em>, 113048. (<a href='https://doi.org/10.1016/j.asoc.2025.113048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to growing demands for low-carbon and flexible manufacturing, this study addresses the Multi-Objective Flexible Job Shop Scheduling Problem with Automated Guided Vehicles (MFJSP-AGVs), optimizing makespan and Total Energy Consumption (TEC). We first develop a novel mixed-integer linear programming model using sequence-based modeling approach, rigorously validated by the Gurobi solver. The innovations of the proposed Improved Multi-Objective Evolutionary Algorithm (IMOEA) are that: 1) A three-layer encoding mechanism uniquely decouples machine selection, AGV assignment, and operation sequencing subproblems, overcoming traditional methods' limitations in handling interdependencies; 2) Ternary crossover and adaptive dual mutation operators synergistically enhance global exploration while maintaining solution diversity, addressing convergence-robustness trade-offs in existing algorithms; 3) A hybrid critical path optimization integrates Tabu Search for critical operations and energy-aware local search for non-critical paths, enabling simultaneous makespan-TEC optimization. The proposed IMOEA algorithm is applied to 55 test sets, and compared with the state-of-the-art four algorithms. Comprehensive experiments demonstrate IMOEA's superiority over EHA, EMOEA, and recent problem-specific algorithms, achieving 91.8 % and 227.6 % average improvements in IGD and HV respectively. These advancements establish a new benchmark for multi-objective AGV scheduling in energy-conscious production systems.},
  archive      = {J_ASOC},
  author       = {Wenlong Li and Huan Li and Yuyan Han and Yuting Wang},
  doi          = {10.1016/j.asoc.2025.113048},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113048},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved multi-objective evolutionary algorithm for the low-carbon flexible job shop scheduling with automated guided vehicles},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label feature selection via label relaxation. <em>ASOC</em>, <em>175</em>, 113047. (<a href='https://doi.org/10.1016/j.asoc.2025.113047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label feature selection (MFS) has emerged as a prevalent strategy to manage high-dimensional multi-label data. Most existing methods assume that the rigid binary label matrix can perfectly fit the pseudo-label matrix during the learning process, so as to preserve the structural information in raw data. However, the original label space with the limited freedom makes it challenging to accurately convert to the pseudo-label matrix. Additionally, most methods utilize different matrix to explore structural information, and ignore the connection of structural information. To tackle these problems, a novel method named multi-label feature selection via label relaxation (LRMFS) is proposed. LRMFS designs a label relaxation regression to transform the rigid binary label matrix into a slack variable matrix, allowing for a more flexible fitting relationship. By leveraging this flexible fitting, LRMFS decomposes the feature selection matrix to a structured subspace, which can learn the graph structures of both features and labels by graph Laplacian. These properties of LRMFS are converted to an objective function, and we further develop an alternative solution for the function optimization. Comparative experiments show that LRMFS exhibits superior performance than eight MFS methods on twelve multi-label data sets.},
  archive      = {J_ASOC},
  author       = {Yuling Fan and Peizhong Liu and Jinghua Liu},
  doi          = {10.1016/j.asoc.2025.113047},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113047},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-label feature selection via label relaxation},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-expression multi-label feature selection based on fuzzy decision. <em>ASOC</em>, <em>175</em>, 113046. (<a href='https://doi.org/10.1016/j.asoc.2025.113046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large amount of high-dimensional data poses a great challenge to multi-label learning. Feature selection is an effective method to alleviate this problem. However, many existing multi-label feature selection models either ignore the intrinsic spatial structure of samples or have no restrictions on the predicted label values. To solve the above problems, a sample self-representation multi-label feature selection method based on fuzzy decision is proposed in this paper. Firstly, a self-representation coefficient matrix of samples is proposed, which not only retains the original data structure information, but also reflects the distribution structure of data. Then, a fuzzy decision function is introduced to fuzzy prediction labels which well represents the membership of a sample to a class and is more consistent with the real label distribution. The L 2 , 1 -norm is imposed on the feature weight matrix to ensure sparsity and the F -norm is introduced into the self-expression matrix to weaken the effects of redundancy and anomalous samples. Finally, the gradient descent method is used to optimize the objective function. Experimental results on 12 multi-label datasets show that the proposed method performs better than other state-of-the-art multi-label feature selection methods, and obtain a significant increase in classification accuracy of about 2%–3% over all the compared approaches.},
  archive      = {J_ASOC},
  author       = {Shibing Pei and Minghao Chen and Changzhong Wang},
  doi          = {10.1016/j.asoc.2025.113046},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113046},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-expression multi-label feature selection based on fuzzy decision},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusing multi-granular-ball fuzzy information to detect outliers. <em>ASOC</em>, <em>175</em>, 113045. (<a href='https://doi.org/10.1016/j.asoc.2025.113045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection plays a critical role in data mining and machine learning, and its application value is widely recognized in several industries. However, despite the growing importance of outlier detection, many current outlier detection methods still rely on a single and fine-granularity processing paradigm. Not only does this paradigm lead to inefficient methods, but it also makes the methods vulnerable to noisy data. Furthermore, this processing paradigm ignores the potential multi-granularity information in the data, which may lead to an incomplete understanding of the intrinsic relations and patterns of the data. To further improve the performance of outlier detection, multi-granular-ball fuzzy information granules-based unsupervised outlier detection method (MGBOD) is proposed in this work. In our method, granular-balls with different granularity are first generated and the fuzzy binary relations between the granular-balls with respect to different attributes are computed. Subsequently, two attribute sequences are constructed based on the importance of the attributes. Then, multi-granular-ball fuzzy binary granular structures are constructed based on these two sequences. Finally, the outlier score of the granular-ball is defined by fusing these granules in the granular structures and mapped to the samples in the granular-ball. Experimental results show that, compared with recently proposed methods, our method demonstrates excellent outlier detection performance under a variety of public datasets. The code is publicly available at https://github.com/Mxeron/MGBOD .},
  archive      = {J_ASOC},
  author       = {Xinyu Su and Shitong Cheng and Dezhong Peng and Hongmei Chen and Zhong Yuan},
  doi          = {10.1016/j.asoc.2025.113045},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113045},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fusing multi-granular-ball fuzzy information to detect outliers},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-aware ensemble learning for face-periocular cross-modality matching. <em>ASOC</em>, <em>175</em>, 113044. (<a href='https://doi.org/10.1016/j.asoc.2025.113044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face and periocular regions serve as complementary biometric modalities in identity recognition. The face-periocular cross-modality matching (FPCM) provides a versatile solution, especially when traditional face recognition systems encounter challenges due to occlusions or the presence of sunglasses, which can obscure the periocular region, rendering it less effective in periocular recognition systems. This paper introduces a novel approach based on attention-aware ensemble learning (AEL) to address these challenges. This notion is embodied in AELNet, which features an attention-aware shared-parameter encoder and multiple classifier heads. AELNet is designed to harness the complementary features of the face and periocular regions, enhancing the quality of joint embeddings. A key aspect of AELNet is its ability to foster diversity among the classifier heads through unique embedding techniques and batch sampling strategies, ultimately boosting FPCM performance. We demonstrate the effectiveness of the AELNet by conducting extensive experiments on five unconstrained periocular-face datasets as a benchmark. Codes are publicly available at https://github.com/tiongsikng/ael_net .},
  archive      = {J_ASOC},
  author       = {Tiong-Sik Ng and Andrew Beng Jin Teoh},
  doi          = {10.1016/j.asoc.2025.113044},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113044},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-aware ensemble learning for face-periocular cross-modality matching},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for optimal firebreak placement in forest fire prevention. <em>ASOC</em>, <em>175</em>, 113043. (<a href='https://doi.org/10.1016/j.asoc.2025.113043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing frequency and intensity of large wildfires have become a significant natural hazard, requiring the development of advanced decision-support tools for resilient landscape design. Existing methods, such as Mixed Integer Programming and Stochastic Optimization, while effective, are computationally demanding. In this study, we introduce a novel Deep Reinforcement Learning (DRL) methodology to optimize the strategic placement of firebreaks across diverse landscapes. We employ Deep Q-Learning, Double Deep Q-Learning, and Dueling Double Deep Q-Learning, integrated with the Cell2Fire fire spread simulator and Convolutional Neural Networks. Our DRL agent successfully learns optimal firebreak locations, demonstrating superior performance compared to heuristics, especially after incorporating a pre-training loop. This improvement ranges between 1.59%–1.7% with respect to the heuristic, depending on the size of the instance, and 4.79%–6.81% when compared to a random solution. Our results highlight the potential of DRL for fire prevention, showing convergence with favorable results in cases as large as 40 × 40 cells. This study represents a pioneering application of reinforcement learning to fire prevention and landscape management.},
  archive      = {J_ASOC},
  author       = {Lucas Murray and Tatiana Castillo and Isaac Martín de Diego and Richard Weber and José Ramón González-Olabarria and Jordi García-Gonzalo and Andrés Weintraub and Jaime Carrasco-Barra},
  doi          = {10.1016/j.asoc.2025.113043},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113043},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning for optimal firebreak placement in forest fire prevention},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariance matrix adaptation driven dynamic multi-population colony predation optimizer: Insights, qualitative analysis, and constrained engineering optimization. <em>ASOC</em>, <em>175</em>, 113041. (<a href='https://doi.org/10.1016/j.asoc.2025.113041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Colony Predation Algorithm (CPA) is a straightforward population-based algorithm with few control parameters. Nevertheless, its initial design has limitations, including a tendency for local optimization and limited search ability, leading to subpar solutions. In response to these concerns, a novel approach named the Covariance Matrix Adaptive-Driven Dynamic Multi-Population Colony Predation Algorithm (ICPA) is introduced. Qualitative analysis experiments are carried out to determine ICPA's feasibility, including historical search trajectory analyses and balanced diversity assessments. Additionally, a comparative study involving 11 well-known algorithms, 11 state-of-the-art algorithms, and four champion algorithms (EBOwithCMAR, SPS_L_SHADE_EIG, LSHADE_cnEpSi, and LSHADE) using the IEEE CEC 2014 test suite confirmed its superior optimization capabilities. Statistical tests consistently rank ICPA first on the Friedman test (with scores of 2.17, 3.37, and 2.27) and demonstrate its outperformance of state-of-the-art algorithms in at least 40 % of tested functions in the Wilcoxon sign-rank test. The Bonferroni Dunn post-hoc statistical test reveals that ICPA significantly outperforms 61.53 % of the compared algorithms. Additionally, the efficacy of ICPA was evaluated on seven constrained real-world engineering problems, encompassing gear train design, speed reducer design, multi-disc clutch and brake design, cantilever beam design, three-bar truss design, I-beam design, and combined economic emission dispatch. The experimental outcomes underscore the potential of ICPA in addressing practical engineering challenges, thereby validating its optimization effectiveness. Utilizing extensive experimentation and comparative analyses, the feasibility, superiority, and effectiveness of ICPA have been substantiated, encompassing both qualitative and quantitative perspectives.},
  archive      = {J_ASOC},
  author       = {Xinsen Zhou and Jie Xing and Wenyong Gui and Ali Asghar Heidari and Zhennao Cai and Huiling Chen and Guoxi Liang},
  doi          = {10.1016/j.asoc.2025.113041},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113041},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Covariance matrix adaptation driven dynamic multi-population colony predation optimizer: Insights, qualitative analysis, and constrained engineering optimization},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secured human-centric sustainable healthcare in industry 5.0 through artificial neural synchronization-guided blockchain and vertical federated learning. <em>ASOC</em>, <em>175</em>, 113040. (<a href='https://doi.org/10.1016/j.asoc.2025.113040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a medical recommendation system that utilizes vertical Federated Learning (FL) to securely access real-time patient and healthcare data to provide personalized treatment recommendations. It utilizes insights from federated servers through sentiment and emotion analysis. The incorporation of blockchain technology into FL addresses concerns related to ownership and privacy by utilizing neural synchronization and hyperledger fabric to safeguard data. Data providers encrypt instances to improve dependability and store them on the blockchain using structured transactions. This solution combines these technologies with professional medical knowledge to cooperatively train Machine Learning (ML) models on dispersed healthcare data. Implementing a knowledge-based recommendation system improves the accuracy and interpretability of disease prediction models. Healthcare professionals can use various patient data while protecting privacy and using collective medical expertise. It builds a blockchain key using Artificial Neural Networks (ANNs) that are aligned neurally in opposite directions. This assures the confidentiality, transparency, and traceability of medical information. The system uses Bagging, Gradient Boost, Light-GBM, N-Gram, Random Forest, Gaussian Naive Bayes, and Federated Deep Learning to improve disease prediction while maintaining raw data privacy through collaborative learning. Evaluation metrics show increased privacy, security, and personalized medical advice, with a focus on safe storage, tailored treatment plans, individualized therapy recommendations, blockchain-based Electronic Health Record (EHR) storage, and performance monitoring. Hyperledger fabric tracks EHR changes on a cloud platform. A comprehensive assessment reveals that this technique has the potential for success in real-world healthcare.},
  archive      = {J_ASOC},
  author       = {Arindam Sarkar},
  doi          = {10.1016/j.asoc.2025.113040},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113040},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Secured human-centric sustainable healthcare in industry 5.0 through artificial neural synchronization-guided blockchain and vertical federated learning},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arithmetic optimization algorithm with cosine composite chaotic mapping in polar coordinate system for economic load dispatching problems in power systems. <em>ASOC</em>, <em>175</em>, 113039. (<a href='https://doi.org/10.1016/j.asoc.2025.113039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Economic load dispatch (ELD) aims to minimize the total cost of generating electricity while satisfying load demand and different operational constraints. The Arithmetic Optimization Algorithm (AOA) with cosine composite chaotic mapping in polar coordinate system is put forward to solve the ELD problems in the power system with the valve point effect, prohibited operation area, transmission loss and other factors. Firstly, seven polar coordinate system chaotic mappings are proposed to be embedded into the MOP and MOA parameters in the AOA. Secondly, a chaotic system based on the cosine transform is put forward. Then, the proposed cosine transform based chaotic system is combined with polar coordinate system chaotic mapping to form polar coordinate system cosine transform composite chaotic mapping. Eventually, these six polar coordinate system cosine transform composite chaotic mapping is then embedded into the MOA and MOP of the AOA to balance the algorithm's global and local search capabilities, improve the performance of the algorithm and avoid falling into the local optima. The superiority of the improved algorithm is verified by employing 12 benchmark test functions in CEC2022. Then, it is compared with the Coati Optimization Algorithm (COA), Prairie Dog Optimization (PDO), Butterfly Optimization Algorithm (BOA), Reptile Search Algorithm (RSA), Bat Algorithm (BAT) and Osprey Optimization Algorithm (OOA) to verify its convergence. The ELD problem for a total demand of 2500 MW is solved by using the AOA with cosine composite chaotic mapping in polar coordinate system. The experimental results show that the improved AOA outperforms other optimization algorithms on the 12 benchmark functions in CEC2022 and the ELD problems.},
  archive      = {J_ASOC},
  author       = {Yi-Xuan Li and Jie-Sheng Wang and Si-Wen Zhang and Shi-Hui Zhang and Xin-Yi Guan and Xin-Ru Ma},
  doi          = {10.1016/j.asoc.2025.113039},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113039},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Arithmetic optimization algorithm with cosine composite chaotic mapping in polar coordinate system for economic load dispatching problems in power systems},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic q-rung orthopair hesitant fuzzy decision-making model based on banzhaf value of fuzzy measure. <em>ASOC</em>, <em>175</em>, 113036. (<a href='https://doi.org/10.1016/j.asoc.2025.113036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-attribute decision-making in dynamic fuzzy environments holds significant practical importance in fields. The changing fuzzy environmental factors and the hesitant psychology during the dynamic period commonly involve potential historical impacts of decisions and dynamic fuzzy intercorrelation. Quantitative indicators of attribute importance derived from fuzzy measures can account for intercorrelation but cannot ensure the dynamic feedback of historical decisions. For these reasons, the existing fuzzy decision-making methods have failure and poor stability when dealing with complex, hesitant dynamic fuzzy decision-making problems. Therefore, this paper aims to construct a dynamic decision-making model within the q-rung orthopair hesitant fuzzy(q-ROHF) environment. First, this paper derives the q-ROHF cross-entropy and four combinations of entropy formulas to derive the calculation method for the fuzzy measure of dynamic attributes. The Banzhaf value is used as a weighting metric to quantify the degree of impact of dynamic attributes to the decision result. Second, based on the Banzhaf value, a dynamic weighting algorithm incorporating a historical feedback mechanism and dynamic intercorrelation is proposed. Further, this paper derives a fuzzy preference relation(FPR) using the q-ROHF generalized fuzzy distance and presents an algorithm for updating the dynamic FPR matrix. Finally, a dynamic FPR-based AQM method is used to construct the dynamic decision-making model. The feasibility and effectiveness of the decision-making model in response to changing complex factors are demonstrated through two distinct real-world cases. Through robustness analysis and advantage analysis, it has been verified that the model exhibits superior dynamic decision-making stability and hesitant decision-making stability compared to existing models. The model can be a powerful tool for dealing with dynamic decision-making problems that involve hesitation and uncertainty.},
  archive      = {J_ASOC},
  author       = {Yibo Wang and Xiuqin Ma and Hongwu Qin and Yuanyuan Chen and Jemal H. Abawajy},
  doi          = {10.1016/j.asoc.2025.113036},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113036},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic q-rung orthopair hesitant fuzzy decision-making model based on banzhaf value of fuzzy measure},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-stage feature fusion network for polyp segmentation. <em>ASOC</em>, <em>175</em>, 113034. (<a href='https://doi.org/10.1016/j.asoc.2025.113034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rising incidence and mortality of colorectal cancer, automatic polyp segmentation has gained significant attention. To address the limitations of existing pyramid-based transformer methods in polyp segmentation, specifically their challenges with feature scale diversity and feature fusion, we propose a transformer-based multi-stage feature fusion network (MSFFNet). First, the Contextual Dilation Fusion (CDF) module fuses adjacent multi-layer features and extracts multi-receptive field features, improving adaptability to polyps of different scales and enhancing feature diversity. Second, the Attention-Driven Feature Enhancement (AFE) module suppresses irrelevant background information and strengthens feature representation. Finally, the Dual-path Feature Fusion (DPF) module effectively integrates multi-level features using concatenation and point-wise addition. Extensive experiments on five datasets using four metrics demonstrate the effectiveness and strong generalization ability of the proposed method.},
  archive      = {J_ASOC},
  author       = {Guangzu Lv and Bin Wang and Cunlu Xu and Weiping Ding and Jun Liu},
  doi          = {10.1016/j.asoc.2025.113034},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113034},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-stage feature fusion network for polyp segmentation},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Categorized attention based hierarchical-agents reinforcement learning for multi-objective dynamic job shop scheduling problem with machine deterioration. <em>ASOC</em>, <em>175</em>, 113032. (<a href='https://doi.org/10.1016/j.asoc.2025.113032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern production activities, machines are often affected by the effect of deterioration, which interferes with production scheduling. Therefore, it is necessary to understand the status information of each machine and job individual in real time, so as to accurately maintain the specific machine with serious deterioration and arrange reasonable job processing to prevent greater losses. To handle this issue, this paper proposes a categorized attention based hierarchical-agents reinforcement learning framework for multi-objective dynamic job shop scheduling problem with machine deterioration. A multi-channel matrix state representation approach is designed to provide detailed information about the current processing status of each machine and job. The categorized attention mechanism is proposed to extract the state information of the machine and job respectively. Finally, the effectiveness of the proposed method is proved by numerical experiments and the robustness is also verified by scheduling experiments in dynamic environments.},
  archive      = {J_ASOC},
  author       = {Yibing Li and Xueci Liang and Jun Guo and Xixing Li and Lei Wang and Baigang Du},
  doi          = {10.1016/j.asoc.2025.113032},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113032},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Categorized attention based hierarchical-agents reinforcement learning for multi-objective dynamic job shop scheduling problem with machine deterioration},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Historical oracle bone character recognition through domain transfer and mutual learning. <em>ASOC</em>, <em>175</em>, 113031. (<a href='https://doi.org/10.1016/j.asoc.2025.113031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oracle bone characters (OBCs) provide crucial primary evidence for studying the history of the Shang Dynasty and the evolution of Chinese characters. However, the difficulty of collecting and annotating authentic OBC images, the intra-class difference, and inter-class similarity pose significant challenges for OBC recognition. In this paper, we propose a domain transfer and mutual learning network (DTML) for cross-domain OBC recognition, which leverages OBCs handprinted and annotated by experts to recognize unlabeled authentic OBCs. The domain transfer strategy shifts authentic OBC samples to match the distribution of handprinted OBC samples and vice versa, preserving the domain-specific features of both handprinted and authentic OBCs while implicitly decreasing the distributional differences between the two domains. The mutual learning strategy leverages high-confidence samples from two distinct domain distributions to guide the training of each other’s domain models, facilitating the transfer of domain-specific knowledge between the two domains. Comprehensive testing shows that our approach establishes a new benchmark on the Oracle-241 dataset, surpassing the latest state-of-the-art recognition accuracy by 10.0%. Our code is temporarily available at https://github.com/ycfang-lab/DTML .},
  archive      = {J_ASOC},
  author       = {Zhengchen Li and Xiuan Wan and Jiahua Wu and Shouyong Pan and Yuchun Fang},
  doi          = {10.1016/j.asoc.2025.113031},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113031},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Historical oracle bone character recognition through domain transfer and mutual learning},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FT-GPNN: A finite-time convergence solution for multi-set constrained optimization. <em>ASOC</em>, <em>175</em>, 113030. (<a href='https://doi.org/10.1016/j.asoc.2025.113030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient Neural Networks (GNNs) have demonstrated remarkable progress in handling optimization problems. However, applying GNNs to multi-constrained optimization problems, particularly those with those involving multi-set constraints, poses several challenges. These challenges arise from the complexity of the derivations and the increasing number of constraints. As the number of constraints increases, the optimization problem becomes more complex, making it more challenging for GNN-based methods to effectively identify the optimal solution. Motivated by these challenges, the Finite-Time Gradient Projection Neural Network (FT-GPNN) is introduced for tackling Multi-set Constrained Optimization (MCO). This innovative solution incorporates an Enhanced Sign-Bi-Power (ESBP) activation function and simplifies the design tailored explicitly for MCO. Furthermore, within the Lyapunov stability framework, the theoretical foundation of this model is strengthened by rigorous proof of local convergence. Building upon this foundation, we further establish that our model can achieve convergence within a finite time. To validate the effectiveness of our approach, we present empirical results from numerical experiments conducted under consistent conditions. Notably, our experiments demonstrate that the model using the ESBP activation function outperforms others in terms of finite-time convergence.},
  archive      = {J_ASOC},
  author       = {Huiting He and Chengze Jiang and Zhiyuan Song and Xiuchun Xiao and Neal Xiong},
  doi          = {10.1016/j.asoc.2025.113030},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113030},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FT-GPNN: A finite-time convergence solution for multi-set constrained optimization},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing a cryptocurrency trading system with deep reinforcement learning utilizing LSTM neural networks and XGBoost feature selection. <em>ASOC</em>, <em>175</em>, 113029. (<a href='https://doi.org/10.1016/j.asoc.2025.113029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to present a cryptocurrency trading strategy that addresses market volatility and decision-making challenges using advanced machine learning techniques and a wide range of predictor variables. Specifically, the proposed method is designed to enhance trading decisions by improving the accuracy of market trend forecasts. The approach consists of two primary steps. First, the XGBoost algorithm is applied to identify the most relevant features from market variables, technical indicators, macroeconomic factors, and blockchain-specific data for each cryptocurrency. In the second step, these selected features are fed into a Double Deep Q-Network (DDQN) algorithm incorporating LSTM (Long Short-Term Memory), BiLSTM (Bidirectional Long Short-Term Memory), and GRU (Gated recurrent units) layers to generate trading signals (buy, hold, sell). The model’s performance, tested on Bitcoin and Ethereum data from July 2021 to March 2023, demonstrates that blockchain variables provide crucial insights for trading strategies. Furthermore, combining XGBoost for feature selection with the DDQN model improves all key trading performance metrics, highlighting the significance of feature selection in optimizing deep reinforcement learning agents.},
  archive      = {J_ASOC},
  author       = {Hamidreza Ghadiri and Ehsan Hajizadeh},
  doi          = {10.1016/j.asoc.2025.113029},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113029},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Designing a cryptocurrency trading system with deep reinforcement learning utilizing LSTM neural networks and XGBoost feature selection},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection via label enhancement and neighborhood rough set for multi-label data with unbalanced distribution. <em>ASOC</em>, <em>175</em>, 113028. (<a href='https://doi.org/10.1016/j.asoc.2025.113028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning has gained significant attention in classification tasks, but challenges remain in handling high-dimensional data. Although feature selection techniques can alleviate these issues, neglecting the unbalanced data distribution problem severely undermines the models’ accuracy. Furthermore, existing methods fail to account for the importance and correlation of labels. In this paper, we present a novel multi-label feature selection algorithm that addresses these issues through three innovations: (1) using k -nearest neighbors to capture local similarities in unbalanced data, (2) enhancing labels by converting them into distributions to enrich semantic information, and (3) introducing a new evaluation function to assess label correlations. A multi-criteria strategy is established to maximize feature-label relevance, minimize redundancy, and strengthen label correlations. Experimental results on fifteen multi-label datasets demonstrate the algorithm’s superiority over five state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Wenbin Qian and Wenyong Ruan and Xiwen Lu and Wenji Yang and Jintao Huang},
  doi          = {10.1016/j.asoc.2025.113028},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113028},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection via label enhancement and neighborhood rough set for multi-label data with unbalanced distribution},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint hierarchical multi-granularity adaptive embedding discriminative learning for unsupervised domain adaptation. <em>ASOC</em>, <em>175</em>, 113026. (<a href='https://doi.org/10.1016/j.asoc.2025.113026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) is an effective technique that aims to transfer knowledge from well-labeled source data to target data that lacks labels and has a different distribution. Most existing methods only considered domain center-wise alignment to reduce global differences across domains, resulting in a coarse alignment. In recent years, researchers further considered aligning class centers to ensure the consistency of local distributions. However, these methods utilized a solely mean vector to represent the entire class distribution, which is still coarse and cannot fully capture the distribution characteristics of intra-class data. Inspired by the “knowledge pyramid” theory, a novel UDA method termed adaptive hierarchical multi-granularity embedded learning (HMGEL) is proposed to solve this problem, which aims to minimize the distribution gap of samples across domains from the perspective of hierarchical multi-granularity. This method can reflect the distribution of samples from coarse to fine, which is helpful for better UDA. Firstly, granular envelopes are created to explore intra-class structures and complex distributional properties at a more fine-grained level. Based on the granular envelopes, domain centers and class centers are combined for cross-domain distribution alignment, allowing for the capture of sample information at hierarchical multi-granularity from coarse to fine. Then, a robust sample-to-granular envelope cross-domain local structure learning strategy is designed to improve the discrimination capability of target domain features under hierarchical multi-granularity. Extensive experiments on five benchmark datasets show that the proposed HMGEL method is effective at a significant level.},
  archive      = {J_ASOC},
  author       = {Pufei Li and Pin Wang and Yongming Li and Yinghua Shen and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2025.113026},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113026},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint hierarchical multi-granularity adaptive embedding discriminative learning for unsupervised domain adaptation},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Projection-based comprehensive multi-view clustering with smooth regularization. <em>ASOC</em>, <em>175</em>, 113025. (<a href='https://doi.org/10.1016/j.asoc.2025.113025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By mining latent representations of data, subspace clustering methods can be more accurate and robust. However, such methods face the following limitations: they lack the ability to explicitly preserve the relationships between individuals within the original data cluster when constructing low dimensional latent representations. Specifically, during data dimensionality reduction, it cannot ensure that the retained information is relevant information between samples. Secondly, data dimensionality reduction may lead to the loss of key information within some samples, affecting the construction of self-representation matrices and reducing clustering performance. To solve these two problems, a new multi-view clustering method is proposed, Projection-based comprehensive multi-view clustering with smooth regularization (PCMCS). We design a smooth regular term for the projection matrix, which can make the data after dimensionality-reduced retain the grouping effect of the original data. Then, we capture the representation matrix of the original data and the data after dimensionality-reduced, and construct the resulting representation matrix as a tensor such that the two self-representation matrices are optimized with respect to each other, which to a certain extent neutralizes the interference of information loss, data redundancy, and noise in the construction of the representation matrices and improves the clustering performance. Experiments are conducted on 8 datasets, demonstrating the effectiveness of PCMCS.},
  archive      = {J_ASOC},
  author       = {Xiaoqian Zhang and Jinghao Li and Shuai Zhao and Yilu Zheng and Lei Pu and Yufeng Chen},
  doi          = {10.1016/j.asoc.2025.113025},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113025},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Projection-based comprehensive multi-view clustering with smooth regularization},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intra and inter-series pattern representations fusion network for multiple time series forecasting. <em>ASOC</em>, <em>175</em>, 113024. (<a href='https://doi.org/10.1016/j.asoc.2025.113024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple time series (MTS) can comprise data collected from various wireless sensor networks in the actual application, and each source provides a distinctive pattern. Most existing neural network methods attempt to model the patterns of individual time series by training a global model using the entire dataset, suffering from insufficient ability to consider the differences among source patterns and lowering the predictability. To address this limitation, we propose the Multiple Time Series Pattern Representation Network(MTS-PRNet), a unified framework consisting of two modules to forecast multiple time series from diverse sources. The first is the intra-series correlation learning module, which explicitly learns the temporal dependencies of time series. The second is the inter-series discriminative representation learning module that learns shapelets as discriminative representations to capture shared features among series. By integrating the covariates map generated by the second module, both intra and inter-series characteristics are captured to provide transferable guidance for increasing predictability. Experiments conducted on 9 datasets verify that our model achieves state-of-the-art performance. In particular, we carry out an ablation study to validate the effectiveness of discriminative representations.},
  archive      = {J_ASOC},
  author       = {Canghong Jin and Tianyi Chen and Hao Ni and Qihao Shi},
  doi          = {10.1016/j.asoc.2025.113024},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113024},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intra and inter-series pattern representations fusion network for multiple time series forecasting},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian graph structure inference neural network based on adaptive connection sampling. <em>ASOC</em>, <em>175</em>, 113018. (<a href='https://doi.org/10.1016/j.asoc.2025.113018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have drawn a lot of interest recently and excel in several areas, including node categorization, recommended systems, link prediction, etc. However, most GNNs by default observe graphs that accurately reflect the relationships between nodes. The feature aggregation of GNN is done by aggregating the neighbor nodes of the node. Therefore, observation graphs are not always compatible with the properties of GNNs. Unlike random regularization techniques that employ constant sampling rates or manually tune them as model hyperparameters. This study proposes a graph-structure learning network based on adaptive connection sampling. The core idea is to use the features generated by each layer of GNNs through adaptive sampling to generate a graph through the Bayesian method and realize the joint optimization of graph structure and adaptive connection sampling through iteration. This study conducts experiments on the data set to evaluate the effectiveness of this method. In the node classification task, the model improves performance by about 3.8% compared to the average of many baselines. It can be seen that learning graph structures is effective and inferring graphs is logical.},
  archive      = {J_ASOC},
  author       = {Mingjie Lu and Zhaowei Liu and Pengda Wang and Haiyang Wang and Dong Yang},
  doi          = {10.1016/j.asoc.2025.113018},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113018},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bayesian graph structure inference neural network based on adaptive connection sampling},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting realized volatility using deep learning quantile function. <em>ASOC</em>, <em>175</em>, 113016. (<a href='https://doi.org/10.1016/j.asoc.2025.113016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of realized volatility is an essential component of effective investment strategies. Existing studies have often focused on modeling selective features of intraday return series, overlooking the comprehensive information embedded within them due to challenges such as microstructure noise and the complexity of handling numerous data points. To address these limitations, this paper proposes a novel deep learning quantile function (DLQF) framework that directly leverages intraday return series to forecast realized volatility. The proposed model integrates a Bi-LSTM network to capture the long memory of realized volatility and a quantile function implemented as a deep neural network to extract rich information from intraday returns. A loss function based on L p distance measures is defined to estimate the probabilistic distribution of intraday returns, enabling both intraday return prediction and realized volatility estimation. Empirical results demonstrate that DLQF outperforms traditional benchmarks across major ETFs, including SPY, DIA, and QQQ, which represent the S&P 500, Dow Jones Industrial Average, and Nasdaq 100, respectively. This model offers significant potential for applications in portfolio optimization, option pricing, and risk management.},
  archive      = {J_ASOC},
  author       = {Jungyoon Song and Hyunju Lee and Jongu Lee and Woojin Chang},
  doi          = {10.1016/j.asoc.2025.113016},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113016},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting realized volatility using deep learning quantile function},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cascade kronecker neuro–fuzzy network based influential node identification. <em>ASOC</em>, <em>175</em>, 113015. (<a href='https://doi.org/10.1016/j.asoc.2025.113015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of information technology has led to increasingly complex networks, making the spread of rumors and diseases more challenging to manage. Identifying influential nodes efficiently and accurately is crucial for predicting and controlling network dynamics. Existing methods often fail to account for the impact of edges or rely on global network configurations, leading to inaccuracies and high computational costs. Here, Cascade Kronecker Neuro-Fuzzy Network (KNFN) is introduced for influential node identification. At first, initial influential node identification is accomplished based on chord distance based betweenness, Betweenness Centrality (BC), closeness of node and Degree Centrality (DC). Then, community creation is carried out utilizing chord distance. In addition, features for the community are considered and based on considered features community detection is done. The detection of community is executed employing Cascade KNFN, which is an assimilation of the Deep Kronecker Network (DKN) with Cascade Neuro-Fuzzy Network (NFN). Lastly, the right community to merge is identified by exploiting chord distance. The evaluation is conducted based on chord distance based betweenness and communication cost and the Cascade KNFN acquired chord distance based betweenness of 0.624 and communication cost of 1.046. This study provides a robust framework for managing complex networks and improving prediction and control in dynamic systems.},
  archive      = {J_ASOC},
  author       = {Koduru Hajarathaiah and Chandra Sekhar Kolli and Subba Reddy Tatireddy and M.P.J. Santosh Kumar and Vijaya Kumar Reddy Radha and Vadisena Venkata Krishna Reddy},
  doi          = {10.1016/j.asoc.2025.113015},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113015},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cascade kronecker neuro–fuzzy network based influential node identification},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A light spatial-frequency network for robust iris segmentation and localization. <em>ASOC</em>, <em>175</em>, 113009. (<a href='https://doi.org/10.1016/j.asoc.2025.113009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris recognition is a high-accuracy biometric technology. However, applying iris recognition in mobile devices still faces significant challenges, such as limited computational ability and uncontrolled image quality. Occlusion, motion blur, glass reflections, overexposure, and other factors increase the difficulties of iris segmentation and localization. To address these problems, we propose a light spatial-frequency network for robust iris segmentation and localization. The Filter Meet Convolution Former module divides the channel into two parts. One part learns detailed information using convolution in the spatial domain, the other part learns the semantic information utilizing the global filter in the frequency domain. The Boundary Enhancement module is designed to improve the boundary prediction, which separates the boundary features and employs the boundary features to guide the network to focus on the boundary part. The experiments were carried out on six datasets. The experimental results indicated that the model outperforms state-of-the-art methods with only 0.38M parameters and 5.57 GFLOPs. The model is robust and maintains stable performance even with noisy data. The code and experimental details will be shared at https://github.com/JINULEMON/SFNet/tree/master .},
  archive      = {J_ASOC},
  author       = {Qi Wang and Chun Xia and Yue Yan and Rui Zhang and Yang Liu},
  doi          = {10.1016/j.asoc.2025.113009},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113009},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A light spatial-frequency network for robust iris segmentation and localization},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view framework for multi-label bioactive peptide classification based on multi-modal representation learning. <em>ASOC</em>, <em>175</em>, 113007. (<a href='https://doi.org/10.1016/j.asoc.2025.113007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diversity and specific biological functions of bioactive peptides make them key regulators in various physiological processes and crucial contributors to the development of new anti-infective drugs. Although existing graph-based deep learning methods effectively model multi-label peptide representation, they often fail to incorporate multi-modal feature representation and extract multi-scale features from various views. To address these limitations, we present a multi-view framework for multi-label bioactive peptide classification based on multi-modal representation Learning by combining amino acid sequences and fusion molecular fingerprints. The peptide molecular graph is constructed by extracting the topological information and node features, respectively. Multi-view branches are designed by developing sequence-based and graph-based models to leverage their distinct and complementary strengths. Specifically, the protein language model ESM-2 is utilized to extract residue features from amino acid sequences deeply. Meanwhile, local features from molecular fingerprints are learned through a Filter Response Normalization layer and a Thresholded Linear Unit. At the same time, the Mamba module is innovatively employed to extract long-range dependencies and reduce time complexity. Our model demonstrates significantly enhanced and robust performance in multi-label bioactive peptide prediction tasks compared with state-of-the-art models, achieving 82.5% coverage, 80.9% precision and 80.3% accuracy on the MFBP dataset. Furthermore, visual analyses demonstrate that the model can effectively capture features from multiple views and highlight the interpretability of the model through the decision process.},
  archive      = {J_ASOC},
  author       = {Yan Kang and Yue Peng and Dongsheng Zheng and Huadong Zhang and Xuekun Yang},
  doi          = {10.1016/j.asoc.2025.113007},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {113007},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-view framework for multi-label bioactive peptide classification based on multi-modal representation learning},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin-based pursuit-evasion gaming strategy optimization for underwater robot grasping. <em>ASOC</em>, <em>175</em>, 112993. (<a href='https://doi.org/10.1016/j.asoc.2025.112993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater robotic grasping challenges are essential for the advancement of underwater robotics and oceanic development. To tackle the difficulties encountered by these robots in grasping, we present an innovative multi-agent learning framework based on a pursuit-evasion game. This framework consists of three phases: initial learning, interactive learning, and independent learning, enabling a gradually enhanced learning experience. We propose a robot pursuit approach utilizing Improved Grey Wolf Optimization (IGWO) and implement the Soft Actor-Critic for learning target evasion strategies. The IGWO augments search and sample methodologies, markedly enhancing search efficacy relative to the conventional Grey Wolf Optimization. Furthermore, we have created virtual reality software for underwater robots and implemented a related digital twin system platform, facilitating the training and education of pursuers and evaders in a simulated environment. Ultimately, we implement this system in a practical underwater pursuit-evasion scenario. Through interactive training and iterative learning, the robotic arm exhibits the capability to strategically pursue an evasive target, while the target demonstrates adaptable escape. Both modeling and experimental results produce excellent outcomes, offering innovative approaches and insights for the dynamic grasping domain of underwater robotics.},
  archive      = {J_ASOC},
  author       = {Xubo Yang and Jian Gao and Peng Wang and Siqing Sun and Yufeng Li},
  doi          = {10.1016/j.asoc.2025.112993},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {112993},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Digital twin-based pursuit-evasion gaming strategy optimization for underwater robot grasping},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive large neighborhood search incorporating mixed-integer linear programming for electric vehicle routing problem with mobile charging and nonlinear battery degradation. <em>ASOC</em>, <em>175</em>, 112988. (<a href='https://doi.org/10.1016/j.asoc.2025.112988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The limited driving range and short battery life are obstacles to the widespread adoption of electric vehicles in urban logistics. This study proposes an electric vehicle routing problem with time window, mobile charging, and nonlinear battery degradation. Mobile charging vehicles (MCVs) can be flexibly scheduled to charge the electric delivery vehicles (EDVs) at customer locations, reducing the electricity consumption caused by the detours to the charging stations. The proposed problem is formulated into an arc-based model that incorporates nonlinear battery degradation costs associated with State of Charge (SOC) and charging strategies, thereby enhancing the complexity of the spatio-temporal synchronization mechanism. Constraining a lower SOC can mitigate the battery degradation of EDVs, but it leads to increased charging demands and makes searching for feasible routing solutions more challenging due to the interdependence between MCVs and EDVs. A hybrid adaptive large neighborhood search heuristic algorithm is developed. Dynamic programming is embedded in the algorithm framework to devise charging schemes considering nonlinear battery degradation for the given EDVs’ routes. A mixed-integer linear programming model is formulated to select the combination of labels with continuous charging decisions and design MCVs’ routes. Extensive numerical experiments are conducted to verify the proposed model and algorithm. Experimental results indicate considering battery degradation in the objectives significantly improves the total system costs by optimizing the SOC and charging quantity. Mobile charging can be an alternative for constructing fixed charging facilities due to the charging flexibility of MCVs. The performance of our algorithm is demonstrated through both large-scale instances and a real-world case study on urban logistics.},
  archive      = {J_ASOC},
  author       = {Senyan Yang and Ruiyan Zhang and Ying Ma and Xingquan Zuo},
  doi          = {10.1016/j.asoc.2025.112988},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {112988},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive large neighborhood search incorporating mixed-integer linear programming for electric vehicle routing problem with mobile charging and nonlinear battery degradation},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unraveling asset pricing with AI: A systematic literature review. <em>ASOC</em>, <em>175</em>, 112978. (<a href='https://doi.org/10.1016/j.asoc.2025.112978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asset pricing, long recognized as a cornerstone of financial studies with multiple Nobel Prizes in Economics, is experiencing a profound transformation through the integration of artificial intelligence (AI). This study highlights the convergence of finance and computer science in asset pricing, offering novel insights into AI-driven approaches through an in-depth analysis of hundreds of research papers. The study begins by examining the key factors influencing asset pricing, highlighting the significance of factor interactions in AI-driven asset pricing models. It then systematically reviews various econometric and machine learning models from both financial and computational perspectives, underscoring the importance of designing predictive asset pricing models based on financial assumptions and principles. This reflects the inevitable convergence of finance and computer science in the field of asset pricing. Finally, the study outlines three research directions, providing actionable guidance for future exploration: (1) the development of large-scale multimodal datasets to equip advanced models with the breadth of information required to enhance foresight, (2) the integration of fundamental economic theories into model design to enhance relevance and resilience, emulating the nuanced decision-making processes of experienced traders, and (3) improving the interpretability of deep learning models to bridge the gap between their outputs and actionable insights. In addition, this study introduces the QuantPlus project, an initiative designed to provide large-scale datasets that empower researchers to evaluate and advance innovative models.},
  archive      = {J_ASOC},
  author       = {Yan Chen and Lin Zhang and Zhilong Xie and Wenjie Zhang and Qing Li},
  doi          = {10.1016/j.asoc.2025.112978},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {112978},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unraveling asset pricing with AI: A systematic literature review},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A virtual try-on network with arm region preservation. <em>ASOC</em>, <em>175</em>, 112960. (<a href='https://doi.org/10.1016/j.asoc.2025.112960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual try-on methods based on image generation have made some progress. However, the try-on results obtained are unnatural due to the difficulty in generating details such as hands. In order to address this issue, we introduce an arm-region preserving virtual try-on network, AP-VITON. First, we constructed an arm region preservation module to achieve the extraction and preservation of arm preservable regions through a series of image processing operations. Second, in order to enhance the image generation process, we combine the try-on generator module with a lightweight vision transformer, MobileViT, with the objective of improving the network’ s ability to capture global information. Finally, we introduce focal frequency loss to overcome the limitation of conventional methods that use only spatial domain loss. Quantitative and qualitative comparisons on a generic virtual try-on dataset show that our approach produces more realistic try-on results, in particular a 36.6% improvement in the KID metric and a 29.5% improvement in the LPIPS metric.},
  archive      = {J_ASOC},
  author       = {Xin Zhang and Jinguang Chen and Lili Ma and Kaibing Zhang},
  doi          = {10.1016/j.asoc.2025.112960},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {112960},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A virtual try-on network with arm region preservation},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated bayesian optimization on random fourier additive margin features and random kernel mapping. <em>ASOC</em>, <em>175</em>, 112925. (<a href='https://doi.org/10.1016/j.asoc.2025.112925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Optimization (BO) is an advanced technique for hyperparameter tuning in AutoML, particularly for optimizing black-box functions. This study mainly proposes the RAF kernel for Gaussian Processes and introduces two novel algorithms : the Federated Bayesian additive marginal Thompson Sampling algorithm (FAT) and the Federated Bayesian random kernel Thompson Sampling algorithm (FAKT), the latter combining RAF with Random Fourier Features (RFF). To enhance privacy, we further develop DP-FAT and DP-FAKT by integrating Differential Privacy, which can reduce the communication costs while safeguarding client data. Experiments show that FAT and FAKT converge 10 communication rounds faster than existing methods (e.g., FTS), significantly improving efficiency in federated black-box optimization. These advancements demonstrate strong potential for large-scale learning tasks with enhanced privacy and reduced overhead.},
  archive      = {J_ASOC},
  author       = {Fazhen Jiang and Xiaoyuan Yang},
  doi          = {10.1016/j.asoc.2025.112925},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {112925},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated bayesian optimization on random fourier additive margin features and random kernel mapping},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safety assessment of intelligent vehicles considering drivers’ risk perception information under interval 2-tuple q-rung orthopair fuzzy sets. <em>ASOC</em>, <em>175</em>, 112919. (<a href='https://doi.org/10.1016/j.asoc.2025.112919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The safety assessment of intelligent vehicles as an important part of the intelligent vehicle industry is used to assess the safety of vehicles on the road. Drivers' risk perception affects their driving behavior, so taking it into account in safety assessments can more accurately assess the safety of intelligent vehicles. Decision-makers often have difficulty choosing the best option from multiple indicators in a safety assessment. The primary objective of this paper is to assess the safety of intelligent vehicles, considering drivers' risk perception in a fuzzy environment. A multi-level evaluation system for the safety assessment of intelligent vehicles is developed, covering functional safety, active and passive vehicle safety, netlink information reliability, and driver risk perception. A hybrid decision-making methodology under Interval 2-tuple q-rung Orthopair Fuzzy Sets is proposed for safety assessments of intelligent vehicles. Subsequently, an empirical application of four safe driving schemes demonstrates the validity and practicality of this integrated methodology. Comparative analysis, sensitivity analysis, and discussions are performed. The results prove that this approach provides an accurate and effective tool for the safety assessment of intelligent vehicles.},
  archive      = {J_ASOC},
  author       = {Danqi Wang and Wengang Deng and Lin Hu and Zhongwei Huang and Yikang Lu and Honghao Zhang},
  doi          = {10.1016/j.asoc.2025.112919},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {112919},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Safety assessment of intelligent vehicles considering drivers’ risk perception information under interval 2-tuple q-rung orthopair fuzzy sets},
  volume       = {175},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data preprocessing techniques and neural networks for trended time series forecasting. <em>ASOC</em>, <em>174</em>, 113063. (<a href='https://doi.org/10.1016/j.asoc.2025.113063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on time series forecasting continues to attract significant attention, particularly in the use of Artificial Neural Networks (ANN) due to their ability to model nonlinear behaviors. However, forecasting economic time series with steep upward trends presents challenges, often leading to poorly fitting predictions. This study addresses the issue by applying differentiation as a preprocessing step. Three real-world time series exhibiting this behavior were analyzed and forecasted using two neural network models—Long Short-Term Memory (LSTM) and Multilayer Perceptron (MLP)—with and without preprocessing. The differentiated series were further processed using techniques such as Empirical Mode Decomposition (EMD) and trend-fluctuation decomposition via Moving Average of Wavelet Transform. The results demonstrate that differentiation significantly enhances forecasting accuracy across all tested models, reducing errors by up to 30 % compared to models without preprocessing. This approach effectively mitigates trend-related distortions, leading to more reliable predictions in complex economic time series.},
  archive      = {J_ASOC},
  author       = {Ana Lazcano and Miguel A. Jaramillo-Morán},
  doi          = {10.1016/j.asoc.2025.113063},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113063},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data preprocessing techniques and neural networks for trended time series forecasting},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling authenticity with diffusion-based face retouching reversal. <em>ASOC</em>, <em>174</em>, 113062. (<a href='https://doi.org/10.1016/j.asoc.2025.113062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unveiling the real appearance of retouched faces to prevent malicious users from deceptive advertising and economic fraud has been an increasing concern in the era of digital economics. This article makes the first attempt to investigate the face retouching reversal (FRR) problem. We first build an FRR dataset, named deepFRR, by collecting 50,000 StyleGAN-generated high-resolution (1024 × 1024) facial images and retouching them via a commercial online API. Then, we present a novel diffusion-based FRR network (FRRffusion) for the FRR task. Our FRRffusion consists of a coarse-to-fine two-stage architecture: A diffusion-based Facial Morpho-Architectonic Restorer (FMAR) is constructed to generate the basic contours of low-resolution faces in the first stage, while a Transformer-based Hyperrealistic Facial Detail Generator (HFDG) is designed to create high-resolution facial details in the second stage. Tested on deepFRR, our FRRffusion surpasses the state-of-the-art image restoration method with 22%, 11%, 20%, and 6% performance improvement in SSIM, PSNR, VGGS, and CLIPS, respectively. Especially, the de-retouched images by our FRRffusion are visually much closer to the raw face images than both the retouched face images and those restored by the state-of-the-art, like GP-UNIT and Stable Diffusion, in terms of qualitative evaluation with 85 subjects. These results sufficiently validate the efficacy of our FRRffusion, bridging the gap between the FRR and generic image restoration tasks. The code is available at https://github.com/GZHU-DVL/FRRffusion .},
  archive      = {J_ASOC},
  author       = {Fengchuang Xing and Xiaowen Shi and Yuan-Gen Wang and Chunsheng Yang},
  doi          = {10.1016/j.asoc.2025.113062},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113062},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unveiling authenticity with diffusion-based face retouching reversal},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nested deep learning with learned network embeddings for software defect prediction. <em>ASOC</em>, <em>174</em>, 113057. (<a href='https://doi.org/10.1016/j.asoc.2025.113057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing software (SW) defect prediction approaches and the models are majorly based on features extracted from the code of the software to build defect datasets for predictive modeling. However, these models fail to sufficiently capture the complex, latent dependencies within the software components, which acts as a hindrance in achieving higher predictive accuracy. This study introduces an improved defect prediction model, the Nested Deep Learning (NDL) model, that leverages network embeddings from call graphs for enhanced representation of intricate hierarchical class dependencies and interactions. This work evaluates six network-embedding algorithms by applying them to call graphs of 10 real software projects, generating embeddings of dimensions 32 and 128. A total of 50 NDL models—with and without dropout layers—are developed, and a comparative evaluation of these models is conducted against traditional classifier-based models. This evaluation demonstrated the superiority of the NDL model with dropout, achieving a mean AUC of 0.87, an 8.98 % improvement over the traditional classifier-based models. Among the evaluated embedding methods, LINE embeddings outperformed others, and integrating network embeddings with software metrics led to a 15.85 % AUC improvement over using software metrics alone. The optimal configuration—combining software metrics with LINE embeddings (dimension 128) in an NDL model with three deep learning layers and dropout—achieved a mean AUC of 0.93, surpassing all other configurations by 3.33–14.81 % . This study is the first to validate the effectiveness of a nested deep learning framework for modeling call graph dependencies through network embeddings, providing a scalable and robust approach for improving software defect prediction.},
  archive      = {J_ASOC},
  author       = {Sweta Mehta and Lov Kumar and Sanjay Misra and K.Sridhar Patnaik and Vikram Singh},
  doi          = {10.1016/j.asoc.2025.113057},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113057},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Nested deep learning with learned network embeddings for software defect prediction},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view self-supervised learning on heterogeneous graphs for recommendation. <em>ASOC</em>, <em>174</em>, 113056. (<a href='https://doi.org/10.1016/j.asoc.2025.113056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have significantly contributed to data mining but face challenges due to sparse graph data and lack of labels. Typically, GNNs rely on simple feature aggregation to leverage unlabeled information, neglecting the richness inherent in unlabeled data within graphs. Graph self-supervised learning methods effectively capitalize on unlabeled information. Nevertheless, most existing graph self-supervised learning methods focus on homogeneous graphs, ignoring the heterogeneity of graphs and mainly considering the graph structure from a single perspective. These methods cannot fully capture the complex semantics and correlations in heterogeneous graphs. It is challenging to design self-supervised learning tasks that can fully capture and represent complex relationships in heterogeneous graphs. In order to address the above problems, we investigate the problem of self-supervised HGNN and propose a new self-supervised learning mechanism for HGNN called Multi-view Self-supervised Learning on Heterogeneous Graphs for Recommendation (MSRec). We introduce a maximum entropy path sampler to help sample meta-paths containing structural context. Encoding information from diverse views defined by various meta-paths, decoding it into a semantic space different from own and optimizing tasks in both local-view and global-view contrastive learning, which facilitates collaborative and mutually supervisory interactions between the two views, leveraging unlabeled information for node embedding learning effectively. According to experimental results, our method demonstrates an optimal performance improvement of approximately 7% in NDCG@10 and about 8% in Prec@10 compared to state-of-the-art models. The experimental results on three real-world datasets demonstrate the superior performance of MSRec compared to state-of-the-art recommendation methods.},
  archive      = {J_ASOC},
  author       = {Yunjia Zhang and Yihao Zhang and Weiwen Liao and Xiaokang Li and Xibin Wang},
  doi          = {10.1016/j.asoc.2025.113056},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113056},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-view self-supervised learning on heterogeneous graphs for recommendation},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-branch attention coupled convolutional domain adaptation network for bearing intelligent fault recognition under unlabeled sample scenarios. <em>ASOC</em>, <em>174</em>, 113053. (<a href='https://doi.org/10.1016/j.asoc.2025.113053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bearing intelligent fault recognition is important to maintain the healthy and stable operation of mechanical equipment. However, it is difficult to have a consistent distribution of the acquired source and target domain data due to the constantly changing operating state of the equipment. Moreover, the acquisition of sufficient labeled data is constrained by both time and economic costs. Most of the existing recognition methods are difficult to perform effective fault recognition when faced with inconsistent data distribution and unlabeled small sample data. To address these issues, this paper proposes a multi-branch attention coupled convolutional domain adaptation network (MACCDAN) for unsupervised cross-domain fault recognition, which contains three unique parts. A cross-attention coupled module (CACM) is firstly designed between two parallel feature extraction branches to guide the intertwined coupling of the two branch features through a dual synergetic attention mechanism. A global feature aggregation module (GFAM) is further presented to conduct the global information fusion, which integrates the dependencies between different branch features and enhances the perception of key features. Additionally, the maximum-similarity minimum-discrepancy adversarial loss (MSMDAL) is formulated as an optimization objective to reduce the discrepancy between the source and target domain, and promote the learning of domain-invariant and discriminative features. The results of the four performance evaluation metrics (i.e., accuracy, precision, recall and F1 score) of the proposed method are all 1.0000 on two datasets. The F1 score of the proposed method is improved by at least 0.03 compared to other methods.},
  archive      = {J_ASOC},
  author       = {Maoyou Ye and Xiaoan Yan and Dong Jiang and Ning Chen},
  doi          = {10.1016/j.asoc.2025.113053},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113053},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-branch attention coupled convolutional domain adaptation network for bearing intelligent fault recognition under unlabeled sample scenarios},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A morphological difference and statistically sparse transformer-based deep neural network for medical image segmentation. <em>ASOC</em>, <em>174</em>, 113052. (<a href='https://doi.org/10.1016/j.asoc.2025.113052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation plays a pivotal role in enhancing disease diagnosis and treatment planning. However, existing methods often struggle with the complexity of lesion boundaries and the computational demands of Transformer-based approaches. To address these challenges, we propose a morphological difference and statistically sparse Transformer-based deep neural network for medical image segmentation, termed MD-SSFormer. It comprises two critical modules: the dual branch encoder (DBEncoder) module, and the morphological difference catcher (MDC). To extract abundant information at different aspects, a novel DBEncoder module integrates the capability of the convolutional neural network-based method in capturing local texture and the ability of the Transformer-based method in modeling global information. Compared to the conventional feature extraction methods, DBEncoder achieves comprehensive improvement. Furthermore, the statistics-based sparse Transformer (SSFormer) module develops an innovative statistical analysis and an adaptive patch-dividing strategy to perform attention-computing, which addresses the computational challenges associated with conventional Transformer-based models. Finally, considering the impacts of the blurry and complex boundaries, the MDC module employs the morphological operation and differential information extractor to refine the details, which achieves high-precision boundary understanding. Experimental results on five public datasets demonstrate MD-SSFormer's superior performance, achieving state-of-the-art Dice scores of 83.60 % on ISIC 2017, 79.52 % on Kvasir-SEG, 61.89 % on BUSI, 78.62 % on BraTS21, and 85.85 % on 3DIRCADb, outperforming other methods in accuracy, precision, and computational efficiency respectively.},
  archive      = {J_ASOC},
  author       = {Dongxu Cheng and Zifang Zhou and Hao Li and Jingwen Zhang and Yan Yang},
  doi          = {10.1016/j.asoc.2025.113052},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113052},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A morphological difference and statistically sparse transformer-based deep neural network for medical image segmentation},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A heuristic-based multi-stage machine learning-based model to design a sustainable, resilient, and agile reverse corn supply chain by considering third-party recycling. <em>ASOC</em>, <em>174</em>, 113042. (<a href='https://doi.org/10.1016/j.asoc.2025.113042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the reverse supply chain configuration problem for the agri-food sector with agility, resilience, and sustainability aspects. To do this, this article proposes a heuristic-based multi-stage machine learning-based model to design a corn reverse logistics based on agility, resilience, and sustainability features. In this way, at the first stage, the performance of the potential recycling partners is evaluated by combining the Categorical Boosting Algorithm (CatBoost) method. In the next stage, a multi-objective model is suggested to configure the corn reverse logistics in which the resilience, agility, and sustainability dimensions are incorporated. Afterwards, we deal with uncertainty by developing a data-driven method based on the chance-constrained fuzzy programming method and the seasonal autoregressive integrated moving average approach. Finally, by choosing a real-world case study, the suggested model is solved by developing a heuristic-based solution procedure. The obtained results showed that the developed heuristic-based solution approach able to find optimal and near-optimal solution in a reasonable time. Based on the achieved outputs, increasing the capacity parameter has a positive impact in the efficiency of the supply chain. Also, results show that when the amount of the initial waste increases, the total profit and environmental impacts of the supply chain have increased, too. Also, the achieved outputs confirm the robustness and efficiency of the developed machine learning-based approach. Then, several sensitivity analyses are presented to examine the role of the key parameters in the research problem. Finally, the managerial insights are provided.},
  archive      = {J_ASOC},
  author       = {Fardin Rezaei Zeynali and Mohammad Parvin and Ali Akbar ForouzeshNejad and Emaad Jeyzanibrahimzade and Mohssen Ghanavati-Nejad and AmirReza Tajally},
  doi          = {10.1016/j.asoc.2025.113042},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113042},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A heuristic-based multi-stage machine learning-based model to design a sustainable, resilient, and agile reverse corn supply chain by considering third-party recycling},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multidimensional fitness function based heuristic algorithm for set covering problems. <em>ASOC</em>, <em>174</em>, 113038. (<a href='https://doi.org/10.1016/j.asoc.2025.113038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The set covering problem (SCP) is a conventional integer programming challenge in combinatorial optimization, with applications spanning fields such as transportation, logistics, and location problems. Solving SCPs efficiently is crucial for optimizing operations in these domains, particularly in location problems, where traditional algorithms often struggle with multidimensional objective spaces. To address such challenges, this study proposes a novel problem-dependent heuristic algorithm to solve SCPs, featuring a new multi-dimensional fitness function, which was evaluated by benchmarking against other heuristic and metaheuristic algorithms. A collection of reproduced and selected OR-library problems of various scales were chosen as benchmark instances to assess the performance of the algorithm. The performance of the algorithm was confirmed as it constructs solutions by leveraging a novel fitness function to address the limitations of time complexity, applicability, and scalability. Computational results demonstrate that the developed algorithm offers competitive solutions for SCPs, showing improvements of up to 88 % and 20 % in terms of time compared to simulated annealing and a preliminary heuristic algorithm, respectively. In terms of quality, the developed algorithm achieved cost reductions of up to 21 % and 11 % compared to these algorithms, respectively.},
  archive      = {J_ASOC},
  author       = {Ahmad Hashemi and Hamed Gholami and Xavier Delorme and Kuan Yew Wong},
  doi          = {10.1016/j.asoc.2025.113038},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113038},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multidimensional fitness function based heuristic algorithm for set covering problems},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the industry 4.0 strategies in the automobile manufacturing firm using a combined compromise solution-based ranking method. <em>ASOC</em>, <em>174</em>, 113037. (<a href='https://doi.org/10.1016/j.asoc.2025.113037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementing industry 4.0 (I4.0) strategies in automobile manufacturing firm leads to the higher demand for newer services, drives innovation, continuously innovates to meet the changing needs and expectations of customers, and enables the development of sustainable solutions. This paper develops a q-rung orthopair fuzzy information (q-ROFI)-based decision support tool to evaluate I4.0 strategies in the automobile manufacturing firm. The proposed framework firstly calculates weight of decision expert using a procedure considering the q-rung orthopair fuzzy set (q-ROFS). Next, an individual opinions of decision experts are aggregated into single decision through q-ROF weighted averaging operator. Further, criteria weights are computed by a combined weighting procedure involving objective weighting through entropy-based procedure and subjective weighting by stepwise weight assessment ratio analysis (SWARA) model with q-ROFI. In the following purpose, new entropy is introduced based on the cross entropy of q-ROFS and new score function is proposed for q-ROFS to evade the limitation of existing q-ROF-score function. On the basis of these steps, a modified combined compromise solution (CoCoSo) approach is presented to assess and prioritize the alternatives under q-ROFS context. Finally, the proposed framework is applied to a case study of I4.0 strategies evaluation problem in automobile manufacturing firm. According to the outcomes, the most suitable strategy among the other strategies over considered twenty-five evaluation criteria for assessing I4.0 strategies in the automobile manufacturing firms is as new business models development strategies (0.319), improving information systems strategies (0.273) and human resource management (HRM) strategies (0.210), respectively. The most significant criteria for assessing I4.0 strategies in the automobile manufacturing firms are technology (0.055), coordination (0.052), and legal problems (0.048), respectively. Moreover, comparison with different existing methods is presented to validate the robustness of introduced method.},
  archive      = {J_ASOC},
  author       = {Arunodaya Raj Mishra and Pratibha Rani and Ahmad M. Alshamrani and Adel Fahad Alrasheedi and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2025.113037},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113037},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessing the industry 4.0 strategies in the automobile manufacturing firm using a combined compromise solution-based ranking method},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A feature engineering based model architecture for modeling initial public offerings. <em>ASOC</em>, <em>174</em>, 113035. (<a href='https://doi.org/10.1016/j.asoc.2025.113035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a model architecture for modeling Initial Public Offerings (IPOs) by incorporating a diverse range of data sources, encompassing both textual and numerical inputs. Language models, machine learning models, and deep learning architectures are combined to make the final ensemble predictions. Several rich features are engineered and interpreted while providing scope for debugging using the game theory-based Shapley Additive exPlanations (SHAP) values. The study results indicate that the feature-engineering is highly eloquent in IPO performance modelling. The study findings have high economic implications range from detecting the market trends to overall market stability.},
  archive      = {J_ASOC},
  author       = {Durga Vaidynathan and Parthajit Kayal and Moinak Maiti},
  doi          = {10.1016/j.asoc.2025.113035},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113035},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A feature engineering based model architecture for modeling initial public offerings},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-reference super-resolution reconstruction of remote sensing images based on hierarchical similarity mapping. <em>ASOC</em>, <em>174</em>, 113027. (<a href='https://doi.org/10.1016/j.asoc.2025.113027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To make full use of the details from multi-reference images and improve the quality of super-resolution reconstruction of remote sensing images, a multi-reference super-resolution reconstruction of remote sensing images based on hierarchical similarity mapping is proposed. It is very important in both military and civilian fields. Firstly, one low resolution image and three reference images are used as the input of VGG network to extract their feature maps at 4 × , 2 × , and 1 × scales. These feature maps at each scale are respectively blocked and used as a set of inputs in subsequent operations. Specifically, the low resolution features are divided into N i blocks, and each block is further divided into N c sub-feature-blocks. And the N m reference image features are divided into N r sub-feature-blocks. Then the N c low-resolution sub-feature blocks are mapped for similarity with the reference features within the range of all reference sub-feature blocks, individual reference features, and all reference image features. The outputs of each layer are then iteratively mapped with the low-resolution features as inputs for next layers. Thus the final features include information from all the reference images and low-resolution image. Subsequently, an adaptive transfer module with multi-reference features and channel attention is used to match and transfer the information of each reference image, while achieving edge smoothing and noise filtering between different reference features. Finally, the quadruple super-resolution reconstruct result is got from the multi-scale feature fusion module and decoder. Experimental results show that our improvements can reconstruct better super-resolution results with more details for utilizing information of multi-reference images, which is superior to single image super-resolution methods and single reference super-resolution methods.},
  archive      = {J_ASOC},
  author       = {Fuzhen Zhu and Qi Zhang and Bing Zhu and Chen Wang},
  doi          = {10.1016/j.asoc.2025.113027},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113027},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-reference super-resolution reconstruction of remote sensing images based on hierarchical similarity mapping},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flight anomaly detection and localization based on flight data fusion and random channel masking. <em>ASOC</em>, <em>174</em>, 113023. (<a href='https://doi.org/10.1016/j.asoc.2025.113023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flight anomaly detection and localization are critical for enhancing aircraft safety through effective analysis of flight data. However, existing methods only detect the timing of anomalies, failing to identify and recover the specific abnormal parameters necessary for assisting flight control systems in correcting the aircraft's state. To address these limitations, this paper proposes a novel anomaly detection and localization method based on random channel masking (RCM). The proposed approach integrates a multi-node synchronous prediction (MNSP) model, which combines graph attention networks and convolutional neural networks to extract both normal and anomalous patterns from extensive flight data. RCM is employed to generate pseudo-anomalous data, enabling the MNSP model to accurately localize and recover affected parameters. The effectiveness of proposed method is validated using real flight data from unmanned aerial vehicle, achieving an average anomaly detection accuracy of 95 % across four distinct types of anomalies. Furthermore, the method successfully localizes specific abnormal parameters with a localization accuracy of no less than 92.5 % across three different anomaly scenarios. In single-parameter anomaly scenarios, the mean squared error of data recovery remains below 0.000082. The study also explores the boundaries of anomaly localization in multi-parameter scenarios, highlighting the algorithm's robustness and applicability under diverse conditions.},
  archive      = {J_ASOC},
  author       = {Jie Zhong and Heng Zhang and Qiang Miao},
  doi          = {10.1016/j.asoc.2025.113023},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113023},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Flight anomaly detection and localization based on flight data fusion and random channel masking},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantum entanglement-based optimization method for complex expensive engineering problems. <em>ASOC</em>, <em>174</em>, 113019. (<a href='https://doi.org/10.1016/j.asoc.2025.113019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the computational costliness and time-consuming nature of complex and expensive engineering (CEE) problems, this paper proposes a genetic algorithm based on quantum entanglement to address these challenges. This method encodes individuals into quantum genes, where each gene bit stores not 0 or 1, but a superposition state of both. By leveraging the uncertainty of the superposition state during the collapse, this method effectively preserves population diversity even with a very small population size. A smaller population size implies fewer calls to time-consuming simulations. Additionally, quantum entangled states are created for parts of an individual's gene, utilizing the characteristic that entangled states instantly affect each other upon collapse, to achieve parallel evolution of parts of the genes in multiple individuals. This parallel evolution significantly increases the search speed of the algorithm, thereby reducing the number of iterations. Fewer iterations also mean fewer calls to simulations. Benchmark function experiments demonstrate that the proposed method is significantly superior to other similar algorithms in a 30D solution space with a population size of 20 and also has certain advantages in a 100D solution space.},
  archive      = {J_ASOC},
  author       = {Fengling Peng and Xing Chen},
  doi          = {10.1016/j.asoc.2025.113019},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113019},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A quantum entanglement-based optimization method for complex expensive engineering problems},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). State-space recurrent neural networks for predictive analytics and latent state estimation. <em>ASOC</em>, <em>174</em>, 113017. (<a href='https://doi.org/10.1016/j.asoc.2025.113017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a framework to predict the remaining life (RL) of degrading systems under sensor condition monitoring. By integrating state-space modeling with stochastic recurrent neural networks, our approach efficiently processes condition-monitoring time-series data and models systems’ latent degradation states. We propose a stochastic model that captures dependencies among latent degradation states, sensor outputs, and RL in a causally coherent manner and utilizes stochastic neural networks to navigate the inherent uncertainties of system dynamics. To enhance the interpretability of RL estimation and latent state modeling, we propose interpretable regularization terms. These terms are incorporated into the loss function to optimize both the prediction precision of estimating remaining life and latent states and control the monotonic behavior of their estimates, thereby improving the model’s overall performance and interpretability. Our methodology is validated through numerical experiments and comparison with benchmark models, demonstrating its potential to improve predictive maintenance strategies by effectively estimating the remaining life and monitoring the state of latent degradation over time.},
  archive      = {J_ASOC},
  author       = {Ramin Moghaddass and Cheng-Bang Chen},
  doi          = {10.1016/j.asoc.2025.113017},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113017},
  shortjournal = {Appl. Soft. Comput.},
  title        = {State-space recurrent neural networks for predictive analytics and latent state estimation},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label learning for fault diagnosis of pumping units with one positive label. <em>ASOC</em>, <em>174</em>, 113014. (<a href='https://doi.org/10.1016/j.asoc.2025.113014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis using the indicator diagram is a fundamental method to evaluate the working status of pumping units. In applications, human experts typically identify only one fault for each indicator diagram. However, multiple types of faults may occur simultaneously. In this paper, we propose a Single-Positive Multi-label learning for Fault Diagnosis of Pumping Units (SPM-FDPU) algorithm to address this issue. Although trained on single-label data, it is capable of multi-label prediction. First, HU invariant moments and convolutional neural networks are used to extract common and label-specific features, respectively. Second, instance, feature, and label correlations are injected into the training process by feature and label manifolds to enhance supervised information. Third, the manifold is used to augment the latent label matrix to help explore discriminant information. Experiments are conducted on the three real indicator diagram data of an oil field and sixteen multi-label benchmark datasets. The results show that the accuracy of the proposed method has achieved 98% in diagnosing multiple faults on indicator diagram datasets, and the mean rank of the proposed method is optimal in terms of six popular evaluation metrics on multi-label benchmark datasets. The source code is available at github.com/Kqian2020/SPM-FDPU .},
  archive      = {J_ASOC},
  author       = {Kun Qian and Jinyu Tang and Qimei Zhao and Shu Zhao and Fan Min},
  doi          = {10.1016/j.asoc.2025.113014},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113014},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-label learning for fault diagnosis of pumping units with one positive label},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent modeling for indoor fire risk prediction during evacuation based on cellular automata and artificial neural network. <em>ASOC</em>, <em>174</em>, 113013. (<a href='https://doi.org/10.1016/j.asoc.2025.113013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fire cases have always posed threats to human lives and property safety, and new approaches have been developed to investigate how people behave during the fire process. Understanding the underlying mechanism under specific scenarios and conditions is critical to find possible ways of reducing social losses. Here, we propose a coupled model that combines FDS and CA, to assess fire risks in a multi-story dormitory building at a university. For this real target case, the settings of automatic sprinklers and temperature alarms will be considered in our coupled model. The aim is to investigate how pedestrians behave under the fire emergencies and how fire safety facilities (exits) shape final evacuation outcomes. To analysis the final outcomes and related factors, we use Event Tree and BP neural network methods to assess and predict individual risk levels. It suggests that controlling the number of people in each dormitory will effectively reduce the fire risk, and the existence of safety facilities can significantly contain fire risks. Early fire warning systems and quick response times are critical to reduce casualties during the evacuation process. Individual risk levels can be efficiently calculated by Event Tree method, and BP neural network can accurately predict fire risk levels. By integrating technologies such as FDS, CA, ETA, and BP neural networks, our model can effectively simulate the dynamic process of the fire evacuation while accurately predicting the fire risks, which establishes an effective link between environmental factors and fire risk assessment. This provides a methodological reference for future fire risk assessment research.},
  archive      = {J_ASOC},
  author       = {Peng Lu},
  doi          = {10.1016/j.asoc.2025.113013},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113013},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-agent modeling for indoor fire risk prediction during evacuation based on cellular automata and artificial neural network},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Short-term forecasting of electricity price using ensemble deep kernel based random vector functional link network. <em>ASOC</em>, <em>174</em>, 113012. (<a href='https://doi.org/10.1016/j.asoc.2025.113012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term electricity price forecasting in a deregulated electrical market is a difficult task as the electricity price exhibits high nonlinearity, sharp price spikes, and seasonality in different frequencies, etc. Thus, this study presents a new approach using an Ensemble Deep Kernel Random Vector Functional Link Network (EDKRVFLN) model hybridized with a Chaotic Sine Cosine Improved Firefly Algorithm (CSCIFA) for short-term electricity price forecasting with better generalization capacity, simple structure, and significant accuracy. Unlike the Ensemble Deep Random Vector Functional Link Network (EDRVFLN) where each stacked layer requires proper choice of the number of hidden nodes and manual tuning of random weights and biases along with the pseudoinverse solution of the output weights in each layer leading to suboptimal model generalization. However, the choice of random weights and biases along with the number of hidden neurons in the proposed EDKRVFLN model can be dispensed by using kernel-based transformation and representation learning. Further each stacked layer of the proposed model utilizes kernel based linear features from the direct links and nonlinearly transformed features from the enhancement nodes from the preceding layers of the prediction model. Also, each layer produces an output by simple invertible kernel matrix inversion based on generalized least squares, and the final output is the ensemble of the outputs from each layer, thus simultaneously producing an ensemble and deep learning framework. Seven electricity price datasets are examined to confirm the supremacy of the proposed model in comparison to several benchmark models.},
  archive      = {J_ASOC},
  author       = {Someswari Perla and Ranjeeta Bisoi and P.K. Dash and A.K. Rout},
  doi          = {10.1016/j.asoc.2025.113012},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113012},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term forecasting of electricity price using ensemble deep kernel based random vector functional link network},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing AI safety of machine unlearning for ensembled models. <em>ASOC</em>, <em>174</em>, 113011. (<a href='https://doi.org/10.1016/j.asoc.2025.113011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, machine unlearning (MU) has received significant attention for its ability to remove specific undesired knowledge from a trained model, thereby ensuring AI safety. Furthermore, efforts have been made to integrate MU into existing Machine Learning as a Service (MLaaS), allowing users to raise requests to remove the influence of their data used in the training phase, after which the server conducts MU to remove its influence based on the unlearning requests. However, previous research reveals that malicious users may manipulate the requests so that the model utility may be significantly compromised after unlearning, which is known as malicious unlearning. In addition, privacy leakage may be exploited by malicious users by analyzing inference results obtained from the original model and the unlearned model. In this connection, we investigate these potential risks, specifically in ensemble models, which are widely adopted in MU because of their efficiency in unlearning and robustness in learning. However, despite these advantages, their vulnerabilities to malicious unlearning and privacy leakage remain largely unexplored. Our work explores malicious unlearning and malicious inference in ensemble settings. We propose a method in which malicious unlearning requests can trigger hidden poisons in ensembles, causing target images to be misclassified as intended by adversaries. Additionally, we introduce a privacy leakage attack where adversaries with black-box access to voting outputs can infer the unlearned label by analyzing the differences between the original and unlearned ensemble outputs. Experimental results demonstrate that these attacks can be highly stealthy and achieve a high success rate. Furthermore, comparative experiments reveal that these attacks present slightly lower stealthiness in ensemble settings compared to single-model scenarios, suggesting that ensemble models have advantages in detecting such malicious activities. These findings reveal that ensemble models are vulnerable to malicious unlearning and privacy leakage and highlight the urgent need for more robust MU designs to ensure AI safety.},
  archive      = {J_ASOC},
  author       = {Huanyi Ye and Jiale Guo and Ziyao Liu and Yu Jiang and Kwok-Yan Lam},
  doi          = {10.1016/j.asoc.2025.113011},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113011},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing AI safety of machine unlearning for ensembled models},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual degradation image inpainting method via adaptive feature fusion and U-net network. <em>ASOC</em>, <em>174</em>, 113010. (<a href='https://doi.org/10.1016/j.asoc.2025.113010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing image inpainting methods are designed to address a single specific task, such as super-resolution, denoising, or colorization, with few models capable of handling dual degradation simultaneously. Moreover, current algorithms that tackle multiple image degradation problems often suffer from complex structures, prolonged training times, and high labor costs. In this paper, we propose a Dual Degradation Network via Adaptive Feature Fusion and U-Net (AFFU). The network employs a Self-Guided Module (SGM) to fuse multi-scale image information, effectively eliminating certain defects in the image. A coder-decoder module with null convolution is utilized to consolidate the semantic information of the image, enabling intermediate image colorization. Additionally, an Adaptive Multi-feature Fusion Module (AMF) and Information Transfer Mechanism (ITM) are introduced to link these two major structures, adaptively selecting and retaining image features during network progression to prevent the loss of useful information. Experimental results demonstrate that the proposed dual image degradation restoration network model, based on adaptive multi-feature fusion, achieves optimal visual generation. Evaluations on CelebA dataset and Landscape dataset show that the proposed method outperforms comparable approaches in terms of Structural Similarity (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS).},
  archive      = {J_ASOC},
  author       = {Yuantao Chen and Runlong Xia and Kai Yang and Ke Zou},
  doi          = {10.1016/j.asoc.2025.113010},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113010},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual degradation image inpainting method via adaptive feature fusion and U-net network},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-aware graph contrastive learning with topological relationship for recommendation. <em>ASOC</em>, <em>174</em>, 113008. (<a href='https://doi.org/10.1016/j.asoc.2025.113008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are a vital tool to guide the overwhelming amount of online information for users, which has been successfully applied to online retail platforms, social networks, etc. Recently, contrastive learning has revealed outstanding performance in recommendation by data augmentation strategies to handle highly sparse data. Most existing work fails to leverage the original network’s topology to construct attention-aware modules that identify user–item interaction importance for guiding node aggregation while preserving key semantics and reducing noise in the reconstructed graph during data augmentation. In this paper, our work proposes an At t e ntion-aware G raph C ontrastive L earning architecture with Topological Relationship (AteGCL) for recommendation. In particular, our AteGCL proposes an attention-aware mechanism with topological relationships to learn the importance between users and items for extracting the local graph dependency, which identifies the importance between nodes by constructing an attention-aware matrix into graph convolutional networks using a random walk with a restart strategy for generating node feature aggregation. We then employ principal component analysis (PCA) for contrastive augmentation and utilize the attention-aware matrix to ease noise from the reconstructed graph generated by PCA and to generate a new view with global collaborative relationships and less noise. Comprehensive experiments on three real-world user–item networks reveal the superiority of our AteGCL over diverse state-of-the-art recommendation approaches. Our code is available at https://github.com/ZZHCodeZera/AteGCL .},
  archive      = {J_ASOC},
  author       = {Xian Mo and Jun Pang and Zihang Zhao},
  doi          = {10.1016/j.asoc.2025.113008},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113008},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-aware graph contrastive learning with topological relationship for recommendation},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Category-level pipe pose and size estimation via geometry-aware adaptive curvature convolution. <em>ASOC</em>, <em>174</em>, 113006. (<a href='https://doi.org/10.1016/j.asoc.2025.113006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pipe pose estimation provides crucial positional information for robots, enhancing assembly efficiency and precision, while its accuracy critically impacts the final product's reliability and quality. To handle unseen pipes, we propose a category-level pipe pose and size estimation network via Normalized Object Coordinate Space (NOCS) representation. Given an RGB image and its corresponding depth map, our network predicts class labels, bounding boxes and instance masks for detection, as well as NOCS maps for pose estimation. Then these predictions are aligned with the depth map to estimate pipe’s pose and size. To better extract complex and variable pipe morphology, geometry-aware adaptive curvature convolution is introduced to dynamically adapt to the slender structure and improve segmentation performance. Facing the lack of pipe pose datasets with enough instances, pose, clutter, occlusion, and illumination variation, we propose a novel domain randomization mixed reality approach to efficiently generate synthetic data, which addresses the limitations of training datasets, making data generation more time- and effort-efficient. Experimental results demonstrate that our Geometry-Aware Adaptive Convolutional Network (GACNet) outperforms other methods and robustly estimates the pose and size of unseen pipes in real-world environments.},
  archive      = {J_ASOC},
  author       = {Jia Hu and Jianhua Liu and Shaoli Liu and Lifeng Wang},
  doi          = {10.1016/j.asoc.2025.113006},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {113006},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Category-level pipe pose and size estimation via geometry-aware adaptive curvature convolution},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discrete differentiated creative search for traveling salesman problem. <em>ASOC</em>, <em>174</em>, 112998. (<a href='https://doi.org/10.1016/j.asoc.2025.112998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel population-based Discrete Differentiated Creative Search (DDCS) is proposed in this paper for solving the traveling salesman problem (TSP). DDCS introduces greedy beam search to adaptively initialize the population and improve the quality of the initial solutions. Second, a multi-edge construction operator, edge-based mathematical operations and a similarity attraction operator are used to guide individuals from different population categories towards higher-quality solutions based on the current solutions. Finally, a random nearest neighbor replacement strategy is used to replace individuals with the same distance heuristically, reducing the assimilation rate of the population. DDCS is tested with 50 instances from TSPLIB and compared with a variety of state-of-the-art and variants of classical algorithms. The results demonstrate that DDCS exhibits superior optimization capability and higher stability.},
  archive      = {J_ASOC},
  author       = {Qi Xu and Kewen Xia and Xiaoyu Chu},
  doi          = {10.1016/j.asoc.2025.112998},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112998},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discrete differentiated creative search for traveling salesman problem},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-short term cross echo state network for time series forecasting task. <em>ASOC</em>, <em>174</em>, 112997. (<a href='https://doi.org/10.1016/j.asoc.2025.112997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigating the dynamics of time series in nonlinear systems has become a prominent research focus in both theoretical and practical domains. Unveiling the intrinsic characteristics of nonlinear time series can significantly enhance the understanding and modelling of nonlinear systems. Among the various time prediction models, Reservoir Computing (RC) has garnered widespread attention due to its distinctive hidden layer architecture. The Echo State Network (ESN) is one of the most representative instances within the RC framework. However, most existing ESNs do not explicitly capture the fixed multi-scale dependencies in time series, and their short-term memory (STM) cannot meet the needs of specific time series. To address these limitations, this paper introduces a novel Echo State Network with a heterogeneous topology, named the Long-short Term Cross Echo State Network (LS-CrossESN). The overall architecture of this model consists of three different types of reservoirs in parallel. And it incorporates a heterogeneous topology structure known as the cross architecture, which merges those of the first reservoir with the state characteristics of the second reservoir, so that the information between the two reservoirs can be transmitted to each other. At the same time, a time-delay operator is inserted in the second reservoir, so that the fused characteristics would not be immediately input to the next layer but transmitted to the deep layer. In this way, the characteristics of input would not decay with the update of the layers. The structure of third reservoir captures the influence of recent historical memory through a specific sliding window technology, and finally the multi-scale states from each layer would be collected for combined prediction. To optimize parameters in this model, an Improved Salp Swarm Algorithm (ISSA) is proposed. The model was tested of eight datasets spanning three categories: Mackey-Glass series, Lorenz chaotic series, Sunspot series, airport temperature series, and two real network traffic datasets. The experimental results demonstrate that the STM of LS-CrossESN is significantly improved compared with Deep-ESN, LS-ESN, DATDR and ADRC. Across all eight datasets, the model exhibits robust performance in both one-step-ahead and multi-step predictions.},
  archive      = {J_ASOC},
  author       = {Dongchen Jiang and Li Cui and Yi Zeng and Meiming You and Guoqiang Wang},
  doi          = {10.1016/j.asoc.2025.112997},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112997},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Long-short term cross echo state network for time series forecasting task},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A crude oil price forecasting framework based on constraint guarantee and pareto fronts shrinking strategy. <em>ASOC</em>, <em>174</em>, 112996. (<a href='https://doi.org/10.1016/j.asoc.2025.112996'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of crude oil prices is essential for making informed energy policy decisions and ensuring energy security. However, crude oil price forecasting is inherently challenging due to the volatile, nonlinear, and complex nature of the market. While ensemble learning approaches have shown promise in enhancing forecasting accuracy, many existing models rely on multi-objective optimization techniques that generate a Pareto frontier of optimal solutions, often making it difficult to select the best solution for practical application. This issue is exacerbated by the fact that some Pareto-optimal solutions are not suitable for real-world decision-making, leading to inefficiencies in model performance. To address these limitations, this research proposes a novel ensemble learning framework that incorporates a Constraint Guarantee Strategy (CGS) and a Pareto Front Shrinking Strategy (PFSS) to enhance both the accuracy and stability of crude oil price forecasting models. The CGS filters out inferior solutions during the optimization process, ensuring that the ensemble model outperforms individual models in terms of forecasting accuracy. The PFSS helps decision-makers select the most relevant solutions from the Pareto frontier by balancing trade-offs between objectives and narrowing down the set of solutions. Our framework is evaluated on three widely used datasets: Brent, WTI, and Dubai crude oil prices, and compared with state-of-the-art models from both the general time-series forecasting domain and crude oil price forecasting. It improves prediction accuracy by approximately 23.2% on the Brent dataset, 4.0% on the WTI dataset, and 21.7% on the Dubai dataset, based on improvements in MAPE. Ablation studies confirm the effectiveness of each component. The discussion further emphasizes the practical applicability and robustness of the framework, confirming its potential for real-world crude oil price forecasting.},
  archive      = {J_ASOC},
  author       = {Yujie Chen and Zhirui Tian},
  doi          = {10.1016/j.asoc.2025.112996},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112996},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A crude oil price forecasting framework based on constraint guarantee and pareto fronts shrinking strategy},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating competitive framework into differential evolution: Comprehensive performance analysis and application in brain tumor detection. <em>ASOC</em>, <em>174</em>, 112995. (<a href='https://doi.org/10.1016/j.asoc.2025.112995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an efficient and effective optimizer based on the Success History Adaptive DE (SHADE) named Competitive Framework DE (CFDE). We integrate three tailored strategies into CFDE: (1) the competitive framework to identify and prioritize potential individuals, (2) the novel DE/loser-to-best/loser-to-winner mutation scheme to fully leverage the information from the population and competition to construct high-quality offspring individuals, and (3) the random memory initialization to diversify the search patterns of the individual. We conduct comprehensive numerical experiments on CEC2017, CEC2020, CEC2022, and eight engineering problems against eleven state-of-the-art optimizers to confirm the superiority and competitiveness of CFDE. Moreover, the sensitivity experiments on hyperparameters validate the robustness of CFDE, and the ablation experiments practically prove the independent contribution of integrated components. Furthermore, we propose a hybrid model named DenseNet-CFDE-ELM for brain tumor detection, where DenseNet-169 is employed for feature selection and CFDE-optimized Extreme Learning Machine (ELM) classifies the brain tumors in MRI scans. Experimental results on the brain tumor dataset downloaded from Kaggle confirm that the proposed DenseNet-CFDE-ELM achieves improvements in accuracy with 1.794%, precision with 1.696%, recall with 1.794%, and F1 score with 1.812% against the second-best ResNet-18 model. These results reveal the potential of CFDE in extensive real-world optimization scenarios. The source code of this research can be downloaded from https://github.com/RuiZhong961230/CFDE .},
  archive      = {J_ASOC},
  author       = {Rui Zhong and Zhongmin Wang and Yujun Zhang and Junbo Jacob Lian and Jun Yu and Huiling Chen},
  doi          = {10.1016/j.asoc.2025.112995},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112995},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating competitive framework into differential evolution: Comprehensive performance analysis and application in brain tumor detection},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy AHP-based trust management mechanism for self-sovereign identity in the metaverse. <em>ASOC</em>, <em>174</em>, 112994. (<a href='https://doi.org/10.1016/j.asoc.2025.112994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-sovereign identity (SSI) technology has advantages and potential for application in the metaverse. However, the decentralization and anonymous interaction of SSI create convenience for malicious attacks, frauds, and conspiracies in the metaverse. It leads to various trust risks and threats to the meta-universe system. To address these challenges, we analyze the risks of SSI systems and constructed a reputation index system. Moreover, we propose a blockchain-based reputation management framework (BBRMF), which can constrain users from engaging in illegal activities such as forgery, fraud, and conspiracy, thereby guaranteeing the security and trustworthiness of the entities involved in the metaverse. In BBRMF, we constructed a reputation evaluation model based on fuzzy analytical hierarchy process (FAHP) to assess the user’s reputation in three dimensions: reliability, trustworthiness and security. To motivate users to accumulate more positive reputation, we set the user’s reputation score into a reputation credential in the form of non-fungible token (NFT), through which users can obtain more benefits and opportunities. Finally, we calculated the reputation value of SSI related entities from multiple perspectives through simulation experiments and comparative analysis. The feasibility of the proposed method is verified, and it is proved that it can effectively resist the interference and attack of malicious scoring nodes. Moreover, the scheme adopts multi-dimensional evaluation indexes and behavioral feature values, which significantly improves the comprehensiveness and accuracy of the reputation assessment. Meanwhile, the weights of the evaluation indexes are derived through objective calculation, ensuring the fairness of the evaluation results, and improving the credibility and repeatability of the reputation assessment.},
  archive      = {J_ASOC},
  author       = {Xiaoling Song and Guangxia Xu and Yongfei Huang},
  doi          = {10.1016/j.asoc.2025.112994},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112994},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy AHP-based trust management mechanism for self-sovereign identity in the metaverse},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combination weighting method using Z-numbers for multi-criteria decision-making. <em>ASOC</em>, <em>174</em>, 112992. (<a href='https://doi.org/10.1016/j.asoc.2025.112992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a hybrid approach to determining criteria weights in Multi-Criteria Decision Making (MCDM), combining subjective weight methods with objective weighting techniques based on Z-number theory. The methodology is applied in a practical context involving the establishment of a bank's call center data analysis platform. Leveraging the inherent uncertainty and reliability considerations in decision-making processes, the hybrid method offers a robust framework for decision support. Through empirical validation and case study analysis, the effectiveness of the proposed approach is demonstrated, highlighting its ability to balance theoretical robustness with practical applicability. The study underscores the importance of ongoing research in MCDM, particularly in developing innovative methods to address the complexities of decision-making environments. Insights from this research provide valuable guidance for practitioners and researchers seeking to enhance MCDM processes across diverse domains.},
  archive      = {J_ASOC},
  author       = {Huan-Jyh Shyur},
  doi          = {10.1016/j.asoc.2025.112992},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112992},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combination weighting method using Z-numbers for multi-criteria decision-making},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-way decision-based model for occupational risk assessment and classification in the healthcare industry. <em>ASOC</em>, <em>174</em>, 112991. (<a href='https://doi.org/10.1016/j.asoc.2025.112991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, occupational health and safety risk assessment (OHSRA) has gained more importance since occupational hazards can cause loss of life, injuries, delays, and cost overruns in an organization. The OHSRA is a critical activity for identifying, analyzing and reducing the potential occupational hazards arising from workplace for corrective actions. In this study, a new OHSRA model is proposed for the risk assessment and classification of occupational hazards by utilizing the criteria importance through inter-criteria correlation (CRITIC) method and three-way decision (TWD). First, the 2-tuple linguistic variables are utilized to express the complex and uncertain risk assessments of occupational hazards provided by experts. Second, an extended CRITIC method is employed to compute the weights of risk criteria by considering their interactions. Then the TWD is improved to determine the risk classifications of occupational hazards by considering their correlations. Finally, a practical case in the healthcare industry is provided to illustrate the feasibility and strengths of the proposed OHSRA model. The results show that the proposed OHSRA model can generate more credible risk classifications of occupational hazards and offer a flexible way for analyzing the risk of occupational hazards.},
  archive      = {J_ASOC},
  author       = {Ran Liu and Hu-Chen Liu and Qi-Zhen Zhang and Hua Shi},
  doi          = {10.1016/j.asoc.2025.112991},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112991},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-way decision-based model for occupational risk assessment and classification in the healthcare industry},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive encoding and comprehensive attention decoding network for medical image segmentation. <em>ASOC</em>, <em>174</em>, 112990. (<a href='https://doi.org/10.1016/j.asoc.2025.112990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation involves partitioning different tissues or lesion areas within medical images. Achieving automatic segmentation can markedly improve efficiency and accuracy, which is significant for biomedical clinical diagnosis. With the rapid development of deep convolutional neural networks (DCNN), U-Net has been widely used in medical image segmentation due to its encoder-decoder structure and skip connection. However, it is still hard for U-Net to handle certain challenging cases. In this study, we propose an adaptive encoding and comprehensive attention decoding network (AA-Net), which is derived from U-Net to address the issues of the semantic gap as well as the loss of spatial information during convolutions. AA-Net takes into account the different characteristics of the encoder and decoder. In the encoder, we design a simple Adaptive Calibration Module (ACM) to improve the representation ability of candidate features. In the decoder, we introduce a Comprehensive Attention Feature Extraction (CAFE) module, which employs multiple attention mechanisms after feature fusion to alleviate the semantic gap. Benefiting from CAFE, AA-Net can better handle the challenging cases where the segmentation targets vary in position, size, and scale. Additionally, we suggest a weighted hybrid loss function for precise boundary segmentation. We validate the effectiveness of AA-Net and each component on three biomedical image datasets. The results demonstrate that our method outperforms state-of-the-art methods in different medical segmentation tasks, proving it is lightweight, efficient, and general.},
  archive      = {J_ASOC},
  author       = {Xin Shu and Aoping Zhang and Zhaoyang Xu and Feng Zhu and Wei Hua},
  doi          = {10.1016/j.asoc.2025.112990},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112990},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive encoding and comprehensive attention decoding network for medical image segmentation},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A nondominated sorting simplified swarm optimization with local search mechanisms for multi-objective vehicle routing problems with time windows. <em>ASOC</em>, <em>174</em>, 112989. (<a href='https://doi.org/10.1016/j.asoc.2025.112989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In addressing the complexities of modern logistics, this study introduces a novel multi-objective formulation for vehicle routing problems with time windows (MO-VRPTW), targeting minimizing travel distance, enhancing customer satisfaction, and equalizing driver workloads. We introduce an innovative hybrid multi-objective evolutionary algorithm (MOEA) leveraging nondominated sorting simplified swarm optimization to effectively merge the advantages of various optimization strategies. A key aspect of this advancement is the incorporation of the Lin−Kernighan − Helsgaun (LKH) heuristic, which delivers a superior initial solution, thereby markedly enhancing the speed of convergence. Additionally, we pioneered a local search method inspired by the A* algorithm designed to refine the search process's exploration and exploitation stages. Solomon's benchmark instances, a recognized standard in the VRPTW field, were used to validate our algorithm's effectiveness. Our algorithm demonstrated superior performance in addressing MO-VRPTW through meticulous statistical analysis, outperforming state-of-the-art algorithms, such as MOPSO, NSGA-II, MOEA/D, and SPEA2, regarding efficiency and solution diversity. This study not only advances algorithmic performance but also thoughtfully considers the interests of key supply chain stakeholders.},
  archive      = {J_ASOC},
  author       = {Chyh-Ming Lai and Chun-Chih Chiu and Tzu-Li Chen},
  doi          = {10.1016/j.asoc.2025.112989},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112989},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A nondominated sorting simplified swarm optimization with local search mechanisms for multi-objective vehicle routing problems with time windows},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cluster-based prepositioning network for enhanced recovery resilience of critical infrastructure system using multiplex network. <em>ASOC</em>, <em>174</em>, 112987. (<a href='https://doi.org/10.1016/j.asoc.2025.112987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient restoration of critical infrastructures for high-impact-low-frequency events heavily relies on pre-disaster restoration resource prepositioning and post-disaster assignment. To find proper locations of restoration resource depots and optimal assignment schemes, a multiplex network approach is proposed to simultaneously model the critical infrastructure network and restoration resource prepositioning network as a coupled multi-layered network. The proposed approach consists of load-weighted critical infrastructure network modeling, cluster-based restoration resource prepositioning network modeling and recovery resilience optimization of a multiplex network from the two networks. Tested on the Hangzhou metro network located in China, experimental results show that resilience loss optimized based on the proposed cluster-based multiplex network is lower than that based on four centrality-guaranteed competitors for all attack scenarios and significantly lower under random severe attacks representing the most severe scenarios. Link flow fluctuations and higher-order of resilience metrics are discussed to provide an insightful suggestion on the design and intervention decisions of infrastructure restoration.},
  archive      = {J_ASOC},
  author       = {Ying Wang and Ou Zhao and Limao Zhang},
  doi          = {10.1016/j.asoc.2025.112987},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112987},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cluster-based prepositioning network for enhanced recovery resilience of critical infrastructure system using multiplex network},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent reinforcement learning system framework based on topological networks in fourier space. <em>ASOC</em>, <em>174</em>, 112986. (<a href='https://doi.org/10.1016/j.asoc.2025.112986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, multi-agent reinforcement learning (MARL) has been applied to various domains such as communications, network management, power systems, and autonomous driving, showcasing broad application scenarios and significant research potential. However, in complex decision-making environments, agents that rely solely on temporal value functions often struggle to capture and extract hidden features and dependencies within long sequences in multi-agent settings. Each agent’s decisions are influenced by a sequence of prior states and actions, leading to complex spatiotemporal dependencies that are challenging to analyze directly in the time domain. Addressing these challenges requires a paradigm shift to analyze such dependencies from a novel perspective. To this end, we propose a Multi-Agent Reinforcement Learning system framework based on Fourier Topological Space from the foundational level. This method involves transforming each agent’s value function into the frequency domain for analysis. Additionally, we design a lightweight weight calculation method based on historical topological relationships in the Fourier topological space. This addresses issues of instability and poor reproducibility in attention weights, along with various other interpretability challenges. The effectiveness of this method is validated through experiments in complex environments such as the StarCraft Multi-Agent Challenge (SMAC) and Google Football. Furthermore, in the Non-monotonic Matrix Game, our method successfully overcame the limitations of non-monotonicity, further proving its wide applicability and superiority. On the application level, the proposed algorithm is also applicable to various multi-agent system domains, such as robotics and factory robotic arm control. The algorithm can control each joint in a coordinated manner to accomplish tasks such as enabling a robot to stand upright or controlling the movements of robotic arms.},
  archive      = {J_ASOC},
  author       = {Licheng Sun and Ao Ding and Hongbin Ma},
  doi          = {10.1016/j.asoc.2025.112986},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112986},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-agent reinforcement learning system framework based on topological networks in fourier space},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic light optimization in vertical farming using an IoT-driven digital twin framework and artificial intelligence. <em>ASOC</em>, <em>174</em>, 112985. (<a href='https://doi.org/10.1016/j.asoc.2025.112985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global agricultural sector faces mounting challenges from climate change, population growth, urbanization, and environmental degradation, necessitating innovative solutions to ensure food security. Urban and peri-urban agriculture, particularly vertical farming, offers a sustainable approach to increase food production while minimizing land use, reducing environmental impact, and enhancing resource efficiency. Unlike conventional vertical farming systems that rely on static spectral recipes with fixed light compositions (e.g., Red-to-Blue ratios derived from historical data), this study introduces an Internet of Things-enabled smart vertical farming system that leverages digital twin technology and a genetic algorithm (GA) to dynamically optimize lettuce growth by adjusting RGB LED spectra throughout the crop cycle. The system monitors and controls key environmental parameters within a growth tower, including temperature, humidity, and lighting. A digital twin facilitates real-time data exchange between physical and virtual components, while the GA iteratively refines the light composition. Over a 34-day cultivation period, the algorithm identified an optimal RGB configuration (R:211, G:169, B:243; maximum intensity: 255) that aligns with spectral values reported in literature for lettuce, despite not directly measuring photobiological metrics such as Photosynthetic Photon Flux Density. To our knowledge, this is the first study to implement a dynamic, GA-driven spectral optimization strategy in vertical farming. While the objective was not to surpass traditional static lighting recipes, the results validate that adaptive methods can reliably converge to established optima. The IoT platform demonstrated robust capabilities in data collection, processing, and actuation, underscoring the promise of adaptive lighting strategies for controlled agriculture. Future research will focus on incorporating additional spectra (e.g., deep red, ultraviolet), automating data collection via image recognition, and analyzing energy efficiency to enhance scalability.},
  archive      = {J_ASOC},
  author       = {Rafael Gomes Alves and Fábio Lima and Ítalo Moraes Rocha Guedes and Salvador Pinillos Gimenez},
  doi          = {10.1016/j.asoc.2025.112985},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112985},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic light optimization in vertical farming using an IoT-driven digital twin framework and artificial intelligence},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural priority model for agile earth observation satellite scheduling using deep reinforcement learning. <em>ASOC</em>, <em>174</em>, 112984. (<a href='https://doi.org/10.1016/j.asoc.2025.112984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The agile earth observation satellite scheduling problem (AEOSSP) is a time-dependent and complex combinatorial optimization challenge that has spurred extensive research for decades. Traditional methods have primarily relied on iterative searching processes to approximate near-optimal solutions, but their efficiency remains limited. To address this issue, we propose a Priority Construction Model (PCM) based on deep reinforcement learning (DRL), forming a learning-based, two-stage construction heuristic. The PCM integrates a Priority Construction Neural Network (PCNN) alongside a Backward-Slacken and Top-Insert (BS-TI) scheduling algorithm. In PCM, the PCNN sequences observation requests, while the BS-TI schedules each sequenced request in accordance with specific constraints, thus freeing the neural policy from the burden of complex constraint checking. Experimental results indicate that following a policy-gradient-based DRL training process, PCM outperforms the state-of-the-art AEOSSP iterative algorithm, achieving better average profits within an exceptionally short construction time in most scenarios. The model study further reveals that PCNN outperforms other DRL policies in terms of priority policy representation, while the PCM exhibits superior generalization capabilities across varying scales and distributions. Therefore, our proposed model presents a valuable reference solution that not only meets the large-scale and rapid response requirements of the AEOSSP but also holds potential for application in upcoming large constellations and emerging management paradigms. More importantly, we introduce a novel framework that separates the DRL optimization process from constraint management, lowering the entry barrier for applying DRL to complex problems. This makes the model adaptable to various optimization challenges in engineering and operations research, thus extending its applicability beyond the AEOSSP domain.},
  archive      = {J_ASOC},
  author       = {Ming Chen and Luona Wei and Jie Chun and Lei He and Shang Xiang and Lining Xing and Yingwu Chen},
  doi          = {10.1016/j.asoc.2025.112984},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112984},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A neural priority model for agile earth observation satellite scheduling using deep reinforcement learning},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transferable adversarial attacks against face recognition using surrogate model fine-tuning. <em>ASOC</em>, <em>174</em>, 112983. (<a href='https://doi.org/10.1016/j.asoc.2025.112983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks have significantly advanced Face Recognition performance yet remain susceptible to adversarial attacks, posing significant security and user privacy threats in real-world applications. In recent years, black box attacks have attracted wide attention to craft highly transferable adversarial examples by training surrogate models. However, most of these methods primarily depend on stealing knowledge by accessing the soft label from the target model using either synthetic training data or data free without awareness of the knowledge type, which can affect the improvement of transferability between the surrogate and the target models. Additionally, these attacks still need to improve the surrogate model’s accuracy without using many queries. To this end, we propose Tune2Transfer, a novel attack method that enhances adversarial transferability by fine-tuning the surrogate model with different types of knowledge with limited queries on the target model by the hard label only. Specifically, it collects a small face image dataset, considering the adversary’s limited knowledge. To overcome the challenge of knowledge type, Tune2Transfer imposes three sampling assumptions: clean images only, the perturbed images, or combining both, generating images on the surrogate model, and then feeding them to the target model to obtain the hard label. The perturbed images are generated by perturbing them using the Covariance Matrix Adaptation Evolution Strategy or Momentum Iteration Fast Gradient Sign Method. Besides, we leverage pre-trained models to fine-tune surrogate models to avoid large queries. In this way, we could leverage knowledge transferred from the target model, resulting in superior transferability. Extensive experiments conducted on two typical datasets demonstrate the efficacy of Tune2Transfer, increasing the attack success rates significantly.},
  archive      = {J_ASOC},
  author       = {Yasmeen M. Khedr and Xin Liu and Haobo Lu and Kun He},
  doi          = {10.1016/j.asoc.2025.112983},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112983},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transferable adversarial attacks against face recognition using surrogate model fine-tuning},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semi-supervised non-negative matrix factorization model for scRNA-seq data analysis. <em>ASOC</em>, <em>174</em>, 112982. (<a href='https://doi.org/10.1016/j.asoc.2025.112982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-cell RNA sequencing (scRNA-seq) technology enables the measurement of cellular gene expression at the single-cell level, thus facilitating cell clustering at the gene level. Despite numerous dimensionality reduction methods developed for scRNA-seq data, many are limited to analyzing individual gene expression matrices and struggle to address false positives and false zero expression entries effectively. Moreover, existing methods often underutilize prior knowledge of similarity and dissimilarity between multi-omics data, leading to the loss of intercellular correlations and shared structural information, thus hindering desired dimensionality reduction outcomes. To address these limitations, a novel model termed joint non-negative matrix factorization with similarity and dissimilarity constraints (SDJNMF) was proposed to tailor for scRNA-seq data clustering. The model leverages prior knowledge of similarity and dissimilarity across multiple gene expression matrices, facilitating joint non-negative matrix factorization to extract common features from multi-omics data. By preserving shared structural and cellular relevance information, SDJNMF enhances the clustering of similar cells while effectively separating dissimilar ones. Furthermore, the SDJNMF model incorporates sparse Singular Value Decomposition during initialization to mitigate noise and redundancy and ensure robust dimensionality reduction. The experimental results demonstrate that the SDJNMF model exhibits superior performance on the 10 datasets, not only outperforming the other 14 algorithms in terms of clustering accuracy on the 9 datasets, but also enhancing the A R I of SDJNMF by an average of 0.0687 in comparison to the second-best algorithm on each dataset. In the visual representation, the model is able to efficiently and accurately cluster similar cells and effectively discriminate different classes of cells from each other. Additionally, the SDJNMF model was applied to identify informative genes and conduct enrichment analysis, validating that genes identified by SDJNMF significantly influence biological processes. Overall, the SDJNMF offers innovative tools for cell cluster identification and advances biological research. The source code of SDJNMF is available online at https://github.com/Jindsmu/SDJNMF .},
  archive      = {J_ASOC},
  author       = {Junjie Lan and Xiaoling Zhuo and Siman Ye and Jin Deng},
  doi          = {10.1016/j.asoc.2025.112982},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112982},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A semi-supervised non-negative matrix factorization model for scRNA-seq data analysis},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdvGrid: A multi-view black-box attack on infrared pedestrian detectors in the physical world. <em>ASOC</em>, <em>174</em>, 112981. (<a href='https://doi.org/10.1016/j.asoc.2025.112981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical adversarial attacks in the visible spectrum have been extensively studied, but research on infrared attacks remains limited. Infrared pedestrian detectors are crucial for modern applications yet vulnerable to adversarial attacks, posing significant security risks. Existing methods using physical perturbations like light bulb arrays or hot/cold patches for black-box attacks have shown limitations in practicality and multi-view support. To address these challenges, we introduce Adversarial Infrared Grid (AdvGrid), a novel approach that models perturbations in a grid format and employs a genetic algorithm for black-box optimization. AdvGrid cyclically applies perturbations to various parts of a pedestrian’s clothing, enabling effective multi-view black-box attacks on infrared detectors. Our extensive experiments demonstrate AdvGrid’s superior performance: Effectiveness: Achieves 80.00% attack success rate in digital environments and 91.86% in physical environments. Stealthiness: Maintains high stealthiness, making it difficult for observers to identify the adversarial patterns. Robustness: Exceeds 50% average attack success rate against mainstream detectors, showcasing its robustness across different scenarios. We also conduct ablation studies, transfer attacks, and adversarial defense evaluations, further confirming AdvGrid’s superiority over baseline methods. Our findings highlight AdvGrid as a powerful tool for advancing the understanding and mitigation of adversarial threats in infrared detection systems.},
  archive      = {J_ASOC},
  author       = {Kalibinuer Tiliwalidi and Chengyin Hu and Guangxi Lu and Ming Jia and Weiwen Shi},
  doi          = {10.1016/j.asoc.2025.112981},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112981},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AdvGrid: A multi-view black-box attack on infrared pedestrian detectors in the physical world},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Series clustering and dynamic periodic patching-based transformer for multivariate time series forecasting. <em>ASOC</em>, <em>174</em>, 112980. (<a href='https://doi.org/10.1016/j.asoc.2025.112980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series forecasting (MTSF) is widely employed in research-intensive domains, such as weather forecasting. Recently, Transformer-based models have outstanding ability to achieve SOTA performance, benefiting from its self-attention mechanism. However, existing models fall short in capturing multivariate inter-dependencies and local semantic representations. To tackle the above limitations, we propose a series clustering and dynamic periodic patching-based Transformer model named CMDPPformer, with two distinctive characteristics: (1) A channel-mixing module based on series clustering is proposed which can strengthen the association between variables with high sequence similarity, and weaken the effect of uncorrelated variables. Concretely, we use whole-time series clustering to group multivariate time series into clusters. After that, variables in the same cluster share the same Transformer backbone while variables in different clusters do not affect each other. (2) A dynamic periodic patching module is introduced which can better capture semantic information and improve Transformer’s local semantic representation. Concretely, multivariate time series after clustering are dynamically segmented into periodic patches as Transformer’s input token. Experimental results show that CMDPPformer can achieve an overall 13.76% and 10.16% relative improvements than SOTA Transformer-based models on seven benchmarks, covering four real-world applications: energy, weather, illness and economic.},
  archive      = {J_ASOC},
  author       = {Yijie Wang and Xiao Wu and Jiaying Zhang and Weiping Wang and Linjiang Zheng and Jiaxing Shang},
  doi          = {10.1016/j.asoc.2025.112980},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112980},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Series clustering and dynamic periodic patching-based transformer for multivariate time series forecasting},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic trend fusion module for traffic flow prediction. <em>ASOC</em>, <em>174</em>, 112979. (<a href='https://doi.org/10.1016/j.asoc.2025.112979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction is essential for applications like transport logistics but remains challenging due to complex spatio-temporal correlations and non-linear traffic patterns. Existing methods often model spatial and temporal dependencies separately, failing to effectively fuse them. To overcome this limitation, the D ynamic S patial- T emporal T rend Trans former ( DST 2 former ) is proposed to capture spatio-temporal correlations through adaptive embedding and to fuse dynamic and static information for learning multi-view dynamic features of traffic networks. The approach employs the D ynamic T rend R epresentation Trans former ( DTRformer ) to generate dynamic trends using encoders for both temporal and spatial dimensions, fused via Cross Spatial-Temporal Attention. Predefined graphs are compressed into a representation graph to extract static attributes and reduce redundancy. Experiments on four real-world traffic datasets demonstrate that our framework achieves state-of-the-art performance.},
  archive      = {J_ASOC},
  author       = {Jing Chen and Haocheng Ye and Zhian Ying and Yuntao Sun and Wenqiang Xu},
  doi          = {10.1016/j.asoc.2025.112979},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112979},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic trend fusion module for traffic flow prediction},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing a forecasting model for time series based on clustering and deep learning algorithms. <em>ASOC</em>, <em>174</em>, 112977. (<a href='https://doi.org/10.1016/j.asoc.2025.112977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a new forecasting model for time series based on the improvement and combination of the cluster analysis (CA) algorithm and deep learning with Convolutional Neural Network (CNN) and Bi-Long Short Term Memory (BiLSTM) model. The proposed model is considered pioneering in this research direction with significant contributions to three main phases. For the first phase, the original series is converted into the percentage change series and is divided into clusters of an appropriate number using the CA algorithm. The next phase involves extracting the features of the new series based on the CNN with suitable parameters and input data enhancement from the results of the first phase. In the final phase, the BiLSTM model is applied to the series established from the second phase, and the forecasting principle for the future is established. The proposed model is detailed in the implementation steps, proving convergence, illustrated by numerical examples, and can be applied to real series using a Matlab procedure. The effectiveness of the proposed model is quite impressive as it surpasses many strong forecasting models on reputable benchmark datasets , including the M3-Competition dataset with 3,003 series, and M4-Competition dataset with 100,000 series.},
  archive      = {J_ASOC},
  author       = {Luan Nguyen-Huynh and Tai Vo-Van},
  doi          = {10.1016/j.asoc.2025.112977},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112977},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Developing a forecasting model for time series based on clustering and deep learning algorithms},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of international market entry strategies for mineral oil companies using a neutrosophic SWARA-CRADIS methodology. <em>ASOC</em>, <em>174</em>, 112976. (<a href='https://doi.org/10.1016/j.asoc.2025.112976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Businesses encounter risks when entering new countries, but there are also opportunities. The strategy a business employs when opting to enter a new market is directly tied to its success. In this regard, the study provides an approach for evaluating new market entry strategies for businesses facilitating in the mineral oil sector and manufacturing industries. The approach includes the development of the type 2 neutrosophic step-wise weight assessment ratio analysis (SWARA)-compromise ranking of alternatives from distance to ideal solution (CRADIS) methodology, which aims to solve the problem by determining the candidate strategies and the criteria to be utilized in their evaluation. The findings revealed that market conditions are the most crucial criterion in selecting strategies for mineral oil companies intending to enter new markets. The magnitude and development potential of the new market to be entered, as well as the status of the actors, all have an impact on market conditions, which are critical for businesses. Moreover, foreign direct investment is found to be the best market entry strategy. This strategy arises because businesses aim to maximize the potential in the market they have recently entered, as well as other factors such as government incentives. The study is expected to benefit production enterprises, the mineral oil sector, the marketing field, and the literature by identifying criteria and option sets, finding the importance degrees of the criteria, selecting the ideal entry strategy, and proposing a methodology for handling uncertain data.},
  archive      = {J_ASOC},
  author       = {Ahmet Aytekin and Hilal Öztürk Küçük and Makbule Aytekin and Vladimir Simic and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2025.112976},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112976},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of international market entry strategies for mineral oil companies using a neutrosophic SWARA-CRADIS methodology},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic environment adaptive online learning with fairness awareness via dual disentanglement. <em>ASOC</em>, <em>174</em>, 112975. (<a href='https://doi.org/10.1016/j.asoc.2025.112975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of Artificial Intelligence (AI) comes with the necessity to consider and mitigate discrimination in machine learning algorithms. Most existing fair machine learning methods are only suitable for short-term and static scenarios, and thus cannot adapt to dynamically changing environments or meet the needs for real-time updates. In open dynamic scenarios, data arriving in batches needs processing in real-time, and the constantly changing environment will lead to data distribution shifts, making it difficult to ensure the fairness of models in the long run. To achieve long-term fairness of models, we propose an online dual disentanglement method that captures fair representations of non-sensitive core information in real-time within constantly changing environments, thereby enhancing the robustness of fair models. Firstly, learned representations are disentangled from environment-specific variation factors through a constrained optimization setup to ensure semantic invariance. Further, a bias disentanglement method based on supervised contrastive learning is designed. While keeping the non-sensitive core information unchanged, the sensitive information is hidden from semantic representations and the spurious correlation with target labels is cut off, so as to achieve the long-term fairness of the model decision. By formulating the fairness-aware online learning problem in dynamic environments as an online optimization problem with the long-term fairness constraint, and theoretically proving that the algorithm achieves sublinear dynamic regret and sublinear violation of cumulative unfairness under certain assumptions. Experimental evaluations on real-world datasets demonstrate the effectiveness of the proposed method, which maintains overall fairness above 80% without compromising utility, outperforming state-of-the-art baseline methods.},
  archive      = {J_ASOC},
  author       = {Qiuling Chen and Ayong Ye and Chuan Huang and Fengyu Wu},
  doi          = {10.1016/j.asoc.2025.112975},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112975},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic environment adaptive online learning with fairness awareness via dual disentanglement},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring coordinated motion patterns of facial landmarks for deepfake video detection. <em>ASOC</em>, <em>174</em>, 112974. (<a href='https://doi.org/10.1016/j.asoc.2025.112974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rich geometric and motion information they contain, recent studies indicate that facial landmark clues have significant potential for deepfake video detection. In this paper, we make a key observation that there exist coordinated motions among different facial landmarks for real individuals. While the forgery methods focus more on appearance realism, thus likely to disrupt the underlying coordinated motion patterns. Inspired by this observation, this paper explores how to leverage coordinated motion patterns among facial landmarks to enhance deepfake detection. First, we introduce a coordinated motion landmarks mining strategy (CMLMS), to effectively identify correlated landmarks. Utilizing these correlations, we propose a landmark temporal dynamic relation module (LTDRM), which focuses on the coordinated motion patterns between landmarks while extracting their spatiotemporal features. Specifically, LTDRM constructs an adjacency matrix based on the correlated landmarks and uses graph convolution to selectively aggregate information between correlated landmarks. Additionally, LTDRM is a plug-and-play module and can boost the performance of existing deepfake detection methods with minimal computational overhead. Experimental results validate the effectiveness and generalizability of our method.},
  archive      = {J_ASOC},
  author       = {Yue Zhang and Run Niu and Xianlin Zhang and Siqi Chen and Mingdao Wang and Xueming Li},
  doi          = {10.1016/j.asoc.2025.112974},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112974},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exploring coordinated motion patterns of facial landmarks for deepfake video detection},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative fuzzer-driven vulnerability detection in the internet of things networks. <em>ASOC</em>, <em>174</em>, 112973. (<a href='https://doi.org/10.1016/j.asoc.2025.112973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) paradigm has displayed tremendous growth in recent years, driving innovations such as Industry 4.0 and the creation of smart environments that enhance efficiency and asset management and enable intelligent decision-making. However, these benefits come with considerable cybersecurity risks due to inherent vulnerabilities within IoT ecosystems. Introducing potentially vulnerable IoT devices into secure environments, like smart airports, introduces new attack surfaces and vectors for exploitation. Identifying such vulnerabilities is challenging, and while traditional methods like penetration testing and vulnerability identification offer solutions, they often fall short due to IoT’s unique data diversity, hardware constraints, and complexity. We propose an intelligent mutation-based fuzzer for IoT vulnerability detection in networks to address these limitations, demonstrated through a smart airport case study. This method leverages Generative Adversarial Network (GAN)-based mutation, utilizing legitimate network communications (i.e., payloads) to produce fuzzed payloads that expose vulnerabilities. Additionally, we incorporate a large language model (LLM)-based risk assessment framework to evaluate the likelihood and impact of identified vulnerabilities, which is crucial for effectively prioritizing threats in interconnected IoT environments. This dual approach of vulnerability detection and LLM-driven risk assessment provides comprehensive insights into IoT security, enabling prioritized response actions. Experiments conducted in the UNSW Canberra IoT testbed confirm that our approach outperforms conventional vulnerability identification methods, offering a scalable solution for effective vulnerability detection and risk prioritization in complex IoT networks.},
  archive      = {J_ASOC},
  author       = {Mohammed Tanvir Masud and Nickolaos Koroniotis and Marwa Keshk and Benjamin Turnbull and Shabnam Kasra Kermanshahi and Nour Moustafa},
  doi          = {10.1016/j.asoc.2025.112973},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112973},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generative fuzzer-driven vulnerability detection in the internet of things networks},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Target temperature field prediction via a thermodynamic knowledge-based artificial neural network. <em>ASOC</em>, <em>174</em>, 112972. (<a href='https://doi.org/10.1016/j.asoc.2025.112972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of artificial intelligence, representation-supervised neural networks have been widely used in the fast solution of physical field. However, a large number of temperature prediction networks do not take environmental parameters into account, or only use parameters as simple input conditions, which greatly reduces the accuracy of their results. This paper proposes an accurate and low-cost method for adding the conditional parameters to intelligent prediction networks. A novel parameter encoder block is designed based on the heat transfer theory achieving thermodynamic knowledge-based parameter feature extraction. Meanwhile, an improved method for inputting time condition is proposed to characterize the temporal characteristics, which can reduce the requirement of dataset for transient temperature prediction, compared with LSTM. In addition, a thermal loss for temperature images is introduced to accelerate the convergence process in the model. Moreover, a CycleGAN-based temperature prediction network (CBTPN) is constructed for fast temperature prediction of a cube or different tanks. Temperature or infrared images predicted by our network exhibit MAE of less than 2.33 % and SSIM of more than 80.21 %. By embedding physical mechanisms into neural networks, this study this study pioneers a structured approach to refining physical parameters into thermodynamic knowledge-based signals for improved image generation, addressing the accuracy and efficiency limitations of data-driven algorithms caused by their insufficient understanding of parameter mechanisms. Finally, parameter cognitive evaluation proves that our approach can not only recognize the accurate semantics of heat transfer parameters, but also sense the meteorological laws.},
  archive      = {J_ASOC},
  author       = {Jincheng Chen and Feiding Zhu and Yuge Han and Dengfeng Ren},
  doi          = {10.1016/j.asoc.2025.112972},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112972},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Target temperature field prediction via a thermodynamic knowledge-based artificial neural network},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning in produce perception of harvesting robots: A comprehensive review. <em>ASOC</em>, <em>174</em>, 112971. (<a href='https://doi.org/10.1016/j.asoc.2025.112971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the global demand for produce has surged, alongside labor shortages, driving the development of agricultural automation, particularly in harvesting robots. Deep learning-based computer vision algorithms have become key to produce perception, demonstrating significant potential. We systematically review the current application of deep learning in produce perception for harvesting robots, providing an in-depth analysis of existing public datasets, with a focus on 2D produce recognition and 3D produce localization. Furthermore, we review and analyze the existing algorithms, highlighting their limitations and challenges. In addition, we explore future research directions of deep learning in produce perception, aiming to promote the continued advancement and innovation of technologies in this area.},
  archive      = {J_ASOC},
  author       = {Yuhao Jin and Xiaoyu Xia and Qizhong Gao and Yong Yue and Eng Gee Lim and Prudence Wong and Weiping Ding and Xiaohui Zhu},
  doi          = {10.1016/j.asoc.2025.112971},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112971},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning in produce perception of harvesting robots: A comprehensive review},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unmanned aerial vehicle takeoff point search algorithm with information sharing strategy of random trees for multi-area coverage task. <em>ASOC</em>, <em>174</em>, 112970. (<a href='https://doi.org/10.1016/j.asoc.2025.112970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel approach to optimize full-coverage search in distributed task areas using a single Unmanned Ground Vehicle (UGV) to deliver an Unmanned Aerial Vehicle (UAV) to the takeoff points of each task area along the shortest possible path. Unlike the traditional Traveling Salesman Problem (TSP), task areas are not fixed nodes, and obstacles must be considered. To address these challenges, a probability-based Rapid-exploration Random Tree ( p -RRT) with an information-sharing strategy is introduced, significantly improving the efficiency of locating takeoff points in complex environments. A dual optimization method further reduces the number of nodes and path length planned by the D* algorithm, achieving up to an 80 % reduction in nodes and improving path efficiency. Additionally, a simulated annealing (SA) algorithm optimizes the connection sequence of takeoff points, reducing total path length by 35.05 % compared to the initial path and 22.66 % compared to the traditional Random Sampling Method (RSM). Experiments confirm that the proposed algorithms can effectively enhance UGV-UAV collaboration with reducing path complexity and improving energy efficiency, and thus streamline multi-area coverage tasks.},
  archive      = {J_ASOC},
  author       = {Shouwen Yao and Xiaoyu Wang and Siqi Huang and Renjie Xu and Yinghua Zhao},
  doi          = {10.1016/j.asoc.2025.112970},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112970},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unmanned aerial vehicle takeoff point search algorithm with information sharing strategy of random trees for multi-area coverage task},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive heuristic algorithm with a collaborative search framework for multi-UAV inspection planning. <em>ASOC</em>, <em>174</em>, 112969. (<a href='https://doi.org/10.1016/j.asoc.2025.112969'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-UAV inspection path planning has become an important research topic for completing inspection tasks before the data acquisition deadline. In this study, we propose an adaptive heuristic algorithm with a collaborative search framework named Sa-VCO to solve the multi-UAV inspection path planning problem. Our study includes three main novelties. First, we design a region-gridding disperse approach that transforms the primitive target regions into a set of standard target subregions, allowing the target regions with greater costs to be inspected by multiple UAVs. Second, we propose an adaptive initial solution generation strategy using the information of graph structure constructed by all targets to reduce redundant computing. Third, we established a collaborative search framework to enhance search efficiency and increase population diversity. A large number of multiple-perspective comparative experiments are provided to test Sa-VCO's performance, and the comparison results demonstrate that Sa-VCO achieves better results than other advanced algorithms, especially on large-scale data sets.},
  archive      = {J_ASOC},
  author       = {Chang He and Haibin Ouyang and Weiqing Huang and Steven Li and Chunliang Zhang and Weiping Ding and Zhi-Hui Zhan},
  doi          = {10.1016/j.asoc.2025.112969},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112969},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive heuristic algorithm with a collaborative search framework for multi-UAV inspection planning},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficiency analysis of binary metaheuristic optimization algorithms for uncapacitated facility location problems. <em>ASOC</em>, <em>174</em>, 112968. (<a href='https://doi.org/10.1016/j.asoc.2025.112968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces binary adaptations of four metaheuristic optimization algorithms: the Binary Coati Optimization Algorithm (BCOA), Binary Mexican Axolotl Optimization Algorithm (BMAO), Binary Dynamic Hunting Leadership Optimization (BDHL), and Binary Aquila Optimizer (BAO). These algorithms were evaluated for their effectiveness in solving Uncapacitated Facility Location (UFL) problems, which aim to minimize total costs associated with customer-facility allocations and facility opening expenses by determining the optimal number of open facilities. Using 15 UFL problem instances from the OR-Lib dataset, the study assessed algorithm performance across 17 transfer functions (TFs), including S-shaped, V-shaped, and other variants, to address the binary nature of these problems. Performance metrics such as the best, worst, average, standard deviation, and GAP values were analyzed for each binary algorithm. Additionally, statistical analyses were conducted to further assess algorithmic performance. The Kolmogorov-Smirnov (KS) normality test was applied to determine the distribution characteristics of the results, followed by either ANOVA or Kruskal-Wallis tests, depending on the normality of the distributions. These statistical tests revealed significant differences in algorithm performance across different problem instances. Rank values were calculated based on GAP values and CPU times to facilitate comparisons across algorithm versions for the 15 UFL problems. Results underscored the critical role of TF selection in optimizing algorithm efficiency: BCOA performed best with TF11, BMAO with TF16 and TF17, BAO with TF10, and BDHL with TF15. Finally, a performance comparison on GAP values was conducted with two state-of-the-art PSO variants adapted for binary optimization. The proposed algorithms demonstrated either superior or competitive performance in solving UFL problems, validating their efficacy in complex optimization tasks and highlighting the influence of TFs on their performance.},
  archive      = {J_ASOC},
  author       = {Tahir Sag and Aysegul Ihsan},
  doi          = {10.1016/j.asoc.2025.112968},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112968},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficiency analysis of binary metaheuristic optimization algorithms for uncapacitated facility location problems},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph convolutional networks with multi-scale dynamics for traffic speed forecasting. <em>ASOC</em>, <em>174</em>, 112966. (<a href='https://doi.org/10.1016/j.asoc.2025.112966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic speed forecasting remains challenging due to complex and variable road conditions. Prior research often overlooks both coarse-grained and fine-grained features in traffic data, hindering a comprehensive capture of traffic data's temporal dependencies. While graph convolutional networks (GCNs) are commonly employed to extract spatial dependencies in traffic networks, they typically view these networks from a static standpoint, failing to consider the dynamic nature of traffic network structures. This limitation restricts their effectiveness in modeling traffic networks. To address these issues, this study introduces a novel deep learning-based spatial-temporal model for precise traffic speed forecasting. This model incorporates a newly developed multi-scale transformation method, which enhances the coarse-grained and fine-grained features in traffic speed data by transforming and fusing traffic speed data, and enabling a more thorough modeling of its temporal dependencies. Additionally, we propose an innovative graph interaction strategy, combines the generated graphs with a dynamic graph convolutional network, to effectively mine the dynamic characteristics of traffic network structures, thereby enhancing the model's accuracy. Extensive experiments on two real-world datasets have demonstrated the robustness and superior performance of the proposed model, with improvements ranging from 2.2 % to 6.1 % over state-of-the-art baselines.},
  archive      = {J_ASOC},
  author       = {Dongping Zhang and Hao Lan and Mengting Wang and Jiabin Yu and Xinghao Jiang and Shifeng Zhang},
  doi          = {10.1016/j.asoc.2025.112966},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112966},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph convolutional networks with multi-scale dynamics for traffic speed forecasting},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LOSS-GAT: Label propagation and one-class semi-supervised graph attention network for fake news detection. <em>ASOC</em>, <em>174</em>, 112965. (<a href='https://doi.org/10.1016/j.asoc.2025.112965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s world of social networks, fake news spreads quickly and causes serious problems. This has made it crucial to develop automated systems to detect and combat disinformation. Machine learning and deep learning are often used to identify fake news, but they struggle due to the lack of labeled news datasets. To address this, the One-Class Learning (OCL) approach uses a small set of labeled data. On the other hand, representing data as a graph enables access to diverse content and structural information, and label propagation methods on graphs can be effective in predicting node labels. In this paper, we adopt a graph-based model for data representation and introduce a semi-supervised and one-class approach for fake news detection, called LOSS-GAT. Initially, we employ a two-step label propagation algorithm, utilizing Graph Neural Networks (GNNs) as an initial classifier to categorize news into two groups: interest (fake) and non-interest (real). Subsequently, we enhance the graph structure using structural augmentation techniques. Ultimately, we predict the final labels for all unlabeled data using a GNN that induces randomness within the local neighborhood of nodes through the aggregation function. We evaluate our proposed method on six common datasets and compare the results against a set of baseline models, including both OCL and binary labeled models. The results demonstrate that LOSS-GAT achieves a significant improvement in performance, with enhancements ranging from 5% (on the FEVER dataset) to 20% (on the FakeNewsNet dataset) in terms of the Macro-F1 metric, all while utilizing only a limited set of labeled fake news data. Noteworthy, LOSS-GAT even outperforms binary labeled models.},
  archive      = {J_ASOC},
  author       = {Batool Lakzaei and Mostafa Haghir Chehreghani and Alireza Bagheri},
  doi          = {10.1016/j.asoc.2025.112965},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112965},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LOSS-GAT: Label propagation and one-class semi-supervised graph attention network for fake news detection},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Art style classification via self-supervised dual-teacher knowledge distillation. <em>ASOC</em>, <em>174</em>, 112964. (<a href='https://doi.org/10.1016/j.asoc.2025.112964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Art style classification plays a crucial role in computational aesthetics. Traditional deep learning-based methods for art style classification typically require a large number of labeled images, which are scarce in the art domain. To address this challenge, we propose a self-supervised learning method specifically tailored for art style classification. Our method effectively learns image style features using unlabeled images. Specifically, we introduce a novel self-supervised learning approach based on the popular contrastive learning framework, incorporating a unique dual-teacher knowledge distillation technique. The two teacher networks provide complementary guidance to the student network. Each teacher network focuses on extracting distinct features, offering diverse perspectives. This collaborative guidance enables the student network to learn detailed and robust representations of art style attributes. Furthermore, recognizing the Gram matrix’s capability to capture image style through feature correlations, we explicitly integrate it into our self-supervised learning framework. We propose a relation alignment loss to train the network, leveraging image relationships. This loss function has shown promising results compared to the commonly used InfoNCE loss. To validate our proposed method, we conducted extensive experiments on three publicly available datasets: WikiArt, Pandora18k, and Flickr. The experimental results demonstrate the superiority of our method, significantly outperforming state-of-the-art self-supervised learning methods. Additionally, when compared with supervised methods, our approach shows competitive results, notably surpassing supervised learning methods on the Flickr dataset. Ablation experiments further verify the efficacy of each component of our proposed network. The code is publicly available at: https://github.com/lm-oc/dual_signal_gram_matrix .},
  archive      = {J_ASOC},
  author       = {Mei Luo and Li Liu and Yue Lu and Ching Y. Suen},
  doi          = {10.1016/j.asoc.2025.112964},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112964},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Art style classification via self-supervised dual-teacher knowledge distillation},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Saccade inspired attentive visual patch transformer for image sentiment analysis. <em>ASOC</em>, <em>174</em>, 112963. (<a href='https://doi.org/10.1016/j.asoc.2025.112963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generation of image-evoked emotion is usually regarded as a transient process in the image sentiment analysis. However, according to the saccade mechanism of the human visual system, the evoked emotion generated during the saccade process changes over time and attention. Based on above analysis, we propose an Attentive Visual Patch Transformer (AVPT), using visual attention sequence to represent the sentiment context of images and predict the possible distribution of sentiment. In AVPT, the spatial structure in the form of patches are reconstructed and reorganized by visual attention shift sequentially. Simultaneously, the temporal characteristics of attention shift are introduced to the relative position encoding, and merged in a self-attention manner to form a spatial–temporal process similarly to the human visual system. Specifically, we propose a sequence attention shift module to simulate the saccade process, which obtains sequence attention and reduces the computational effort by group attentive convolutional gate recurrent unit. Then, a spatial–temporal correlation encoder module is proposed to encode temporal attention with spatial visual features and obtain the sequential visual features of saccade. Finally, a self-attention fusion module is used to extract the correlation hidden in the relative encoding features. Our proposed AVPT achieves excellent performance on visual sentiment distribution prediction and is comparable to state-of-the-art methods, as demonstrated by extensive experiments on the Flickr_LDL and Twitter_LDL datasets.},
  archive      = {J_ASOC},
  author       = {Jing Zhang and Jixiang Zhu and Han Sun and Xinzhou Zhang and Jiangpei Liu},
  doi          = {10.1016/j.asoc.2025.112963},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112963},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Saccade inspired attentive visual patch transformer for image sentiment analysis},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual linear latent space constrained generative adversarial networks for hyperspectral image classification. <em>ASOC</em>, <em>174</em>, 112962. (<a href='https://doi.org/10.1016/j.asoc.2025.112962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image classification is a critical challenge in remote sensing due to the high dimensionality of the data and the scarcity of labeled samples. In this study, we propose a novel Dual-Line Latent Space Constrained Generative Adversarial Network (DLC-GAN) that integrates spatial and spectral feature extraction through dual pathways and incorporates latent space constraints to enhance classification robustness. Unlike existing methods, the DLC-GAN employs a bilinear structure to extract complementary information, improving both feature representation and classification accuracy. The model was evaluated on benchmark datasets such as Indian Pines, Pavia University, and Salinas, achieving state-of-the-art performance. Specifically, the DLC-GAN demonstrated improvements in overall accuracy by 5–11 % compared to recent methods and exhibited superior adaptability to limited training data. These findings underscore the potential of DLC-GAN in addressing critical challenges in hyperspectral image classification, with promising applications in environmental monitoring, agricultural management, and urban planning.},
  archive      = {J_ASOC},
  author       = {Kefen Mou and Sha Gao and Muhammet Deveci and Seifedine Kadry},
  doi          = {10.1016/j.asoc.2025.112962},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112962},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual linear latent space constrained generative adversarial networks for hyperspectral image classification},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-block ladder-style transformer model with multi-subspace feature adjustment for object re-identification. <em>ASOC</em>, <em>174</em>, 112961. (<a href='https://doi.org/10.1016/j.asoc.2025.112961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object re-identification aims to retrieve specific objects across multiple cameras and has garnered significant attention. Currently, transformer-based methods have taken a dominant position. However, most approaches embed inherent transformer encoders for feature extraction directly. These methods handle all patch tokens uniformly, failing to distinguish salient and non-salient patch tokens for discriminative feature expression. To this end, this work proposes a novel inter-block ladder-style transformer (IBLSFormer) for object re-identification. Firstly, a multi-subspace feature adjustment (MSFA) module is designed to adjust the patch features via class-patch interaction in multiple subspaces including Euclidean distance subspace, cosine distance subspace, and KL divergence subspace. The MSFA module can enhance the salient patch tokens and weaken the non-salient patch tokens simultaneously to focus on discriminative patches. Afterwards, an IBLSFormer is designed by inserting MSFA modules with distinct configurations into the vision transformer. The narrow-to-wide ladder-style constraints are embedded in MSFA modules based on embedding depth to highlight the feature differences across different levels and ameliorate the feature learning. Our method achieves mAP/Rank-1 of 88.7%/95.3%, 81.4%/90.4%, 80.0%/97.1%, and 89.4%/83.6% on four object re-identification datasets. Extensive experiments show IBLSFormer is superior to other methods in learning discriminative and robust representations for object re-identification.},
  archive      = {J_ASOC},
  author       = {Zhi Yu and Zhiyong Huang and Mingyang Hou and Jiaming Pei and Daming Sun},
  doi          = {10.1016/j.asoc.2025.112961},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112961},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Inter-block ladder-style transformer model with multi-subspace feature adjustment for object re-identification},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometrical invariant generative invisible hyperlinks based on feature points. <em>ASOC</em>, <em>174</em>, 112959. (<a href='https://doi.org/10.1016/j.asoc.2025.112959'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the visual diversity of Quick Response (QR) codes while ensuring their robust decoding capabilities, this paper introduces an innovative invisible hyperlink generation system. The system can use a message sequence to directly generate a hyperlink image. By harnessing the latent space of a suggested feature point generation network, the system extends the robustness of image feature points to the hyperlink images it generates. Specially, an image generation network is first designed to synthesize high-quality images based on feature point data. Subsequently, a set of lightweight message encoder and decoder are introduced to embed message bits into the latent space of the image generation network. Experimental results show that the proposed invisible hyperlink generation system can successfully generate images containing hyperlinks, exhibiting remarkable resilience against common signal processing and geometric distortions. It harbors diverse potential applications, encompassing website URLs, contact information, product specifics, and numerous other use cases.},
  archive      = {J_ASOC},
  author       = {Zecheng Peng and Bingwen Feng and Xiaotao Xu and Jilian Zhang and Donghong Cai and Wei Lu},
  doi          = {10.1016/j.asoc.2025.112959},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112959},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Geometrical invariant generative invisible hyperlinks based on feature points},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentiment analysis and emotion recognition in social media: A comprehensive survey. <em>ASOC</em>, <em>174</em>, 112958. (<a href='https://doi.org/10.1016/j.asoc.2025.112958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment Analysis (SA) and emotion recognition is the fundamental dialogue system that recently gained more attention. It is applied in many scenarios like mining the opinions of the speaker’s conversation and enhancing the feedback of the robot agent. Furthermore, the live conversation is used to generate the talks through certain sentiments to enhance the human-machine interaction. This survey focuses the researchers on handling the SA and classification of various sentences in social media by reviewing various approaches. This analysis explains the 50 research articles from different methods used for SA and sentiment classification in social media. Finally, the evaluation of this survey is performed based on the publication year, various approaches, evaluation metrics, and tools. Moreover, the collected 50 research papers are categorized into different techniques, such as deep learning (DL) based methods, machine learning (ML) based methods, lexicon-based methods, hybrid-based methods, and dependency-based methods. Therefore, from this survey, it is clearly shown that the DL-based method is the most utilized approach in many research papers. Similarly, python is the most used tool for SA and classification, and real-time dataset is a commonly used dataset for SA and classification. Likewise, accuracy is repeatedly employed in metrics with the highest value.},
  archive      = {J_ASOC},
  author       = {Mrunmayee Bachate and Suchitra S},
  doi          = {10.1016/j.asoc.2025.112958},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112958},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sentiment analysis and emotion recognition in social media: A comprehensive survey},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Short-term forecasting for port throughput time series based on multi-modal fuzzy information granule. <em>ASOC</em>, <em>174</em>, 112957. (<a href='https://doi.org/10.1016/j.asoc.2025.112957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Port throughput forecasting is a crucial task that enables port managers to efficiently plan operations, optimize resource utilization, and manage risks. Simultaneously, accurate throughput predictions can prevent port congestion, reduce logistics delays, and enhance cargo handling efficiency, thereby improving port operational efficiency and customer satisfaction. While the existing models often show poor prediction accuracy, because they fail to capture data information comprehensive and produce the iterative errors in short-term forecasting. To address these challenges, a novel short-term time series prediction model is designed, fuzzy information granule (FIG) based model. Different from the existing models, our model incorporates an algorithm based on l 1 -trend filtering to dissect port throughput data into linear trend series and random series, effectively revealing the multi-modal information within data--linear modality and non-linear modality. These multiple modalities allow for a better understanding of throughput changes. Following such multi-modal characteristics, the multi-modal FIG, comprising Gaussian polynomial FIG and Gaussian FIG, is constructed, where the former reflects data linear modality and the latter reflects non-linear modality. Through meticulous data information mining and description, the innovative model achieves short-term forecasting at the granular level, reducing the cumulative errors in iteration. The novel designed FIG based model demonstrates superior accuracy and reliability compared to other 10 models across four metrics, mean absolute error, root mean squared error, mean absolute percentage error, and Wilcoxon signed rank test, which are tested on the data from ports including Ningbo, New York, Shanghai, Singapore, Qingdao, and Malaysia. The application of our model in short-term port throughput forecasting holds significant potential impact in both port operations management and computer science domains.},
  archive      = {J_ASOC},
  author       = {Fang Li and Wen Tong and Xiyang Yang},
  doi          = {10.1016/j.asoc.2025.112957},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112957},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term forecasting for port throughput time series based on multi-modal fuzzy information granule},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiplex network influence maximization based on representation learning method. <em>ASOC</em>, <em>174</em>, 112956. (<a href='https://doi.org/10.1016/j.asoc.2025.112956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization based on representation learning has garnered significant attention in recent years, with numerous studies focusing on monolayer networks. However, given the inherent complexity and multiplicity of social networks, addressing the Multiplex network Influence Maximization (MIM) problem is more practical. The MIM problem aims to find a set of seed nodes to maximize the spread of influence throughout the multiplex network. To tackle this issue, this paper introduces a reverse random walk centrality method based on multiplex network representation learning. This method leverages multiplex network representation learning to derive node embeddings across different layers of the network. By calculating similarity weights between nodes within each layer, a reverse random walk is performed to quantify node importance based on the frequency of visits. The top-k nodes with the highest visit counts are then selected as seed nodes. Both single influence propagation and a coupled spread model that integrates competitive and cooperative influence dynamics are considered. Extensive experiments on several real-world datasets demonstrate that the proposed method outperforms existing techniques in terms of effectiveness, providing robust seed node selection for influence maximization. These findings highlight the efficiency and applicability of the proposed method for practical multiplex network scenarios.},
  archive      = {J_ASOC},
  author       = {Hegui Zhang and Dapeng Zhang and Yun Wan and Renbin Pan and Gang Kou},
  doi          = {10.1016/j.asoc.2025.112956},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112956},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiplex network influence maximization based on representation learning method},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal fine-grained reasoning for post quality evaluation. <em>ASOC</em>, <em>174</em>, 112955. (<a href='https://doi.org/10.1016/j.asoc.2025.112955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate assessment of post quality frequently necessitates complex relational reasoning skills that emulate human cognitive processes, thereby requiring the modeling of nuanced relationships. However, existing research on post-quality assessment suffers from the following problems: (1) They are often categorization tasks that rely solely on unimodal data, which inadequately captures information in multimodal contexts and fails to differentiate the quality of students’ posts finely. (2) They ignore the noise in the multimodal deep fusion between posts and topics, which may produce misleading information for the model. (3) They do not adequately capture the complex and fine-grained relationships between post and topic, resulting in an inaccurate evaluation, such as relevance and comprehensiveness. Based on the above challenges, the Multimodal Fine-grained Topic-post Relational Reasoning(MFTRR) framework is proposed for modeling fine-grained cues by simulating the human thinking process. It consists of the local–global semantic correlation reasoning module and the multi-level evidential relational reasoning module. Specifically, MFTRR addresses the challenge of unimodal and categorization task limitations by framing post-quality assessment as a ranking task and integrating multimodal data to more effectively distinguish quality differences. To capture the most relevant semantic relationships, the Local–Global Semantic Correlation Reasoning Module enables deep interactions between posts and topics at both local and global scales. It is complemented by a topic-based maximum information fusion mechanism to filter out noise. Furthermore, to model complex and subtle relational reasoning, the Multi-Level Evidential Relational Reasoning Module analyzes topic-post relationships at both macro and micro levels by identifying critical cues and delving into granular relational cues. MFTRR is evaluated using three newly curated multimodal topic-post datasets, in addition to the publicly available Lazada-Home dataset. Experimental results indicate that MFTRR outperforms state-of-the-art baselines, achieving a 9.52% improvement in the NDCG@3 metric compared to the best text-only method on the Art History course dataset.},
  archive      = {J_ASOC},
  author       = {Xiaoxu Guo and Siyan Liang and Yachao Cui and Juxiang Zhou and Lei Wang and Han Cao},
  doi          = {10.1016/j.asoc.2025.112955},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112955},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multimodal fine-grained reasoning for post quality evaluation},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method based on generative adversarial networks for disentangling physical and chemical properties of stars in astronomical spectra. <em>ASOC</em>, <em>174</em>, 112954. (<a href='https://doi.org/10.1016/j.asoc.2025.112954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents the design of an autoencoder architecture that uses adversarial training in the context of astrophysical spectral analysis. We aim to develop a middle representation of stellar spectra in which the influence of the most prominent physical properties, such as surface temperature and gravity, is effectively removed. This allows the variance within the representation to primarily reflect the effects of the star’s chemical composition on the spectrum. We apply a scheme of deep learning to unravel in the latent space the desired parameters of the rest of the information contained in the data. This work proposes a version of adversarial training that uses one discriminator per parameter to be disentangled, avoiding the exponential combination that occurs when using a single discriminator. Synthetic astronomical data from the APOGEE and Gaia surveys were used to test the method’s effectiveness. Our approach demonstrates a marked improvement in disentangling, reflected in an improvement in the R 2 score of up to 0.7. Additionally, we introduce an ad-hoc framework, GANDALF, designed to facilitate visualization and adaptation of the methodology to other domains in astronomical spectroscopy.},
  archive      = {J_ASOC},
  author       = {Raúl Santoveña and Carlos Dafonte and Minia Manteiga},
  doi          = {10.1016/j.asoc.2025.112954},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112954},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A method based on generative adversarial networks for disentangling physical and chemical properties of stars in astronomical spectra},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral image classification based on ConvGRU and spectral–spatial joint attention. <em>ASOC</em>, <em>174</em>, 112949. (<a href='https://doi.org/10.1016/j.asoc.2025.112949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In hyperspectral image classification, methods based on spectral–spatial joint attention mechanisms have demonstrated the ability to effectively enhance feature extraction. However, existing approaches still face limitations: spectral attention mechanisms often lack local–global feature interaction, spatial attention fails to fully exploit multi-scale information, and the joint modeling of spectral and spatial features remains insufficiently explored. To address these issues, this paper proposes a spectral–spatial joint attention network based on Convolutional Gated Recurrent Units (ConvGRU). First, a Local-Global Spectral Attention (LGSA) mechanism is designed, where one-dimensional convolution extracts local spectral features and fully connected layers enable global feature interaction. Second, a Multi-Scale Spatial Attention (MSSA) mechanism is introduced, employing three convolutional branches with different receptive fields to capture spatial features, followed by hierarchical feature fusion via 1 × 1 convolution. Finally, a channel-level feature fusion strategy based on ConvGRU is proposed, leveraging sequence modeling to achieve channel-wise joint enhancement of LGSA and MSSA, thereby enabling deep coupling of spectral and spatial features. Comparative experiments on three public datasets demonstrate that the proposed method outperforms seven state-of-the-art algorithms in terms of classification performance.},
  archive      = {J_ASOC},
  author       = {Ronghua Shang and Jie Yang and Jie Feng and Yangyang Li and Songhua Xu},
  doi          = {10.1016/j.asoc.2025.112949},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112949},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hyperspectral image classification based on ConvGRU and spectral–spatial joint attention},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptation framework with unified embedding reconstruction for cross-corpus speech emotion recognition. <em>ASOC</em>, <em>174</em>, 112948. (<a href='https://doi.org/10.1016/j.asoc.2025.112948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging for speech emotion recognition (SER) to maintain robustness under cross-domain scenarios. Unsupervised domain adaptation (UDA) algorithms have been explored to address the domain shift in SER without relying on emotion labels in the target domain. As a promising framework in UDAs, self-supervised learning (SSL)-based domain exploration (SDE) investigates the domain and structural information within the target domain, aligning domain discrepancies while preserving the model’s emotion discrimination capability. However, SSL often inadvertently introduces emotion-irrelevant information, adversely affecting the UDA performance. To resolve this, we introduce a novel UDA framework called unified SDE (U-SDE), where both source and target domains conduct a unified SSL task. In the source domain, U-SDE guides the source SSL to focus on emotion-related information due to supervised emotion classification constraints. Simultaneously, in the target domain, shared network weights enable the target SSL branch to concentrate on intrinsic emotional and domain features. However, simply using existing SSL algorithms to implement this framework might disrupt the training of the supervised SER branch. To overcome this, we propose the embedding reconstruction of masked speech (ERMS) algorithm. In ERMS, the emotion encoder transforms the embedding of the masked speech to match the embedding of its corresponding unmasked speech, thereby capturing the emotion discriminative feature within the sample. Finally, we employ ERMS to realize the proposed U-SDE paradigm, termed unified ERMS (U-ERMS). We conducted systematic cross-domain SER experiments by designing 52 scenarios using seven well-known datasets. Experimental results showed that the proposed U-ERMS achieved state-of-the-art performance in cross-domain SERs.},
  archive      = {J_ASOC},
  author       = {Ruiteng Zhang and Jianguo Wei and Xugang Lu and Yongwei Li and Wenhuan Lu and Lin Zhang and Junhai Xu},
  doi          = {10.1016/j.asoc.2025.112948},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112948},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptation framework with unified embedding reconstruction for cross-corpus speech emotion recognition},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grey prediction evolution algorithm with a dominator guidance strategy for solving multi-level image thresholding. <em>ASOC</em>, <em>174</em>, 112947. (<a href='https://doi.org/10.1016/j.asoc.2025.112947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-level thresholding (MLT) stands as a pivotal method for extracting target information from images. Meta-heuristic algorithms provide an efficient way to implement MLT and retains more research space for accuracy optimization of high-dimensional multi-level thresholding (HDMLT) of images than they do for low-dimensional multi-level thresholding (LDMIT). In order to improve the algorithmic accuracy in solving the high-dimensional problems, a grey prediction evolution algorithm with a dominator guidance strategy (GPEdg) is proposed in this paper. GPEdg employs Otsu’s method as its objective function to find the best threshold configuration. The novel operator in the algorithm, i.e., a dominator guidance (dg) strategy, uses a linear combination of three difference vectors to guide the top 50% individuals of populations to learn from the top 20% of them. An efficient balance of search abilities suitable for solving HDMLT problems is expected to be achieved by injecting the local search capability of the dg strategy into GPE’s powerful global search capability. Furthermore, a thresholding morphological profile based method (TMP) leverages the thresholding results generated by GPEdg to train a support vector machine (SVM) for hyperspectral image classification. Numerical experiments are conducted for the newly proposed algorithm and five state-of-the-art algorithms on three image datasets to compare the performance in six metrics, i.e., peak signal-to-noise ratio, structural similarity index, features similarity index, objective function value, stability and time consumption. Overall accuracy and average accuracy are tested on two commonly used hyperspectral image data. The results show that GPEdg exhibits outstanding thresholding performance while TMP enhances the classification accuracy of these images. If this paper is accepted, Matlab_codes associated with this paper will be uploaded to https://github.com/Zhongbo-Hu/Prediction-Evolutionary-Algorithm-HOMEPAGE},
  archive      = {J_ASOC},
  author       = {Peixin Yang and Zhongbo Hu and Yang Zhou and Qinghua Su and Wentao Xiong},
  doi          = {10.1016/j.asoc.2025.112947},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112947},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grey prediction evolution algorithm with a dominator guidance strategy for solving multi-level image thresholding},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pseudo-deep unsupervised model-based clustering for brain tumor detection in magnetic resonance images. <em>ASOC</em>, <em>174</em>, 112940. (<a href='https://doi.org/10.1016/j.asoc.2025.112940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel unsupervised pseudo-deep algorithm, Mixtures of Factor Analyzers based on Rows Iterative Clustering with Image Rotation according to Dimension (MFARICIRD), for accurate and automatic detection of brain tumors in grayscale magnetic resonance images. The main goal is to extract the region of interest (ROI) containing tumors using a pseudo-deep framework with iterative clustering. This framework incorporates a layer by layer structure inspired by deep learning, using a mixture of agent analysts to optimize the clustering process. In each computational layer, the algorithm first rotates the image 90 degrees clockwise if the number of rows is greater than the number of columns. Then, it performs clustering on the image using the estimated parameters of optimal clusters and the optimal number of clusters until the tumor-containing cluster is identified. The identified cluster is used as input for the next layer. These computational processes are iterated layer by layer until the proposed convergence criterion, which acts as the stopping rule, is satisfied. The algorithm uses fuzzy c-mean clustering to binarize the output cluster of the final layer (ROI), enabling the exact extraction of the tumor shape. Finally, it localizes the detected tumor within the original image. A comprehensive evaluation of the effectiveness of the proposed algorithm is presented on the BraTS2020, BraTS2019, and BraTS2018 datasets, demonstrating exceptional performance in detecting tumors with different locations, sizes, and complexities. The algorithm achieved an accuracy and Dice similarity coefficient of 99.96 + 0.0073 % and 98.97 % on the BraTS2018, 99.97 + 0.0087 % and 99.22 + 0.0051 % on the BraTS2019, and 99.98 + 0.0038 % and 99.33 + 0.0072 % on the BraTS2020. The results highlight the remarkable capability of the algorithm in dealing with complex and high-dimensional images, especially in detecting small and unclear tumors. Moreover, it outperforms existing diagnostic methods, significantly improving the accuracy and reliability of brain tumor detection.},
  archive      = {J_ASOC},
  author       = {Rahman Farnoosh and Fatemeh Aghagoli},
  doi          = {10.1016/j.asoc.2025.112940},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112940},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pseudo-deep unsupervised model-based clustering for brain tumor detection in magnetic resonance images},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mindful human digital twins: Integrating theory of mind with multi-agent reinforcement learning. <em>ASOC</em>, <em>174</em>, 112939. (<a href='https://doi.org/10.1016/j.asoc.2025.112939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent Reinforcement Learning (MARL) is focused on enabling autonomous agents to learn and adapt to complex environments through interactions with their surroundings and other agents. A key challenge in MARL is developing agents with the human-like capacity to understand, predict, and respond to the intentions and mental states of their peers. This capability, commonly referred to as the Theory of Mind (ToM), is central to fostering more sophisticated and realistic interactions among autonomous agents. In this paper, we propose a novel approach that leverages Theory-Theory (TT) and Simulation-Theory (ST) to enhance ToM within the MARL framework. Building on the Digital Twins (DT) framework, we introduce the Mindful Human Digital Twin (MHDT). These intelligent systems enriched with ToM capabilities bridge the gap between artificial agents and human-like interactions. In this work, we utilized OpenAI Gymnasium to perform simulations and evaluate the effectiveness of our approach. This work represents a significant step forward in Artificial Intelligence (AI), resulting in socially intelligent systems capable of natural and intuitive interactions with both their environment and other agents. This approach is particularly effective in addressing critical social challenges such as school bullying. This research not only advances the growing field of MARL but also paves the way for sophisticated AI systems with enhanced ToM abilities, tailored for complex and sensitive real-world applications.},
  archive      = {J_ASOC},
  author       = {Luis Zhinin-Vera and Elena Pretel and Víctor López-Jaquero and Elena Navarro and Pascual González},
  doi          = {10.1016/j.asoc.2025.112939},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112939},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mindful human digital twins: Integrating theory of mind with multi-agent reinforcement learning},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A particle swarm optimization and constraint programming-based approach for integrated process planning and scheduling with lot streaming problem. <em>ASOC</em>, <em>174</em>, 112938. (<a href='https://doi.org/10.1016/j.asoc.2025.112938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the integrated process planning and scheduling with lot streaming (IPPS-LS) problem, which consists of lot splitting, process planning, and shop scheduling. Although the IPPS-LS problem is common in the manufacturing of flexible process products, it has not been extensively studied due to its high complexity. Hence, this study develops an enhanced particle swarm optimization algorithm based on constraint programming (CP) to minimize makespan. The proposed algorithm employs finite condition and relaxation models for particle reconfiguration and re-optimization. To achieve it, two types of relaxation models are constructed by decomposing the multiple constraints of the CP model. The algorithm dynamically updates particle encoding sequences based on model accuracy, effectively reducing invalid searches and accelerating the search process. The proposed algorithm is compared with models and other metaheuristic algorithms on 120 test instances. The impact of the relaxed CP strategy and particle swarm optimization algorithm on the proposed algorithm performance is also analyzed. Finally, a significance of difference validation is performed. Computational experiments demonstrate the efficiency of the proposed algorithm in solving the IPPS-LS problem of varying scales. In addition, the relaxed CP strategy exhibits a more significant improvement effect for medium-scale problems compared to small and large-scale problems.},
  archive      = {J_ASOC},
  author       = {Mengya Zhang and Xinyu Li and Liang Gao and Qihao Liu},
  doi          = {10.1016/j.asoc.2025.112938},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112938},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A particle swarm optimization and constraint programming-based approach for integrated process planning and scheduling with lot streaming problem},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heuristic-aided fusion serial cascaded deep network for handwritten character recognition from handwritten images using optimization strategy. <em>ASOC</em>, <em>174</em>, 112937. (<a href='https://doi.org/10.1016/j.asoc.2025.112937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten Character Recognition (HCR) identifies and interprets handwritten text, converting it into machine-readable characters. However, it faces challenges with South Indian languages due to their complex scripts and intricate characters. So, in this work, a novel HCR model was introduced for automatic recognition of handwritten characters in South Indian languages. At first, required Telugu and Kannada handwritten images are collected from standard sources, and given into the segmentation phase using Active Contour. Then, the textural pattern generation is performed using the Adaptive Local Weber Pattern (ALWP). The weights in the ALWP are optimally tuned using the Improved Snow Leopard Optimization Algorithm (ISLOA). Finally, the ALWP generates a textural pattern, and then the generated textural pattern is given to the Hybrid Serial Cascaded Deep Network (HSCDNet) for the final handwritten character recognition process. Here, the Convolutional Autoencoder (CAE) and Deep Belief Network (DBN) are serially connected to the network. Finally, the implemented HCR model’s performance is evaluated by comparing it with various traditional approaches. The developed model attained the accuracy of dataset 1 is 93.45 %, dataset 2 is 93.11 %, and dataset 3 is 94.13 %. Thus, it proved that the developed model can effectively and easily recognize the curves in the Telugu and Kannada handwritten scripts, making it suitable for a wide range of applications.},
  archive      = {J_ASOC},
  author       = {Triveni Banavatu and Govindaswamy Parthasarathy},
  doi          = {10.1016/j.asoc.2025.112937},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112937},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heuristic-aided fusion serial cascaded deep network for handwritten character recognition from handwritten images using optimization strategy},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized echo state network for error compensation based on transfer learning. <em>ASOC</em>, <em>174</em>, 112935. (<a href='https://doi.org/10.1016/j.asoc.2025.112935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo State Network (ESN) is widely applied in nonlinear system modeling, but its performance is often limited by a lack of error autocorrelation analysis, leading to reduced modeling accuracy. Existing extensions, such as SR-ESN and ERBM, primarily focus on structural optimization or feature representation but fail to effectively address autocorrelation errors. To overcome these limitations, we propose a Transfer Learning-based Echo State Network (TLESN) that compensates for errors in realtime to enhance prediction accuracy. The TLESN integrates a computing layer based on ESN and a compensation layer employing transfer learning, which dynamically adjusts output weights. To validate the proposed model, experiments are conducted on the Mackey-Glass time series, a practical Sunspot dataset, and a real-world industrial dataset. Results demonstrate that TLESN effectively mitigates autocorrelation errors, achieving at least a 17% improvement in prediction accuracy compared to existing ESN extensions.},
  archive      = {J_ASOC},
  author       = {Yingqin Zhu and Yue Liu and Zhaozhao Zhang and Wen Yu},
  doi          = {10.1016/j.asoc.2025.112935},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112935},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized echo state network for error compensation based on transfer learning},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain adaptive person re-identification with noise optimization and dynamic weighting. <em>ASOC</em>, <em>174</em>, 112932. (<a href='https://doi.org/10.1016/j.asoc.2025.112932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptive person re-identification (Re-ID) faces challenges due to inherent noise from limited domain transferability and the uncertainty in pseudo-label generation. To address this, we propose NODW (Noise Optimization and Dynamic Weighting), a comprehensive domain adaptive person Re-ID framework that systematically tackles these issues through quantitative noise assessment and dynamic optimization. Our method proposes: (1) an enhanced ResNet50-pro backbone specifically designed for cross-domain feature extraction, (2) a silhouette coefficient-based module for pseudo-label quality assessment with dynamic weighting, (3) a Maximum Mean Discrepancy (MMD)-based module for minimizing domain transferability limitations, and (4) a robust consistency supervision mechanism to ensure stable feature learning. Extensive experiments demonstrate state-of-the-art performance across multiple domain transfer tasks, achieving mAP scores of 73.8% (Market to Duke), 84.7% (Duke to Market), 34.2% (Market to MSMT), and 35.6% (Duke to MSMT). These results represent significant improvements over existing methods, particularly in challenging scenarios with large domain gaps, validating the effectiveness of our noise-aware adaptation strategy.},
  archive      = {J_ASOC},
  author       = {Zhengyang Wang and Xiufen Ye and Xue Shang and Shuxiang Guo},
  doi          = {10.1016/j.asoc.2025.112932},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112932},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Domain adaptive person re-identification with noise optimization and dynamic weighting},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software and hardware synergy for accelerated plant disease identification. <em>ASOC</em>, <em>174</em>, 112926. (<a href='https://doi.org/10.1016/j.asoc.2025.112926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant diseases are one of the main causes of reduced crop yields. Therefore, it is necessary to adopt timely and effective identification methods and take corresponding measures. Some physical and biological detection methods have been proposed by researchers, but these methods require specialized techniques and expensive detection costs. Furthermore, the limited number of skilled technicians means that disease identification is not always timely or effective. To achieve real-time identification of plant diseases in remote areas where network and circuit are not well connected, We adopted a hardware and software co-acceleration approach to implement the design. First, we designed a lightweight convolutional neural network (CNN) and trained this network using a knowledge distillation approach. Then, we quantified the model parameters into int8 type using a method of model quantization to further compress the model size. After compression, the model size of the network is 0.035 MB and the recognition accuracy is 94.06% on the test set of the experiment. In order to deploy the proposed network on resource constrained Field-Programmable Gate Array (FPGA) devices, we used time-division multiplexing and feature map segmentation to deploy the network. Finally, our design is implemented on ZYNQ7020 with an inference speed of 35.73ms/frame and a power consumption of 1.97 W. The experimental results show that our design has the advantages of consuming less FPGA resources, low power consumption, high speed and portability. It can be used for disease recognition in multiple plant classes.},
  archive      = {J_ASOC},
  author       = {Hongxing Wen and Chuandong Li and Xinpei Wang and Ling Chen},
  doi          = {10.1016/j.asoc.2025.112926},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112926},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Software and hardware synergy for accelerated plant disease identification},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced function approximation and applications to image scaling: A new family of exponential sampling neural network kantorovich operators. <em>ASOC</em>, <em>174</em>, 112923. (<a href='https://doi.org/10.1016/j.asoc.2025.112923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel family of exponential sampling type neural network Kantorovich operators, extending the work of Bajpeyi and Kumar (2021) and Bajpeyi (2023). Unlike previous research focused on approximating continuous functions, our operators are designed to handle Lebesgue integrable functions, offering enhanced versatility. We establish convergence theorems, analyze asymptotic behavior, and demonstrate the effectiveness of linear combinations for improving convergence rates. Our analysis extends to the multivariate setting, highlighting the operators’ capability in approximating a wide range of functions. To evaluate the practical performance of our proposed operators, we conducted numerical experiments with different sigmoidal functions and parameter values. Our findings reveal that operators activated by the Parametric sigmoid function consistently outperform those activated by other sigmoidal functions, achieving up to 20.70% reduction in maximum absolute error and 10.03% reduction in root mean squared errors. When applied to image scaling, our operators demonstrated superior performance compared to state-of-the-art methods like nearest neighbor, bilinear, and bicubic interpolation. For the ’Baboon’ image, we observed up to 5.62% increase in Peak Signal-to-Noise Ratio (PSNR) and 5.25% increase in Structural Similarity Index Measure (SSIM). Similar enhancements were observed for the ’Flowers’ and ’Retina’ images. The paper includes a detailed description of the image processing algorithm, along with a flowchart illustrating the implementation. These results underscore the operators’ potential in various machine learning tasks, motivating further research into their applications and optimization.},
  archive      = {J_ASOC},
  author       = {P.N. Agrawal and Behar Baxhaku and Artan Berisha},
  doi          = {10.1016/j.asoc.2025.112923},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112923},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhanced function approximation and applications to image scaling: A new family of exponential sampling neural network kantorovich operators},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust ensemble classifier for imbalanced data via adaptive variety oversampling and embedded sampling rate. <em>ASOC</em>, <em>174</em>, 112922. (<a href='https://doi.org/10.1016/j.asoc.2025.112922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel hybrid approach for addressing imbalanced data classification. The core concept involves devising a data-based oversampling algorithm to partially re-balance the data and employing an ensemble algorithm to enhance model performance. The merits of the proposed hybrid method can be highlighted as follows: (1) rather than re-balancing the data completely, an incomplete yet rational sampling rate is adopted to synthesize new samples, which can reduce the extreme imbalance ratio as well as avoid the overlap and redundancy by the complete re-balance. After oversampling, an improved adaptive boosting method is used to further contribute to the classification result; (2) with the help of temporarily generating samples in a triangular region of four selected target samples, a new synthesizing method is provided, which contributes greatly to the diversity of the new synthetic samples and the guarantee of the correctness and safety; (3) besides the number of correctly classified minority samples, the imbalance ratio of raw data is considered to make the ensemble classifier serve a further focus on minority samples and proved theoretically effective in mitigating the skew of the classification hyperplane on minority samples.},
  archive      = {J_ASOC},
  author       = {Jun Dou and Yan Song and Guoliang Wei and Xinchen Guo},
  doi          = {10.1016/j.asoc.2025.112922},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112922},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A robust ensemble classifier for imbalanced data via adaptive variety oversampling and embedded sampling rate},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting time series using convolutional neural network with multiplicative neuron. <em>ASOC</em>, <em>174</em>, 112921. (<a href='https://doi.org/10.1016/j.asoc.2025.112921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are proven to be efficient in time series forecasting, however architectural selection remains a challenging task. This work aims to propose CNN, utilizing single multiplicative neuron model in forecasting time series, intended to eliminate architectural complexities of classical CNN ensuring its computational efficiency. Applicability of proposed approach is employed on financial time series datasets such as Index, Stocks, Cryptocurrencies and a commodity in evaluating the model’s performance on the basis of RMSE, MAE and R 2 values. Further, time-delay effects were also observed in datasets which has been analyzed to improve the accuracy of the proposed model. Based on the lowest RMSE and MAE values, and higher R 2 values, the optimal delay value has been analyzed which has been used for forecasting. The result demonstrates that in data sets like NIFTY50, SBI, Bitcoin, and Natural Gas, the forecasting efficiency is improved when compared to classical CNN. The results obtained can be used to draw valuable insights for decision making, which will enable future studies and facilitate easy adaptation in analyzing time series.},
  archive      = {J_ASOC},
  author       = {Shobhit Nigam},
  doi          = {10.1016/j.asoc.2025.112921},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112921},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting time series using convolutional neural network with multiplicative neuron},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised partially labeled heterogeneous feature selection based on information-theoretic three-way decision model. <em>ASOC</em>, <em>174</em>, 112880. (<a href='https://doi.org/10.1016/j.asoc.2025.112880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays an increasingly vital role in addressing large-scale partially labeled heterogeneous data. Three-way decision (TWD) theory is an important extension of classical two-way decision, which provides an approach to acquire a ternary classification of the universe as acceptance region, rejection region and boundary region, respectively, while the boundary region can capture the uncertain information. In this paper, taking consideration of heterogeneous data possessing tremendous unlabeled samples, we present two kinds of feature representation metric based on unlabeled sample selection mechanism to construct more effective feature selection models. Specifically, a generalized variable-precision neighborhood rough set model is first proposed based on a TWD model developed by optimal threshold pair, which describes the relationships between features and labels from a more fine-grained level. Second, a unlabeled sample selection framework is proposed to comprehensively measure the importance of unlabeled samples based on their uncertainty, graph density and label transfer ability. We then define six TWD-based measures which reveal nonlinear correlation and inconsistency between features and labels by extended information entropy and complementary entropy, respectively. Furthermore, the unified feature measures are established to boost global feature selection in partially labeled heterogeneous datasets. Finally, the corresponding feature selection algorithm is designed, and the comparative experiments demonstrate the effectiveness and efficiency.},
  archive      = {J_ASOC},
  author       = {Qianqian Sun and Hongying Zhang and Weiping Ding},
  doi          = {10.1016/j.asoc.2025.112880},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112880},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semi-supervised partially labeled heterogeneous feature selection based on information-theoretic three-way decision model},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse-view planar 3D reconstruction method based on hierarchical token pooling transformer. <em>ASOC</em>, <em>174</em>, 112833. (<a href='https://doi.org/10.1016/j.asoc.2025.112833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse-view planar 3D reconstruction aims to recover scene information from limited camera frames, which poses a fundamental problem in computer vision. Although previous methods have made significant improvements in this field, they have not adequately considered the multi-scale properties of the surrounding environment, thus limiting the reconstruction performance. Additionally, the conventional feed-forward network in the vanilla Transformer is constructed using fully connected layers, lacking the ability to capture local information from image features. To address these two problems, this paper proposes a sparse-view planar 3D reconstruction method based on hierarchical token pooling Transformer ( i.e . HTP-Formers). Specifically, we utilize average pooling layers with various ratios in Transformer model to capture multi-scale features. Subsequently, we propose a depth-wise convolution based inverted residual feed-forward network to enhance local information extraction performance at negligible computational cost. To demonstrate the effectiveness of HTP-Formers on planar 3D reconstruction tasks, we thoroughly evaluate the proposed model on Matterport3D public dataset. Especially, HTP-Formers improves performance by 6.1% and 18.3% in translational and rotational errors, respectively, outperforming most existing planar 3D reconstruction methods in terms of planar correspondence inference and relative camera pose estimation.},
  archive      = {J_ASOC},
  author       = {Jiahui Zhang and Jinfu Yang and Fuji Fu and Jiaqi Ma},
  doi          = {10.1016/j.asoc.2025.112833},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112833},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sparse-view planar 3D reconstruction method based on hierarchical token pooling transformer},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-path geometric relation-aware transformer for point cloud classification and segmentation. <em>ASOC</em>, <em>174</em>, 112801. (<a href='https://doi.org/10.1016/j.asoc.2025.112801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud analysis is a challenging task due to the disorder and irregularity of the point cloud data. Traditional methods focus on constructing local or global geometric feature extractors to leverage the geometric features of the point cloud. However, traditional methods have the disadvantage of high time complexity and consume numerous resources because of the sophisticated extractors. This paper proposes a dual-path geometric relation-aware transformer network (DuGREAT) to aggregate local and global features, which balances computational cost, efficiency, and accuracy. DuGREAT constructs a channel adaptive multi-layer perceptron module by expanding channels of residual blocks to reinforce the local feature extraction. To obtain the geometric relation of a 3D object, we design a geometric point disentangler to categorize the point cloud into key and non-key points, respectively represented by an object’s local and global regions. With a relation-aware transformer network, the global path processes the non-key points to extract deep-level global features, and the local path focuses on the key points to calculate the difference between the local and global features. To the best of our knowledge, this is one of the successful attempts to capture and enhance geometric relations and feature fusion between key and non-key points, which provides insights for point cloud classification and segmentation. Fitted with a soft geometric affine module to alleviate the irregularity of the point cloud, DuGREAT acts better than several state-of-the-art methods on multiple datasets. Specifically, our DuGREAT achieves 94.6% accuracy on the ModelNet40 classification task and 87.3% accuracy on the ScanObjectNN classification task, outperforming the other transformer models. DuGREAT-simple, a simple version with fewer FLOPs, attains 94.2% and 87.0% accuracy on the ModelNet40 classification and ScanObjectNN classification tasks, respectively, while maintaining a faster inference speed (595.7 samples/s). Furthermore, we validate the effectiveness of the modules in DuGREAT and achieve instance mean Intersection over Union (mIoU) of 86.5% (DuGREAT) and 86.1% (DuGREAT-simple) on the ShapeNetPart segmentation task.},
  archive      = {J_ASOC},
  author       = {Xiangli Li and Qifan Wang and Baozhi Qiu},
  doi          = {10.1016/j.asoc.2025.112801},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112801},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-path geometric relation-aware transformer for point cloud classification and segmentation},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective multi-task evolutionary algorithm based on source task transfer. <em>ASOC</em>, <em>174</em>, 112732. (<a href='https://doi.org/10.1016/j.asoc.2025.112732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, many optimization problems often do not exist in isolation, they usually have complex interactions and dependencies, and have multiple optimization goals. In order to improve the performance of individual task solving, an evolutionary multi-task multi-objective optimization algorithm (MTMOO) is proposed. However, most of the current evolutionary algorithms are based on the assumption that the prior knowledge (experience in solving optimization problems) is zero, which makes the ability of the algorithm to solve problems cannot be improved with the increase of historical experience, and greatly limits the adaptability and learning ability of the algorithm. In order to overcome this limitation, this paper proposed a multi-objective and multi-task adaptive migration Evolutionary algorithm (MOMFEA-STT). The algorithm constructs the parameter sharing model of historical task and target task online. By identifying the degree of association between different tasks, the intensity of cross-task knowledge transfer is automatically adjusted to maximize the capture, sharing and utilization of common useful knowledge to solve related tasks. In addition, in order to strengthen the exploration and exploitation ability of the algorithm and avoid the problem that the algorithm is easy to fall into the local optimum, the MOMFEA-STT adopts a new method of generation of children, generates children by using spiral search mode, and constantly adjusts the search direction of the algorithm. Experimental results show that the proposed method outperforms the existing algorithms on the multi-task optimization benchmark problems.},
  archive      = {J_ASOC},
  author       = {Zheng-Yi Chai and ying Nie and Yan-Lun Li},
  doi          = {10.1016/j.asoc.2025.112732},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112732},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective multi-task evolutionary algorithm based on source task transfer},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calculating forgotten effects using fuzzy numbers based on embedded experton structures. <em>ASOC</em>, <em>174</em>, 112720. (<a href='https://doi.org/10.1016/j.asoc.2025.112720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The method of calculating Forgotten Effects is designed to process a single source of information, preventing the use of multiple sources, which can generate bias in the results and avoid the wide use of this methodology in the investigations. This research aims to establish a mechanism for processing Expertons as structured data in their natural form, without decreasing entropy within the matrices of the Forgotten Effects, and thus be able to process information from many sources. This process aims to maintain the original values during the whole calculating process and avoid distorted information. The presented algorithm for calculating Forgotten Effects in decision-making under uncertainty utilizes fuzzy numbers in Expertons embedded and constitutes an innovative approach. This approach complements conventional methodologies that often rely on averages or weights to derive a single value and confirm hypotheses. This model has a different paradigm, as it detects relationships that are not visible for systematic analysis. Additionally, this research introduces various types of focus based on risk profiles for calculation strategies, yielding prudent, optimistic, and pessimistic scenarios. Moreover, the proposed algorithm provides the flexibility to choose between uncertainty or precision-focused processing to adapt to the specific approach of each context. Ultimately, an applied example is presented to validate the effectiveness and validity of the proposed model. As a contribution, this research offers the novelty of being able to calculate forgotten effects, not only based on one expert but also with an unlimited number by using the algorithm designed through the Expertons.},
  archive      = {J_ASOC},
  author       = {Darley Biviana Pacheco Cubillos and Josefa Boria Reverter and Jaime Gil Lafuente},
  doi          = {10.1016/j.asoc.2025.112720},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112720},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Calculating forgotten effects using fuzzy numbers based on embedded experton structures},
  volume       = {174},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale frequency feature fusion transformer for pediatric echocardiography analysis. <em>ASOC</em>, <em>173</em>, 112950. (<a href='https://doi.org/10.1016/j.asoc.2025.112950'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Congenital heart disease (CHD) is the most common birth defect. Echocardiography analysis is essential for identifying pediatric CHD, yet existing methods often focus on single functions and lack comprehensive analysis. In ultrasound data analysis, low-frequency features reflect the overall structure and morphology of the heart, while high-frequency features emphasize the detailed characteristics of cardiac structures. However, existing intelligent analysis models apply a uniform feature processing approach to all echocardiography features, lacking adaptive learning for features at different frequencies. This limitation reduces the model’s capacity to effectively capture both the global shape and local details of echocardiography target structures. Therefore, this paper proposes a multi-scale frequency feature fusion Transformer (MFT-Former) model for the multi-functional analysis of pediatric echocardiography. Specifically, this paper first effectively implements the extraction and aggregation of coarse and fine-grained features of echocardiography by setting a multi-scale patch embedding unit. Secondly, we design a grouped frequency feature fusion Transformer block (GFFT_Block), which contains two parallel branches: an adaptive frequency feature fusion branch (AFF_Branch) and a cross-learning wavelet Transformer branch (CWT_Branch). The former is used for adaptive learning of different frequency features in a local range. The latter achieves cross-learning of different frequency features on a global range and avoids the loss of information caused by dimensionality reduction operations. Based on the parasternal short-axis view, this model identifies ventricular septal defects via echocardiography classification and realizes quantitative analysis through structural segmentation and measurement. Experimental results demonstrate that the MFT-Former outperforms the state-of-the-art algorithms and offers new perspectives for multi-functional echocardiography analysis.},
  archive      = {J_ASOC},
  author       = {Cheng Zhao and Yuanlin Liu and Weiling Chen and Zhuo Xiang and Yiyao Liu and Bei Xia and Jing Qin and Tianfu Wang and Baiying Lei},
  doi          = {10.1016/j.asoc.2025.112950},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112950},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale frequency feature fusion transformer for pediatric echocardiography analysis},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable dahl-LRN neural-network for accurately modelling the systems with rate-dependent asymmetric hysteresis. <em>ASOC</em>, <em>173</em>, 112936. (<a href='https://doi.org/10.1016/j.asoc.2025.112936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motion accuracy and stability of piezoelectric positioning systems are significantly compromised by inherent hysteresis and other nonlinearities. This paper presents an innovative method integrating the Dahl model with Layer Recurrent Neural Networks (LRN) to model piezoelectric actuators accurately. Initially, the Dahl model is reformulated into a neural network structure, resulting in the Dahl Neural Network (DahlNN), which strictly adheres to the underlying mathematical equations. The weights of this network directly correspond to the parameters of the Dahl equations, thereby creating a transparent neural network architecture with clear physical significance and interpretability. Subsequently, the DahlNN is enhanced by incorporating feedback mechanisms and recurrent effects from LRN, improving its ability to describe asymmetric and rate-dependent hysteresis characteristics. Extensive experiments demonstrate that, compared to LRN models without physical knowledge guidance, the proposed Dahl-LRN model reduces peak-to-valley fluctuations by 70 % and decreases the average error by approximately 97.3 %, with only a 5 % increase in computational time while maintaining interpretability and achieving superior modelling performance. Through this approach, this paper aims to provide a novel perspective on leveraging physical information to advance the application of deep learning in modelling complex physical phenomena.},
  archive      = {J_ASOC},
  author       = {Lei Ni and Hongfei Wang and Guoqiang Chen and Lanqiang Zhang and Na Yao and Geng Wang},
  doi          = {10.1016/j.asoc.2025.112936},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112936},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interpretable dahl-LRN neural-network for accurately modelling the systems with rate-dependent asymmetric hysteresis},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous collaborative filtering contrastive learning for social recommendation. <em>ASOC</em>, <em>173</em>, 112934. (<a href='https://doi.org/10.1016/j.asoc.2025.112934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering methods based on Graph Neural Networks (GNNs) have gained increasing popularity in recommendation systems. These methods enhance the representation of users and items by leveraging the information of graph structure from interaction data, improving recommendation performance. However, they often face limitations due to the data sparsity issue that is common in recommendation systems. In the constructed user–item heterogeneous bipartite graph, sparse interaction data leads to a scarcity of neighbor nodes impeding the acquisition of sufficient collaborative signals via the message-passing mechanism among these neighbor nodes. We have observed that users and items can be grouped according to characteristic similarities. These groups’ common feature information can serve as supplementary data to aid in the embedding learning. So, we present the Heterogeneous Collaborative Filtering Contrastive Learning (HCFCL) method, which aims to extract two types of heterogeneous collaborative signals from interaction data: those based on neighbor nodes and those based on group features. Specifically, we design an embedding generative hypergraph network to extract group common feature information founded on the heterogeneous bipartite graph. The group common feature information is transferred via a meta network and personalized bridge functions according to individual characteristics. Additionally, the HCFCL model, combined with contrastive learning, captures the consistency of the heterogeneous collaborative signals to enhance representation. The experiment demonstrates the superior performance of the HCFCL model compared to other methods evaluated on three public datasets, demonstrating excellent and stable performance in mitigating the data sparsity issue.},
  archive      = {J_ASOC},
  author       = {Chaojun Meng and Changfan Pan and Hongji Shu and Qing Wang and Hanghui Guo and Jia Zhu},
  doi          = {10.1016/j.asoc.2025.112934},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112934},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heterogeneous collaborative filtering contrastive learning for social recommendation},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic optimization for generating adversarial malware based on prioritized evolutionary computing. <em>ASOC</em>, <em>173</em>, 112933. (<a href='https://doi.org/10.1016/j.asoc.2025.112933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has been widely applied to malware detection tasks; but unfortunately, they exhibit significant vulnerability to adversarial attacks and can be easily circumvented using perturbation carefully crafted. Concurrently, we are witnessing a corresponding increase in the attention dedicated to adversarial attacks against malware detection models. Nevertheless, current research on adversarial examples still faces obstacles such as poor escape effectiveness and difficulty in preserving functionality. Particularly, greedily recruiting the best manipulations from a vast search space often leads to poor diversity of adversarial perturbation sequence. To rectify these shortcomings, this paper proposes an automated, continuously optimized approach for generating malware adversarial examples based on evolutionary computing. Our method filters effective action sequences from a large pool of random manipulations, assigning different priorities to different actions. The generation and optimization of adversarial examples are formalized as a sparse minimization optimization problem based on a fixed-length action vector. We introduce AOP-Mal, a novel genetic framework to automatically generate and optimize adversarial examples. The initialization and evolution of the population depend on the priority of actions, as well as the proposed novel evolutionary operator. The experimental results demonstrate that our attack strategy effectively bypasses the detection mechanisms and outperforms most state-of-the-art malware adversarial frameworks. Our hope is to help researchers understand the intentions of attackers and explore more powerful defense mechanisms.},
  archive      = {J_ASOC},
  author       = {Yaochang Xu and Yong Fang and Yijia Xu and Zhan Wang},
  doi          = {10.1016/j.asoc.2025.112933},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112933},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic optimization for generating adversarial malware based on prioritized evolutionary computing},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online ensemble learning-based anomaly detection for IoT systems. <em>ASOC</em>, <em>173</em>, 112931. (<a href='https://doi.org/10.1016/j.asoc.2025.112931'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern era of digital transformation, the evolution of fifth-generation (5G) wireless networks has played a pivotal role in revolutionizing communication technology and accelerating the growth of smart technology applications. As an integral element of smart technology, the Internet of Things (IoT) grapples with the problem of limited hardware performance. Cloud and fog computing-based IoT systems offer an effective solution but often encounter concept drift issues in real-time data processing due to the dynamic and imbalanced nature of IoT environments, leading to performance degradation. In this study, we propose a novel framework for drift-adaptive ensemble learning called the Adaptive Exponentially Weighted Average Ensemble (AEWAE), which consists of three stages: IoT data preprocessing, base model learning, and online ensembling. It integrates four advanced online learning methods within an ensemble approach. The crucial parameter of the AEWAE method is fine-tuned using the Particle Swarm Optimization (PSO) technique. Experimental results on four public datasets demonstrate that AEWAE-based anomaly detection effectively detects concept drift and identifies anomalies in imbalanced IoT data streams, outperforming other baseline methods in terms of accuracy, F1 score, false alarm rate (FAR), and latency.},
  archive      = {J_ASOC},
  author       = {Yafeng Wu and Lan Liu and Yongjie Yu and Guiming Chen and Junhan Hu},
  doi          = {10.1016/j.asoc.2025.112931},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112931},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online ensemble learning-based anomaly detection for IoT systems},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual residual learning of frequency fingerprints in detecting synthesized biomedical imagery. <em>ASOC</em>, <em>173</em>, 112930. (<a href='https://doi.org/10.1016/j.asoc.2025.112930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial synthesis of biomedical imagery is an evolving threat yet under-addressed. The integrity of medical imaging is important for accurate diagnosis and treatment. This study addresses the potential threat of fabricated biomedical imagery, focusing on synthetic dermatological lesions and CT nodules. The Representation Similarity Matrix measured the quantitative authenticity to account for similarities of synthesized data with authentic data. The study explores traces of manipulation from frequency signatures of synthesized imagery. We propose a novel combinatorial architecture, the Dual Residual Network (DRN), capturing hidden residual traces from low-frequency fingerprints of synthetic data and exposing hidden forgeries. DRN achieves near-perfect detection rates with an accuracy of 98.80% for CT nodules and 98.97% for lesions. Equal Error Rates of the model on the two datasets exhibited a marginal improvement of 57.87% in the CT nodules compared to the skin lesions. Sensitivity and specificity play a significant role in medical diagnostics. The model achieved sensitivities of 99.31% and 98.45% and specificity of 98.80% and 99.60% for each dataset, respectively. Further verification of the frequency traces was performed by analyzing gradients in the target concepts that led to decision-making. This study equips the medical field with a powerful tool to combat the evolving threat of synthetic fraud, safeguarding patient and client safety. The potential of the technique extends beyond healthcare, offering a blueprint for tackling synthetic data across diverse domains.},
  archive      = {J_ASOC},
  author       = {Misaj Sharafudeen and Vinod Chandra S.S.},
  doi          = {10.1016/j.asoc.2025.112930},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112930},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual residual learning of frequency fingerprints in detecting synthesized biomedical imagery},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph attention-based policy gradient method with an adaptive embedding strategy for k-center problems. <em>ASOC</em>, <em>173</em>, 112929. (<a href='https://doi.org/10.1016/j.asoc.2025.112929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k -center problem (KCP) is a well-known NP-hard combinatorial optimization challenge in the field of computer science and operations research, aiming to determine optimal locations for k centers within a given set of nodes to minimize the maximum distance from each node to its nearest center. In contrast to conventional algorithms that have inherent limitations in handling the trade-off between solution quality and computational efficiency, this study proposes a new method based on a graph attention mechanism with an encoder-decoder architecture to find high-quality solutions for KCPs by directly learning heuristics from the graph. Specifically, the encoder processes the input feature of the graph and capture intricate spatial patterns and dependencies among nodes, whereas the decoder leverages the encoded information and attention weights to iteratively generate solutions for the KCP. Moreover, an adaptive embedding strategy is developed to handle the specific attributes and constraints inherent in different KCP instances. To find high-quality solutions, a policy gradient method with an exponential moving average baseline is developed to update and learn the optimal model parameters. A comprehensive set of experiments on multiple problem sizes are conducted to systematically compared the performance of the proposed method with a wide range of baseline methods across four types of KCPs, including the standard KCP, capacitated KCP, non-uniform KCP, and dynamic KCP. The experimental results demonstrate the competitive performance of the graph attention-based method in addressing KCPs.},
  archive      = {J_ASOC},
  author       = {Zhonghao Zhao and Carman K.M. Lee and Xiaoyuan Yan},
  doi          = {10.1016/j.asoc.2025.112929},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112929},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A graph attention-based policy gradient method with an adaptive embedding strategy for k-center problems},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified transductive and inductive learning framework for few-shot learning using graph neural networks. <em>ASOC</em>, <em>173</em>, 112928. (<a href='https://doi.org/10.1016/j.asoc.2025.112928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have shown their effectiveness in integrating feature embeddings for image and video processing tasks. While initially developed for inductive learning, GNNs have been extended to support transductive learning, enabling them to learn from partially labeled graphs. However, the combination of transductive and inductive learning in existing GNN-based models lacks proper theoretical justification, and GNNs with information propagation mechanisms often encounter the over-smoothing problem, especially in Few-Shot Learning (FSL) tasks. In this paper, we propose a unified transductive and inductive learning GNN model named FGCN for FSL tasks. The proposed FGCN differentiates between the roles of inductive and transductive learning, while quantifying the contributions of intra-properties within entities and inner-relationships between neighboring entities. By addressing the over-smoothing problem comprehensively, the FGCN offers a promising approach for FSL tasks. Our findings demonstrate that the proposed FGCN model achieves a significant improvement in accuracy over state-of-the-art methods, as evidenced by experiments on four standard Few-Shot Learning benchmarks. For example, in the 5-Way 5-Shot scenario, the proposed FGCN achieved an accuracy increase of 7.70% on the Mini-ImageNet, compared to the state-of-the-art result obtained by the MCGN model.},
  archive      = {J_ASOC},
  author       = {Jie Chang and Haodong Ren and Zuoyong Li and Yinlong Xu and Taotao Lai},
  doi          = {10.1016/j.asoc.2025.112928},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112928},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A unified transductive and inductive learning framework for few-shot learning using graph neural networks},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective evolutionary algorithm with two balancing mechanisms for heterogeneous UAV swarm path planning. <em>ASOC</em>, <em>173</em>, 112927. (<a href='https://doi.org/10.1016/j.asoc.2025.112927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) swarm path planning involves creating efficient routes based on task requirements to enable collaborative flight. Compared to homogeneous UAV swarm, the application scenarios of heterogeneous UAV swarm have become increasingly widespread. They can fully leverage the various capabilities of drones and show higher economic benefits. Existing research mainly focuses on homogeneous UAV swarms, and the model for uniformly describing heterogeneous UAV swarm from a functional perspective is insufficient. Differences in dynamic constraints and energy consumption models create challenges for accurately characterizing the path planning problem of heterogeneous UAV swarm. To supplement the above deficiencies, this article designs the scenario and composition structure of heterogeneous UAV swarm. The path-planning problem of heterogeneous UAV swarm is modeled as a multi-objective optimization (MOO) problem, in which a comprehensive energy consumption objective is constructed. To better balance multiple objectives and obtain high-quality solutions, a MOO evolutionary algorithm based on heterogeneous UAV swarm, namely HMOEA, is proposed. Specifically, HMOEA is implemented by combining the proposed two strategies. To verify the model’s feasibility and the algorithm’s effectiveness, numerical simulations and prototype simulations are provided. In numerical simulations, the proposed algorithm was compared with various advanced algorithms, i.e., NSGA-II, CIACO, AP-GWO, CL-DMSPSO, and DSNSGA-III, in two designed terrain problems. The results demonstrate that HMOEA not only outperforms the compared algorithms on convergence and diversity indicators increased over 4% and 2% respectively. Normal flight results were achieved in the two scenarios served by the prototype simulation, namely, urban buildings and forest scenes. Specific implementation and application can be achieved in military or civilian scenarios like reconnaissance and strike missions, search and rescue missions. The proposed model can adapt to more task scenarios, and the proposed method can provide faster and higher quality results for heterogeneous UAV swarm routes. In actual deployment, adjusting model parameters and optimizing the computing environment according to application requirements are worth further investigation to achieve optimal effect.},
  archive      = {J_ASOC},
  author       = {Xiuju Xu and Chengyu Xie and Linru Ma and Lin Yang and Tao Zhang},
  doi          = {10.1016/j.asoc.2025.112927},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112927},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective evolutionary algorithm with two balancing mechanisms for heterogeneous UAV swarm path planning},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A random searching algorithm for efficiently solving the connectivity-oriented robust optimization problem on large-scale networked systems. <em>ASOC</em>, <em>173</em>, 112924. (<a href='https://doi.org/10.1016/j.asoc.2025.112924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By consolidating part of the links to be invulnerable, there will be no connectivity degradation in a network under expected network failure intensity. Although existing link consolidation methods can handle large-scale networks, their solutions are far from optimal. Redundancy in existing solutions can be quantified by the connectivity of the pure graph consisting of the necessary subset of links, and existing methods improve pure graph connectivity to far above the expected value. Fortunately, we have found a special superset of link cuts, and proved that it can reduce consolidation links by removing the right set of links from existing solutions while maintaining the desired connectivity. In response to the high complexity of searching for the optimal superset, we found one kind of superset that is close to the optimal solution and easy to locate, significantly reducing the number of links that need to be consolidated with a slight increase in preprocessing overhead. Experiments have shown that in large networks, the algorithm can provide a protection effect of over 99.9%, and can lead to 60% overhead savings compared to existing high-speed algorithms under the same computing time. On small-scale networks where the optimal algorithm is feasible, the average additional cost compared to the optimal result can be controlled within 1%. Thus, while ensuring accuracy, it can further approach the optimal solution compared to existing algorithms, significantly reducing the overhead of infrastructure consolidation.},
  archive      = {J_ASOC},
  author       = {Wei Wei and Guobin Sun and Qinghui Zhang},
  doi          = {10.1016/j.asoc.2025.112924},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112924},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A random searching algorithm for efficiently solving the connectivity-oriented robust optimization problem on large-scale networked systems},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hesitant fuzzy linguistic mahalanobis distance-based TOPSIS method for evaluating regional green development levels. <em>ASOC</em>, <em>173</em>, 112920. (<a href='https://doi.org/10.1016/j.asoc.2025.112920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectively understanding regional green development levels (GDLs) is a key prerequisite for the implementation of green development strategies. In this study, we propose an innovative hesitant fuzzy linguistic Mahalanobis distance-based TOPSIS (HLMD-TOPSIS) method to evaluate regional GDLs with hesitant fuzzy linguistic term sets (HFLTSs). The developed HLMD-TOPSIS method can well optimize the weight allocation based on the correlation relationship between criteria under HFLTSs environments. The improved relative closeness index used in the HLMD-TOPSIS method not only optimizes the evaluated index to better adapt the actual contexts but also reflects some balance between the shortest distance from positive ideal solution and the farthest distance from negative ideal solution. More importantly, a new evaluation index system of regional GDLs within twelve criteria is further established from three dimensions of green economy, ecological environment, and green policy. With this new evaluation index system, the developed HLMD-TOPSIS method is applied to evaluate the GDLs of Hubei province in China, and its feasibility is demonstrated. The results show that the innovative HLMD-TOPSIS method is reliable for evaluating regional GDLs, and provides valuable inspiration and references for improving the evaluation efficiency of practical regional GDLs. The advantage of the developed HLMD-TOPSIS method is also indicated with comparison analysis. Besides, a series of new hesitant fuzzy linguistic Mahalanobis distance measures and new statistical measurements are also proposed to measure HFLTSs data, which provide a theoretical basis for the processing of HFLTSs information.},
  archive      = {J_ASOC},
  author       = {Xiaolu Zhang and Haiyan Wu},
  doi          = {10.1016/j.asoc.2025.112920},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112920},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hesitant fuzzy linguistic mahalanobis distance-based TOPSIS method for evaluating regional green development levels},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating commuters' travel mode choice using the Z-number extension of parsimonious best worst method. <em>ASOC</em>, <em>173</em>, 112918. (<a href='https://doi.org/10.1016/j.asoc.2025.112918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates commuters' travel mode choices in Dublin City, Ireland, using a novel fuzzy multi-criteria decision-making model. The model integrates the Best-Worst Method (BWM) with fuzzy Z-numbers and the Parsimonious concept to handle uncertainties and simplify decision-making. The innovative aspect of this model lies in its ability to combine these methodologies, offering a streamlined yet comprehensive tool for urban transportation analysis. The hypothesis tested is whether the proposed model can effectively evaluate and prioritize travel mode choices while maintaining simplicity and reliability. The methodology involves surveying four experts and applying fuzzy Parsimonious Z-BWM to assess six travel modes. The results indicate that the model provides a robust framework for evaluating travel mode choices, with cars being identified as the most preferred mode. A comparative analysis with traditional BWM and direct evaluation methods demonstrates the model's efficiency and consistency in ranking preferences.},
  archive      = {J_ASOC},
  author       = {Sarbast Moslem},
  doi          = {10.1016/j.asoc.2025.112918},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112918},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating commuters' travel mode choice using the Z-number extension of parsimonious best worst method},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-strategy combined whale optimization algorithm for cascade reservoir operation of complex engineering optimization. <em>ASOC</em>, <em>173</em>, 112917. (<a href='https://doi.org/10.1016/j.asoc.2025.112917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir operation optimization is a complex nonlinear problem involving multiple variables and physical constraints, making it one of the most challenging optimal issues in water resources management. The Whale Optimization Algorithm (WOA) features a straightforward mechanism and exceptional optimization performance. However, with escalating problem complexity, problems such as premature convergence and inadequate global exploration emerge. This study proposes a multi-strategy combined whale optimization algorithm (SCWOA) to address these problems. The approach retains the powerful exploitation mechanism of WOA while implementing the following improvements: introducing parallel multiplication and division operators to enhance global exploration capability, adopting the dual-strategy encirclement mechanism to enrich population diversity, and integrating dynamic spiral mechanism to improve solution accuracy coupling the adaptive escape mechanism to reduce the local stagnation times. Subsequently, numerical experiments are conducted to compare and analyze SCWOA with seven commonly used optimization algorithms across 53 benchmark functions. The analysis results indicate that SCWOA surpasses existing algorithms in global optimization accuracy, robustness, and exploration ability when handling most complex problems with varying dimensions and modes. Furthermore, a generation operation model of a practical hydropower system in China is developed under multiple constraints, such as ice prevention, flood control, and water supply. The operation results show that the schemes of SCWOA generate higher power generation than existing algorithms under different scenarios, effectively improving hydropower utilization rates. Therefore, a novel approach is provided for solving complex reservoir operation optimization problems.},
  archive      = {J_ASOC},
  author       = {Ziqi Hou and Huichun Peng and Jiqing Li},
  doi          = {10.1016/j.asoc.2025.112917},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112917},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel multi-strategy combined whale optimization algorithm for cascade reservoir operation of complex engineering optimization},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine-learning component for multi-start metaheuristics to solve the capacitated vehicle routing problem. <em>ASOC</em>, <em>173</em>, 112916. (<a href='https://doi.org/10.1016/j.asoc.2025.112916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Start metaheuristics (MSM) are commonly used to solve vehicle routing problems (VRPs). These methods create different initial solutions and improve them through local-search. The goal of these methods is to deliver the best solution found. We introduce initial-solution classification (ISC) to predict if a local-search algorithm should be applied to initial solutions in MSM. This leads to a faster convergence of MSM and higher-quality solutions when computation time is limited. In this work, we extract features of a capacitated VRP (CVRP) solution, by transforming the structure of a solution into quantitative metrics (i.e.number of customers in each route, average compactness of a route, or number of intersections between routes). With these features and a machine-learning classifier (random forest), we show how ISC – significantly – improves the performance of greedy randomized adaptive search procedure (GRASP), over benchmark instances from the CVRP literature. With the objective of evaluating ISC’s performance with different local-search algorithms, we implemented a local-search composed of classical neighborhoods from the literature and another local-search with only a variation of Ruin-and-Recreate. In both cases, ISC significantly improves the quality of the solutions found in almost all the evaluated instances.},
  archive      = {J_ASOC},
  author       = {Juan Pablo Mesa and Alejandro Montoya and Raul Ramos-Pollan and Mauricio Toro},
  doi          = {10.1016/j.asoc.2025.112916},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112916},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Machine-learning component for multi-start metaheuristics to solve the capacitated vehicle routing problem},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CaRGI: Causal semantic representation learning via generative intervention for single domain generalization. <em>ASOC</em>, <em>173</em>, 112910. (<a href='https://doi.org/10.1016/j.asoc.2025.112910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single Domain Generalization (SDG) is a realistically challenging case in domain generalization, which has only one available source domain and is committed to generalizing the knowledge acquired from this single domain to unseen target domains. Due to the lack of diversity information, data augmentation to expand the data distribution is the mainstream method for SDG. Most low-level data augmentation methods cannot model large domain shifts and simulated domains are fragile, which prevents the model from thoroughly improving the generalization ability. We bridge SDG to causal concepts, novelly analyzing the reason why limited generated domain shifts restrict the improvement of generalization ability from the view of the backdoor path, and propose a causally stable framework CaRGI ( Ca usal Semantic R epresentation Learning via G enerative I ntervention). Firstly, we construct the inclusive causal directed acyclic graph and utilize causal analysis to gradually explore the relationship between the generated domain shift and generalization ability. We regard domain expansion as the causal intervention and secondly propose the joint generative intervention module with dynamic to enlarge the domain shift with semantic consistency, which is dedicated to eliminating spurious confounding effects by blocking the backdoor path. Thirdly, counterfactual inference is applied to implement causal semantic representation learning. In this process, the min–max game in the latent space plays between the generative intervention module and the prediction module, which engage in alternating training that benefit each other in the spiral development. Specially, we innovatively perform maximization and minimization operations in shallow and deep layers respectively. CaRGI can finally learn causal semantic representations and improve the stable generalization ability. Extensive experimental results on several widely used datasets verify the feasibility and effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Youjia Shao and Shaohui Wang and Wencang Zhao},
  doi          = {10.1016/j.asoc.2025.112910},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112910},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CaRGI: Causal semantic representation learning via generative intervention for single domain generalization},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive trustworthy prototype learning for multi-modality myocardial pathology segmentation. <em>ASOC</em>, <em>173</em>, 112909. (<a href='https://doi.org/10.1016/j.asoc.2025.112909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modality Myocardial Pathology Segmentation (MyoPS) is essential for assessing risks and planning treatments in myocardial infarction (MI). However, multi-modality myocardial semantic segmentation remains challenging due to the low contrast of pathologic regions between modalities and limited medical training data. To address these issues, we propose C ontrastive TR ustworthy prototype L earning (CTRL) for the dynamic fusion of multi-modality cardiac MRI images. Specifically, CTRL enhances the contrast of intra-modality features by discriminating the intra-modality contribution to the segmentation effect with instance-level confidence and then introduces contrast loss to constrain the modality-specific pathological region features. To incorporate cross-modality information, we combine frequency and spatial domain features to dynamically aggregate global and specific pathological features between modalities and discriminatively aggregate cross-modality semantic expressions. In addition, we constructed a prototype learning memory module to recall multi-modality data a priori, which enables the model to retain essential information to leverage limited data fully. Extensive experimental results on the available MyoPS 2020 dataset for myocardial pathology segmentation demonstrate that CTRL outperforms state-of-the-art methods by 1.6% and can be adapted for heterogeneous medical image representation.},
  archive      = {J_ASOC},
  author       = {Jingjing Liu and Ao Wei and Lijuan Cao and Xiao He and Chang Tang},
  doi          = {10.1016/j.asoc.2025.112909},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112909},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive trustworthy prototype learning for multi-modality myocardial pathology segmentation},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An advanced ensemble framework for defending against obfuscated windows, android, and IoT malware. <em>ASOC</em>, <em>173</em>, 112908. (<a href='https://doi.org/10.1016/j.asoc.2025.112908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection and analysis of malware binaries pose significant challenges due to their obfuscated and packed nature, rendering traditional static analysis techniques ineffective. Extracting static features in a dynamic environment where malware exhibits its actual behavior becomes crucial to detecting malware accurately. This article addresses this challenge by analyzing static features extracted from real-time Windows, Android, and IoT applications within a dynamic environment. To tackle this problem, we propose an Advanced Ensemble Framework (AEF) that combines embedded feature selection and an advanced stacking ensemble technique. The embedded feature selection approach effectively reduces the number of highly correlated features by over 70%, employing a combination of filter and wrapper methods. Furthermore, the advanced stacking ensemble approach combines two-level learners: a base learner with state-of-the-art classifiers adept at handling raw features and meta-learner trains using transfer features and probabilities obtained from the previous base classifiers. A 5-fold cross-training scheme based on cross-validation is introduced to prevent overfitting during the training. It also helps to reduce overfitting by training the model on multiple subsets of the data. The model learns patterns from different parts of the dataset, which can lead to a more generalized model. Pre-processed datasets from the Canadian Institute of Cybersecurity comprising obfuscated Windows malware, Android malware, and IoT malicious attacks are used to evaluate AEF. Additionally, to further assess the efficiency, compatibility, and robustness of AEF, we utilized an additional dataset of obfuscated Windows malware that includes memory dump images. Extensive experiments are conducted to evaluate the proposed defender using publicly available real-time datasets. The results show that AEF effectively counters obfuscation techniques, offering a flexible, practical, and efficient solution for malware detection across various datasets. Furthermore, the prediction time of the proposed approach is 0 . 042 ms for CICMalDroid-2020, 0 . 16 ms for IoMT-2024, 0 . 055 ms for CIC-MalMemory-2022, and 0 . 15 ms for Dumpaware10 malware datasets.},
  archive      = {J_ASOC},
  author       = {Danish Vasan and Junaid Akram and Mohammad Hammoudeh and Adel F. Ahmed},
  doi          = {10.1016/j.asoc.2025.112908},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112908},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An advanced ensemble framework for defending against obfuscated windows, android, and IoT malware},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based backdoor attacks against natural language processing models. <em>ASOC</em>, <em>173</em>, 112907. (<a href='https://doi.org/10.1016/j.asoc.2025.112907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backdoor attacks against natural language processing (NLP) models are surging with enhanced attack success rates. However, these backdoor attacks are limited in sentence fluency, grammar errors, and stealthiness. To address these issues, this study proposes an attention-based backdoor attack that generates high-quality backdoor-poisoned samples. The proposed backdoor employs class activation mapping (CAM) to generate backdoor texts with a baseline convolutional neural network in two steps: trigger generation and trigger insertion. The trigger generation leverages high-frequency words as candidate trigger patterns that are subsequently used to generate poisoned texts with high stealthiness and effectiveness. These candidate words are then inserted into computed positions of clean texts, under a low poisoning rate, based on available positions computed with the CAM-based attention method. Through extensive experiments on five benchmark datasets, the proposed CAM-based backdoor attack demonstrates a more excellent performance than the other five backdoor attacks from multiple aspects, including utility, effectiveness, and stealthiness. The proposed method is more robust than other attacks because it maintains almost the highest attack success rate against four benchmark defense methods.},
  archive      = {J_ASOC},
  author       = {Yunchun Zhang and Qi Wang and Shaohui Min and Ruifeng Zuo and Feiyang Huang and Hao Liu and Shaowen Yao},
  doi          = {10.1016/j.asoc.2025.112907},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112907},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-based backdoor attacks against natural language processing models},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving federated UAV data collection framework for autonomous path optimization in maritime operations. <em>ASOC</em>, <em>173</em>, 112906. (<a href='https://doi.org/10.1016/j.asoc.2025.112906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring data privacy and operational security has become critical in light of escalating cyber threats and the logistical complexity of autonomous maritime operations. Autonomous maritime systems face such challenges in securely processing and managing large amounts of real-time data while maintaining resilience against cyber attacks. This paper considers these challenges by presenting a federated privacy-preserving UAV data collection framework to optimize autonomous path planning and protect sensitive maritime information. Using UAVs as edge nodes for decentralized data processing allows the framework to integrate federated learning, maintain data privacy, and improve cybersecurity. The proposed framework contains five distinct layers, where the data collection layer’s role is to collect real-time data on vessel and environmental conditions. The privacy-preserving edge intelligence layer enables secure localized data processing at the edge. The threat mitigation and optimization layer performs machine learning models for route optimization and intrusion detection. The orchestration layer is implemented to coordinate UAV operations and manages aggregated model parameters for system-wide efficiency, whereas the user interaction layer provides operators with secure, real-time insights into system performance and operational metrics. Simulations and implementations demonstrate that this multilayered architecture improves route accuracy, fortifies data security, and achieves a 20% reduction in emissions, underscoring its potential to advance autonomous navigation and secure, efficient mission planning in maritime cyber–physical systems. The proposed edge-intelligent federated UAV system demonstrates superior performance compared to other approaches, achieving the highest accuracy (99.1%), F1 score (98.9%), and recall (99.3%), while utilizing a larger hybrid dataset (80,000 samples) with 30 features, optimized through principal component analysis, and addressing multiple target attributes such as C O 2 emissions, energy efficiency, and route accuracy.},
  archive      = {J_ASOC},
  author       = {Wei Min and Mohammed Saleh Ali Muthanna and Maha Ibrahim and Reem Alkanhel and Ammar Muthanna and Abdelkader Laouid},
  doi          = {10.1016/j.asoc.2025.112906},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112906},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Privacy-preserving federated UAV data collection framework for autonomous path optimization in maritime operations},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unknown web attack threat detection based on large language model. <em>ASOC</em>, <em>173</em>, 112905. (<a href='https://doi.org/10.1016/j.asoc.2025.112905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unknown attacks pose a significant threat to current cyber defenses. Traditional methods for detecting abnormal user behaviors rely on explicit associations and content information, often overlooking implicit causal relationships. Additionally, the frequent emergence of new attack types and the scarcity of training data limit their effectiveness. The paper proposes a novel approach for detecting abnormal user behaviors using large language models (LLMs), addressing these challenges under low-resource conditions. Our method extracts implicit causal relationships from system logs to build behavior graphs and employs label-free graph contrastive invariant learning to generate causal feature vectors. A multi-agent framework, including narrator and decision-maker agents, is used to improve descriptive text generation, while the Translator more efficiently converts causal vectors into meaningful descriptions. Experimental results on the WAB-dataset demonstrate that implicit causal relationships enhance the graph structure’s ability to represent abnormal behaviors. The integration of LLMs enables superior behavior analysis with fewer resources compared to traditional methods. Additionally, the comprehensibility of the generated texts and the efficiency of the Translator provide a strong foundation for supporting security professionals in understanding and analyzing abnormal behaviors in real-world scenarios.},
  archive      = {J_ASOC},
  author       = {Yijia Xu and Qiang Zhang and Huaxin Deng and Zhonglin Liu and Cheng Yang and Yong Fang},
  doi          = {10.1016/j.asoc.2025.112905},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112905},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unknown web attack threat detection based on large language model},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simplified swarm optimization-based restricted boltzmann machine algorithm for a time series clustering. <em>ASOC</em>, <em>173</em>, 112903. (<a href='https://doi.org/10.1016/j.asoc.2025.112903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series clustering is an important area of research, driven by the prevalence of time series data in domains such as power and maintenance data. However, most existing clustering algorithms are not specialized for time series data. Therefore, this study introduces a metaheuristics-based approach, utilizing a restricted Boltzmann machine (RBM) for feature extraction in time series clustering. The RBM, a neural network algorithm with two layers and undirected weights, is kept simple to avoid complex parameter settings. The study employs the RBM as an encoder, coupled with a K -step contrastive divergence and an improved, simplified swarm optimization (iSSO) algorithm for training. The proposed hybrid of iSSO-based RBM and K -means algorithm is compared with RBM and particle swarm optimization-based RBM across the tested time series datasets. Results indicate that the proposed algorithm yields superior performance, reconstructing time series data with minimal error and achieving the highest clustering accuracy compared to other baseline algorithms.},
  archive      = {J_ASOC},
  author       = {Ferani E. Zulvia and R.J. Kuo},
  doi          = {10.1016/j.asoc.2025.112903},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112903},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A simplified swarm optimization-based restricted boltzmann machine algorithm for a time series clustering},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent decision making under conflicting and cooperative scenarios with incomplete information. <em>ASOC</em>, <em>173</em>, 112898. (<a href='https://doi.org/10.1016/j.asoc.2025.112898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real business world, banks and firms often need to make decisions under uncertainty using incomplete information. In this study, an intelligent game arena will be established to demonstrate the dynamic decision process with information asymmetry under heterogeneous competition scenarios (Conflicting and Cooperative). The game is such implemented that actions (drawn from a serial of real policies) and the consequenced rewards for banks and firms are first to be linked by a Hybrid Archimedean Copula ( HAC ). Then, the HAC would be further optimized by the Multi-objective Integer Quadratic Programming ( MIQP ) in order to mimic the dynamic decision process in the true business environment. Finally, a Double Deep-Q Network ( DDQN ) algorithm is used to simulate dynamic decision processes. The intelligent dynamic game presented in this study not only shows the outcomes (optimal rewards, actions, and competition strategies) resulting from authenic dynamic human rational intelligent decision-making processes under conflicting and cooperative competitions scenarios with incomplete information, both also give the probabilities of taking such actions and strategies in order to reach the optimal long-term rewards. The area goes one step forward by presenting the entire convergence processes of the games before they could reach equilibriums based on heterogeneous actions and strategies. As a result, the framework proposed in this study would offer a transparent experimentable arena to dissect the black-boxed human decision-making process in the real business world and thus might be of importance for studying organizational behaviors, operations, and strategies.},
  archive      = {J_ASOC},
  author       = {Chang Liu and Bowen Deng and Xuancheng Ye and Min Guo},
  doi          = {10.1016/j.asoc.2025.112898},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112898},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent decision making under conflicting and cooperative scenarios with incomplete information},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDSINet: A spatiotemporal dual-scale interaction network for traffic prediction. <em>ASOC</em>, <em>173</em>, 112892. (<a href='https://doi.org/10.1016/j.asoc.2025.112892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic forecasting is essential for smart city development. However, existing spatiotemporal modeling methods often face significant challenges, including limitations in handling complex temporal dependencies, capturing multiscale spatial relationships, and modeling the interaction between temporal and spatial features. These challenges arise due to the reliance on extended historical data, fixed adjacency matrices, and the lack of dynamic spatiotemporal interaction modeling. To address these issues, we propose the Spatiotemporal Dual-Scale Interaction Network (SDSINet). SDSINet introduces an implicit temporal information enhancement method that embeds temporal identity information into feature representations, reducing the computational overhead and improving the modeling of global temporal features. Additionally, SDSINet integrates a dynamic multiscale spatial modeling approach that combines adaptive and scale-specific graphs, enabling the model to capture both local and global spatial dependencies. Furthermore, SDSINet incorporates a dual-scale spatiotemporal interaction learning framework that captures short-term and long-term temporal dependencies as well as multiscale spatial correlations. Extensive experiments on real-world datasets – traffic flow (PeMS04), speed (PeMSD7(M)), and demand (NYCBike Drop-off/Pick-up) – demonstrate that SDSINet outperforms existing state-of-the-art methods in prediction accuracy and computational efficiency. Notably, SDSINet achieves a 14.03% reduction in MAE on the NYCBike Drop-off dataset compared to AFDGCN, setting a new benchmark for traffic forecasting.},
  archive      = {J_ASOC},
  author       = {Shiyu Yang and Qunyong Wu},
  doi          = {10.1016/j.asoc.2025.112892},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112892},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SDSINet: A spatiotemporal dual-scale interaction network for traffic prediction},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Many-objective optimisation of offshore wind farms. <em>ASOC</em>, <em>173</em>, 112879. (<a href='https://doi.org/10.1016/j.asoc.2025.112879'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of offshore wind farms is computationally challenging, requiring the simultaneous optimisation of many conflicting objectives. Solving this problem is of paramount importance if society is to meet ambitious net zero goals. The problem is solved by identifying an optimal arrangement of individual turbines such that all objectives are optimised. However, a single solution does not exist due to the inherent conflict between objectives, and a set of solutions must be identified. As well as the challenge in generating optimal solution sets, there exists a decision support task if the solutions are to be effectively presented to a decision maker. This study focuses on six key objectives: wind farm efficiency, annual energy production, electric cable length, number of wind turbines, levelised cost of energy, and total area. Two evolutionary algorithms, NSGA-II and NSGA-III, were employed to explore the solution space efficiently. Performance evaluation was performed using spacing, generational distance, and hypervolume metrics. The aforementioned algorithms and metrics were applied to three wind farm layouts: a discrete layout and two continuous layouts. The NSGA-III algorithm was shown to perform better than its predecessor (NSGA-II). The difference was small, albeit significant. Previous works (e.g., Rodrigues et al. (2016); Mytilinou and Kolios (2017)) in which many-objective optimisations were discussed provided little insight into the visualisation and interpretation of the results. While the mentioned work used parallel coordinate plots, this work provides a deeper insight by presenting the results via Principal Component Analysis (PCA) and Multi Dimensional Scaling (MDS) plots. The best solution, containing 6188 wind farm layouts, was found by the NSGA-III algorithm on a continuous wind farm layout with repair mechanism. From the best solution, the wind farms containing 27, 102 and 160 wind turbines were selected and compared with the real wind farms located around the UK. It was demonstrated that the optimiser could identify better wind farm layouts concerning annual energy production, efficiency, and LCOE than the real wind farm layouts of Rhyl Flats and Greater Gabbard.},
  archive      = {J_ASOC},
  author       = {Pawel L. Manikowski and Matthew J. Craven and David J. Walker},
  doi          = {10.1016/j.asoc.2025.112879},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112879},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Many-objective optimisation of offshore wind farms},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative transformer U-shaped network for medical image segmentation. <em>ASOC</em>, <em>173</em>, 112841. (<a href='https://doi.org/10.1016/j.asoc.2025.112841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in the Transformer have shown significant ability to understand the relationship between the lesion area and surrounding tissue, especially for medical image analysis. Existing medical image segmentation algorithms based on transformers often suffer from limited feature extraction granularity and overlook the semantic relationships between multi-scale features. To solve the above limitations, we propose CoTransUNet: a collaborative transformer U-shaped network, that effectively captures fine-grained features and long-range dependencies by performing context extraction between multiple scales. The designed Correlation Extraction (CE) module bridges the encoder and decoder to achieve effective interaction and information transfer. Specifically, a collaborative mechanism in the encoder is proposed that can efficiently exploit inductive bias to extract local fine-grained features of the image while having the ability to capture long-distance feature dependencies. Besides, the CE module focuses on deeply integrating contextual information of multi-scale features, which enriches feature representation by exploiting the intrinsic correlation between features at different scales. It can extract not only local and global features but also capture semantic information related to different multi-scale features simultaneously. Compared to TransUNet, CoTransUNet achieves a 4.91% improvement in DSC on the Synapse multi-organ segmentation dataset while using only a quarter of the parameters. The extensive experiments on three datasets, including skin lesion segmentation (ISIC2016, ISIC2017, ISIC2018) demonstrates that CoTransUNet achieves DSC scores of 92.18%, 85.59%, and 88.75%, respectively, and on Synapse multi organ segmentation achieves DSC score of 82.39% , which outperforms the baseline and other promising methods.},
  archive      = {J_ASOC},
  author       = {Yufei Gao and Shichao Zhang and Lei Shi and Guohua Zhao and Yucheng Shi},
  doi          = {10.1016/j.asoc.2025.112841},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112841},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Collaborative transformer U-shaped network for medical image segmentation},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage coevolutionary algorithm based on adaptive weights for complex constrained multiobjective optimization. <em>ASOC</em>, <em>173</em>, 112825. (<a href='https://doi.org/10.1016/j.asoc.2025.112825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the constrained multiobjective optimization problems (CMOPs), various complex constraints need to be satisfied simultaneously, which further challenges evolutionary algorithms in balancing feasibility, convergence and diversity. Recent advances in evolutionary computation have led to the development of multi-stage and multi-population strategies for handling CMOPs. However, most algorithms have shown poor performance when dealing with problems with low feasible ratio or discontinuous feasible regions. To address this issue, we propose a two-stage coevolutionary algorithm based on adaptive weights (AW-TCEA), aiming to balance convergence, diversity and feasibility to handle CMOPs with complex Pareto fronts (PFs). Specifically, the first stage uses two populations to explore the objective space and feasible regions respectively, one driven by objective information and the other by feasible information. The second stage adopts a set of weight vectors to search for unexplored feasible regions to enhance diversity. In addition, for handling complex constrained PFs, a novel adaptive weight adjustment strategy is proposed to explore ineffective directions and develop potential regions. Experimental comparisons with multiple state-of-the-art algorithms are performed on 50 test problems and 5 real-world problems. The results show that the proposed algorithm exhibits better performance on various CMOPs.},
  archive      = {J_ASOC},
  author       = {Guangpeng Li and Li Li and Guoyong Cai},
  doi          = {10.1016/j.asoc.2025.112825},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112825},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage coevolutionary algorithm based on adaptive weights for complex constrained multiobjective optimization},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A TG-AGD anomaly image detection model based on residual bottleneck attention and time series prediction. <em>ASOC</em>, <em>173</em>, 112746. (<a href='https://doi.org/10.1016/j.asoc.2025.112746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a crucial task in the field of deep learning, with applications in security monitoring, quality control, and medical diagnosis. However, existing anomaly detection methods often consume significant computational resources, leading to premature bottlenecks. This study proposes an anomaly image detection method based on residual bottleneck attention integrated with time series prediction. Initially, the study explores several common time series prediction models and establishes a time prediction model tailored to the abnormal characteristics of images. It then examines the limitations of current attention mechanisms in anomaly detection. The proposed method combines the most widely applicable and effective residual bottleneck processing with the previously developed time series prediction model to create the TG-AGD anomaly image detection model. Extensive experiments were conducted using detailed anomaly image data from various fields. The results indicate that the TG-AGD anomaly image detection model significantly outperforms traditional single time series prediction or attention mechanism approaches, demonstrating high accuracy and robustness in detecting anomalies.},
  archive      = {J_ASOC},
  author       = {Yang Li and Suqin Xiong and Qiuyang Li and Zhiru Chen},
  doi          = {10.1016/j.asoc.2025.112746},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {112746},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A TG-AGD anomaly image detection model based on residual bottleneck attention and time series prediction},
  volume       = {173},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective optimization algorithm for gene selection and classification in cancer study. <em>ASOC</em>, <em>172</em>, 112911. (<a href='https://doi.org/10.1016/j.asoc.2025.112911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, non-clinical diagnostics and predictive modeling in statistical genomics have gained increased attention, particularly in the analysis of microarray gene expression data. One of the key advantages of using microarray data is its ability to analyze the expression levels of thousands of genes simultaneously. A key challenge remains the efficient identification of a small subset of informative genes that are statistically correlated with specific groups of Messenger Ribonucleic Acid (mRNA) tissue samples, followed by a meaningful biological interpretation of the findings. This paper introduces a two-stage hybrid multi-objective optimization (MOO) algorithm for feature selection and classification of mRNA samples into their respective groups. The proposed method enhances the classification performance of Support Vector Machines (SVMs) by framing gene selection as a MOO problem. Initially, a filter method—using either the t-test or F-test—was employed to eliminate noisy genes, which are prevalent in microarray gene expression data. Subsequently, the genes selected by the filter methods were further refined through a MOO approach, applying Pareto optimality criteria to identify a minimal yet optimal gene subset. The effectiveness of the proposed method was evaluated using both simulated and published high-dimensional microarray datasets, considering out-of-bag (OOB) accuracy, misclassification error rates, and several other performance metrics. The results demonstrated that the proposed method is robust, achieving high prediction accuracy with minimal gene subsets. Furthermore, it outperformed existing methods in its ability to select a small number of gene biomarkers that are strongly correlated with the biological response class.},
  archive      = {J_ASOC},
  author       = {Alabi W. Banjoko and Waheed B. Yahya and Oyebayo R. Olaniran},
  doi          = {10.1016/j.asoc.2025.112911},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112911},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective optimization algorithm for gene selection and classification in cancer study},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An agricultural leaf disease segmentation model applying multi-scale coordinate attention mechanism. <em>ASOC</em>, <em>172</em>, 112904. (<a href='https://doi.org/10.1016/j.asoc.2025.112904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precise identification of disease-affected leaves is crucial for protecting the health of crops in the field of agricultural pest and disease control. To enhance the efficiency and accuracy of disease leaf identification, we introduce a multiscale coordinate attention method called MSCoord along with a semantic segmentation model called MCUNet to improve the effectiveness and accuracy of disease leaf detection. MCUNet has good inference speed and high precision for extracting disease leaf information at various scales. This model is trained with deep supervision, rendering it amenable to pruning and adaptable to tasks of varying scales. Using datasets that are available to the public, we tested MCUNet experimentally. The results show the above-average segmentation ability of MCUNet, achieving a remarkable mIoU of 90.1 % on the Pascal VOC 2012 dataset. When applied to disease leaf segmentation datasets, it achieves an outstanding mIoU of 94.81 %. In the rice leaf disease dataset, which demands high-precision segmentation, MCUNet achieves a mIoU of 92.01 %. MCUNet has considerable potential for study and real-world application in the field of agro-pest and disease management. In the application work of MCUNet model, it performs well and can achieve pixel level segmentation of heterogeneous images in natural apple orchard, providing a prerequisite for elastic registration of heterogeneous images.},
  archive      = {J_ASOC},
  author       = {Renyuan Gu and Liqun Liu},
  doi          = {10.1016/j.asoc.2025.112904},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112904},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An agricultural leaf disease segmentation model applying multi-scale coordinate attention mechanism},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient fuzzy-based high utility pattern computing and analyzing approach with temporal properties. <em>ASOC</em>, <em>172</em>, 112902. (<a href='https://doi.org/10.1016/j.asoc.2025.112902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy logic in soft computing deals with intuitive and comprehensive intelligence to find solutions to problems in the uncertain real world. Considering the fuzzy set concept and knowledge discovery of utility-driven patterns simultaneously, quantities of sets of items hidden within vast data can be represented in an easy-to-understand linguistic representation. This can lead to more reasonable decision-making. Together with these fascinating results, temporal fuzzy utility pattern analysis has emerged as a significant area in the last few years to consider the duration of transactions in temporal quantitative data. The latest temporal approaches have improved resource efficiency by storing information on patterns with efficient data structures. However, although a list-based approach is known to be robust and follows a mechanism that does not generate candidates, it requires explosive comparison operations that are unsuitable for processing long-length patterns, especially in big data analysis. To solve this issue, we present a novel indexed list-based structure along with a data analysis method designed to allow rapid pattern growth as well as prevent the generation of candidates for discovering high temporal fuzzy utility patterns. Performance tests on real and synthetic datasets demonstrate that the proposed approach exhibits superior time efficiency and scalability relative to state-of-the-art methods with minimal compromise in memory, all while extracting accurate results. Moreover, comprehensive experiments demonstrate the capability of the proposed method for practical use cases and its effectiveness in search space pruning.},
  archive      = {J_ASOC},
  author       = {Unil Yun and Hyeonmo Kim and Hanju Kim and Seungwan Park},
  doi          = {10.1016/j.asoc.2025.112902},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112902},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient fuzzy-based high utility pattern computing and analyzing approach with temporal properties},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven evolutionary algorithms based on initialization selection strategies, POX crossover and multi-point random mutation for flexible job shop scheduling problems. <em>ASOC</em>, <em>172</em>, 112901. (<a href='https://doi.org/10.1016/j.asoc.2025.112901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the fields of manufacturing and production, the precise solution of the flexible job shop scheduling problem (FJSP) is crucial for improving production efficiency and optimizing resource allocation. However, the complexity of FJSP often leads traditional optimization methods to face high computational costs and lengthy processing times. To address this problem, we propose a data-driven evolutionary algorithm based on initialization selection strategies, POX crossover, and multi-point random mutation (DDEA-PMI). This algorithm replaces the real objective function by constructing a radial basis function (RBF) surrogate model to reduce expensive computational costs and shorten solution time. In the process of solving FJSP, we use global selection (GS), local selection (LS), and random selection (RS) initialization selection strategies to obtain an initial population with high diversity. In order to reduce the generation of infeasible solutions, we use the POX crossover operator, which selects partial gene sequences from the parent generation and maps them to the offspring to preserve excellent features and ensure the feasibility of the solution. In addition, we design a multi-point random mutation operation to enhance the diversity of the population. Through the multi-point mutation strategy, it is able to explore more comprehensively in the solution space to increase the possibility of finding the optimal solution. To verify the effectiveness of DDEA-PMI, we compare it with three same types of data-driven evolutionary algorithms. We compare and analyze the DDEA-PMI with three algorithms after removing one of our proposed strategies. The experimental results show that DDEA-PMI is effective and has advantages in solving FJSP.},
  archive      = {J_ASOC},
  author       = {Ruxin Zhao and Lixiang Fu and Jiajie Kang and Chang Liu and Wei Wang and Haizhou Wu and Yang Shi and Chao Jiang and Rui Wang},
  doi          = {10.1016/j.asoc.2025.112901},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112901},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven evolutionary algorithms based on initialization selection strategies, POX crossover and multi-point random mutation for flexible job shop scheduling problems},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A differential evolution algorithm with diversity dynamic adjustment and two-phase constraint handling strategy for solving a pension fund investment problem under market uncertainty. <em>ASOC</em>, <em>172</em>, 112900. (<a href='https://doi.org/10.1016/j.asoc.2025.112900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the optimization of investment strategies (ISs) for pension fund under market uncertainty. The target for this problem is to maximize the overall desired wealth (ODW) at the closed of the planning period while considering the risk of market uncertainty. To begin with, this problem is modeled as a discrete-time stochastic dynamic optimization problem (DTSDOP) with hard constraints (HCs) and uncertain constraints (UCs). Different from existing models, the main goal is to maximize ODW, rather than maximizing the wealth for individual investors, and to make sure that the pension fund is able to fulfill all obligations with a desired probability, this model introduces UCs instead of constraints on the variance. Additionally, this model takes into account the risk of market uncertainty, which helps to compare the optimal ISs with and without market uncertainty. Then, a deterministic transformation method is proposed for converting DTSDOP to a deterministic problem with HCs by fully utilizing the information of DTSDOP. Following that, a diversity dynamic adjustment and two-phase constraint handling strategy-based differential evolution algorithm (DDA-TPCHS-DEA) is proposed for attaining a global optimum for the discrete-time deterministic dynamic optimization problem (DTDDOP), in which constraints and the cost function are balanced by a diversity dynamic adjustment strategy, and a two-phase constraint handling strategy is utilized for finding the boundary knowledge for feasible and infeasible domains. Finally, the superiority of the proposed method is illustrated utilizing 21 test functions, an optimal design problem for robot grippers, and a pension fund investment problem under market uncertainty. The source code of the proposed method and its supplementary material are available in the following GitHub repository: https://github.com/xwu-gznu/DDA-TPRHS-DEA},
  archive      = {J_ASOC},
  author       = {Xiang Wu and Haozheng Meng and Xiaolan Yuan and Qunxian Zheng and Jinxing Lin and Kanjian Zhang},
  doi          = {10.1016/j.asoc.2025.112900},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112900},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A differential evolution algorithm with diversity dynamic adjustment and two-phase constraint handling strategy for solving a pension fund investment problem under market uncertainty},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The cross-interval reconstruction and heuristic calculation to deal with the continuous-valued attribute in the learning process. <em>ASOC</em>, <em>172</em>, 112897. (<a href='https://doi.org/10.1016/j.asoc.2025.112897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discretization is a widely used technique for organizing and simplifying continuous-valued data in classification problems, facilitating subsequent analytical applications. However, the lack of an effective approach for addressing soft boundary classification can complicate the interpretation and accuracy of results. This paper focuses on discretization methods within soft boundary classification environments, aiming to improve classification outcomes and enhance interpretability for specific applications. Building on rule-based discretization, we propose a novel approach to enhance the handling of soft boundary classification. A detailed illustrative example is provided to demonstrate the effectiveness of the proposed Cross-Interval Recursion (CIR) method and Heuristic Cross-Interval Recursion (HCIR) algorithm. Our results show that the CIR-based rule discretization method and its evaluation mechanism effectively mitigate noise interference from class boundary points, improving interpretability and promoting greater generalization in soft boundary classification. The performance of our algorithm outperforms existing methods, including EqQua-CIR, EqVal-CIR, and other rule-based discretization techniques, particularly in terms of classification accuracy when dealing with boundary points at high granularity. When compared to classic classifiers and other rule-based discretization approaches, our method demonstrates that rule-based classifiers are more effective than direct approaches in handling soft boundary issues. Furthermore, the alignment between the classifier and the sample data plays a critical role in determining classification performance. Our approach offers significant potential as a breakthrough in addressing soft boundary classification of continuous-valued attributes, leveraging interval reconstruction, and enhancing classification robustness.},
  archive      = {J_ASOC},
  author       = {Wei Zhou and Wenqiang Zhu and Jin Chen and Zeshui Xu},
  doi          = {10.1016/j.asoc.2025.112897},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112897},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The cross-interval reconstruction and heuristic calculation to deal with the continuous-valued attribute in the learning process},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained recognition of grape leaf diseases based on transfer learning and convolutional block attention module. <em>ASOC</em>, <em>172</em>, 112896. (<a href='https://doi.org/10.1016/j.asoc.2025.112896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grape leaves are critical indicators for disease monitoring. Implementing precise prevention and control measures based on the type and severity of diseases is crucial for improving grape yields. However, the subtle differences in leaf features across disease types and progression stages pose significant challenges for traditional classification methods. Existing convolutional neural network (CNN)-based approaches, though capable of automatic feature extraction, suffer from large parameter sizes, high computational complexity, and limited generalization, making them unsuitable for resource-constrained applications. This paper proposes a lightweight grape disease recognition method based on the GC-MobileNet model, designed for efficient classification and fine-grained grading of diseases. GC-MobileNet improves the MobileNetV3 model by incorporating Ghost modules to replace some inverted residual structures, significantly reducing parameters while enhancing feature extraction efficiency. Additionally, the CBAM (Convolutional Block Attention Module) attention mechanism is employed to strengthen the model's spatial and channel feature representation capabilities. The LeakyReLU activation function is introduced to preserve more positive and negative features in feature maps, addressing deficiencies in disease feature extraction. Furthermore, transfer learning and data augmentation strategies are adopted to enhance the model's generalization in complex scenarios. Experiments on a dataset comprising various grape leaf disease types and severity levels demonstrate that GC-MobileNet achieves a classification accuracy of 98.63 %, improving by 6.51 % compared to MobileNetV3, with significant performance advantages over VGG16, ResNet50, and SqueezeNet. The model's parameter size is only 2.08 M, significantly reducing memory usage and improving convergence speed, showcasing strong robustness and adaptability to resource-constrained environments. With its efficiency, lightweight design, and robustness, GC-MobileNet offers a reliable technical solution for precision agricultural management and mobile-based disease detection.},
  archive      = {J_ASOC},
  author       = {Wu Canghai and Gu Xingxiang and Xiong Huanliang and Huang Huixin},
  doi          = {10.1016/j.asoc.2025.112896},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112896},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fine-grained recognition of grape leaf diseases based on transfer learning and convolutional block attention module},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-stage filter-based local feature selection using an immune algorithm for high-dimensional microarray data. <em>ASOC</em>, <em>172</em>, 112895. (<a href='https://doi.org/10.1016/j.asoc.2025.112895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is crucial for disease classification and prognosis in high-dimensional microarray data, as it reduces dimensionality, enhances model accuracy, and improves computational efficiency. However, most existing methods rely on a global set of features and overlook the relationships between features and sample subspaces. To address this limitation, this study proposes a filter-based local feature selection method based on an immune algorithm (IA-FLFS). This method dynamically assigns a unique feature subset to each sample neighborhood, replacing reliance on a global feature subset. It incorporates three key innovations: (1) a single-stage, filter-based approach that considers feature interactions and is independent of any learning model, ensuring high effectiveness and efficiency for high-dimensional datasets; (2) an enhanced clonal selection algorithm is utilized to identify feature subsets, enhancing search capabilities through filter-based initialization, adaptive differential evolution-based mutation, and symmetric uncertainty-based local search. (3) thread-level parallelism is applied to each feature subset, significantly reducing computation time. Experimental results on twelve datasets demonstrate IA-FLFS’s superior accuracy, efficiency, and ability to produce smaller feature subsets, outperforming fourteen state-of-the-art feature selection methods on most datasets. Notably, compared to other local feature selection algorithms, it achieves significant accuracy improvements on over eight datasets, highlighting its potential as a powerful and efficient tool for high-dimensional microarray analysis.},
  archive      = {J_ASOC},
  author       = {Yi Wang and Wenshan Li and Tao Li and Hao Tian},
  doi          = {10.1016/j.asoc.2025.112895},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112895},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Single-stage filter-based local feature selection using an immune algorithm for high-dimensional microarray data},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-lingual entity alignment based on complex relationships and fine-grained attributes. <em>ASOC</em>, <em>172</em>, 112894. (<a href='https://doi.org/10.1016/j.asoc.2025.112894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph entity alignment seeks to match equivalent entities across different graphs, a critical task for enabling cross-lingual knowledge fusion. Mainstream methods use representation learning for entity alignment based on vector distances, but struggle with complex relational semantics and underutilize fine-grained attribute information crucial for alignment. To overcome the above problems, this paper proposes a c ross-lingual e ntity a lignment model based on complex r elationships and fine-grained a ttributes (CEARA). The proposed model effectively handles relational semantics by distinguishing their varying impacts on entity embeddings and extracting detailed attribute information to enhance alignment accuracy. Additionally, it integrates entity name string similarity to complement missing or noisy relational and attribute data, further improving alignment reliability. To mitigate alignment conflicts, the model employs a global alignment strategy. Experimental results on three cross-lingual datasets demonstrate that CEARA not only outperforms representative baseline models but also achieves Hits@1 scores exceeding 95% across all datasets, highlighting its effectiveness and robustness for cross-lingual alignment. This paper contributes to the advancement of cross-lingual knowledge discovery and application.},
  archive      = {J_ASOC},
  author       = {Beibei Zhu and Ruijie Tian and Xiaosong Yuan and Ridong Han and Yan Yang and Bo Fu},
  doi          = {10.1016/j.asoc.2025.112894},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112894},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-lingual entity alignment based on complex relationships and fine-grained attributes},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting mechanical properties of concrete structures using metaheuristic-optimization-based machine learning models. <em>ASOC</em>, <em>172</em>, 112893. (<a href='https://doi.org/10.1016/j.asoc.2025.112893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More powerful machine learning (ML) models for predicting the behavior of concrete structures could considerably improve the efficiency and safety of civil engineering. This study presents a Metalearning system for systematically integrating metaheuristic optimization algorithms with ML models to create robust hybrid models. The system combines 15 advanced metaheuristic algorithms with 11 powerful ML models, generating over 150 hybrid ensemble models. This research is distinguished by the creation of a comprehensive hybridized ML framework, the development of novel hybrid models not previously explored in the literature, and the demonstrated superiority of metaheuristic-optimized homogeneous ensemble models over traditional ensemble and single hybrid models. The effectiveness of the proposed system was validated through three real-world case studies, showcasing superior predictive performance compared to existing ML models and traditional structural formulas. In particular, a least-squares support vector regression (LSSVR) model optimized with forensic-based investigation (FBI) achieved the highest accuracy for predicting shear strength in two-way flat reinforced concrete slabs; its root mean square error was 73.06 kN, mean absolute error was 42.82 kN, mean absolute percentage error was 10.58 %, and R 2 was 0.970. The FBI-(ANN+LSSVR) outperformed other models in predicting the ultimate bearing capacity of shallow foundations. The FBI-LSSVR-FS model achieved outstanding predictive performance for the construction cost index with MAPE values of 1.23 %. The key contributions of this study are the establishment of a reliable system for advanced ML hybridization to enhance generalizability, development of numerous innovative ensemble models that have not been previously used, and development of a user-friendly interface to support structural engineers in effectively applying ML-based inference models to solve practical problems in the civil engineering.},
  archive      = {J_ASOC},
  author       = {Ngoc-Mai Nguyen},
  doi          = {10.1016/j.asoc.2025.112893},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112893},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting mechanical properties of concrete structures using metaheuristic-optimization-based machine learning models},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fill-UNet: Extended composite semantic segmentation. <em>ASOC</em>, <em>172</em>, 112891. (<a href='https://doi.org/10.1016/j.asoc.2025.112891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of image segmentation directly affects the precision of object recognition. To address the limitations of the U-Net model in capturing global contextual information and leveraging deep semantic features, an extended composite semantic segmentation model, Fill-UNet, is proposed for extracting deep semantic features. The proposed Semantic Collaborative Filtering Attention Module (SCFAM) enhances the model's ability to perceive channel information by compressing channel-direction features while preserving high-resolution mask semantic details. The integration of a Transformer structure into the encoder-decoder connections enables the extraction of deep pixel-level features. Furthermore, the designed Multi-Scale Semantic Feature Association mechanism (MSFA), which combines short and long skip connections with a Parallel Weighted Fusion Module (PWFM), strengthens multi-scale feature fusion, particularly for small objects. Extensive experiments conducted on the PASCAL VOC2012, Cityscapes, CamVid, and Pascal Context datasets demonstrate that Fill-UNet achieves significant improvements in pixel segmentation accuracy while offering a slight increase in segmentation speed compared to state-of-the-art methods. The code will be available at https://github.com/ZzYH-i/Fill-UNet .},
  archive      = {J_ASOC},
  author       = {Qunpo Liu and Yi Zhao and Weiping Ding and Xuhui Bu and Naohiko Hanajima},
  doi          = {10.1016/j.asoc.2025.112891},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112891},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fill-UNet: Extended composite semantic segmentation},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of deep learning algorithms for predicting construction project delays in saudi arabia. <em>ASOC</em>, <em>172</em>, 112890. (<a href='https://doi.org/10.1016/j.asoc.2025.112890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction projects in Saudi Arabia often encounter delays, which present significant challenges to project managers and result in financial losses and stakeholder dissatisfaction. Effectively managing these delays is essential for maintaining project timelines and optimizing resource use. This study explores the hypothesis that advanced deep learning algorithms can significantly improve the prediction and management of construction project delays in Saudi Arabia. The research focuses on three algorithms: Generative Adversarial Networks (GAN), Long Short-Term Memory (LSTM), and Multilayer Perceptron (MLP), evaluating their effectiveness across datasets with varying class imbalances. A structured methodology was employed to assess the algorithms based on key performance metrics, including accuracy, precision, sensitivity, specificity, and misclassification errors. GAN, LSTM, and MLP were trained and tested using real-world construction project data, incorporating tools such as k-fold cross-validation for validation. The GAN model achieved the highest accuracy at 91 %, with a misclassification error of 9 %, outperforming both LSTM (accuracy: 88 %, error: 12 %) and MLP (accuracy: 83 %, error: 17 %). GAN also demonstrated superior precision (90 %) and sensitivity (87 %), making it the most reliable algorithm for delay risk assessment. While LSTM was effective, it had slightly lower precision (88 %) but exhibited strong generalization to unseen data. MLP showed the weakest performance, with higher misclassification rates and lower robustness. These findings suggest that deep learning models, particularly GAN, can significantly improve decision-making and delay mitigation in construction projects.},
  archive      = {J_ASOC},
  author       = {Saleh Alsulamy},
  doi          = {10.1016/j.asoc.2025.112890},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112890},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comparative analysis of deep learning algorithms for predicting construction project delays in saudi arabia},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An ensemble deep learning network based on 2D convolutional neural network and 1D LSTM with self-attention for bearing fault diagnosis. <em>ASOC</em>, <em>172</em>, 112889. (<a href='https://doi.org/10.1016/j.asoc.2025.112889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent classification methods based on deep learning (DL) have become widely adopted for bearing fault diagnosis (BFD). However, it is acknowledged that relying on single feature extraction methods may not yield comprehensive representations of the information features. Additionally, DL-based approaches for extracting features from vibration signals typically utilize either one-dimensional (1D) or two-dimensional (2D) networks, which can restrict the network's ability to extract features effectively. In this paper, a time series data representation method called the relative angle matrix (RAM) method is firstly proposed. This method converts 1D time series into 2D images by calculating the angle differences between multiple vectors and a central vector, thereby extracting the hidden spatial features present in the original data. Then, this paper introduces an ensemble deep learning network called 1D2D-EDL, which integrates 1D-based and 2D-based DL mechanisms for feature extraction and classification, leveraging the strengths of each approach. The 1D2D-EDL comprises two channels: the 1D channel combines long short-term memory (LSTM) and multi-head self-attention (MSA) to process raw 1D time series data, facilitating feature extraction in both the time and frequency domains. Meanwhile, the 2D channel employs convolutional neural network (CNN) components to process 2D images for spatial feature extraction, which are derived from the original time series data using the RAM method. Finally, the feature information from these two channels is fused using a feature fusion method. To preliminarily validate the effectiveness of the RAM method, three competitive 2D conversion methods are employed, including Gramian angular difference field (GADF), Gramian angular sum field (GASF), and Markov transition field (MTF). These methods are applied alongside the proposed RAM method within the same CNN network for fault diagnosis testing. The results indicate that the RAM method significantly enhances the diagnostic accuracy of the CNN compared to the other 2D conversion methods. Furthermore, the bearing fault dataset from the University of Ottawa is utilized to validate the performance of the 1D2D-EDL. A comparative analysis with other DL methods using multiple statistical metrics demonstrates the superiority of the 1D2D-EDL. Specifically, when diagnosing faults under four different speed conditions, the 1D2D-EDL attains accuracy rates of 100 %, 99.33 %, 100 %, and 100 %, respectively. This study proposes the incorporation of a novel perspective classifier to enhance DL models for bearing fault diagnosis. The source code of RAM is available at https://ww2.mathworks.cn/matlabcentral/fileexchange/180197-relative-angle-matrix-ram .},
  archive      = {J_ASOC},
  author       = {Liying Wang and Weiguo Zhao},
  doi          = {10.1016/j.asoc.2025.112889},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112889},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble deep learning network based on 2D convolutional neural network and 1D LSTM with self-attention for bearing fault diagnosis},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An innovative aggregation operator for enhanced decision-making: A study on interval-valued pythagorean fuzzy soft sets in material selection. <em>ASOC</em>, <em>172</em>, 112888. (<a href='https://doi.org/10.1016/j.asoc.2025.112888'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the use of Pythagorean fuzzy soft sets (PFSSs) in group decision-making to handle the uncertainty and ambiguity in information data. Aggregation operators take an important role in separating problems, from the perception of two prospect circulations and performing perceptivity between them. In this article, we generalize the notion of interval-valued intuitionistic fuzzy soft sets (IVIFSSs) into interval-valued PFSSs (IVPFSSs) to enhance decision-making processes under conditions of uncertainty and ambiguity. Compared to the current interval-valued Pythagorean fuzzy set, the IVPFSSs handle uncertain and ambiguous information with efficiency. To support this framework, new operational laws for IVPFSSs are developed. Furthermore, two innovative aggregation operators i.e. the interval-valued Pythagorean fuzzy soft set weighted average (IVPFSSWA) and the interval-valued Pythagorean fuzzy soft set weighted geometric (IVPFSSWG)—have been constructed based on the operational laws with their fundamental properties. These operators facilitate the aggregation of fuzzy data effectively and reliably. To address this, a resilient multi-criteria decision-making (MCDM) technique is designed using the proposed aggregation operators, specifically for material selection in product design and manufacturing. A real-world application illustrates the practicality of the method, focusing on material selection. The results highlight the model’s effectiveness and reliability in handling fuzzy data based on IVPFSSs. Finally, the results are compared with some existing operators to check the reliability of the proposed aggregation operators. In addition, a comprehensive comparison analysis is carried out to establish the enhanced performance, feasibility and robustness of the obtained results. However, there are number of shortcomings to the suggested operators, such as their high computational complexity, sensitivity to weight assignments, complexity in interpretability and difficulties in managing uncertainty.},
  archive      = {J_ASOC},
  author       = {Diptirekha Sahoo and Prashanta Kumar Parida and Sandhya Priya Baral and Bibudhendu Pati},
  doi          = {10.1016/j.asoc.2025.112888},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112888},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An innovative aggregation operator for enhanced decision-making: A study on interval-valued pythagorean fuzzy soft sets in material selection},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JaunENet: An effective non-invasive detection of multi-class jaundice deep learning method with limited labeled data. <em>ASOC</em>, <em>172</em>, 112878. (<a href='https://doi.org/10.1016/j.asoc.2025.112878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jaundice, caused by elevated bilirubin levels, manifests as yellow discoloration of the eyes, mucous membranes, and skin, often serving as a clinical indicator of conditions such as hepatitis or liver cancer. This study introduces a non-invasive, multi-class jaundice detection framework that utilizes weakly supervised pre-training on large-scale medical images, followed by transfer learning and fine-tuning on 450 collected jaundice cases. Compared to existing studies, our classification approach is more detailed, encompassing a wider range of jaundice samples, including cases of occult jaundice, thereby enabling the accurate detection of more complex and subtle forms of the condition. Our model demonstrates exceptional performance on an independent test set, achieving an accuracy of 98.9 %, sensitivity of 0.991, specificity of 0.999, AUC of 0.999, and an F1-score of 0.990. Notably, the model’s computational efficiency is optimized for mobile deployment, requiring only 0.128 GFLOPs per image. Furthermore, the reliability of the model in identifying nuanced pathological features is validated through SHAP-based interpretability analyses. These findings highlight that weakly supervised pre-training outperforms methods reliant on detailed annotations, providing profound insights into small-sample deep learning applications in medical imaging and paving the way for more precise and scalable diagnostic tools.},
  archive      = {J_ASOC},
  author       = {Yuanting Ma and Yu Meng and Xiaojun Li and Yutong Fu and Yan Xu and Yanfei Lu and Futian Weng},
  doi          = {10.1016/j.asoc.2025.112878},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112878},
  shortjournal = {Appl. Soft. Comput.},
  title        = {JaunENet: An effective non-invasive detection of multi-class jaundice deep learning method with limited labeled data},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis and evaluation of autoencoder-driven dimensionality reduction for face recognition pipelines. <em>ASOC</em>, <em>172</em>, 112877. (<a href='https://doi.org/10.1016/j.asoc.2025.112877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The artificial intelligence landscape provides an ever-growing range of technologies for biometric identification, many of which use facial recognition. Usually, it conveys the usage of a database connection for storing and matching facial features, which raises privacy and security concerns. A novel Autoencoder-Driven Dimensionality Reduction ( A D D R ) architecture is proposed that enables connectionless biometric identification pipelines using a low-density QR code. It allows direct facial validation as a self-contained asset without storage or connection requirements. ADDR has been tested on top of Google’s FaceNet model embeddings and reconstructs them with a minimal loss. The accuracy in the reconstructions lets us classify the faces using the same metrics as the ones used in the original FaceNet model. At the same time, the embeddings’ compactness allows them to be stored in low-density QR codes that standard cameras can process. Several autoencoder strategies and loss functions have been designed and tested using open-source facial datasets, considering the underlying geometry of the embeddings. Also, an efficient QR encoding mechanism is defined and tested to increase the compression capabilities. The iVAA(V) (Inter-Vector Angular Alignment loss with V dimensionality) is demonstrated as the best-performing ADDR configuration. Using a dimensionality reduction of 128/16, it achieved an AUC of 0.956, which is close to FaceNet’s performance and improves the results of other dimensionality reduction techniques.},
  archive      = {J_ASOC},
  author       = {Alvaro Paricio-Garcia and Miguel A. Lopez-Carmona and Sergio Sierra-Arquero and Pablo Manglano-Redondo},
  doi          = {10.1016/j.asoc.2025.112877},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112877},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analysis and evaluation of autoencoder-driven dimensionality reduction for face recognition pipelines},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A seasonal-series LSTM network for irregular urban function zone recognition using sentinel-2 images. <em>ASOC</em>, <em>172</em>, 112876. (<a href='https://doi.org/10.1016/j.asoc.2025.112876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban Functional Zone (UFZ) serves as the fundamental unit for urban planning and management, exerting a significant influence on the enhancement of urban administrative efficacy and the optimization of urban spatial configurations. The delineation of UFZs has benefited from the rich information and fine features provided by high spatial resolution (HSR) remote sensing images. It is recognized that the temporal dynamics of ground objects exhibit seasonal disparities across various UFZs. However, HSR images typically lack seasonal information and come with high acquisition costs. Therefore, this study introduces a novel classification framework for UFZ, leveraging the all-seasonal availability of Sentinel-2 remote sensing images. This framework is designed to capture the spectral, spatial, and temporal features intrinsic to UFZs, thereby enabling a detailed mapping of these zones. The proposed method is articulated in three sequential stages: Initially, to balance the significant scale difference of block (i.e., the fundamental mapping unit) size in 10-meter resolution remote sensing images, an adaptive gradient perception (AGP) mechanism is used to guide the feature extraction of different-scale blocks. Subsequently, the bag of visual words (BOVW) model is deployed to distill block-level spectral-spatial features. This is complemented by the introduction of a seasonal series LSTM network, engineered to apprehend block-level temporal dynamic, particularly focusing on the spectral-temporal signatures that distinguish different UFZs. The proposed framework is applied to UFZ classification in five typical cities in China. The resultant overall accuracy (OA) for all cases reaches around 93 %, marking a noteworthy improvement of approximately 7 % over existing methods. Our results demonstrate the superiority and portability of this framework, as well as the significant potential of open-source remote sensing images in large-scale UFZ mapping.},
  archive      = {J_ASOC},
  author       = {Ting Hu and Mengyu Han and Zixuan Guo},
  doi          = {10.1016/j.asoc.2025.112876},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112876},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A seasonal-series LSTM network for irregular urban function zone recognition using sentinel-2 images},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of hofstede’s cultural dimensions in 50 countries using TFN-generalized choquet integrals. <em>ASOC</em>, <em>172</em>, 112875. (<a href='https://doi.org/10.1016/j.asoc.2025.112875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel application of the triangular fuzzy number (TFN)-generalized Choquet integral to Hofstede’s cultural dimensions data from 50 countries, introducing a robust methodology for analyzing cultural diversity and its economic implications. A 5-level standard evaluation is proposed as an integrated value indicator, enabling a comprehensive interpretation of Hofstede’s cultural dimensions and their relationships with key economic indicators, including GDP, unemployment rate, and population growth rate. To operationalize this evaluation, an algorithm is developed to quantify cultural dimensions into standardized linguistic symbols based on the 5-level evaluation. This algorithm integrates the TFN-generalized Choquet integral with fuzzy measures and a utility function, facilitating the classification of countries into tendency levels for comparative analysis. The algorithm enhances interpretability by mapping numerical data to intuitive categories, making the findings actionable for policymakers. Through a comparative analysis, the study identifies role model countries exhibiting exemplary development patterns and explores correlations between cultural tendency levels and economic performance. The findings validate three hypotheses linking cultural dimensions to economic outcomes and provide actionable recommendations to enhance economic growth, employment, and population dynamics. This framework equips policymakers with valuable tools for leveraging cultural awareness to drive sustainable economic development.},
  archive      = {J_ASOC},
  author       = {Haejin Jang and Dojin Kim and Hyeonseo Kim and Lee-Chae Jang},
  doi          = {10.1016/j.asoc.2025.112875},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112875},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Applications of hofstede’s cultural dimensions in 50 countries using TFN-generalized choquet integrals},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep-based gaussian mixture model algorithm for large-scale many objective optimization. <em>ASOC</em>, <em>172</em>, 112874. (<a href='https://doi.org/10.1016/j.asoc.2025.112874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the number of decision variables increases, the curse of dimensionality becomes a significant challenge in many practical multi-objective optimization problems. This issue is further exacerbated in large-scale many-objective optimization problems (MaOPs), where the growing number of optimization objectives makes it increasingly difficult for evolutionary algorithms to find optimal solutions. In this study, we propose a deep Gaussian mixture model algorithm tailored for large-scale MaOPs. The novelty of this approach lies in its hierarchical detection of interactions and redundancies among decision variables, enabling a more effective grouping of variables. Specifically, a Gaussian mixture model-based framework is used to model the problem, allowing for the preliminary grouping of decision variables. The proposed Grouping Decision Variables using the Gaussian Mixture Model (GDVG) algorithm categorizes variables into two types: convergence-related and diversity-related variables. Additionally, a Linkage Identification Measurement with Chaos (LIMC) method is introduced for grouping convergence-related variables based on their interactions. For diversity-related variables, we present a Trivial Variable Detection Scheme (TVDS) to identify and group variables that contribute to diversity. The experimental results demonstrate that the proposed method outperforms other competitive algorithms on most benchmark test cases, particularly showcasing its effectiveness in large-scale MaOPs.},
  archive      = {J_ASOC},
  author       = {Mingjing Wang and Xiaoping Li and Long Chen and Huiling Chen and Chi Chen and Minzhe Liu},
  doi          = {10.1016/j.asoc.2025.112874},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112874},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep-based gaussian mixture model algorithm for large-scale many objective optimization},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A constraint priority decision framework for constrained multi-objective optimization with complex feasible regions. <em>ASOC</em>, <em>172</em>, 112873. (<a href='https://doi.org/10.1016/j.asoc.2025.112873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained multi-objective optimization problems (CMOPs) present significant challenges due to the simultaneous consideration of objectives and constraints, which becomes particularly arduous when the feasible regions are exceedingly complex. Most of the existing algorithms fail to obtain high-quality solutions for the CMOPs with complex feasible regions. To address this issue, this paper proposes a Constraint Priority Decision framework applied to multi-stage evolutionary algorithms, which incorporates constraints sequentially throughout the solution process to facilitate the retention of optimal diversity and feasibility within the population. Specifically, the proposed framework employs a traditional multi-objective evolutionary algorithm as the optimizer and decomposes various constraints of the CMOP. These constraints are introduced independently into the optimizer, generating an index value for each respective constraint. Following this, a judgment matrix is constructed based on these indices to grade the constraints, thus facilitating the establishment of a priority sequence for multiple constraints. Furthermore, a two-stage strategy is implemented in this study. After incorporating all constraints into the algorithm, the ϵ -constrained method is utilized to impose constraints on the entire problem to increase the genetic diversity of the population while maintaining the feasibility of the population. The experimental results derived from four popular benchmark suites and six real-world applications indicate that the proposed framework surpassed multiple state-of-the-art constrained multi-objective evolutionary algorithms in addressing CMOPs with complex feasible regions.},
  archive      = {J_ASOC},
  author       = {Pengguo Yan and Ye Tian and Jiesheng Wang and Yu Liu},
  doi          = {10.1016/j.asoc.2025.112873},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112873},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A constraint priority decision framework for constrained multi-objective optimization with complex feasible regions},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection using guided population based genetic algorithm with modified crossover and parent selection. <em>ASOC</em>, <em>172</em>, 112872. (<a href='https://doi.org/10.1016/j.asoc.2025.112872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the contemporary landscape, the imperative for cost-effective solutions is paramount, especially when dealing with extensively large dimensional datasets like gene expression datasets. The use of machine learning and data mining techniques in processing these voluminous and complex datasets presents a significant challenge in terms of time and resource consumption. A notable obstacle in dataset analysis is the prevalence of extraneous features or attributes. This is particularly evident in numerous medical datasets, which are often burdened with unnecessary attributes, complicating the task of classifications or prediction algorithms in obtaining precise results. However, the application of metaheuristic optimization algorithms shows remarkable proficiency in isolating pertinent feature vectors, thus markedly improving the efficiency and cost-effectiveness of data processing endeavors. We propose a novel feature selection method using a Genetic Algorithm (GA) that enhances initial population diversity by clustering features during initialization. The paper also introduces a modified crossover technique for generating offspring and employs an adaptive threshold-based Roulette Wheel for parent selection, ensuring effective feature selection. We evaluate the proposed feature selection method on 17 UCI datasets with 3 of them having a very high number of features and the obtained results are found to be better than many state-of-the-art methods both in terms of the classification accuracy and the reduction in the number of features. We also apply our method on 5 microarray-based gene expression datasets, used for the prediction of cancer, in order to ensure scalability and robustness of our method as a feature selector in real-life scenarios. This link provides the source code of the proposed method.},
  archive      = {J_ASOC},
  author       = {Anurup Naskar and Soumyajit Ghosh and Mahantapas Kundu and Ram Sarkar},
  doi          = {10.1016/j.asoc.2025.112872},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112872},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection using guided population based genetic algorithm with modified crossover and parent selection},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning for electrocardiogram classification using lead correlation and decorrelation. <em>ASOC</em>, <em>172</em>, 112871. (<a href='https://doi.org/10.1016/j.asoc.2025.112871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the development of deep learning has shown potential in the automatic analysis of electrocardiogram (ECG), aiding cardiologists in detecting cardiovascular diseases (CVDs). Generally, deep learning models depend on numerous labeled ECGs to train, but manual labeling of ECGs is costly as it requires considerable time and expertise. Self-supervised learning (SSL) can solve this problem by pretraining deep learning models with unlabeled ECGs, mitigating their reliance on labeled ECGs. This work proposes lead correlation and decorrelation (LCD) for effective and efficient SSL of ECGs. Concretely, LCD combines intra-lead correlation, inter-lead correlation, intra-lead and inter-lead decorrelation in pretraining. These mechanisms utilize multilead ECG characteristics: intra-lead invariance, inter-lead invariance, inter-lead variance, and intra-lead redundancy. After pretraining, LCD can provide a generic encoder for feature extraction of any ECG lead in a classification task. Benefitting from the effective pretraining mechanism, models with the encoders pretrained by LCD outperform most of the baselines. Compared with the best baseline, they achieve better/comparable classification performances in the same tasks with less pretraining time. Furthermore, LCD helps the models focus on critical features when training with insufficient labeled ECGs, reducing the reliance on labeled ECGs by 4 ∼ 6 × . All the results demonstrate that LCD is an effective and efficient method, boosting a broader application of deep learning to automatic ECG analysis. The code is available at https://github.com/Aiwiscal/ECG_SSL_LCD .},
  archive      = {J_ASOC},
  author       = {Wenhan Liu and Shurong Pan and Sheng Chang and Qijun Huang and Nan Jiang},
  doi          = {10.1016/j.asoc.2025.112871},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112871},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-supervised learning for electrocardiogram classification using lead correlation and decorrelation},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel attribute reduction in high-dimensional data: An efficient MapReduce strategy with fuzzy discernibility matrix. <em>ASOC</em>, <em>172</em>, 112870. (<a href='https://doi.org/10.1016/j.asoc.2025.112870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hybrid paradigm of fuzzy-rough set theory, which combines fuzzy and rough sets, has proven effective in attribute reduction for hybrid decision systems encompassing both numerical and categorical attributes. However, current parallel/distributed approaches are limited to handling datasets with either categorical or numerical attributes and often rely on fuzzy dependency measures. There exists little research on parallel/distributed attribute reduction for large-scale hybrid decision systems. The challenge of handling high-dimensional data in hybrid decision systems necessitates efficient distributed computing techniques to ensure scalability and performance. MapReduce, a widely used framework for distributed processing, provides an organized approach to handling large-scale data. Despite its potential, there is a noticeable lack of attribute reduction techniques that leverage MapReduce’s capabilities with a fuzzy discernibility matrix, which can significantly improve the efficiency of processing high-dimensional hybrid datasets. This paper introduces a vertically partitioned fuzzy discernibility matrix within the MapReduce computation model to address the high dimensionality of hybrid datasets. The proposed MapReduce strategy for attribute reduction minimizes data movement during the shuffle and sort phase, overcoming limitations present in existing approaches. Furthermore, the method’s efficiency is enhanced by integrating a feature known as SAT-region removal, which removes matrix entries that satisfy the maximum satisfiability conditions during the attribute reduction process. Extensive experimental analysis validates the proposed method, demonstrating its superior performance compared to recent parallel/distributed methods in attribute reduction.},
  archive      = {J_ASOC},
  author       = {Pandu Sowkuntla and P.S.V.S. Sai Prasad},
  doi          = {10.1016/j.asoc.2025.112870},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112870},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parallel attribute reduction in high-dimensional data: An efficient MapReduce strategy with fuzzy discernibility matrix},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Space-depth mutual compensation for fine-grained fabric defect detection model. <em>ASOC</em>, <em>172</em>, 112869. (<a href='https://doi.org/10.1016/j.asoc.2025.112869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, using the deep learning approach in the textile industry for defect detection has emerged as a prominent research. However, detecting fabric defects remains challenging due to the small size and small number of fabric defect features. Traditional down-sampling operations that result in loss of feature information, interpolation up-sampling operations that add a lot of background redundant information, and interference with fabric images from external sources such as lighting or electromagnetic devices are significant barriers to achieving accurate defect detection using existing methods. In this work, we introduced a lightweight fabric defect detection method with enhanced resistance to interference. Firstly, we use YOLOv7-tiny as the basic model and integrate the Spatial Pyramid Dilated Convolution (SPD) and Efficient Channel Attention (ECA) modules to enhance the original MP-1 and Effective Long-Range Aggregation Network (ELAN) modules to retain fine-grained information, solve the problem of down-sampled feature loss and improve feature importance allocation. Secondly, a distinctive up-sampling Module (DTS) was proposed to replace the traditional interpolation up-sampling. The module expands the feature map size without adding extraneous information, thus ensuring more efficient integration of features of different sizes. Finally, a novel noise filtering technique called the Color Space Iterative (CSI) method was proposed to filter noise interference quickly and conveniently. Experiments on the open-source DAGM and TILDA defect datasets, as well as supplementary tests on CIFAR10 datasets for the CSI method, have yielded promising results. With a mere 3.4M parameters, the proposed lightweight model underscores the method’s superiority over the baseline in balancing model parameters, detection speed, and accuracy.},
  archive      = {J_ASOC},
  author       = {Kailong Zhou and Jianhui Jia and Weitao Wu and Miao Qian and Zhong Xiang},
  doi          = {10.1016/j.asoc.2025.112869},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112869},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Space-depth mutual compensation for fine-grained fabric defect detection model},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain adaptation via feature disentanglement for cross-domain image classification. <em>ASOC</em>, <em>172</em>, 112868. (<a href='https://doi.org/10.1016/j.asoc.2025.112868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification is an important application area of soft computing. In many real-world application scenarios, image classifiers are applied to domains that differ from the original training data. This so-called domain shift significantly reduces classification accuracy. To tackle this issue, unsupervised domain adaptation (UDA) techniques have been developed to bridge the gap between source and target domains. These techniques achieve this by transferring knowledge from a labeled source domain to an unlabeled target domain. We develop a novel and effective coarse-to-fine domain adaptation method called Domain Adaptation via Feature Disentanglement (DAFD), which has two new key components: First, our Class-Relevant Feature Selection (CRFS) module disentangles class-relevant features from class-irrelevant ones. This prevents the network from overfitting to irrelevant data and enhances its focus on crucial information for accurate classification. This reduces the complexity of domain alignment, which improves the classification accuracy on the target domain. Second, our Dynamic Local Maximum Mean Discrepancy module DLMMD achieves a fine-grained feature alignment by minimizing the discrepancy among class-relevant features from different domains. The alignment process now becomes more adaptive and contextually sensitive, enhancing the ability of the model to recognize domain-specific patterns and characteristics. The combination of the CRFS and DLMMD modules results in an effective alignment of class-relevant features. Domain knowledge is successfully transferred from the source to the target domain. Our comprehensive experiments on four standard datasets demonstrate that DAFD is robust and highly effective in cross-domain image classification tasks.},
  archive      = {J_ASOC},
  author       = {Zhi-Ze Wu and Chang-Jiang Du and Xin-Qi Wang and Le Zou and Fan Cheng and Teng Li and Fu-Dong Nian and Thomas Weise and Xiao-Feng Wang},
  doi          = {10.1016/j.asoc.2025.112868},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112868},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Domain adaptation via feature disentanglement for cross-domain image classification},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Basic emotion detection accuracy using artificial intelligence approaches in facial emotions recognition system: A systematic review. <em>ASOC</em>, <em>172</em>, 112867. (<a href='https://doi.org/10.1016/j.asoc.2025.112867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial emotion recognition (FER) systems are pivotal in advancing human communication by interpreting emotions such as happiness, sadness, anger, fear, surprise, and disgust through artificial intelligence (AI). This systematic review examines the accuracy of detecting basic emotions, evaluates the features, algorithms, and datasets used in FER systems, and proposes a taxonomy for their integration into healthcare. A comprehensive search of six databases, covering publications from January 1990 to March 2023, identified 4073 articles, with 35 studies meeting inclusion criteria. The review revealed that happiness and surprise achieved the highest mean detection accuracies (96.42 % and 96.32 %, respectively), whereas anger and disgust exhibited lower accuracies (91.68 % and 93.71 %, respectively). Fear and sadness had a mean accuracy of 93.87 %. Among AI algorithms, GFFNN demonstrated the highest accuracy (100 %), followed by KNN (97.99 %) and DDBNN (97.77 %). CNN and SVM were the most commonly used algorithms, showing competitive accuracies. The CK+ dataset, while extensively employed, demonstrated a mean accuracy of 96.08 %, lower than RAVDESS, Oulu-CASIA, and other databases. This taxonomy provides insights into FER systems' capabilities to enhance patient care by identifying emotional states, pain levels, and overall well-being. Future research should adopt diverse datasets and advanced algorithms to improve FER accuracy, enabling robust integration of these systems into healthcare practices.},
  archive      = {J_ASOC},
  author       = {Chia-Feng Hsu and Sriyani Padmalatha Konara Mudiyanselage and Rismia Agustina and Mei-Feng Lin},
  doi          = {10.1016/j.asoc.2025.112867},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112867},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Basic emotion detection accuracy using artificial intelligence approaches in facial emotions recognition system: A systematic review},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving an imperfect EPQ model with safety stock for type-I and type-II screening error under constrained fuzzy newton interpolation approach. <em>ASOC</em>, <em>172</em>, 112866. (<a href='https://doi.org/10.1016/j.asoc.2025.112866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with an industrial production process of a single item with safety stock and deterioration over time. First of all, we have considered an economic production quantity (EPQ) inventory model where the items are screened multiple times and the screening process itself has Type-I and Type-II errors. Some parts of the imperfect items are reworkable (serviceable) and the unusable items are discarded from the inventory instantly. Incorporating rework cost, disposal cost and screening cost in the inventory process, a total average system cost function has been studied and it has been optimized analytically. But to capture the flexibilities of the demand rate and all unit cost components (for comparative analysis) a fuzzy model has been developed. Indeed, we know that the defuzzification is a crucial step in any fuzzy inferential system, aimed at converting fuzzy outputs into equivalent crisp values for final decision-making. To get the model optimum we optimize the fuzzy membership function developed with the help of Newton’s general interpolation formula for the proposed constrained non-linear optimization problem. The major novelties of this work include the construction of a new fuzzy membership function and techniques of decision making by means of a solution algorithm. For model validation, a numerical example has been analyzed on the basis of a case study and it has been compared with some of the existing methods. Findings reveal that the proposed method dominates others and up to 39.36% cost reduction is possible as a whole. Finally, sensitivity analysis and graphical illustrations have been carried out, followed by scope of future work.},
  archive      = {J_ASOC},
  author       = {Mou Jana and Sujit Kumar De and Adrijit Goswami},
  doi          = {10.1016/j.asoc.2025.112866},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112866},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving an imperfect EPQ model with safety stock for type-I and type-II screening error under constrained fuzzy newton interpolation approach},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On generalized sugeno’s class generator and parametrized intuitionistic fuzzy approach for enhancing low-light images. <em>ASOC</em>, <em>172</em>, 112865. (<a href='https://doi.org/10.1016/j.asoc.2025.112865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing low-light images poses a significant challenge in terms of pixel distortion, color degradation, detail loss, over enhancement and noise amplification, particularly in images that have both low light and normal light region. In recent years, researchers have increasingly turned their attention to intuitionistic fuzzy set based approaches for low light image enhancement due to their flexibility in the representation of a pixel. In this work, the generalized Sugeno’s class of generating function is proposed. Since the parameter value in the existing generating functions lies in an unbounded interval, it is difficult to find the best parameter value. By using the proposed generalized version, a few intuitionistic generating functions are analyzed where the parameter value lies in a bounded interval. A searching algorithm is also proposed to find the parameter value that maximizes the entropy of an image for any membership and generating function. Regardless of the number of decimals, the proposed approach finds the best parameter value iteratively. Then, in HSI color space, an enhancement model is designed utilizing the intuitionistic fuzzy image achieved using best parameter value and contrast-limited adaptive histogram equalization. The proposed method performs better compared to the state-of-the-art models. Also, seven image quality mathematical metrics — entropy, SSIM, correlation coefficient ( r ) , PSNR, AMBE, number of edge pixels ( N g ) and the fitness function are implemented to compare the proposed and state-of-the-art models.},
  archive      = {J_ASOC},
  author       = {Maheshkumar C.V. and David Raj M. and Saraswathi D.},
  doi          = {10.1016/j.asoc.2025.112865},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112865},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On generalized sugeno’s class generator and parametrized intuitionistic fuzzy approach for enhancing low-light images},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning efficient branch-and-bound for solving mixed integer linear programs. <em>ASOC</em>, <em>172</em>, 112863. (<a href='https://doi.org/10.1016/j.asoc.2025.112863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed Integer Linear Programs (MILPs) are widely used to model various real-world optimization problems, traditionally solved using the branch-and-bound (B&B) algorithm framework. Recent advances in Machine Learning (ML) have inspired enhancements in B&B by enabling data-driven decision-making. Two critical decisions in B&B are node selection and variable selection, which directly influence computational efficiency. While prior studies have applied ML to enhance these decisions, they have predominantly focused on either node selection or variable selection, addressing the decision individually and overlooking the significant interdependence between the two. This paper introduces a novel ML-based approach that integrates both decisions within the B&B framework using a unified neural network architecture. By leveraging a bipartite graph representation of MILPs and employing Graph Neural Networks, the model learns adaptive strategies tailored to different problem types through imitation of expert-designed policies. Experiments on various benchmarks show that the integrated policy adapts better to different problem classes than models targeting individual decisions, delivering strong performance in solving time, search tree size, and optimization dynamics across various configurations. It also surpasses competitive baselines, including the state-of-the-art open-source solver SCIP and a recent reinforcement learning-based approach, demonstrating its potential for broader application in MILP solving.},
  archive      = {J_ASOC},
  author       = {Shuhan Du and Junbo Tong and Wenhui Fan},
  doi          = {10.1016/j.asoc.2025.112863},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112863},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning efficient branch-and-bound for solving mixed integer linear programs},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A directed batch growing self-organizing map based niching differential evolution for multimodal optimization problems. <em>ASOC</em>, <em>172</em>, 112862. (<a href='https://doi.org/10.1016/j.asoc.2025.112862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world optimization problems naturally result in multiple optimal solutions, thereby falling in the class of multimodal optimization problems (MMOPs). A task of finding a plurality of optimal solutions for MMOPs comes under the scope of multimodal optimization algorithms (MMOAs). To solve MMOPs, niching techniques are usually employed by proactively modifying standard evolutionary algorithms (EAs) to form stable subpopulations around multiple niches within their evolving populations. This way, each optimum can germinate and eventually help form a cloud of solutions around each optimum parallely, thereby finding multiple (but a finite number of) optima simultaneously. However, several existing niching techniques suffer from common drawbacks, such as sensitivity with niching parameters or poor performance on high-dimensional problems. An efficient niching technique needs an effective population partitioning method around distinct leading solutions representing each optimum. In this paper, we propose a directed batch growing self-organizing map based niching differential evolution (DBGSOM-NDE). For this purpose, a standard differential evolution (DE) method is divided into two overlapping phases: (i) population-wide search (PS) and (ii) niche-wide search (NS). PS executes neighborhood search around each individual, promoting exploration, while NS explores only the leaders, thus reducing the effect of exploration for a better search intensification around the leaders using a Cauchy-distribution based local search to improve them. We evaluate the role of each operator of the proposed approach DBGSOM-NDE and compare its performance with a number of state-of-the-art niching techniques demonstrating its competitiveness and superiority, especially on high-dimensional and nonlinear problems taken from the existing literature. Finally, a hyper-parametric study is provided demonstrating weak dependence of them to the algorithm’s performance.},
  archive      = {J_ASOC},
  author       = {Mahesh Shankar and Palaniappan Ramu and Kalyanmoy Deb},
  doi          = {10.1016/j.asoc.2025.112862},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112862},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A directed batch growing self-organizing map based niching differential evolution for multimodal optimization problems},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep uncertainty quantification algorithms for confidence-aware hope classification of breast cancer patients based on their cognitive features. <em>ASOC</em>, <em>172</em>, 112860. (<a href='https://doi.org/10.1016/j.asoc.2025.112860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The profound effect of hope on the well-being of cancer patients is undeniable. Hope has been acknowledged as an effective coping mechanism for dealing with the diagnosis and mitigating emotional distress. Breast cancer (BC) patients, in particular, undergo significant physical and psychological changes that can affect their hope. Models can be developed to assess patients’ hope, which is crucial for enhancing healthcare delivery efficiency. In this study, cognitive data of 1990 patients diagnosed with BC is used to develop three uncertainty quantification (UQ) models, including Monte Carlo dropout (MCD), Bayesian ensemble, and ensemble Monte Carlo dropout (EMCD), to determine the hope of BC patients based on their cognitive features. The proposed models provide confidence in classifications, reducing risks and enhancing decision-making. Various qualitative and quantitative performance assessment metrics are utilized to evaluate the proposed models. Experimental results demonstrate that the proposed models accurately classify hope in BC patients. Moreover, the MCD and EMCD show high levels of uncertainty sensitivity, which indicates they assign high uncertainty to their incorrect classifications, leading to more reliable results. The proposed framework offers valuable insights to clinicians to provide psychological support services to improve hope and ultimately increase satisfaction with healthcare quality.},
  archive      = {J_ASOC},
  author       = {AmirReza Tajally and Javad Zarean and Ali Bozorgi-Amiri and Reza Tavakkoli-Moghaddam},
  doi          = {10.1016/j.asoc.2025.112860},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112860},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep uncertainty quantification algorithms for confidence-aware hope classification of breast cancer patients based on their cognitive features},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical adaptive multi-scale hypergraph attention convolution network for skeleton-based action recognition. <em>ASOC</em>, <em>172</em>, 112855. (<a href='https://doi.org/10.1016/j.asoc.2025.112855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Graph Convolutional Networks (GCN) have been widely used for skeleton-based human action recognition (HAR). However, the inherent graph topology design strategy and equally treated the spatial and temporal information in feature aggregation are two main problems existed in GCN-based skeleton HAR. To effectively address those issues, a hierarchical adaptive multi-scale hypergraph attention convolution network (HAM-HGNet) is proposed in our work. Firstly, a hierarchical adaptive clustering partition module is formulated to construct dynamic graph topology for inferring the joint- to parts -to group interaction according their movements in different actions. Then, a multi-scale hypergraph attention convolution module is designed to relax the restriction of the fixed topology, maintaining the inherent spatial characteristic of the skeleton joints. Finally, a temporal segmentation attention constrained encoding module is constructed to model the relationship between different joints in several consecutive frames. Experimental results on two benchmark datasets, i.e. NTU-RGB+D 60 (93.1 % on X-View and 96.7 % on X-Sub) and NTU-RGB+D 120 (90.1 % on X-Sub and 90.8 % on X-Set), validate the proposed model can achieve the state-of-the-art performance for skeleton-based action recognition.},
  archive      = {J_ASOC},
  author       = {Honghong Yang and Sai Wang and Lu Jiang and Yuping Su and Yumei Zhang},
  doi          = {10.1016/j.asoc.2025.112855},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112855},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical adaptive multi-scale hypergraph attention convolution network for skeleton-based action recognition},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Portfolio management using online reinforcement learning with adaptive exploration and multi-task self-supervised representation. <em>ASOC</em>, <em>172</em>, 112846. (<a href='https://doi.org/10.1016/j.asoc.2025.112846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) has been widely used to make continuous trading decisions in portfolio management. However, traditional quantitative trading methods often generalize poorly under certain market conditions, whereas the output of prediction-based approaches cannot be easily translated into actionable insights for trading. Market volatility, noisy signals, and unrealistic simulation environments also exacerbate these challenges. To address the aforementioned limitations, we developed a novel framework that combines Multi-task self-supervised learning (MTSSL) and adaptive exploration (AdapExp) modules. The MTSSL module leverages auxiliary tasks to learn meaningful financial market representations from alternative data, whereas the AdapExp module enhances RL training efficiency by improving the fidelity of the simulation environment. Experimental results obtained in backtesting conducted in real financial markets indicate that the proposed framework achieved approximately 13% higher returns relative to state-of-the-art models. Furthermore, this framework can be used with various RL methods to considerably improve their performance.},
  archive      = {J_ASOC},
  author       = {Chuan-Yun Sang and Szu-Hao Huang and Chiao-Ting Chen and Heng-Ta Chang},
  doi          = {10.1016/j.asoc.2025.112846},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112846},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Portfolio management using online reinforcement learning with adaptive exploration and multi-task self-supervised representation},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A meta-learning based neural network and LSTM for univariate time series missing data imputation. <em>ASOC</em>, <em>172</em>, 112845. (<a href='https://doi.org/10.1016/j.asoc.2025.112845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series are regularly collected data that describe the average evolution of an event over time, making them increasingly relevant in areas such as business, natural sciences and medicine. A major challenge related to time series is data loss, and several approaches have been developed to recover missing values in univariate time series (UTS). This work aims to improve the imputation of missing data in univariate and heterogeneous time series. Thus, we built a diverse database covering different time series domains and selected a set of data imputation techniques. The results show that imputation in time series is challenging, especially due to the variability of the series, the position of missing data and the number of samples passed to each technique. The HybridLSTM network, developed in this study, proved effective in recommending the most suitable imputation techniques for each series, resulting in a lower average error than using a single technique or recent techniques such as Pix2Pix and Moment. In addition, adopting a hybrid loss function, which considers multi-class and multi-label tasks, contributed to optimal or near-optimal performance, even in cases where the ideal was not achieved. These advances were possible thanks to the efficient but simple construction of metadata and the innovative approach of locally combining several imputation techniques within the same series. We observed that meta-learning has great potential to be applied in real contexts where the ideal technique is not previously known and the data has not been pre-treated in terms of data values. Moreover, as our experiments were very close to this context, it became useful, as the model performed very close to the ideal, validating the applicability of the adaptive meta-learning approach to optimize the imputation of missing data in real contexts.},
  archive      = {J_ASOC},
  author       = {Mauricio Morais Almeida and João Dallyson Sousa Almeida and Darlan Bruno Pontes Quintanilha and Geraldo Braz Júnior and Aristófanes Correa Silva},
  doi          = {10.1016/j.asoc.2025.112845},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112845},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A meta-learning based neural network and LSTM for univariate time series missing data imputation},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rule generation in rough set non-deterministic information analysis (RNIA) and some applications of the obtained rules. <em>ASOC</em>, <em>172</em>, 112842. (<a href='https://doi.org/10.1016/j.asoc.2025.112842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We submitted this paper to the special issue, “Four Decades of Rough Set Theory: Achievements and Future.” We have researched a theory and an execution tool for rule generation from a Deterministic Information System (DIS) and a Non-deterministic Information System (NIS). We developed the NIS-Apriori algorithm, which combines the rough sets-based concept and the Apriori algorithm, for rule generation from NIS. We term this research series as a Rough set Non-deterministic Information Analysis (RNIA). In the first half of this paper, we describe the framework of RNIA and its execution environment, as well as the results we achieved. Then, later in this paper, we enumerate various applications of RNIA, such as detection of data dependencies, decision support, estimation and completion of missing values, the problem of learning DIS from NIS, and generation of rules from non-tabular and multiple heterogeneous data sets. They are our current and prospective subjects. RNIA’s capabilities can lead to several developments.},
  archive      = {J_ASOC},
  author       = {Hiroshi Sakai and Michinori Nakata and Dominik Ślęzak and Junzo Watada},
  doi          = {10.1016/j.asoc.2025.112842},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112842},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Rule generation in rough set non-deterministic information analysis (RNIA) and some applications of the obtained rules},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting cell–cell communication by combining heterogeneous ensemble deep learning and weighted geometric mean. <em>ASOC</em>, <em>172</em>, 112839. (<a href='https://doi.org/10.1016/j.asoc.2025.112839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell–cell communication (CCC) is essential to tumor growth, metastasis, and resistance to therapy. The rapid development of single-cell RNA sequencing (scRNA-seq) technologies facilitates us to more accurately decipher cellular signal transduction. CCC is usually mediated by interacting ligand–receptor pairs. Thus, it is crucial to construct a comprehensive ligand–receptor interaction (LRI) database for decoding CCC. Moreover, ordinary ligand–receptor scoring metrics-based CCC inference methods need to be further explored. To solve the above two problems, in this study, we propose a novel computational framework called CellGDnG to analyze CCC within the tumor microenvironment. CellGDnG first utilizes a heterogeneous ensemble deep learning model to identify potential LRIs. Next, it adopts a weighted geometric mean-based strategy to infer CCC. A series of in-depth comparative experiments of CellGDnG and other popular tools demonstrated its ability to decode CCC precisely and effectively. Furthermore, CellGDnG offered heatmap view, circle plot, and sankey diagram to visualize CCC. Notably, the predicted top 3 LRIs mediating breast cancer CCC could be its potential therapeutically tractable drug targets. Available as an open-source tool, CellGDnG provides valuable clues to unveil cell–cell signal transduction and develop new targeted drugs.},
  archive      = {J_ASOC},
  author       = {Lihong Peng and Longlong Liu and Liangliang Huang and Zongzheng Bai and Min Chen and Xing Chen},
  doi          = {10.1016/j.asoc.2025.112839},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112839},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting cell–cell communication by combining heterogeneous ensemble deep learning and weighted geometric mean},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge assisted differential evolution extreme gradient boost algorithm for estimating mangrove aboveground biomass. <em>ASOC</em>, <em>172</em>, 112838. (<a href='https://doi.org/10.1016/j.asoc.2025.112838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel Knowledge Assisted Differential Evolution Extreme Gradient Boost algorithm (KADE-XGBoost) for estimating mangrove aboveground biomass in the Maowei Sea, Beibu Gulf of China. The proposed algorithm combines differential evolution and extreme gradient boosting to address hyperparameter optimization and feature selection simultaneously. Additionally, a two-stage knowledge assisted strategy is proposed to retain key features for evolution and prevent the algorithm from converging to local optima. Experimental results using a dataset of 227 quadrat data from field surveys demonstrate that KADE-XGBoost outperforms other machine learning and heuristic-based models, achieving the best results with an R 2 value of 0.8413 and an R M S E of 216.2867. The KADE-XGBoost’s prediction range for mangrove aboveground biomass is 4.4218-218.2612 Mg/ha, showcasing its potential as a reliable algorithm for estimating large-scale mangrove aboveground biomass.},
  archive      = {J_ASOC},
  author       = {Yangdi Shen and Zuowen Liao and Yichao Tian and Jin Tao and JinXuan Luo and Jiale Wang and Qiang Zhang},
  doi          = {10.1016/j.asoc.2025.112838},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112838},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge assisted differential evolution extreme gradient boost algorithm for estimating mangrove aboveground biomass},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vanishing mask refinement in semi-supervised video object segmentation. <em>ASOC</em>, <em>172</em>, 112837. (<a href='https://doi.org/10.1016/j.asoc.2025.112837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Video Object Segmentation Enhanced with Segment Anything Model (VOS-E-SAM), a multi-stage architecture for Semi-supervised Video Object Segmentation (SVOS) using the foundational Segment Anything Model (SAM) architecture, aimed at addressing the challenges of mask degradation over time in long video sequences. Our architectural approach enhances the object masks produced by the XMem model by incorporating SAM. This integration uses various input combinations and low-level computer vision techniques to generate point prompts, in order to improve mask continuity and accuracy throughout the entire video cycle. The main challenge addressed is the fading or vanishing of object masks in long video sequences due to problems such as changes in object appearance, occlusions, camera movements, and approach changes. Both the baseline architecture and the newer high-quality version are tested, addressing the primary challenge of object mask fading or vanishing in long video sequences due to changes in object appearance, occlusions, camera movements, and variations in approach. Through rigorous experimentation with different prompt configurations, we identified an outstanding configuration of SAM inputs to improve mask refinement. Evaluations on benchmark long video datasets, such as LongDataset and LVOS, show that our approach significantly improves mask quality in single-object extended video sequences proven by percentage increments on jaccard index ( J ) and contour accuracy ( F ) based metrics (mean, recall and decay). Our results show remarkable improvements in mask persistence and accuracy, which sets a new standard for the integration of foundational models in video segmentation and lays the foundation for future research in this field. Github. VOS-E-SAM},
  archive      = {J_ASOC},
  author       = {Javier Pita and Juan P. Llerena and Miguel A. Patricio and Antonio Berlanga and Luis Usero},
  doi          = {10.1016/j.asoc.2025.112837},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112837},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Vanishing mask refinement in semi-supervised video object segmentation},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A customer behavior-driven clustering method in the planogram design domain. <em>ASOC</em>, <em>172</em>, 112836. (<a href='https://doi.org/10.1016/j.asoc.2025.112836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The arrangement of products on supermarkets’ shelves (i.e., the planogram) has a strong impact on customer behavior, affecting their likelihood of choosing a product over another. Therefore, it is of fundamental importance, for retailers, to be able to quantitatively measure the differences between planograms. Most of the existing works in the planogram design domain are focused on compliance tasks, i.e., checking that a real-world shelf matches a given planogram. On the other hand, to our best knowledge, no work in the literature focuses on measuring the similarity between planograms. In this work, we leverage customer behavioral principles from the planogram design domain to build a data-driven clustering method, centered on customer behavior. More specifically, we achieve this by using a two-level clustering approach that, on the first level, discriminates between different shelf shapes, while on the second level, it discriminates the arrangement of products on the shelves. The proposed method allows planogram designers to efficiently modify and improve existing planograms, based on the desired customer behavior. Finally, our approach ensures domain coherence, avoiding bias propagation during the design of new planograms. The experimental results confirm that our quantitative approach correctly mirrors customer behavioral principles in the planogram design domain.},
  archive      = {J_ASOC},
  author       = {Francesco Silverio and Mario Cantalupo and Leonardo Lucio Custode and Giovanni Iacca},
  doi          = {10.1016/j.asoc.2025.112836},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112836},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A customer behavior-driven clustering method in the planogram design domain},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A holistic approach for resource-constrained neural network architecture search. <em>ASOC</em>, <em>172</em>, 112832. (<a href='https://doi.org/10.1016/j.asoc.2025.112832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of Artificial Neural Networks (ANN) is critical for their performance. The research field called Neural Network Search (NAS) investigates automated design strategies. This work proposes a novel NAS stack that stands out in three facets. First, the representation scheme encodes problem-specific ANN as plain vectors of numbers without needing auxiliary conversion models. Second, it is a pioneer in relying on the TLBO meta-heuristic. This optimizer supports large-scale problems and only expects two parameters, contrasting with other meta-heuristics used for NAS. Third, the stack includes a new evaluation predictor that avoids evaluating non-promising architectures. It combines several machine learning methods that train as the optimizer evaluates solutions, which avoids preliminary preparing this component and makes it self-adaptive. The proposal has been tested by using it to build a CIFAR-10 classifier while forcing the architecture to have fewer than 150,000 parameters, assuming that the resulting network must be deployed in a resource-constrained IoT device. The designs found with and without the predictor achieve validation accuracies of 78.68% and 80.65%, respectively. Both outperform a larger model from the recent literature. The predictor slightly constraints the evolution of solutions, but it approximately halves the computational effort. After extending the test to the CIFAR-100 dataset, the proposal achieves a validation accuracy of 65.43% with 478,006 parameters in its fastest configuration, competing with current results in the literature.},
  archive      = {J_ASOC},
  author       = {M. Lupión and N.C. Cruz and E.M. Ortigosa and P.M. Ortigosa},
  doi          = {10.1016/j.asoc.2025.112832},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112832},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A holistic approach for resource-constrained neural network architecture search},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A gradient-supported analysis of pareto front in multi-objective extremal optimization-based processor load balancing. <em>ASOC</em>, <em>172</em>, 112830. (<a href='https://doi.org/10.1016/j.asoc.2025.112830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns methodology for exploiting the multi-objective Extremal Optimization for load-balancing algorithms in high-performance distributed systems. In clusters and data centers, there has always been a trade-off between contradictory goals such as obtaining high performance, reducing inter-node communication, task or virtual machine migration, and energy savings. Thus, a multi-objective optimization strategy should be provided based on task migration to achieve an efficient processor load balance in the executive distributed environment, which is an NP-hard computational problem. The paper proposes a new selection scheme for the final load-balanced solution in the Pareto front. In this gradient-supported scheme, we examine lexicographic solutions relaxed by a margin of allowable loss, provided that the remaining optimization criteria are improved. This has been achieved by calculating the gradients of the tangent lines connecting the analyzed lexicographic solutions and the subsequent Pareto front points. The algorithm has been evaluated by comparative simulation experiments with application program graphs run in distributed systems. The evaluation, which included a comparison with a genetic algorithm, confirmed the very good performance of the proposed gradient-based Pareto front selection method.},
  archive      = {J_ASOC},
  author       = {Ivanoe De Falco and Eryk Laskowski and Richard Olejnik and Umberto Scafuri and Ernesto Tarantino and Marek Tudruj},
  doi          = {10.1016/j.asoc.2025.112830},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112830},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A gradient-supported analysis of pareto front in multi-objective extremal optimization-based processor load balancing},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GRU-ARX model-based adaptive error compensation predictive control strategy with application to quadrotor. <em>ASOC</em>, <em>172</em>, 112829. (<a href='https://doi.org/10.1016/j.asoc.2025.112829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a class of nonlinear dynamic systems, accurately characterizing the dynamic characteristics by building their physical models is still challenging. To deal with this issue, a novel deep learning network architecture, gated recurrent unit (GRU) neural network-based ARX model (GRU-ARX model), is developed in this study. In this model, the GRU network is executed to capture potential nonlinear mapping features of the system. And the pseudo linear ARX structure is adopted for making controller design easier, with the state-dependent parameters updated at each execution point. In view of this model, the model predictive control (MPC) algorithms for controlling the real nonlinear plant can be availably designed. However, faced with the appearance of sensibility with respect to internal or/and external factors in practical applications, the time-varying model may not perform well in control accuracy and robustness specification. Consequently, the operation of selecting the correction coefficients adaptively is combined with the MPC strategy to establish the adaptive MPC protocol focused on the error compensation, allowing for achieving the improved control accuracy and performance. Especially, the designed GRU-ARX model-based control algorithms, without and with the adaptive error compensation law are successfully applied to a practical quadrotor system, and the effectiveness of the accessed algorithms can be demonstrated by comparative results of real-time control experiments. These outcomes showcase that the proposed adaptive error compensation MPC algorithm exhibits superior control performance compared to other model-based controllers in trajectory tracking and anti-interference experiments, revealing its advantages over the traditional MPC algorithm.},
  archive      = {J_ASOC},
  author       = {Binbin Tian and Hui Peng and Zaihua Zhou},
  doi          = {10.1016/j.asoc.2025.112829},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112829},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GRU-ARX model-based adaptive error compensation predictive control strategy with application to quadrotor},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy kano model proposal for sustainable product design: Mobile application feature analysis. <em>ASOC</em>, <em>172</em>, 112824. (<a href='https://doi.org/10.1016/j.asoc.2025.112824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Companies aim to maximize profits by effectively designing mobile applications to promote their services in a competitive market. However, identifying the design features that significantly impact mobile applications is challenging due to their subjective nature. Traditional Kano approaches face limitations, such as information loss caused by considering only the most frequent values. To address these limitations, this study proposes a novel fuzzy Kano approach to better manage the subjectivity in human judgments and the uncertainty in user preferences. This approach uncovers hidden preference levels, accounts for uncertainties, resolves dual classification issues, compares membership degrees, and emphasizes subtle details that may otherwise be overlooked. The fuzzy Kano approach was applied to survey data from 100 participants, covering 33 mobile application features. By classifying these features, the fuzzy Kano model examined their influence on user satisfaction and quality perception. The results demonstrated the feasibility and effectiveness of the proposed method, identifying key features—such as Product Details, Order Management and Returns, and Product Opinions and Reviews—that, if absent, could lead to customer dissatisfaction. Additionally, the findings revealed significant differences between the fuzzy and traditional Kano models and highlighted variations in mobile application characteristics across different demographic groups, providing valuable insights for mobile application design.},
  archive      = {J_ASOC},
  author       = {Necip Fazıl Karakurt and Selcuk Cebi},
  doi          = {10.1016/j.asoc.2025.112824},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112824},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy kano model proposal for sustainable product design: Mobile application feature analysis},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical feature transformation attack: Generate transferable adversarial examples for face recognition. <em>ASOC</em>, <em>172</em>, 112823. (<a href='https://doi.org/10.1016/j.asoc.2025.112823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has achieved remarkable success due to the development of deep convolutional neural networks (DCNNs). However, DCNNs are vulnerable to adversarial examples, which poses potential security risks to face recognition systems. Transferable adversarial examples, which can be generated by substitute models and transferred to target models, significantly compromise the robustness of DCNNs. The prevailing transfer-based adversarial attacks commonly rely on static substitute models to produce transferable adversarial examples, rendering them susceptible to overfitting to substitute model, thereby diminishing their transferability. To this end, we propose a novel transfer-based black-box method named Hierarchical Feature Transformation Attack (HFTA) to generate transferable adversarial examples. HFTA increases the diversity of substitute models to form dynamic substitute models by randomly corrupting units from substitute networks, effectively mitigating the risk of adversarial examples overfitting to substitute models. Simultaneously, hierarchical feature transformations are implicitly integrated to enhance the transferability of adversarial examples. The proposed HFTA can be easily combined with any gradient-based transferable attacks. The experiments on the diverse-faces LFW dataset and the facial attribute-rich CelebA dataset demonstrate that HFTA significantly enhances the transferability of adversarial examples, with an average increase in attack success rate of over 10%. Moreover, the amplification of HFTA on the success rate of attacks against other methods can reach up to 62.8%, underscoring the effectiveness of HFTA in enhancing the dynamism and implicit integration of surrogate models. Besides, the adversarial faces generated by HFTA achieve lower feature similarity and higher image quality than those generated by other attack methods.},
  archive      = {J_ASOC},
  author       = {Yuanbo Li and Cong Hu and Rui Wang and Xiaojun Wu},
  doi          = {10.1016/j.asoc.2025.112823},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112823},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical feature transformation attack: Generate transferable adversarial examples for face recognition},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concurrent attentional reconstruction network for 3D point cloud reconstruction from single image. <em>ASOC</em>, <em>172</em>, 112821. (<a href='https://doi.org/10.1016/j.asoc.2025.112821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconstruction of point clouds from a 2D single image is challenging due to the complex nature of images that cannot be projected easily. Also, the images include multiple objects that require huge concentration while transforming them into a point cloud representation. Deep learning-based methods are recently gaining attention due to the high-performance outcomes enlisted in the image processing domain. This paper introduces a two-tiered deep learning-based reconstruction model known as concurrent attentional reconstruction network (CARN) to better reconstruct a 3D point cloud from a 2D single image. The feature extraction and the point cloud prediction are the two modules employed in the proposed CARN model. Here, concurrent excited DenseNet (An important development in the field of computer vision is the use of dense convolutional neural networks (DenseNets) linked with concurrent squeeze-and-excitation (CSE) modules for feature extraction in 3D point cloud reconstruction from a single image. The attention-dense gated recurrent unit (AD-GRU) is the next module employed for point cloud reconstruction. The proposed CARN model is trained and tested with publicly available ShapeNet datasets. The Python platform is applied for implementation, and various performance metrics such as accuracy, Earth Mover's Distance (EMD), and Chamfer Distance (CD) are analyzed with existing methods. The proposed CARN model acquires an overall accuracy above 99 % and obtains minimum CD and EMD values for the ShapeNet dataset.},
  archive      = {J_ASOC},
  author       = {PremaLatha Velagapalli and Nikhat Parveen},
  doi          = {10.1016/j.asoc.2025.112821},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112821},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Concurrent attentional reconstruction network for 3D point cloud reconstruction from single image},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple unmanned surface vehicles pathfinding in dynamic environment. <em>ASOC</em>, <em>172</em>, 112820. (<a href='https://doi.org/10.1016/j.asoc.2025.112820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the burgeoning field of autonomous maritime operations, efficiently coordinating and navigating multiple unmanned surface vehicles (USVs) in dynamic environments is a significant challenge. This study presents an enhanced Q-learning algorithm designed to improve pathfinding for multiple USVs in such settings. The algorithm innovates on the traditional Q-learning framework by adjusting the learning rate, Epsilon-greedy strategy, and penalty and reward functions, integrating a collision avoidance mechanism specifically tailored for complex maritime navigation. Extensive simulations across six diverse scenarios – ranging from single to multiple USVs operations in both static and dynamic obstacle environments – demonstrate the algorithm’s superior adaptability and efficiency compared to existing methods. Notably, in single USV scenarios, the improved Q-learning algorithm not only plots more direct paths but also reduces computational demands significantly over traditional path planning methods such as the A ∗ and APF algorithms. In multi-USV scenarios, it demonstrates robust performance, reducing calculation times by an average of 55.51% compared to SARSA, 49.14% compared to the original Q-learning, and 45.26% compared to the Speedy Q-learning approach. These advancements underscore the algorithm’s potential to enhance autonomous maritime navigation, laying a strong foundation for future improvements in the safety and efficiency of USV operations.},
  archive      = {J_ASOC},
  author       = {Yuqing Lin and Liang Du and Kum Fai Yuen},
  doi          = {10.1016/j.asoc.2025.112820},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112820},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiple unmanned surface vehicles pathfinding in dynamic environment},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A synergistic oversampling technique with differential evolution and safe level synthetic minority oversampling. <em>ASOC</em>, <em>172</em>, 112819. (<a href='https://doi.org/10.1016/j.asoc.2025.112819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification problems often face challenges when dealing with imbalanced datasets, leading to decreased performance. To address this issue, balancing the dataset becomes imperative for improved classification accuracy. Among various methods proposed in the literature, oversampling techniques are fundamental approaches to mitigating class imbalance. Synthetic Minority Over-sampling Technique (SMOTE) is a foundational technique in this domain. However, a more refined approach, Safe-Level-SMOTE, selectively utilizes crucial minority instances to generate synthetic samples. Another notable method, the Differential Evolution-Based Oversampling Approach for Highly Imbalanced Datasets (DEBOHID), leverages a differential evolution algorithm to handle highly imbalanced datasets effectively. This study presents a novel oversampling method (SL-D) that integrates Safe-Level-SMOTE with DEBOHID. SL-D offers three distinct variants: SL-D-Max, SL-D-Min, and SL-D-Mean, each tailored to specific scenarios. We introduce an adaptive calculation mechanism for the proposed method's crossover rate ( CR ) parameter. Our experimentation utilizes Decision Trees (DT), Support Vector Machines (SVM), and k -nearest neighbor (kNN) classifiers across forty-four highly imbalanced datasets. Results indicate that the SL-D-Max variant outperforms nine state-of-the-art oversampling approaches, as evidenced by superior performance metrics such as G-Mean and Area Under the Curve (AUC). Furthermore, statistical analysis employing the Friedman Test confirms the significant superiority of SL-D-Max. This study underscores the efficacy of the proposed hybrid oversampling technique in addressing imbalanced data classification challenges and highlights its potential for practical applications.},
  archive      = {J_ASOC},
  author       = {Ahmet Cevahir Cinar},
  doi          = {10.1016/j.asoc.2025.112819},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112819},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A synergistic oversampling technique with differential evolution and safe level synthetic minority oversampling},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taxonomy of metrics for effectively estimating quantum software projects: A fuzzy-AHP based analysis. <em>ASOC</em>, <em>172</em>, 112816. (<a href='https://doi.org/10.1016/j.asoc.2025.112816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing represents a revolutionary shift in computing, yet developing quantum software is significantly more complex than traditional software engineering. Existing research provides limited guidance on estimating costs, development efforts, and timelines within this emerging paradigm. This lack of guidance leaves a critical gap for software organizations aiming to manage quantum projects effectively. To address this gap, the proposed study investigates the key metrics influencing estimation in quantum software development. Through a comprehensive literature review, we identified 13 critical metrics categorized into four groups: technical complexity, resource availability, team expertise, and project environment. In the next phase, a survey-based empirical study was conducted to validate the identified metrics and their categories. Additionally, we applied the fuzzy-AHP method to determine the relative significance of each metric. Our results culminate in a prioritized taxonomical framework that provides a structured approach for managing quantum software development estimations. The findings suggest that adopting the proposed framework can significantly enhance overall project management within the quantum software engineering domain.},
  archive      = {J_ASOC},
  author       = {Mohammad Shameem and Mohammad Nadeem and Mahmood Niazi and Sajjad Mahmood and Ankur Kumar},
  doi          = {10.1016/j.asoc.2025.112816},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112816},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Taxonomy of metrics for effectively estimating quantum software projects: A fuzzy-AHP based analysis},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pythagorean cubic fuzzy multiple attributes group decision method for sustainable supply chain management. <em>ASOC</em>, <em>172</em>, 112802. (<a href='https://doi.org/10.1016/j.asoc.2025.112802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Pythagorean cubic fuzzy set (PCFS) is composed of Pythagorean fuzzy values and interval details. Unlike interval Pythagorean fuzzy sets, PCFS contains more data and can be valuable in complex multi-attribute group decision making (MAGDM). However, as a novel fuzzy set, certain essential principles of PCFS, such as the scoring function's implausibility and the absence of operations, require improvement. To address these concerns, we have refined the PCFS scoring function and introduced a new PCFS operation. Additionally, we have developed a PCFS reliability measure to account for uncertain expert opinions and attribute weights in MAGDM. Furthermore, overcoming the challenge of collecting PCFS evaluation data presents an obstacle. In the context of content distribution, the Heronian-mean (HM) operator tackles attribute association. While most existing Pythagorean-cubic fuzzy aggregation operators have an algebraic nature, we leverage the HM operator to establish a variety of Pythagorean cubic fuzzy aggregation operators. These operators showcase properties such as equivalence, monotonicity, boundedness, and commutative invariance. Finally, grounded in the Pythagorean cubic fuzzy HM aggregation operator, we introduce a MAGDM approach for sustainable supply chain management (SSCM). We conduct a practicality and superiority comparison with the existing Pythagorean cubic fuzzy aggregation operator. The primary contribution of this article is to enrich the research on aggregation operators of PCFS and expand their social applications in the realm of SSCM.},
  archive      = {J_ASOC},
  author       = {Fei Wang},
  doi          = {10.1016/j.asoc.2025.112802},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112802},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pythagorean cubic fuzzy multiple attributes group decision method for sustainable supply chain management},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolving temporal convolutional neural networks for modeling turntable servo systems. <em>ASOC</em>, <em>172</em>, 112800. (<a href='https://doi.org/10.1016/j.asoc.2025.112800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Turntable servo systems are important experimental equipment used for semi-physical simulation and testing of aircraft, which have very strict requirements for tracking performance. To achieve high-precision servo performance, some advanced model-based control methods have been used recently, but the control performance is greatly influenced by the accuracy of the model. Therefore, this paper proposes evolving temporal convolutional networks (TCNs) for accurately modeling turntable servo systems. Considering the advantages of TCNs in capturing local dependencies in sequence data and parallel computing, a TCN is adopted to compensate the unmodeled dynamics to significantly improve the accuracy of the existing dynamics model. Then, an evolutionary algorithm (EA) based architecture optimization algorithm for the above TCN model is proposed, and the network performance is continuously improved by simulating the population evolution in nature. Finally, the weight inheritance method is utilized to evaluate the performance of individuals during the evolution process to effectively improve the execution efficiency of the whole algorithm. In addition, the subnets inherited from the supernet meet the real-time requirements of the actual tasks by setting real-time constraints on the supernet. The experimental results demonstrate the superiority of the proposed model against the peer competitors in terms of the prediction performance. In addition, the proposed optimization algorithm also shows higher execution efficiency as verified by ablation experiments.},
  archive      = {J_ASOC},
  author       = {Cheng Xie and Bing Xue and Mengjie Zhang and Songlin Chen and Yang Liu},
  doi          = {10.1016/j.asoc.2025.112800},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112800},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolving temporal convolutional neural networks for modeling turntable servo systems},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A double-layer forecasting model for PV power forecasting based on GRU-informer-SVR and blending ensemble learning framework. <em>ASOC</em>, <em>172</em>, 112768. (<a href='https://doi.org/10.1016/j.asoc.2025.112768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of power market reform, the proportion of new energy sources participating in power market transactions has been increasing. Photovoltaic (PV) power generation is characterized by intermittency and volatility, which brings risks to power system operation. To improve the prediction accuracy of PV power, we propose a double-layer prediction model of GRU–Informer and SVR based on the Blending ensemble learning framework that considers feature screening and weather clustering. First, XGBoost is used to calculate the importance of weather features and filter the features. Second, K-means clustering is used to classify the weather data into three types: sunny, cloudy and mutation. Third, a GRU–Informer and SVR double-layer prediction model that was constructed based on the Blending ensemble model is used. The first-layer base-learner uses the training set to train the GRU with Informer and outputs the first-layer prediction results. The second-layer meta-learner uses the first-layer prediction results to train the SVR and generate the final prediction results. The empirical analysis results show that the proposed model can achieve higher fitting accuracy R 2 of more than 98 % under all three weather conditions and is a promising approach in terms of PV power generation forecasting.},
  archive      = {J_ASOC},
  author       = {Xiaomin Xu and Luoyun Guan and Zhiyi Wang and Runkun Yao and Xiao Guan},
  doi          = {10.1016/j.asoc.2025.112768},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112768},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A double-layer forecasting model for PV power forecasting based on GRU-informer-SVR and blending ensemble learning framework},
  volume       = {172},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of corporate default risk considering ESG performance and unbalanced samples. <em>ASOC</em>, <em>171</em>, 112864. (<a href='https://doi.org/10.1016/j.asoc.2025.112864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper constructs a corporate default risk prediction model taking ESG scores into account in an unbalanced sample state. Four indicators are introduced—the ESG composite score, environmental dimension score, social dimension score, and governance dimension score—to assess an enterprise's capacity for sustainable development as well as its level of greenness and low carbon emissions. These indicators improve and supplement the current corporate default prediction indicator system. The Focal Loss function and the cost-sensitive decision threshold are used to improve the traditional Stacking model at the algorithmic aspect. The CS-FL-Stacking model is then built to address the problem of sample class imbalance. After conducting an empirical analysis with data from 3006 Chinese A-share listed companies during 2021–2023, the following conclusions are drawn: (1) The inclusion of ESG indicators can somewhat enhance the model's prediction ability and reduce the misclassification loss. (2) The CS-FL-Stacking model generally outperforms the benchmark model in terms of accuracy and other indicators. It also considerably improves its capacity to identify minority samples, which can effectively address the problem of unbalanced sample classification. (3) Relevant recommendations are provided for the improvement of the CS-FL-Stacking model and the application of ESG indicators in corporate risk management in light of the analysis just mentioned.},
  archive      = {J_ASOC},
  author       = {Ruyue Chang and Xuejuan Liu and Wanjun Deng},
  doi          = {10.1016/j.asoc.2025.112864},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112864},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction of corporate default risk considering ESG performance and unbalanced samples},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metaheuristic search algorithms in frequency constrained truss problems: Four improved evolutionary algorithms, optimal solutions and stability analysis. <em>ASOC</em>, <em>171</em>, 112854. (<a href='https://doi.org/10.1016/j.asoc.2025.112854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Truss problems with f requency c onstraints (TPFCs) are among the most complex real-world engineering optimization problems in the literature due to the non-linearity of the objective and constraint functions and the geometric structure of the search spaces. These problems have many local solutions due to the irregular geometric structure of the search spaces. Therefore, it is a challenge for meta-heuristic search (MHS) algorithms to converge stably to the global optimum solution for TPFCs. To overcome this challenge, this paper presents four new algorithms with improved performance for the optimization of TPFCs. The methodology of the research and the contributions to the literature are as follows: (i) a TPFC benchmark suite consisting of five different problem types was presented, (ii) for each problem in the benchmark suite, 152 different MHS algorithms were tested and the ones with the best convergence performance were identified, (iii) the update mechanisms of these algorithms that perform competitively on TPFCs were redesigned using the Natural Survivor Method (NSM). Thus, four different MHS algorithms with improved performance were proposed for the optimization of TPFCs, (iv) the optimal solutions for TPFCs were presented, (v) the stability of the proposed algorithms for TPFCs was analyzed and the times and success rates of finding feasible solutions were presented. According to the results of the statistical analysis, the optimal and feasible solutions for the 10/37/52/72/200 bar truss problems were found by the NSM-MadDE, NSM-LSHADE-CnEpSin, NSM-LSHADE-SPACMA and NSM-BO algorithms introduced in this paper.},
  archive      = {J_ASOC},
  author       = {Hasan Tahsin Öztürk and Hamdi Tolga Kahraman},
  doi          = {10.1016/j.asoc.2025.112854},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112854},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristic search algorithms in frequency constrained truss problems: Four improved evolutionary algorithms, optimal solutions and stability analysis},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble multi-label classification using closed frequent labelsets and label taxonomies. <em>ASOC</em>, <em>171</em>, 112853. (<a href='https://doi.org/10.1016/j.asoc.2025.112853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensembles are computational models that combine the strengths of multiple algorithms or models to enhance predictive accuracy, robustness, and generalization across various applications in machine learning and data analysis. They can mitigate the risk of overfitting and improve model stability, reducing the impact of individual algorithmic biases. These are valuable tools for achieving superior performance in complex and dynamic real-world scenarios. Despite constant advances in this research area, recent studies have shown that state-of-the-art ensembles for multi-label classification are still based on classical ensemble methods from 2016. This study proposes three new ensemble algorithms, called the ensemble of flat-to-hierarchical (EF2H) versions, developed using the F2H multi-label classification model. The F2H algorithm transforms the multi-label problem into a hierarchical multi-label problem to generate predictions. Experiments were conducted with 32 multi-label datasets, and the results were compared with those of the state-of-the-art algorithms in this field. The results demonstrate that the EF2H versions are highly competitive algorithms, outperforming the well-known ensemble of classifier chains (ECC) and achieving predictive performance equivalent to that of the random forest of decision trees with binary relevance (RFDTBR) and random forest of predictive clustering trees (RFPCT) algorithms.},
  archive      = {J_ASOC},
  author       = {Mauri Ferrandin and Ricardo Cerri},
  doi          = {10.1016/j.asoc.2025.112853},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112853},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble multi-label classification using closed frequent labelsets and label taxonomies},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neighbor self-embedding graph model for clustering ensemble. <em>ASOC</em>, <em>171</em>, 112844. (<a href='https://doi.org/10.1016/j.asoc.2025.112844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering ensemble is an important method in machine learning and data mining for achieving robust and consistent results by integrating multiple base clustering results. However, existing clustering ensemble methods often overlook self-supervised information in the data, treating data points assigned to the same cluster as equivalent, regardless of their relative distances from the cluster center, which may hinder clustering ensemble methods improvement performance. To address this issue, we propose the Neighbor Self-embedding Graph Model for Clustering Ensemble (NSGMCE), which leverages self-supervised embeddings derived from diverse base clustering methods to extract structural information intrinsic to the data while preserving the characteristics of the base clustering results. Specifically, unlike traditional methods that directly rely on pseudo-labels from base clustering, NSGMCE treats self-supervised embeddings as new feature representations, retaining the advantages of ensemble learning while mitigating the impact of erroneous pseudo-labels. Subsequently, this self-supervised embeddings is used to construct an neighbor self-embedding graph, which is optimized and pruned during the alternating minimization inference process to obtain the final consensus result. The objective function of NSGMCE is formulated as a convex optimization problem, this smooth and continuous objective ensures its convergence and allows for efficient solving to obtain the global optimal solution, which we have theoretically proven. Extensive experiments were conducted comparing NSGMCE with eight state-of-the-art clustering ensemble methods across 12 datasets, demonstrating its superior performance. Specifically, NSGMCE achieved an 11.8% improvement in average accuracy over the second-best method and ranked first in terms of average Purity, Rand Index, and Mirkin metric. Further analysis confirmed that NSGMCE effectively leverages self-supervised embeddings to enhance robustness and stability.},
  archive      = {J_ASOC},
  author       = {Siyang Li and Peng Zhao and Hongjun Wang and Huan Wang and Tianrui Li},
  doi          = {10.1016/j.asoc.2025.112844},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112844},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neighbor self-embedding graph model for clustering ensemble},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-strategy fuzzy controller based on polar coordinates. <em>ASOC</em>, <em>171</em>, 112843. (<a href='https://doi.org/10.1016/j.asoc.2025.112843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a multi-strategy fuzzy controller (MSFC) based on the Takagi–Sugeno (T–S) model, where the membership functions are based on polar coordinates. Adopting a polar coordinate system helps to reduce the number of rules significantly, especially when fuzzy variables exhibit extreme values. This approach also allows more rules to be assigned in certain directions (angles) than others, while the radius represents the distance to the reference. The MSFC uses three strategies: constant input (CI), where a predefined gain is set in the exterior rules; standard discrete state–space model linear–quadratic regulator (LQR), in the intermediate rules; and an incremental state–space model LQR (INC-LQR) in the central rule. A technique employing fuzzy fusion is suggested to facilitate smooth transitions among the various methods. The results show that the proposed MSFC in polar coordinates has a faster transient response and zero steady-state error compared to other fuzzy controllers, and it requires fewer rules compared to an MSFC based on Cartesian coordinates.},
  archive      = {J_ASOC},
  author       = {Pablo García Peris and Basil Mohammed Al-Hadithi},
  doi          = {10.1016/j.asoc.2025.112843},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112843},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-strategy fuzzy controller based on polar coordinates},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Space-distributed machine learning based on climate lag effect: Dynamic prediction of tuberculosis. <em>ASOC</em>, <em>171</em>, 112840. (<a href='https://doi.org/10.1016/j.asoc.2025.112840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An in-depth grasp of the complex relationship between climate and disease is crucial for fostering the development of public health security. However, certain limitations, such as overlooking spatial heterogeneity and lag effects, persist when revealing this complex relationship. With tuberculosis (TB) as a case, a pioneering space-distributed machine learning (SDML) framework is introduced to enhance TB prediction accuracy and unveil its complex relationship with climatic factors. Results demonstrate the nonlinear, intricate relationship between TB and climate variables, emphasizing the significant lag effect of climate variables on TB. Model comparisons demonstrate that SDML has a significant improvement in prediction, particularly in lag effect identification. The determination coefficient average of SDML (0.786) surpasses that of traditional machine learning (ML, 0.719). Utilizing an interpretable ML method to identify the impact of climate variables on TB, this study reveals evident spatial heterogeneity in the response of TB to climate. The spatial heterogeneity of the effects of extreme climate on TB suggests regionalized prevention and control strategies for diverse regions. This study provides a novel perspective on comprehending the intricate relationship between TB and climate, showcasing the feasibility of artificial intelligence-assisted scientific discovery.},
  archive      = {J_ASOC},
  author       = {Shuo Wang and Ziheng Li and Tianzuo Zhang and Mengqing Li and Liyao Wang and Jinglan Hong},
  doi          = {10.1016/j.asoc.2025.112840},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112840},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Space-distributed machine learning based on climate lag effect: Dynamic prediction of tuberculosis},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep non-local point cloud denoising network. <em>ASOC</em>, <em>171</em>, 112835. (<a href='https://doi.org/10.1016/j.asoc.2025.112835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an efficient representation of objects, the 3D point cloud is increasingly prevalent in various application fields. However, raw point clouds captured from scanning devices often contain noise, which significantly impairs the performance of downstream tasks such as surface reconstruction and object recognition. Consequently, point cloud denoising has emerged as a crucial task in geometry modeling and processing. Although deep learning has proven effective in this domain, existing learning-based methods predominantly focus on local information and tend to neglect the non-local features inherent in 3D point clouds. In this paper, we propose a deep non-local point cloud denoising network, DnPCD-Net, to address this issue. DnPCD-Net consists of three key components: 1) a feature extraction module that extracts local features for each point; 2) a densely-connected Transformer module that captures long-range dependencies across the input point set and feature channels; and 3) a feature fusion module that adaptively combines local and non-local features. Extensive experiments on both synthetic and real-scanned datasets demonstrate that DnPCD-Net achieves superior denoising performance, with statistically significant improvements in Chamfer Distance and Earth Mover’s Distance, as well as better visual quality, confirming its effectiveness and robustness in practical applications.},
  archive      = {J_ASOC},
  author       = {Huankun Sheng and Ying Li},
  doi          = {10.1016/j.asoc.2025.112835},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112835},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep non-local point cloud denoising network},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Small aerial object detection through GAN-integrated feature pyramid networks. <em>ASOC</em>, <em>171</em>, 112834. (<a href='https://doi.org/10.1016/j.asoc.2025.112834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small Aerial Object Detection (SAOD) is a pivotal research domain in computer vision, with significant applications in environmental regulation, intelligent surveillance, and autonomous vehicles. However, SAOD remains challenging due to low resolution, background noise, and variable object sizes. In this study, we propose a novel Feature Pyramid Generative Adversarial Network (FPGAN) to address these issues. FPGAN enhances feature extraction across multiple scales, improving precision, recall, and accuracy in detecting small aerial objects of diverse sizes. Furthermore, we integrate an Edge Sharpening Network (ESN) using the U-Net architecture to mitigate noise and distortions generated during adversarial learning, resulting in the FPGAN+ESN model. Extensive experiments on three SAOD datasets, namely DOTA, COWC, and OGST, demonstrate that our model outperforms state-of-the-art methods, showcasing remarkable improvements in detection accuracy. The proposed FPGAN+ESN approach enhances the resolution of small aerial objects and improves edge quality, leading to more robust and efficient SAOD. Our findings underscore the potential of the FPGAN+ESN model for tackling the complexities associated with SAOD tasks.},
  archive      = {J_ASOC},
  author       = {Usman Ahmad and Jing Liang and Tianlei Ma and Kunjie Yu and Faisal Mehmood and Farhad Banoori},
  doi          = {10.1016/j.asoc.2025.112834},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112834},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Small aerial object detection through GAN-integrated feature pyramid networks},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridge crack detection algorithm designed based on YOLOv8. <em>ASOC</em>, <em>171</em>, 112831. (<a href='https://doi.org/10.1016/j.asoc.2025.112831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity of the operating environment and the influence of natural factors, bridges are prone to various forms of damage, including cracks. However, traditional bridge detection methods often encounter challenges in terms of low detection accuracy and high computational resource consumption. The practical significance of accurate bridge crack inspection to society can be summarized as follows: ensuring bridge safety, preventing major accidents, maintaining bridge structural health, optimizing bridge management decisions, promoting scientific and technological progress, and enhancing public trust. The practical significance of accurate bridge crack inspection extends beyond the safety and stability of the bridge itself. It also relates to the safety of people's lives and property, as well as the harmony and stability of society. This study presents a bridge crack detection algorithm tailored around the YOLOv8 framework. Initially, the SPPF_UniRepLk module is incorporated into the algorithm's backbone network, aiming to bolster its capacity to capture and extract pertinent image features. Additionally, to further grasp the global dependencies between feature maps, a Global Channel Spatial Attention (GCSA) mechanism is introduced, which enhances the algorithm's sensitivity to global contextual information. Finally, in the neck network component of the algorithm, the Coordattention-Concat module is utilized to achieve the integration and refinement of multi-source features through nonlinear transformations and feature reweighting techniques, thereby significantly elevating the overall performance of the algorithm. The experimental outcomes demonstrate that the proposed bridge crack detection algorithm designed based on YOLOv8 achieves a mAP50–95 of 72.1 %, which is capable of accurately detecting cracks.},
  archive      = {J_ASOC},
  author       = {Haibo Xia and Qi Li and Xian Qin and Wenbin Zhuang and Haotian Ming and Xiaoyun Yang and Yiwei Liu},
  doi          = {10.1016/j.asoc.2025.112831},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112831},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bridge crack detection algorithm designed based on YOLOv8},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Black-box adversarial examples via frequency distortion against fault diagnosis systems. <em>ASOC</em>, <em>171</em>, 112828. (<a href='https://doi.org/10.1016/j.asoc.2025.112828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has significantly impacted prognostic and health management, but its susceptibility to adversarial attacks raises security risks for fault diagnosis systems. Previous research on the adversarial robustness of these systems is limited by unrealistic assumptions about prior model knowledge, which is often unobtainable in the real world, and by a lack of integration of domain-specific knowledge, particularly frequency information crucial for identifying unique characteristics for machinery states. To address these limitations and enhance robustness assessments, we propose a novel adversarial attack method that exploits frequency distortion. Our approach corrupts both frequency components and waveforms of vibration signals from rotating machinery, enabling a more thorough evaluation of system vulnerability without requiring access to model information. Through extensive experiments on two bearing datasets, including a self-collected dataset, we demonstrate the effectiveness of the proposed method in generating malicious yet imperceptible examples that remarkably degrade model performance, even without access to model information. In realistic attack scenarios for fault diagnosis systems, our approach produces adversarial examples that mimic unique frequency components associated with the deceived machinery states, leading to average performance drops of approximately 13 and 19 percentage points higher than existing methods on the two datasets, respectively. These results reveal potential risks for deep learning models embedded in fault diagnosis systems, highlighting the need for enhanced robustness against adversarial attacks.},
  archive      = {J_ASOC},
  author       = {Sangho Lee and Hoki Kim and Woojin Lee and Youngdoo Son},
  doi          = {10.1016/j.asoc.2025.112828},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112828},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Black-box adversarial examples via frequency distortion against fault diagnosis systems},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep neural network-based intelligent health monitoring system for oil and gas pipelines. <em>ASOC</em>, <em>171</em>, 112827. (<a href='https://doi.org/10.1016/j.asoc.2025.112827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oil and gas pipelines are critical infrastructures that require continuous monitoring to ensure public safety and prevent economic losses. This paper addresses the challenges associated with pipeline failures by proposing a Deep Neural Network (DNN)-based Structural Health Monitoring (SHM) system for real-time monitoring of oil and gas pipelines. The system utilizes installed transducers and ultrasound guided waves to collect data about the structural health without the need for pipeline shutdown. The DNN-based SHM system predicts three crucial crack parameters: crack location, width, and depth. The performance of the proposed system is compared with five commonly used Machine Learning (ML) approaches. The results demonstrate that the DNN-based SHM system outperforms the other ML-based systems, achieving 18 % less prediction error than the most accurate of the other ML approaches. Moreover, the average prediction accuracy with the proposed DNN approach for crack location, width, and depth were 97 %, 93 % and 96 %, respectively. The findings highlight the potential of DNNs for accurate and efficient pipeline health monitoring, contributing to improved decision-making and safe pipeline operations.},
  archive      = {J_ASOC},
  author       = {Mohamed Almahakeri and Ahmad Jobran Al-Mahasneh and Mohammed Abu Mallouh and Basel Jouda},
  doi          = {10.1016/j.asoc.2025.112827},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112827},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep neural network-based intelligent health monitoring system for oil and gas pipelines},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing prediction accuracy for high-dimensional small-sample-size microarray data cancer by combining chebyshev interpolation with new dual-net GAN. <em>ASOC</em>, <em>171</em>, 112826. (<a href='https://doi.org/10.1016/j.asoc.2025.112826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is a complex and multi-cellular interaction disease in the human immune system with compelling morbidity and mortality in patients. To improve cancer patient survival rate, medical professionals and clinicians commonly formulate prudent and effective cancer treatment plans by using the assistance of human microarray data. Studies reveal that gene expression profiling from microarray data has been frequently used to identify gene mutations in human cancers. But practically, this is an expensive and time-consuming technique when working with high-dimensional microarray cancer datasets. As a result, more recently, deep learning techniques in the field of artificial intelligence (AI) have become a widely effective and worthy technique to implement for cancer diagnosis and prognosis predictions in clinical applications. However, under high-dimensional but small-sample-size (HDSSS), statistically less sufficient patient sample cases, classic deep learning and machine learning models often fail to provide reliable prediction results in cancer diagnosis and prognosis. To tackle these tasks, this study focus effort on developing an AI approach termed Cheby-Dual-GAN virtual sample generation (VSG) technique, which combines Chebyshev interpolation with a novel dual-net GAN (Generative Adversarial Networks) generator, to generate high-dimensional artificial samples resembling the original data. We fed Chebyshev points and their membership function (MF) into the dual-net generator to generate highly representative virtual samples while avoiding mode collapse in GAN. Six microarray cancer datasets were used to demonstrate efficacy of the proposed method. Based on these microarray cancer datasets, we compared the Cheby-Dual-GAN method with two state-of-the-art VSG techniques. With different small training sample sizes, the proposed method can achieve significantly superior prediction accuracy in two predictive models compared to the two VSG techniques in terms of Accuracy (ACC), F-measure, mean absolute error (MAE), and mean absolute percentage error (MAPE) indicators. As a theoretical validation, the paired t-test is employed to determine whether the suggested method has statistically significant differences from the other methods. The results of paired t-test demonstrate that the proposed Cheby-Dual-GAN method enjoys significant improvement effects in terms of ACC, F-measure, MAE, MAPE indicators. According to our experimental results, the proposed method successfully outperforms other VSG methods for HDSSS microarray cancer datasets.},
  archive      = {J_ASOC},
  author       = {Liang-Sian Lin and Yao-San Lin and Der-Chiang Li and Susan C. Hu and Chih-I Huang},
  doi          = {10.1016/j.asoc.2025.112826},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112826},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing prediction accuracy for high-dimensional small-sample-size microarray data cancer by combining chebyshev interpolation with new dual-net GAN},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of personalized individual semantics in MAGDM with preference information. <em>ASOC</em>, <em>171</em>, 112822. (<a href='https://doi.org/10.1016/j.asoc.2025.112822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subjective and objective evaluations are often utilized simultaneously in Multi-Attribute Group Decision Making (MAGDM) problems. Decision makers’ different understanding of linguistic terms, i.e., personalized individual semantics, may influence decision making results. In this study, we propose a novel MAGDM model to deal with these problems based on two types of preference information: an objective multi-attribute linguistic decision matrix and subjective alternative rankings. In the first stage, we introduce a consistency-driven model to obtain the personalized interval numerical scales associated with each linguistic term and attribute weights. In the second stage, we introduce dynamic personalized individual semantics to maximize consensus by minimizing the discrepancy between individual opinions and collective opinion. The optimal alternative is then determined based on overall scores of the alternatives. We finally apply the proposed model in the selection of new energy vehicles, and conduct simulation experiments and comparative analysis. The results show that considering personalized individual semantics and weights of the decision makers brings much benefit for consensus reaching process in group decision making. A high group consensus level can be reached in this model, and the computational complexity of this model is acceptable.},
  archive      = {J_ASOC},
  author       = {Hongbin Liu and Zhuoyu Xu},
  doi          = {10.1016/j.asoc.2025.112822},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112822},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of personalized individual semantics in MAGDM with preference information},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage optimized unified adversarial patch for attacking visible-infrared cross-modal detectors in the physical world. <em>ASOC</em>, <em>171</em>, 112818. (<a href='https://doi.org/10.1016/j.asoc.2025.112818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared cross-modal object detectors, leveraging both visible and infrared imaging technologies, play a pivotal role in vision-based systems. However, they necessitate thorough security scrutiny due to the risks posed by physical adversarial attacks, which employ tailored physical inputs to deceive vision-based models, posing significant peril to the integrity of vision-based systems. While previous research has predominantly focused on the security of visible and infrared detectors individually, real-world scenarios often necessitate the use of visible-infrared cross-modal detectors with heightened reliability. However, there is a notable dearth of comprehensive security evaluations for these hybrid systems. Despite some efforts to explore attacks on cross-modal detectors, developing a robust and practical strategy remains a significant challenge. This study introduces TOUAP, a novel two-stage adversarial patch technique designed specifically for real-world, black-box visible-infrared detectors. TOUAP initiates with octagonal-shape optimization to create infrared adversarial samples, leveraging this to disrupt infrared detection. Subsequently, it generates visible patches resembling color QR codes while preserving the geometry of the infrared patch for precise cropping, thereby undermining the visible detector. The extensive experimental validation in both digital and physical domains emphatically underscores the superior effectiveness and robustness of TOUAP, outperforming conventional baseline methods convincingly.},
  archive      = {J_ASOC},
  author       = {Chengyin Hu and Weiwen Shi and Wen Yao and Tingsong Jiang and Ling Tian and Wen Li},
  doi          = {10.1016/j.asoc.2025.112818},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112818},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage optimized unified adversarial patch for attacking visible-infrared cross-modal detectors in the physical world},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explainable machine learning (XAI) framework for classification of intricate dancing posture among indian bharatanatyam dancers. <em>ASOC</em>, <em>171</em>, 112817. (<a href='https://doi.org/10.1016/j.asoc.2025.112817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {India, the cradle of ancient civilizations such as the Indus Valley, has a rich tradition of classical dance forms like Kathak, Bharatanatyam, Kuchipudi, Manipuri, and Odissi. Indian classical dance (ICD), deeply embedded in the cultural fabric, has developed detailed grammar, rules, and stagecraft. Bharatanatyam, one of the oldest and most esteemed dance forms, is known for its coordinated movements, balanced postures, and expressive gestures. Understanding the underlying semantics of performing arts like Bharatanatyam is challenging, especially since long dresses occlude the legs and camera viewpoints affect classification performance. While existing methods achieve varying accuracies in posture recognition, real-world applications often see reduced performance due to these challenges. This study aims to enhance the analysis of Bharatanatyam postures using a force platform to record real-time vertical ground reaction force (VGRF) data. Key contributions include recording real-time force data for six intricate Bharatanatyam postures, extracting VGRF features, classifying postures using machine learning techniques, and implementing Explainable AI (XAI) methods to elucidate feature contributions. The novelty of this approach lies in using force platform data instead of video images, achieving 98.2 % accuracy, and addressing issues like occlusions and background clutter. The comparative results demonstrate that XAI methods outperform traditional classifiers, providing robust solutions for accurate dance posture analysis and reducing expert-based errors in classifications, thereby bridging traditional dance art with cutting-edge technologies.},
  archive      = {J_ASOC},
  author       = {K. Adalarasu and RM. Kuppan Chetty and K. Ghousiya Begum and S. Harini and Mukund Janardhanan},
  doi          = {10.1016/j.asoc.2025.112817},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112817},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An explainable machine learning (XAI) framework for classification of intricate dancing posture among indian bharatanatyam dancers},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrigendum to “Suppressed possibilistic fuzzy c-means clustering based on shadow sets for noisy data with imbalanced sizes” [Appl. soft comput. 167 (2024) 112263]. <em>ASOC</em>, <em>171</em>, 112815. (<a href='https://doi.org/10.1016/j.asoc.2025.112815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Haiyan Yu and Honglei Li and Xiaoyu Xu and Qian Gao and Rong Lan},
  doi          = {10.1016/j.asoc.2025.112815},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112815},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Suppressed possibilistic fuzzy c-means clustering based on shadow sets for noisy data with imbalanced sizes” [Appl. soft comput. 167 (2024) 112263]},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient point cloud representation learning with a recurrent hierarchical framework. <em>ASOC</em>, <em>171</em>, 112814. (<a href='https://doi.org/10.1016/j.asoc.2025.112814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud analysis is developing rapidly. Due to point clouds’ disordered and unstructured nature, an increasing number of methods are designed to be voluminous and complex in exchange for performance improvement. This tendency renders models inefficient and substantially increases the parameter and computation demands. To challenge the bias that high-performance models necessarily require high parameter count and computational complexity in the field, we propose an efficient recurrent hierarchical framework called PointLoop, which achieves impressive performance with low parameter count and computation. Furthermore, local features are fundamental in shaping the representation of point clouds. However, the uneven distribution of points results in a local feature generalization issue in dense and sparse regions. To this end, we propose a module called MNPNet that is tailor-made for local spatial geometry feature abstraction within PointLoop. It can be integrated into other networks. We conduct meticulous control and ablation experiments on multiple challenging benchmarks. Experimental results show that PointLoop has an efficient point cloud representation ability. In 3D semantic and part segmentation, PointLoop outperforms the state-of-the-art method with only 7.2% and 13.3% parameter counts, respectively. In 3D classification, PointLoop achieves comparable performance with merely 11.4% of the parameters of the state-of-the-art method.},
  archive      = {J_ASOC},
  author       = {Ziming Wang and Boxiang Zhang and Ming Ma and Yue Wang and Taoli Du and Wenhui Li},
  doi          = {10.1016/j.asoc.2025.112814},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112814},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient point cloud representation learning with a recurrent hierarchical framework},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Methodology for online detection and classification of power quality disturbances based on FPGA. <em>ASOC</em>, <em>171</em>, 112813. (<a href='https://doi.org/10.1016/j.asoc.2025.112813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transition from conventional energy systems to decentralized generation based on renewable energy sources presents significant challenges. Sophisticated devices are required to monitor and manage the real-time flow and quality of energy. These tools require efficient algorithms that minimize computational complexity, particularly for real-time applications. This work proposes a novel, computationally efficient methodology for the real-time detection and classification of seven types of power quality disturbances (PQDs) based on Multiresolution Analysis of the Discrete Wavelet Transform (MRA-DWT) and feature extraction methods such as RMS and Logarithmic Energy Entropy. The extracted distinctive feature vector, consisting of seven elements, serves as input to a classifier based on a Feed Forward Neural Network (FFNN). The classifier identifies the type of disturbance in 8.30 microseconds, achieving classification accuracies of 97.7% with synthetic data and 98.57% with real data obtained from an arbitrary waveform generator. The proposed algorithm was implemented on the Pynq-Z1 board from Xilinx using Vitis IDE and enables online acquisition and feature extraction from approximation and detail coefficients across five levels of DWT decomposition. The system processes data within times shorter than the sampling period, remaining within 10% of the maximum processing speed required for a 10 kHz sampling rate. Its fully sequential operation avoids storing input signals or DWT coefficients. A detailed system performance analysis was also conducted, evaluating each input sample’s acquisition and processing times. The study considered 2000 samples obtained from the laboratory, demonstrating the system’s effectiveness for online and real-time applications.},
  archive      = {J_ASOC},
  author       = {Eilen García Rodríguez and Enrique Reyes Archundia and José A. Gutiérrez Gnecchi and Arturo Méndez Patiño and Marco V. Chávez Báez and Oscar I. Coronado Reyes and Néstor F. Guerrero Rodríguez},
  doi          = {10.1016/j.asoc.2025.112813},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112813},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Methodology for online detection and classification of power quality disturbances based on FPGA},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotional classification in COVID-19: Analyzing chinese microblogs with domain-adapted contrastive learning. <em>ASOC</em>, <em>171</em>, 112812. (<a href='https://doi.org/10.1016/j.asoc.2025.112812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion analysis for COVID-19 is a domain-specific task, such as the epidemic, which plays a significant part in scientific research institutions and governments to track the emotional changes and trends of society. When introducing general domain textual information, currently used techniques just concentrate on learning the domain-invariant information to reduce domain discrepancy but ignore the maximum use of domain-general information to solve the problem of domain-specific data scarcity. As a result of this inspiration, we develop a domain-adapted contrastive learning-based emotion classification model, which consists of three modules: text representation, emotion identification, and domain adaptation. In this model, the text representation module is used to obtain a representation of sentences, and then the domain adaptation module is employed to pull the representation space of domain-specific data and domain-general data to overcome domain discrepancy and ultimately achieve better performance in the emotion identification module. To fortify our model, we propose two different contrastive learning strategies in the domain adaptation module. Experimental results on the SMP2020-EWECT show that our two strategies achieve F-values of 66.28% and 67.39% respectively, which significantly outperform the baselines despite the scarcity of domain-specific data. Interpretability analysis further demonstrates that the model employing domain-adapted contrastive learning can better understand domain text emotions.},
  archive      = {J_ASOC},
  author       = {Nankai Lin and Hongyan Wu and Aimin Yang and Lianxi Wang},
  doi          = {10.1016/j.asoc.2025.112812},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112812},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Emotional classification in COVID-19: Analyzing chinese microblogs with domain-adapted contrastive learning},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision transformer with window sequence merging mechanism for image classification. <em>ASOC</em>, <em>171</em>, 112811. (<a href='https://doi.org/10.1016/j.asoc.2025.112811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViTs) have demonstrated significant progress and promising performance in image classification. However, there are still some challenges and limitations for ViTs, such as dependence on large datasets and high computational requirements. To address these issues, we propose a novel window sequence merging-based ViT (wsm-ViT for short) to enhance the capability of ViTs in representation learning. The wsm-ViT features a hierarchical architecture consisting of four stages. The first stage includes an overlapping patch tokenizer and a token mixing block. The subsequent three stages incorporate convolutional window-merging layers as well as token mixing blocks. At each stage, the window-wise stepped self-attention is computed within each local window, followed by the aggregation of global information based on the association degree using global query and key vectors across all windows. Crucially, a window sequence merging mechanism facilitates robust inter-window information interaction. The hierarchical architecture provides flexibility for modeling at various scales while maintaining linear computational complexity. Extensive experiments conducted on five datasets with varying sizes and resolutions show that wsm-ViT achieves superior performance compared to many existing Transformer models. Additionally, its performance on the specialized Food-101 dataset also highlights its potential for future applications in practical fields such as food safety and nutrition industries.},
  archive      = {J_ASOC},
  author       = {Erjie Jiao and Qiangkui Leng and Jiamei Guo and Xiangfu Meng and Changzhong Wang},
  doi          = {10.1016/j.asoc.2025.112811},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112811},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Vision transformer with window sequence merging mechanism for image classification},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy attention-based deep neural networks for acute lymphoblastic leukemia diagnosis. <em>ASOC</em>, <em>171</em>, 112810. (<a href='https://doi.org/10.1016/j.asoc.2025.112810'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research introduces the Fuzzy Squeeze-and-Excitation Densely Connected Convolutional Networks with Orthogonal Projection Loss for precise diagnosis of Acute Lymphoblastic Leukemia. This advanced framework uniquely combines fuzzy neural networks, the Squeeze-and-Excitation module, and Orthogonal Projection Loss to address the intricate challenges in medical image analysis. Our quantitative evaluations demonstrate significant improvements over existing models. The proposed model achieved an outstanding accuracy of 99.26 %, surpassing other baseline models by at least 3.01 %. Precision, recall, and F1 score also showed remarkable enhancements, with respective values of 98.88 %, 98.94 %, and 98.91 %, outperforming the best-performing baseline model, which attained an accuracy of 96.09 % and an F1 score of 94.27 %. These results were statistically validated using a two-tailed paired t-test, confirming the model's robustness with a significance level of 0.01. The integration of fuzzy logic and deep learning techniques not only improved the diagnostic accuracy but also enhanced the model's adaptability to the high variability and complexity of medical images, marking a significant advancement in AI-assisted diagnostics for personalized patient care.},
  archive      = {J_ASOC},
  author       = {Tairan Zhang and Gang Xue},
  doi          = {10.1016/j.asoc.2025.112810},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112810},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy attention-based deep neural networks for acute lymphoblastic leukemia diagnosis},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A tightly-coupled dense monocular visual-inertial odometry system with lightweight depth estimation network. <em>ASOC</em>, <em>171</em>, 112809. (<a href='https://doi.org/10.1016/j.asoc.2025.112809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various fields such as unmanned aerial vehicles (UAVs) and autonomous driving, monocular dense Simultaneous Localization and Mapping (SLAM) and Visual Odometry (VO) allow devices of above mentioned fields to estimate their position and orientation in real-time while constructing dense maps, relying solely on a single camera sensor. However, existing solutions for dense SLAM/VO systems often come with high computational costs and lead to issues, such as scale drift and reduced localization accuracy, making them less practical than their sparse counterparts. We present MVS-VIO, a novel dense monocular visual inertial odometry system composed of two main components: real-time pose estimation and global Truncated Signed Distance Function (TSDF) reconstruction. The first component is LW-MVSNET, a lightweight multi-view depth estimation network that utilizes only three views and 68 depth hypotheses. The adaptive view aggregation (AVA) and adaptive depth hypotheses (ADH) modules can effectively reject inaccurate depth estimation results, preventing significant error accumulation during runtime by adopting an uncertainty mask. The second is a tightly-coupled optimization method leveraging a deep photometric error. To address the problem of underutilization of information due to a delayed generation of depth estimation, we incorporate a delayed marginalization strategy to optimize all the variables. LW-MVSNET is trained on the Replica dataset and performs good generalization on the TUM-RGBD and the EuRoC datasets, and the ablation study further validates the effectiveness of our modules. Notably, in all real-world sequences of the EuRoC dataset, our proposed MVS-VIO system outperforms comparable dense monocular systems. It operates stably in all eleven sequences at a rate of 10.08 frames per second (FPS), and achieves an average absolute trajectory error (ATE) of 0.066 meters, which represents state-of-the-art performance. This demonstrates that our method can reconstruct dense maps in real-time while maintaining a level of accuracy comparable to that of sparse systems.},
  archive      = {J_ASOC},
  author       = {Xin Wang and Zuoming Zhang and Luchen Li},
  doi          = {10.1016/j.asoc.2025.112809},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112809},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A tightly-coupled dense monocular visual-inertial odometry system with lightweight depth estimation network},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLAP: Fireworks algorithm based probabilistic planning. <em>ASOC</em>, <em>171</em>, 112808. (<a href='https://doi.org/10.1016/j.asoc.2025.112808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic planning based on the relational dynamic influence diagram language focuses on actions’ concurrent and probabilistic effects that lead to an enormous search space. While the complexity of this problem makes it challenging for existing probabilistic planners to solve all outcomes deterministically within the time budget, parallel computing aims to tackle complex issues within a fixed time limit by facilitating more resources. Thus, motivated by evolutionary computing, we propose a novel planning approach, FLAP( Fireworks aLgorithm based probAbilistic Planning ), to improve the quality and the stability of probabilistic planning for the first time. FLAPschedules multiple planner clients and adapts the fireworks algorithm in which future states can be generated via simulating fireworks explosion, evaluated by the reward-aware fitness, and then assigned to different planners for the environment evolution. The search strategy of FLAPis a look-ahead enhanced anytime Monte Carlo Tree Search, where a tree structure is composed representing plan traces and a fitness function that captures traces’ rewards is used to guide the growth of the tree. FLAPis implemented and evaluated on multiple competition domains and gains plan executions with higher rewards than the state-of-the-art planner. By sharing information among planners, not only the best execution of plans but also the small variance of all generated plans show the superiority of our approach. 1},
  archive      = {J_ASOC},
  author       = {Dongning Rao and Shanzhen Yi and Zhihua Jiang},
  doi          = {10.1016/j.asoc.2025.112808},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112808},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FLAP: Fireworks algorithm based probabilistic planning},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heuristic-machine learning models for solar radiation forecasting in köppen climate zones. <em>ASOC</em>, <em>171</em>, 112807. (<a href='https://doi.org/10.1016/j.asoc.2025.112807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the effectiveness of an integrated heuristic-machine learning approach in forecasting solar radiation in various Köppen climate zones. Our objective was to refine the model selection process for solar zoning, which involves characterizing solar radiation patterns in various geographic regions. We evaluated 107 heuristic models in 1216 automatic weather stations spread across tropical, arid, and temperate zones. Model performance was assessed based on criteria such as similarity of the data and distance to determine the most effective models. Of the 107 models tested, 98 proved optimal for at least one weather station. However, only 35.4% of all weather stations met the minimum performance benchmark, with 38.7% in Climate A, 31.5% in Climate B, and 26.3% in Climate C. Kmeans++, Hierarchical Clustering and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) techniques were used to classify regions based on solar irradiance. By integrating climatic data, including Köppen climate types and meteorological variables, with geographical location. Key findings reveal significant clustering patterns related to specific climate types , demonstrating how solar radiation correlates with data dispersion and climatic subtypes. For Climate A, clusters primarily formed around a tropical subtype with irregular precipitation patterns, where climate data yielded well-defined clusters, and data dispersion showed more irregular grouping, often favoring multivariate linear and polynomial models. Climate B clusters around hot desert climates with rainfall in winter, showing a preference for daily multivariate linear models, while Climate C identifies optimal models only for weather stations with mild and low data dispersion. Differences between climate-based and data dispersion-based clustering were evident: climate information aligned closely with climatic subtypes, whereas data dispersion captured geographic dependencies. DBSCAN was particularly effective at identifying patterns in arid and temperate climates. This research not only clarifies the relationship between climate types and model performance, but also improves the methodology for selecting appropriate models in regions with scarce data, contributing to significant advancements in the development of gray-box models for solar radiation forecasting.},
  archive      = {J_ASOC},
  author       = {Ilse Cervantes and Carlos A. Cervantes-Ortiz and David-Hiram Vazquez-Santana and Amadeo Arguelles},
  doi          = {10.1016/j.asoc.2025.112807},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112807},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heuristic-machine learning models for solar radiation forecasting in köppen climate zones},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The value of expert judgments in decision support systems. <em>ASOC</em>, <em>171</em>, 112806. (<a href='https://doi.org/10.1016/j.asoc.2025.112806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenge to improve a decision support system (DSS) based on expert judgments; the literature proposes to improve accuracy and performance by increasing the sophistication and complexity of the DSS, but at what cost? This study presents a model for encoding a DSS based on expert judgments and evaluating its efficiency, establishing a three-part analysis structure: information requirements (number of judgments), quality requirements (quality assurance mechanisms), and algorithmic complexity. With a focus on the cost of judgments, a systematic and quantitative coding of the performance and cost in each part of the DSS is established. A “break-even point” efficiency measure, defined as the maximum percentage of the optimal performance that can be paid per unit of resources, is proposed to ensure that the use of the DSS remains profitable. Counterintuitively, the results of a case study show that the efficiency of DSSs does not necessarily increase with respect to the informativeness level of DSSs. Overall, this study provides a new method for evaluating the efficiency of DSSs.},
  archive      = {J_ASOC},
  author       = {Carlos Sáenz-Royo and Francisco Chiclana},
  doi          = {10.1016/j.asoc.2025.112806},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112806},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The value of expert judgments in decision support systems},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path analysis for effective fault localization in deep neural networks. <em>ASOC</em>, <em>171</em>, 112805. (<a href='https://doi.org/10.1016/j.asoc.2025.112805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has revolutionized numerous fields, yet the reliability of Deep Neural Networks (DNNs) remains a concern due to their complexity and data dependency. Traditional software fault localization methods, such as Spectrum-based Fault Localization (SBFL), have been adapted for DNNs but often fall short in effectiveness. These methods typically overlook the propagation of faults through neural pathways, resulting in less precise fault detection. Research indicates that examining neural pathways, rather than individual neurons, is crucial because issues in one neuron can affect its entire pathway. By investigating these interconnected pathways, we can better identify and address problems arising from the collective activity of neurons. To address this limitation, we introduce the NP-SBFL method, which leverages Layer-wise Relevance Propagation (LRP) to identify essential faulty neural pathways. Our method explores multiple fault sources to accurately pinpoint faulty neurons by analyzing their interconnections. Additionally, our multi-stage gradient ascent (MGA) technique, an extension of gradient ascent (GA), enables sequential neuron activation to enhance fault detection. We evaluated NP-SBFL-MGA on the well-established MNIST and CIFAR-10 datasets, comparing it to other methods like DeepFault and NP-SBFL-GA, as well as three neuron measures: Tarantula, Ochiai, and Barinel. Our evaluation utilized all training and test samples—60,000 for MNIST and 50,000 for CIFAR-10—and revealed that NP-SBFL-MGA significantly outperformed the baselines in identifying suspicious pathways and generating adversarial inputs. Notably, Tarantula with NP-SBFL-MGA achieved a remarkable 96.75 % fault detection rate compared to DeepFault’s 89.90 %. NP-SBFL-MGA also demonstrated comparable results to the baselines on additional benchmarks, highlighting a strong correlation between critical path coverage and the number of failed tests in DNN fault localization.},
  archive      = {J_ASOC},
  author       = {Soroush Hashemifar and Saeed Parsa and Akram Kalaee},
  doi          = {10.1016/j.asoc.2025.112805},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112805},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Path analysis for effective fault localization in deep neural networks},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class-specific feature selection through analysis of relevance, redundancy, and complementarity. <em>ASOC</em>, <em>171</em>, 112804. (<a href='https://doi.org/10.1016/j.asoc.2025.112804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information-theoretic feature selection (ITFS) relies on understanding feature relevance, redundancy, and complementarity through two complementary facets: conceptual formulation for interpretation and computational formulation for algorithm development. Traditional ITFS methods belong to classification-based feature selection, which selects a single feature subset for all classes. This single-subset approach limits their ability to capture class-specific characteristics. In contrast, class-specific feature selection (CSFS) aims to select distinct feature subsets tailored to each individual class. To address this limitation, we introduce a novel class-specific ITFS method by examining relevance, redundancy, and complementarity from both conceptual and computational perspectives. As the basis for both conceptual and computational analysis, we first introduce several class-specific information-theoretic metrics and examine their relationships. From a conceptual perspective, we establish formal definitions of feature relevance, redundancy, and complementarity for CSFS. From a computational perspective, we propose a class-specific maximal-relevance minimal-redundancy maximal-complementarity (CSMRmRMC) criterion for feature assessment. This criterion enables the evaluation of feature subsets with respect to individual classes. By incorporating this criterion into a sequential forward search strategy, we develop an algorithm that selects the most relevant feature subsets tailored to each class. Extensive experiments demonstrate our method’s superior classification performance across diverse benchmark methods.},
  archive      = {J_ASOC},
  author       = {Xi-Ao Ma and Chunhua Ju},
  doi          = {10.1016/j.asoc.2025.112804},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112804},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Class-specific feature selection through analysis of relevance, redundancy, and complementarity},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to multi-attribute group decision-making: Optimistic and pessimistic three-state three-way decision models. <em>ASOC</em>, <em>171</em>, 112803. (<a href='https://doi.org/10.1016/j.asoc.2025.112803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three-state three-way decision (TSTWD) model extends the classic three-way decision (TWD) model by incorporating an uncertainty state. To properly describe decision risks inherent in multi-attribute group decision-making (MAGDM) problem, this paper proposes a novel approach employing optimistic and pessimistic TSTWD models. Firstly, the optimistic and pessimistic TSTWD models are constructed to effectively capture the diverse attitudes of decision-makers (DMs) towards different decision-making scenarios. Secondly, the relative loss functions of the proposed two TSTWD models are derived based on the attribute evaluation values, respectively, and some properties of three threshold functions are discussed in detail. Thirdly, a framework to solve the MAGDM problem is developed based on proposed optimistic and pessimistic TSTWD models, in which the conditional probabilities with respect to the three states are calculated based on the positive ideal solution, negative ideal solution, and compromise solution, respectively. Simultaneously, a loss function aggregation method is presented to determine the comprehensive losses of alternatives. Finally, the feasibility and effectiveness of the proposed method are verified through illustrative examples, sensitivity as well as comparative analysis.},
  archive      = {J_ASOC},
  author       = {Yanbing Ju and Yanxin Xu and Han Wang and Tian Ju and Xia Li and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2025.112803},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112803},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel approach to multi-attribute group decision-making: Optimistic and pessimistic three-state three-way decision models},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustness analysis of multi-layer feedforward artificial neural networks for finite element model updating. <em>ASOC</em>, <em>171</em>, 112799. (<a href='https://doi.org/10.1016/j.asoc.2025.112799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite element model updating requires a computational solution procedure for minimizing the errors between the responses of an actual structure and the predictions of its numerical model. Modal data have shown great potential to be used as the inputs of neural networks for model updating; however, the neural network robustness depends on the combination of modal data and the network architecture selection. This paper discusses the neural network methodology for updating models and highlights the critical points for incorporating modal data as the parameters determining the network architecture. Several arrangements are examined for the input layer of networks that employ multi-layer perceptron and radial basis functions for estimating the structural parameters of a cantilever beam and a benchmark structural grid. The modal outputs of the analytical models feed the input layer of the neural networks to estimate the elemental stiffness and mass parameters. Noise-free and noise-contaminated simulated modal data are used to verify the noise resistance of the network architectures. Full factorial and Latin hypercube techniques are exploited for sampling the training data. The results indicate that including mode shape information of the lowest modes in the input layer could improve the parameter estimation accuracy more than considering natural frequencies of the higher modes. Also, the estimation procedure shows negligible sensitivity to the sparsity of input samples in the full factorial method, which can alleviate its computational intensity. Further, the radial basis function neural networks can decrease the training time without loss of estimation accuracy, while their noise-resistance is comparable with that of the multi-layer perceptron networks. The presented methodology simplifies the neural network's topological aspects to selecting the number of hidden neurons, reducing subjective complexity. Hence, judiciously selecting the number of hidden neurons to prevent overfitting allows for flexibility in choosing other topological elements, with minimal effects on the results. By gaining insights from the proposed strategies for network topology design, one can employ shallow artificial neural networks for structural parameter estimation using modal data with minimal concerns about the robustness of the finite element model updating results with respect to the network configuration.},
  archive      = {J_ASOC},
  author       = {Milad Mehrkash and Erin Santini-Bell},
  doi          = {10.1016/j.asoc.2025.112799},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112799},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robustness analysis of multi-layer feedforward artificial neural networks for finite element model updating},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A transformer-based deep learning approach for recognition of forgery methods in spoofing speech attribution. <em>ASOC</em>, <em>171</em>, 112798. (<a href='https://doi.org/10.1016/j.asoc.2025.112798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forged speech is the intentional alteration of digital speech using speech editors or deepfake techniques for the purpose of disseminating malicious information. Nowadays, deep learning-based generative models can produce high-quality forged speech. This poses new challenges to audio forensics, such as the analysis of how the forgery happens. Existing methods either confine themselves to the detection/classification of spoofing or lack explainable properties on the obtained detectors/classifiers. In this article, beyond the spoofing detectors/classifiers, t-vector and s-vector are proposed to attribute spoofing speech, which contributes to finding the category of the involved forgery methods. With the sub-band log Mel-spectrogram as input, a pre-trained transformer is finetuned towards the forgery method recognition. The finetuned transformer produces an embedding representation for each input utterance, which is called t-vector. The t-vector is subsequently projected through a linear layer with the sigmoid activation to a score vector (named by s-vector), whose entries are confidential probabilities that the forgery method of input utterance is classified into each category. To measure the similarities between those vectors, a class-dependent threshold method is proposed to identify possible unknown forgery categories in open-set recognition scenarios. Extensive experiments are conducted on three benchmark datasets in the research of forgery method recognition, i.e., the datasets from the 2019 Automatic Speaker Verification spoofing and countermeasures challenge (ASVspoof2019), 2022 IEEE Signal Processing Cup Forgery method Recognition (SPCFR2022), and 2023 Fake Audio Detection (FAD2023). The results show that our proposed method achieves the highest accuracy on all the datasets in both closed-set and open-set recognition scenarios: 99.9 %/93.9 % on ASVspoof2019, 100 %/94.0 % on SPCFR2022, and 98.6 %/90.9 % on FAD2023. Furthermore, the noise robustness of the proposed method has also been demonstrated through experiments.},
  archive      = {J_ASOC},
  author       = {Qiang Zhang and Xiongwei Zhang and Meng Sun and Jibin Yang},
  doi          = {10.1016/j.asoc.2025.112798},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112798},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A transformer-based deep learning approach for recognition of forgery methods in spoofing speech attribution},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging uncertainty-guided spatial–temporal mutuality for skeleton-based action recognition. <em>ASOC</em>, <em>171</em>, 112797. (<a href='https://doi.org/10.1016/j.asoc.2025.112797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton representation has garnered considerable attention due to its robust and compact depiction of human actions. Recently, Graph Convolutional Networks (GCNs) have become a mainstream paradigm for skeleton-based action recognition. Nevertheless, existing approaches model the spatial–temporal dependencies by extracting spatial and temporal features separately. Co-occurrence relations of space and time are rarely considered, which can integratedly learn where and when to focus within the features space. Moreover, the current framework lacks awareness of its prediction uncertainty and exhibits relatively poor performance in classifying diverse actions. To tackle the above problems, in this paper, we propose the Uncertainty-guided Spatial–temporal Mutuality (UGSTM) that consists of three novel components. Firstly, we introduce a plug-and-play module termed Spatial–temporal Mutuality (STM) that decouples the feature channels into space, time, and spatial–temporal parts, then performs sufficient feature interactions to learn the comprehensive and efficient spatial–temporal dependencies. To leverage the uncertainty for guiding the model to make more confident and accurate predictions, we use evidential learning to quantify prediction uncertainty based on the theory of subjective logic. Specifically, the model learns a function that collects the evidence of predictions by placing a Dirichlet distribution on the class probabilities and we introduce a loss function about the learned uncertainty to assist the model in making more proper decisions. Finally, we integrated it with the gradient-based regularized loss to jointly emphasize the discriminative features of various classes. We conduct extensive experiments on multiple representative datasets, including Kinetics-skeleton , NTU RGB+D 60 , NTU RGB+D 120 , UAV-Human and Northwestern-UCLA , to validate our effectiveness. The experimental results show the proposed UGSTM’s Top-1 accuracy across five datasets can outperform the recent SOTA model by up to 1.3%.},
  archive      = {J_ASOC},
  author       = {Kunlun Wu and Bo Peng and Donghai Zhai},
  doi          = {10.1016/j.asoc.2025.112797},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112797},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Leveraging uncertainty-guided spatial–temporal mutuality for skeleton-based action recognition},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing credibility assessment in online social networks using multimodal deep learning. <em>ASOC</em>, <em>171</em>, 112796. (<a href='https://doi.org/10.1016/j.asoc.2025.112796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, users post digital content in different modes over online platforms from anywhere and anytime. This unverified content can be consumed quickly by a wider audience and may influence them. Researchers have suggested various models to determine the credibility of user-generated content (UGC) on online platforms. Most of these approaches consider a single mode of data, i.e., either text or image, whereas UGC on most online platforms is multimodal. However, developing an efficient multimodal framework is still an open research challenge. This paper presents a Multimodal Credibility Assessment Framework (MmCAF) to classify the given user content into credible or not credible with minimal error. It concentrates on sentence-level embeddings to represent entire sentences and their semantic information as vectors. Furthermore, to extract complex image features, MmCAF uses transfer learning and compound model-scaling-based deep learning networks. Several experiments have been performed on three datasets to validate the efficacy of the proposed framework. In addition, extensive experiments with respect to fusion strategies and comparison with baseline models have been carried out to ascertain model effectiveness. The comparison of MmCAF with other state-of-the-art models shows that MmCAF has a significant performance gain of a minimum of 6% over other state-of-the-arts.},
  archive      = {J_ASOC},
  author       = {Monika Choudhary and Satyendra Singh Chouhan},
  doi          = {10.1016/j.asoc.2025.112796},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112796},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing credibility assessment in online social networks using multimodal deep learning},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple instance learning with noisy labels based on symmetry loss. <em>ASOC</em>, <em>171</em>, 112795. (<a href='https://doi.org/10.1016/j.asoc.2025.112795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple instance learning (MIL) serves as a crucial form of weakly supervised learning, characterized by each training bag containing multiple instances with unknown labels, and each bag being assigned either a positive or negative label. The majority of prior studies have operated under the assumption that the labels in training datasets are entirely accurate. However, in many real-world scenarios, various factors such as limited knowledge may lead to labeling errors, resulting in label noise. Motivated by these challenges, this paper investigates MIL problems with label noise. To address this issue, we propose two frameworks: area under the receiver operating characteristic curve (AUC) maximization and balanced error rate (BER) minimization that highlight the advantages of incorporating symmetric loss. Moreover, we provide theoretical analysis to establish error bounds and ensure consistency. Finally, experimental results on seven datasets using six different losses not only demonstrate the effectiveness but also emphasize how introducing symmetric loss leads to superior performance. Furthermore, experiments on a convex barrier hinge loss further validate the significance of maintaining a symmetric condition despite its lack of symmetry everywhere.},
  archive      = {J_ASOC},
  author       = {Xuan Zhang and Yitian Xu and Xuhua Liu},
  doi          = {10.1016/j.asoc.2025.112795},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112795},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiple instance learning with noisy labels based on symmetry loss},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discovering ensembles of small language models out of scarcely labelled data for fake news detection. <em>ASOC</em>, <em>171</em>, 112794. (<a href='https://doi.org/10.1016/j.asoc.2025.112794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, news can be rapidly published and shared through several different channels (e.g., Twitter , Facebook , Instagram , etc.) and reach every person worldwide. However, this information is typically unverified and/or interpreted according to the publisher’s point of view. Consequently, malicious users can leverage these unofficial channels to share misleading or false news to manipulate the readers’ opinions and make fake news viral. In this scenario, the early detection of this malicious information is challenging as it requires coping with several issues, which primarily include the scarcity of up-to-date labelled examples and the volume, velocity and variety of news data. To address these issues, we here propose an efficient Semi-Supervised-Learning (SSL) approach to the discovery of a novel temporal ensemble of deep fake-news classifiers. The approach exploits a pseudo-labelling scheme to learn multiple (small-sized pre-trained) BERT-like models in a data- and compute- efficient manner, while effectively curbing the risks of error propagation and confirmation bias that affect standard self-training methods. The results of extensive experiments conducted on two benchmark datasets confirm the ability of the proposed solution to reach a satisfactory balance between classification accuracy and computation efficiency.},
  archive      = {J_ASOC},
  author       = {G. Folino and M. Guarascio and L. Pontieri and P. Zicari},
  doi          = {10.1016/j.asoc.2025.112794},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112794},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discovering ensembles of small language models out of scarcely labelled data for fake news detection},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recognition of egyptian hieroglyphic texts through focused generic segmentation and cross-validation voting. <em>ASOC</em>, <em>171</em>, 112793. (<a href='https://doi.org/10.1016/j.asoc.2025.112793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ancient Egyptian hieroglyphs form part of a complex language that has attracted the attention of Egyptologists, historians, and amateurs for centuries. In use for more than 3000 years, they consist of hundreds of symbols that can be transcribed into their Latin phonemes. Although there have been some previous works on the recognition of hieroglyphs through computer vision, this is a study of unprecedented depths and presents several unique contributions. On the one hand, we have created the largest and most complete dataset of existing Egyptian hieroglyphs to date, covering all the main symbols used on stelae. On the other, we have carried out a systematic analysis of detection, segmentation, and classification methods, focusing our research on a composite method of focused generic segmentation and classification with an ensemble model of ConvNeXt backbones using Cross-Validation Voting (CVV). Our trained model has been evaluated against several carved or painted stone stelae, obtaining excellent results. To the best of our knowledge, there is currently no other methodology capable of obtaining the classification results presented in this paper, and the method and the dataset presented represent a very significant advancement in the development of automated methods for reading Egyptian hieroglyphic texts.},
  archive      = {J_ASOC},
  author       = {Raúl Fuentes-Ferrer and Jaime Duque-Domingo and Pedro Javier Herrera},
  doi          = {10.1016/j.asoc.2025.112793},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112793},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Recognition of egyptian hieroglyphic texts through focused generic segmentation and cross-validation voting},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-channel collaborative transformer for continual learning. <em>ASOC</em>, <em>171</em>, 112792. (<a href='https://doi.org/10.1016/j.asoc.2025.112792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have been successful in many static tasks, but they often suffer from catastrophic forgetting in incremental scenarios. Recent trends suggest that dynamic structural methods, which use separate branches for each task, can effectively mitigate this issue in continual learning. However, these methods often require substantial memory and place excessive emphasis on preventing task interference or only allow task knowledge sharing to a certain extent (typically limited to task-general knowledge in shallow layers). Moreover, when using distillation loss to guide the learning of new tasks, it is generally confined to aligning the output of the last layer of the old and new models. This paper proposes a novel Dual-Channel Collaborative Transformer (DCCT) based on an encoder/decoder framework. The framework comprises a stabilization channel and an accumulation channel. The stabilization channel retains knowledge from the old tasks, while the accumulation channel learns knowledge from the new task. Additionally, we incorporate prompt techniques to design a novel Task-Specific Attention Block (TSAB) that trains task-specific tokens with small parameters for each task. TSAB enables the extraction of task-specific knowledge and facilitates active collaboration among them. Furthermore, we introduce a Multi-layer Spatial Attention Map Distillation Loss (MSAD) that leverages the more generalized knowledge from intermediate layers, enhancing the model’s adaptability to various tasks. We conduct extensive experiments on two benchmark datasets, CIFAR100 and ImageNet100, as well as the real-world food image classification dataset Food101. The results demonstrate that the proposed DCCT model achieves favorable results.},
  archive      = {J_ASOC},
  author       = {Haonan Cai and Yizhe Wang and Yong Luo and Keming Mao},
  doi          = {10.1016/j.asoc.2025.112792},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112792},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual-channel collaborative transformer for continual learning},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective initialization via lightweight coresets for large-scale gaussian mixture clustering. <em>ASOC</em>, <em>171</em>, 112791. (<a href='https://doi.org/10.1016/j.asoc.2025.112791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian mixture clustering is widely used in cluster analysis due to its efficacy. As the amount of data constantly grows, enhancing the efficiency of large-scale Gaussian mixture clustering has become the focus of research. The efficiency and accuracy of the large-scale Gaussian mixture clustering are highly influenced by the quality of its initial cluster centers. This paper introduces Core-AFK- MC 2 , an improved initialization approach designed to enhance the performance of the large-scale Gaussian mixture clustering algorithm by reducing the randomness in the initialization phase. This method combines the lightweight coresets technique and the global initialization strategy. We employ the coreset technique to compress large-scale datasets, and then use the importance distribution from the coreset construction to guide the selection of the initial cluster centers, which can significantly reduce the randomness of the initialization process. Furthermore, we perform initialization on the full dataset rather than the coreset, which can effectively prevent clustering results from falling into local optima. This method can produce better-quality initial cluster centers, reducing the number of iterations required for clustering algorithm convergence and enhancing the accuracy of clustering results. We use the Core-AFK- MC 2 method for large-scale Gaussian mixture clustering and propose a clustering algorithm named vc-AFK-GMM. Numerical experiments on standard large-scale benchmarks, including KDD, CIFAR-10, SONG, and SUSY, demonstrate that Core-AFK- MC 2 can reduce the number of iterations required for convergence, while vc-AFK-GMM improves the clustering accuracy by 0.8% compared to the already highly efficient clustering algorithm vc-GMM.},
  archive      = {J_ASOC},
  author       = {Qian Wang and Chuanli Wang and Chutian Wu and Dongjun Xin and Jingwen Chen},
  doi          = {10.1016/j.asoc.2025.112791},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112791},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effective initialization via lightweight coresets for large-scale gaussian mixture clustering},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Possibilistic modeling for fingerprint image quality assessment in automatic identification systems. <em>ASOC</em>, <em>171</em>, 112790. (<a href='https://doi.org/10.1016/j.asoc.2025.112790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of fingerprint images is a critical factor in the reliability of Automatic Identification Systems. In this paper, we address the challenge of ensuring high-quality data, free from uncertainty, in Fingerprint-based Automatic Identification Systems by introducing innovative possibilistic models and constructing a comprehensive database with ground truth on fingerprint image quality. Leveraging recent advancements in No-Reference Fingerprint Image Quality Assessment methods and utilizing benchmark datasets, labeled by person identity, our contributions include constructing a database labeled by image quality and introducing a two-level abstraction approach for fingerprint image quality modeling. The first level employs local quality indicators modeled using possibility distributions, which are then analyzed using possibilistic quality indicators in the second level. Possibility distributions, a key concept in possibility theory, are a well-established method for representing uncertain knowledge. Our contributions encompass the introduction of discriminative quality indicators, a sensitivity-aware feature selection process, and a benchmark package for the biometric community. Experimental results highlight the effectiveness of our modeling approach, demonstrating its ability to classify fingerprint images into two quality classes, namely, good and bad, while capturing intricate nuances of different quality levels. Unlike conventional fingerprint image quality assessment methods, this pioneering framework can be applied across different databases without the need for parametrization, providing a more nuanced and accurate representation of fingerprint image quality.},
  archive      = {J_ASOC},
  author       = {Sonda Ammar Bouhamed},
  doi          = {10.1016/j.asoc.2025.112790},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112790},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Possibilistic modeling for fingerprint image quality assessment in automatic identification systems},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rough neighborhood graph: A method for proximity modeling and data clustering. <em>ASOC</em>, <em>171</em>, 112789. (<a href='https://doi.org/10.1016/j.asoc.2025.112789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the novel concept of a rough neighborhood graph as a method for proximity modeling and data clustering. Unlike previous research in applying rough sets in graph theory, we decided to use an undirected graph, which will be a convenient structure for community discovery algorithms. Simultaneously, the approach proposed in this paper is closer to spectral clustering from the perspective of dataset representation. Our approach is also not a variation of rough fuzzy K-means; namely, the clusters detected by our method do not have to be concentric, and we do not need to define a specific number of clusters we want to discover. We also use a rough set-inspired framework to represent proximity relations between object pairs in the dataset. Our definition of the neighborhood is distance-based, not approximation-based. Due to this, we can process any dataset in which objects are in metric space (a defined metric allows pairwise comparison). We have validated our approach on various benchmark datasets, achieving nearly perfect clustering results that overcome the limitations of other popular algorithms. All required data and source codes for the proposed method can be downloaded from an online repository, and the results presented in this paper can be reproduced.},
  archive      = {J_ASOC},
  author       = {Tomasz Hachaj and Jarosław Wąs},
  doi          = {10.1016/j.asoc.2025.112789},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112789},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Rough neighborhood graph: A method for proximity modeling and data clustering},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evolutionary extreme learning machine algorithm for multi-cube unit single-layer neural networks. <em>ASOC</em>, <em>171</em>, 112788. (<a href='https://doi.org/10.1016/j.asoc.2025.112788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-order unit type is the most commonly used neuron architecture, which multiplies each input with a corresponding weight. These types have a restriction to linear separable problems, and overcoming this issue would require more advanced units. Higher-order units like the multi-cube neuron treat the input vector as a set of multi-dimensional hyper-cubes where each cube’s site corresponds to a weight. Training multi-cube unit (MCU) single-layer neural networks (SLNNs) is possible with the use of the extreme learning machine (ELM) algorithm, which has a very fast training speed because it does not use an iterative process like gradient-based methods. The training procedure begins by randomizing the hidden layer weights and thresholds, and then, with the help of the Moore–Penrose pseudo-inverse, it can analytically calculate the output layer(s) weights. Earlier works with MCU SLNNs showed significantly increased generalization performance in classification and regression problems compared to traditional low-order neuron types. The proposed evolutionary higher-order ELM (EHO-ELM) algorithm utilizes a modified self-adaptive genetic algorithm (GA) to create an SLNN containing MCUs in its hidden and output layers. EHO-ELM can automatically determine the optimal number and structure of cubic sub-units for each MCU of the neural network. Also, it can automatically tune the hidden layer weights and thresholds to increase the constructed network’s generalization ability. According to our knowledge, it is the first algorithm that can optimize the hidden weights vector and the sub-cube structure of MCUs at the same time. This paper’s experimental work section compares EHO-ELM in 25 datasets with 14 existing machine-learning methods. The compared approaches include ten gradient-based methods, support vector machine (SVM), and three ELM-based methods. The experimental results revealed that the proposed method had the best generalization performance. The significance of these results was verified using the Wilcoxon sign-rank test.},
  archive      = {J_ASOC},
  author       = {Vasileios Christou and Alexandros T. Tzallas and Christos Gogos and Markos G. Tsipouras and Georgios Tsoumanis and Nikolaos Giannakeas},
  doi          = {10.1016/j.asoc.2025.112788},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112788},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary extreme learning machine algorithm for multi-cube unit single-layer neural networks},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic scheduling for multi-objective flexible job shop via deep reinforcement learning. <em>ASOC</em>, <em>171</em>, 112787. (<a href='https://doi.org/10.1016/j.asoc.2025.112787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic scheduling methods for multi-objective flexible job shop are in high demand for actual production. Deep reinforcement learning (DRL) can realize offline training and online solving, which has become a promising method for dynamic multi-objective flexible job shop scheduling problem (DMOFJSSP). The dynamic event we focus on is new job insertions. In this paper, we first propose a new Markov decision process model to enhance the learning effect of the algorithm, where the new state representation is designed according to the dynamic and multi-objective characteristics of the problem, which helps the agent to capture more comprehensive state information and improve its decision-making ability; the new action space is composed of operation-machine pairs combined with invalid action masking technology to effectively explore the solution space; the two new reward functions are designed, which are closely related to multi-objective optimization, and both can effectively guide the agent to recognize key actions. Next, we propose a lightweight network architecture to realize representation learning and policy learning, which reduces the computational complexity of the algorithm to some extent. Finally, we evaluate the generalization of the policy model on two test datasets from different literatures. Experimental results show that the proposed method can achieve better optimization results compared with the well-known composite priority dispatching rules and the state-of-the-art models. The implementation code for the proposed method is available at https://github.com/cslxju/DMOFJSSP_DRL .},
  archive      = {J_ASOC},
  author       = {Erdong Yuan and Liejun Wang and Shiji Song and Shuli Cheng and Wei Fan},
  doi          = {10.1016/j.asoc.2025.112787},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112787},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic scheduling for multi-objective flexible job shop via deep reinforcement learning},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Authentication system selection for performance appraisal in human resource management using an intuitionistic fuzzy CIMAS-ARLON model. <em>ASOC</em>, <em>171</em>, 112786. (<a href='https://doi.org/10.1016/j.asoc.2025.112786'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authentication systems play a significant role in ensuring security in the use of information systems. In the context of human resource management, authentication systems are effectively utilized for performance appraisal based on information systems. Various authentication systems exist to support information systems, each with its strengths and weaknesses. Companies often face uncertainty when selecting the most suitable authentication system for performance appraisal systems. To address this uncertainty, the primary aim of this research is to propose a decision support system for companies to select the optimal authentication system. The multi-attribute group decision-making approach is adopted for authentication system selection. The intuitionistic fuzzy (IF) set-based criteria importance assessment (CIMAS)-alternative ranking using two-step logarithmic normalization (ARLON) hybrid model is developed. The novel IF-CIMAS method is employed to determine the importance levels of the criteria. The new IF-ARLON method is used for ranking alternatives of authentication systems. An algorithm demonstrating the application steps for the proposed hybrid model is developed. Two real-case studies are conducted based on this algorithm. The first case study focuses on authentication system selection from the perspective of information systems, while the second case study addresses authentication system selection from the viewpoint of users. For the creation of the initial decision matrices for both case studies, face-to-face interviews were conducted with experts to collect data based on linguistic scales. The research findings indicate that the "something you are" authentication system is identified as the best choice in both approaches. At this point, it is recommended to practitioners that the implementation of 'something you are' authentication systems provides a more secure and practical performance appraisal for addressing the authentication system selection problem. Additionally, robustness tests based on sensitivity analysis scenarios are conducted in both case studies, providing evidence of the reliability of the hybrid method.},
  archive      = {J_ASOC},
  author       = {Galip Cihan Yalçın and Karahan Kara and Sercan Edinsel and Esra Gökçen Kaygısız and Vladimir Simic and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2025.112786},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112786},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Authentication system selection for performance appraisal in human resource management using an intuitionistic fuzzy CIMAS-ARLON model},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning in industrial machinery: A critical review of bearing fault classification methods. <em>ASOC</em>, <em>171</em>, 112785. (<a href='https://doi.org/10.1016/j.asoc.2025.112785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The review provides an overview of the state-of-the-art in Deep Learning (DL) algorithms for rolling bearing fault classification which remains vital in industrial sectors including transportation, energy, manufacturing, and so forth. Even though they experience a variety of faults, rolling bearings are very crucial in ensuring machine efficiency. This prompts the review of the DL application, which is continuously growing, for intelligently detecting these faults. It comprehensively analyses DL models including Convolutional Neural Networks (CNNs), Auto-Encoders (AEs), Deep Belief Neural Networks (DBNs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), and some advanced networks i.e., Transfer Learning (TL), Transformer Neural Network (TNN), Self-Supervised Learning (SSL), Federated Learning (FL), Meta-learning and Interpreting Neural Networks assessing their effectiveness and limitations in fault classification. Thus, the current review is unique among available literature at present since it bridges this crucial gap by including all forms of advanced networks and gives an insight into the potential and challenges. Besides, it emphasizes the importance of different sensing techniques and key datasets in the field to show the contribution towards advancements of DL applications. Finally, referring to current challenges and recommendations for future research directions encompassing environmental adaption, sensor deployment, data preprocessing, model training enhancements, algorithm selection, classifier development, and systematic documentation frame the conclusive part of the paper. This review will serve as an important source for diligent researchers in legitimacy approaches of machinery reliability improvement by means of DL-based techniques for rolling bearing fault classification.},
  archive      = {J_ASOC},
  author       = {Attiq Ur Rehman and Weidong Jiao and Yonghua Jiang and Jianan Wei and Muhammad Sohaib and Jianfeng Sun and Shiju E and Khalil Ur Rehman and Yongwei Chi},
  doi          = {10.1016/j.asoc.2025.112785},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112785},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning in industrial machinery: A critical review of bearing fault classification methods},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the benefit of feature selection and ensemble feature selection for fuzzy k-nearest neighbor classification. <em>ASOC</em>, <em>171</em>, 112784. (<a href='https://doi.org/10.1016/j.asoc.2025.112784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the K-nearest neighbor classifier and, by extension, for the fuzzy K-nearest neighbor classifier, the ability to classify new observations well depends on how the distance to the neighbors is calculated. Feature selection and feature weighting are two approaches to focus the distance calculation on those features that are relevant for the classification. The aim of this research is to investigate the benefit of feature selection and ensemble feature selection for fuzzy K-nearest neighbor classification. Eight filter methods for feature selection and three feature selection ensembles (intersection, mean rank, union) using function perturbation are tested on twelve real-world data sets against using no feature selection. The results for the two classifiers demonstrate that both feature selection and ensemble feature selection address the problem of considering all features in the distance calculation for fuzzy k-nearest neighbor classifiers by removing features without significantly decreasing the classification error and, in some cases, even improving it. Moreover, ensemble feature selection using function perturbation with mean rank tends to lead to lower or similar test set errors than the average over the individual feature selection methods, indicating the possible additional benefit of using ensembles compared to individual methods.},
  archive      = {J_ASOC},
  author       = {Christoph Lohrmann and Alena Lohrmann and Mahinda Mailagaha Kumbure},
  doi          = {10.1016/j.asoc.2025.112784},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112784},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On the benefit of feature selection and ensemble feature selection for fuzzy k-nearest neighbor classification},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust graph contrastive learning with multi-hop views for node classification. <em>ASOC</em>, <em>171</em>, 112783. (<a href='https://doi.org/10.1016/j.asoc.2025.112783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has been proven successful in graph self-supervised learning by addressing label scarcity in real-world applications. Most existing methods fail to effectively incorporate multi-hop information into the augmented views but only focus on augmenting the raw input data, which incurs massive computational and memory costs. To address such limitations, we propose a novel approach, Multi-hop Views Graph Contrastive Learning (MHVGCL), that enhances the node classification performance for graphs. Specifically, in contrast to existing methods that generate multiple augmented heads via neural networks, our approach generates these heads by exploiting multi-hop information, which is obtained iteratively from a single output head. This technique can extract more comprehensive structural information without destroying the graph topology. We further devise a multi-hop contrastive loss function to maximize agreements among the multi-hop views of the same node, while minimizing them among different nodes. This design contributes to more robust representation learning that keeps structural attributes invariant under different augmented views. Numerical experimental results on a variety of benchmarks demonstrate the significant superiority of our approach over other advanced methods, by learning more discriminative node representations even with extremely limited labels.},
  archive      = {J_ASOC},
  author       = {Yutong Wang and Junheng Zhang and Ren Cao and Minhao Zou and Chun Guan and Siyang Leng},
  doi          = {10.1016/j.asoc.2025.112783},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112783},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust graph contrastive learning with multi-hop views for node classification},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fire anomaly detection based on low-rank adaption fine-tuning and localization using gradient filtering. <em>ASOC</em>, <em>171</em>, 112782. (<a href='https://doi.org/10.1016/j.asoc.2025.112782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fires can cause significant damage to both natural and industrial environments. Relying on traditional sensor-based detection or human surveillance for fire anomaly detection is risky, error-prone, and involves high costs. Therefore, automatic fire anomaly detection using computer vision has become an important technology. However, previous methods have not been able to meet the high accuracy requirements of real-world applications, and their generalizability and adaptability in various environments have also been limited. To address these issues, we propose a novel fire anomaly detection framework comprising the LRA-SwinCB model and the Gradient Filtering algorithm. The LRA-SwinCB model combines the Swin Transformer and the Classification Boost head(CB-head) module, trained using the Parameter-Efficient Transfer fine-tuning algorithm called Low-Rank Adaptation(LRA), to enhance the model’s generalization and prediction performance. The Gradient Filtering algorithm utilizes backward propagation gradients to localize fire anomalies, improving the accuracy and interpretability of the model. Our proposed method demonstrates superior performance while significantly reducing the number of model parameters. Our method not only in the public fire anomaly detection datasets, achieves an accuracy of 99.79%, showcasing excellent performance of fire anomaly detection, but also in real-world application datasets, achieves an accuracy of 99.58%, further validating the effectiveness of our proposed method.},
  archive      = {J_ASOC},
  author       = {Yicheng Qiu and Feng Sha and Li Niu and Guangyu Zhang},
  doi          = {10.1016/j.asoc.2025.112782},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112782},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fire anomaly detection based on low-rank adaption fine-tuning and localization using gradient filtering},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curb parking occupancy prediction based on real-time fusion of multi-view spatial-temporal information using graph attention gated networks. <em>ASOC</em>, <em>171</em>, 112781. (<a href='https://doi.org/10.1016/j.asoc.2025.112781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective curb parking management is crucial for reducing traffic congestion, minimizing cruising time, and lowering pollution in smart cities through accurate, timely occupancy predictions. However, traditional spatial-temporal fusion methods often rely on sequential concatenation, leading to spatial information lag and limited real-time fusion. These methods also overlook critical spatial features for curb parking, such as accessibility metrics. To address these limitations, this study introduces the Multi-View Graph Attention Gated Recurrent Unit (MGA-GRU) model, which innovatively fuses spatial-temporal data via graph attention gated networks to enhance curb parking occupancy predictions. Validated on a curb parking dataset from Lanzhou city, China (July 2019 to December 2020), the MGA-GRU model significantly outperforms baseline methods in both short-term and long-term predictions. Ablation experiments demonstrate that each component—the graph attention gates, multi-view graph structure, and real-time fusion—significantly enhances the model’s predictive accuracy, highlighting their collective importance in advancing occupancy prediction. By refining spatial-temporal fusion unit, the MGA-GRU model offers vital insights for optimizing curb parking resources and supports efficient traffic management and sustainable urban development.},
  archive      = {J_ASOC},
  author       = {Chonghui Qian and Kexu Yang and Jiangping He and Xiaojing Peng and Hengjun Huang},
  doi          = {10.1016/j.asoc.2025.112781},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112781},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Curb parking occupancy prediction based on real-time fusion of multi-view spatial-temporal information using graph attention gated networks},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective fitness landscape-based estimation of distribution algorithm for distributed heterogeneous flexible job shop scheduling problem. <em>ASOC</em>, <em>171</em>, 112780. (<a href='https://doi.org/10.1016/j.asoc.2025.112780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distributed heterogeneous flexible job shop scheduling problem with sequence-dependent setup time (DHFJSP-SDST) exists in discrete manufacturing systems. However, without incorporating problem knowledge to achieve effective exploration, many existing algorithms struggle to find high-quality solutions. A multi-objective fitness landscape-based estimation of distribution algorithm (MFLEDA) is introduced to address the DHFJSP-SDST and minimize both makespan and total energy consumption (TEC). For the three sub-problem, three probabilistic models are utilized to generate new solutions to overcome the issue of premature convergence. The multi-objective fitness landscape is exploited to extract problem knowledge and achieve adaptive selection of local search operators. Moreover, an energy-saving strategy is proposed to further reduce energy consumption. Twenty instances are exploited to evaluate the effectiveness of the MFLEDA. The experimental results indicate that the MFLEDA outperforms the comparison algorithms in solving DHFJSP-SDST.},
  archive      = {J_ASOC},
  author       = {Fuqing Zhao and Mengjie Li and Ningning Zhu and Tianpeng Xu and Jonrinaldi},
  doi          = {10.1016/j.asoc.2025.112780},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112780},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective fitness landscape-based estimation of distribution algorithm for distributed heterogeneous flexible job shop scheduling problem},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-scale analysis method with multi-feature selection for house prices forecasting. <em>ASOC</em>, <em>171</em>, 112779. (<a href='https://doi.org/10.1016/j.asoc.2025.112779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the high complexity and heterogeneity of house prices, and massive factors influencing house price fluctuations bring critical challenges for house prices forecasting. To capture the informative predictors and improve the prediction accuracy, a multi-scale analysis method with multi-feature selection (MSA-MFS) is proposed. The MSA-MFS involves four main steps. First, multi-source factors affecting house prices are collected from various perspectives to provide a series of predictive variables. Second, multi-feature selection is achieved by the proposed propensity co-association matrix-based ensemble feature selection (PCMEFS) method. Third, multi-scale analysis is performed via multivariate variational mode decomposition and the mode reconstruction strategy. Finally, multi-scale prediction and ensemble are conducted for final output from the viewpoint of individual timescales. For verification, two real-world datasets are used as the sample data. The experimental results demonstrate the proposed method achieves superior prediction performance with mean absolute percentage error (MAPE) less than 5.2 % on different datasets. This indicates the proposed method can be used as a reliable tool for house prices forecasting.},
  archive      = {J_ASOC},
  author       = {Jin Shao and Lean Yu and Nengmin Zeng and Jingke Hong and Xianzhu Wang},
  doi          = {10.1016/j.asoc.2025.112779},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112779},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-scale analysis method with multi-feature selection for house prices forecasting},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal 2D-cycle-generation framework for time series classification. <em>ASOC</em>, <em>171</em>, 112778. (<a href='https://doi.org/10.1016/j.asoc.2025.112778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In time series classification tasks, most datasets have small data volumes or are inconvenient to collect. Therefore, we proposed a data augmentation framework based on two-dimensional time series data (CycleTime). CycleTime consists of an optimal dimension transformation method, an optimal data augmentation method, and an optimal classifier respectively. Through comparative experiments, Gramian angular difference field, optimized CycleGAN and FCN are selected to form the optimal framework. The experimental results indicate that CycleTime can effectively improve the classification accuracy of both EEG and HAR time series data. On average across five datasets, CycleTime improved the results by 1.4 % compared with the existing benchmark methods. CycleTime can provide reference value and theoretical basis for other tasks of time series.},
  archive      = {J_ASOC},
  author       = {Xi Chen and Xiu Jin and Hua Zhang and Jianghui Xiong and Youhui Deng and Xiaodan Zhang},
  doi          = {10.1016/j.asoc.2025.112778},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112778},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Temporal 2D-cycle-generation framework for time series classification},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Space mission trajectory optimization via competitive differential evolution with independent success history adaptation. <em>ASOC</em>, <em>171</em>, 112777. (<a href='https://doi.org/10.1016/j.asoc.2025.112777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel Independent Success History Adaptation Competitive Differential Evolution (ISHACDE) algorithm to address the functional optimization problems and the Space Mission Trajectory Optimization (SMTO). ISHACDE is developed based on the efficient optimizer Competitive Differential Evolution (CDE) and integrates an independent success history adaptation scheme. This scheme inherits the hypothesis from Success History Adaptive Differential Evolution (SHADE) that the scaling factor F and crossover rate C r from success evolution may contribute to accelerating the evolution of the whole population, and we further hypothesize that the independent evolution of F in CDE may perform better. We conduct comprehensive numerical experiments on median-scale CEC2017, large-scale CEC2020, small-scale CEC2022, and the single-objective GTOPX benchmark to evaluate the performance of ISHACDE. Ten state-of-the-art optimizers and ten recently proposed optimizers are employed as competitor algorithms. The experimental results and statistical analysis confirm the competitiveness of the proposed ISHACDE against twenty optimizers, and the ablation experiments practically prove the effectiveness of the independent success history adaptation scheme. The source code of this research can be found in https://github.com/RuiZhong961230/ISHACDE .},
  archive      = {J_ASOC},
  author       = {Rui Zhong and Abdelazim G. Hussien and Shilong Zhang and Yuefeng Xu and Jun Yu},
  doi          = {10.1016/j.asoc.2025.112777},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112777},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Space mission trajectory optimization via competitive differential evolution with independent success history adaptation},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A priority rule heuristic for the multi-skilled resource-constrained project scheduling problem. <em>ASOC</em>, <em>171</em>, 112776. (<a href='https://doi.org/10.1016/j.asoc.2025.112776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research presents a priority rule heuristic approach for the multi-skilled resource-constrained project scheduling problem. The approach is based on a parallel schedule generation scheme which includes a new resource assignment procedure. The scheme combines three types of priority rules in order to schedule activities and assign resources to the skill requirements of these activities. In computational experiments, skill- and resource rule combinations are evaluated and selected based on two metrics using a Pareto Front approach. These rule combinations are then integrated with various activity priority rules after which their solution quality is evaluated. The heuristic approach and the selected rules are then employed to solve all project instances of the MSLIB dataset. It is shown that, on average, the presented approach is able to obtain solutions with a comparable quality to the solution quality of a meta-heuristic procedure from literature. Additionally, new best known solutions are obtained for the MSLIB dataset. The practical applicability of the heuristic is validated by solving empirical project instances.},
  archive      = {J_ASOC},
  author       = {Guillaume Vermeire and Mario Vanhoucke},
  doi          = {10.1016/j.asoc.2025.112776},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112776},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A priority rule heuristic for the multi-skilled resource-constrained project scheduling problem},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VSTDet: A lightweight small object detection network inspired by the ventral visual pathway. <em>ASOC</em>, <em>171</em>, 112775. (<a href='https://doi.org/10.1016/j.asoc.2025.112775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is difficult for object detection networks to effectively pay attention to the characteristics of small objects, especially when edge computing devices such as drones process their aerial images, and it is even more challenging to accurately detect a large number of different small objects with similar semantic information. Biological vision has a high degree of selectivity and sensitivity to similar objects by the interaction of visual mechanisms in the V1/V2-V4-IT ventral visual pathway, so a new lightweight one-stage model VSTDet is proposed. Specifically, inspired by the antagonistic receptive field and brightness contrast in the V1/V2 area, the orientation selection coding, and the graphic background separation in the V4 area to promote feature integration, the feature extraction network VCBNet and the multi-attribute information integration VDE module were designed. These designs deepen the prominence of small object feature information, to improve the ability of the network to extract small object feature information. In addition, inspired by the analysis of spatial context understanding in the IT area, this paper proposes a lightweight ITT head for information interaction detection head. The proposed VSTDet model has been experimentally evaluated on multiple small-object datasets. On the VisDrone2019 and AI-TODv2 datasets, VSTDet-l attains AP50 scores of 50.3% and 63.3%, respectively, with a parameter count of only 3.51M, reaching the state of the art in lightweight small object detection.},
  archive      = {J_ASOC},
  author       = {Yansong Niu and Chuan Lin and Xintong Jiang and Zhenshen Qu},
  doi          = {10.1016/j.asoc.2025.112775},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112775},
  shortjournal = {Appl. Soft. Comput.},
  title        = {VSTDet: A lightweight small object detection network inspired by the ventral visual pathway},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven social media text analysis during crisis: A review for natural disasters and pandemics. <em>ASOC</em>, <em>171</em>, 112774. (<a href='https://doi.org/10.1016/j.asoc.2025.112774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During crises such as disasters and pandemics, people share diverse data on social media, providing actionable information crucial for real-time crisis response. Such valuable information is helpful for crisis response authorities to prepare a real-time understanding of the crisis. This study reviews the application of Artificial Intelligence (AI) based text analysis techniques, specifically Machine Learning (ML), Deep Learning (DL), and Topic Modeling, in analyzing social media text for crisis response. Despite extensive research, a comprehensive review of these methods for crisis text analysis in crisis response is lacking. This study addresses this gap by reviewing 84 recent research studies. Our review study reveals that sequence-based neural networks often outperform other models in crisis text identification, while DL transformer-based models like BERT, Roberta provide superior contextual understanding for improved sentiment analysis. Additionally, integrating topic modeling with neural embeddings enhances situational awareness analysis. This study identifies significant research gaps, including the need for multilingual models, more robust handling of low-resource languages, and the development of real-time, explainable AI models to enhance transparency and trust in crisis response systems.},
  archive      = {J_ASOC},
  author       = {Junaid Abdul Wahid and Mingliang Xu and Muhammad Ayoub and Xiaoheng Jiang and Shi Lei and Yufei Gao and Shabir Hussain and Yu Yang},
  doi          = {10.1016/j.asoc.2025.112774},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112774},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AI-driven social media text analysis during crisis: A review for natural disasters and pandemics},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combinatorial optimization for UAV swarm path planning and task assignment in multi-obstacle battlefield environment. <em>ASOC</em>, <em>171</em>, 112773. (<a href='https://doi.org/10.1016/j.asoc.2025.112773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at improving the autonomous control performance and comprehensive combat capability of Unmanned Aerial Vehicle (UAV), this paper presents a combinatorial optimization for UAV swarm path planning and task assignment, including two optimization problems. For path planning, Rapidly-exploring Random Tree*(RRT*) - Artificial Potential Field (APF) algorithm is improved by introducing adaptive potential field adjustment and global search. The comparison simulations show that the optimization time by the improved RRT*-APF can be reduced by more than 80 % while achieving the shortest path. For task assignment, with the shortest path as input, the discrete PSO considering multiple constraints is employed for the optimal assignment scheme. Compared with other algorithms, the algorithm used can obtain the optimal solution. Finally, two-dimensional and three-dimensional cases are carried out. Case results show that the proposed method can not only effectively plan reasonable paths, but also obtain the optimal task assignment scheme.},
  archive      = {J_ASOC},
  author       = {Cong Guo and Lei Huang and Kuo Tian},
  doi          = {10.1016/j.asoc.2025.112773},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112773},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combinatorial optimization for UAV swarm path planning and task assignment in multi-obstacle battlefield environment},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized processes to create diverse models for one-class classifier ensembles. <em>ASOC</em>, <em>171</em>, 112772. (<a href='https://doi.org/10.1016/j.asoc.2025.112772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-class classification (OCC) is a machine learning technique employed in scenarios involving imbalanced data, limited labeled data, or the need to detect anomalies or novel patterns. One-Class Classification by Ensembles of Regression (OCCER) models have shown excellent performance for OCC problems. In this paper, we study whether the combinations of ensemble methods such as random subspace and random projection with OCCER can be used to improve the performance of OCCER. Our main objective is to explore the impact of diversity creation methods, including bagging, random subspace, and random projections, on enhancing the performance of the OCC ensemble method. Additionally, we investigate the feasibility of applying the OCCER ensemble method to a novel low-dimensional space generated by autoencoders, all while maintaining high prediction accuracy. The experimental results over 37 datasets demonstrate that the creation of diverse OCCER models using random subspace and random projection substantially improves the performance of the OCCER. The diversity creation method, Random subspaces, outperforms other diversity creation methods.},
  archive      = {J_ASOC},
  author       = {Deepthi Amuru and Amir Ahmad and Zia Abbas},
  doi          = {10.1016/j.asoc.2025.112772},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112772},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Randomized processes to create diverse models for one-class classifier ensembles},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal deep learning for credit rating prediction using text and numerical data streams. <em>ASOC</em>, <em>171</em>, 112771. (<a href='https://doi.org/10.1016/j.asoc.2025.112771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowing which factors are significant in credit rating assessments leads to better decision-making. However, the focus of the literature thus far has been mostly on structured data, and fewer studies have addressed unstructured or multimodal datasets. In this paper, we present an analysis of the most effective architectures for the fusion of deep learning models to predict company credit rating classes, using structured and unstructured datasets of different types. In these models, we tested various combinations of fusion strategies with selected deep-learning models, including convolutional neural networks (CNNs) and variants of recurrent neural networks (RNNs), and pre-trained language models (BERT). We study data fusion strategies in terms of level (including early and intermediate fusion) and techniques (including concatenation and cross-attention). Our results show that a CNN-based multi-modal model with a hybrid fusion strategy outperformed other multimodal techniques. In addition, by comparing simple architectures with more complex ones, we found that more sophisticated deep learning models do not necessarily produce the highest performance. Furthermore, we found that the text channel plays a more significant role than numeric data, with the contribution of text achieving an AUC of 0.91, while the maximum AUC of numeric channels was 0.808. Finally, rating agencies on short, medium, and long-term performance show that Moody’s credit ratings outperform those of other agencies like Standard & Poor’s and Fitch Ratings.},
  archive      = {J_ASOC},
  author       = {Mahsa Tavakoli and Rohitash Chandra and Fengrui Tian and Cristián Bravo},
  doi          = {10.1016/j.asoc.2025.112771},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112771},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-modal deep learning for credit rating prediction using text and numerical data streams},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRL-based structured task offloading decision in intelligent transportation scenarios. <em>ASOC</em>, <em>171</em>, 112770. (<a href='https://doi.org/10.1016/j.asoc.2025.112770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the surge in traffic data and the diversification of service needs, a large number of latency-sensitive and computationally intensive vehicular applications have emerged in intelligent transportation scenarios. Multi-user vehicular edge computing deploys lightweight servers at the network edge, has emerged as a promising paradigm for reducing latency in vehicular applications. However, the high mobility of vehicular terminals and the structured tasks present several challenges to the task offloading decision in intelligent transportation scenarios. Firstly, a method for predicting the location is designed to address the high mobility of vehicular terminals. Secondly, the structured tasks are divided into different types of subtasks to better manage shared resources and reduce resource contention. Finally, the problem is formulated as a multi-objective optimization problem to jointly optimize system time and energy consumption, and a DRL-based structured task offloading strategy is proposed. Simulation results validate the convergence and effectiveness of the strategy. Compared with baseline solutions, the proposed solution can reduce the system time and energy consumption by 9% and 6.7% respectively.},
  archive      = {J_ASOC},
  author       = {Si-feng Zhu and Cheng-tai Liu and Hai Zhu and Hao Chen and Rui Qiao and Xiao-yu Wu},
  doi          = {10.1016/j.asoc.2025.112770},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112770},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DRL-based structured task offloading decision in intelligent transportation scenarios},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Community detection by spectral methods in multi-layer networks. <em>ASOC</em>, <em>171</em>, 112769. (<a href='https://doi.org/10.1016/j.asoc.2025.112769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection in multi-layer networks is a crucial problem in network analysis. In this paper, we analyze the performance of two spectral clustering algorithms for community detection within the framework of the multi-layer degree-corrected stochastic block model (MLDCSBM) framework. One algorithm is based on the sum of adjacency matrices, while the other utilizes the debiased sum of squared adjacency matrices. We also provide their accelerated versions through subsampling to handle large-scale multi-layer networks. We establish consistency results for community detection of the two proposed methods under MLDCSBM as the size of the network and/or the number of layers increases. Our theorems demonstrate the advantages of utilizing multiple layers for community detection. Our analysis also indicates that spectral clustering with the debiased sum of squared adjacency matrices is generally superior to spectral clustering with the sum of adjacency matrices. Furthermore, we provide a strategy to estimate the number of communities in multi-layer networks by maximizing the averaged modularity. Substantial numerical simulations demonstrate the superiority of our algorithm employing the debiased sum of squared adjacency matrices over existing methods for community detection in multi-layer networks, the high computational efficiency of our accelerated algorithms for large-scale multi-layer networks, and the high accuracy of our strategy in estimating the number of communities. Finally, the analysis of several real-world multi-layer networks yields meaningful insights.},
  archive      = {J_ASOC},
  author       = {Huan Qing},
  doi          = {10.1016/j.asoc.2025.112769},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112769},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Community detection by spectral methods in multi-layer networks},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated generation adversarial and semi-supervised network for corpus callosum and cavum septum pellucidum complex segmentation in fetal brain ultrasound via progressive training. <em>ASOC</em>, <em>171</em>, 112767. (<a href='https://doi.org/10.1016/j.asoc.2025.112767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In prenatal diagnostics, measuring the Corpus Callosum (CC) is crucial for assessing fetal brain development, with the Cavum Septum Pellucidum (CSP) also playing a key role due to its close association with the CC. Our study addresses the challenge of distinguishing these interconnected structures by segmenting the Corpus Callosum and Cavum Septum Pellucidum Complex (CCSP) as a unified entity in mid-sagittal fetal brain ultrasound images. This approach ensures accurate biological measurements that reflect their combined significance in brain development. To improve segmentation accuracy and reduce errors inherent in manual methods, we propose the Fetal Brian Ultrasound Semi-supervised Generative Adversarial Segmentation Network (FB-SGASNet) tailored for few-shot datasets. FB-SGASNet enhances clinical applicability through: (i) an integrated framework combining the Target Segmentation Module (TSM) and Data Expansion Module (DEM) with a semi-supervised learning strategy; (ii) a progressive training strategy promoting parameter sharing between TSM and DEM; (iii) the introduction of Feature Fusion Attention Module (FFAM) and Dual-Stream Feature Attention Module (DSFAM) to improve key anatomical feature recognition; and (iv) the use of a specialized Fetal Brain CCSP dataset (FB-CCSP) with 200 annotated images for network training and validation. FB-SGASNet provides a practical solution for CCSP segmentation in few-shot datasets, enhancing the efficiency and accuracy of fetal brain ultrasound analysis, reducing reliance on specialized expertise, and enabling more timely prenatal evaluations.},
  archive      = {J_ASOC},
  author       = {Qifeng Wang and Dan Zhao and Hao Ma and Xiangjun Yang and Bin Liu},
  doi          = {10.1016/j.asoc.2025.112767},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112767},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrated generation adversarial and semi-supervised network for corpus callosum and cavum septum pellucidum complex segmentation in fetal brain ultrasound via progressive training},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing grid-tied solar energy systems with adaptive interval type-2 fuzzy tuned affine projection lorentzian control for improved power quality. <em>ASOC</em>, <em>171</em>, 112766. (<a href='https://doi.org/10.1016/j.asoc.2025.112766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing integration of distributed energy resources has significantly impacted power quality in modern grids. This article addresses the challenge by introducing an advanced control methodology for grid-tied solar energy systems, employing an interval type-2 fuzzy tuned affine projection Lorentzian technique to enhance the performance of three-phase voltage source converters, thereby achieving superior power quality. Key features of the proposed approach include harmonic suppression and load balancing, which are crucial for maintaining grid stability. The proposed technique effectively resolves the tradeoff between steady-state error and convergence rate, ensuring robust performance regardless of load distortions. The algorithm's robustness and stability are rigorously tested under varying conditions, such as fluctuating irradiance, balanced loads and unbalanced loads. Validation through real-time controllers and software-in-loop implementation using the dSPACE 1202 controller demonstrates the system's efficacy in both dynamic and steady-state scenarios. This research exemplifies the practical application of interval type-2 fuzzy to enhance the reliability and stability of grid-tied solar energy systems.},
  archive      = {J_ASOC},
  author       = {Jayant Sharma and C.K. Sundarabalan and N.S. Srinath and C. Balasundar},
  doi          = {10.1016/j.asoc.2025.112766},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112766},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing grid-tied solar energy systems with adaptive interval type-2 fuzzy tuned affine projection lorentzian control for improved power quality},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aspect-based sentiment analysis by knowledge and attention integrated graph convolutional network. <em>ASOC</em>, <em>171</em>, 112763. (<a href='https://doi.org/10.1016/j.asoc.2025.112763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-Based Sentiment Analysis (ABSA) aims to determine the sentiment polarity associated with specific aspects within a sentence. Recent advancements have demonstrated that the application of Graph Convolutional Networks (GCNs) to syntactic graphs yields substantial improvements in performance. However, challenges remain, including inconsistent word segmentation between Pre-trained Language Models (PLMs) and dependency parsers, heterogeneous embeddings for knowledge semantics, and the neglect of varying syntactic and semantic relation strengths. This paper proposes the Knowledge and Attention integrated GCN model (KA-GCN), which addresses the aforementioned issues. In particular, we propose a syntactic graph expansion algorithm to align word segmentation with PLM tokenization, thus resolving the aforementioned segmentation inconsistencies. To address the issue of heterogeneous embedding, we incorporate SentiWordNet knowledge into sentences to generate sentence-knowledge trees, and develop Knowledge-enabled RoBERTa (K-RoBERTa) for consistent encoding representations. Furthermore, we propose a position-aware attention mechanism that calculates attention scores, reflecting the strengths of syntactic and semantic relations. This mechanism also facilitates the generation of aspect-specific sentiment representations. These innovations enable KA-GCN to effectively integrate sentiment and syntactic knowledge, thereby providing an advanced approach to ABSA. Evaluation on public datasets demonstrates that KA-GCN achieves the highest performance in ABSA, with ablation studies confirming the effectiveness of each proposed technique.},
  archive      = {J_ASOC},
  author       = {Bingtao Wan and Peng Wu and Pu Han and Gang Li},
  doi          = {10.1016/j.asoc.2025.112763},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112763},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Aspect-based sentiment analysis by knowledge and attention integrated graph convolutional network},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Country-level assessment of COVID-19 performance: A cluster-based MACONT-CRITIC analysis. <em>ASOC</em>, <em>171</em>, 112762. (<a href='https://doi.org/10.1016/j.asoc.2025.112762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19, a highly contagious respiratory virus, emerged in Wuhan in December 2019, leading to a global health emergency and subsequent pandemic declaration. Despite preventive measures, millions have been diagnosed and millions more have lost their lives, highlighting the urgent need for efficient diagnostics and effective interventions. This study presents a comprehensive framework based on integrated machine learning-decision making (ML-MCDM) to assess and compare the performance of countries during the COVID-19. The aim is to evaluate the performance of countries and identify the effective strategies for controlling the pandemic. The framework introduces a new criterion entitled ‘Resilience’ which aims to assess a country’s capability to address peak diseases by identifying the occurrence of peaks and calculating the duration between the peak and the return to a normal state. Then, it employs K-Means clustering to group countries based on their performance indicators. The countries are then ranked within each cluster using the CRITIC-MACONT framework. The present study introduces a novel approach by integrating MACONT and CRITIC methodologies, marking the first instance of such integration. Additionally, the incorporation of machine learning techniques enhances their proficiency in effectively ranking the alternatives. The results of the analysis, conducted until March 2023, using the COVID-19 dataset, demonstrate that four clusters effectively evaluate the performance of countries and, the ‘Resilience’ criterion emerges as the most significant among the evaluated criteria. Based on the results, the proposed framework effectively ranks the countries and provides valuable insights for pandemic control strategies.},
  archive      = {J_ASOC},
  author       = {Amirreza Salehi Amiri and Ardavan Babaei and Majid Khedmati},
  doi          = {10.1016/j.asoc.2025.112762},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112762},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Country-level assessment of COVID-19 performance: A cluster-based MACONT-CRITIC analysis},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-based k-nearest neighbors algorithm for electromechanical impedance based detection of damage in adhesive joints. <em>ASOC</em>, <em>171</em>, 112761. (<a href='https://doi.org/10.1016/j.asoc.2025.112761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural adhesive joints are susceptible to many forms of damage that may not be detectable with currently deployed Non-Destructive Tests (NDT). The Electromechanical Impedance Spectroscopy (EMIS) based Structural Health Monitoring (SHM) methodology constitutes an alternative, where a given structure is continuously monitored for damage, thus outperforming NDT. While much has been done in the applicability of EMIS in metallic and composite structures, only preliminary research has been done on the EMIS-based integrity monitoring of adhesive joints. In this paper, two different k -Nearest Neighbor ( k NN) approaches are used for both damage detection and damage quantification. Initially, peaks are extracted from the real component of the measured impedance of instrumented adhesive joint specimina, which are then inputted to either a conventional k NN or a novel parallel k NN model, where each individual model is fed with its respective peak information. For void detection, the parallel classifier approach presents a moderately better performance, with an average accuracy of 98% under optimal conditions, but, for damage quantification, a significant improvement in classification is observed. In all cases, the use of the Canberra distance allows for a significant increase in classification accuracy.},
  archive      = {J_ASOC},
  author       = {A. Francisco G. Tenreiro and António M. Lopes and Lucas F.M. da Silva},
  doi          = {10.1016/j.asoc.2025.112761},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112761},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Frequency-based k-nearest neighbors algorithm for electromechanical impedance based detection of damage in adhesive joints},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing critical success factors for green supplier selection: A combined DEMATEL-ISM approach and convolutional neural network based consensus model. <em>ASOC</em>, <em>171</em>, 112760. (<a href='https://doi.org/10.1016/j.asoc.2025.112760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an indispensable component of supply chain operations management and practices, green supplier selection (GSS) problem has attracted extensive attentions with the increasing consciousness of environmental sustainability and its long-term influence on business and production issues. Analyzing and identifying critical success factors (CSFs) involving the success of GSS process, and evaluating the cause-effect and hierarchal relationship among those factors are known to be useful for achieving sustainable goals of enterprise and organization in this context. A better understanding of these driving criteria and the causality among them may accelerate the adoption process as well as optimizing selection results. The existing methods on addressing CSFs have a gap among classical hypothesis of decision-making process and knowledge-oriented problem structuring. Hence, we extend a new hybrid technique integrating decision-making trial and evaluation laboratory, interpretive structural modeling and convolutional neural network based consensus model for decision-makers. Here the new technique contributes to the cumulative efforts for assessment of green suppliers especially by providing a practical tool that can reconcile the proven issues in GSS practices. Empirical results revealed that the three most prominent CSFs are green technology capability, product quality management and environmental management system respectively. Besides, hierarchal digraph shown that all the CSFs are interacted with each other in different manners, understanding the hierarchical structure of internal relationship among those CSFs will assist managers make decision conveniently and effectively. The new insights and implications proposed in this study may potentially promote the efficiency and effectiveness of GSS practices, corporate reputation and image, and establish positive relationships with regulators.},
  archive      = {J_ASOC},
  author       = {Chuanjin Zhu and Nan Zhu and Songyin Zheng and Limin Zou and Xia Wang},
  doi          = {10.1016/j.asoc.2025.112760},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112760},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analyzing critical success factors for green supplier selection: A combined DEMATEL-ISM approach and convolutional neural network based consensus model},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven predictive maintenance using an enhanced TOPSIS approach for complex fuzzy information with Z-numbers. <em>ASOC</em>, <em>171</em>, 112759. (<a href='https://doi.org/10.1016/j.asoc.2025.112759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce complex fuzzy Z-numbers (CFZNs) and pioneered a novel extension of traditional fuzzy set theory by combining complex fuzzy sets and Z-numbers. We systematically explore the fundamental operations of CFZNs, such as union, complement, intersection, subset, and equality, as well as their operational rules. The averaging prioritized aggregation operators (PAgO), weighted averaging PAgO, geometric PAgO, and weighted geometric PAgO are developed. Theorems and properties, like monotonicity, idempotency, and boundedness of these AgOs, are discussed. The novel distance measures are defined to play a crucial role in modeling, analyzing, and solving complex problems in diverse fields. Moreover, we present an innovative multi-criteria decision-making algorithm founded on CFZNs, harnessing their enhanced uncertainty representation capabilities with reliability. Subsequently, we present a case study on utilizing artificial intelligence (AI). The study highlights the significance of AI in the field of predictive maintenance, demonstrating its capacity to improve the dependability of equipment, reduce downtime, and optimize maintenance practices in industrial environments. Furthermore, the extended CFZN-TOPSIS methodology is proposed under the developed framework. Finally, we conduct a comparison analysis and conclude the whole study.},
  archive      = {J_ASOC},
  author       = {Zainab Saif and Shahzaib Ashraf and Muhammad Shazib Hameed and Muneeba Kousar and Vladimir Simic and Nezir Aydin},
  doi          = {10.1016/j.asoc.2025.112759},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112759},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AI-driven predictive maintenance using an enhanced TOPSIS approach for complex fuzzy information with Z-numbers},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new ranking aggregation model based on S3WD and PSO for hybrid multi-criteria decision making. <em>ASOC</em>, <em>171</em>, 112758. (<a href='https://doi.org/10.1016/j.asoc.2025.112758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-criteria decision making (MCDM) is an effective approach for tackling real-world decision problems. However, applying different MCDM methods to the same problem may lead to diverse and conflicting results. Integrating the results of multiple MCDM methods, known as hybrid MCDM, is a powerful way to resolve these inconsistencies. This paper proposes a novel approach that combines sequential three-way decision(S3WD), ranking aggregation, and Particle Swarm Optimization(PSO) to address the hybrid MCDM problems. Specifically, this paper employs an improved criteria importance through inter-criteria correlation(CRITIC) to objectively determine the weights of various MCDM methods. Next, a divergence index is introduced to measure the stability of the ranking positions of alternatives. The sequential three-way decision is then utilized to hierarchically allocate alternatives into different decision regions according to the divergence index and decision thresholds. The distance matrix and minimum distance matrix are later defined to transform partial rankings into full rankings. Finally, a new ranking aggregation model based on Particle Swarm Optimization is proposed to solve hybrid MCDM problems. The advantages of the newly proposed model are validated through several evaluation parameters. Experimental results demonstrate that the new model significantly outperforms traditional ranking aggregation models.},
  archive      = {J_ASOC},
  author       = {Jin Qian and Di Wang and Ying Yu and XiBei Yang and Shang Gao},
  doi          = {10.1016/j.asoc.2025.112758},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112758},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new ranking aggregation model based on S3WD and PSO for hybrid multi-criteria decision making},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-way multi-label classification: A review, a framework, and new challenges. <em>ASOC</em>, <em>171</em>, 112757. (<a href='https://doi.org/10.1016/j.asoc.2025.112757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-label classification task is more challenging than the degenerated case of single-label classification due to diversified uncertainty. Uncertainty in multi-label classification not only involves label dependency but also the inconsistency of label correlation and imbalanced label association. While three-way decision methods excel in characterizing multifaceted uncertainty, developing a well-established three-way decision-based framework for multi-label classification remains challenging. Based on the historical developments of decision-theoretic rough sets and sequential three-way decision, this paper presents a systematic review of representative three-way multi-label classification models. By analyzing the contributions from existing studies, the multifaceted uncertainty of multi-label classification is classified into label uncertainty, correlation uncertainty, and structure uncertainty. To effectively deal with the structure uncertainty, some essential subproblems are identified, and a general three-way-based framework called the multi-label sequential decision-theoretic three-way decision (ML-SD3WD) model is presented. The ML-SD3WD model sequentially handles three-way topic generation, three-way label assignment, and three-way label enhancement by integrating decision-theoretic rough set with sequential three-way decision. Furthermore, some emerging directions in customizing the ML-SD3WD model are delineated. Finally, limitations from both the label side and feature side are discussed and corresponding solutions are offered for uncertainty-driven solutions in practical applications. The results of this review will offer a road map for knowledge discovery in multi-label classification.},
  archive      = {J_ASOC},
  author       = {Yuanjian Zhang and Tianna Zhao and Duoqian Miao and Yiyu Yao},
  doi          = {10.1016/j.asoc.2025.112757},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112757},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way multi-label classification: A review, a framework, and new challenges},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NADG-GAM: Neighbor aggregation-based neurological disease–gene identification via optimal generative adjacency matrix. <em>ASOC</em>, <em>171</em>, 112756. (<a href='https://doi.org/10.1016/j.asoc.2025.112756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of disease-related genes is crucial for advancing early diagnosis, treatment, disease prevention, and precision medicine. The high dimensionality of bioinformatics data and the lack of effective entity and connection extraction techniques frequently result in inadequate predictions in disease–gene studies. To address these challenges, this study proposes a neighbor aggregation-based neurological disease–gene identification method, Neighbor Aggregation via an Optimal Generative Adjacency Matrix (NADG-GAM). The proposed approach incorporates a flexible feature dimensionality reduction strategy tailored to diverse feature matrices across varying scenarios. The NADG-GAM algorithm leverages an optimal generative adjacency matrix strategy that employs a self-adaptive mechanism to construct affinity matrices optimized for distinct bioinformatics networks. The proposed algorithm enables low-dimensional node representation learning by combining dimensionality-reduced feature matrices with optimum generative adjacency matrices to aggregate neighbor information. Comprehensive experiments were performed on neurological disease datasets, including comparison analyses, ablation tests, and parameter sensitivity assessments, utilizing six standard evaluation criteria. The results indicate higher performance of our model, including accuracy, scalability, adaptability, etc., across various parameters compared to baseline methods. Specifically, it achieved improvements of at least 1.2%, 7.4%, 7%, 6.7%, and 1.3% in AP, Precision, Recall, F1-Score, and AUC. The candidate gene prediction results of NADG-GAM are significant for understanding disease formation mechanisms and experimental verification, and it is promising for diagnosing and treating neurological diseases. Meanwhile, the framework that this algorithm offers functions as an effective idea for broader disease-related studies.},
  archive      = {J_ASOC},
  author       = {Mengyuan Jin and Ziyi Deng and Yin Zhang and Jia Liu and Fang Hu},
  doi          = {10.1016/j.asoc.2025.112756},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112756},
  shortjournal = {Appl. Soft. Comput.},
  title        = {NADG-GAM: Neighbor aggregation-based neurological disease–gene identification via optimal generative adjacency matrix},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hard example learning based on neural collapse for class-imbalanced semantic segmentation. <em>ASOC</em>, <em>171</em>, 112755. (<a href='https://doi.org/10.1016/j.asoc.2025.112755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world datasets exhibit significant category imbalance, where certain categories contain only a small number of samples, while others have a disproportionately large number. In semantic segmentation, this imbalance is further exacerbated by the varying total number of pixels across categories. Neural networks often tend to overfit categories with abundant samples, while severely underrepresenting those with fewer samples, thereby compromising the generalization ability of model. To address this challenge, we propose a fully supervised semantic segmentation approach based on hard example learning and neural collapse. This method leverages a simplex equiangular tight frame classifier to guide the update gradient of the model and introduces a weighted hard sample loss function to effectively identify and utilize hard samples with latent value. Experimental results demonstrate that the proposed method could promote the model to reach the optimal value of neural collapse, and realize the high-precision semantic segmentation on class-imbalanced datasets.},
  archive      = {J_ASOC},
  author       = {Lu Xie and Weigang Li and Yuntao Zhao},
  doi          = {10.1016/j.asoc.2025.112755},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112755},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hard example learning based on neural collapse for class-imbalanced semantic segmentation},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing dementia prediction models: Leveraging temporal patterns and class-balancing methods. <em>ASOC</em>, <em>171</em>, 112754. (<a href='https://doi.org/10.1016/j.asoc.2025.112754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting dementia with machine learning (classification) models learned from longitudinal data remains challenging. This paper introduces an innovative approach for learning predictive dementia models that leverage temporal patterns derived from longitudinal data. Specifically, we propose two types of automatically constructed temporal features based on monotonicity patterns of features’ values and decision tree-based patterns. The constructed temporal features were added to the original dataset to improve the predictive performance of well-known classifiers, XGBoost and Random Forest. We also investigated using several types of class-balancing methods to cope with the large degree of imbalanced classes in our dataset. We assessed the impact of the constructed temporal features and different types of class-balancing methods (and their combinations) on improving classifiers’ predictive performance on a dementia dataset derived from the English Longitudinal Study of Ageing. We also report the most important predictive features in the best dementia prediction models learned in our experiments.},
  archive      = {J_ASOC},
  author       = {Flavio Luiz Seixas and Elaine Rangel Seixas and Alex A. Freitas},
  doi          = {10.1016/j.asoc.2025.112754},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112754},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing dementia prediction models: Leveraging temporal patterns and class-balancing methods},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A local minima escape procedure to improve the convergence of differential evolution. <em>ASOC</em>, <em>171</em>, 112753. (<a href='https://doi.org/10.1016/j.asoc.2025.112753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an original algorithm that significantly improves the convergence of differential evolution (DE) regardless of the strategy used to calculate the mutant vectors of the population. It is well-known that the DE routine requires special tuning, depending on the optimization problem, to increase the convergence rate, but this does not always lead to the desired result, and DE can get stuck in a local minimum (LM). To address this problem, we develop a local minima escape procedure (LMEP) that allows the algorithm to detect a current LM and escape from it. To achieve this, the current population undergoes a so-called ‘parameter shake-up’, and DE continues to run in the standard mode. LMEP has several settings, which need to be determined, and their effect on convergence is investigated. The performance of LMEP is first tested on the benchmark Rastrigin and Griewank function. Simulations show that LMEP improves the convergence of DE for all classical strategies. An illustrative example of the effectiveness of LMEP when applied to a realistic problem is the optimization of semiclassical quantum simulations of the linear optical response of photosynthetic pigment-protein complexes. As a result, we obtain an increase in convergence of between 25–30 % and 100 % compared to classical DE.},
  archive      = {J_ASOC},
  author       = {Denis D. Chesalin and Roman Y. Pishchalnikov},
  doi          = {10.1016/j.asoc.2025.112753},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112753},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A local minima escape procedure to improve the convergence of differential evolution},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolution-based visual transformer with dual space shifting attention hashing for image retrieval. <em>ASOC</em>, <em>171</em>, 112752. (<a href='https://doi.org/10.1016/j.asoc.2025.112752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current trend in hashing methods entails utilizing Convolutional Neural Networks (CNNs) to extract high-dimensional semantic information and transform it into binary hash codes using hash functions. Nevertheless, CNNs predominantly focus on extracting local information. Among the recently emerged hashing methods, Vision Transformer (ViT) can capture the interdependencies between different locations in an image via a self-attention mechanism and specifically concentrates on extracting global information. Based on this understanding, in this section, a lightweight Convolutional Visual Transformer-based Dual Space Shifting Attention Hashing (CTSAH) is designed for image retrieval. First, a Convolution-based Visual Transformer feature extraction framework named CVT is designed. By integrating convolution with visual transformation, the network can transfer local features and low-level information extracted through convolution to the ViT layer for global relationship modeling and high-level semantic understanding. This fusion approach is intended to enhance feature extraction and enable the model to better comprehend the image. Second, a Dual Space Shifting Attention (DSSA) module is added to the CVT. This module unfolds the feature map along different channel dimensions, divides the unfolded feature map into several parts, and then fuses the split parts using the split attention operation. Finally, a new classification loss is applied during optimization to minimize the intra-class distance of the hash codes. The proposed method achieves excellent performance on the CIFAR-10, NUS-WIDE, and MS-COCO datasets, with mean average precision up to 94.90%, 87.24%, and 89.78%, respectively, which is informative for practical applications in image retrieval.},
  archive      = {J_ASOC},
  author       = {Huan Ren and Shuli Cheng and Liejun Wang and Yongming Li},
  doi          = {10.1016/j.asoc.2025.112752},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112752},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Convolution-based visual transformer with dual space shifting attention hashing for image retrieval},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logistic hub location problem under fuzzy extended Z-numbers to consider the uncertainty and reliable group decision-making. <em>ASOC</em>, <em>171</em>, 112751. (<a href='https://doi.org/10.1016/j.asoc.2025.112751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges faced by global operations, top management must react urgently and rapidly. To stay competitive and advance the supply and logistics processes, a management system must be able to respond to the operational requirements and market. One of the mistakes that can be made when it comes to achieving successful operations and supply is the ignorance of the risk and uncertainty that is an inevitable part of leading logistic operations. Sometimes, it is essential to expand the infrastructure and the processes to commit stakeholders and meet customer expectations, in addition to the routine managerial responsibilities. This research aims to model a logistic hub location selection problem to aid logistic managers, enabling them to respond to the market as quickly as possible. We propose a new approach to extended Z-numbers (Z E -numbers) in multi-criteria decision analysis to uncover the reliability and uncertainty in the decision problem. In the developed group decision-making model, we consider the experts' votes and the decision-makers' opinions, assisting logistics managers in locating the most appropriate location for establishing a logistics hub. The Z E -numbers are integrated with the Multi-Attribute Border Approximation Area Comparison (MABAC) and Best-Worst Method (BWM). The outcomes of this study show that cost and infrastructure are the most important indexes for choosing the logistic hub locations for this research. Also, security is determined as the least important index. The findings of this research show that Santander is the top location, and Almeria and Murcia have been deemed less crucial to improving the hub location problem.},
  archive      = {J_ASOC},
  author       = {Gholamreza Haseli and Morteza Yazdani and Mayssam Tarighi Shaayesteh and Mostafa Hajiaghaei-Keshteli},
  doi          = {10.1016/j.asoc.2025.112751},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112751},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Logistic hub location problem under fuzzy extended Z-numbers to consider the uncertainty and reliable group decision-making},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter-free dynamic fidelity selection for automated machine learning. <em>ASOC</em>, <em>171</em>, 112750. (<a href='https://doi.org/10.1016/j.asoc.2025.112750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameter optimization (HPO) and neural architecture search (NAS) have developed dramatically with the evolution of deep neural networks (DNNs). HPO and NAS require DNN training when evaluating a set of hyperparameters and a network architecture, respectively. Therefore, these tasks impose expensive computational time because the objective function is repeatedly evaluated during the search process. To expedite this process, multi-fidelity optimization algorithms have been developed for efficient optimization by exploiting a cheap-to-evaluate low-fidelity objective function. Furthermore, population-based algorithms, such as genetic algorithms and evolution strategies, that leverage parallel processing are also employed to hasten optimization. In this paper, we propose a new algorithm, referred to as parameter-free dynamic fidelity selection (PF-DFS), for efficiently performing HPO and NAS when using the ranking-based evolutionary algorithms. We present an evaluation of the effectiveness of PF-DFS with two prominent ranking-based evolutionary algorithms on 38 multi-fidelity optimization problems of HPO and NAS. Our experimental results demonstrate that PF-DFS accelerates the search speed by 2.5%–24.9% while maintaining the quality of the obtained solutions, as compared to the optimizers without PF-DFS. Furthermore, we demonstrate that CMA-ES with PF-DFS outperforms Hyperband and DEHB (combining differential evolution with Hyperband), widely-used/state-of-the-art multi-fidelity optimization algorithms, in HPO for classification and segmentation tasks of real-world scenarios.},
  archive      = {J_ASOC},
  author       = {Shintaro Takenaga and Yoshihiko Ozaki and Masaki Onishi},
  doi          = {10.1016/j.asoc.2025.112750},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112750},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parameter-free dynamic fidelity selection for automated machine learning},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Own-background contrastive learning guided by pseudo-label for semi-supervised medical image segmentation. <em>ASOC</em>, <em>171</em>, 112749. (<a href='https://doi.org/10.1016/j.asoc.2025.112749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although recent works in fully supervised learning has achieved significant success in medical image segmentation, obtaining high-quality pixel-wise expert annotations remains a challenge in medical imaging. Therefore, semi-supervised learning (SemiSL) has recently attract increasing attention to medical image segmentation. Contrastive learning (CL) framework has demonstrated strong inter-class separability in the field of image segmentation. However, its application in medical imaging is impacted by the long-tailed distribution of data, leading to imbalanced learning. To this end, We propose a novel Own-background Contrastive Learning (OBCL) framework for semi-supervised medical segmentation. Unlike other CL frameworks, OBCL can effectively incorporate background pixels into CL and avoid being constrained by the long-tailed distribution of the data. We leverage a student–teacher model to generate pseudo-labels, guiding the creation of foreground–background feature pairs, then use of discriminative class information learned in CL to produce accurate multi-class segmentation. Specifically, we decompose a multi-class segmentation task into multiple binary segmentation tasks, each focusing on segmenting a specific foreground class and the background in the images. Additionally, we integrate a small fraction of foreground information into the background features to improve inter-class separability, called Own-background. We achieve significant improvements on both cardiac MRI and colonoscopy polyp segmentation tasks compared with state-of-the-art methods. Even with only a 5% labeling rate for model training, we achieved 88.30% Dice on the ACDC dataset.},
  archive      = {J_ASOC},
  author       = {Huijie Fan and Jinghan Cao and Xi’ai Chen and Sen Lin and Kemal Polat and Jingchun Zhou},
  doi          = {10.1016/j.asoc.2025.112749},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112749},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Own-background contrastive learning guided by pseudo-label for semi-supervised medical image segmentation},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of index futures movement using TimeGAN and 3D-CNN: Empirical evidence from korea and the united states. <em>ASOC</em>, <em>171</em>, 112748. (<a href='https://doi.org/10.1016/j.asoc.2025.112748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past several years, a growing number of researchers and investors have displayed a keen interest in predicting stock price movements, actively conducting research that leverages cutting-edge deep learning models to capture the complexities of the stock market. In contrast, research on predicting price movements in the futures market, which is essential for risk management and forecasting the future value of underlying assets, is nearly absent. In this study, we propose a robust and efficient algorithmic trading model designed for the futures market, which exhibits significant time-series volatility, similar to the stock market. We assess the model’s performance in both the Korean and U.S. markets. Using time-series generative adversarial networks (TimeGAN), we augment time-series data into three dimensions and employ a 3-dimensional convolutional neural network (3DCNN) to identify temporal correlations within a multidimensional spatial structure. We thoroughly assess the performance of the proposed model by subjecting it to rigorous backtesting within a simulated environment that closely emulates real-world conditions in the futures market. The results demonstrate improvements of approximately 1.35 times in risk-adjusted returns and up to 6390 times in cost efficiency over both short- and long-term timeframes. This demonstrates that the TimeGAN augmentation and 3D expansion method outperforms existing methodologies in terms of risk-adjusted returns and cost efficiency, effectively capturing the volatility of the futures market. The main objective of this research is to provide investors with a more proficient decision-making tool through algorithmic trading that seamlessly functions in the real-world futures market.},
  archive      = {J_ASOC},
  author       = {Woojung Kim and Jiyoung Jeon and Sanghoe Kim and Minwoo Jang and Heesoo Lee and Sanghyuk Yoo and Kyong Joo Oh},
  doi          = {10.1016/j.asoc.2025.112748},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112748},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction of index futures movement using TimeGAN and 3D-CNN: Empirical evidence from korea and the united states},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning frameworks in smart e-healthcare: A systematic literature review with bias evaluation. <em>ASOC</em>, <em>171</em>, 112747. (<a href='https://doi.org/10.1016/j.asoc.2025.112747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Federated learning has become the popular model training approach for healthcare applications. FL is essential in executing collaborative model training, which avoids gathering private data in a central location. This study has explored the FL in health informatics, summarized using statistical analysis and risk-of-bias assessment. This article aims to serve as a detailed resource for FL researchers, healthcare professionals, policymakers, and data scientists interested in understanding the current landscape of FL in healthcare. We adopted the PRISMA strategy to select 111 relevant studies using 4 databases, including Web of Science, Scopus, PubMed, and ScienceDirect. These studies have been analyzed based on 29 AI and FL attributes, including FL types, aggregation strategies, AI modalities, and performance metrics. Composite scores were calculated and ranked after clustering the studies. Furthermore, we have estimated a bias cut-off score to identify low and high-biased studies. We utilized the ranking bias score method to determine the bias cut-off for selected healthcare FL studies. Our results demonstrated the intersection of the “cumulative plot of the mean score and the frequency plot curve of the studies” which determined the cut-off of 0.46, separating the studies into high-biased and low-biased studies. Consequently, we estimated 19 studies with scores below 0.46, considered highly biased, while 29 were in the low-bias category. Conclusively, studies have shown that FL-based models outperform traditional pooled and local learning methods in case of accuracy, sensitivity, AUC, F1 score, and precision. Most FL studies in healthcare use CNN and MLP for local training, with FedAvg as the aggregating function. Further, we provided seven recommendations with recent challenges faced by healthcare FL to minimize biases in future FL healthcare studies and enhance research quality.},
  archive      = {J_ASOC},
  author       = {Soumyaranjan Panda and Rajni Dubey and Biswajit Jena and Vikas Pareek and Lung-Wen Tsai and Sanjay Saxena},
  doi          = {10.1016/j.asoc.2025.112747},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112747},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated learning frameworks in smart e-healthcare: A systematic literature review with bias evaluation},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards the generation of hierarchical attack models from cybersecurity vulnerabilities using language models. <em>ASOC</em>, <em>171</em>, 112745. (<a href='https://doi.org/10.1016/j.asoc.2025.112745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the use of pre-trained language models and siamese neural networks to discern sibling relationships between text-based cybersecurity vulnerability data. The ultimate purpose of the approach presented in this paper is towards the construction of hierarchical attack models based on a set of text descriptions characterising potential or observed vulnerabilities in a given system. Due to the nature of the data, and the uncertainty sensitive environment in which the problem is presented, a practically oriented soft computing approach is necessary. Therefore, a key focus of this work is to investigate practical questions surrounding the reliability of predicted links towards the construction of such models, to which end conceptual and practical challenges and solutions associated with the proposed approach are outlined, such as dataset complexity and stability of predictions. Accordingly, the contributions of this paper focus on training neural networks using a pre-trained language model for predicting sibling relationships between cybersecurity vulnerabilities, then outlining how to apply this predictive model towards the generation of hierarchical attack models. In addition, two data sampling mechanisms for tackling data complexity and a consensus mechanism for reducing the amount of false positive predictions are outlined. Each of these approaches is compared and contrasted using empirical results from three sets of cybersecurity data to determine their effectiveness.},
  archive      = {J_ASOC},
  author       = {Kacper Sowka and Vasile Palade and Xiaorui Jiang and Hesam Jadidbonab},
  doi          = {10.1016/j.asoc.2025.112745},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112745},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards the generation of hierarchical attack models from cybersecurity vulnerabilities using language models},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A differential evolution-based single-level algorithm for jointly optimizing the deployment and flight trajectory of UAV-assisted data collection system. <em>ASOC</em>, <em>171</em>, 112744. (<a href='https://doi.org/10.1016/j.asoc.2025.112744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an unmanned aerial vehicle (UAV)-assisted Internet of Things (IoT) data collection system, where a UAV collects data from IoT devices at various stop points and returns to its starting point. Our goal is to minimize energy consumption by jointly optimizing the UAV’s deployment and flight trajectory. This problem is complex and NP-hard. To address this, this paper proposes the Differential Evolution with Variable Population Size and Route (DEVIPSR) algorithm, a single-level model that improves upon traditional Differential Evolution (DE). Each individual in the population represents both the position and order of stop points in the UAV’s trajectory, allowing comprehensive optimization of deployment and flight planning. This paper also introduces a double replacement (DR) strategy and an initialization strategy to enhance convergence speed. The LKH algorithm is used to finalize the trajectory optimization. Experimental results show that DEVIPSR algorithm outperforms multi-level optimization models by reducing total energy consumption by approximately 18.26%.},
  archive      = {J_ASOC},
  author       = {Lianhai Lin and Liqin Tian and Zhigang Wang and Wenguang Yang},
  doi          = {10.1016/j.asoc.2025.112744},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112744},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A differential evolution-based single-level algorithm for jointly optimizing the deployment and flight trajectory of UAV-assisted data collection system},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models for newspaper sentiment analysis during COVID-19: The guardian. <em>ASOC</em>, <em>171</em>, 112743. (<a href='https://doi.org/10.1016/j.asoc.2025.112743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the COVID-19 pandemic, news media coverage encompassed topics such as viral transmission, allocation of medical resources, and government response measures. Studies have analysed sentiment on social media platforms during COVID-19 to understand the public’s response to the rising cases and the government strategies implemented to control the spread of the virus. Sentiment analysis can enable an understanding of the dynamics in the social and psychological well-being of a group during the pandemic. Apart from social media, newspapers have played a vital role in disseminating information, including information from the government, experts, and the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we selected The Guardian newspaper and conducted a sentiment analysis during various stages of COVID-19 including initial transmission, lockdowns and vaccination. We employed novel large language models (LLMs) and refined them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian, with a dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offered a more diverse emotional expression during the pandemic. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre- and during COVID-19 across news sections including Australia, UK, World News, and Opinion.},
  archive      = {J_ASOC},
  author       = {Rohitash Chandra and Baicheng Zhu and Qingying Fang and Eka Shinjikashvili},
  doi          = {10.1016/j.asoc.2025.112743},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112743},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Large language models for newspaper sentiment analysis during COVID-19: The guardian},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A genetic algorithm-based methodology for the structural optimization of voronoi flat roofs. <em>ASOC</em>, <em>171</em>, 112742. (<a href='https://doi.org/10.1016/j.asoc.2025.112742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voronoi tessellations are a mathematical concept that appears in many examples in nature, such as the skin of giraffes, dry soil, and vegetable cells. In the context of biomimicry, these tessellations have been used to build impressive structures worldwide that are both aesthetically pleasing and structurally efficient. This paper proposes a methodology based on genetic algorithms (GA) to determine the structural topology of Voronoi flat roofs with tubular steel cross sections and a given boundary. The design variables correspond to the number and position of the Voronoi centers that form the tessellations within the roof, as well as the dimensions of the structural elements. This representation of the design variables creates an unstructured optimization problem. Such characteristic is addressed by an implicit redundant representation of possible solutions, which generates chromosomes with varying numbers of variables. The objective function relates to the weight of the roof, considering constraints raised in technical and constructive issues. The methodology was applied to four different roof boundaries: triangular, pentagonal, square, and rhombic. In general, the results provide optimal aesthetic solutions with a few Voronoi tessellations, based on the algorithm configuration and the multimodal nature of the search space. Convergence analysis indicates the possibility of the algorithm getting stuck in an optimum local and shows the progressive reduction of Voronoi centers. Lastly, it is observed that the maximum displacement constraint leads to the shape of the optimal roof.},
  archive      = {J_ASOC},
  author       = {Juan Sebastián Fontalvo García and María Juliana Moya Olivares and Alfonso Gomez and Jesús Daniel Villalba Morales},
  doi          = {10.1016/j.asoc.2025.112742},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112742},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A genetic algorithm-based methodology for the structural optimization of voronoi flat roofs},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bi-objective evolutionary algorithm for distributed production scheduling with eligibility restrictions. <em>ASOC</em>, <em>171</em>, 112738. (<a href='https://doi.org/10.1016/j.asoc.2025.112738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distributed manufacturing model has emerged as a highly effective strategy in the manufacturing sector, esteemed for its prowess in cutting enterprise operating costs and elevating market responsiveness. This paper focuses on a new energy vehicle battery manufacturer and provides a comprehensive exploration of the distributed production scheduling process. Particular emphasis is placed on factory eligibility restrictions and the inherent fuzziness of transportation durations. Our objectives are twofold: to minimize total costs, encompassing both production and transportation expenses, while simultaneously maximizing overall customer satisfaction. Given the stringent eligibility criteria, orders are strictly confined to designated factories. Customer satisfaction, a vital metric, is significantly influenced by the alignment between the fuzzy arrival time and the due time of an order. This alignment is often shaped by consumer sensitivity and transportation duration's inherent uncertainties. To tackle this intricate challenge, we propose a bi-objective approach that integrates the genetic algorithm and variable neighborhood search (GA-VNS). Experimental results reveal that in over two-thirds of the test instances, the ratio of non-dominated solutions ( Rnd ) of the GA-VNS algorithm exceeds 80 %, and in all instances, its average distance ( AD ) to the reference set is less than 0.03, demonstrating significant advantages over NSGA-II, MOABC, and MOWOA algorithms.},
  archive      = {J_ASOC},
  author       = {Chaoming Hu and Teng Zhang and Yao Gao and Xinbao Liu and Xubiao Wang},
  doi          = {10.1016/j.asoc.2025.112738},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112738},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bi-objective evolutionary algorithm for distributed production scheduling with eligibility restrictions},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-segmentation-based approach for early detection and type recognition of single-phase ground fault in resonant distribution networks. <em>ASOC</em>, <em>171</em>, 112736. (<a href='https://doi.org/10.1016/j.asoc.2025.112736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex single-phase ground fault (SPGF) is a challenging problem for early detection and type recognition in resonant distribution networks. This paper proposes a novel semantic-segmentation-based approach that leverages the morphological information of zero-sequence voltage signals to extract diverse semantic features representing fault inception (FI), fault disappearance (FD), and short-term transient fault (STF). A 1D-UNet model is employed to classify each sample point into one of these categories, which enables the determination of the moment and duration of SPGF. Based on these features, three types of SPGF are recognized: permanent fault (PF), long-term transient fault(LTF), and short-term transient fault (STF). Due to its low power consumption and cost-effectiveness, an industrial prototype integrated with the proposed approach has been developed using a Raspberry Pi board. The proposed approach achieves an overall accuracy of over 94 % in classifying sample points across diverse categories. Specifically, the individual accuracies for detecting sample points belonging to FI, FD, and STF were 0.978, 0.968, and 0.971, respectively. From an engineering application perspective, the proposed approach effectively identifies the moment of fault occurrence, whether it is PF, LTF, or STF. The maximum, minimum, and median triggering deviations were 10.8 ms, −6.4 ms, and −0.4 ms, respectively, significantly outperforming existing methods in terms of fault moment triggering deviation. The experimental results demonstrate that the proposed approach works effectively for early detection and type recognition of SPGF, showcasing significant potential for further expansion and broader application.},
  archive      = {J_ASOC},
  author       = {Jian-Hong Gao and Mou-Fa Guo and Shuyue Lin and Duan-Yu Chen and Hao Bai},
  doi          = {10.1016/j.asoc.2025.112736},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112736},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semantic-segmentation-based approach for early detection and type recognition of single-phase ground fault in resonant distribution networks},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multidimensional assessment of the european energy union: Integrating artificial intelligence and quantum fuzzy ranking approaches. <em>ASOC</em>, <em>171</em>, 112735. (<a href='https://doi.org/10.1016/j.asoc.2025.112735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this study is to determine the significant indicators of European Energy Union policy. A novel artificial intelligence (AI)-enhanced and hybrid quantum fuzzy ranking approach are taken into consideration in this regard. There are three different stages in this hybrid model. The first stage includes the weighting the dimensions of the European energy union with AI-enhanced prioritization. The second stage consists of measuring the multidimensional performance for European energy union with artificial intelligence-enhanced ranking. In the final stage, the multidimensional performance for European energy union is evaluated by using quantum picture fuzzy rough sets based VIKOR methodology. The main contribution of this study is making a comparative evaluation with both artificial intelligence approach and fuzzy decision-making methodology. This situation provides an opportunity check the validity of the findings. In the analysis process, a novel artificial intelligence-based ranking methodology is proposed. Therefore, more effective evaluations can be conducted. The findings indicate that energy security, market functioning, and innovation have the best priorities. In case of maximum group utility, financial feasibility and technological infrastructure have the best ranking performance among the perspectives. The results with the highest maximum group utility are same with the priority results of the AI-enhanced ranking technique.},
  archive      = {J_ASOC},
  author       = {Peide Liu and Hasan Dinçer and Serhat Yüksel},
  doi          = {10.1016/j.asoc.2025.112735},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112735},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multidimensional assessment of the european energy union: Integrating artificial intelligence and quantum fuzzy ranking approaches},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective optimization algorithms for freight allocation in a food grain supply chain. <em>ASOC</em>, <em>171</em>, 112729. (<a href='https://doi.org/10.1016/j.asoc.2025.112729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses multi-objective optimization of a freight allocation problem and presents the case of a food grain organization in India (FOI). The inventory and warehouse parameters that are relevant in the regional level allocation of food grains (using freight trains) are represented using three penalty factors, namely rake penalty factor, capacity utilization penalty factor, and weekly penalty factor. The article formulates a tri-objective optimization model to minimize each of the three penalty factors. Two customized multi-objective optimization algorithms are developed based on Multi-Objective Simulated Annealing (MOSA) and Elitist Non-dominated Sorting Genetic Algorithm II (NSGA II) to solve the formulated model. The algorithms are tested and validated via computational experiments designed using historical data collected from the FOI. The algorithms help the transportation managers at the FOI to generate improved and balanced transportation plans (with respect to the three objectives) in a quick time. Further, the performance of the algorithms is compared based on seven different performance metrics reported in literature. The MOSA-based algorithm performs equally or better than the NSGA II-based algorithm with respect to four performance metrics.},
  archive      = {J_ASOC},
  author       = {Anoop K. P. and Vinay V. Panicker and Jerin Siby and Aryadutt C. S.},
  doi          = {10.1016/j.asoc.2025.112729},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112729},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective optimization algorithms for freight allocation in a food grain supply chain},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing memory-efficient multimodal networks for image classification using differential evolution. <em>ASOC</em>, <em>171</em>, 112714. (<a href='https://doi.org/10.1016/j.asoc.2025.112714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach to developing tuned model architectures and hyperparameter sets for multimodal classification, leveraging Differential Evolution to optimize for memory-efficient design (MEMODE). The evolved model combines image channel data with vectorized word matrices generated through trained Word2Vec embeddings, as a resourceful convolutional processing of text and image inputs. Additionally, to enrich the semantic representations drawn from text-based inputs, a pretrained ALBERT module is included to process a parallel input stream. The custom Differential Evolution algorithm employed utilises a penalized objective function that directly accounts for model size and Mean F1 performance. We focus on the critical task of hyperparameter optimization and neural architecture search, while accounting for a set design limit of 100 MB. Significantly, the network evolved through the MEMODE algorithm achieved a validation mean F1 score of 0.8621, outperforming over 1600 competing architectures, including manually constructed CNNs, AlexNet, ResNet variations, Inception modules, and models generated by alternative algorithms such as Bayesian Optimization, Particle Swarm Optimization and the Genetic Algorithm. These results emphasize the capacity of Differential Evolution for autonomous network design and hyperparameter tuning when training memory-efficient models.},
  archive      = {J_ASOC},
  author       = {T. Hielscher and S.A. Hadigheh},
  doi          = {10.1016/j.asoc.2025.112714},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112714},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing memory-efficient multimodal networks for image classification using differential evolution},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge enhanced data analytics based on three-way decision and granular computing. <em>ASOC</em>, <em>171</em>, 112712. (<a href='https://doi.org/10.1016/j.asoc.2025.112712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Weiping Ding and Jose Carlos R. Alcantud and Jianming Zhan and Oscar Castillo and Yiyu Yao},
  doi          = {10.1016/j.asoc.2025.112712},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112712},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge enhanced data analytics based on three-way decision and granular computing},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-driven dynamic multi-objective evaluation and attention optimization for tuning microwave filters. <em>ASOC</em>, <em>171</em>, 112702. (<a href='https://doi.org/10.1016/j.asoc.2025.112702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the increasing demand for microwave filters (MFs) in communication systems, optimization-based automatic tuning technique has emerged to accelerate the production process of MFs. However, two issues limit the efficiency of optimization: (1) The static multi-objective evaluation method fails to account for shifting priorities of objectives under different states of MFs, and (2) the internally improved optimization algorithms still trap in redundant exploration in face of external high-dimensional space. Motivated by the manual tuning modes, this paper introduces two kinds of knowledge to solve the above challenges, and proposes a knowledge-driven dynamic multi-objective evaluation and attention optimization approach. The contributions are as follows: First, a dynamic multi-objective evaluation method is designed to focus on different objectives under various tuning states. Second, the local attention space with great potential to contain feasible solutions is identified and used as the optimization space, rather than the entire space. Third, a self-balancing particle swarm optimization method is devised to maintain both exploration and exploitation capabilities within the attention space, efficiently searching for targets. Simulation and experiment results demonstrate that the proposed approach is effective, which contains very high stability for tuning and reduces almost 50% optimization iteration number compared to other approaches.},
  archive      = {J_ASOC},
  author       = {Linwei Guo and Weihua Cao and Leyu Bi and Wenkai Hu and Min Wu},
  doi          = {10.1016/j.asoc.2025.112702},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112702},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge-driven dynamic multi-objective evaluation and attention optimization for tuning microwave filters},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SE-ResNeXt-50-CNN: A deep learning model for lung cancer classification. <em>ASOC</em>, <em>171</em>, 112696. (<a href='https://doi.org/10.1016/j.asoc.2025.112696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing a robust and accurate lung cancer (LCa) classification model is essential in medical diagnostics. With its high global prevalence and propensity for latent manifestation in the early stages, LCa emphasizes the critical need for effective detection and classification methods. LCa is one of the most prevalent and lethal forms of cancer globally, contributing significantly to cancer-related mortality. Its asymptomatic nature in its early stages frequently results in delayed diagnoses, coinciding with more advanced and less treatable disease phases. A successful classification model can close this diagnostic void by identifying complex abnormalities in medical images. This research concentrates on detecting and classifying LCa using deep learning (DL) techniques, particularly the SE-ResNeXt-50 architecture and Convolutional Neural Networks (CNNs). The proposed SE-ResNeXt-50-CNN model combines advanced feature extraction with robust classification, dynamic quadri-histogram equalization (QDHE)-based preprocessing, data augmentation, and hyperparameter optimization. Consistent performance across cross-validation folds demonstrates the robustness of the model. The model achieved an accuracy of 99.15 %, sensitivity of 97.58 %, precision of 99.51 %, specificity of 99.80 %, and f1-score of 98.54 %. The efficiency of the SE-ResNeXt-50-CNN model surpasses that of many existing models. This performance demonstrates the model's potential to improve the accuracy of LCa classification in medical imaging.},
  archive      = {J_ASOC},
  author       = {A. Priya and P. Shyamala Bharathi},
  doi          = {10.1016/j.asoc.2025.112696},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112696},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SE-ResNeXt-50-CNN: A deep learning model for lung cancer classification},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital-twin-enabled online wrinkling monitoring of metal tube bending manufacturing: A multi-fidelity approach using forward-convolution-GAN. <em>ASOC</em>, <em>171</em>, 112684. (<a href='https://doi.org/10.1016/j.asoc.2024.112684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most vital metal bent-tube defects, wrinkling not only compromises the aesthetics of the finished product but also weakens the structural integrity of the tube, rendering it unsuitable for its intended application. Mainly, due to the complex constraints and interfacial behaviors between the dies and the tube, in-process monitoring and prediction of wrinkling occurring in the tube inner side is challenging, thereby limiting the cross-sectional control accuracy and efficiency. To break through this limitation, a digital-twin-enabled method is developed to achieve online monitoring and prediction of the wrinkling defect during tube bending. The strain sensors, serve as a bridge connecting the physical entity and the digital one, are employed inside the tube. Due to forming dies interference, the strain sensors can only be arranged at specific positions on the tube inner surface. As a countermeasure, an FC-GAN (Forward Convolution-Generative Adversarial Networks) based multi-fidelity method is proposed by utilizing combined data from measured results in the straight part and the simulation results in the bent part of the tube. Furthermore, a time adaptation module is incorporated to enhance the prediction efficiency, enabling online detection of wrinkling defects. The experimental verification is conducted using aluminum tube bending as a typical case. The verification demonstrates that the proposed method can provide superior prediction accuracy and efficiency as compared with other multi-fidelity fusion methods.},
  archive      = {J_ASOC},
  author       = {Zili Wang and Jie Li and Yujun Yuan and Shuyou Zhang and Weifei Hu and Jun Ma and Jianrong Tan},
  doi          = {10.1016/j.asoc.2024.112684},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112684},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Digital-twin-enabled online wrinkling monitoring of metal tube bending manufacturing: A multi-fidelity approach using forward-convolution-GAN},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage genetic algorithm-based robust scheduling approach for multi-factory production with uncertain shipping lead-time: Optimizing on-time delivery and cost. <em>ASOC</em>, <em>171</em>, 112670. (<a href='https://doi.org/10.1016/j.asoc.2024.112670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manufacturers face the dual challenge of balancing on-time delivery and operating cost, which is further complicated by uncertain shipping lead-time within multi-factory production network. Traditional production scheduling studies rarely consider the effects of this uncertainty from the manufacturer’s perspective, largely due to insufficient robust data. In response, we introduce a novel variant of the multi-factory production scheduling problem that integrates uncertain shipping lead-time using robust optimization (RO). To effectively address the extensive search space and exponential time complexity of this problem, we proposed an innovative two-stage genetic algorithm (TSGA) that incorporates two tailored rules: an initial population generation rule and a constraint-checking mechanism. Experiments on small and large-scale problem instances validated the TSGA's superiority over contemporary benchmarks—Gurobi and NSGA-II/MOEAD—in 5 key indicators related to Pareto frontiers’ quality and distribution. Monte-Carlo simulations validated our robust scheduling approach’s robustness, reducing the fluctuation range of delivery deviation days by an average of 78.7 % and 26.4 %, respectively, compared to deterministic and stochastic methods. Comparative assessment with two other RO methods with different uncertainty sets confirmed the proposed approach’s superior balance between robustness and conservativeness.},
  archive      = {J_ASOC},
  author       = {Yingying Chen and He Luo and Zhiming Cai and Bo Wang and Xiaonong Lu},
  doi          = {10.1016/j.asoc.2024.112670},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112670},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage genetic algorithm-based robust scheduling approach for multi-factory production with uncertain shipping lead-time: Optimizing on-time delivery and cost},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting and differentiating the DDoS and flash events using adaptive deep learning models. <em>ASOC</em>, <em>171</em>, 112669. (<a href='https://doi.org/10.1016/j.asoc.2024.112669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problems The Flash Event (FE) has been used to characterize the overloading condition, which is caused by multiple lawful requests and is the same as the characteristic of Distributed Denial of Service (DDoS). As a result, the DDoS assault has fully interrupted and degraded the operation of online services by propagating numerous packets in the shape of genuine requests to target web servers. However, a DDoS attack can make a website and service unavailable to reasonable users, causing lost income and injuring client relationships. Sites may remain online throughout a DoS assault. They may turn slow or insensitive, making it difficult or not possible for users to interrelate. Proposed solution The purpose of this paper is to detect and differentiate the DDoS attacks and FE architecture. The required data is gathered from online sources and then transformed. The relevant features are selected from this transformed data in two ways: through Principal Component Analysis (PCA)-based feature extraction, and optimal feature selection using the developed Wild Geese Migration and Hunger Games Search Optimization (WGMHGSO) by combining the Wild Geese Migration Optimization (GMO) and Hunger Games Search (HGS). For example, selected features are fused and sent to the detection and differentiation stage. Deep learning architectures such as Deep Convolutional Temporal Networks (DCTN) and Transformer Long Short-Term Memory (TRANS-LSTM) are used to detect and differentiate the DDoS and FEs. The parameters in the DCTN-TRANS-LSTM are optimized using the same WGMHGSO to improve the detection and differentiation performance of the developed model. Result The accuracy of the proposed WGMHGSO- DCTN-TRANS-LSTM model is 94.04 %, and other models such as CNN, RNN, DCTN, and TRANS-LSTM given the accuracy to be 90.4 %, 91.84 %, 91.96 %, and 92.2 %.},
  archive      = {J_ASOC},
  author       = {Naorem Nalini Devi and Khundrakpam Johnson Singh},
  doi          = {10.1016/j.asoc.2024.112669},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112669},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detecting and differentiating the DDoS and flash events using adaptive deep learning models},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSPI: Heterogeneous graph neural network classification aggregation algorithm based on size neighbor path identification. <em>ASOC</em>, <em>171</em>, 112656. (<a href='https://doi.org/10.1016/j.asoc.2024.112656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of existing heterogeneous graph neural network algorithms (HGNNs) rely on meta-paths to capture the substantial semantic information present in heterogeneous graphs (also known as heterogeneous information networks, or HINs). However, the majority of these HGNNs prioritize diverse approaches to feature aggregation, neglecting to consider the intrinsic characteristics of the meta-paths themselves. This paper studies meta-paths in four commonly used datasets and identifies significant discrepancies in the number of neighbors connected by different meta-paths. At the same time, the noise information contained in large neighbor paths will have an adverse impact on model performance. To avoid the impact of harmful noise, this paper proposes a Heterogeneous Graph Neural Network Classification and Aggregation Algorithm Based on Large and Small Neighbor Path Identification (LSPI). LSPI initially identifies the relative difference percentage between distinct meta-paths through the use of a Path Discriminator (PD), subsequently categorizing the meta-paths as either large neighbor paths or small neighbor paths in accordance with the results. To mitigate the impact of noise interference in large neighbor paths, LSPI employs a two-pronged approach. First, it calculates the transition probabilities between nodes within the large paths. Second, it selects more similar neighbor nodes from both topological and feature perspectives. This is done based on the transition probabilities and feature similarities. Subsequently, the small neighbor paths and the filtered large neighbor paths are aggregated using different graph convolution components, thereby obtaining feature information under disparate subgraphs. Finally, LSPI generates the final node embeddings by fusing the feature information from different subgraphs through subgraph-level attention. Extensive experiments have validated the superiority of LSPI, and compared to other models, LSPI shows significant performance improvements in the context of large neighbor paths. The complete reproducible code has been published at: https://github.com/liuhua811/LSPIA .},
  archive      = {J_ASOC},
  author       = {Yufei Zhao and Shiduo Wang and Hua Duan},
  doi          = {10.1016/j.asoc.2024.112656},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112656},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LSPI: Heterogeneous graph neural network classification aggregation algorithm based on size neighbor path identification},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sampling policy to accelerate high utility pattern mining at given accuracy level. <em>ASOC</em>, <em>171</em>, 112606. (<a href='https://doi.org/10.1016/j.asoc.2024.112606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Utility Pattern (HUP) mining is a popular topic in data mining, where a HUP can be a combination of products having high profit, a set of websites having high combined hits, and so on. HUP mining often requires significant computational resources for scanning, parsing, and processing transactions. When the database contains a large number of transactions, the algorithm will be very slow. When faced with databases with large-scale transactions, sampling is a commonly used data mining acceleration method. However, in the field of HUP mining, there has been little research on sampling, and the only existing research cannot avoid overestimating the sample size. To efficiently obtain HUPs from large databases, we propose a method called SAH (Sampling Policy to Accelerate HUP Mining), which utilizes a sample from the original database with a given accuracy level to accelerate the HUP mining. Given an accuracy level ( ϵ , δ ) , the SAH uses Hoeffding’s Inequality to estimate sample size, and can theoretically guarantee that for a pattern X , the fluctuation of X ’s relative utility caused by sampling is limited in [ − ϵ , ϵ ] with the probability at least 1 − δ . Furthermore, for every database, the SAH takes sample from a well designed virtual distribution to avoid the overestimation of sample size brought by the characteristic of Hoeffding’s Inequality. Experimental studies show that the speedup brought by SAH ranges from 1.6 to 47.8, and in most cases, the speedup exceeds 5. Meanwhile, the results obtained by the algorithm accelerated by SAH are reliable, and the sample size estimated by SAH is reasonable.},
  archive      = {J_ASOC},
  author       = {Zhang Zhongjie and Huang Jian},
  doi          = {10.1016/j.asoc.2024.112606},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112606},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient sampling policy to accelerate high utility pattern mining at given accuracy level},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-scale multi-head attention network for stock trend prediction considering textual factors. <em>ASOC</em>, <em>171</em>, 112388. (<a href='https://doi.org/10.1016/j.asoc.2024.112388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate prediction of stock prices assists investors in effectively planning their trading strategies, thereby enhancing investment returns. In response to the diversity of stock features and complexity of financial markets, this study introduces a comprehensive model, namely, EDBANet. Various feature factors contribute to stock price fluctuations, involving complex mechanisms. First, by introducing textual data, this study proposes a Sentiment Weight Harmony Model (EWH), which aims to solve issues related to accuracy and effectiveness of investors’ emotional information. Thereafter, we propose a differential smooth fourier analysis algorithm (DST) to effectively solve the problem of non-stationary of stock price data and enhance the extraction of periodic and frequency domain characteristics of stock prices. We also propose a stock price prediction network, Bi-LSTM-MHSA (BLMA), which incorporates a multi-scale multi-head attention (MHSA) module. By introducing the MHSA module into the context of multidimensional information, the network explores features at different time scales, aiding in a more comprehensive understanding of the dynamic characteristics of the stock market. To further improve model performance, we additionally propose a new hybrid optimizer that enhances the stability of the predictive model while also speeding up the convergence of the optimization process and improving the robustness of the model. The experimental results show that the proposed EDBANet network has the best prediction effect on the 4 self-built data sets and 6 public data sets of China’s A-shares, respectively, indicating that the algorithm has good robustness, and investors can formulate more accurate trading strategies based on the prediction results of this network.},
  archive      = {J_ASOC},
  author       = {Li Wan and Yuan Tao and Jiaqi Wang and Wenke Zhu and Chunling Tang and Guoxiong Zhou},
  doi          = {10.1016/j.asoc.2024.112388},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {112388},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-scale multi-head attention network for stock trend prediction considering textual factors},
  volume       = {171},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging LSTM-SMI and ARIMA architecture for robust wind power plant forecasting. <em>ASOC</em>, <em>170</em>, 112765. (<a href='https://doi.org/10.1016/j.asoc.2025.112765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind power prediction is a critical goal for power engineers, aimed at forecasting the power output for applicable power plants. However, the complex, nonlinear, non-stationary, and dynamic nature of wind power makes accurate prediction challenging, leading to inefficient grid integration and reduced profitability. Additionally, the vanishing gradient problem diminishes accuracy, causing poor retention of long-term patterns in the data, particularly with traditional short-term machine learning algorithms. In response, this research proposes a novel application of the Long Short-Term Memory (LSTM) algorithm integrated with Seasonal Mean Imputation (SMI), designed for multivariate time series analysis to predict the power output using various parameters including humidity, pressure, temperature, wind velocity, and wind direction at the Fauji Wind Energy Limited-I (50 MW) in Gharo, Thatta District, Pakistan. The results from the proposed LSTM-SMI are compared with state-of-the-art counterparts, namely, the Autoregressive Integrated Moving Average (ARIMA) and traditional Mean Imputation (MI) method, using performance metrics such as Symmetric Mean Absolute Percentage Error (SMAPE), Normalized Root Mean Square Error (NRMSE), and Normalized Mean Absolute Error (NMAE). Beside promising results from LSTM-SMI, the study expanded to include comparative performance with state of the art Deep Learning and Machine Learning counterpart models such as Gated Recurrent Units (GRU), Extreme Gradient Boosting (XGBoost) and Recurrent Neural Networks (RNN) for better assessment. The GRU model achieved a SMAPE of 7.68 %, compared to 9.13 % with LSTM, while application to a univariate dataset with both SMI and MI, the GRU demonstrated robust performance, with the GRU-SMI variant showing a SMAPE of 5.49 %, that illustrate the effectiveness. Outcomes demonstrates the potential of advanced recurrent neural network GRU architectures for enhanced predictive accuracy in wind power forecasting.},
  archive      = {J_ASOC},
  author       = {Saifullah Khan and Yasir Muhammad and Ihtesham Jadoon and Saeed Ehsan Awan and Muhammad Asif Zahoor Raja},
  doi          = {10.1016/j.asoc.2025.112765},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112765},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Leveraging LSTM-SMI and ARIMA architecture for robust wind power plant forecasting},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An inverse reinforcement learning algorithm with population evolution mechanism for the multi-objective flexible job-shop scheduling problem under time-of-use electricity tariffs. <em>ASOC</em>, <em>170</em>, 112764. (<a href='https://doi.org/10.1016/j.asoc.2025.112764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective flexible job shop scheduling problem under time-of-use electricity tariff (MOFJSP-TOU) is investigated for the electrolytic aluminum process in the aluminum industry. The problem includes the conflicting objectives of makespan and total electricity charge (TEC). The electricity charges are considered during different time slots and the operation hours of different jobs on different machines. Under the consideration of the time-of-use electricity pricing policy, the mixed-integer linear programming (MILP) model of MOFJSP-TOU has been abstracted. A population optimization algorithm dynamically guided by inverse reinforcement learning (PODGIRL) is presented to address MOFJSP-TOU. The reward function is generated from the characteristic of the optimum solution extracted by inverse reinforcement learning (IRL). The end-to-end scheduling scheme is obtained by Q-learning, which is guided by the learning reward function. The convergence of the population is improved by the variable domain search operation based on the idea of decomposition. The experimental results show that the performance of the proposed PODGIRL outperformed the state of art algorithms.},
  archive      = {J_ASOC},
  author       = {Fuqing Zhao and Weiyuan Wang and Ningning Zhu and Tianpeng Xu},
  doi          = {10.1016/j.asoc.2025.112764},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112764},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An inverse reinforcement learning algorithm with population evolution mechanism for the multi-objective flexible job-shop scheduling problem under time-of-use electricity tariffs},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An approach to portfolio optimization with time series forecasting algorithms and machine learning techniques. <em>ASOC</em>, <em>170</em>, 112741. (<a href='https://doi.org/10.1016/j.asoc.2025.112741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of identifying suitable stocks for portfolio inclusion, particularly in the context of complex stock forecasting dynamics characterized by nonlinear time series and various influencing factors, is addressed in this study. To tackle this challenge, an approach combining the auto-regressive integrated moving average (ARIMA) and least-square support vector machine (LS-SVM) models is proposed for stock selection. Furthermore, the mean–variance portfolio optimization model is utilized for optimal portfolio selection. The effectiveness of this approach is demonstrated through comprehensive comparisons with alternative machine learning models, including support vector machines (SVM), LS-SVM, ARIMA, combined ARIMA+SVM models, and several benchmarking models from the existing literature. Validation of the proposed technique is conducted using historical data from the Bombay Stock Exchange (BSE), India.},
  archive      = {J_ASOC},
  author       = {Jyotirmayee Behera and Pankaj Kumar},
  doi          = {10.1016/j.asoc.2025.112741},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112741},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An approach to portfolio optimization with time series forecasting algorithms and machine learning techniques},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic Q&A multi-label classification based on adaptive multi-scale feature extraction. <em>ASOC</em>, <em>170</em>, 112740. (<a href='https://doi.org/10.1016/j.asoc.2025.112740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In community question answering (CQA), questioners use labels for question and answer (Q&A) classification when asking questions. Since the answerers do not have the same understanding and perspective of the question, the original labels cannot accurately reflect the Q&A categories with constantly given answers. Therefore, this paper proposes a dynamic Q&A multi-label classification approach based on adaptive multi-scale feature extraction. First, global and local semantic features of Q&As are extracted based on bidirectional long short-term memory network and convolutional neural network models, respectively. Second, the label features extraction and fusion method is proposed. The semantic features of the labels are extracted, the label structure graph based on horizontal and vertical dependencies is constructed, and the label structure and semantic features are fused using the graph attention network integrating multi-head self-attention mechanism. Afterward, the label-aware local features of Q&As are constructed using the attention mechanism and fused with global features of Q&A using the multi-head self-attention, thereby multi-scale fusion classification features of Q&A are established. Then, to adaptively extract the core multi-scale fusion features, a multi-objective feature selection model is established and an improved binary multi-objective Sinh Cosh optimizer algorithm is proposed to solve the model. Finally, a classification prediction layer based on a multilayer perceptron is constructed to obtain the multi-label classification results of Q&A documents. The experimental results based on real Q&A data show the superior performance of the proposed method and validate the effectiveness of the proposed four modules.},
  archive      = {J_ASOC},
  author       = {Ying Li and Ming Li and Xiaoyi Zhang and Jin Ding},
  doi          = {10.1016/j.asoc.2025.112740},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112740},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic Q&A multi-label classification based on adaptive multi-scale feature extraction},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incomplete multi-view feature selection with adaptive consensus graph constraint for parkinson's disease diagnosis. <em>ASOC</em>, <em>170</em>, 112739. (<a href='https://doi.org/10.1016/j.asoc.2025.112739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson's disease (PD) is a neurodegenerative condition common among the elderly, with optimal treatment ideally administered in its early stages. Given the high rate of misdiagnosis of PD, it is crucial to introduce a new method to assist in its diagnosis. However, the current challenge lies in the high dimensionality of medical neuroimaging data, coupled with the issue of missing data, both of which can impact the performance of the algorithm. To address this, we propose a novel semi-supervised feature selection method to aid the diagnosis and score prediction of PD. Specifically, this method introduces a low-dimensional consensus representation to explore the complete data structure. By integrating the similarity matrix of the low-dimensional consensus representation with adaptive learning, we derive the optimal similarity matrix for each data view. Subsequently, we use the similarity matrix from each view to input the missing data within the respective view. To validate our approach, we utilize data from the public Parkinson's Progression Markers Initiative (PPMI) dataset and simulate three scenarios: complete data, 15 % missing data in each view, and 30 % missing data in each view. Experimental results demonstrate that our method outperforms existing methods in both complete and incomplete datasets.},
  archive      = {J_ASOC},
  author       = {Zhongwei Huang and Jianqiang Li and Jun Wan and Jianxia Chen and Zhi Yang and Ming Shi and Ran Zhou and Haitao Gan},
  doi          = {10.1016/j.asoc.2025.112739},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112739},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incomplete multi-view feature selection with adaptive consensus graph constraint for parkinson's disease diagnosis},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-adaptive single and simultaneous fault diagnosis for rotating machinery via redefined signal quality indicator and parallel ensemble network. <em>ASOC</em>, <em>170</em>, 112737. (<a href='https://doi.org/10.1016/j.asoc.2025.112737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotating machinery fault diagnosis plays a crucial role in industrial applications. However, existing methods face tremendous challenges in dealing with nonlinear noisy signals and intricate simultaneous-fault scenario. Dedicated to address this issue, a neoteric compound fault diagnosis method is proposed by using redefined signal quality indicator (RSQI) and parallel ensemble network. In this paper, RSQI is devised to eliminate noise components, and it can balance the noise reduction and signal fidelity. By further exploring the functionality of light gradient boosting machines (LGBM), parallel ensemble network containing two heterogeneous LGBMs is constructed. One is used to identify fault numbers, and the other is used for the single or simultaneous-fault scenario recognition. The proposed network is self-adaptive to the precious nature of the issue without user intervention for empirical threshold decision, and the two heterogeneous LGBMs can concurrently execute for responding to the diagnostic task in real time. Finally, two experimental studies are conducted to validate the proposed method. The experimental results of five multi-criteria decision-making (MCDM) methods indicate that the proposed method is competitive in the classification performance and algorithm robustness.},
  archive      = {J_ASOC},
  author       = {Weixiong Jiang and Kaiwei Yu and Jun Wu and Tianjiao Dai and Haiping Zhu},
  doi          = {10.1016/j.asoc.2025.112737},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112737},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-adaptive single and simultaneous fault diagnosis for rotating machinery via redefined signal quality indicator and parallel ensemble network},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG generalizable representations learning via masked fractional fourier domain modeling. <em>ASOC</em>, <em>170</em>, 112731. (<a href='https://doi.org/10.1016/j.asoc.2025.112731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods currently represent the state-of-the-art (SOTA) in electroencephalography (EEG) decoding, primarily focusing on the development of supervised models. However, most supervised methods are task-specific and lack the ability to generate generalizable latent features for use across different BCI paradigms. Additionally, as subjects engage in diverse brain–computer interaction tasks, the distribution of recorded EEG data varies according to the specific cognitive paradigms involved. The process of data collection and model training for each task is time-consuming. One potential solution is to construct a pre-trained model capable of transferring knowledge across various tasks. To improve the generalization ability of pre-trained models, we propose a novel masked autoencoder based on fractional Fourier domain reconstruction, denoted as Masked Fractional Fourier Domain Modeling (MFrFM), for learning generalizable time–frequency features. We systematically explore the effects of different degradation methods used within the denoising autoencoder to enhance the robustness of the pre-training model. Moreover, we examine the impact of various masking strategies on model performance. Our experiments demonstrate that the pre-trained MFrFM can effectively capture generalizable representations. Additionally, we conduct a comprehensive evaluation of fine-tuning performance through both cross-task and intra-task experiments. The experimental results show that MFrFM achieves a maximum accuracy of 98.09% in transferring from MI to SSVEP, and 79.76% in transferring from SSVEP to MI. The code is available at https://github.com/zshubin/MFrFM-for-cross-task-EEG-pre-training .},
  archive      = {J_ASOC},
  author       = {Shubin Zhang and Dong An and Jincun Liu and Yaoguang Wei},
  doi          = {10.1016/j.asoc.2025.112731},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112731},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EEG generalizable representations learning via masked fractional fourier domain modeling},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional generative adversarial networks for the generation of strong ground motion parameters using KiK-net ground motion records. <em>ASOC</em>, <em>170</em>, 112730. (<a href='https://doi.org/10.1016/j.asoc.2025.112730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction of ground motion parameters is important for seismic risk analysis and the seismic design of structures. The limited availability of strong ground motion records (peak ground acceleration (PGA) > 100 gal) limits the application of machine learning techniques in this domain. In this study, we utilized the KiK-net database to identify three site parameters— V S30 , V S-Zhole , and borehole depth—corresponding to the peak underground acceleration (PUA) on the downhole and spectral acceleration (Sa) across 15 different periods as input features for the machine learning model. The output parameters were PGA and Sa across 15 different periods. Pairs of strong ground motion records (PGA > 100 gal) are extracted from the original KiK-net database and processed using a conditional generative adversarial network (cGAN) model to synthesize ground motion parameters, then the original KiK-net database is supplemented. Regression prediction tests were performed on both the original and augmented databases and evaluated using three regression metrics: root mean square error (RMSE), mean absolute error (MAE), and R 2 . The findings indicate that the model trained with synthetic data generated by the cGAN enhances the prediction accuracy of ground motion parameters. A comparative analysis of the ground motion amplification coefficients between station records and cGAN-generated data across various spectral periods was performed. The amplification coefficients across different spectral periods aligned with the ground-motion records, thereby validating the reliability of the cGAN-generated synthetic ground-motion data from a seismological perspective. Furthermore, the ground motion parameters produced by the cGAN represent a valuable resource for future machine learning investigations.},
  archive      = {J_ASOC},
  author       = {Zhenning Ba and Jingxuan Zhao and Fangbo Wang and Linghui Lyu},
  doi          = {10.1016/j.asoc.2025.112730},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112730},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Conditional generative adversarial networks for the generation of strong ground motion parameters using KiK-net ground motion records},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Considering the imperfect cooperation among workers in the two-sided partial disassembly line balancing problem and the corresponding multi-modal multi-objective solution algorithm. <em>ASOC</em>, <em>170</em>, 112728. (<a href='https://doi.org/10.1016/j.asoc.2025.112728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In existing research on the balance of the two-sided disassembly line, the additional disassembly time caused by the imperfect cooperation of workers on both sides of the disassembly line is often disregarded. This oversight hinders current research from accurately reflecting the actual disassembly situation. In response to the above issue, this study proposes a two-sided partial disassembly line balancing problem with imperfect worker cooperation (TPDLBP-IWC). Furthermore, in existing studies, the problem of disassembly line balancing is typically treated as a multi-objective optimization problem and solved using multi-objective evolutionary algorithms, leading to a scarcity of equivalent Pareto optimal solutions. A lack of equivalent Pareto optimal solutions in practical applications may bias decision-makers' overall understanding of the issue, resulting in unnecessary economic losses. Addressing the aforementioned problem, this research initially considers the disassembly line balancing problem as a multi-modal multi-objective optimization problem and proposes a multi-modal multi-objective evolutionary algorithm (MMEA-DLBP) tailored for the disassembly line balancing problem. To find a more comprehensive set of optimal solutions and enhance the algorithm's performance, this paper introduces for the first time an Equilibrium Monte Carlo Tree Initialization (EMCI) approach from the perspective of improving population diversity. EMCI can effectively increase the initial population's coverage in the solution space, thereby enhancing the diversity of the initial population. Secondly, a Double Pareto Elite Selection Strategy Based on Disassembly Sequence Distance (DPES-DSD) is proposed, which effectively maintains the diversity of the population in both the decision and objective spaces and assists the algorithm in obtaining more equivalent Pareto optimal solutions. Experimental results show that MMEA-DLBP can effectively solve the disassembly line balancing problem. Compared with other algorithms, MMEA-DLBP provides a more comprehensive set of high-quality disassembly solutions, thereby offering decision-makers a wider range of choices.},
  archive      = {J_ASOC},
  author       = {ZhenYu Xu and Yong Han and Donglin Zhu},
  doi          = {10.1016/j.asoc.2025.112728},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112728},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Considering the imperfect cooperation among workers in the two-sided partial disassembly line balancing problem and the corresponding multi-modal multi-objective solution algorithm},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on expensive optimization problems using differential evolution. <em>ASOC</em>, <em>170</em>, 112727. (<a href='https://doi.org/10.1016/j.asoc.2025.112727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world problems that require substantial execution time and computational resources for evaluating candidate solutions can be termed Expensive Optimization Problems (EOPs). The major challenge in solving EOPs lies in that evaluating the candidate solutions comes with a high and sometimes even prohibitive cost, thereby presenting notable obstacles for current optimization methods. Differential Evolution (DE), as a branch of evolutionary algorithms, has been widely used to tackle various optimization problems due to its fast convergence speed and powerful search capability. However, the performance of DE may deteriorate rapidly when solving EOPs where the number of function evaluations is limited. Recently, considerable efforts have been dedicated to improving DE-based algorithms for solving EOPs. However, there is a lack of a systematic survey on modifications to DE-based algorithms for tackling EOPs. By collecting and analyzing current DE-based algorithms, this survey presents a comprehensive overview of modifications on DE-based algorithms. The paper divides these modifications into three categories: framework improvement, surrogate-assisted approximation, and parallel and distributed implementation. According to the type of EOPs, existing DE-based algorithms are also classified and analyzed. Lastly, we present current challenges and future directions of employing DE-based algorithms for EOPs. By providing both novice and experienced researchers with a fresh perspective on utilizing DE-based algorithms for tackling EOPs, this survey aims to assist researchers in designing more efficient algorithms for EOPs.},
  archive      = {J_ASOC},
  author       = {Chongle Ren and Zhenyu Meng},
  doi          = {10.1016/j.asoc.2025.112727},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112727},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A survey on expensive optimization problems using differential evolution},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot fault diagnosis of particle accelerator power system using a bidirectional discriminative prototype network. <em>ASOC</em>, <em>170</em>, 112726. (<a href='https://doi.org/10.1016/j.asoc.2025.112726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Faults in power systems can result in costly downtime and potentially catastrophic damage in high-energy particle accelerators. Diagnosing these faults presents a significant challenge due to the scarcity of fault samples across various abnormal conditions—a common scenario in real-world applications where power systems predominantly operate under normal conditions. To address this challenge, we propose a bi-directional discriminative prototype network (BiDPN) for few-shot fault diagnosis, specifically designed for particle accelerator power systems. Our approach utilizes the abundance of normal operational data to train a feature extractor, which generates robust, discriminative features for fault diagnosis. These features are mapped to prototype vectors, enabling accurate multi-class fault classification even with extremely few fault samples. Experimental results on the high-voltage conversion modulator at the Scattered Neutron Source show that BiDPN can effectively and reliably diagnose faults with very few fault samples, highlighting its practical applicability in scenarios with limited abnormal data.},
  archive      = {J_ASOC},
  author       = {Zhe Yang and Rongbing Ye and Lingli Jiang and Jianyu Long and Yunwei Huang and Chuan Li},
  doi          = {10.1016/j.asoc.2025.112726},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112726},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Few-shot fault diagnosis of particle accelerator power system using a bidirectional discriminative prototype network},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous remote sensing image change detection network based on multi-scale feature modal transformation. <em>ASOC</em>, <em>170</em>, 112725. (<a href='https://doi.org/10.1016/j.asoc.2025.112725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of remote sensing (RS) earth observation technology, it is possible to monitor land use/land cover change with multi-modal RS data. The challenge of heterogeneous RS image change detection (HRSI-CD) is determining how to unify the feature space of images such that they are comparable. However, the majority of existing methods often extract features from the bi-temporal heterogeneous images (BTHIs) in order to acquire shared features. During the process of feature extraction, the correlation and interactivity between images captured at different times are disregarded, which leads to the information loss during images conversion and greatly limits the performance of CD. The correlation mechanism between heterogeneous images needs to be further explored and utilized. A new multi-scale cross-modal fusion network based on style-transfer (MCFNet) is proposed in this paper to solve these problems. The idea of MCFNet involves the restructuring of the content and style representation of heterogeneous images. The network employs content encoders and style encoders to extract multi-scale content and style features from BTHIs. It also enforces consistency in style representation by applying similarity style constraints at the minimum scale, ensuring that related pixels have consistent styles. On this basis, we propose a cross-modal feature cross-fusion mechanism that integrates the content and style features of the two images, effectively exploiting the association between the heterogeneous images. Furthermore, a multi-scale feature fusion method is proposed to effectively aggregate the content and style features of images across various scales. This technique aims to achieve precise style transfer while maintaining the semantic information of the images. We verify the proposed MCFNet on three HRSI-CD datasets. The experimental results demonstrate that the proposed method possesses significant advantages in comparison to the prevailing methods. On three datasets, MCFNet outperforms other advanced algorithms by at least 4% in terms of the Kappa indicator.},
  archive      = {J_ASOC},
  author       = {Wei Cheng and Yining Feng and Yicen Sun and Xianghai Wang},
  doi          = {10.1016/j.asoc.2025.112725},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112725},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heterogeneous remote sensing image change detection network based on multi-scale feature modal transformation},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equipping high-order fuzzy cognitive map with interpretable weights for multivariate time series forecasting. <em>ASOC</em>, <em>170</em>, 112724. (<a href='https://doi.org/10.1016/j.asoc.2025.112724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being a kind of fuzzy cognitive maps (FCMs), the high-order FCMs (HFCMs) are designed for multivariate time series (MTS) to forecast a future vector by using multiple past vectors rather than one past vector. The HFCM-based MTS forecasting model realizes the forecasting by using the causalities between past vectors and future vector, which are described by a set of real-valued weights falling in interval [-1, 1] and determined by some given training data. However, for sake of various uncertain factors in measuring and collecting the training data, the real-valued weights determined by such kind of obtained training data cannot exactly characterize the causalities between vectors. In other words, the trained real-valued weights may not be the true real-valued weights. In these scenarios, it becomes necessary to equip an HFCM with interval-valued weights, resulting in an interval high-order FCM (IHFCM). For an IHFCM, how to determine its interval-valued weights poses a significant challenge. Facing with this challenge, we first propose a principle of justifiable granularity for vector-valued input-output data (PJG-VID), which relies on two fundamental criteria: maximizing the coverage of actual vector-valued output by interval-valued vector-valued outputs and enhancing the semantic specificity of interval-valued weights. Utilizing PJG-VID, we then formulate an optimization problem to determine the interval-valued weights of an IHFCM. And finally, we develop an MTS forecasting model employing this novel IHFCM. The interval-valued weights determined through our method not only accurately capture the causalities between vectors but also facilitate a direct and semantically clear interpretation. As a result, the MTS forecasting model presented in this paper achieves remarkable performance in terms of forecasting accuracy and semantic interpretability.},
  archive      = {J_ASOC},
  author       = {Chenxi Ouyang and Fusheng Yu and Fei Yang},
  doi          = {10.1016/j.asoc.2025.112724},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112724},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Equipping high-order fuzzy cognitive map with interpretable weights for multivariate time series forecasting},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global-local graph convolutional broad network for hyperspectral image classification. <em>ASOC</em>, <em>170</em>, 112723. (<a href='https://doi.org/10.1016/j.asoc.2025.112723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conventional broad learning system (BLS) struggles to represent the complex nonlinear features of hyperspectral images (HSI) due to its reliance on linear sparse feature extraction methods. Additionally, traditional BLS models focus primarily on class separability, ignoring the manifold structure that characterizes relationships between samples. To address these issues, previous research has incorporated graph convolutional networks (GCNs) and manifold learning into the BLS framework, but these methods often emphasize only local manifold structures, overlooking global structural information. In this paper, we propose a Global-Local Graph Convolutional Broad Network (GLGBN) for HSI classification. GLGBN addresses both global and local manifold structures, optimizing the classification boundary by minimizing local scatter and maximizing global scatter. It uses linear discriminant analysis (LDA) to preserve global manifold structure and locality preserving projections (LPP) to model local relationships via a Laplacian graph. This dual approach ensures that similar samples remain close while dissimilar samples are separated, enhancing classification accuracy. The proposed GLGBN model demonstrated outstanding overall accuracy across multiple public datasets: 95.31% on Indian Pines, 97.67% on Pavia University and 98.37% on Salinas, surpassing several classical and state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Yonghe Chu and Jun Cao and Jiashuang Huang and Hengrong Ju and Guangen Liu and Heling Cao and Weiping Ding},
  doi          = {10.1016/j.asoc.2025.112723},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112723},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Global-local graph convolutional broad network for hyperspectral image classification},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An EEG-based single-channel dual-stream automatic sleep staging network with transfer learning. <em>ASOC</em>, <em>170</em>, 112722. (<a href='https://doi.org/10.1016/j.asoc.2025.112722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sleep staging is the essential step in assessing sleep quality and diagnosing sleep disorders. With the increasing demand for in-home portable sleep monitoring and closed-loop stimulation applications, developing a lightweight, efficient and widely applicable automatic sleep staging model is highly important. This paper proposes a single-channel dual-stream automatic sleep staging model with transfer learning for end-to-end fast sleep staging. First, we design temporally fine-grained and coarse-grained streaming networks for extracting single-channel multigranularity temporal features on the basis of multiscale one-dimensional convolution to capture the morphological features of various individuals. Then, the strong temporal dependence between individuals is captured in combination with bidirectional gated recurrent units to enhance the connections between electroencephalogram and morphological features in different sleep stages. Second, to alleviate the problem of class imbalance in sleep data, we use resampling and focus loss functions to improve the classification performance of a few classes in sleep stages. Finally, we validate the cross-subject and cross-modal performance of the model using both public and homemade datasets, and the results show that it performs well in experiments with healthy, insomnia patients, obstructive sleep apnoea and other subjects, demonstrating strong transferability. In addition, we visualize the feature distribution as well as the classification performance of the model by interpreting the hidden units and t-distributed stochastic neighbour embedding and reveal the brain regions suitable for automatic sleep staging. This can provide theoretical support for the future development of wearable sleep monitoring devices and closed-loop stimulation applications.},
  archive      = {J_ASOC},
  author       = {Shaofei Ying and Pengrui Li and Jiping Chen and Wenpeng Cao and Haokai Zhang and Dongrui Gao and Tiejun Liu},
  doi          = {10.1016/j.asoc.2025.112722},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112722},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An EEG-based single-channel dual-stream automatic sleep staging network with transfer learning},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The fuzzy hypergraph neural network model based on sparse k-nearest neighborhood granule. <em>ASOC</em>, <em>170</em>, 112721. (<a href='https://doi.org/10.1016/j.asoc.2025.112721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hypergraph Neural Network (HGNN) model, widely applied in hyperedge prediction and node classification, encodes high-order data correlation in hypergraph structure. However, the classical HGNN model cannot eliminate the heterogeneity of data. To address the issue, the fuzzy hypergraph neural network model (FHGNN) based on sparse k -nearest neighborhood granule is proposed in this paper, where the framework consists of three procedures: Hyperedge modeling, Fuzzy hypergraph construction, and Fuzzy hypergraph convolution. First, the hyperedge granule model is constructed based on k -nearest neighbors of nodes, viewed through the lens of granular computing. Moreover, we assigned different optimal k values to nodes based on the proposed sparse constraint function. We construct the hyperedge model of optimal k nodes with low heterogeneity, eliminating heterogeneity of data. Second, we introduced Dempster–Shafer (D–S) evidence theory to fuse different belief functions for describing the fuzzy membership between nodes and hyperedge. Third, we constructed the fuzzy hypergraph convolution model based on fuzzy membership. The weight of nodes with high heterogeneity is low, reducing data heterogeneity. The experimental results on citation network and visual object datasets demonstrate that the proposed FHGNN model outperforms other comparison methods. Based on the result of the schizophrenic dataset, the proposed FHGNN model eliminates the heterogeneity of data, improving diagnosis accuracy in schizophrenia patients and providing a new direction for the HGNN model optimization.},
  archive      = {J_ASOC},
  author       = {Tao Yin and Weiping Ding and Hengrong Ju and Jiashuang Huang and Yuepeng Chen},
  doi          = {10.1016/j.asoc.2025.112721},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112721},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The fuzzy hypergraph neural network model based on sparse k-nearest neighborhood granule},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semi-supervised soft prototype-based autonomous fuzzy ensemble system for network intrusion detection. <em>ASOC</em>, <em>170</em>, 112719. (<a href='https://doi.org/10.1016/j.asoc.2025.112719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating cyber-attacks in recent years have created a significant demand for advanced intrusion detection systems. However, the dynamic characteristics of network data streams, scarcity of labelled data, imbalance of class distribution and concerns about the explainability of predictive models have greatly restricted the applicability of conventional machine learning and cutting-edge deep learning techniques in real-world scenarios for intrusion detection. In this paper, a novel soft prototype-based fuzzy ensemble intrusion detection system is proposed to autonomously exploit semi-supervised learning from network data streams by exploiting the pseudo-labelling method. To reduce the pseudo-labelling errors, although the proposed ensemble system involves all the base models for joint pseudo-labelling, it only utilises pseudo-labelled data with the highest consensus for self-improvement. To foster generalisation, the proposed ensemble system leverages sampling techniques to address the class imbalance within the labelled and pseudo-labelled data, and the qualities of base models are constantly monitored, ensuring that weaker base models are efficiently substituted. Additionally, instead of discarding these challenging-to-classify samples during online semi-supervised learning, the proposed ensemble system summarises them into a smaller number of unlabelled soft prototypes, allowing human experts to contribute to the learning at any point by manually labelling these soft prototypes to further augment the learned knowledge base. Numerical examples on public network intrusion detection datasets demonstrated the superior performance of the proposed ensemble system.},
  archive      = {J_ASOC},
  author       = {Xiaowei Gu and Gareth Howells and Haiyue Yuan},
  doi          = {10.1016/j.asoc.2025.112719},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112719},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A semi-supervised soft prototype-based autonomous fuzzy ensemble system for network intrusion detection},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An oversampling technique based on noise detection and geometry. <em>ASOC</em>, <em>170</em>, 112718. (<a href='https://doi.org/10.1016/j.asoc.2025.112718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of machine learning, one of the key techniques for dealing with imbalanced data issues is the SMOTE algorithm. Although SMOTE and its variants have shown good performance on many datasets, they generally do not make use of information from the majority class samples. In this paper, a more robust and reliable technique, noise detection and geometric oversampling (NG-SMOTE), will be employed to balance the data. NG-SMOTE first uses an ensemble filter to eliminate the noise samples in the original data, and then clusters the remaining minority samples to obtain k cluster centers. Then, the closest majority sample to each cluster center is found, and a hypersphere is constructed between the two samples. Finally, a sample is generated on the hypersphere and the new sample is interpolated between this sample and the cluster center. The above steps will be repeated until the dataset reaches balance. To verify the effectiveness of NG-SMOTE, we conducted comparative experiments with some oversampling techniques on 27 data sets. In the experiment, the proposed oversampling technology improved by 4 %-9 % compared to other oversampling technologies in Recall, AUC, F1-measure and G-mean respectively. In addition, the statistical test results showed significant differences except for SMOTE-ENN and IW-SMOTE.},
  archive      = {J_ASOC},
  author       = {Pengfei Sun and Zhiping Wang and Liyan Jia and Lin Wang},
  doi          = {10.1016/j.asoc.2025.112718},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112718},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An oversampling technique based on noise detection and geometry},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new semi-supervised fuzzy clustering method based on latent representation learning and information fusion. <em>ASOC</em>, <em>170</em>, 112717. (<a href='https://doi.org/10.1016/j.asoc.2025.112717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy clustering is a simple but efficient clustering method, which aims to deal with ambiguous and overlapping data classification boundaries and provide detailed membership degree information. However, the complex structure of high-dimensional data in the real world easily causes the inefficiency of widely used distance metrics. In addition, traditional fuzzy clustering methods cannot utilize supervision information to guide membership degree matrix learning, which limits clustering performance. In this paper, we propose a new semi-supervised fuzzy clustering method based on latent representation learning and information fusion. Specifically, our method utilizes a deep autoencoder to learn the underlying low-dimensional structure of data, thereby mitigating the impact of redundant features. Moreover, a semi-supervised constraint term based on information fusion is designed to make full use of prior knowledge to supervise the clustering process. On the one hand, a new constraint distance is proposed by leveraging pairwise constraint information to re-evaluate the membership degree of data. On the other hand, the semi-supervised constraint term is constructed based on label information to guide the membership degree matrix learning. Comprehensive experiments on a variety of standard datasets show that our method achieves better performance compared with state-of-the-art baseline methods, demonstrating the effectiveness of the proposed method in fuzzy clustering.},
  archive      = {J_ASOC},
  author       = {Hengdong Zhu and Baoshuo Kan and Yong Li and Enliang Yan and Heng Weng and Fu Lee Wang and Tianyong Hao},
  doi          = {10.1016/j.asoc.2025.112717},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112717},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new semi-supervised fuzzy clustering method based on latent representation learning and information fusion},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-driven feature selection via granular-rectangular neighborhood rough sets for interval-valued data sets. <em>ASOC</em>, <em>170</em>, 112716. (<a href='https://doi.org/10.1016/j.asoc.2025.112716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the burgeoning landscape of big data analytics, interval-valued datasets are indispensable for modeling uncertainty and vagueness, with significant implications for sectors such as healthcare and environmental science. Feature selection, a linchpin in data mining, is paramount for streamlining data processing and bolstering predictive models. However, the literature on feature extraction within interval-valued information systems is notably sparse. This paper proposes a groundbreaking feature selection framework that skillfully addresses the complexities of interval-valued data. The method innovatively utilizes a fully connected weighted undirected graph to encapsulate interval data, combining graph-theoretic insights with granular-rectangular neighborhood rough set theory. By evaluating the significance of each attribute based on its importance to the entire information system, and applying matrix power series to accelerate computations, the framework ensures both robust classification performance and the elimination of redundancy, marking a significant advancement in this field. Through comparative experiments on 12 public datasets with 7 other algorithms, theoretical analysis, and experimental results demonstrate that the proposed method not only exhibits high effectiveness in handling interval-valued data but also further improves efficiency and classification performance. In addition, the method also shows significant advantages in reducing the dependence on prior knowledge and improving the interpretability of the model, which fully proves its applicability and reliability in large-scale data analysis.},
  archive      = {J_ASOC},
  author       = {Xiaoyan Zhang and Xuan Shen},
  doi          = {10.1016/j.asoc.2025.112716},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112716},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph-driven feature selection via granular-rectangular neighborhood rough sets for interval-valued data sets},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment of digital transformation indicators to prioritize sustainable financial services using q-rung orthopair fuzzy rough decision-making model. <em>ASOC</em>, <em>170</em>, 112715. (<a href='https://doi.org/10.1016/j.asoc.2025.112715'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the importance of environmental and social responsibility gains momentum, the financial sector is ever more aware of its dynamic role in assisting the evolution to a low-carbon economy and a sustainable future. Digital Transformation (DT) is the application of technologies for building innovative models, procedures and systems that generates more profitable revenue, greater competitive advantage and better productivity. However, the DT in sustainable financial services is slower than the other sectors. In this regard, very few articles have been presented in the literature. This paper aims to assess the sustainable financial services based on the indicators of DT using a decision support system. In this framework, the classical Alternative Ranking Order Method Accounting for Two-Step Normalization (AROMAN) is extended from q-rung orthopair fuzzy rough information (q-ROFRI) and combined with the decision-making experts (DMEs) and criteria weight-determination models. In this method, firstly the DMEs’ weights are calculated using standard deviation-based tool. Second, the DMEs’ opinions are aggregated through novel Sugeno-Weber weighted averaging operator. In this regard, Sugeno-Weber operators are proposed for q-ROFRI. Third, an integrated criteria weighting formula is determined in which the objective weights are calculated using a distance measure-based model, while the subjective weights are derived via relative closeness coefficient-based model with q-ROFRI. For this purpose, a novel distance measure is proposed for q-ROFRI. Combining all these steps, a hybrid q-rung orthopair fuzzy rough AROMAN method is introduced in which the weighted normalized ratings are computed through a new score function. Further, an empirical study is presented to prioritize the sustainable financial services with respect to DT indicators’ assessment, which also shows the practicality and feasibility of presented method. The findings of this work show that the option “ green banking ” is the most optimal choice among a set of five sustainable financial services with the overall evaluation grade of 0.384, while the ranking order of options is green banking (0.384) > values-based banking (0.3745) > ESG finance (0.3714) > climate finance (0.356) > social banking (0.3384). To prove the stability of obtained results, sensitivity assessment is conducted on the basis of different values of considered parameters. At last, comparison with extant methods is made to exemplify the robustness of proposed framework under the context of q-ROFRI.},
  archive      = {J_ASOC},
  author       = {Pratibha Rani and Arunodaya Raj Mishra and Dragan Pamucar and Ahmad M. Alshamrani and Adel Fahad Alrasheedi},
  doi          = {10.1016/j.asoc.2025.112715},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112715},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessment of digital transformation indicators to prioritize sustainable financial services using q-rung orthopair fuzzy rough decision-making model},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cascading adaptive binary image feature maps with vision transformer for iris spoof detection. <em>ASOC</em>, <em>170</em>, 112713. (<a href='https://doi.org/10.1016/j.asoc.2025.112713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris presentation attack detection (PAD) module is an essential component of iris recognition system used to address the susceptibility against various spoof attacks. Although convolutional neural network (CNN)-based iris PAD methods have exhibited remarkable performance but suffers from limited generalization capabilities to unseen attacks. Furthermore, recent deep learning-assisted PAD approaches uphold parameter dependency and inductive bias problems for extracting texture information from images. Thereby pertaining to these issues, the current expansion of the Vision Transformer (ViT) architecture has surrogated the CNN model for image classification tasks. Hence, in this work we are using ViT for the first time in iris spoof detection problem along with newly explored handcrafted local image features. We propose a novel hybrid approach for iris spoof detection by coalescing unique local image features with efficient vision transformer. Three novel and discriminatory image feature maps (Central Local Adaptive Binary Patterns (CLABP), Left Local Adaptive Binary Patterns (LLABP) and Right Local Adaptive Binary Patterns (RLABP) are extracted from iris images and given as input to ViT model. Afterward, ViT extract multiple global context information from iris feature maps that is aimed at various head scales. Finally, classification task to discriminate live and fake iris images is performed by ViT. The experimental results demonstrate superior performance of our approach on benchmark datasets such as Notre Dame, IIITD-WVU and NDCLD15 with an average classification error rate (ACER) of 1.94 %, 1.52 %, and 1.97 % respectively.},
  archive      = {J_ASOC},
  author       = {Deepika Sharma and Arvind Selwal},
  doi          = {10.1016/j.asoc.2025.112713},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112713},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cascading adaptive binary image feature maps with vision transformer for iris spoof detection},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive NSGA-Ⅱ for electric vehicle routing problem with charging/discharging based on time-of-use electricity pricing and diverse charging stations. <em>ASOC</em>, <em>170</em>, 112704. (<a href='https://doi.org/10.1016/j.asoc.2025.112704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current research on the electric vehicle routing problem (EVRP) predominantly focuses on customer characteristics or the diversity of charging mechanisms, while relatively insufficient attention is paid to the influence of energy interactions facilitated by vehicle-to-grid (V2G) technology on route planning. This study presents a novel approach to EVRP with charging/discharging based on time-of-use (TOU) electricity pricing and diverse charging stations. The proposed method enables electric vehicles to select charging stations for charging or discharging en route, depending on electricity price fluctuations, thus offering opportunities for cost reduction and profit enhancement in logistics distribution. A tailored adaptive non-dominated sorting genetic algorithm-Ⅱ (ANSGA-Ⅱ) is developed to address the problem, which integrates adaptive probability calculation, hybrid population generation, and neighborhood search operators. Testing on benchmark instances demonstrates that the proposed ANSGA-Ⅱ effectively addresses the problem, exhibiting strong convergence. The optimized routing allows vehicles to efficiently engage in vehicle-grid interactions, incentivized by TOU pricing, yielding significant profits for logistics companies, amounting to approximately 20.82 % of total logistics costs. This approach provides a new strategic avenue for optimizing logistics operations. Ultimately, sensitivity analysis elucidates the correlation among TOU electricity pricing, logistics costs, and discharging profits.},
  archive      = {J_ASOC},
  author       = {Junyu Li and Changshi Liu and Kunxiang Yi and Lijun Fan and Zhang Wu},
  doi          = {10.1016/j.asoc.2025.112704},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112704},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive NSGA-Ⅱ for electric vehicle routing problem with charging/discharging based on time-of-use electricity pricing and diverse charging stations},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neuro-fuzzy based indian topographic map understanding system. <em>ASOC</em>, <em>170</em>, 112703. (<a href='https://doi.org/10.1016/j.asoc.2025.112703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information of interest to any geospatial application requires being extracted from Topographic maps (TMs). Extracting information from topographic map represents one of the major bottlenecks due to complex distribution of geographic elements and highly interconnected nature of map features. The need for automated topographic map understanding arose because current methods for the information extraction are not adequate and an automated method is necessary in terms of time and economic efficiency. This work reports on an implementation of Topographic map understanding system to extract spatial information and to provide this information in machine-readable data formats to preserve the digital repository. This data can be used for analytical purposes required in generation of. The paper presents Indian topographic map understanding system (ITMUS) that is characterized by the human mentation and learning capabilities. The ITMUS is comprised of image processing routines, Structure feature descriptors, and adaptive Neuro-fuzzy inference system. The Fuzzy inferencing has been implemented using Sugeno model which utilizes the initial crude domain knowledge about the map legends. Further, system has been trained for various sample regions selected from Open Series Map (OSM) Indian topographic maps. Results of implementation are evaluated against reference data of Survey of India and manual recognition. It has been found that the overall recognition rate of the system is 90.91 %. Further, the system’s overall accuracy is determined to be 92.77 %.},
  archive      = {J_ASOC},
  author       = {Gitanjali Ganpatrao Nikam and Jayanta Kumar Ghosh},
  doi          = {10.1016/j.asoc.2025.112703},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112703},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neuro-fuzzy based indian topographic map understanding system},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards complex scene text reading with selective region proposal and two-stage deep reinforcement learning. <em>ASOC</em>, <em>170</em>, 112701. (<a href='https://doi.org/10.1016/j.asoc.2025.112701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenging task of accurately detecting and recognizing text from images captured in intricate real-world settings is known as text reading. The inherent complexity and variability of real-world scenes often make conventional optical character recognition systems ineffective. Furthermore, although deep learning-based systems perform well with horizontal text in natural scenes, they frequently encounter difficulties with oriented text. To address these limitations, we propose a multi-oriented scene text reading framework utilizing a selective region proposal technique based on Scale-Invariant Feature Transform keypoints. This approach focuses on relevant text regions, enhancing efficiency and precision. We further refine text localization through bounding box regression. A two-stage deep reinforcement learning framework, incorporating character/word awareness, is employed to correct and align detected characters, and to validate annotated words. This framework utilizes a generative adversarial network for enhanced character extraction and recognition. Our extensive evaluations on five benchmark datasets demonstrate the effectiveness of our approach in handling real-world scene text reading challenges, achieving promising results compared to state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Riadh Harizi and Rim Walha and Fadoua Drira},
  doi          = {10.1016/j.asoc.2025.112701},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112701},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards complex scene text reading with selective region proposal and two-stage deep reinforcement learning},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-based topic model for public opinion analysis in sudden-onset disasters. <em>ASOC</em>, <em>170</em>, 112700. (<a href='https://doi.org/10.1016/j.asoc.2025.112700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sudden-onset disasters have put forward more stringent requirements for the government to carry out public opinion analysis work. However, most existing topic models ignore the contextual semantics of disaster texts, and fail to balance the robustness and the training cost. To address these issues, a neural clustering topic model is proposed in this work. The topic probability distribution of the LDA model is integrated with the distribution semantic vector generated by a lite BERT. The fused vectors are reconstructed by a nonlinear manifold learning algorithm, and re-clustered into topics by a mini-batch based k- means++ algorithm. Compared to state-of-the-art models on three sudden-onset disaster datasets, the proposed model shows an increase of 1.79 % in average topic coherence and 33.87 % in topic diversity. Meanwhile, the inference time is reduced by 84.09 % on average. The visual study of the latent process of the proposed model reflects that its ability to compact intra-cluster vector distances and sparse inter-cluster vector distances is the potential reason for its better performance. It can be considered that the application of the proposed model can help the government enhance its ability to manage negative public opinions in sudden-onset disasters.},
  archive      = {J_ASOC},
  author       = {Yulong Ma and Xinsheng Zhang and Runzhou Wang},
  doi          = {10.1016/j.asoc.2025.112700},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112700},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semantic-based topic model for public opinion analysis in sudden-onset disasters},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object pose tracking using multimodal knowledge from RGB images and quaternion-based rotation contexts. <em>ASOC</em>, <em>170</em>, 112699. (<a href='https://doi.org/10.1016/j.asoc.2025.112699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose an approach for 3D object pose tracking on videos acquired by a calibrated camera. The pose proposals are leveraged to guide feature learning on RGB image sequences. We propose a multimodal X-Net that has two inputs and two outputs. An object id with quaternion representing object rotation in the previous frame are fed to the first input, whereas the object subwindow from the current RGB image is fed to the second one. We demonstrate that owing to such a pose proposal the X-Net learns attention blobs representing the object rotation in the former frame. It segments the object of interest and delivers heatmaps representing object keypoints. The keypoints are utilized in tracking by optimization, whereas the segmented object is leveraged in a pose verification that is responsible for detecting poor keypoints due to outliers and detection errors. After detecting this the pose is refined by Hierarchical Optimistic Optimization. We demonstrate experimentally that the keypoints are detected with superior accuracy in comparison to the accuracy of SOTA algorithms and tracking accuracy is competing with SOTA accuracies achieved on challenging YCB-Video and OPT datasets.},
  archive      = {J_ASOC},
  author       = {Mateusz Majcher and Bogdan Kwolek},
  doi          = {10.1016/j.asoc.2025.112699},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112699},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Object pose tracking using multimodal knowledge from RGB images and quaternion-based rotation contexts},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph enhanced spatial–temporal transformer for traffic flow forecasting. <em>ASOC</em>, <em>170</em>, 112698. (<a href='https://doi.org/10.1016/j.asoc.2025.112698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow forecasting, which aims to predict future traffic patterns based on current conditions, is a crucial yet challenging task in intelligent transportation systems due to the complex spatial–temporal relationships involved. Existing methods often struggle to effectively capture these intricate spatial dependencies and temporal patterns. To address these limitations, we propose a graph enhanced spatial–temporal Transformer (GE-STT), which integrates a graph enhanced module and a spatial–temporal Transformer module for improved prediction accuracy. Specifically, the graph enhanced module combines a Graph Convolutional Network (GCN) with a Gated Recurrent Unit (GRU) to obtain enriched spatial–temporal features, and introduce the original traffic data as a correction term to deal with the errors in the enhancement process. The spatial–temporal Transformer then leverages these enhanced features for final prediction. Experimental results on four traffic datasets show that GE-STT achieves superior performance under various metrics. Compared with the best baseline in different datasets, the performance of GE-STT is improved by up to 8% under the MAE metric, highlighting its robustness and effectiveness in traffic flow forecasting tasks.},
  archive      = {J_ASOC},
  author       = {Weishan Kong and Yanni Ju and Shiyuan Zhang and Jun Wang and Liwei Huang and Hong Qu},
  doi          = {10.1016/j.asoc.2025.112698},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112698},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph enhanced spatial–temporal transformer for traffic flow forecasting},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A knowledge-driven memetic algorithm for distributed green flexible job shop scheduling considering the endurance of machines. <em>ASOC</em>, <em>170</em>, 112697. (<a href='https://doi.org/10.1016/j.asoc.2025.112697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flexible job shop scheduling (FJSP) stands as one of the most pivotal scheduling problems, attracting considerable research efforts aimed at discovering improved solutions. The distributed variant of FJSP, which extends the problem’s scope, has garnered substantial interest among scholars. Recognizing the inherent limitation in machine endurance and the inevitable degradation with accumulating workloads, the significance of preventive maintenance in enhancing machine reliability is emphasized and ensuring process control. This study endeavors to concurrently optimize three key metrics: makespan, total energy consumption, and maintenance cost. To this end, a knowledge-driven memetic algorithm is tailored specifically to the problem’s characteristics. Our approach commences with a hybrid initialization incorporating eight strategic approaches, crafted to address the factory assignment, operation sequence, and machine selection subproblems, thereby yielding an initial population characterized by high quality and diversity. Subsequently, genetic operators are employed to generate offspring, wherein elite segments from exemplary solutions are selectively inherited during crossover. A two-stage mutation mechanism is introduced to foster the emergence of novel individuals. Finally, three tailored local search strategies are executed, striking a balance between exploration and exploitation.Comprehensive experimental findings emphasize the superior performance of the proposed algorithm in addressing the pertinent problem. The experimental results presented in this paper indicate increases of 20% and 141% in Hypervolume (HV) values and Metric for Diversity (DM) values, respectively, while the reduction in Inverted Generation Distance (IGD) values amounts to 85%, thereby demonstrating the effectiveness of our proposed methodology.},
  archive      = {J_ASOC},
  author       = {Libao Deng and Yixuan Qiu and Yuanzhu Di and Lili Zhang},
  doi          = {10.1016/j.asoc.2025.112697},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112697},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A knowledge-driven memetic algorithm for distributed green flexible job shop scheduling considering the endurance of machines},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite-time convergent gradient-zeroing neurodynamic system for solving temporally-variant linear simultaneous equation. <em>ASOC</em>, <em>170</em>, 112695. (<a href='https://doi.org/10.1016/j.asoc.2025.112695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that gradient information plays an essential role in GNS (gradient neurodynamic system) for finding the solution to static problems, and derivative information plays an essential role in ZNS (zeroing neurodynamic system) for finding the solution to temporally-variant problems. This fact prompts us to search for a way to simultaneously utilize them for better performance. Motivated by this point, a novel finite-time convergent GAGZNS (gradient-activation gradient-zeroing neurodynamic system) is designed and proposed to online solve temporally-variant LSE (linear simultaneous equation). The proposed GAGZNS utilizes both gradient information and derivative information, and thus can materialize a faster FTC (finite-time convergence) as compared with the ZNS. The property of FTC and the corresponding upper bound of convergence time are derived through strict theoretical proof and verified through two simulation examples. Finally, on the basis of the AoA (angle-of-arrival) technology, we conduct another example of mobile object localization to exhibit the practicality of the proposed GAGZNS.},
  archive      = {J_ASOC},
  author       = {Zhiguo Tan and Yunong Zhang},
  doi          = {10.1016/j.asoc.2025.112695},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112695},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Finite-time convergent gradient-zeroing neurodynamic system for solving temporally-variant linear simultaneous equation},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An off-policy maximum entropy deep reinforcement learning method for data-driven secondary frequency control of island microgrid. <em>ASOC</em>, <em>170</em>, 112694. (<a href='https://doi.org/10.1016/j.asoc.2025.112694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In islanded microgrids integrating energy storage and renewable sources, achieving optimal frequency control often involves balancing performance and cost, where improved control typically incurs higher expenses. This study introduces a Data-Driven Secondary Frequency Control (DD-SFC) approach that incorporates the Off-Policy Maximum Entropy Deep Reinforcement Learning (OME-DRL) algorithm to enhance both frequency control performance and cost efficiency. The OME-DRL algorithm mitigates issues related to high sample complexity and poor convergence in conventional methods by utilizing Maximum Entropy exploration, which bolsters the system's robustness and decision-making accuracy. The proposed DD-SFC method, validated on the Southern Power Grid microgrid model, demonstrates superior performance over 14 competing algorithms, achieving a reduction in frequency deviation by up to 3.7 times and a decrease in generation costs by 0.017 %. These results highlight the method's scalability and practical effectiveness in real-world applications.},
  archive      = {J_ASOC},
  author       = {Xiangmin Huang and Jun Zeng and Tianlun Wang and Shunqi Zeng},
  doi          = {10.1016/j.asoc.2025.112694},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112694},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An off-policy maximum entropy deep reinforcement learning method for data-driven secondary frequency control of island microgrid},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-stage jaya algorithm for optimizing valve openings in heating system networks. <em>ASOC</em>, <em>170</em>, 112693. (<a href='https://doi.org/10.1016/j.asoc.2025.112693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the improvement of people's living standards, more and more industries are focusing on enhancing user satisfaction. However, research on optimizing the balancing valve openings in secondary heating networks to improve user satisfaction is relatively scarce. To address this gap, this paper aims to maximize user satisfaction by solving the balancing valve opening adjustment (BVOA) problem. For this problem, a mixed-integer linear programming model is established, and the dual-stage Jaya (DJaya) algorithm is proposed. In DJaya, a problem-based random initialization method is introduced to generate high-quality solutions, followed by the development of a dynamic sculpting stage to enhance the global search capability of the algorithm. Additionally, an enhance core stage is designed to optimize the low-quality parts of the solution, thereby improving the overall quality of the solution. Finally, extensive experiments with different heating scales and numbers of balancing valves demonstrate the effectiveness of DJaya in solving the BVOA problem. Adjusting the balancing valves based on the solutions obtained by the DJaya algorithm can, on average, improve user satisfaction by approximately 7.89 %.},
  archive      = {J_ASOC},
  author       = {Chao Xu and Yangli Jia and Hongyan Sang and Leilei Meng and Biao Zhang and Wenqiang Zou},
  doi          = {10.1016/j.asoc.2025.112693},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112693},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-stage jaya algorithm for optimizing valve openings in heating system networks},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decreasing adversarial transferability using gradient information of attack paths. <em>ASOC</em>, <em>170</em>, 112692. (<a href='https://doi.org/10.1016/j.asoc.2025.112692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial transferability is an intriguing yet dangerous property of deep neural networks (DNNs), enabling the potential for black-box adversarial attacks. To better safeguard DNN-based AI models against such attacks, adversarial defense is explored through the lens of reducing the transferability of adversarial examples. From the perspective of decision boundaries in convolutional neural network classification models, we propose a novel method called A dversarial P ath T ransferability R educed ( APTR ). This method is grounded in rigorous theoretical derivation and empirical validation, utilizing an untrained surrogate model as a reference. During training, the surrogate model and the target model are jointly trained using cross-entropy loss, allowing the target model to learn to maintain an orthogonal decision boundary relative to that of the surrogate model while preserving classification performance. This approach effectively cuts off adversarial transferability from unknown networks. Extensive experiments on CIFAR-10, SVHN, and DeepFake datasets demonstrate that APTR outperforms three state-of-the-art (SOTA) optimization baselines against seven black-box attacks. Notably, the results indicate that networks trained with a surrogate model of one architecture can also defend against black-box attacks from models of different architectures, where the APTR method exhibits black-box defense capabilities comparable to adversarial training without the trade-off problem of adversarial training.},
  archive      = {J_ASOC},
  author       = {Mengjun Xu and Lei Liu and Pengfei Xia and Ziqiang Li and Bin Li},
  doi          = {10.1016/j.asoc.2025.112692},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112692},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decreasing adversarial transferability using gradient information of attack paths},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metaheuristic search algorithms in real-time charge scheduling optimisation: A suite of benchmark problems and research on stability-analysis. <em>ASOC</em>, <em>170</em>, 112691. (<a href='https://doi.org/10.1016/j.asoc.2025.112691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most important challenges in the optimization of real-time charging scheduling (CS) problems are (i) the need to model CS problems with a large number of decision variables for precise control, (ii) the increase in computational complexity with the high penetration of electric vehicles, and (iii) the lack of research on the stability and computation time of optimization algorithms on CS problems. In this paper, we design a real-time model and introduce the CS Benchmark Problems (CSBP) suite of twelve problems of four different types. Furthermore, a driver satisfaction model is introduced for the first time to analyse the impact of the results on user satisfaction. Best known solutions for all problems in CSBP are presented for the first time in this study. According to the statistical analysis results, the three competitive algorithms among 66 competitors in the optimization of CSs are LSHADE-CnEpSin, LSHADE-SPACMA and LRFDB-COA. Stability and computational complexity analyses revealed that LSHADE-SPACMA is the most successful algorithm for problems where consumers outnumber prosumer and LRFDB-COA is the most successful algorithm for problems where consumers equal or exceed prosumer. When the performance of the algorithms is evaluated regardless of the problem type, LSHADE-Spacma is the most stable algorithm with an overall success rate of 100 % on CSs. In addition, the average peak load shaving for the best known solutions of the algorithms with the highest success rate for each problem is calculated to be 94.84 %, and the average satisfaction score for all drivers is calculated to be 0.81.},
  archive      = {J_ASOC},
  author       = {Furkan Üstünsoy and H.Hüseyin Sayan and Hamdi Tolga Kahraman},
  doi          = {10.1016/j.asoc.2025.112691},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112691},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristic search algorithms in real-time charge scheduling optimisation: A suite of benchmark problems and research on stability-analysis},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering branch-and-bound algorithms via reinforcement and imitation learning for enhanced search. <em>ASOC</em>, <em>170</em>, 112690. (<a href='https://doi.org/10.1016/j.asoc.2025.112690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solution solvers rely on sophisticated heuristics honed through decades of research to tackle various instances of combinatorial optimization (CO) encountered in practical scenarios. Recent studies demonstrate that leveraging the data’s shared internal structure of instances allows reinforcement learning (RL) to achieve state-of-the-art performance in mastering advanced heuristics. However, existing RL-based algorithms depend on trial-and-error and extensive reward engineering tailored to individual data, leading to time-consuming and inefficient real-world applications. Moreover, these methods, incorporating construction heuristics, still fall short in accuracy compared to most solution solvers, limiting their practical utility. This paper introduces RAIL* , a novel framework that integrates RL and generative adversarial imitation learning (GAIL) to address the challenge by searching in branch-and-bound (B&B) algorithms. RAIL* adopts a policy architecture with dual decoders, aligning with RL’s sequence decoding and GAIL’s edge decoding. RL provides expert trajectories for GAIL to imitate, while GAIL utilizes the learned reward function to reciprocate to RL. The collaboration between the two components facilitates iterative improvements in the learned policy and reward function by complementing and constraining each other.},
  archive      = {J_ASOC},
  author       = {Qi Wang},
  doi          = {10.1016/j.asoc.2025.112690},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112690},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Empowering branch-and-bound algorithms via reinforcement and imitation learning for enhanced search},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-dimensional co-evolutionary algorithm for multi-objective resource-constrained flexible flowshop with robotic transportation. <em>ASOC</em>, <em>170</em>, 112689. (<a href='https://doi.org/10.1016/j.asoc.2024.112689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a realistic flexible or hybrid flowshop scheduling problem (HFS) is investigated, in which the following constraints are embedded, i.e., resource-dependent processing, robotic arm loading, and transportation. To solve the considered problem, a multi-dimensional co-evolutionary algorithm (MDCEA) is proposed to minimize makespan and total energy consumption (TEC) simultaneously. First, in the MDCEA, solutions are encoded by a three-dimensional vector with a two-phase decoding heuristic. Then, the initialized population is divided into three subsets to focus on different search tasks. To improve the efficiency of the global search task, a dual-population-based variable dimension cooperative search method is developed. In addition, to explore the promising non-dominated solutions in different dimensions, a Q-learning-based dimension detection search method is designed for the local search task. Finally, to keep the diversity in the evolutionary process, a knowledge-based individual transfer strategy is conducted for populations. The proposed algorithm was tested on 25 randomly generated instances, and detailed comparisons verified the efficiency and robustness compared to six state-of-the-art algorithms was achieved.},
  archive      = {J_ASOC},
  author       = {Jia-ke Li and Rong-hao Li and Jun-qing Li and Xin Yu and Ying Xu},
  doi          = {10.1016/j.asoc.2024.112689},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112689},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-dimensional co-evolutionary algorithm for multi-objective resource-constrained flexible flowshop with robotic transportation},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural network with strong interpretability for solving optimization problems. <em>ASOC</em>, <em>170</em>, 112688. (<a href='https://doi.org/10.1016/j.asoc.2024.112688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and rapid solutions to optimization problems are crucial in scientific and engineering fields. To address diverse optimization challenges, this paper proposes an Optimization Problem Solving Network (OPSN), a novel neural network with strong interpretability. By introducing the Lagrange multiplier method and Karush–Kuhn–Tucker (KKT) conditions, the theoretical conditions that must be satisfied to obtain the optimal solution are derived. Based on these conditions, OPSN is developed. The network structure of OPSN is similar to that of the backpropagation (BP) neural network; however, the forward computation formula is redesigned to incorporate the constraint information inherent in the optimization problem. By introducing the penalty coefficient, the activation functions of the network nodes are redefined to assess whether the constraints are satisfied. Additionally, a special node is added to capture information about the objective function. To reduce the adjustable parameters, the network input is fixed at 1, and the normalization algorithm has been developed based on the search range of each variable. The network is initialized with random weights as starting values. Extensive testing on 15 CEC2017 benchmark functions and six real-world engineering problems demonstrates OPSN’s superior performance, faster convergence, and broad applicability compared to five established optimization algorithms.},
  archive      = {J_ASOC},
  author       = {Jianhan Fan and Yiming Liao and Jianxiao Zou and Kaiji Liao and Shicai Fan},
  doi          = {10.1016/j.asoc.2024.112688},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112688},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A neural network with strong interpretability for solving optimization problems},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-output extended belief rule-base system and its parameter learning schemes. <em>ASOC</em>, <em>170</em>, 112687. (<a href='https://doi.org/10.1016/j.asoc.2024.112687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an advanced rule-based system, the extended belief rule-base (EBRB) system with single output has been applied in various areas due to its flexibility in knowledge representation. However, multi-output problems have not been adequately addressed in existing EBRB systems, although these problems are not unusual. In this study, an innovative multi-output EBRB (MO-EBRB) system is proposed with a unique inference scheme to handle multi-output problems. Also, a parameter learning scheme is designed to determine optimal parameter values in MO-EBRB system by constructing a multi-objective optimization model. The TOPSIS technique is used to select the optimal solution from a set of Pareto-optimal solutions generated by a nondominated sorting genetic algorithm. The effectiveness of the proposed system is demonstrated through its application in the auxiliary diagnosis of thyroid nodules. Comparison experiments indicate that the proposed MO-EBRB system could provide more accurate inference findings for the diagnosis of thyroid nodules compared to single output EBRB systems and other multi-output methods.},
  archive      = {J_ASOC},
  author       = {Bingbing Hou and Min Xue and Jun Liu and Zijian Wu},
  doi          = {10.1016/j.asoc.2024.112687},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112687},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-output extended belief rule-base system and its parameter learning schemes},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Motor imagery EEG signals classification using a transformer-GCN approach. <em>ASOC</em>, <em>170</em>, 112686. (<a href='https://doi.org/10.1016/j.asoc.2024.112686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Brain-Computer Interface (BCI) serves as a vital link between the brain and both internal and external environments, with broad applications in medicine and rehabilitation. Motor Imagery Electroencephalogram (MI-EEG) signals, which capture brain activity during motor imagery tasks, are particularly advantageous due to their spontaneous nature and high temporal resolution. While recent studies have advanced the classification of MI-EEG signals, developing a unified model that incorporates all relevant aspects remains a critical challenge. This study aims to construct a comprehensive model by leveraging the unique features of MI-EEG signals. We propose a hybrid approach combining Transformer and Graph Convolutional Network (GCN) techniques to enhance MI-EEG signal classification. The Transformer model is employed for pretraining to emphasize temporal dynamics, while the GCN is designed to capture spatial dependencies. Furthermore, we explore various GCN architectures that integrate frequency and temporal information within the node embeddings. Our method achieves a subject-level average classification accuracy of 97.43 % on the Physionet dataset, outperforming recent models. These findings underscore the importance of integrating frequency, phase, spatial, and temporal information, demonstrating a significant improvement in MI-EEG signal classification accuracy.},
  archive      = {J_ASOC},
  author       = {Arezoo Hamidi and Kourosh Kiani},
  doi          = {10.1016/j.asoc.2024.112686},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112686},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Motor imagery EEG signals classification using a transformer-GCN approach},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TFGNet: Frequency-guided saliency detection for complex scenes. <em>ASOC</em>, <em>170</em>, 112685. (<a href='https://doi.org/10.1016/j.asoc.2024.112685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) with accurate boundaries in complex and chaotic natural or social scenes remains a significant challenge. Many edge-aware or/and two-branch models rely on exchanging global and local information between multistage features, which can propagate errors and lead to incorrect predictions. To address this issue, this work explores the fundamental problems in current U-Net architecture-based SOD models from the perspective of image spatial frequency decomposition and synthesis. A concise and efficient Frequency-Guided Network (TFGNet) is proposed that simultaneously learns the boundary details (high-spatial frequency) and inner regions (low-spatial frequency) of salient regions in two separate branches. Each branch utilizes a Multiscale Frequency Feature Enhancement (FFE) module to learn pixel-wise frequency features and a Transformer-based decoder to learn mask-wise frequency features, improving a comprehensive understanding of salient regions. TFGNet eliminates the need to exchange global and local features at intermediate layers of the two branches, thereby reducing interference from erroneous information. A hybrid loss function is also proposed to combine BCE, IoU, and Histogram dissimilarity to ensure pixel accuracy, structural integrity, and frequency distribution consistency between ground truth and predicted saliency maps. Comprehensive evaluations have been conducted on five widely used SOD datasets and one underwater SOD dataset, demonstrating the superior performance of TFGNet compared to state-of-the-art methods. The codes and results are available at https://github.com/yiwangtz/TFGNet .},
  archive      = {J_ASOC},
  author       = {Yi Wang and Ruili Wang and Juncheng Liu and Rui Xu and Tianzhu Wang and Feng Hou and Bin Liu and Na Lei},
  doi          = {10.1016/j.asoc.2024.112685},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112685},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TFGNet: Frequency-guided saliency detection for complex scenes},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A skin lesion segmentation network with edge and body fusion. <em>ASOC</em>, <em>170</em>, 112683. (<a href='https://doi.org/10.1016/j.asoc.2024.112683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Purpose: In this paper, a novel local cross-attention Unet(LCAUnet) is proposed to enhance the completeness of representation with the fusion of edge and body features, which are often paid little attention in traditional methods. Method: First, two separate branches are set for edge and body segmentation with convolutional neural networks(CNNs) and Transformer based architectures, respectively. Then, the local cross-attention feature fusion(LCAF) module is utilized to merge feature maps of the edge and body of the same level via local cross-attention operation in the encoder stage, and the edge-body interactions can be captured hierarchically. Furthermore, the prior guided multi-scale knowledge fusion(PGMF) module is embedded for feature integration with prior guided multi-scale adaption. Result: Comprehensive experiments on publicly available datasets ISIC 2017, ISIC 2018, and PH2 demonstrate that LCAUnet outperforms most state-of-the-art methods in metrics such as 1.31% improvement in Dice. The ablation studies also verify the effectiveness of the proposed fusion techniques.},
  archive      = {J_ASOC},
  author       = {Gao Wang and Qisen Ma and Yiyang Li and Keming Mao and Lisheng Xu and Yuhai Zhao},
  doi          = {10.1016/j.asoc.2024.112683},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112683},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A skin lesion segmentation network with edge and body fusion},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex-valued random vector functional link neural network based on real augmented representation and its applications. <em>ASOC</em>, <em>170</em>, 112682. (<a href='https://doi.org/10.1016/j.asoc.2024.112682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random vector functional link neural network (RVFL) is an important noniterative neural model due to its fast learning speed, universal approximation capability, and excellent scalability. However, the traditional real-valued RVFL usually leads to suboptimal solutions in complex-valued signal processing as it cannot fully capture the correlations between the real and imaginary channels and fails to make use of complex statistics. To this end, this paper develops a framework for complex-valued RVFL (CRVFL) towards complex-valued signal processing. The framework jointly considers the statistics of complex-valued signals, incremental learning, and computational efficiency. To fully capture second-order statistics, we propose an augmented CRVFL (ACRVFL) by integrating conjugates in the input and hidden layers. An incremental ACRVFL with growing hidden neurons is then developed to obtain the optimal network structure. To address computational challenges with augmentation, we propose the CRVFL based on real augmented representation (CRVFL-RA) and establish its equivalence to ACRVFL. Computational complexity analysis shows that CRVFL-RA reduces multiplication operations by 75% and addition operations by more than 50% compared to ACRVFL. Simulation results on complex-valued signal processing and classification tasks confirm both the efficiency of the proposed algorithms and the contribution of the direct links between the input layer and output layer of CRVFL.},
  archive      = {J_ASOC},
  author       = {Chunyang Liu and Huisheng Zhang and Luyao Chen and Feng Li},
  doi          = {10.1016/j.asoc.2024.112682},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112682},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Complex-valued random vector functional link neural network based on real augmented representation and its applications},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-type radar deployment for UAV swarms defense coverage using firework algorithm with determinantal point processes under complex terrain. <em>ASOC</em>, <em>170</em>, 112681. (<a href='https://doi.org/10.1016/j.asoc.2024.112681'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of UAV swarm technology has been becoming increasingly important in modern and future warfare. The low-small-slow characteristics of UAV swarms pose significant challenges to ground and air defense systems, especially in complex terrain conditions. A critical issue in this context is the defense coverage provided by multi-type radar systems, which is influenced by factors such as deployment environments, deployment algorithms, etc. Despite its importance, this topic has received limited attention in existing research. To address this gap, we first model radar coverage based on terrain constraints and define two metrics, the space coverage ratio and the height-level coverage ratio. These metrics are used to evaluate the effectiveness of multi-type radar coverage under complex terrain. Also, we present the optimization problem for multi-type radar deployment by dividing the defense region into the alert area (AA) and the priority detection area (PDA). Then, we propose a novel Fireworks Algorithm (FWA), named DPP-FWA, which incorporates the Determinantal Point Process (DPP) in its selection strategy for fireworks. This approach balances the quality and diversity of the fireworks, enhancing the effectiveness of the selection process. Finally, simulation results based on ten benchmark functions and multi-type radar deployment scenarios indicate that the proposed DPP-FWA outperforms the Enhanced Fireworks Algorithm (EFWA), Particle Swarm Optimization, and Genetic Algorithm in terms of stability, convergence speed, and accuracy when using SRTM (Shuttle Radar Topography Mission) terrain data. Notably, the results demonstrate that DPP-FWA achieves a high defense coverage ratio ( > 90%). Furthermore, the analysis reveals that the algorithm’s complexity is acceptable. In conclusion, the proposed DPP-FWA effectively meets the UAV swarm defense coverage requirements for multi-type radar deployment under complex terrain, providing a valuable foundation for such defense strategies.},
  archive      = {J_ASOC},
  author       = {Ruxuan Ding and Shengbo Hu and Zehua Xing and Tingting Yan},
  doi          = {10.1016/j.asoc.2024.112681},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112681},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-type radar deployment for UAV swarms defense coverage using firework algorithm with determinantal point processes under complex terrain},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised discovery of 3D structural elements for scanned indoor scenes. <em>ASOC</em>, <em>170</em>, 112680. (<a href='https://doi.org/10.1016/j.asoc.2024.112680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the growing demand for effective 3D sensing applications by presenting a comprehensive point cloud segmentation method developed for large indoor spaces. Our approach recognises the challenges associated with (un)ordered data and presents a robust algorithm capable of dealing with irregularities caused by measurement inaccuracies, e.g. occlusion, noise, outliers and discontinuous data transitions. The method uses a multi-step filtering approach that sequentially navigates through Gaussian map, distance space and regular grid representations. Connected component analysis, structural rules and assumptions guide the unsupervised clustering of structural elements (SEs), e.g. walls, ceilings and floors. The method is adaptable to various datasets, including joint 2D-3D datasets such as true RGB-D data. A colour metric is introduced to account for illumination effects during scanning and to ensure the generalisability of the method. The importance of detecting SEs lies in their role as input to deep neural networks, which improve the accuracy of SLAM algorithms and influence the quality of subsequent indoor residual object detection. This paper introduces density-based clustering of objects using colour similarity measures and low-level features to further refine the segmentation by eliminating outliers and improving the detection of sharp shapes. The proposed method represents a sophisticated and versatile solution that overcomes scene complexity and makes an important contribution to applications in scene understanding, SLAM and indoor object recognition.},
  archive      = {J_ASOC},
  author       = {Miloš Antić and Andrej Zdešar and José Antonio Iglesias and Araceli Sanchis and Igor Škrjanc},
  doi          = {10.1016/j.asoc.2024.112680},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112680},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised discovery of 3D structural elements for scanned indoor scenes},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobjective multi-space collaboration model for addressing spectral variability in hyperspectral image unmixing. <em>ASOC</em>, <em>170</em>, 112679. (<a href='https://doi.org/10.1016/j.asoc.2024.112679'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral remote sensing image unmixing poses a significant challenge, especially in the precise identification of pure spectral signatures (endmembers). This identification is a multifaceted optimization problem, often leading to locally optimal solutions given the inherent complexity and high dimensionality of hyperspectral images. A single objective function proves insufficient to model the rich spectral variability of endmembers. Recent advances in multiobjective evolutionary optimization for endmember bundle extraction (EBE) have illuminated the potential of multiobjective optimization techniques to address spectral variability. Yet, issues like repeated endmembers and high dimensionality have not been adequately addressed in the evolutionary process. This article introduces the Multiobjective Multi-Space Collaboration (MOMSC) model, coupled with a multiobjective genetic algorithm, to bridge these gaps. Within MOMSC, a novel multiple subspace generation strategy is devised, targeting the unveiling of spectral diversity across varied feature spaces and enhancing the capture of total spectral variability. Trios of these generated subspaces are integrated to form a multiobjective EBE framework. Given the unique nature of endmembers, tailored strategies in the genetic algorithm, such as gene segment pool allocation and spatial crowding distance calculation, are proposed to counteract high dimensionality. An innovative replacement mechanism is also proposed, refining the search space for subsequent subspace groups, circumventing the repeated endmember dilemma, and bolstering the odds of acquiring more endmember variabilities. Experimental results from three real hyperspectral images validate the effectiveness of our proposed MOMSC approach.},
  archive      = {J_ASOC},
  author       = {Pengrui Wang and Linfu Xie and Xiaoqiong Qin and Rong Liu},
  doi          = {10.1016/j.asoc.2024.112679},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112679},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiobjective multi-space collaboration model for addressing spectral variability in hyperspectral image unmixing},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence for accurate classification of respiratory abnormality levels using image-based features and interpretable insights. <em>ASOC</em>, <em>170</em>, 112678. (<a href='https://doi.org/10.1016/j.asoc.2024.112678'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of respiratory abnormality levels is crucial for early detection and diagnosis of respiratory diseases, making it a pivotal area in the field of medical diagnostics. This study proposes a novel artificial intelligence approach for accurate classification of respiratory abnormality levels. By transforming respiratory sound time-series data into image representations using recurrent plot, Markov transition field, and Gramian angular field, we capture intricate temporal patterns and spatial relationships. A deep neural network autonomously extracts discriminative features from these representations, subsequently integrated into machine learning classifiers. Leveraging the International Conference on Biomedical and Health Informatics (ICBHI) database, our methodology achieves remarkable classification accuracy of 100% for both binary and four-class scenarios, accurately distinguishing normal from abnormal sounds, and discriminating between crackles, wheezes, and their combinations. The SHapley Additive exPlanations (SHAP) method enhances interpretability, providing insights into feature importance and decision-making processes. This interpretable and high-performing approach offers significant promise for enhancing the accuracy and reliability of respiratory disorder diagnosis and treatment planning in clinical settings, potentially improving patient outcomes and healthcare efficiency.},
  archive      = {J_ASOC},
  author       = {Wei Zeng and Liangmin Shan and Qinghui Wang and Fenglin Liu and Ying Wang and Chengzhi Yuan and Shaoyi Du},
  doi          = {10.1016/j.asoc.2024.112678},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112678},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Artificial intelligence for accurate classification of respiratory abnormality levels using image-based features and interpretable insights},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep generative approaches for oversampling in imbalanced data classification problems: A comprehensive review and comparative analysis. <em>ASOC</em>, <em>170</em>, 112677. (<a href='https://doi.org/10.1016/j.asoc.2024.112677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are inherent issues with classifying imbalanced data, especially in classifying minority class samples. With an emphasis on the use of deep generative methodologies, this study offers a thorough investigation of oversampling strategies for imbalanced data classification. This paper begins with a summary of unbalanced data categorization and the need for oversampling techniques. Then traditional approaches including SMOTE, ADASYN, and random oversampling are introduced and discussed. This study then discusses deep generative models and how oversampling may be used to address imbalanced data problem using Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). A comparative study between deep generative and conventional oversampling techniques is performed concerning a comprehensive evaluation of the difficulties, restrictions, and possible risks associated with applying deep generative approaches. The paper concludes with recommendations for future researches and highlights the need for addressing challenges in oversampling approaches for imbalanced data classification.},
  archive      = {J_ASOC},
  author       = {Mozafar Hayaeian Shirvan and Mohammad Hossein Moattar and Mehdi Hosseinzadeh},
  doi          = {10.1016/j.asoc.2024.112677},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112677},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep generative approaches for oversampling in imbalanced data classification problems: A comprehensive review and comparative analysis},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight PolSAR image classification algorithm based on multi-scale feature extraction and local spatial information perception. <em>ASOC</em>, <em>170</em>, 112676. (<a href='https://doi.org/10.1016/j.asoc.2024.112676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with optical images, polarimetric synthetic aperture radar (PolSAR) images have richer feature information. However, traditional convolutional neural networks (CNNs) tend to extract more redundant information when processing features from PolSAR data. This results in weak network performance and difficulty in applying to actual scenarios. To solve this problem, this paper proposes a new lightweight neural network, Ghost-Inception and coordinate attention network (GICANet), for the classification of PolSAR images. First, in view of the complex scattering mechanism of PolSAR images, GICANet uses ghost convolution instead of standard convolution, and builds a Ghost-Inception module to achieve multi-scale feature extraction and reduce redundant information extraction. Second, GICANet designs a new mean-variance coordinated coordinate attention mechanism, which enhances the network’s perception of spatial information and local pixel positions to make it more sensitive to the local texture information of PolSAR data. Finally, GICANet uses attention feature enhancement (AFE) to fuse the shallow features of PloSAR data with deep features. And enhanced in the attention module to capture pixel-level information of images more effectively. Compared with traditional CNN, GICANet is more lightweight, with network parameters and calculation volume reduced by 87.72 % and 74.20 % respectively. Experimental results with six state-of-the-art algorithms on four PolSAR image classification datasets show that GICANet achieves better experimental results.},
  archive      = {J_ASOC},
  author       = {Ronghua Shang and Mingwei Hu and Jie Feng and Weitong Zhang and Songhua Xu},
  doi          = {10.1016/j.asoc.2024.112676},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112676},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight PolSAR image classification algorithm based on multi-scale feature extraction and local spatial information perception},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic block size optimization in the LOF algorithm for efficient anomaly detection. <em>ASOC</em>, <em>170</em>, 112675. (<a href='https://doi.org/10.1016/j.asoc.2024.112675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in data is crucial across various fields, including finance and healthcare. The Local Outlier Factor (LOF) algorithm is frequently employed to identify atypical data points. In this article, we continue our investigation into block size optimization, presenting an innovative approach that accelerates the anomaly detection process while preserving high accuracy. Throughout this research, we introduce a score objective function used for optimizing the anomaly detection process. The score comprises three key components: execution time, percentage coverage, and the sum of distances within the analysed data block. We intentionally minimize execution time and the sum of distances while maximizing percentage coverage, leading to process efficiency and improved compactness of the data block. In our previous research, we proposed an innovative approach that utilizes block size optimization to accelerate the anomaly detection process while maintaining high accuracy. Additionally, we propose an algorithm for calculating the optimal block size, which adapts to the number of data points N , available RAM, and the parameters a , b , c , and k . The parameters a and b influence scaling the constant c , which determines the impact of N on the block size, while k specifies the optimal number of nearest neighbours, introducing flexibility into the anomaly detection process. Our goal is to optimize block size for efficient resource utilization and effective data processing. Integrating these two aspects enables efficient anomaly detection in large datasets while effectively managing hardware resources. The results of our experiments conducted on various datasets unequivocally confirm the effectiveness of our approach, which not only enhances the performance of the LOF algorithm but also maintains high anomaly detection efficiency.},
  archive      = {J_ASOC},
  author       = {Czesław Horyń and Agnieszka Nowak-Brzezińska},
  doi          = {10.1016/j.asoc.2024.112675},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112675},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic block size optimization in the LOF algorithm for efficient anomaly detection},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary induced survival trees for medical prognosis assessment. <em>ASOC</em>, <em>170</em>, 112674. (<a href='https://doi.org/10.1016/j.asoc.2024.112674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis focuses on predicting the time of a specific event, known as failure. In the analysis of survival data, it is crucial to fully leverage censored observations for which we do not have precise event time information. Decision trees are among the most frequently applied machine learning techniques for survival analysis, but to adequately address this issue, it is necessary to transform them into survival trees. This involves equipping the leaves with, for instance, local Kaplan–Meier estimators. Until now, survival trees have predominantly been generated using a greedy approach through classical top-down induction that uses local optimization. Recently, one of the most promising directions in decision tree approach is global learning. The paper proposes an evolutionary algorithm for survival tree induction, which concurrently searches for the tree structure, univariate tests in internal nodes, and Kaplan–Meier estimators in leaves. The fitness function is based on an integrated Brier score, and by introducing a penalty term related to the tree size, it becomes possible to control the interpretability of the obtained predictor. The work investigated, among other aspects, the impact of censoring, and the results obtained from both synthetic and real-life medical datasets are encouraging. The comparison of the predictive ability of the proposed method with already-known univariate survival trees shows statistically significant differences.},
  archive      = {J_ASOC},
  author       = {Malgorzata Kretowska and Marek Kretowski},
  doi          = {10.1016/j.asoc.2024.112674},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112674},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary induced survival trees for medical prognosis assessment},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph convolutional neural networks for clinical diagnosis of monkeypox infections using skin virological images. <em>ASOC</em>, <em>170</em>, 112673. (<a href='https://doi.org/10.1016/j.asoc.2024.112673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Monkeypox virus (Mpoxv), characterized by its distinct vesiculopustular rash, has re-emerged as a significant zoonotic pathogen, posing severe public health risks and potential bioterrorism threats. Although less virulent than smallpox, the persistence of Mpoxv infections necessitates advanced diagnostic tools and proactive mitigation strategies. Dermatological virological imaging is significant for automatic Mpoxv detection and classification, yet its fidelity is often compromised by low-resolution data, mainly in the incipient stages of the infection. Conventional deep learning models mostly struggle to capture higher-order dependencies and complex feature interactions within virological images, leading to suboptimal outcomes. An advanced hybrid hypergraph convolutional neural networks (HGCNs) architecture is introduced in response. In this architecture hypergraph effectively models intricate correlations and enables the detect subtle patterns. At the same time, CNN components contribute robust feature extraction, refined through relational modeling, leads optimal detection and classification of Mpoxv infection. The HGCNs were trained and validated using two different validation approaches, including the Holdout method (HM) and a stratified 3-fold cross-validation (3-FCV), yielding HM accuracy of 0.9888, precision of 0.9813, recall of 0.9958, F1 Score of 0.9885, specificity of 0.9890, Micro AUC of 0.9892, and an average time per epoch of 0.5512 s, while 3-FCV achieved an average accuracy of 0.9917, precision of 0.9931, recall of 0.9912, F1 score of 0.9922, specificity of 0.9941, Micro AUC of 0.9903, and an average time per epoch of 0.6151 s. Furthermore, the use of Grad-CAM facilitates precise localization of infected regions within the images. The performance highlights the proposed model’s effectiveness as a powerful tool in computational virology, delivering high accuracy and interpretable diagnostics for Mpoxv infections. Data availability The dataset is freely available online.},
  archive      = {J_ASOC},
  author       = {Sajid Hussain and Xu Songhua and Muhammad Usman Aslam and Muhammad Waqas and Fida Hussain},
  doi          = {10.1016/j.asoc.2024.112673},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112673},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hypergraph convolutional neural networks for clinical diagnosis of monkeypox infections using skin virological images},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From overfitting to robustness: Quantity, quality, and variety oriented negative sample selection in graph contrastive learning. <em>ASOC</em>, <em>170</em>, 112672. (<a href='https://doi.org/10.1016/j.asoc.2024.112672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning (GCL) aims to contrast positive–negative counterparts to learn the node embeddings, whereas graph data augmentation methods are employed to generate these positive–negative samples. The quantity, quality, and variety of negative samples relative to positive samples play crucial roles in learning meaningful embeddings for node classification downstream tasks. Excessive quantity, low-quality, and less variation of negative samples cause the model to be overfitted for particular nodes, resulting in less robust models. To solve the overfitting problem in the GCL paradigm, this study proposes a novel Cumulative Sample Selection (CSS) algorithm by comprehensively considering the quantity, quality, and variety of negative samples. Initially, three negative sample pools are constructed: easy, medium, and hard negative samples, which contain 25%, 50%, and 25% of the total available negative samples, respectively. Later, 10% negative samples are selected from each of these three sample pools for training the model. After that, a decision agent module evaluates the model training results and decides whether to explore more negative samples from the three negative sample pools by increasing the ratio or to continue exploiting the current sampling ratio. The proposed algorithm is integrated into a proposed graph contrastive learning framework named NegAmplify ( Neg ative samples Amplifi cation). NegAmplify is compared to the state-of-the-art methods on nine node classification datasets, with seven achieving better node classification accuracy with up to 2.86% improvement. The implementation of NegAmplify is available at https://github.com//mhadnanali/NegAmplify .},
  archive      = {J_ASOC},
  author       = {Adnan Ali and Jinlong Li and Huanhuan Chen and Ali Kashif Bashir},
  doi          = {10.1016/j.asoc.2024.112672},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112672},
  shortjournal = {Appl. Soft. Comput.},
  title        = {From overfitting to robustness: Quantity, quality, and variety oriented negative sample selection in graph contrastive learning},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image inpainting based on CNN-transformer framework via structure and texture restoration. <em>ASOC</em>, <em>170</em>, 112671. (<a href='https://doi.org/10.1016/j.asoc.2024.112671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both coherent structure and fine texture are of great importance in image inpainting. However, existing methods fail to consider both structure and texture simultaneously, resulting in discontinuous edges, blurred details and artifacts of inpainting results. In addition, Convolutional Neural Network (CNN) suffers from its limited modeling capability for long-range dependencies. To address these issues, we propose a novel method called Image inpainting based on CNN-Transformer framework via structure and texture restoration (CTSTNet). In CTSTNet, the Structure Reconstruction Network (SRN) restores the overall structure of the damaged image by leveraging its ability to model long dependencies. Meanwhile, the Texture Reconstruction Network (TRN) restores the grayscale map and edge map to generate the texture of the damaged image. Finally, Image Completion Network (ICN) fuses the structure and texture using a Context Aggregation Module (CAM) to generate the inpainting result. Experimental results on two public datasets CelebA, Paris StreetView, and our constructed mural dataset MuralCN, demonstrate that CTSTNet achieves significant improvements in evaluation metrics and visual quality compared to other related methods. This indicates that our CTSTNet can effectively restore images and can also be applied in the field of cultural relic image restoration.},
  archive      = {J_ASOC},
  author       = {Zhan Li and Nan Han and Yuning Wang and Yanan Zhang and Jing Yan and Yingfei Du and Guohua Geng},
  doi          = {10.1016/j.asoc.2024.112671},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112671},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image inpainting based on CNN-transformer framework via structure and texture restoration},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian network ensemble models applied to seismic liquefaction prediction based on different in-situ test databases. <em>ASOC</em>, <em>170</em>, 112668. (<a href='https://doi.org/10.1016/j.asoc.2024.112668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of three different in-situ test databases of standard penetration test (SPT), cone penetration test (CPT), and shear velocity test (V s ) to build an ensemble seismic liquefaction prediction model using the Bayesian network (BN) method not only helps to improve the prediction accuracy but also overcomes the problem of conflicting prediction results from the three basic models based on different in-situ tests. However, there is no relevant research to investigate the performance impact of integrated modes on the ensemble BN model and to identify the optimal integrated mode in seismic liquefaction prediction. Therefore, this study first constructs different ensemble BN models using the existing three basic BN models (called the BN-SPT, BN-CPT, and BN-V s models) based on different in-situ test databases, and then investigates the effect of different integration modes such as converging, sequential, parallel, and hybrid on the performance of the ensemble BN models. Finally, several integration strategies are proposed for the construction of an ensemble BN model. After comparing these ensemble BN models with single basic BN models, the results show that not all integrated modes further improve the prediction accuracy of seismic liquefaction, while most of the ensemble BN models perform much better than the single basic BN models. It is verified that the fusion of three in-situ test data to construct an ensemble BN model is conducive to solving the problem of contradictory prediction results of individual BN models. Among the ensemble BN models, the parallel mode performs the best with 92.2 % for back-judgment accuracy and 87.6 % for prediction accuracy (improving the accuracy by 2.6 % and 4.0 %, respectively, compared to the best-performing BN-SPT model in the basic BN models), thus the researcher should prioritize the parallel mode when constructing the ensemble BN model. The sequential mode performs the second, and the hybrid mode is the worst with some hybrid models not even better than the basic BN models due to error propagation and contradictory results in the integration process. These causes are then discussed in detail. For the sequential or hybrid BN models, it is beneficial to place the basic BN models with better performance at the top of the ensemble model to obtain a better-performing ensemble model. When the training sample size is small, it is more important to focus on selecting basic models with fewer input variables, rather than selecting the basic models with high complexity and prediction accuracy. Furthermore, the parallel BN model composed of three basic BN models is comparable to the random forest (RF) models with 100 decision trees, and its complexity and computational time cost are much lower than the RF models. The findings of this paper can inform BN integration learning.},
  archive      = {J_ASOC},
  author       = {Wenjun Zou and Jilei Hu},
  doi          = {10.1016/j.asoc.2024.112668},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112668},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bayesian network ensemble models applied to seismic liquefaction prediction based on different in-situ test databases},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data-driven approach to microgrid fault detection and classification using taguchi-optimized CNNs and wavelet transform. <em>ASOC</em>, <em>170</em>, 112667. (<a href='https://doi.org/10.1016/j.asoc.2024.112667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of microgrids into the bulk power system introduces inherent uncertainties that challenge conventional protection systems, encompassing factors such as low fault currents, operational modes, penetration levels of renewable sources, load variations, and network topology. These uncertainties significantly impact the overall reliability of the electrical system. In the event of a fault occurrence within or external to the microgrid, swift disconnection from the primary grid is imperative. This disconnection is facilitated through the immediate operation of a static switch positioned proximate to the common coupling point. Such rapid action is essential to mitigate potential damages and expedite the restoration of electrical services. To ensure the delivery of reliable and high-quality energy to end consumers while alleviating stress on the utility grid, this paper introduces a novel methodology for the efficient detection, classification, and localization of faults in a microgrid cluster connected to the external grid. The proposed system addresses diverse irregular conditions, including conventional faults, high-impedance faults, islanding scenarios, and adverse events, covering several zones within the microgrid cluster and the external electrical grid. The proposed approach is based on a fusion of the Taguchi methodology and the discrete Wavelet transform. This combination enables the optimization of convolutional neural network training using scalograms generated from the fault signals. The results demonstrate the model’s high performance, achieving 99.25 % accuracy in fault localization and 99.13 % in fault detection and classification, all within less than 10 ms. In comparison, traditional methods like support vector machine and decision trees require over 16 ms with lower accuracy, underscoring the superior speed and precision of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Paul Arévalo and Antonio Cano and Olena Fedoseienko and Francisco Jurado},
  doi          = {10.1016/j.asoc.2024.112667},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112667},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A data-driven approach to microgrid fault detection and classification using taguchi-optimized CNNs and wavelet transform},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-occurrence graph convolutional networks with approximate entailment for knowledge graph embedding. <em>ASOC</em>, <em>170</em>, 112666. (<a href='https://doi.org/10.1016/j.asoc.2024.112666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of knowledge graph completion (KGC) is to address the issue of missing triples and enhance the overall completeness of the knowledge graph. However, existing methods face three key challenges: (1) Weak semantic correlation between entities and relations in the knowledge graph. (2) Insufficient extraction of local features in the model. (3) Limited ability to represent complex semantic relations. This paper proposes a G raph C onvolutional N etwork framework that leverages C o- o ccurrence features, local structural features, and A pproximate E ntailment (CoAE-GCN). The CoAE-GCN model is designed to overcome these challenges. The CoAE-GCN model addresses these issues by (1) enumerating the co-occurrence of entities and relations and using resulting weighted information as input for the model. (2) We employ Graph Neural Networks (GNNs) to learn structural features while using attention mechanisms to capture local structural features from incoming and outgoing neighbors. (3) We are applying approximate entailment to enhance the representational capacity of relations. Experimental results on benchmark datasets demonstrate that the CoAE-GCN model is outperformance and effective.},
  archive      = {J_ASOC},
  author       = {Dong Zhang and Wenhao Li and Tianbo Qiu and Guanyu Li},
  doi          = {10.1016/j.asoc.2024.112666},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112666},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Co-occurrence graph convolutional networks with approximate entailment for knowledge graph embedding},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating cumulative prospect theory into the graph model with application to the conflicts on discharging fukushima nuclear wastewater. <em>ASOC</em>, <em>170</em>, 112665. (<a href='https://doi.org/10.1016/j.asoc.2024.112665'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twelve years after the Fukushima nuclear accident, on August 24, 2023, the Fukushima Daiichi Nuclear Power Plant (FDNPP) in Japan officially started discharging contaminated water into the sea. Nuclear waste contains a large number of radioactive substances. This reckless discharge action will not only cause serious harm to human health but also damage the marine ecology, which directly causes conflicts between Japanese government and neighboring countries. As global uncertainty risks continue to increase, the most prominent problem is that decision-makers’ (DMs’) preferences in the conflict of nuclear wastewater discharge need to consider the influence of personal internal psychological behavior and external environment of conflict problems. It can be used to ensure maximization of interests of each stakeholder in conflicts. Thus, in this paper, a new preference elicitation technique is originally constructed in the framework of the graph model for conflict resolution (GMCR) to promptly and efficiently describe DM’s real preference relations from the perspective of cumulative prospect theory. Considering the influence of bounded rationality of DMs and external environment of conflict problems on focus DM’s preference ranking, an interval multi-attribute preference ranking method based on cumulative prospect theory is proposed in conflicts under complex conditions. Based on this, a set of new stability definitions is defined in GMCR based on cumulative prospect theory to obtain the optional solution of conflicts. As a result, four generated states are constructed in the conflicts on discharging Fukushima nuclear wastewater into the ocean to character DM’s strategies and options, and one feasible state “ s 1 ” is analyzed as stable state based on the proposed stability definitions and is used to assist DM in resolving conflict and achieving win-win situations. Finally, the optional solution represents that consultation and cooperation of all DMs is an effective way to avoid further escalation of conflicts on discharging Fukushima nuclear wastewater into the ocean. The innovation of this paper lies in the development of preference acquisition techniques and stability definitions in GMCR based on cumulative prospect theory, which can determine the DM’s preference ranking over all states in complex and uncertain environments and provide an effective tool for resolving conflicts on discharging Fukushima nuclear wastewater into the ocean.},
  archive      = {J_ASOC},
  author       = {Dayong Wang and Xiaoying Lai and Carlos Llopis-Albert and Xiaowei Wen and Yejun Xu},
  doi          = {10.1016/j.asoc.2024.112665},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112665},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating cumulative prospect theory into the graph model with application to the conflicts on discharging fukushima nuclear wastewater},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open continual sampling with hypersphere knowledge transfer for rapid feature selection. <em>ASOC</em>, <em>170</em>, 112664. (<a href='https://doi.org/10.1016/j.asoc.2024.112664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a widely used data preprocessing technique, but it still faces two major challenges: (1) data in open and dynamic environments may continually emerge unknown classes, and (2) the ever-growing scale of data. To address these challenges, this paper proposes a novel Open Continual Sampling (OCS) method that combines the advantages of continual learning and three-way sampling, aiming to discover unknown knowledge and transfer known knowledge. OCS can detect unknown classes by constructing a hypersphere knowledge base and sampling the most uncertain instances at each class decision boundary from the unknown data, thereby effectively reducing redundant sample computations. Based on OCS, we introduce a rapid feature selection framework (OCS-FS). Guided by the prior knowledge base, this framework rapidly calculates the importance of a small number of candidate features on representative samples, thereby incrementally selecting the optimal feature subset for the new data. After completing the learning process for the new period, the knowledge base is updated to reinforce old knowledge and integrate new knowledge. Extensive experiments on public benchmark datasets demonstrate that our method significantly outperforms existing state-of-the-art feature selection methods in both effectiveness and efficiency.},
  archive      = {J_ASOC},
  author       = {Xuemei Cao and Xiangkun Wang and Haoyang Liang and Bingjun Wei and Xin Yang},
  doi          = {10.1016/j.asoc.2024.112664},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112664},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Open continual sampling with hypersphere knowledge transfer for rapid feature selection},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models for cyber resilience: A comprehensive review, challenges, and future perspectives. <em>ASOC</em>, <em>170</em>, 112663. (<a href='https://doi.org/10.1016/j.asoc.2024.112663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interconnect cyber system is used by various users and organizations worldwide to perform different activities. These activities are combined with digital information and systems around the organizations to obtain higher accuracy and performance. However, these combinations of activities have faced cyber threats and attacks by single or multiple attackers. So, protecting and saving users' and organizations' sensitive data is a big challenge. So, the cyber resilience concept refers to the ability to prepare, absorb, recover, and adapt against cyberattacks and threats. It is used to mitigate cyberattacks and risks by the ability of the system to recover from threats. Artificial intelligence models enhance cyber resilience using machine learning and deep learning models. One of the most common components of artificial intelligence is large language models (LLM). It is used to understand language from text data and extract features to predict future words or missing in text datasets. LLM can enhance cyber resilience by providing various benefits for users and organizations. We divide the cyber resilience strategies into five parts. We review the LLM in each part, including security posture, data privacy and protection, security awareness, network security, and security automation. The fundamentals of LLMs are introduced as pre-trained models, transformers, encoders, and decoders. Then, we review the challenges of LLM in cyber resilience and cyber defense methods to overcome these challenges. We applied the LLM into three case studies including two for email spam text classifications and one for cyber threat detection. We obtained higher accuracy including 96.67 %, 90.70 %, and 89.94 % from three case studies respectively. Then we compared our LLM with other traditional machine learning models. The results show the LLM has higher accuracy, precision, recall, and f1 score compared with other models. Finally, the future directions of LLM in cyber resilience are provided.},
  archive      = {J_ASOC},
  author       = {Weiping Ding and Mohamed Abdel-Basset and Ahmed M. Ali and Nour Moustafa},
  doi          = {10.1016/j.asoc.2024.112663},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112663},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Large language models for cyber resilience: A comprehensive review, challenges, and future perspectives},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi period portfolio optimization: Incorporating stochastic predictions and heuristic algorithms. <em>ASOC</em>, <em>170</em>, 112662. (<a href='https://doi.org/10.1016/j.asoc.2024.112662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of economics and financial markets, optimal asset allocation strategies are essential for investor satisfaction and success. This paper delves into the complex landscape of multi-period portfolio selection, where the objective is to maximize wealth while minimizing investment risk. The core challenge of this research lies in addressing the complexity and uncertainty inherent in multi-period portfolio selection under stochastic conditions. The study introduces a framework for multi-period portfolio selection, considering N risky assets over T time periods. Stochastic return rates are modeled using a stochastic distribution, with the objective of maximizing wealth under risk constraints. The study presents an empirical case study involving the S&P500 market index, demonstrating the applicability of the proposed approach. Utilizing a random forest model, the paper predicts future returns, incorporating these predictions into a deterministic model via chance constraints. The contributions of the paper are substantial and multifaceted. Firstly, it introduces bankruptcy constraints, providing a more realistic approach to portfolio optimization and addressing an often-overlooked aspect of financial modeling. Secondly, transaction costs, a critical consideration in real-world scenarios, are integrated into the model, significantly enhancing the accuracy and practical relevance of portfolio optimization strategies. Thirdly, uncertainty management is rigorously tackled through stochastic approaches, ensuring the development of robust strategies that can accommodate varying market conditions. The paper also introduces risk-adjusted performance measures, enabling more informed decision-making by considering both risk and returns. Innovatively, this paper employs the Random Forest technique to predict return rates, thereby substantially enhancing the precision of investment predictions. Additionally, the Root System Growth Algorithm adds a heuristic dimension to problem-solving, effectively bridging the gap between computational and solution efficiency. The findings highlight the pivotal role of optimal allocation strategies in mitigating investment risks. The proposed approach yields impressive final wealth values and consistently performs well across different risk levels.},
  archive      = {J_ASOC},
  author       = {Seyedeh Asra Ahmadi and Peiman Ghasemi},
  doi          = {10.1016/j.asoc.2024.112662},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112662},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi period portfolio optimization: Incorporating stochastic predictions and heuristic algorithms},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single batch-processing machine scheduling problem with interval grey processing time. <em>ASOC</em>, <em>170</em>, 112661. (<a href='https://doi.org/10.1016/j.asoc.2024.112661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a single batch-processing machine scheduling problem with uncertain processing time. The uncertain processing time is characterized by interval grey number. A grey mixed integer linear programming model is established to formulate this uncertain scheduling problem to minimize the makespan. To solve this problem, a genetic algorithm with targeted population generation and neighbourhood search is designed. The results of experiments demonstrate that the proposed algorithm has excellent performance in both efficiency and stability. The resulting scheduling scheme can be shown through the Gantt chart with interval grey processing time, offering a novel approach for visualizing scheduling schemes with uncertain processing time.},
  archive      = {J_ASOC},
  author       = {Naiming Xie and Yihang Qin and Nanlei Chen and Yingjie Yang},
  doi          = {10.1016/j.asoc.2024.112661},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112661},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Single batch-processing machine scheduling problem with interval grey processing time},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reinforcement learning-assisted multi-objective evolutionary algorithm for generating green change plans of complex products. <em>ASOC</em>, <em>170</em>, 112660. (<a href='https://doi.org/10.1016/j.asoc.2024.112660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design change planning is an inevitable part of the product development process. Evolutionary algorithms (EAs) have been widely adopted to search for optimal change paths due to their strong global search capabilities. However, many existing approaches overlook key environmental factors like carbon emissions. Furthermore, EAs often struggle with premature convergence when solving complex design problems. This paper aims to develop an effective algorithm for green product design changes by incorporating carbon emission metrics and reinforcement learning techniques. Firstly, a constrained multi-objective optimization model for the green product change planning problem is built for the first time. Besides change cost and duration, a green indicator, i.e., carbon emissions, is introduced into the model, which can make obtained change plans more suitable for actual needs. Next, a multi-strategy self-switching multi-objective evolutionary algorithm assisted by reinforcement learning (R-MSMOEA) is developed to improve the performance of EA on solving the above model. Finally, the proposed model and algorithm are applied in the design change problem of a specific type of Skyworth TV, and experimental results verify their feasibility and effectiveness.},
  archive      = {J_ASOC},
  author       = {Ruizhao Zheng and Yong Zhang and Xiaoyan Sun and Lei Yang and Xianfang Song},
  doi          = {10.1016/j.asoc.2024.112660},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112660},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A reinforcement learning-assisted multi-objective evolutionary algorithm for generating green change plans of complex products},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated conceptual design of mechanisms based on thompson sampling and monte carlo tree search. <em>ASOC</em>, <em>170</em>, 112659. (<a href='https://doi.org/10.1016/j.asoc.2024.112659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conceptual design of mechanisms is a crucial part of achieving product innovation as mechanisms perform the transmission and transformation of specific motions in the machine. However, existing approaches for automated synthesis of mechanisms are either inefficient or prone to a loss of optimal solutions. To fill this gap, a systematic online decision-making method using Thompson Sampling (TS) based Monte Carlo Tree Search (MCTS) for automated conceptual design of mechanisms is proposed. The functional transformation relationships between inputs and outputs of the intended mechanism system are used to determine combinatorial patterns. Then, a functional representation model is constructed based on the combination rules of motion features and the inference relationships of function elements to represent a range of primitive mechanisms as fundamental building blocks. Finally, the optimal action selection strategy based on TS is applied into MCTS to develop Dirichlet based Monte Carlo Tree Search (D-MCTS) algorithm for searching mechanism building blocks. In addition, the conceptual design of the beat-up mechanism as well as the stitching and feeding mechanism are conducted to validate the feasibility of the proposed approach. Compared with specialized heuristics, D-MCTS achieves higher efficiency in finding the best combination of mechanism building blocks. Compared with other common algorithms, D-MCTS can always avoid the local optima trap to find the global optimal solution without any necessary hyper-parameter tuning. The proposed method exhibits a more balanced performance in exploration and exploitation, which provides better solutions for mechanism synthesis of given requirements.},
  archive      = {J_ASOC},
  author       = {Jiangmin Mao and Yingdan Zhu and Gang Chen and Chun Yan and Wuxiang Zhang},
  doi          = {10.1016/j.asoc.2024.112659},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112659},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated conceptual design of mechanisms based on thompson sampling and monte carlo tree search},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting experience accumulation in stock price prediction with continual learning. <em>ASOC</em>, <em>170</em>, 112658. (<a href='https://doi.org/10.1016/j.asoc.2024.112658'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting stock prices is difficult due to the influence of both long-term patterns and short-term fluctuations in the stock market. Traditional machine learning algorithms struggle to capture the relationship between these factors and are susceptible to catastrophic forgetting, which causes them to forget past experiences when learning new data. Yet, the experience gained from analyzing previous data has a considerable impact on predicting future stock prices. This paper proposes a solution to consolidate past experience by means of the Experience-accumulated Transformer (EAT) model, which accumulates experience through the self-attention mechanism of Transformer to model long-term dependencies and elastic weight consolidation (EWC). This enables us to identify long-term patterns and better capture short-term fluctuations, which leads to improvements in stock price prediction accuracy. The proposed model is evaluated based on the trading data of all constituent stocks of the HS 300 and compared to benchmark models. The experimental results show that after five years of sufficient training, the EAT model reduces the Mean Squared Error and improves the Walk-Forward Testing (WFT) Sharpe Ratio by 25.95 % and 53.24 %, respectively, compared to conventional training (two years). When compared to the best benchmark model, the EAT model outperforms it by 18.63 % and 31.05 %, respectively.},
  archive      = {J_ASOC},
  author       = {Cheng Zhao and Ping Hu and Xiaomin Yao},
  doi          = {10.1016/j.asoc.2024.112658},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112658},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exploiting experience accumulation in stock price prediction with continual learning},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral image classification based on mixed similarity graph convolutional network and pixel refinement. <em>ASOC</em>, <em>170</em>, 112657. (<a href='https://doi.org/10.1016/j.asoc.2024.112657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional convolutional neural network cannot extract Non-Euclidean spatial information in hyperspectral image classification, while the superpixel-based graph convolutional network relies on node aggregation and superpixel segmentation accuracy. To solve these problems, a mixed similarity graph convolutional network and pixel refinement method (MSGCN-CRF) is proposed in this paper. Firstly, homogeneous superpixel-level features are obtained as nodes using a superpixel segmentation algorithm, and then a mixed similarity method is designed to aggregate nodes. This method combines spectral intensity similarity and spectral curve similarity to obtain more discriminant node features. Secondly, a two-layer convolutional layer is used to remove noise from the original hyperspectral image. Then node features are constructed and input into the two-layer graph convolutional network, which using the adjacency matrix generated by mixed similarity to guide superpixel node aggregation. Finally, pixel refinement is implemented by fully connect CRF, which is used to correct the false prediction caused by the segmentation error of superpixels, and to obtain more accurate classification results using spatial and spectral information between pixels. Experimental results with a small amount of training samples on three different datasets show that the proposed MSGCN-CRF can obtain better classification results than the seven state-of-the-art classification methods.},
  archive      = {J_ASOC},
  author       = {Ronghua Shang and Keyao Zhu and Huidong Chang and Weitong Zhang and Jie Feng and Songhua Xu},
  doi          = {10.1016/j.asoc.2024.112657},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112657},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hyperspectral image classification based on mixed similarity graph convolutional network and pixel refinement},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-population multi-tasking tabu search with variable neighborhood search algorithm to solve post-disaster clustered repairman problem with priorities. <em>ASOC</em>, <em>170</em>, 112655. (<a href='https://doi.org/10.1016/j.asoc.2024.112655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Clustered Traveling Repairman Problem (cTRP) is an extended variant of the Traveling Repairman Problem (TRP), where customers are grouped into clusters that must be visited contiguously. However, the problem in post-disaster contexts has not yet been considered under the following constraints. First, the repairman requires additional time to remove debris, which adds debris removal time to the travel cost. Second, vertices in each cluster have varying priorities depending on their importance, with higher-priority vertices offering greater benefits when reached. This paper addresses these challenges by first defining the problem in post-disaster scenarios and then introducing a novel metaheuristic, TS-MMP, based on Multitasking Multipopulation Optimization (MMPO). This approach enables concurrent and independent task execution by integrating Randomized Neighborhood Search (RNVS), Tabu Search (TS), and dynamic knowledge sharing to improve problem-solving efficiency. In TS-MMP, the dynamic knowledge transfer mechanism ensures diversification, while TS and RNVS enhance intensification capabilities. Tabu lists prevent the search process from revisiting previously explored solution spaces. As a result, TS-MMP achieves superior solutions compared to other algorithms. Empirical results demonstrate that optimal solutions for instances with up to 30 vertices can be solved exactly using both the proposed formulation and TS-VNS-MMP. Moreover, TS-VNS-MMP provides high-quality solutions within a reasonable time for larger instances, confirming its impressive efficiency.},
  archive      = {J_ASOC},
  author       = {Ha-Bang Ban and Dang-Hai Pham},
  doi          = {10.1016/j.asoc.2024.112655},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112655},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-population multi-tasking tabu search with variable neighborhood search algorithm to solve post-disaster clustered repairman problem with priorities},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gene expression selection for cancer classification using intelligent collaborative filtering and hamming distance guided multi-objective swarm optimization. <em>ASOC</em>, <em>170</em>, 112654. (<a href='https://doi.org/10.1016/j.asoc.2024.112654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dimensional microarray cancer datasets contain thousands of genes with a very few numbers of samples. High class imbalance, presence of noisy and redundant genes and overlapping nature of extracted features among different disease classes deteriorate the disease prediction accuracy. An intelligent collaborative filtering (ICF) assisted and hamming distance guided multi-objective swarm intelligence framework (HIMS) is proposed for efficient selection of optimal gene set for disease identification. In the framework, first intelligent collaborative filtering (ICF) has been introduced to improve the prediction ability which combines the features from different feature selection tools. Then, a multi-objective multi-population search (MOMPS) algorithm has been proposed which contributes as a core part of HIMS. It generates more diversified solutions by avoiding local trapping. Hamming distance operator has been applied here as an alternative of sorting mechanism for the selection of Pareto optimal solutions. It also helps to reduce the computational complexity. Along with that, a time-varying U-shaped function is introduced for the binary conversion process for feature selection. Extensive experiments were conducted on 16 different single and multi-class datasets to study the efficacy of HIMS. The experimental results show that HIMS performs favorably well in comparison with other existing techniques with fewer numbers of genes.},
  archive      = {J_ASOC},
  author       = {Prativa Agarwalla and Sumitra Mukhopadhyay},
  doi          = {10.1016/j.asoc.2024.112654},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112654},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Gene expression selection for cancer classification using intelligent collaborative filtering and hamming distance guided multi-objective swarm optimization},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Altruistic NSGA-II with abandonment threshold and double selection strategy for solving multi-objective optimization problems. <em>ASOC</em>, <em>170</em>, 112653. (<a href='https://doi.org/10.1016/j.asoc.2024.112653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective optimization problems (MOPs) are significant in real world and often solved by using multi-objective evolutionary algorithms (MOEAs). However, the existing MOEAs are all facing challenges of falling into local optimization, low convergence speed and uneven distribution. To solve the above challenges, this study proposed a novel algorithm called altruistic NSGA-II (ANSGA-II), which embeds the central idea of altruism into NSGA-II. In the procedure, nurturing cost is self-adaptively composed by Pareto cost and crowd cost to better contribute to different periods in iterations. Besides, the abandonment threshold is also self-adaptive according to the abandonment situation of last generation, which accelerates convergence speed and assists population in escaping from local optimization. Moreover, double selections strategy consisting of k-nearest neighbor selection and non-dominated selection helps to balance convergence and diversity of population. The experimental results determine optimal ranges of parameters and validate the utility of each strategy. The comparisons with other algorithms demonstrate the great competitiveness of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Jianhong Chen and Haibin Ouyang and Steven Li and Chunliang Zhang and Zhi-Hui Zhan},
  doi          = {10.1016/j.asoc.2024.112653},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112653},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Altruistic NSGA-II with abandonment threshold and double selection strategy for solving multi-objective optimization problems},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wave energy forecasting: A state-of-the-art survey and a comprehensive evaluation. <em>ASOC</em>, <em>170</em>, 112652. (<a href='https://doi.org/10.1016/j.asoc.2024.112652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wave energy, a promising renewable energy source, has the potential to diversify the global energy mix significantly. Accurate forecasting of significant wave height (SWH) is crucial for enhancing the efficiency and reliability of wave energy conversion systems. As interest in this field grows, research into SWH forecasting has expanded dramatically. This comprehensive survey evaluates sixteen SWH forecasting methods, including Persistence, decision trees, deep neural networks, random neural networks, and random forests. The paper begins by establishing a detailed taxonomy that categorizes SWH forecasting algorithms, providing a framework to interpret the complexities of different methodological approaches. We then explore the interconnections between ensemble learning and decomposition-based frameworks and the integration of individual forecasting techniques within ensemble and hybrid models. In our empirical analysis, we rigorously assess the performance of these state-of-the-art algorithms using multiple, diverse datasets. Our findings reveal that ensemble methods generally surpass individual techniques in accuracy, with the extreme learning machine ranking as the least effective among the randomized neural networks. Looking ahead, we identify limitations in current forecasting models and propose new directions for research, including improvements in SWH model architecture, SWH data imperfection, forecasts for new buoy, and multimodality-enhanced methods.},
  archive      = {J_ASOC},
  author       = {Ruobin Gao and Xiaocai Zhang and Maohan Liang and Ponnuthurai Nagaratnam Suganthan and Heng Dong},
  doi          = {10.1016/j.asoc.2024.112652},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112652},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Wave energy forecasting: A state-of-the-art survey and a comprehensive evaluation},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Late acceptance hill climbing based algorithm for unmanned aerial vehicles (UAV) path planning problem. <em>ASOC</em>, <em>170</em>, 112651. (<a href='https://doi.org/10.1016/j.asoc.2024.112651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research innovatively applies the Late Acceptance Hill-Climbing (LAHC) algorithm to unmanned aerial vehicle (UAV) path planning in complex urban environments, marked by irregularly shaped threat areas. Moving beyond conventional models that simplify threat representations, our approach meticulously calculates collision costs, factoring in both the size and proximity of threat areas to waypoints, thereby enhancing path safety and feasibility. The standout feature of the LAHC algorithm is its memory-based strategy, which is simple yet remarkably effective, as evidenced by its superior performance over leading meta-heuristic algorithms across various urban flight scenarios. The study’s findings are significant, revealing that LAHC not only excels in optimizing path costs but also outperforms in terms of the number of iterations to convergence and average execution time. Additionally, a detailed convergence and complexity analysis of LAHC is conducted, providing deeper insights into its operational efficiency. Key contributions of this study include the development of realistic benchmark flight environments, the introduction of a novel method for collision cost calculation in urban settings, and the successful demonstration of LAHC’s rapid optimization capabilities and high-quality solutions.},
  archive      = {J_ASOC},
  author       = {Emad Deilam Salehi and MohammadAmin Fazli},
  doi          = {10.1016/j.asoc.2024.112651},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112651},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Late acceptance hill climbing based algorithm for unmanned aerial vehicles (UAV) path planning problem},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-objective hybrid evolutionary algorithm based on variable weight strategy for distributed hybrid flowshop scheduling with batch processing machines and variable sublots. <em>ASOC</em>, <em>170</em>, 112650. (<a href='https://doi.org/10.1016/j.asoc.2024.112650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In production systems, traditional machines and Batch Processing Machines (BPMs) commonly coexist within the same production line or workspace, collaboratively executing production tasks. Owing to the capacity constraints of BPMs, task changes are unavoidable during processing. Investigating potential task changes and achieving a balance between production efficiency and energy consumption are vital for contemporary manufacturing practices. Consequently, this paper examines a Distributed Hybrid Flowshop Scheduling Problem (DHFSP-BVS), characterized by the presence of batch processing machines and variable sublots. The problem encompasses five closely interconnected subproblems, namely factory allocation, task sequencing, task splitting, machine assignment, and speed selection, and exhibits significant NP-hard complexity. To address this intricate challenge, a mathematical model focusing on makespan and energy consumption criteria is developed, alongside the introduction of a novel multi-objective hybrid evolutionary algorithm (NHMOEA/D) based on a variable weight strategy. Specifically, a heuristic initialization technique is devised, drawing from the concept of balanced energy consumption. Additionally, a weight adjustment strategy and a variable neighborhood search mechanism are integrated to facilitate the evolutionary process of the population. Furthermore, a specialized sublot control strategy is formulated to manage task alterations, alongside the proposition of a dynamic energy-saving approach to minimize energy consumption even further. Ultimately, the effectiveness of the proposed NHMOEA/D is verified across 100 instances. The findings demonstrate that NHMOEA/D is capable of efficiently tackling the DHFSP-BVS and surpasses five other leading algorithms, namely MMBO/D, TMOA/D, MOHIG, IMOEA/D, and MDABC, in terms of performance.},
  archive      = {J_ASOC},
  author       = {Chengshuai Li and Yuyan Han and Biao Zhang and Yuting Wang and Junqing Li and Kaizhou Gao},
  doi          = {10.1016/j.asoc.2024.112650},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112650},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel multi-objective hybrid evolutionary algorithm based on variable weight strategy for distributed hybrid flowshop scheduling with batch processing machines and variable sublots},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced secondary decomposition model considering energy price for carbon price prediction. <em>ASOC</em>, <em>170</em>, 112648. (<a href='https://doi.org/10.1016/j.asoc.2024.112648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient carbon price forecasting is crucial for the stable operation of carbon trading system. However, influenced by multiple external factors, the carbon price change presented a complex law. Many previous studies introduced secondary decomposition to reduce the complexity of data, which led to the issues of over-decomposition and high time cost. Therefore, a carbon price prediction model was proposed based on an enhanced secondary decomposition structure. Meanwhile, energy price variations were considered, which enhanced the model’s learning ability to the external information. For illustration, four representative carbon emission trading markets were employed to assess prediction performance. The proposed secondary decomposition structure effectively addressed the issues of over-decomposition and high time cost. It reduced the comprehensive complexity by 147.95 % and enhanced the forecasting efficiency by 64.21 %. Furthermore, the modification by energy price provides grateful prediction environment, which substantially improves the accuracy and stability of carbon price forecasting. Therefore, the proposed model can serve as a reliable prediction tool and provide valuable reference for participants.},
  archive      = {J_ASOC},
  author       = {Gen Deng and Shunyu Zhao and Xiaoyao Yu and Yelin Wang and Youjie Li},
  doi          = {10.1016/j.asoc.2024.112648},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112648},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An enhanced secondary decomposition model considering energy price for carbon price prediction},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ELICIT information-based robust large-scale minimum cost consensus model under social networks. <em>ASOC</em>, <em>170</em>, 112647. (<a href='https://doi.org/10.1016/j.asoc.2024.112647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-Scale Group Decision-Making (LSGDM) in social network context has emerged as a research focus in decision sciences. Social relationships implicated in the network influence Decision-Makers’ (DMs) preferences and group consensus. However, existing research often overlooks the potential impact of uncertain adjustment costs driven by social relationships among DMs on the Consensus-Reaching Process (CRP). To address this issue, this paper develops a new Extended Linguistic Expressions with Symbolic Translation (ELICIT) information-based robust large-scale minimum cost consensus model under social networks. Firstly, the ELICIT model is used to represent DMs’ preferences, enhancing preference elicitation under uncertain conditions. Secondly, DMs’ weights are objectively determined based on the following–follower network, and the social network cost function is integrated into the Comprehensive Minimum Cost Consensus (CMCC) model. Then, three robust consensus models are developed to manage the uncertain adjustment costs of DMs within the network. Afterward, an ELICIT-based PROMETHEE ranking method is designed. Finally, a case study on selecting Healthcare Waste (HCW) treatment technology is conducted. The implemented sensitivity and comparative analysis demonstrate the effectiveness and advantages of the proposed method.},
  archive      = {J_ASOC},
  author       = {Yefan Han and Bapi Dutta and Diego García-Zamora and Ying Ji and Shaojian Qu and Luis Martínez},
  doi          = {10.1016/j.asoc.2024.112647},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112647},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ELICIT information-based robust large-scale minimum cost consensus model under social networks},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical deep learning-based recurrent convolutional neural network for robust voltage and frequency operation management in microgrids. <em>ASOC</em>, <em>170</em>, 112645. (<a href='https://doi.org/10.1016/j.asoc.2024.112645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microgrids (MGs) integrate various dynamic energy sources, often making it challenging for traditional control techniques to manage these complexities. This study addresses key requirements faced by MG systems, particularly in the robust regulation of voltage and frequency (V/F) components. The proposed two-coating and layer control strategy aims to optimize MG operation effectively. To determine the coefficients for the power droop controller (PDC), a novel method called the hierarchical deep learning-based recurrent convolutional neural network (HDL-RCNN) is presented. This method incorporates feedback from system components and formats the measured data into a grid structure before inputting it into the RCNN platform. This formatting allows for the automatic extraction of temporal and spatial features crucial for V/F stability. The RCNN architecture includes several layers, such as long short-term memory (LSTM) layers, convolutional layers, highly coupled layers, and other cascaded features. Results show significant improvements in voltage and frequency control: voltage oscillations in MG 1 were reduced from 0.028pu to 0.004pu, and frequency fluctuations in MG 2 decreased from 0.025pu to 0.007pu. Additionally, the method ensures the voltage stabilizes at 1pu, with minimal fluctuations, and provides robust performance across dynamic load changes and noisy conditions. These findings were validated through extensive testing on a MATLAB/Simulink platform, demonstrating the effectiveness of the HDL-RCNN in enhancing V/F stability and operational reliability in MGs.},
  archive      = {J_ASOC},
  author       = {Nima Khosravi and Hamid Reza Abdolmohammadi},
  doi          = {10.1016/j.asoc.2024.112645},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112645},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hierarchical deep learning-based recurrent convolutional neural network for robust voltage and frequency operation management in microgrids},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TIEOD: Three-way concept-based information entropy for outlier detection. <em>ASOC</em>, <em>170</em>, 112642. (<a href='https://doi.org/10.1016/j.asoc.2024.112642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is an attractive research area in data mining, which is intended to find out the few data objects that are abnormal to the normal data set. Formal concept analysis is an efficacious mathematical tool to perform data analysis and processing. Three-way concepts contain both information of co-having and co-not-having, and reflect the correlation among objects (attributes). Information entropy reflects the degree of uncertainty of the system. Information entropy-based outlier detection methods have been widely studied and have shown excellent performance, but most current information entropy-based methods contain parameters, which leads to detection results are sensitive to parameters settings and taking longer detection times. Aiming at this deficiency, this paper constructs a three-way concept-based information entropy outlier detection method. Firstly, the information entropy of the formal context is defined by utilizing three-way granular concepts, and then the relative entropy of each object is defined. According to it, the relative cardinality-based outlier degree of each object is given, and then the outlier factor of the object is defined by combining with the relative entropy. Then the three-way concept information entropy-based outlier factor is presented and the associated algorithm is proposed. Finally, the effectiveness and efficiency of the proposed algorithm is verified on a public dataset.},
  archive      = {J_ASOC},
  author       = {Qian Hu and Jun Zhang and Jusheng Mi and Zhong Yuan and Meizheng Li},
  doi          = {10.1016/j.asoc.2024.112642},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112642},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TIEOD: Three-way concept-based information entropy for outlier detection},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing inference speed in reparameterized convolutional neural network for vibration-based damage detection. <em>ASOC</em>, <em>170</em>, 112640. (<a href='https://doi.org/10.1016/j.asoc.2024.112640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural health monitoring (SHM) technology has been widely used in civil engineering, and vibration-based damage detection (VBDD) technology is an important component of SHM research. With the advancement of deep learning, a plethora of deep learning-based algorithms have been applied to VBDD. The accuracy of VBDD is constantly improving with the assistance of various deep learning techniques. However, studies on the efficiency of VBDD tasks based on neural network are still relatively few, and lightweight network technology has been proven to be an effective way to improve efficiency of neural network. In this paper, a novel neural network based on reparameterization is presented, which can decouple the model training and deployment, and maintain high accuracy under the consideration of model inference speed. Specifically, a convolutional neural network with multiple 1 × 1 convolution is used in the training, and all layers of convolution are fused during testing and inference of the model to obtain a VGG-style network with a lighter structure and higher accuracy for deployment. Experiments on benchmark datasets from IASC-ASCE and the Z24 dataset show that the proposed method can make VBDD work better.},
  archive      = {J_ASOC},
  author       = {Di Wang and Yuanming Lu and Xiangli Yang and Die Liu and Xianyi Yang and Jianxi Yang},
  doi          = {10.1016/j.asoc.2024.112640},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112640},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing inference speed in reparameterized convolutional neural network for vibration-based damage detection},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constraint-handling techniques for reusable launch vehicle reentry trajectory optimization using marine predator whale optimizer. <em>ASOC</em>, <em>170</em>, 112637. (<a href='https://doi.org/10.1016/j.asoc.2024.112637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reentry trajectory optimization for reusable launch vehicles (RLVs) is a class of optimal control problems with multiple highly nonlinear constraints. Nature-inspired algorithms (NIAs), which can somehow reduce the reliance on initial points, function differentiability, and convexity, with gradient-free nature and ease of implementation, have been actively applied in RLV reentry trajectory optimization problems. As NIAs are primarily designed for unconstrained optimization, constraint-handling techniques (CHTs), which play a crucial role in addressing RLV trajectory optimization issues and significantly impact the overall quality of the solutions, are necessary to guide the search towards feasible regions. However, the existing literature has not yet, or at least not systematically, investigated how well the current CHTs perform. Additionally, an in-depth analysis of parametric approaches and a performance evaluation framework is not yet available. To bridge this gap, this research constructs a benchmark model based on Space Shuttle reentry scenarios, investigates the effects of collocation type and interpolation method, and compares the performance of eight CHTs. An improved marine predator whale optimization algorithm is developed as a direct search engine, with results analyzed using the Wilcoxon signed rank and the Friedman tests. The results show that the ε -constrained technique and multi-objective-based CHTs with the algorithm, are somewhat superior in overall performance, and can produce relatively high-quality solutions over other competitors, while the optimization framework facilitates algorithm integration and CHTs for RLV reentry trajectory optimization problems.},
  archive      = {J_ASOC},
  author       = {Ya Su and Yi Liu},
  doi          = {10.1016/j.asoc.2024.112637},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112637},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constraint-handling techniques for reusable launch vehicle reentry trajectory optimization using marine predator whale optimizer},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fine-grained approach for visual interpretability of convolutional neural networks. <em>ASOC</em>, <em>170</em>, 112635. (<a href='https://doi.org/10.1016/j.asoc.2024.112635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Multilayer network-based Visual Interpreter (MuVI), a framework for visual interpretability of Convolutional Neural Networks (CNNs) based on their mapping into multilayer networks. The peculiarity of MuVI is that it constructs a pixel-level heatmap of the salient parts of an image processed by a CNN, where the importance of each pixel depends on all layers of the CNN and not only on the final ones, as in the existing approaches in the literature. MuVI first maps the CNN into a multilayer network. It then uses this representation to identify the parts of the CNN that most influence the prediction results by extracting those paths within the multilayer network whose nodes correspond to the most active areas of the feature maps. The weight of the paths is given by the sum of the weights of the arcs corresponding to the activations across all feature maps of the CNN; this characteristic allows MuVI to consider all layers of the CNN, not just the last ones. Finally, MuVI constructs the visual interpretability heatmap by selecting the paths with the highest weights. The experimental tests performed show that MuVI is able to achieve very satisfactory results in terms of AUC insertion (0.25), AUC deletion (0.11), % Increase in Confidence (12.32), Average Drop % (51.22), Pointing Game Accuracy (0.28) and Computation time ( 26 . 226 s ). These results, taking all these measures together, are better than those obtained by the classical approaches already proposed in the literature, such as SmoothGrad, Grad-CAM, Grad-CAM++, and RISE. They are also comparable to state-of-the-art approaches in the literature, such as Score-CAM and HSIC.},
  archive      = {J_ASOC},
  author       = {Alessia Amelio and Gianluca Bonifazi and Francesco Cauteruccio and Enrico Corradini and Michele Marchetti and Domenico Ursino and Luca Virgili},
  doi          = {10.1016/j.asoc.2024.112635},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112635},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fine-grained approach for visual interpretability of convolutional neural networks},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WbPINN: Weight balanced physics-informed neural networks for multi-objective learning. <em>ASOC</em>, <em>170</em>, 112632. (<a href='https://doi.org/10.1016/j.asoc.2024.112632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the growing popularity of physics-informed neural networks (PINNs), they face significant challenges in convergence and accuracy. The different terms in the PINNs loss function have varying magnitudes and may compete, complicating the simultaneous minimization of all terms during training, as distinct loss weights are needed to regulate performance. Furthermore, multi-objective learning addresses multiple objectives together, sharing inductive biases that may conflict. Thus, the weights of different factors must be carefully balanced. This paper proposes a new loss function construction method and an adaptive loss weighting strategy called wbPINN. This approach incorporates a correlation loss term and a penalty term to recognize the relationships between main loss function terms and the coupling in multi-objective training. This innovative approach divides the objective loss function into three components and utilizes the correlation regularization term to reduce the generalization error. And the weight coefficients of different loss terms are adjusted adaptively to get better optimization performance and higher accuracy. Finally, numerical experiments solving the three-dimensional elliptic problem with complex interface geometry and the Navier–Stokes equation test the robustness and performance of the presented method. Compared to PINNs and three other recent methods, the proposed wbPINN significantly outperform PINNs by two orders of magnitude in terms of convergence and accuracy across various tests. In most cases, wbPINN achieve prediction accuracy and computational efficiency comparable to other advanced adaptive weighting methods and can be easily generalized to complex multi-objective optimization problems.},
  archive      = {J_ASOC},
  author       = {Fujun Cao and Xiaobin Guo and Xinzheng Dong and Dongfang Yuan},
  doi          = {10.1016/j.asoc.2024.112632},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112632},
  shortjournal = {Appl. Soft. Comput.},
  title        = {WbPINN: Weight balanced physics-informed neural networks for multi-objective learning},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CNN-transformer network for student learning effect prediction using EEG signals based on spatio-temporal feature fusion. <em>ASOC</em>, <em>170</em>, 112631. (<a href='https://doi.org/10.1016/j.asoc.2024.112631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online education has become one of the most important forms of modern education. The uncertainty of the online learning process and the instability of its outcomes make accurately evaluating the outcomes of online learning a significant challenge. Brain–computer interface (BCI) based on deep learning enables analysis and recognition of complex neural patterns, which is a potential solution to predicting learning outcomes from the corresponding learning process. To validate this feasibility, we propose a hybrid CNN-Transformer (HCT) model to distinguish excellent and average learning outcomes from electroencephalogram (EEG) signals recorded during learning. The CNN part learns local spatial representations of multi-channel EEG, while the sparse self-attention fusion with Transformers learns global inter-correlations especially in long-term temporal dependencies. After model training, Grad-CAM is incorporated into the model to visualize and statistically analyze the features learned by the deep network, providing possible explanations for the model’s decision-making process and the neural patterns during learning. The proposed HCT-learn reached 90.13% accuracy with leave-one-subject-out cross-validation in binary learning outcome classification. It outperforms classical and hybrid solutions like EEGNet and ConFormer by at least 2.5% in accuracy, indicating its efficiency in learning spatial–temporal representations of EEG. This study effectively utilizes the potential of combining deep learning with EEG and lays the foundation for applications that dynamically monitor and predict learning outcomes. It guides teachers to make timely adjustments to their teaching strategies during the teaching process and for students to make timely adjustments to their learning methods during the learning process.},
  archive      = {J_ASOC},
  author       = {Hui Xie and Zexiao Dong and Huiting Yang and Yanxia Luo and Shenghan Ren and Pengyuan Zhang and Jiangshan He and Chunli Jia and Yuqiang Yang and Mingzhe Jiang and Xinbo Gao and Xueli Chen},
  doi          = {10.1016/j.asoc.2024.112631},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112631},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CNN-transformer network for student learning effect prediction using EEG signals based on spatio-temporal feature fusion},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coupled zeroing neural network for removing mixed noises in solving time-varying problems. <em>ASOC</em>, <em>170</em>, 112630. (<a href='https://doi.org/10.1016/j.asoc.2024.112630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harmonic noise frequently arouses by the disturbances in industrial applications, which would be a great threat to the security, stability and service life of equipment in some large and critical facilities, especially in power systems. Therefore, finding a way to resist harmonic noise is highly important. The zeroing neural networks (ZNN) have lately gained exceptional success in solving time-varying problems (TVP) as a result of its efficiency. Inspired by the effectiveness of ZNN and the dynamic system model design principles in control theory, we initially develop a coupled anti-mixed noise ZNN (AMNZNN) model that can resist the combination of single harmonic and non-harmonic noise (e.g., random noise). Then, an extended AMZNN model is further designed to remove the combination of multi-harmonic noise and non-harmonic noise. Additionally, comparisons among original ZNN (OZNN), integration-enhanced ZNN (IEZNN), harmonic-noise-tolerant ZNN (HNTZNN) and the proposed AMNZNN for time-varying matrix inversion (TVMI) under the mixture of harmonic noise and random noise are experimented to demonstrate the proposed AMNZNN model’s superior ability in resisting mixed noise. Finally, by applying the proposed extended formalism to power systems and microphone arrays in denoising, the effectiveness of the proposed method to resist multi-harmonic and random noises is further verified in scientific applications.},
  archive      = {J_ASOC},
  author       = {Jun Cai and Shitao Zhong and Wenjing Zhang and Chenfu Yi},
  doi          = {10.1016/j.asoc.2024.112630},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112630},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A coupled zeroing neural network for removing mixed noises in solving time-varying problems},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BERT-driven stock price trend prediction utilizing tokenized stock data and multi-step optimization approach. <em>ASOC</em>, <em>170</em>, 112627. (<a href='https://doi.org/10.1016/j.asoc.2024.112627'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock price trend forecasting is an important and challenging problem, and effective forecasting will help to improve investment returns and reduce investment risks. Previous studies are usually limited to modeling specific stocks, while the information contained in a single stock is limited and independent, so exploring the common patterns of stock price trend changes is a worthwhile direction of research. Based on this, this paper proposes the Stocks-BERT framework, which aims to investigate the semantic commonalities contained in the stock market. The framework is mainly divided into three modules: firstly, this paper proposes Ranking division and Base mapping to turn daily trading data into “words”, so that continuous trading data forms “sentences”, and constructs the “BERT training sentences” for training. Then, in the BERT pre-training stage, the Steps training method is used for the input data of 987 stocks (S&P 500 and CCTV Finance 500) to make the model converge faster and improve the effect. Finally, in the fine-tuning stage, it is proposed to use the word vectors extracted from the Embedding Layer to optimize the classifier in the output layer to adapt to individual stock characteristics. The results show that the Stocks-BERT framework model proposed in this paper is superior to the state-of-the-art models on two publicly available datasets (ACL18 and KDD17) and 14 stock market index datasets (INDEX14). The accuracy can reach more than 60% on individual stocks and stock market indexes, and it shows good generalization. In addition, it is found that even though the Chinese and American stock markets are in different economic environments, the models trained using Chinese stock market data and American stock market data show similar results in terms of outcome performance, indicating that the two markets have similar expressive power.},
  archive      = {J_ASOC},
  author       = {Xiaojian Teng and Liang Zhang and Peiwen Gao and Chuanwei Yu and Song Sun},
  doi          = {10.1016/j.asoc.2024.112627},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112627},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BERT-driven stock price trend prediction utilizing tokenized stock data and multi-step optimization approach},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Air quality index prediction through TimeGAN data recovery and PSO-optimized VMD-deep learning framework. <em>ASOC</em>, <em>170</em>, 112626. (<a href='https://doi.org/10.1016/j.asoc.2024.112626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the economy, air pollution has become increasingly severe. Accurate prediction of the Air Quality Index (AQI) is crucial for safeguarding public health and the environment. However, AQI time series exhibit strong randomness and volatility, posing challenges for traditional forecasting methods to achieve precise AQI predictions. Therefore, we propose a new AQI hybrid prediction model, TG-Hybrid model, which integrates generative artificial intelligence, signal decomposition techniques, artificial intelligence methods, and optimization algorithms. In the proposed model, missing values in the data are handled using generative adversarial networks, effectively addressing the issue of a large number of missing values in time series data. Autoregressive integrated moving average is employed to forecast the linear components of the data, while variational mode decomposition decomposes AQI into multiple modes. Particle swarm optimization is used to combine the prediction results of convolutional neural network combined with bidirectional long short-term memory and extreme gradient boosting. Additionally, AQI prediction experiments were conducted using air pollution data from Tangshan and Beijing, and compared with fifteen other models. The results indicate that the root mean square error for Tangshan and Beijing are 6.407 and 7.485, respectively, significantly outperforming other baseline models.},
  archive      = {J_ASOC},
  author       = {Kenan Wang and Tianning Yang and Shanshan Kong and Mingduo Li},
  doi          = {10.1016/j.asoc.2024.112626},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112626},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Air quality index prediction through TimeGAN data recovery and PSO-optimized VMD-deep learning framework},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of attribute control charts under uncertainty with normality analyses: Impact of operator hesitancy during inspection processes in manufacturing industry with a real case application. <em>ASOC</em>, <em>170</em>, 112625. (<a href='https://doi.org/10.1016/j.asoc.2024.112625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control charts (CCs) are one of the most effective tools for tracking variations and stability over time and are widely used generally in manufacturing to monitor and control process’ performance. When using attribute control charts (ACCs), one of the most crucial challenges is addressing uncertainty arising from both the use of linguistic terms (LTs) for quality assessment and inspectors' hesitancy in their judgments. Additionally, analyzing data normality is critical, as CCs are constructed based on the assumption of normally distributed data. To systematically address these uncertainties, our proposed method integrates fuzzy Z-numbers to enhance the robustness of LT evaluations and account for inspector hesitancies in the number of non-conformities per unit with a constant sample size ( u ) and number of non-conformities per unit with varying sample sizes ( c ) ACCs. Additionally, it provides a rigorous framework for assessing data normality under uncertain conditions to ensure the reliability of control limits. Furthermore, a fuzzy Mamdani inference system is integrated into the proposed approach to enhance decision-making accuracy, as it reduces loss of information by providing more sensitive and detailed outputs. These detailed outputs provide valuable support for both managers and operators, enabling more precise decision-making. The proposed approach is validated through on a real-case application in the automotive sector, and its effectiveness is confirmed by average run length (ARL) tests. Additionally, a comparison analysis for the proposed method with some existing methods is summarized. Results obtained indicate that the proposed approach significantly enhances performance by addressing operator hesitancy during inspections and mitigating the imprecision introduced by LTs through computing with words concept.},
  archive      = {J_ASOC},
  author       = {İhsan Kaya and Esra İlbahar and Fatma Kutlu Gündoğdu and Ali Karaşan and Kübra Yazır and Elifnaz Olgaç},
  doi          = {10.1016/j.asoc.2024.112625},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112625},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design of attribute control charts under uncertainty with normality analyses: Impact of operator hesitancy during inspection processes in manufacturing industry with a real case application},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive application of transfer learning, unsupervised learning and supervised learning in debris flow susceptibility mapping. <em>ASOC</em>, <em>170</em>, 112612. (<a href='https://doi.org/10.1016/j.asoc.2024.112612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning based debris flow susceptibility mapping (DFSM) is usually a simple supervised learning problem. Inadequate reliable samples in a single study area are usually one of the main factors limiting the performance of a model. This study is to provide a comprehensive and innovative approach for DFSM based on transfer learning, unsupervised learning and supervised learning, which is expected to reduce the limitations of the sample problem. A transfer learning approach called transfer component analysis (TCA) was utilized to project samples from different study areas into a common latent feature space to form a unified study area. The fuzzy C-mean (FCM) clustering algorithm belonging to unsupervised learning was used to cluster the unified study area into several homogeneous regions for independent processing to solve the spatial stratification heterogeneity problem. Another unsupervised learning algorithm named isolation forest (IF) was used to perform anomaly detection on all the samples to improve the reliability of negative samples. With all the datasets prepared, multiple random forest (RF) models representing supervised learning could be built. Traditional supervised learning models based on a single study area were also prepared for comparison. All the models were assessed based on the area under receiver operating characteristic curves (AUC) and statistical results. The results showed that the TCA method could effectively reduce the differences in feature distribution for different study areas. The application of FCM and IF could effectively deal with the problem of spatial stratification heterogeneity and improve the reliability of the negative samples respectively. The comprehensive model (AUC=0.93) proposed in this study is significantly better than that of traditional models (AUC=0.90 and 0.87) in terms of generalization ability, which could be widely applied when performing DFSM on a large scale.},
  archive      = {J_ASOC},
  author       = {Ruiyuan Gao and Changming Wang and Di Wu and Hailiang Liu and Xiaoyang Liu},
  doi          = {10.1016/j.asoc.2024.112612},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112612},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comprehensive application of transfer learning, unsupervised learning and supervised learning in debris flow susceptibility mapping},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pigmented skin disease classification via deep learning with an attention mechanism. <em>ASOC</em>, <em>170</em>, 112571. (<a href='https://doi.org/10.1016/j.asoc.2024.112571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pigmented skin disease is common; doctors need to observe and analyze pigmented skin disease images for diagnostic purposes. However, due to regional differences, diagnoses are subjective, resulting in high misdiagnosis rates. Therefore, this paper proposes a deep learning-based method for classifying pigmented skin disease images named the skin-global attention block (Skin-GAB). This method automatically classifies pigmented skin disease images through a system architecture that includes image augmentation, image segmentation, cluster analysis, segmented and original image classification, and network fusion. Additionally, this paper utilizes the GAB attention mechanism to encode the height, width, and channel of the feature maps, allowing the model to automatically learn crucial features from pigmented skin disease images and focus its attention on task-relevant information, thereby capturing disparities in input feature maps and further enhancing the model’s performance. The experimental results show that the proposed method performs well in terms of accuracy and practicality. Compared to using Xception as the classification network and the convolutional block attention module (CBAM) as the attention mechanism on the HAM10000 dataset, the system architecture proposed in this paper provides an improvement in accuracy of 2.89%. Therefore, this method will provide more accurate and efficient technical support for medical fields such as pigmented skin disease diagnosis and treatment.},
  archive      = {J_ASOC},
  author       = {Jinbo Chen and Qian Jiang and Zhuang Ai and Qihao Wei and Sha Xu and Baohai Hao and Yaping Lu and Xuan Huang and Liuqing Chen},
  doi          = {10.1016/j.asoc.2024.112571},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112571},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pigmented skin disease classification via deep learning with an attention mechanism},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parallel genetic algorithm for multi-criteria path routing on complex real-world road networks. <em>ASOC</em>, <em>170</em>, 112559. (<a href='https://doi.org/10.1016/j.asoc.2024.112559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Parallel Genetic Algorithm (PGA) specifically designed for multi-criteria vehicle routing is described in this work. The algorithm aims to enhance the existing routing methods by offering users the ability to choose their preferred path from a set of optimal paths optimised on multiple objectives. The objectives are optimised using a novel fitness metric that prioritises minimising path length while also maximising access to specific amenities such as pubs, hotels, and charging stations. The developed approach, called Parallel Optimal-route Search (POS), follows a hybrid model using both global parallelisation and island-based approaches. A Loop-Free Path-Composer (LFPC) is described and this genetic operator generates new paths for evaluation and is shown to yield a more diverse set of solutions in contrast to other commonly used approaches, such as Node Based Crossover and Path Mutation (NBCPM). Our approach is validated on highly complex, large-scale real-world road networks, with sizes ranging from 3,000 and 10,000 nodes. We present a systematic study comparing the performance of our proposed LFPC operator against the traditional NBCPM operators. Additionally, we evaluate the effectiveness of our proposed POS algorithm in comparison to the well-known Non-dominated Sorting Genetic Algorithm II and III.},
  archive      = {J_ASOC},
  author       = {Harish Sharma and Edgar Galván and Peter Mooney},
  doi          = {10.1016/j.asoc.2024.112559},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112559},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A parallel genetic algorithm for multi-criteria path routing on complex real-world road networks},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust kidney carcinoma prognosis and characterization using swin-ViT and DeepLabV3+ with multi-model transfer learning. <em>ASOC</em>, <em>170</em>, 112518. (<a href='https://doi.org/10.1016/j.asoc.2024.112518'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic kidney disease (CKD) remains a significant health epidemic despite surgical advancements and a global shortage of nephrologists. Medical data analysis and clinical examination can assist healthcare professionals in identifying CKD in its early stages. Computer-based techniques can enhance diagnostic efficiency and accuracy in medical imaging, but current deep-learning-based models have limitations in extracting global information and multi-label classification. The cutting-edge variants of the Vision transformers (ViT) models show promise in capturing information from multiple image regions but require large-scale dataset training. The study proposes a novel approach based on the Swin-ViT model and various modified DL-based pre-trained models (ConvNext, Swint-Unet, MedT, mResNet152V2, EfficientNet-B7, and mVGG19) for prognosis-assisted kidney image and multilabel grading, with changes made to the final layers using the transfer learning paradigm to improve performance and reduce dependence on data scales. Transfer learning can improve classification performance and model generalization in small-scale datasets while reducing training costs. Data enhancement and batch size cutting can improve performance, but smaller batches lead to longer training times. The experiment evaluates the Swin-ViT model for multi-label classification of CKD images via transfer learning, demonstrating that it has a stronger multi-label classification performance and that its attention mechanism is beneficial for precision-focused lesions. The study also generates attention mechanism GradCAM based on the Swin-ViT model, improving interpretability. Ablation experiments demonstrate that incorporating CNN and Transformer models outperform single-structure models. Future research can focus on extracting complex diseases and high-level semantic data to boost the model’s ability.},
  archive      = {J_ASOC},
  author       = {Amjad Rehman and Tariq Mahmood and Tanzila Saba},
  doi          = {10.1016/j.asoc.2024.112518},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112518},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust kidney carcinoma prognosis and characterization using swin-ViT and DeepLabV3+ with multi-model transfer learning},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymmetric variable depth learning automaton and its application in defending against selfish mining attacks on bitcoin. <em>ASOC</em>, <em>170</em>, 112416. (<a href='https://doi.org/10.1016/j.asoc.2024.112416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning Automaton (LA), a branch of reinforcement learning, initially began with the Fixed Structure Learning Automaton (FSLA) family and was later expanded to include the Variable Structure Learning Automaton (VSLA) family. Tuning the depth of the FSLA in complex environments has long been a challenging task, significantly limiting the ability to effectively navigate the exploration–exploitation dilemma. No solution has been found for this open problem yet. This study addresses this issue by introducing a novel hybrid learning automaton model called Asymmetric Variable Depth Hybrid Learning Automaton (AVDHLA). The AVDHLA model intelligently learns the depth of fixed structure LA in an autonomous manner by combining the L K N , K from the FSLA class and Variable Action Set LA (VASLA) from the VSLA class. Computer simulations are conducted to validate the proposed model in diverse environments, including both stationary and non-stationary (Markovian switching and State-dependent) scenarios. Performance evaluation is based on predefined metrics, such as total number of rewards (TNR) and action switching (TNAS). Statistical tests indicate that across both stationary and non-stationary environments, the AVDHLA consistently outperforms the L K N , K in terms of TNR and TNAS across the majority of experiments. Moreover, the AVDHLA model is applied in two key applications. Firstly, it is used to defend against the selfish mining attack in Bitcoin and is compared with the well-known tie-breaking mechanism. Simulation results consistently demonstrate that our proposed method increases the threshold for successful selfish mining attacks from 25% to 40%. Secondly, the AVDHLA model has been applied to develop a novel learning automaton-based recommendation system. The results demonstrate the superiority of the proposed method in terms of the Click-Through Rate (CTR) and Precision compared to previous approaches.},
  archive      = {J_ASOC},
  author       = {Ali Nikhalat-Jahromi and Ali Mohammad Saghiri and Mohammad Reza Meybodi},
  doi          = {10.1016/j.asoc.2024.112416},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {112416},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Asymmetric variable depth learning automaton and its application in defending against selfish mining attacks on bitcoin},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural ordinary differential equations for robust parameter estimation in dynamic systems with physical priors. <em>ASOC</em>, <em>169</em>, 112649. (<a href='https://doi.org/10.1016/j.asoc.2024.112649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel parameter estimation method based on Neural Ordinary Differential Equations (Neural ODE). The method addresses the challenges of limited data and noise interference in dynamic system modeling.By integrating neural networks with physical prior knowledge, the framework encodes the system's physical parameters as neural network parameters. This approach enables end-to-end learning from limited observational data. In numerical experiments on a damped oscillator system with Gaussian noise of standard deviation 0.2, the method estimates system parameters with relative errors below 2 %. For the complex nonlinear Lorenz system, it estimates key parameters σ, ρ, and β with relative errors of 0.1 %, 0.4 %, and 1.17 %, respectively. These results significantly outperform traditional methods.The numerical experiments demonstrate that the method provides accurate parameter estimation and effective system dynamics reconstruction. It performs well under small sample sizes and significant noise conditions.},
  archive      = {J_ASOC},
  author       = {Yong Yang and Haibin Li},
  doi          = {10.1016/j.asoc.2024.112649},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112649},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neural ordinary differential equations for robust parameter estimation in dynamic systems with physical priors},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater glider motion parameter generation based on structure-optimized deep belief network and BP neural network. <em>ASOC</em>, <em>169</em>, 112646. (<a href='https://doi.org/10.1016/j.asoc.2024.112646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The underwater glider (UG) must promptly adjust its motion parameters to adapt to varying ocean conditions, ensuring mission safety and navigation efficiency. Therefore, this paper proposes a novel UG motion parameter generation method to rapidly determine the optimal pitch angle and diving depth. This method addresses two key challenges: dataset construction and generation model accuracy. First, an effective dataset is constructed by extracting relevant features from path segments, seabed topography, and ocean currents. Second, we present a structure-optimized deep belief network and back propagation neural network (SO-DBN-BPNN) model, which automates the design of network structures. This model maximizes the feature extraction capability of DBN and the generalization ability of BPNN, significantly enhancing overall performance. The SO-DBN-BPNN model achieves determination coefficients of 93.005% for pitch angle and 99.903% for diving depth, outperforming eight state-of-the-art models in accuracy. Furthermore, the proposed method is applied to assist the UG executing multi-point exploration missions. Simulation results show that this method can rapidly generate high-precision motion parameters, offering reliable support for UG navigation.},
  archive      = {J_ASOC},
  author       = {Hao Hu and Yongjian Zhou and Zhao Zhang and Xingguang Peng},
  doi          = {10.1016/j.asoc.2024.112646},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112646},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Underwater glider motion parameter generation based on structure-optimized deep belief network and BP neural network},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective ensemble learning framework for designing low-carbon ultra-high performance concrete (UHPC). <em>ASOC</em>, <em>169</em>, 112644. (<a href='https://doi.org/10.1016/j.asoc.2024.112644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high-performance concrete (UHPC) is an advanced cement-based material with outstanding properties. The relationship between its composition and performance is highly complex, which makes designing UHPC with desirable properties by traditional empirical methods challenging. This study proposes a multi-objective ensemble learning framework to design low-carbon UHPC. The framework employs advanced algorithms for data generation, ensemble learning regression, and multi-objective optimization. A dataset with 21 UHPC constituent materials was collected. The CTGAN data generation algorithm was used to reasonably expand the dataset to improve its quality. An ensemble learning regression prediction model with intelligent optimization algorithms for hyperparameter tuning was developed to predict the mini-slump, 28-day compressive strength, and flexural strength of UHPC mixtures. This regression model served as the foundation for the AGE-MOEA-II multi-objective optimization model. The proposed framework was employed to design two types of UHPC with different objectives. The first type aimed to maximize compressive and flexural strength, while the second type sought to minimize UHPC carbon emissions while maintaining the performance goals of the first type. The results demonstrated that the ensemble learning model significantly improved the accuracy of the regression prediction model, achieving a coefficient of determination (R²) exceeding 0.95. Compared to the first design, the second optimized UHPC design reduced carbon emissions by more than 40 %. This data-driven approach delivers notable environmental and economic benefits in UHPC mix design.},
  archive      = {J_ASOC},
  author       = {Yuting Zhang and Meihui Yi and Wenyong Mei and Zhaofei Long and Lei Peng and Guangcheng Long},
  doi          = {10.1016/j.asoc.2024.112644},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112644},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective ensemble learning framework for designing low-carbon ultra-high performance concrete (UHPC)},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chaotic loss-based spiking neural network for privacy-preserving bullying detection in public places. <em>ASOC</em>, <em>169</em>, 112643. (<a href='https://doi.org/10.1016/j.asoc.2024.112643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bullying is currently inflicting severe harm on the personal safety and mental health of many individuals. This harm is multifaceted, impacting victims profoundly and generating negative consequences for perpetrators and the broader social environment. The proliferation of surveillance cameras and advancements in artificial intelligence technology has made it possible to monitor physical bullying in public places. However, the omnipresence of these cameras raises significant concerns regarding personal privacy breaches. To address the challenge of detecting bullying actions in public places while safeguarding privacy, this study introduces a bullying detection method based on neuromorphic vision. By leveraging the characteristics of spatiotemporal spiking signals, a balance between privacy preservation and bullying recognition is achieved. Additionally, this study develops the SNN-Chaos model and proposes a training methodology based on the chaotic loss functions, thereby enhancing the accuracy and classification precision of bullying action recognition. The experimental results demonstrate that the proposed chaotic loss functions generate a pronounced chaotic effect during the training process of the Spiking Neural Network (SNN) model. The mean Average Precision (mAP) of the SNN-Chaos model reaches 80.8 %, representing an improvement of 4.4 % over the CNN model (X3D) and 10.7 % over the traditional SNN model (VGG-SNN) in terms of bullying detection accuracy. Despite the rapid completion of some bullying actions and the difficulty for the human eye to discern their spiking images, the SNN-Chaos model achieves accurate recognition and classification. This study offers a novel approach to ensuring both personal privacy and physical safety, while also providing theoretical support for the training of brain-inspired and large-scale SNN architectures.},
  archive      = {J_ASOC},
  author       = {Jing Zhang and Tianlang Yang and Cheng Jiang and Jingwei Liu and Haoran Zhang},
  doi          = {10.1016/j.asoc.2024.112643},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112643},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chaotic loss-based spiking neural network for privacy-preserving bullying detection in public places},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncredibility with fuzzy variables in uncertainty and its use in decision support in making sense of heart sounds. <em>ASOC</em>, <em>169</em>, 112641. (<a href='https://doi.org/10.1016/j.asoc.2024.112641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study discusses the theory of uncertainty in real decisions, other models of uncertainty, and the expression of uncertainty with fuzzy sets and their application areas. The concepts of measurable sets, Borel sets, uncertainty space, credibility, and fuzzy sets frequently applied in decision-making are examined. Then, definitions and concepts of uncredibility are made, and examples and theorems are shown. A model is proposed with the motivation to contribute to the meaning of uncertainties. In this model, information to be used in the decision process based on the uncredibility of fuzzy variables is obtained. The main purpose of this study is to emphasize the importance of uncredibility in an environment of uncertainty and to state that in fuzzy environments, when the max(Cr-UnCr) average approaches 0, the probability of events occurring decreases, and when it approaches 1, the probability of events occurring increases. Calculating max(Cr-Uncr) values makes an important contribution to the interpretation of uncertainties. The uncredibility model is also applied to real-life examples, it shows the probability of an event occurring. Normal heart sounds and the sounds of people with heart disease are examined, the model is applied, and the model supports the detection of people with heart disease. The uncredibility model also indicates the amount of risk and has a proportional forecasting feature. It has been observed that the uncredibility measurement is effective in graded evaluation. A model about uncredibility is created and it is recommended to be used in the evaluation criteria. In uncertainty environments, the uncredibility value of an event is calculated and, as a result, is an important factor in the evaluation criterion. Mathematics subject classification 81S07-90C70-60A86},
  archive      = {J_ASOC},
  author       = {İbrahim Şanlıbaba},
  doi          = {10.1016/j.asoc.2024.112641},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112641},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncredibility with fuzzy variables in uncertainty and its use in decision support in making sense of heart sounds},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating infant development through a novel hybrid intuitionistic fuzzy classification system. <em>ASOC</em>, <em>169</em>, 112639. (<a href='https://doi.org/10.1016/j.asoc.2024.112639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preterm birth is a global public health concern, necessitating efficient and accurate neurodevelopmental assessments. This study firstly attempts that develops a novel hybrid classification system combining clustering algorithms, intuitionistic fuzzy sets (IFSs), and classification methods to improve neurodevelopmental assessments for preterm infants by analyzing growth trends between 6 months and 1 year of age. The proposed system integrates clustering techniques, including k-means (KM), fuzzy C-means (FCM), and kernel FCM (KFCM), with IFSs to address the vagueness in growth data. These clusters are then classified using advanced algorithms, such as long short-term memory networks (LSTM), support vector machines (SVM), K-nearest neighbours (KNN), naive Bayes (NB), and backpropagation neural networks (BPNN), to predict cognitive, language, and motor outcomes at 1 year old. Experimental results demonstrated that the KFCM+IFS+LSTM approach achieved the highest accuracy (average cognitive score: 0.95) and robustness (low variability) compared to other methods. The LSTM models consistently outperformed KNN and BPNN, which showed weaker and less consistent results. This hybrid system offers a scalable and precise solution for assessing neurodevelopment in very low birth weight preterm infants, reducing reliance on resource-intensive specialist evaluations and advancing early childhood healthcare methodologies.},
  archive      = {J_ASOC},
  author       = {Ya-Chi Hsu and Ting-Yu Lin and Kuo-Ping Lin and Yu-Tse Tsan and Kuo-Chen Hung},
  doi          = {10.1016/j.asoc.2024.112639},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112639},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating infant development through a novel hybrid intuitionistic fuzzy classification system},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble learning prediction model for lithium-ion battery remaining useful life based on embedded feature selection. <em>ASOC</em>, <em>169</em>, 112638. (<a href='https://doi.org/10.1016/j.asoc.2024.112638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To prevent potential dangers, it is crucial to accurately predict the Remaining Useful Life (RUL) of Lithium-Ion Batteries (LIBs) as capacity gradually decreases during use. Based on the Stanford MIT Battery Life Data, our researchers find the selection of input features difficult. Therefore, a new embedded Feature Selection (FS) method (LS-Embed-BHRSA) was proposed, which combines the Laplace score (LS) algorithm with the Binary Hybrid Reptile Search Algorithm (BHRSA). It is an organic fusion method of filtering and wrapping, composing an embedding structure that can quickly and accurately find the best feature subset. In addition, a Stacking multi-model integration strategy was also proposed to completely use the advantages of different models and improve the prediction accuracy of LIBs’ RUL. The validation results showed that the Hybrid Reptile Search Algorithm (HRSA) performed the best on the 2022 test function, and LS-Embed-BHRSA found the best features in comparison with other FS architectures on the UCI classification data. The testing results of the MIT data show that the overall proposed prediction model exceeds other machine learning models in metrics, such as RMSE, MAE, MAPE and R2, indicating its strong competitiveness.},
  archive      = {J_ASOC},
  author       = {Xiao-Tian Wang and Song-Bo Zhang and Jie-Sheng Wang and Xun Liu and Yun-Cheng Sun and Yi-Peng Shang-Guan and Ze-Zheng Zhang},
  doi          = {10.1016/j.asoc.2024.112638},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112638},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble learning prediction model for lithium-ion battery remaining useful life based on embedded feature selection},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An immunohistochemical scoring network based on multi-branch and dual attention mechanisms for the evaluation of biomarker PCNA in esophageal cancer. <em>ASOC</em>, <em>169</em>, 112636. (<a href='https://doi.org/10.1016/j.asoc.2024.112636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immunohistochemical (IHC) detection is crucial for diagnosing esophageal cancer. Proliferating Cell Nuclear Antigen (PCNA) is a key biomarker in IHC analysis, aiding in tumor characterization and prognosis. However, manual scoring methods are labor-intensive, subjective, and inconsistent. Automated methods, while available, often struggle with tightly integrating segmentation and classification tasks. They also lack mechanisms to focus on critical features and face challenges specific to PCNA IHC images, limiting their accuracy, robustness, and interpretability. We propose PH-ScoreNet for automatic PCNA IHC scoring in esophageal cancer. The network features a dual-branch architecture: a jump-symmetric nucleus detection branch and a fine-grained classification branch with residual and feature fusion layers. Shared features pass through skip connections to leverage task complementarity. A dual attention mechanism, focusing on staining intensity and spatial distribution, highlights key scoring features. The model is trained with a multi-task joint loss function and a weakly supervised annotation strategy, enabling end-to-end scoring on large datasets. To our knowledge, this is the first system for automatic H-score assessment of PCNA in esophageal cancer. Our dataset was obtained from the Institute of Life Sciences and Bioengineering at Beijing Jiaotong University. We present experimental results demonstrating the effectiveness of our approach. Additionally, we evaluated our method on an IHC image dataset of breast cancer to assess its broader applicability.},
  archive      = {J_ASOC},
  author       = {Zihao He and Dongyao Jia and Yinan Shi and Hong Jiang and Chuanwang Zhang and Ziqi Li and Nengkai Wu},
  doi          = {10.1016/j.asoc.2024.112636},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112636},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An immunohistochemical scoring network based on multi-branch and dual attention mechanisms for the evaluation of biomarker PCNA in esophageal cancer},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic multi-swarm whale optimization algorithm based on elite tuning for high-dimensional feature selection classification problems. <em>ASOC</em>, <em>169</em>, 112634. (<a href='https://doi.org/10.1016/j.asoc.2024.112634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection algorithms are crucial technologies for reducing the dimensionality of high-dimensional data. However, the exponential expansion of the decision space due to high-dimensional data makes most feature selection algorithms highly susceptible to local optima. To increase the quality of feature selection, we propose a dynamic multi-swarm whale optimization algorithm based on elite tuning (EMSWOA). First, we propose a dynamic multiple swarm construction mechanism based on the centroid distance metric, which enhances the exploitation and exploration abilities of the algorithm by improving the connectivity between sub-population topologies and preventing the algorithm from falling into precocious convergence. Secondly, to address the issue of invalid probability flips caused by random thresholds during feature space transformation, we design a novel elite tuning mechanism. This mechanism uses the high-quality information in the elite solution to assess and correct the probability of feature flipping; it effectively improves the algorithm's recognition of important features and reduces the interference of invalid probability flips in the optimization search. Finally, we validated the EMSWOA against six comparison algorithms on 19 datasets. The experimental results demonstrate that, compared with the most effective benchmark algorithms, the EMSWOA yields an average accuracy increase of 2.91 % and an average reduction in the number of features by a factor of 3.58.},
  archive      = {J_ASOC},
  author       = {Fahui Miao and Yong Wu and Guanjie Yan and Xiaomeng Si},
  doi          = {10.1016/j.asoc.2024.112634},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112634},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic multi-swarm whale optimization algorithm based on elite tuning for high-dimensional feature selection classification problems},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view knowledge graph convolutional networks for recommendation. <em>ASOC</em>, <em>169</em>, 112633. (<a href='https://doi.org/10.1016/j.asoc.2024.112633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems based on knowledge graphs (KGs) have attracted increasing attention recently, which alleviates the sparsity and cold-start issues by modeling user–item interactions with side information. However, most KG-based recommendation systems focus on shallow models due to the over-smoothing issue and neglect users’ long-term preferences. Moreover, KG-based methods are susceptible to noisy interactions, which reduces the robustness of the recommendation system. In this work, we propose a recommendation model based on a multi-view knowledge graph convolutional network (MKGCN) to address these issues. Specifically, to mitigate the impact of noisy interactions in a KG, we construct multi-view knowledge graphs from raw data by random sampling to learn multiple representations. Moreover, we develop a multi-layered knowledge graph convolutional network by introducing the initial residual connection to alleviate the over-smoothing issue. This approach enables effective capture of high-order connectivity and exploration of users’ potential long-term preferences. Furthermore, the graph self-attention mechanism is utilized to filter out inherent noise and refine the recommended results. Experiment results on real-world datasets demonstrate the effectiveness of MKGCN and its superiority over several state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Xiaofeng Wang and Zengjie Zhang and Guodong Shen and Shuaiming Lai and Yuntao Chen and Shuailei Zhu},
  doi          = {10.1016/j.asoc.2024.112633},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112633},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-view knowledge graph convolutional networks for recommendation},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tool wear state recognition for variable sensor combinations by deep forest with parameter adaptive fine-tuning. <em>ASOC</em>, <em>169</em>, 112629. (<a href='https://doi.org/10.1016/j.asoc.2024.112629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To explore the prediction accuracy of tool wear and achieve model parameter adaptive fine-tuning of a tool wear monitoring model used under different sensor combinations, this paper develops a tool wear state recognition model of deep forest (DF) with automatically select signal features and model parameter adaptive fine-tuning under variable sensor combinations, which can and improve the recognition accuracy and effectively reduce the model tuning work under different sensor combinations. In this paper, based on the autonomous feature selection ability of DF, DF is selected to establish an identification model for tool wear without additional feature selection operation, this was done to avoid the uncertainty of additional feature selection. Then, with the help of the characteristic that the number of layers of DF can be adaptively adjusted according to training data, DF is utilized to achieve model parameter adaptive fine-tuning of tool wear state recognition under variable sensor combinations. The sensors data of cutting force (F), vibration (V), acoustic emission (AE) and tool wear from Ti-5Al-5Mo-5V-1Cr-1Fe (TC18) milling were used for accuracy validation and analysis of feature selection ability of the developed model. Comparing with deep learning models from related literature and machine learning models with additional feature selection, DF obtains the highest prediction accuracy in F, V, F+V and F+V+AE with 92.65 %, 88.24 %, 97.06 % and 97.06 %. In addition, the developed recognition model has also achieved excellent recognition accuracy on the data of the numerical control machine tool health prediction competition published by the Prognostic and Health Management Society (PHM). These verifies the excellent prediction performance and feature selection ability of the developed model. This research provides effective guidance for the selection recognition models in the process of tool wear monitoring and expands the engineering application of DF.},
  archive      = {J_ASOC},
  author       = {Xin Wang and Ning Li and Dabin Lu and Xiaoping Liao and Juan Lu},
  doi          = {10.1016/j.asoc.2024.112629},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112629},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tool wear state recognition for variable sensor combinations by deep forest with parameter adaptive fine-tuning},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative estimation of distribution algorithm based on fitness landscape characteristic. <em>ASOC</em>, <em>169</em>, 112628. (<a href='https://doi.org/10.1016/j.asoc.2024.112628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of distribution algorithm (EDA) employs a probabilistic model to characterize the distribution of promising solutions, making it effective for tackling complex optimization problems. However, sampling based on a single probabilistic model generates a population with poor diversity and adequately fails to explore the solution space, leading to premature convergence of the algorithm. A collaborative estimation of distribution algorithm based on fitness landscape (FL-CEDA) is proposed to improve the performance of EDA in complex optimization problems. The shift of the mean and the adaptive shrinking of the covariance matrix are combined to guide the rapid evolution of the population to promising regions. The collaboration-operation of integrating mirrored sampling and Gaussian sampling is exploited to balance exploration and exploitation. By quantifying the ruggedness of the local fitness landscape, the sampling method that fits the problem characteristics is adaptively selected to improve the sampling efficiency. The performance of the FL-CEDA is verified on the CEC2017 benchmark test suite. The results illustrate that the FL-CEDA outperforms other state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Fuqing Zhao and Mengjie Li and Yang Yu and Ningning Zhu and Tianpeng Xu},
  doi          = {10.1016/j.asoc.2024.112628},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112628},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A collaborative estimation of distribution algorithm based on fitness landscape characteristic},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing sustainable desalination plants with advanced ML-based uncertainty analysis. <em>ASOC</em>, <em>169</em>, 112624. (<a href='https://doi.org/10.1016/j.asoc.2024.112624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable, cutting-edge environmental research and innovative solutions to critical environmental challenges are essential for a sustainable future. The desalination industry is increasingly interested in predicting the performance of hybrid nanofiltration/reverse osmosis (NF/RO) systems for treating very saline brine to recover valuable resources. This study aimed to optimize the effectiveness of a Long-Short-Term Memory (LSTM) model by applying two distinct metaheuristic optimization techniques: Genetic Algorithm (GA) and Crow Search Algorithm (CSA). The focus was on leveraging these algorithms for uncertainty analysis and forecasting in hybrid NF/RO desalination processes within Saudi Arabia. Furthermore, Particle Swarm Optimization (PSO) was utilized to determine the most suitable data models, consisting of three and four parameters, for this modelling purpose. Statistical tests based on analysis of variance (ANOVA) test (ANOVA F-test and the Welch F-test), covariance analysis (covariance matrix, t-statistics, and p-values), and pairwise Granger causality tests were conducted. Statistical methods and visual techniques were employed to assess and compare the precision of the LSTM model when integrated with the metaheuristic algorithms (LSTM-GA and LSTM-CSA) against the standalone LSTM model. The results demonstrated that the metaheuristic algorithms provided higher accuracy than the standalone LSTM model in both data models. In hybrid NF/RO desalination prediction, the LSTM-GA (MAE=0.13), LSTM-CSA (MAE=0.14), and LSTM (0.20) models achieved increasingly higher accuracy. Besides, the Monte Carlo method was utilized to evaluate the uncertainty linked with the predictions. The findings indicated that the LSTM model integrated with the GA (LSTM-GA) demonstrated the slightest uncertainty in its predictions.},
  archive      = {J_ASOC},
  author       = {Sani I. Abba and Jamilu Usman and Abdullah Bafaqeer and Babatunde A. Salami and Zaharaddeen Karami Lawal and Abdulmajid Lawal and A.G. Usman and Isam H. Aljundi},
  doi          = {10.1016/j.asoc.2024.112624},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112624},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing sustainable desalination plants with advanced ML-based uncertainty analysis},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spiking neural self-attention network for sequence recommendation. <em>ASOC</em>, <em>169</em>, 112623. (<a href='https://doi.org/10.1016/j.asoc.2024.112623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation plays an important role in providing a more personalized and accurate recommendation experience, and helps users discover new content that they may be interested in. However, sequence recommendation still faces long-term dependency issues (early behavior has a significant impact on subsequent recommendation results) and requiring high real-time performance. Moreover, the existing models based on recurrent networks or attention mechanisms still have shortcomings in addressing the long-term dependency problem and dynamic real-time performance of sequence recommendations. To address these challenges, we propose a long short-termmemory-spiking neural neural P (LSTM-SNP) self attention network, termed LSAF, for sequence recommendation that integrates long- and short-term user sequences. For this purpose, a three-channel structure is designed, where the long-term and short-term sequences are processed respectively by two self attention channels, and an LSTM-SNP channel is used to learn the user’s long-term dynamics. Then, these learned long-term and short-term features together are integrated by a self attention layer, and the prediction score and the predicted item (i.e. the next interaction item for user) can be obtained. In the LSAF model, the LSTM-SNP can effectively capture long-term and short-term dependency and nonlinear temporal characteristics. We evaluate the proposed LSAF model on three real-world datasets. The comparative experimental results with 13 baseline methods indicate that LSAF provides a competitive method for sequence recommendation.},
  archive      = {J_ASOC},
  author       = {Xinzhu Bai and Yanping Huang and Hong Peng and Qian Yang and Jun Wang and Zhicai Liu},
  doi          = {10.1016/j.asoc.2024.112623},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112623},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spiking neural self-attention network for sequence recommendation},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-resource knowledge graph completion based on knowledge distillation driven by large language models. <em>ASOC</em>, <em>169</em>, 112622. (<a href='https://doi.org/10.1016/j.asoc.2024.112622'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph completion (KGC) refines the existing knowledge graph (KG) by predicting missing entities or relations. Existing methods are mainly based on embeddings or texts but only perform better with abundant labeled data. Hence, KGC in resource-constrained settings is a significant problem, which faces challenges of data imbalance across relations and lack of relation label semantics. Considering that Large Language Models (LLMs) demonstrate powerful reasoning and generation capabilities, this work proposes an LLM-driven Knowledge Graph Completion Distillation (KGCD) model to address low-resource KGC. A two-stage framework is developed, involving teacher-student distillation by using LLM to improve reasoning, followed by fine-tuning on real-world low-resource datasets. To deal with data imbalance, a hybrid prompt design for LLM is proposed, which includes rethink and open prompts. Furthermore, a virtual relation label generation strategy enhances the model’s understanding of triples. Extensive experiments on three benchmarks have shown that KGCD’s effectiveness for low-resource KGC, achieving improvements in Mean Reciprocal Rank (MRR) by 11% and Hits@1 by 10% on the WN18, MRR by 10% and Hits@1 by 14% on the WN18RR, and MRR by 12% and Hits@1 by 11% on the YAGO3-10.},
  archive      = {J_ASOC},
  author       = {Wenlong Hou and Weidong Zhao and Ning Jia and Xianhui Liu},
  doi          = {10.1016/j.asoc.2024.112622},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112622},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Low-resource knowledge graph completion based on knowledge distillation driven by large language models},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level transfer learning using incremental granularities for environmental sound classification and detection. <em>ASOC</em>, <em>169</em>, 112619. (<a href='https://doi.org/10.1016/j.asoc.2024.112619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As sound recognition and classification models become more complex, their performance improves, but this comes with higher computational demands. This work addresses the application challenges in Environmental Sound Classification (ESC), where tasks often face the constraints of limited computational resources on edge devices and data scarcity. Leveraging the success of our previous Multi-Level Transfer Learning work in natural language processing and image recognition, we propose a Multi-Level Transfer Learning Using Incremental Granularities framework for ESC tasks. Our method utilizes audio features at multiple granular levels to provide varied learning characteristics, significantly enhancing the performance of small models without increasing model parameters too much. Experimental results show notable accuracy improvements across multiple models, particularly for smaller models. Specifically, our framework improved accuracy by up to 6.45% on the ESC-50 dataset and up to 8.6% on our custom chainsaw sound dataset, highlighting the advantages of transfer learning across different granularities.},
  archive      = {J_ASOC},
  author       = {Jia-Wei Chang and Hao-Shang Ma and Zhong-Yun Hu},
  doi          = {10.1016/j.asoc.2024.112619},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112619},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-level transfer learning using incremental granularities for environmental sound classification and detection},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of industry 4.0 adoption strategies in small and medium enterprises: A circular-fermatean fuzzy decision-making approach. <em>ASOC</em>, <em>169</em>, 112618. (<a href='https://doi.org/10.1016/j.asoc.2024.112618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evaluation of Industry 4.0 (I4.0) technology adoption strategies (I4.0AS) in Small and Medium Enterprises (SMEs) from I4.0 technologies and technology-organization-environment (TOE) ecosystem perspectives poses a significant challenge due to three primary concerns: the importance of criteria, data variability for each individual I4.0AS, and uncertainty in expert opinions. This complexity arises from the consideration of diverse criteria groups for I4.0 deployment in the SMEs sector with a focus on the TOE context, which is linked to a second criteria group characterized by uncertain evaluative data. Although research in this area has increased, a comprehensive assessment methodology tailored to the unique needs of SMEs remains elusive. Addressing this gap is crucial to provide SME decision-makers with detailed insights that enhance their strategic choices. In our study, we introduce a holistic evaluation of I4.0ASs, emphasizing the performance dynamics within the TOE framework. Central to our assessment methodology is the integration of advanced Circular-Fermatean Fuzzy sets (C-FFS), designed to capture uncertain evaluative data. This three-phased methodology formulates the Circular-Fermatean Fuzzy Sets-Fuzzy-weighted zero-inconsistency (C-FFS–FWZIC) approach and evaluates I4.0ASs from both the I4.0 technology and TOE-ecosystem perspectives. Through a rigorous examination of 37 distinct I4.0ASs based on 30 I4.0 technology perspective criteria and 114 TOE-ecosystem perspective criteria, our study illuminates their efficacy across these dual perspectives. The results indicate that I4.0AS23 ranked first according to the I4.0 technologies perspective but 26th according to the TOE-ecosystem perspective, while I4.0AS1 ranked first according to the TOE-ecosystem perspective and 15th according to the I4.0 technologies perspective. I4.0AS25 yielded consistent results, scoring 6th place in both perspectives. Additionally, the resilience and versatility of our methodology are validated through an in-depth sensitivity and comparative analysis, reinforcing its potential as a valuable tool for future industry applications.},
  archive      = {J_ASOC},
  author       = {Dareen Abu-Lail and Nahia Mourad and Sarah Qahtan and A.A. Zaidan and Hassan A. Alsattar and B.B. Zaidan and Dragan Pamucar and Muhammet Deveci and Witold Pedrycz and Dursun Delen},
  doi          = {10.1016/j.asoc.2024.112618},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112618},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of industry 4.0 adoption strategies in small and medium enterprises: A circular-fermatean fuzzy decision-making approach},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing convergence and diversity preservation in dual search space for large scale particle swarm optimization. <em>ASOC</em>, <em>169</em>, 112617. (<a href='https://doi.org/10.1016/j.asoc.2024.112617'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balancing convergence and diversity is a crucial challenge for particle swarm optimization in addressing large-scale optimization problems. A main reason is that it is difficult to simultaneously effectively manage diversity with specific parameters in both objective space and decision space. To address this issue, this paper introduces a novel velocity update structure that integrates diversity preservation in both objective space and decision space alongside convergence. Aligned with the proposed update structure, a new diversity enhancement mechanism is proposed. This mechanism comprises an entropy-based diversity preservation strategy and an adaptive difference-mutation-based diversity preservation strategy, designed to preserve diversity in objective space and decision space, respectively. By utilizing a dynamic convergence learning strategy, a novel large-scale swarm optimizer capable of explicitly and simultaneously balancing convergence and diversity in both spaces is developed. This paper theoretically proves the stability and analyzes the search behavior of the proposed algorithm. Comprehensive experiments are then conducted using two large-scale benchmark test suites, a real-case application model and several state-of-the-art algorithms. The results demonstrate the competitiveness of the proposed algorithm in large-scale optimization and the effectiveness of the proposed strategies in balancing convergence and diversity.},
  archive      = {J_ASOC},
  author       = {Weian Guo and Li Li and Minchong Chen and Wenke Ni and Lei Wang and Dongyang Li},
  doi          = {10.1016/j.asoc.2024.112617},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112617},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Balancing convergence and diversity preservation in dual search space for large scale particle swarm optimization},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Link prediction in bipartite networks via deep autoencoder-like nonnegative matrix factorization. <em>ASOC</em>, <em>169</em>, 112616. (<a href='https://doi.org/10.1016/j.asoc.2024.112616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A bipartite network is a special type of network structure that possesses unique value and practical significance in numerous fields, including recommender systems, social networks, bioinformatics, and transportation networks. However, in real-world scenarios, bipartite networks often encounter data sparsity issues, meaning there is inadequate connection data between network nodes, which current research is inadequately equipped to address. In this paper, we propose a method for link prediction in bipartite networks called Similarity Enhancement Deep Autoencoder-like Nonnegative Matrix Factorization (SE-DANMF). The nonnegativity constraint of nonnegative matrix factorization assists in preserving the original data’s structure and characteristics within our model. The hidden structures and patterns in the original data can be effectively revealed by deeply factorizing the shallow nonnegative matrix and incorporating an autoencoder into the factorization process. During the training process of the deep autoencoder, we introduce a similarity matrix to increase the weights between similar nodes, making it easier to learn common node features in a sparse bipartite network. Furthermore, enhanced similarity also aids in reconstructing the original matrix more accurately during the nonnegative matrix factorization stage. A large number of experimental results of the proposed SE-DANMF on 9 datasets clearly highlight its advantages compared to 10 baseline methods in bipartite link prediction.},
  archive      = {J_ASOC},
  author       = {Wei Yu and Jiale Fu and Yanxia Zhao and Hongjin Shi and Xue Chen and Shigen Shen and Xiao-Zhi Gao},
  doi          = {10.1016/j.asoc.2024.112616},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112616},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Link prediction in bipartite networks via deep autoencoder-like nonnegative matrix factorization},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure and energy efficient routing protocol for underwater wireless sensor network using running city game optimization with XGBoost algorithm. <em>ASOC</em>, <em>169</em>, 112615. (<a href='https://doi.org/10.1016/j.asoc.2024.112615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectives An emerging method for investigating undersea resources is referred to as underwater wireless sensor networks (UWSNs). Energy consumption and security are essential components in the UWSNs ecosystem because the environment of UWSNs is susceptible to various security assaults. Many energy-efficient routing protocols have been created, but they face challenges like transmission collisions and energy loss because of increased data redundancy. Secure Energy Efficient Routing Protocol (SEERP) is developed to overcome these challenges. Methods The sensor nodes are placed at random in the underwater experimental region. Depending on factors such as residual energy, entropy, transmission latency, and packet error rate, the attack nodes are separated from the normal node. The types of attack packets are identified using an Extreme Gradient Boosting Machine (XGB) classifier. Then the attack packets are grouped into three types such as low, moderate and high, using agglomerative clustering. After grouping the attack packets, redundant nodes are detected based on the Node Similarity Preserving Graph Convolutional Network (NSPGCN). The identified redundant nodes are replaced with high-impact attack packets in order to reduce the transmission collision issue. Finally, the data are transmitted securely through the optimal path by employing Running City Game Optimization (RCGO). Results This proposed energy-efficient routing protocol is tested with several metrics that attain better performance, like 9.1 J average residual energy, 91 % packet delivery ratio, 2.57 % throughput value and 1.4 % delay. Thus, the approaches employed in the suggested approach are the best option for resolving the security, energy consumption, and network lifespan problems.},
  archive      = {J_ASOC},
  author       = {A. Shenbagharaman and B. Paramasivan},
  doi          = {10.1016/j.asoc.2024.112615},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112615},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Secure and energy efficient routing protocol for underwater wireless sensor network using running city game optimization with XGBoost algorithm},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A genetic programming based cooperative evolutionary algorithm for flexible job shop with crane transportation and setup times. <em>ASOC</em>, <em>169</em>, 112614. (<a href='https://doi.org/10.1016/j.asoc.2024.112614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confronted with increasingly complex industrial scenarios, limited transportation resources and complicated time constraints introduce significant challenges to production efficiency, requiring more robust and adaptive scheduling heuristics. In this study, a flexible job shop scheduling problem with single crane transportation and sequence-dependent setup time is considered. To address the problem, a mixed integer linear programming model is established, where two objectives, including the maximum completion time and total energy consumption, are determined simultaneously. Additionally, a genetic programming (GP) based cooperative evolutionary algorithm is developed to address the problem, in which GP is investigated as a hyper-heuristic to construct a set of problem-specific dispatching rules (DRs). The GP-based hyper-heuristic (GPHH) first evolves a set of DRs during iterations and then applies these DRs to the initialization of the population. Next, four critical path-based neighborhood structures combined with an adaptive local search mechanism are used to enhance the exploitation capability of the algorithm. The simulation results demonstrate that the GPHH used for initialization significantly outperforms other classical heuristics in convergence capability, while the proposed GP-CEA algorithm also surpasses six state-of-the-art algorithms in exploration and exploitation, achieving superior performance on the HV, IGD, and SC metrics in approximately 59.4 %, 46.9 %, and 50.0 % of instances, respectively.},
  archive      = {J_ASOC},
  author       = {Xiaolong Chen and Junqing Li and Zunxun Wang and Jiake Li and Kaizhou Gao},
  doi          = {10.1016/j.asoc.2024.112614},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112614},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A genetic programming based cooperative evolutionary algorithm for flexible job shop with crane transportation and setup times},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cybersecurity-aware log management system for critical water infrastructures. <em>ASOC</em>, <em>169</em>, 112613. (<a href='https://doi.org/10.1016/j.asoc.2024.112613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber threats are increasingly targeting critical water infrastructures, requiring robust cybersecurity measures to ensure the continuous and safe delivery of water services. This paper presents a comprehensive, cybersecurity-aware log management system specifically designed for critical water infrastructures. The system leverages advanced data collection, analysis, and real-time monitoring to effectively detect and mitigate cyber threats. Key features include integration with existing infrastructure, scalability to handle large volumes of log data, and machine learning algorithms for enhanced threat detection and response. Our solution demonstrated significant improvements in threat detection accuracy, response times, and overall system resilience through rigorous testing in real-world scenarios. This paper discusses the design, implementation, and performance evaluation of the proposed log management system, highlighting its potential to strengthen the cybersecurity posture of critical water infrastructures.},
  archive      = {J_ASOC},
  author       = {Deniz Dural Balta and Seda Balta Kaç and Musa Balta and Nur Banu Oğur and Süleyman Eken},
  doi          = {10.1016/j.asoc.2024.112613},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112613},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cybersecurity-aware log management system for critical water infrastructures},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging temporal dependency in probabilistic electric load forecasting. <em>ASOC</em>, <em>169</em>, 112611. (<a href='https://doi.org/10.1016/j.asoc.2024.112611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging past observations enhances predictive performance by capturing the time series nature of electric loads. Existing studies often overlook the distinct roles of past observations and exogenous features in model training, and neglects temporal dependencies within electric loads, leading to less interpretable models. Focusing on probabilistic electric load forecasting, this work adopts support vector regression to describe relationships between exogenous features and electric loads; then incorporates temporal dependencies through autoregressive errors. Dependency among errors are further used to refine prediction intervals. Procedures for model fitting and generating both one-step and multi-step prediction intervals are provided, along with theoretical guarantees. Simulations and real-world experiments validate the effectiveness of the proposed approaches in achieving prediction intervals with desired coverage probabilities.},
  archive      = {J_ASOC},
  author       = {Yaoli Zhang and Ye Tian and Yunyi Zhang},
  doi          = {10.1016/j.asoc.2024.112611},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112611},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Leveraging temporal dependency in probabilistic electric load forecasting},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relieving popularity bias in recommender systems via user group-level augmentation. <em>ASOC</em>, <em>169</em>, 112610. (<a href='https://doi.org/10.1016/j.asoc.2024.112610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems frequently encounter the challenge of popularity bias: the interaction data exhibit a pronounced skew or a long-tailed distribution across items. This imbalance can cause models to disproportionately recommend popular items, deviating from accurately reflecting users’ true preferences. While existing methods often employ propensity-based unbiased learning or causal inference to tackle data imbalance. However, they overlook the imbalance issue among user groups, potentially neglecting the influence of popularity on preference representation for different user groups. In addressing this issue, our work categorizes users into two groups: those who are intended for popularity ( Popularity-intended ) and those who are not ( Popularity-unintended ). We propose a novel framework called RPB4Rec ( R elieving P opularity B ias for Rec ommendation) designed to perform data augmentation at the user group level, relieving popularity bias stemming from imbalanced interactions among user groups. For each actual user–item interaction, RPB4Rec generates augmented interactions, which are then utilized in a contrastive learning paradigm. To precisely modulate the influence of augmented data on model training, we incorporate an adaptive temperature coefficient τ for contrastive learning, which is dynamically adjusted based on the training loss. To validate the effectiveness of our proposed method, we conducted extensive evaluations using three diverse datasets: the Yahoo! R3 dataset, the Coat Shopping dataset, and the Kuairand video dataset. Our comprehensive experiments across these real-world datasets validate the superiority of our approach. Specifically, the experimental results show that RPB4Rec can outperform the state-of-the-art method by average 5% in Recall@20 and NDCG@20, indicating that RPB4Rec is highly effective in relieving popularity bias in recommender systems.},
  archive      = {J_ASOC},
  author       = {Ming He and Zihao Zhang and Han Zhang and Chang Liu},
  doi          = {10.1016/j.asoc.2024.112610},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112610},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Relieving popularity bias in recommender systems via user group-level augmentation},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A time-aware developer recommendation approach based on multi-feature fusion. <em>ASOC</em>, <em>169</em>, 112609. (<a href='https://doi.org/10.1016/j.asoc.2024.112609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommending developers suitable for Pull Requests (PRs) review holds paramount importance in facilitating evolution and knowledge sharing on GitHub. Current researches primarily focus on modeling developer expertise or developer-PR interactions using historical review data. However, these approaches lack consideration for integrating domain-specific PRs and temporal changes into developer recommendations, which results in sub-optimal performance. To address these problems, we introduce a time-aware developer recommendation approach based on multi-feature fusion, called TFRec. We utilize the BERT model, trained via contrastive learning, to embed textual PR information (e.g., title and description), facilitating efficient semantic learning and generalization improvement. Additionally, we employ high-order propagation and embedding to fuse the semantic and structure features of historical data and capture potential collaborative relationships from developer-PR interactions. Furthermore, a time-aware attention is introduced to encode the temporal features into the propagation and embedding process. Extensive experiments on a dataset of 65k PRs from 10 GitHub open-source projects demonstrate the superior performance of our proposed method compared to state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Lu Zhang and Shizhan Chen and Guodong Fan and Hongyue Wu and Hongqi Chen and Zhiyong Feng},
  doi          = {10.1016/j.asoc.2024.112609},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112609},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A time-aware developer recommendation approach based on multi-feature fusion},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Use of conditional generative adversarial networks to create demographic collaborative filtering datasets. <em>ASOC</em>, <em>169</em>, 112608. (<a href='https://doi.org/10.1016/j.asoc.2024.112608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a method to create synthetic collaborative filtering datasets that can be used to test both current and new fair recommender systems models. The proposed “Conditional Generative Adversarial Network for Recommender Systems (CGANRS)” method generalizes the existing generative adversarial network for recommender systems one, and it makes use of a conditional generative adversarial network to artificially generate synthetic profiles from a source dataset such as MovieLens. The created datasets can be parameterized to have different sizes and to include different number of users and items. Additionally, the provided parameters include the proportion of multi-categorical demographic information such as the number of male vs. female users, or the proportions of very young, young, adult, and senior users. To test the proposed method, three sets of synthetic databases have been created, containing different a) numbers of users, b) numbers of items, and c) proportions of male users versus female users. Results show an adequate behavior of the generated datasets, testing their a) profiles separability, b) main statistical distributions, and c) recommendation accuracies. Synthetic data sets created using the proposed conditional generative adversarial network for recommender systems method are particularly useful to improve research in the fairness field of the recommender systems area. To extend its use and to facilitate reproducibility, the source code is provided to generate as many demographic datasets as desired, as well as the artificially generated datasets in this research. Some promising future works are proposed, including a) the variation of the stochastic Gaussian distribution used to create the random noise vectors that feed the adversarial network generator model, and b) testing the fairness of the most relevant collaborative filtering models on different synthetic scenarios.},
  archive      = {J_ASOC},
  author       = {Jesús Bobadilla and Abraham Gutiérrez},
  doi          = {10.1016/j.asoc.2024.112608},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112608},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Use of conditional generative adversarial networks to create demographic collaborative filtering datasets},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group decision making method for site selection of car sharing stations in istanbul using spherical fuzzy rough numbers. <em>ASOC</em>, <em>169</em>, 112607. (<a href='https://doi.org/10.1016/j.asoc.2024.112607'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One significant component of the sharing economy, which is expanding globally, is car sharing. In order to broaden their potential and marketing shares, service suppliers want to build more car sharing stations. In order to address the location selection challenge of car sharing stations, a new model is presented in this study that offers a convenient methodology for evaluating possible car sharing locations. In this paper, an extension of the Stepwise Weight Assessment Ratio Analysis (SWARA) technique using spherical fuzzy rough numbers (SFRN) is presented to compute the weights of criteria. The SWARA technique describes the proportional significance of one criterion in comparison to the preceding criterion. Additionally, extended SFR-MULTIMOORA is proposed for ranking of alternatives. In MULTIMOORA, dominance theory is implemented to aggregate the utility values of three ranking techniques. In this study, spherical fuzzy rough numbers are used to tackle complex multi-criteria group decision making (MCGDM) problems. Highly complex data sets with a range of ambiguity levels can be managed using SFRNs thanks to their more dependable and adaptable data processing way. For convenience of understanding, a flowchart is presented to illustrate the SFR-SWARA-MULTIMOORA approach. The suggested approach is applied in a case study of Istanbul, where the task is to select optimal new car-sharing station among four locations. After that, comparison of proposed method is made with the SFR-TOPSIS and SFR-WASPAS approaches. At the end, sensitivity analysis is carried out to confirm the precision of the calculations of suggested method.},
  archive      = {J_ASOC},
  author       = {Muhammad Akram and Safeena Azam and Cengiz Kahraman},
  doi          = {10.1016/j.asoc.2024.112607},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112607},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Group decision making method for site selection of car sharing stations in istanbul using spherical fuzzy rough numbers},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An autonomous differential evolution based on reinforcement learning for cooperative countermeasures of unmanned aerial vehicles. <em>ASOC</em>, <em>169</em>, 112605. (<a href='https://doi.org/10.1016/j.asoc.2024.112605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, reinforcement learning has been used to improve differential evolution algorithms due to its outstanding performance in strategy selection. However, most existing improved algorithms treat the entire population as a single reinforcement learning agent, applying the same decision to individuals regardless of their different evolutionary states. This approach neglects the differences among individuals within the population during evolution, reducing the likelihood of individuals evolving in promising directions. Therefore, this paper proposes an Autonomous Differential Evolution (AuDE) algorithm guided by the cumulative performance of individuals. In AuDE, at the individual level, the rate of increase in each individual's cumulative reward is used to guide the selection of appropriate search strategies. This ensures that all individuals accumulate experience from their own evolutionary search process, rather than relying on the experiences of others or the population, which may not align with their unique characteristics. Additionally, at the global level, a population backtracking method with stagnation detection is proposed. This method fully utilizes the learned cumulative experience information to enhance the global search ability of AuDE, thereby strengthening the search capability of the entire population. To verify the effectiveness and advantages of AuDE, 15 functions from CEC2015, 28 functions from CEC2017, and a real-world optimization problem on cooperative countermeasures of unmanned aerial vehicles were used to evaluate its performance compared with state-of-the-art DE variants. The experimental results indicate that the overall performance of AuDE is superior to other compared algorithms.},
  archive      = {J_ASOC},
  author       = {Zijian Cao and Kai Xu and Haowen Jia and Yanfang Fu and Chuan Heng Foh and Feng Tian},
  doi          = {10.1016/j.asoc.2024.112605},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112605},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An autonomous differential evolution based on reinforcement learning for cooperative countermeasures of unmanned aerial vehicles},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating the limit state space of quasi-nonlinear fuzzy cognitive maps. <em>ASOC</em>, <em>169</em>, 112604. (<a href='https://doi.org/10.1016/j.asoc.2024.112604'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi-Nonlinear Fuzzy Cognitive Maps (q-FCMs) generalize the classic Fuzzy Cognitive Maps (FCMs) by incorporating a nonlinearity coefficient that is related to the model’s convergence. While q-FCMs can be configured to avoid unique fixed-point attractors, there is still limited knowledge of their dynamic behavior. In this paper, we propose two iterative, mathematically-driven algorithms that allow estimating the limit state space of any q-FCM model. These algorithms produce accurate lower and upper bounds for the activation values of neural concepts in each iteration without using any information about the initial conditions. As a result, we can determine which activation values will never be produced by a neural concept regardless of the initial conditions used to perform the simulations. In addition, these algorithms could help determine whether a classic FCM model will converge to a unique fixed-point attractor. As a second contribution, we demonstrate that the covering of neural concepts decreases as the nonlinearity coefficient approaches its maximal value. However, large covering values do not necessarily translate into better approximation capabilities, especially in the case of nonlinear problems. This finding points to a trade-off between the model’s nonlinearity and the number of reachable states.},
  archive      = {J_ASOC},
  author       = {Leonardo Concepción and Gonzalo Nápoles and Agnieszka Jastrzębska and Isel Grau and Yamisleydi Salgueiro},
  doi          = {10.1016/j.asoc.2024.112604},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112604},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Estimating the limit state space of quasi-nonlinear fuzzy cognitive maps},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network traffic analysis based on cybersecurity intrusion detection through an effective automated separate guided attention federated graph neural network. <em>ASOC</em>, <em>169</em>, 112603. (<a href='https://doi.org/10.1016/j.asoc.2024.112603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for the Internet of Things (IoT) and various distributed devices has highlighted the need for reliable and effective intrusion detection systems to safeguard physical devices and data from cyber threats. However, managing large volumes of data with diverse dimensions and security features poses significant challenges, including reduced recognition accuracy, increased computational time, a large number of false positives, and overfitting issues. Integrating Artificial Intelligence (AI) into intrusion detection systems has recently gained attention to enable intelligent recognition and protection against cyber threats. This research introduces a novel architecture called Automated Separate Guided Attention Federated Graph Neural Network (ASGAFGNN) for predicting and detecting cyberattacks. Initially, cyberattack data is gathered and pre-processed to enhance its quality. The pre-processed data then undergoes feature extraction to obtain global, local, and temporal features using a hybrid vision transformer with bidirectional long short-term memory. The extracted features are further processed using Batched Sparse Principal Component Analysis (BSPCA) and Intuitionistic Fuzzy C-Means (IFCM) for feature reduction. Finally, ASGAFGNN is deployed to detect and categorize network traffic as benign or malicious. An Enhanced Osprey Optimization Algorithm (EOOA) minimizes the network’s loss function. The proposed technique is executed on four datasets including CICIDS2017, UNR-IDD, NSL-KDD, and NF-UQ-NIDS-v2, achieving detection accuracies of 99.98 %, 99.44 %, 99.74 %, and 99.95 %, respectively and it outperforms the existing schemes.},
  archive      = {J_ASOC},
  author       = {Smarajit Ghosh},
  doi          = {10.1016/j.asoc.2024.112603},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112603},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Network traffic analysis based on cybersecurity intrusion detection through an effective automated separate guided attention federated graph neural network},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Amodal instance segmentation with dual guidance from contextual and shape priors. <em>ASOC</em>, <em>169</em>, 112602. (<a href='https://doi.org/10.1016/j.asoc.2024.112602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human perception possesses the remarkable ability to mentally reconstruct the complete structure of occluded objects, which has inspired researchers to pursue amodal instance segmentation for a more comprehensive understanding of the scene. Previous works have shown promising results, but they often capture the contextual dependencies in an unsupervised way, which can lead to undesirable contextual dependencies and unreasonable feature representations. To tackle this problem, we propose a Pixel Affinity-Parsing (PAP) module trained with the Pixel Affinity Loss (PAL). Embedded into CNN, the PAP module can leverage learned contextual priors to guide the network to explicitly distinguish different relationships between pixels, thus capturing the intra-class and inter-class contextual dependencies in a non-local and supervised way. This process helps to yield robust feature representations to prevent the network from misjudging. To demonstrate the effectiveness of the PAP module, we design an effective Pixel Affinity-Parsing Network (PAPNet). Notably, PAPNet also introduces shape priors to guide the amodal mask refinement process, thus preventing implausible shapes in the predicted masks. Consequently, with the dual guidance of contextual and shape priors, PAPNet can reconstruct the full shape of occluded objects accurately and reasonably. Experimental results demonstrate that the proposed PAPNet outperforms existing state-of-the-art methods on multiple amodal datasets. Specifically, on the KINS dataset, PAPNet achieves 37.1% AP, 60.6% AP 50 and 39.8% AP 75 , surpassing C2F-Seg by 0.6%, 2.4% and 2.8%. On the D2SA dataset, PAPNet achieves 71.70% AP, 85.98% AP 50 and 77.10% AP 75 , surpassing PGExp by 0.75% and 0.33% in AP 50 and AP 75 , and being comparable to PGExp in AP. On the COCOA-cls dataset, PAPNet achieves 41.29% AP, 60.95% AP 50 and 46.17% AP 75 , surpassing PGExp by 3.74%, 3.21% and 4.76%. On the CWALT dataset, PAPNet achieves 72.51% AP, 85.02% AP 50 and 80.47% AP 75 , surpassing VRSPNet by 5.38%, 0.07% and 5.35%. The code is available at https://github.com/jiaoZ7688/PAP-Net .},
  archive      = {J_ASOC},
  author       = {Jiao Zhan and Yarong Luo and Chi Guo and Yejun Wu and Bohan Yang and Jingrong Wang and Jingnan Liu},
  doi          = {10.1016/j.asoc.2024.112602},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112602},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Amodal instance segmentation with dual guidance from contextual and shape priors},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning-driven dual neighborhood structure artificial bee colony algorithm for continuous optimization problem. <em>ASOC</em>, <em>169</em>, 112601. (<a href='https://doi.org/10.1016/j.asoc.2024.112601'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of optimization is rife with complex problems that demand efficient, robust, and adaptive algorithms to navigate multimodal landscapes and identify global optima. The artificial bee colony algorithm (ABC) has been widely recognized for its simplicity and effectiveness in global optimization tasks. However, the traditional ABC’s fixed neighborhood structure limits its adaptability in dynamic and complex search spaces. To address the above problem, we propose a novel reinforcement learning-driven dual neighborhood structure artificial bee colony algorithm with adaptive neighborhood search (RL_DNSABC) that introduces a dual neighborhood structure, allowing for more nuanced exploration and exploitation of the search space. Then, our approach leverages reinforcement learning to dynamically adjust the size of the neighborhood structure within the ABC based on the performance feedback. This adaptive mechanism ensures that the algorithm maintains a balance between exploration and exploitation, leading to improved convergence properties and robustness against local optima. Moreover, a novel individual selection technique and three modified search strategies with different preferences are devised based on the dual neighborhood structure. We demonstrate the efficacy of the RL_DNSABC through three benchmark sets and practical continuous engineering optimization problems and compare its performance against standard ABC and the other nineteen ABC variants. The results indicate that the RL_DNSABC achieves a competitive performance, showcasing its potential in solving complex optimization problems.},
  archive      = {J_ASOC},
  author       = {Tingyu Ye and Ping Zhang and Hui Wang and Hongliang Zeng and Jiahua Wang and Tao Zeng},
  doi          = {10.1016/j.asoc.2024.112601},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112601},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reinforcement learning-driven dual neighborhood structure artificial bee colony algorithm for continuous optimization problem},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal fusion point-interval forecasting: A comprehensive approach for financial time series prediction. <em>ASOC</em>, <em>169</em>, 112600. (<a href='https://doi.org/10.1016/j.asoc.2024.112600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of rapid information technology development, the financial markets are increasingly inundated with vast amounts of data, thereby underscoring the critical importance of accurate long-term forecasting of financial sequences. However, the development of a comprehensive long-term point-interval prediction system for financial time series remains an area requiring significant further research. To address this gap, we introduce the TFMADR model, designed to enhance both the accuracy and robustness of long-term financial sequence predictions. Specifically, we integrate the Temporal Fusion Transformer (TFT) long-term forecasting model with the DeepAR probabilistic forecasting model, combining their strengths to optimize prediction outcomes. In addition, we employ the Multi-Objective Simultaneous Search Algorithm (MSSA) for multi-objective optimization, enabling us to identify the optimal fusion parameter configuration. To further refine the predictive system, we incorporate correction factors aimed at improving both the precision and reliability of the forecasts. Comparative experiments conducted across four distinct financial markets reveal the superior predictive accuracy, robustness, and uncertainty analysis capabilities of the TFMADR model. These results demonstrate its potential for providing a more holistic understanding of dynamic fluctuations in financial markets, offering valuable insights for investors seeking to optimize strategies and mitigate risks. The innovative integration of deep learning models, multi-objective optimization, and corrective enhancements positions the TFMADR model as a promising tool for the future of financial sequence forecasting.},
  archive      = {J_ASOC},
  author       = {Xianghui Qi and Zhangyong Xu and Fenghu Wang},
  doi          = {10.1016/j.asoc.2024.112600},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112600},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Temporal fusion point-interval forecasting: A comprehensive approach for financial time series prediction},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D reconstruction of arbitrary granular media utilizing vision foundation model. <em>ASOC</em>, <em>169</em>, 112599. (<a href='https://doi.org/10.1016/j.asoc.2024.112599'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing three-dimensional (3D) granular microstructures through X-ray micro-computed tomography (μCT) imaging is significant for elucidating micromechanical behaviors of granular media and optimizing geotechnical designs. However, due to the irregular morphology and dense packing of granular media, traditional image-processing techniques often lack the precision required for accurate reconstructions. This paper presents a novel framework for accurate 3D reconstruction of arbitrary granular media using vision foundation models (VFMs). Two-dimensional (2D) mask maps representing the granular media are extracted from μCT images along the x , y , and z -axes using VFMs and then processed by a two-step strategy to repair textures and remove noises. An optimal transport (OT)-based method is employed to reconstruct a complete 3D mask map based on 2D mask maps. The proposed method is applied to reconstruct two carbonate sand samples with irregular grain shapes and four lentil samples composed of nearly 10,000 grains captured in triaxial loading, utilizing various VFMs and prompt configurations. The framework demonstrated a 50 % improvement in reconstruction accuracy over the state-of-the-art method for carbonate sands and achieved a 95 % accuracy for lentil samples. This advancement offers a more effective alternative for investigating the micromechanics of granular media.},
  archive      = {J_ASOC},
  author       = {Ruidong LI and Zhen-Yu YIN and Shao-Heng HE},
  doi          = {10.1016/j.asoc.2024.112599},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112599},
  shortjournal = {Appl. Soft. Comput.},
  title        = {3D reconstruction of arbitrary granular media utilizing vision foundation model},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimax bilevel fractional optimization for imaging in electrical capacitance tomography. <em>ASOC</em>, <em>169</em>, 112598. (<a href='https://doi.org/10.1016/j.asoc.2024.112598'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrical capacitance tomography shows great promise in measuring multiphase flow parameters, but its effectiveness is hampered by poor image reconstruction quality. To unlock the technique’s full potential, the image reconstruction problem is reformulated as a minimax bilevel fractional optimization problem. This innovative model accommodates uncertainties in both the reconstruction model and measurement data, while enabling automatic parameter adjustment. It leverages priors related to measurement physics, collected data, and reconstruction objects, and effectively bridges the gap between machine learning and the physical mechanisms underlying the measurement process. A potent nested optimizer that merges the proposed fractional optimization solver with the differential evolution algorithm is proposed to solve the minimax bilevel fractional optimization imaging model. The minimax bilevel physics-informed random vector functional link network is proposed to infer the supervised learning prior. To maintain predictions in line with core physical significances, the training process incorporates measurement physics through a newly devised minimax bilevel optimization training model. Comparative analysis demonstrates that this novel reconstruction approach significantly outperforms leading imaging algorithms in terms of reconstruction quality and noise robustness. This research presents a comprehensive solution to imaging challenges, and the insights and methodologies contribute to the advancement of computational imaging and electrical capacitance tomography.},
  archive      = {J_ASOC},
  author       = {Jing Lei and Qibin Liu},
  doi          = {10.1016/j.asoc.2024.112598},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112598},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Minimax bilevel fractional optimization for imaging in electrical capacitance tomography},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient object detection mechanism with LSTM-based object recognition for computer night vision images in edge and cloud environments. <em>ASOC</em>, <em>169</em>, 112597. (<a href='https://doi.org/10.1016/j.asoc.2024.112597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is the critical part for system considering the surveillances, images and so on. Traditionally, the object detection is being helped to identify the objects present in the source to get the related information and used for various applications. Nevertheless, the earlier detection models are focused on images with normal illumination level, when object detection in low illumination images is quite ignored. Object detection in images with high illumination is a simple task, and most of the research is also done in that field. However, identifying and locating the objects in night vision or images with low illumination is complex. Due to the advancement, the deep learning method is emerging in imaging technology and application. Yet, it hinders to achieve the same performance while detecting the night vision images due to bad light and low illumination condition. Therefore, a new night vision object detection system is executed in this work. To initiate this work, the mandatory images for conducting the research are gathered from the benchmark dataset. After that, the gathered images are enhanced by the multi-scale retinex network. These enhanced images are given to the object detection phase. Here, the object detection is carried out by the invented 3D Swin Transformer-based Yolov7 structure and the recognition is done using the Long Short-Term Memory (LSTM) approach. Lastly, the outcome achieved from the 3D Swin Transformer-based Deep Learning for Object Detection and Recognition Network (3DST-DL-ODRNet) is correlated with the classical object detection and the recognition system to verify the efficacy of the proposed model. Finally, the proposed system is examined with various measures for detection as well as recognition. Also to ensure the efficiency, the proposed framework is to be compared with classical approaches. On the analysis, the high value of IoU and mAP is achieved as 0.99 and 94.03 for dataset 1, whereas 0.96 and 94.77 for dataset 2. Similarly, in order to recognize the objects, the high accuracy value is obtained as 94.13 and 94.09 for dataset 1 and 2, respectively. Hence, through the experimental results, the suggested model is appropriately performed the object detection and recognition of night vision images.},
  archive      = {J_ASOC},
  author       = {Charles Prabu V and Pandiaraja Perumal},
  doi          = {10.1016/j.asoc.2024.112597},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112597},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient object detection mechanism with LSTM-based object recognition for computer night vision images in edge and cloud environments},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning with noisy labels via mamba and entropy KNN framework. <em>ASOC</em>, <em>169</em>, 112596. (<a href='https://doi.org/10.1016/j.asoc.2024.112596'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from corrupted data marginally degrades model performance. As deep learning proliferates, the need for large, accurately labeled datasets becomes crucial. Central to this challenge is the concept of learning with noisy labels (LNL). A crucial element in achieving successful learning via this approach is an initial warm-up phase that strengthens the discriminative power of the backbone. To address this issue, we introduce RelabelMamba, a novel model designed for noisy data that employs image sequences with position embeddings and leverages bidirectional state space models to compress visual representations. In contrast to traditional methods that rely on Convolutional Neural Networks (CNNs) with limited local receptive fields or Vision Transformers (ViT) which provide global receptive fields at the expense of quadratic complexity, our model achieves global receptive fields coverage with linear complexity. Additionally, we introduce two innovative components: Augment Entropy-variance K-Nearest Neighbors Relabel (AEKR), a label correction strategy that uses feature similarity and entropy-variance uncertainty to precisely correct mislabeled samples by evaluating their proximity in feature space, assigning normalized weights to nearest neighbors, and utilizing a score matrix with a predefined threshold to identify clean samples. This method also augments the data categories with fewer corrected samples, bringing their counts up to the average. Adaptpuzzle, enhanced by a refined mixup strategy, adjusts the mixing coefficient based on the variance of the data to optimize the fusion of sample pairs and their labels. Our results have either surpassed or closely matched the state-of-the-art on the CIFAR-10/100, Clothing1M, and ANIMAL-10N datasets, demonstrating the efficacy of RelabelMamba in LNL.},
  archive      = {J_ASOC},
  author       = {Ningwei Wang and Weiqiang Jin and Shirou Jing and Haixia Bi and Guang Yang},
  doi          = {10.1016/j.asoc.2024.112596},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112596},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning with noisy labels via mamba and entropy KNN framework},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked face recognition via dual-branch convolutional self-attention network. <em>ASOC</em>, <em>169</em>, 112595. (<a href='https://doi.org/10.1016/j.asoc.2024.112595'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has made great progress with the development of deep learning technology. However, when face recognition encounters mask occlusion, the recognition accuracy decreases significantly due to the loss of crucial face features and an increase in the intra-class divergence. The breakthrough in the accuracy of masked face recognition (MFR) is always accompanied by the explosive growth of network scale, which severely limits the application of the MFR models. To address these shortcomings, this study proposes a lightweight dual-branch convolutional self-attention network (LDCSN) for MFR, which consists of whole and partial branches. For the whole branch, a modified convolutional self-attention module (MCSAM) is constructed to extract local- and global-range features by taking advantage of the convolutional neural network (CNN) and Transformer. In the MCSAM, a modified multi-head self-attention (MMHSA) is designed to supplement channel information for vanilla MHSA, which can extract more discriminative global feature representation. For the partial branch, an upper face attention module (UFAM) extracting multi-scale upper face features is constructed and performed in the training phase to guide the whole branch to pay more attention to upper face areas, which can reduce intra-class distance by emphasizing the importance of unmasked face areas. The proposed MFR method is verified through experiments on multiple public datasets, and the results demonstrate that the proposed method can achieve superior performance compared with the state-of-the-art methods at less computational cost. Specifically, the proposed method improves the recognition accuracy on the MFR2 and Masked-whn datasets by 1.78 % and 2.05 %, respectively, compared to the comparison MFR method with the best performance.},
  archive      = {J_ASOC},
  author       = {Weiguo Wan and Runlin Wen and Linghan Deng and Yong Yang},
  doi          = {10.1016/j.asoc.2024.112595},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112595},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Masked face recognition via dual-branch convolutional self-attention network},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double deep Q-network-based dynamic offloading decision-making for mobile edge computing with regular hexagonal deployment structure of servers. <em>ASOC</em>, <em>169</em>, 112594. (<a href='https://doi.org/10.1016/j.asoc.2024.112594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering that the increasingly complex two-dimensional characteristics of real-world application scenarios such as diverse user needs and random user mobility bring great challenges to the applicability and effectiveness of traditional mobile edge computing (MEC) system models with linear deployment structure of edge servers, how to design a system model that can achieve efficient service offloading strategies while keeping a sound computation load balance across the edge servers is the key problem to be solved. To address the challenges faced by foregoing traditional MEC systems and the problem outlined above, this study proposes a double deep Q-network (DDQN) based dynamic offloading decision-making approach for MEC with regular hexagonal deployment structure of servers. First, we present an MEC system model in which edge servers are deployed in a regular hexagonal structure to cover a large region and balance the computation load across the edge servers as much as possible. The superiority of the proposed regular hexagonal structure-based server deployment in uniformly partitioning two-dimensional space is theoretically demonstrated by comparing it with three other possible server deployment structures. The proposed deployment's performance in terms of load balance and transmission efficiency for the overall system is also analyzed. Then, we formulate the dynamic offloading problem considered in the MEC system with sequential subtask linking relationships as an optimization problem aimed at minimizing the total task service latency of the system, and propose a novel DDQN-based dynamic offloading decision-making algorithm to optimize offloading decisions in the system. Finally, we conduct extensive experiments on the proposed system model and the proposed algorithm to demonstrate their effectiveness, and experimental results demonstrate that the proposals of this study are far superior to other similar studies and have good scalability and acceptable responsiveness, regardless of whether the complex application scenarios are mainly caused by the numbers of users or tasks.},
  archive      = {J_ASOC},
  author       = {Xiaoan Tang and Tianxiang Tang and Zibo Shen and Handong Zheng and Weiping Ding},
  doi          = {10.1016/j.asoc.2024.112594},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112594},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Double deep Q-network-based dynamic offloading decision-making for mobile edge computing with regular hexagonal deployment structure of servers},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A genetic algorithm for vehicle routing problems with time windows based on cluster of geographic positions and time windows. <em>ASOC</em>, <em>169</em>, 112593. (<a href='https://doi.org/10.1016/j.asoc.2024.112593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle routing problem with time windows (VRPTW) has attracted many scholars’ attention because it plays an important role in distribution and logistics. Many studies show that (meta-)heuristics are practical approaches for VRPTW. However, how to efficiently utilizing characteristics of customers’ time windows and geographic distributions is neglected in last few years. Thus, this paper proposes an improved genetic algorithm (GA) for VRPTW based on a c lustering method and the l ongest c ommon s ubstring (LCS) of elite and inferior individuals. In the proposed algorithm, called CLCS-GA, cluster information of customers’ geographic distributions and time windows is utilized to initialize three subpopulations with distinct properties. Moreover, during the evolutionary process, the LCS of elite and inferior individuals is utilized in the crossover and mutation operators to speed up the convergence and help individuals jump out of local optima. When performing the local search, which is a crucial operator for optimizing VRPTW, relatedness measured by customers’ geographic distribution and time windows are considered, aiming to overcome the blindness of the common local search. Comprehensive properties of CLCS-GA are extensively testified by 56 VRPTW instances, in which seven state-of-art algorithms are adopted as peer algorithms. Moreover, distinct characteristics of the proposed strategies are also analyzed based on a set of experiments.},
  archive      = {J_ASOC},
  author       = {Jiani Liu and Lei Tong and Xuewen Xia},
  doi          = {10.1016/j.asoc.2024.112593},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112593},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A genetic algorithm for vehicle routing problems with time windows based on cluster of geographic positions and time windows},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrigendum to “Multi population-based chaotic differential evolution for multi-modal and multi-objective optimization problems” appl. soft comput. 132 (2023) 109909. <em>ASOC</em>, <em>169</em>, 112592. (<a href='https://doi.org/10.1016/j.asoc.2024.112592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Hafiz Tayyab Rauf and Jiechao Gao and Ahmad Almadhor and Ali Haider and Yu-Dong Zhang and Fadi Al-Turjman},
  doi          = {10.1016/j.asoc.2024.112592},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112592},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Multi population-based chaotic differential evolution for multi-modal and multi-objective optimization problems” appl. soft comput. 132 (2023) 109909},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrigendum to “Towards an effective model for lung disease classification using dense capsule nets for early classification of lung diseases” appl. soft comput. 124 (2022) 109077. <em>ASOC</em>, <em>169</em>, 112591. (<a href='https://doi.org/10.1016/j.asoc.2024.112591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Faizan Karim and Munam Ali Shah and Hasan Ali Khattak and Zoobia Ameer and Umar Shoaib and Hafiz Tayyab Rauf and Fadi Al-Turjman},
  doi          = {10.1016/j.asoc.2024.112591},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112591},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Towards an effective model for lung disease classification using dense capsule nets for early classification of lung diseases” appl. soft comput. 124 (2022) 109077},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic evaluation of the safety risk during shield construction near existing tunnels via a pair-copula bayesian network. <em>ASOC</em>, <em>169</em>, 112583. (<a href='https://doi.org/10.1016/j.asoc.2024.112583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shield tunnel construction near existing tunnels can lead to the deformation and settlement of adjacent tunnels, posing safety risks. To analyze and evaluate the key factors influencing such safety risks, a risk assessment method based on the pair-copula Bayesian network (PCBN) model is proposed. The PCBN model, which integrates a Bayesian network (BN) with a pair copula (PC), is applied to simulate a complex risk factor system and related risk factors. Using the constructed PCBN model, a case study was conducted focusing on excavation between Wuhan Metro Lines 8 and 3, and it was determined that the deformation and settlement risk during shield tunnel construction falls into the safe risk category (Level 3). The key risk factors that significantly impact deformation and settlement risk were identified through correlation analysis and were ranked by importance as follows: advance rate, total thrust, and grouting pressure. Compared with traditional risk assessment methods and BNs, the PCBN model can flexibly handle complex relationships among risk factors, especially nonlinear and tail dependencies, due to the integration of PCs. This approach not only enhances the accuracy of risk assessment but also improves the interpretability of the model, with transparent internal mechanisms, largely due to the use of the BN. On the basis of these findings, decision-making recommendations are proposed to reduce the risk of shield tunnel construction near existing tunnels. The implementation of these recommendations in the Wuhan Metro project has yielded positive outcomes.},
  archive      = {J_ASOC},
  author       = {Hongyu Chen and Yu Lei and Lingyu Xia and Muhammet Deveci and Zhen-Song Chen and Yang Liu},
  doi          = {10.1016/j.asoc.2024.112583},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112583},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic evaluation of the safety risk during shield construction near existing tunnels via a pair-copula bayesian network},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-gated deep graph network with attention mechanisms for taxi demand prediction. <em>ASOC</em>, <em>169</em>, 112582. (<a href='https://doi.org/10.1016/j.asoc.2024.112582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate taxi demand prediction across urban road networks is critical for optimizing taxi operations and improving urban traffic management. Traditional approaches to this problem typically rely on static temporal and spatial correlations within the road network, assuming these correlations remain constant. However, taxi demand correlations are inherently dynamic, influenced by the complex and evolving patterns of passenger requests. To address this challenge, we propose MuDGN, a Multi-Gated Deep Graph Network model, designed to predict taxi demand variations across different areas of an urban road network. The MuDGN model integrates a graph multi-attention network, a graph convolutional network (GCN) layer, and a multi-gate mechanism to achieve accurate and robust predictions. The GCN layer enhances spatial feature representation, while the multi-gate mechanism, equipped with dual gating units, further improves predictive performance. Comprehensive experiments conducted on two real-world taxi demand datasets demonstrate the superiority of MuDGN over three traditional prediction models and four state-of-the-art deep graph network models in both single-period and multi-period taxi demand prediction scenarios. These results underscore the effectiveness of MuDGN in addressing the dynamic and complex nature of taxi demand forecasting.},
  archive      = {J_ASOC},
  author       = {Feng Guo and Zhaoxia Guo and Haifan Tang and Tao Huang and Youkai Wu},
  doi          = {10.1016/j.asoc.2024.112582},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112582},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-gated deep graph network with attention mechanisms for taxi demand prediction},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The distance and entropy measures-based intuitionistic fuzzy C-means and similarity matrix clustering algorithms and their applications. <em>ASOC</em>, <em>169</em>, 112581. (<a href='https://doi.org/10.1016/j.asoc.2024.112581'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the similarity between datasets by distance/similarity measures is important in most clustering algorithms. Therefore, this paper characterizes two clustering algorithms with improved distance measures. In the similarity matrix clustering algorithm, the mixed correlation coefficient and two probability parameters are suggested into the weighted probabilistic Euclidean distance (WPED) measure, which expresses the randomness and fuzziness of intuitionistic fuzzy sets. Second, based on the characteristics of dynamic time warping (DTW), a DTW-based probabilistic Euclidean distance (PED-DTW) method is introduced. The attribute ordering rule is developed with the help of entropy, which is beneficial for PED-DTW in achieving optimal path matching. Furthermore, a DTW-based intuitionistic fuzzy C-means algorithm is constructed. Finally, we performed experiments using car datasets and 7 UCI datasets. Compared with the existing method, the similarity matrix clustering algorithm is feasible. The DTW-based intuitionistic fuzzy C-means algorithms is compared with six clustering methods on four performance metrics, and the performance of our algorithm is analyzed. Our algorithm performs well on four performance metrics. Experimental results indicate that the clustering algorithms proposed in this paper are practical and outperform their existing counterparts. Also, the proposed algorithm gives good results in image segmentation.},
  archive      = {J_ASOC},
  author       = {Yueyue Zhang and Han-Liang Huang},
  doi          = {10.1016/j.asoc.2024.112581},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112581},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The distance and entropy measures-based intuitionistic fuzzy C-means and similarity matrix clustering algorithms and their applications},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). L1-norm twin support vector quantile regression. <em>ASOC</em>, <em>169</em>, 112580. (<a href='https://doi.org/10.1016/j.asoc.2024.112580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data, characterized by heterogeneity and asymmetry, have commonly emerged in various domains, such as finance, environmental science, and bioinformatics. In this paper, we propose L 1 -norm twin support vector quantile regression ( L 1 -TSVQR) for feature selection in heterogeneous and asymmetric data. When an L 1 -norm regularization term is incorporated, L 1 -TSVQR has an inherent sparsity-inducing property, which drives the regression coefficients towards zero to facilitate feature selection. To address the heterogeneity, L 1 -TSVQR employs a quantile parameter to yield a family of regression curves for representing the heterogeneous information across all data points. Additionally, L 1 -TSVQR constructs two smaller linear programming problems to yield two nonparallel hyperplanes, effectively capturing the lower and upper bounds of asymmetric information at each quantile level. The experimental results demonstrate that L 1 -TSVQR can precisely and efficiently identify representative features, thereby capturing heterogeneous and asymmetric information in high-dimensional datasets.},
  archive      = {J_ASOC},
  author       = {Ya-Fen Ye and Chen-Xuan Wang and Jia-Sen Tian and Wei-Jie Chen},
  doi          = {10.1016/j.asoc.2024.112580},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112580},
  shortjournal = {Appl. Soft. Comput.},
  title        = {L1-norm twin support vector quantile regression},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soft computing techniques in multi-criteria recommender systems: A comprehensive review. <em>ASOC</em>, <em>169</em>, 112579. (<a href='https://doi.org/10.1016/j.asoc.2024.112579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RS) play a crucial role in aiding decision-making by filtering information and reducing information overload. Multiple approaches such as collaborative filtering, content-based filtering, and hybrid filtering have revolutionized RS. The RS applications is predominantly used in diverse domains such as e-commerce, e-learning, and tourism. The emergence of multi-criteria recommender systems (MCRS) has further enhanced user experiences and satisfaction by incorporating user ratings on various item factors. However, the integration of multi-criteria ratings into RS poses challenges related to multidimensionality, data sparsity, and criteria weight elicitation. To provide accurate recommendations, researchers have proposed a range of soft computing-based MCRS approaches. In this paper, we review and classify the diverse soft computing techniques employed in MCRS. Moreover, we present a compact framework that utilizes soft computing techniques in multi-criteria collaborative filtering-based RS. We anticipate that this paper will benefit researchers engaged in RS and soft computing fields, offering valuable insights for future research and development.},
  archive      = {J_ASOC},
  author       = {Khalid Anwar and Mohammed Wasid and Aasim Zafar and M.A. Ganaie and Arshad Iqbal},
  doi          = {10.1016/j.asoc.2024.112579},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112579},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soft computing techniques in multi-criteria recommender systems: A comprehensive review},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dandelion center detection in perennial ryegrass with heat maps using convolutional neural networks. <em>ASOC</em>, <em>169</em>, 112576. (<a href='https://doi.org/10.1016/j.asoc.2024.112576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial agricultural practices largely involve automated weed control processes and techniques. This phenomenon can also be seen in horticulture and landscaping. Generally, dandelion weeds (Taraxacum officinale) are a common pest and their detection is necessary for any type of removal. To address this detection issue, a cost-effective and intuitive labeling method using Heat maps is proposed for marking dandelion plant centers within perennial rye-grass. This method relies on approximate localization as opposed to pinpoint accuracy. An expandable lightweight Convolutional Neural Network (CNN) is built on a base network to generate detection output maps at two resolutions. Multiple loss functions are expanded to multi-instance predictions and their combinations are examined through ablation to assess and rank their performance. Different methods of computing standard performance metrics are also explored. Also, different backbone networks are also shown to reveal varying performance advantages. Through these methods, dandelion weed centers can consistently be located with robustness to noise and erroneous labels and with good precision. Furthermore, our method is almost entirely end-to-end. The experimental results demonstrate that our methods outperform Semantic Segmentation models in the precision of output maps while avoiding the need of intensive labeling costs. In addition, when applying Hierarchical Clustering to the segmentation maps for a complete comparison in center detection, our methods double the accuracy and do not require the manual tuning of cluster parameters. Our proposed application of soft computing can be used in the landscaping industry and adapted to other fields with relative ease. The binary classification and object detection tasks of locating dandelion plants can be extended to multi-class problems with other plants.},
  archive      = {J_ASOC},
  author       = {Ibrahim Babiker and Jamal Bentahar and Wen-Fang Xie},
  doi          = {10.1016/j.asoc.2024.112576},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112576},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dandelion center detection in perennial ryegrass with heat maps using convolutional neural networks},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for agile earth observation satellites scheduling problem with variable image duration. <em>ASOC</em>, <em>169</em>, 112575. (<a href='https://doi.org/10.1016/j.asoc.2024.112575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Agile Earth Observation Satellites (AEOS) possess active imaging capabilities, which enable variable image durations during observations. Due to the problem’s high complexity, traditional heuristic algorithms are unable to provide optimal solutions within a feasible timeframe. In this paper, we propose an improved deep reinforcement learning approach (IDRL), including IDRL based on optimal observation quality (IDRL-MQ) and IDRL based on the longest observation duration (IDRL-MD), to solve the multi-objective scheduling problem for AEOS with variable image duration (MO-SPVID). Two different strategies IDRL-MQ and IDRL-MD, were designed to determine the start time and duration of observation. The experimental results demonstrate that IDRL-MD outperforms both IDRL-MQ and the superior heuristic algorithm ALNS-NSGAII in terms of solution quality and solution diversity. The demonstrated effectiveness of the appropriate heuristic strategies provides evidence for their rationality. Furthermore, the results obtained on instances of varying scales indicate that IDRL exhibits a high level of generality and robustness.},
  archive      = {J_ASOC},
  author       = {Man Wang and Zhongbao Zhou and Zhongxiang Chang and Enming Chen and Ruiyang Li},
  doi          = {10.1016/j.asoc.2024.112575},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112575},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning for agile earth observation satellites scheduling problem with variable image duration},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection in high-dimensional classification via an adaptive multifactor evolutionary algorithm with local search. <em>ASOC</em>, <em>169</em>, 112574. (<a href='https://doi.org/10.1016/j.asoc.2024.112574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As datasets grow in dimension and sample size, feature selection becomes increasingly important in machine learning. Features are often associated with multiple tasks, so adopting a multi-task optimization framework in feature selection can improve its classification performance. Multifactor optimization provides a powerful evolutionary multi-tasking paradigm capable of simultaneously handling multiple related optimization tasks. Taking inspiration from these, this article proposes a parameter adaptive multifactor feature selection algorithm (AMFEA). To help the algorithm escape from local optima, AMFEA uses a local search strategy to assist the algorithm in finding the global optimum. In addition, AMFEA has designed an adaptive knowledge transfer parameter matrix that dynamically adjusts parameter sizes based on the population’s fitness to control the frequency of knowledge transfer between tasks. This effectively transfers knowledge between different tasks and helps the algorithm converge quickly. Experimental results on 18 high-dimensional datasets show that AMFEA significantly improves classification accuracy compared with evolutionary algorithms and traditional feature selection methods.},
  archive      = {J_ASOC},
  author       = {Zhihui Li and Hong Li and Weifeng Gao and Jin Xie and Adam Slowik},
  doi          = {10.1016/j.asoc.2024.112574},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112574},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection in high-dimensional classification via an adaptive multifactor evolutionary algorithm with local search},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A system of trading in the foreign exchange market based on multi-criteria optimization under belief-plausibility uncertainty. <em>ASOC</em>, <em>169</em>, 112573. (<a href='https://doi.org/10.1016/j.asoc.2024.112573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new Forex multi-criteria trading model based on the Belief-Plausibility ( B e l − P l ) extension is developed and validated. The B e l − P l approach is, in fact, a meta-theory in relation to the classical theory of fuzzy sets F S T and the intuitionistic theory of fuzzy sets ( I F S T ). This allows us to use more available information about the simulated process than these theories. The developed model consists of two interconnected, but different parts combined in the automated trading system ( A T S ). The first part of the Forex trading model is responsible for generating multi-indicator trading signals based on a set of new proposed technical analysis indicators that have been extended in the framework of the B e l − P l approach. The second part of the model is a multiple-criteria B e l − P l extended hierarchical trading model estimation. These two parts of the model are combined into the developed A T S , which implements the appropriate optimization procedure at the optimization stages and the simulation of the trading process at the testing or real trading stages. The advantages of the developed model and A T S were shown by comparing their results with the results obtained using fuzzy multi-criteria models and a single-criterion model. All studies were conducted over the past two years (2022–2023) for the H1 and H4 timeframes and three currency pairs EURUSD, USDJPY and GBPUSD.},
  archive      = {J_ASOC},
  author       = {Krzysztof Kaczmarek and Pavel Sevastjanov and Ludmila Dymova and Adam Kulawik and Leszek Rutkowski},
  doi          = {10.1016/j.asoc.2024.112573},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112573},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A system of trading in the foreign exchange market based on multi-criteria optimization under belief-plausibility uncertainty},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy multi-neighborhood entropy-based interactive feature selection for unsupervised outlier detection. <em>ASOC</em>, <em>169</em>, 112572. (<a href='https://doi.org/10.1016/j.asoc.2024.112572'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection is one of the important techniques for unsupervised knowledge discovery, which aims to reduce the dimensionality of conditional feature sets as much as possible to improve the efficiency and accuracy of the algorithm. However, existing methods have the following two challenges: (1) They are mainly applicable to select numerical or nominal features and cannot effectively select heterogeneous features; (2) The relevance and redundancy are primarily considered to construct feature evaluation indexes, ignoring the interaction information of heterogeneous features. To solve the challenges mentioned above, this paper proposes an unsupervised heterogeneous feature selection method based on fuzzy multi-neighborhood entropy, which also considers the multi-correlation of features to select heterogeneous features. First, the fuzzy multi-neighborhood granule is constructed by considering the distribution characteristics of the data. Then, the concept of fuzzy entropy is introduced to define the fuzzy multi-neighborhood entropy and its associated uncertainty measures, and the relationship between them is discussed. Next, the relevance, redundancy, and interactivity among attributes are defined, and the idea of maximum relevance-minimum redundancy-maximum interactivity is used to construct the evaluation indexes of heterogeneous features. Finally, experiments are conducted on several publicly unbalanced datasets, and the results are in comparison with existing algorithms. The experimental results show that the proposed algorithm can select fewer heterogeneous features to improve the efficiency of outlier detection tasks. The code is publicly available online at https://github.com/BELLoney/MNIFS .},
  archive      = {J_ASOC},
  author       = {Siyu Yang and Zhong Yuan and Chuan Luo and Hongmei Chen and Dezhong Peng},
  doi          = {10.1016/j.asoc.2024.112572},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112572},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy multi-neighborhood entropy-based interactive feature selection for unsupervised outlier detection},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A joint entity and relation extraction framework for handling negative samples problems in named entity recognition. <em>ASOC</em>, <em>169</em>, 112570. (<a href='https://doi.org/10.1016/j.asoc.2024.112570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific articles and reports contain various domain-specific knowledge in the form of entities and relations between them. In recent years, such knowledge including overlapping entities are extracted mainly by span-based joint extraction methods, aiming at collecting key semantic information about text topics and assisting researchers to understand the texts. Existing span-based methods of joint extraction have mainly focused on acquiring more comprehensive span embeddings in entity classification, but the data imbalance and hard negative samples problems have not been fully explored, leading to incorrect entity and relation classification in knowledge extraction. To this end, we propose a joint entity and relation extraction framework (JEREF) to better learn negative and positive samples in entity classification. Specifically, JEREF not only provides a binary boundary predictor to learn positive sample boundaries but also supplies a learning strategy consisting of self-paced learning and span-level contrastive learning to balance the data distribution and distinguish hard negative samples. Our framework is evaluated on SciERC and ADE datasets strictly, outperforming other state-of-the-art methods while achieving 42.15% and 83.24% micro − F 1 scores on joint extraction, respectively.},
  archive      = {J_ASOC},
  author       = {Hongbin Zhang and Guangyu Lin and Kezhou Chen and Nankai Lin and Lianglun Cheng and Aimin Yang},
  doi          = {10.1016/j.asoc.2024.112570},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112570},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A joint entity and relation extraction framework for handling negative samples problems in named entity recognition},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single image super-resolution with lightweight multi-scale dilated attention network. <em>ASOC</em>, <em>169</em>, 112569. (<a href='https://doi.org/10.1016/j.asoc.2024.112569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution is widely applied in image enhancement, medical imaging, satellite remote sensing, and more. It aims to increase image resolution, making details clearer and providing higher quality visual information for visual analysis and data interpretation. Recently, deep learning based super-resolution has advanced significantly but often comes with excessive computational costs owing to a high number of parameters. To address this problem, researchers are increasingly focusing on convolutional neural networks (CNN) with attention mechanisms. However, small convolution kernels can limit the receptive field and potentially lead to information loss. Conversely, large convolution kernels, while capable of capturing extensive information, introduce redundancy due to increased parameters. Thus, we propose a multi-scale dilated attention network (MDAN). MDAN employs multiple feature refinement modules (FRMs) for feature extraction and integrates high-/low-frequency feature information using multiple residual connections. Each FRM comprises a local residual module (LRM) and a global attention module (GAM). LRM integrates blueprint separable convolution with residual connections to achieve lightweight deep feature extraction, effectively addressing the issue of excessive computational costs associated with high parameter counts. GAM incorporates a multi-scale dilated attention block (MDAB) and a blueprint feed-forward network (BFN), strategically expanding the convolution kernel size in a lightweight manner, overcoming the limitations of small convolution kernels that can restrict the receptive field and potentially result in information loss. Experimental results show that MDAN achieves state-of-the-art results in terms of Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), while notably reducing the number of parameters. For example,on the Set5 dataset for upscaling factor × 4, our MDAN achieved a PSNR value of 32.41 and an SSIM value of 0.8977 with Params at 330k. MDAN offers a more precise and lightweight solution for the field of single image super-resolution.},
  archive      = {J_ASOC},
  author       = {Xiaogang Song and Xinchao Pang and Lei Zhang and Xiaofeng Lu and Xinhong Hei},
  doi          = {10.1016/j.asoc.2024.112569},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112569},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Single image super-resolution with lightweight multi-scale dilated attention network},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal gait design for a soft quadruped robot via multi-fidelity bayesian optimization. <em>ASOC</em>, <em>169</em>, 112568. (<a href='https://doi.org/10.1016/j.asoc.2024.112568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on the locomotion capability improvement in a tendon-driven soft quadruped robot through an online adaptive learning approach. Leveraging the inverse kinematics model of the soft quadruped robot, we employ a central pattern generator to design a parametric gait pattern, and use Bayesian optimization (BO) to find the optimal parameters. Further, to address the challenges of modeling discrepancies, we implement a multi-fidelity BO approach, combining data from both simulation and physical experiments throughout training and optimization. This strategy enables the adaptive refinement of the gait pattern and ensures a smooth transition from simulation to real-world deployment for the controller. Compared to previous result using a fixed gait pattern, the multi-fidelity BO approach improves the robot’s average walking speed from 0.14 m/s to 0.214 m/s, an increase of 52.7%. Moreover, we integrate a computational task off-loading architecture by edge computing, which reduces the onboard computational and memory overhead, to improve real-time control performance and facilitate an effective online learning process. The proposed approach successfully achieves optimal walking gait design for physical deployment with high efficiency, effectively addressing challenges related to the reality gap in soft robotics.},
  archive      = {J_ASOC},
  author       = {Kaige Tan and Xuezhi Niu and Qinglei Ji and Lei Feng and Martin Törngren},
  doi          = {10.1016/j.asoc.2024.112568},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112568},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal gait design for a soft quadruped robot via multi-fidelity bayesian optimization},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive probabilistic neural network based edge data center authentication for secure load balancing in fog computing. <em>ASOC</em>, <em>169</em>, 112567. (<a href='https://doi.org/10.1016/j.asoc.2024.112567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of current technological advances has promising of fog computing, that increases the processing capacity of devices and provides fresh approaches for conventional applications in industry. Fog computing has become increasingly vital given the huge and sophisticated system of sensors to manage the information stream. Node authentication and secure load balancing are important in fog computing. The rationale behind developing this model is that the existing deep learning models are affected by the node authentication performance and also generate issues like high latency developed by the cloud-based architecture while processing huge amount of data, and node employment. Thus, the objective of the developed model is to implement an advanced deep learning method for better node authentication and an appropriate enhanced optimization scheme for performing optimal resource allocation. Additionally, secure load balancing is done at edge node centers in fog computing to allocate the workload for each node securely. Initially, the network initialization attributes in fog computing are provided to the authentication process. Here, the deep learning-based Edge Data Centre (EDC) authentication takes place with the help of the Adaptive Probabilistic Neural Network (A-PNN) model. The major intention of introducing an A-PNN method is to check whether the node is authenticated or not. If the EDC node is authenticated, then the A-PNN model suggests the particular EDC node for the resource allocation process, or else the node will be ignored or removed from the process. Here, the new hybrid approaches as Hybrid Heap with African Buffalo Optimization Algorithm (HH-ABO) are suggested for execute the process of optimization. Here, the parameters of the A-PNN model are tuned for maximizing accuracy, precision, and NPV and also minimizing the False Positive Rate (FPR) rate. Then the authenticated node is subjected to the load-balancing scheme. The allocation of allocation is carried out in the load balancing scheme. Here, the diverse constraints like resource allocation, energy consumption, cost, the execution time of the task, security by authentication, latency, and task completion time are considered for formulating the objective function using the HH-ABO algorithm. At last, it is performed well than the traditional methods using diverse parameters. The outcome demonstrate that the developed model achieved better results on load balancing at edge nodes in fog computing. The proposed model effectively resolves the load balancing issues and also it distributes workload for each node accurately. It effectively resolves the latency and deployment issues. In addition, the modern applications based on the proposed scheme will allow million of workloads from one node to another in a fast and reliable manner.},
  archive      = {J_ASOC},
  author       = {V. Gowri and B. Baranidharan},
  doi          = {10.1016/j.asoc.2024.112567},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112567},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive probabilistic neural network based edge data center authentication for secure load balancing in fog computing},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of metaverse-based digital transformation strategies via an interval-valued q-rung orthopair fuzzy methodology. <em>ASOC</em>, <em>169</em>, 112566. (<a href='https://doi.org/10.1016/j.asoc.2024.112566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Metaverse is expected to trigger substantial changes to the tourism industry. For example, hotels may change the guest booking experience by implementing this revolutionary technology, which allows customers to take virtual tours of hotel rooms before booking them. In this regard, the study aims to solve the problem of determining the best digital transformation strategy by examining different characteristics of metaverse applications in Turkish corporate travel and hotel companies. The interval-valued q-rung orthopair fuzzy methodology was used to solve the problem in the study. The study's findings include important implications for the literature, businesses, and the tourism industry about metaverse and real-world problems. Furthermore, the implementation scenarios and directions of digital transformation strategies for the metaverse are discussed. Businesses can use the study's proposed model to develop new business models based on metaverse advancements. According to the findings, the virtual tour route for the Metaverse application is the most important criterion. As a result, practitioners in the tourism sector must understand the significance of the virtual tour route in achieving successful digital transformation outcomes. The study's findings reveal that developing a high-level business model rather than traditional business models is the ideal strategy for digital transformation in the tourism industry. It also provides an overview of digital transformation strategy achievements in different areas of the travel and hotel businesses by evaluating the current status of metaverse applications and reporting on the potential problems in building and maintaining these applications. The study is a pioneer in giving a comprehensive perspective on metaverse application solutions, development, management, and digital transformation strategies in corporate hotel and travel businesses.},
  archive      = {J_ASOC},
  author       = {Ahmet Aytekin and Selçuk Korucuk and Hakan Akyurt and Hamza Doğan and Željko Stević and Edmundas Kazimieras Zavadskas},
  doi          = {10.1016/j.asoc.2024.112566},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112566},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of metaverse-based digital transformation strategies via an interval-valued q-rung orthopair fuzzy methodology},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic linguistic carpooling matching decision-making considering time satisfaction based on prospect theory. <em>ASOC</em>, <em>169</em>, 112564. (<a href='https://doi.org/10.1016/j.asoc.2024.112564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the travel efficiency of carpooling demanders and reduce travel costs, a carpooling matching decision-making method is proposed that considers preference satisfaction and time satisfaction in a probabilistic linguistic environment. First, the multiattribute many-to-one carpooling matching problem in the probabilistic linguistic environment is described. To solve this problem, based on probabilistic linguistic term set (PLTS) evaluation matrices, attribute prospect values are calculated via prospect theory, and based on probabilistic linguistic preference relations (PLPRs) for attributes, group consistency attribute weights are calculated. Second, cumulative prospect values are calculated by aggregating attribute prospect values and attribute weights and then standardized to obtain the preference satisfactions of passengers and drivers. Third, based on the interval expected waiting time provided by passengers and the estimated arrival time provided by drivers, the time satisfaction of passengers is calculated considering time tendency coefficients. Then, the comprehensive satisfaction of passengers is calculated by aggregating preference satisfaction and time satisfaction via the geometric average method. Furthermore, based on the comprehensive satisfaction of passengers and the preference satisfaction of drivers, a many-to-one carpooling matching model is constructed and then solved to obtain an optimal many-to-one carpooling matching scheme. An example analysis shows the effectiveness, reliability and accuracy of the proposed method. The main innovations of this study are as follows: (1) A calculation method for the improved score of PLTSs considering hesitation is proposed. (2) Two algorithms for calculating unknown attribute weights are designed based on PLPRs. (3) A calculation method for time satisfaction considering time tendency coefficients is presented. (4) A model for probabilistic linguistic multiattribute many-to-one carpooling matching considering time satisfaction is built.},
  archive      = {J_ASOC},
  author       = {Qi Yue and Shijie Huang and Yuan Tao and Xufang Li and Yu Gao and Jialin Ren},
  doi          = {10.1016/j.asoc.2024.112564},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112564},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Probabilistic linguistic carpooling matching decision-making considering time satisfaction based on prospect theory},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multidimensional latent cognitive diagnosis based on rough concept analysis and item response theory. <em>ASOC</em>, <em>169</em>, 112563. (<a href='https://doi.org/10.1016/j.asoc.2024.112563'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid increase of the amount of learning resources has brought numerous challenges to personalized learning. Although various cognitive diagnosis models and path recommendation models have been developed, there exist many challenges, especially in the accuracy of diagnosing learners’ latent cognitive traits and efficiency of optimizing personalized learning paths. In this paper, we build a multidimensional cognitive diagnosis model by combining fuzzy set theory and item response theory. In particular, we use the maximum likelihood estimation method to assess learners’ cognitive state on knowledge/skills and learning resources. In addition, based on rough concept analysis and genetic algorithms, we develop a model of personalized learning path optimization. Finally, data experiments are conducted to demonstrate the efficiency of the proposed models.},
  archive      = {J_ASOC},
  author       = {Lankun Guo and Zhimin Liu and Guozhi Ma and Qingguo Li},
  doi          = {10.1016/j.asoc.2024.112563},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112563},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multidimensional latent cognitive diagnosis based on rough concept analysis and item response theory},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distance-based three-way conflict analysis for fuzzy multiset. <em>ASOC</em>, <em>169</em>, 112562. (<a href='https://doi.org/10.1016/j.asoc.2024.112562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy multisets(FMs), as an extension of fuzzy sets, serve as an effective tool in real-life scenarios for describing uncertain information, effectively mitigating the loss of information. In combination with conflict situations, FMs can adequately represent agents’ attitudes in decision-making. However, current research rarely explores conflict analysis(CA) within the framework of fuzzy multiset situation tables(FMSTs). In this paper, we introduce a three-way conflict analysis model(TWCAM) by combining three-way decision theory(TWDT) and CA based on fuzzy multiset information systems(FMISs). First of all, the proposed TWCAM incorporates the concepts of conflict, neutral, and alliance sets for both single and multiple issues, utilizing conflict distance and conflict function to compute these sets. Moreover, a new method is proposed to objectively determine issue weights by considering both absolute and relative weights. According to Bayesian minimum risk theory, this study presents a new way of determining thresholds to trisect the agents. Additionally, we propose the notion of maximal coalitions(MCs) in FMSTs as a generalization of Pawlak’s model. Finally, we utilize a case study to prove the practicability of our model and some comparative analyses to illustrate its effectiveness and superiority.},
  archive      = {J_ASOC},
  author       = {Jing Tang and Chunfang Chen and Fen Rao},
  doi          = {10.1016/j.asoc.2024.112562},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112562},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A distance-based three-way conflict analysis for fuzzy multiset},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Elite-based butterfly optimization algorithm and its application in speckle projection technique. <em>ASOC</em>, <em>169</em>, 112561. (<a href='https://doi.org/10.1016/j.asoc.2024.112561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the butterfly optimization algorithm (BOA) has attracted a lot of attention because of its precise balance mechanism between exploitation and exploration. However, due to the shortcomings such as insufficient accuracy and slow convergence speed, the practical application of BOA is limited to a certain extent. This paper has two main contributions: 1) An elite-based butterfly optimization algorithm (eBOA) is proposed, which introduces an elite subgroup and gives full play to the positive role of elite elements in evolution. eBOA uses the optimal butterfly and the random elite butterfly to intervene in the global and local search operators respectively, thereby guiding the population to evolve in a more potential direction. Then, eBOA superimposes an existing parameter as a weight on the basis vector of the global search operator to speed up the convergence. 2) A pre-processing method for speckle images and disparity data that can be used in deep learning is proposed, and a novel 3D speckle reconstruction method is generated by combining eBOA with a deep network. In terms of results, comparative experiments based on 20 benchmark functions and 7 advanced meta-heuristic algorithms show that eBOA has excellent optimization capability; The comparison experiments of 5 deep networks confirm that the new reconstruction method can significantly improve the accuracy of speckle reconstruction models, and once again verify the remarkable optimization performance of eBOA.},
  archive      = {J_ASOC},
  author       = {Xuxu Zhong and Binbin Liang},
  doi          = {10.1016/j.asoc.2024.112561},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112561},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Elite-based butterfly optimization algorithm and its application in speckle projection technique},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Membrane computing for IoT task offloading: An efficient multi-objective constrained optimization framework. <em>ASOC</em>, <em>169</em>, 112560. (<a href='https://doi.org/10.1016/j.asoc.2024.112560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational tasks associated with Internet of Things (IoT) applications have become increasingly complex, with IoT devices (IoTDs) now being utilized in a multitude of contexts across people's daily lives. In view of the restricted computing resources and battery life of IoT devices, an increasing number of computing tasks are being handled by cloud servers. However, the restricted communication range of cloud servers gives rise to significant communication costs. Consequently, these complex computational tasks are transferred to edge servers situated in close proximity to IoT devices, with the objective of mitigating transmission delay and costs. To address the challenge of offloading decisions and obtain a multi-objective offloading decision that satisfies multiple constraints in an edge-cloud scenario, this study presents a framework of multi-objective constrained evolutionary algorithms based on membrane computing for solving constrained multi-objective computational offloading problems (MCMS-CMOEA). The framework draws inspiration from the independent parallelism observed within membrane computing membranes and the information exchange between them. Its objective is to minimize latency and energy consumption. The framework is structured into three phases, with the aim of exploring the solution space and increasing the diversity of the final solution set in a stepwise manner. In order to investigate the performance of the MCMS-CMOEA algorithm, a comparative analysis was conducted, in which the proposed algorithm was tested against nine state-of-the-art Constrained Multi-Objective Evolutionary Algorithms (CMOEAs) in three constrained multi-objective benchmark suites with different characteristics and challenges. Furthermore, the MCMS-CMOEA is compared with four offloading schemes on computational offloading problems of varying dimensions. The convergence and diversity of the algorithm are evaluated by two different comprehensive metrics: inverted generational distance (IGD) and hypervolume (HV). The numerical results of both the benchmark suite and the computational offloading problem demonstrate that the proposed algorithm exhibits superior convergence and diversity. Furthermore, the algorithm's composite results on different experiments are also more efficient and competitive, which validates the proposed algorithm as a promising approach for solving constrained multi-objective computational offloading problems.},
  archive      = {J_ASOC},
  author       = {Shouheng Tuo and Yihao Huyan and Ting Fan and Yong Zhao},
  doi          = {10.1016/j.asoc.2024.112560},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112560},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Membrane computing for IoT task offloading: An efficient multi-objective constrained optimization framework},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segmentation algorithm for honeycomb lung CT images based on pyramidal pooling channel transformer. <em>ASOC</em>, <em>169</em>, 112557. (<a href='https://doi.org/10.1016/j.asoc.2024.112557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate lesion segmentation plays a crucial role in the clinical diagnosis of honeycomb lung. However, the diversity of honeycomb lung shapes, distributions, and textures makes honeycomb lung segmentation a challenging task. Nonnegligible drawbacks exist when using pure Convolutional Neural Network (CNN) and pure Transformer methods for lesion segmentation. The convolution operation in CNN cannot capture comprehensive global information, which seriously affects the accuracy of lesion segmentation. The Transformer, which can extract global features effectively, is very sensitive to the positional information in the input sequence, and is difficult to capture local features, which makes the model very easy to overfit or generate inaccurate segmentation results. To solve the above problems, a segmentation model is proposed for optimizing skip connections of U-shaped convolutional networks using a channel Transformer, which uses the Gaussian context transformer and ConvMixer to optimize the encoder-decoder structure of the network. In addition, simple skip connections are improved using the Pyramidal Pooling Channel Transformer. This module efficiently exploits the semantic information in multi-scale channel features, thus giving sufficient information for decoding. Compared with other classical deep learning methods, our model has better results for the segmentation of honeycomb lung lesions, where the IoU is 89.12 %, Dice coefficient is 93.82 %, mIoU is 94.45 %, and precision is 91.31 %, which is better than other methods.},
  archive      = {J_ASOC},
  author       = {Li Gang and Li Zhichao and Zhang Ling and Cheng Guijuan and Zhang Kairu and Li Rui},
  doi          = {10.1016/j.asoc.2024.112557},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112557},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Segmentation algorithm for honeycomb lung CT images based on pyramidal pooling channel transformer},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical image zero watermarking algorithm based on dual-tree complex wavelet transform, AlexNet and discrete cosine transform. <em>ASOC</em>, <em>169</em>, 112556. (<a href='https://doi.org/10.1016/j.asoc.2024.112556'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed for medical image using zero watermarking, dual-tree complex wavelet transform (DTCWT)-AlexNet and discrete cosine transform (DCT). Furthermore, we utilize the pre-training network AlexNet to extract significant characteristics from medical images. In addition, DTCWT and DCT are employed to convert the complex features, while a perceptual hash function is utilized to cause the feature vector. The original watermark is chaotically scrambled, an exclusive-OR (XOR) gate is inserted into the medical image, and a logical key vector is generated and stored to encrypt the watermark. In general, the medical image under test is used to cutting the important features and create a feature vector using the feature extraction technique. Subsequently, the logical key vector and the feature vector undergo an XOR operation to produce the encrypted watermark. By calculating the normalized correlation (NC) coefficient, the recovered watermark can be utilized to regulate the rights and specifics of the watermark in the medical image after obtaining and decrypting the encrypted watermark with greater than 0.5 NC values. The algorithm's design method tackles the challenge of protecting watermarks from conventional (Gaussian noise, JPEG, median filter) and geometric attacks ( rotation clockwise, anticlockwise, scaling, translation left, translation right, clipping X and Y direction) by incorporating the principles of DTCWT, AlexNet, DCT, and zero watermarking. Encrypting the watermark image through a scrambling process enhances the security of the approach, by ensuring that the watermark should not affect the quality of the image in any way. The research demonstrates that the proposed algorithm is robust and secure for protecting sensitive health information.},
  archive      = {J_ASOC},
  author       = {Saqib Ali Nawaz and Jingbing Li and Dekai Li and Muhammad Usman Shoukat and Uzair Aslam Bhatti and Muhammad Ahmad Raza},
  doi          = {10.1016/j.asoc.2024.112556},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112556},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Medical image zero watermarking algorithm based on dual-tree complex wavelet transform, AlexNet and discrete cosine transform},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benchmark for the scheduling problems of airport ground support operations and a case study. <em>ASOC</em>, <em>169</em>, 112555. (<a href='https://doi.org/10.1016/j.asoc.2024.112555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Airport ground support operations are critical services provided for aircraft after landing and before takeoff and have a great impact on airport management efficiency and aviation economics. Recently, there has been a growing interest in addressing scheduling issues related to airport ground support operations. However, the lack of publicly available and standardized benchmarks presents a challenge for researchers aiming to conduct comprehensive studies as well as effectively benchmark various solution algorithms. Hence, in this paper, an algorithm to generate diverse instances has been proposed. Using this algorithm, a set of benchmarks has been organized. Particularly, a set of features and an iterative selection mechanism are proposed to increase the diversity of generated instances. To validate that the benchmark can be supported to investigate the scheduling problems of airport ground support operations and reveal different performances among various algorithms, a multi-objective optimization problem is introduced as a case study. Then, five heuristic algorithms are evaluated on different sets of instances. Finally, the experimental results demonstrate that the benchmark is capable of distinguishing various algorithms even on different metrics.},
  archive      = {J_ASOC},
  author       = {Zhihao Cai and Wanru Gao and Ran Feng and Yafei Li and Mingliang Xu},
  doi          = {10.1016/j.asoc.2024.112555},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112555},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Benchmark for the scheduling problems of airport ground support operations and a case study},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure quantum‐based adder design for protecting machine learning systems against Side‐Channel attacks. <em>ASOC</em>, <em>169</em>, 112554. (<a href='https://doi.org/10.1016/j.asoc.2024.112554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) has recently been adopted in various application domains. Usually, a well-performing ML model relies on a large volume of training data and powerful computational resources. Recently, hardware accelerators utilizing field programmable gate arrays (FPGAs) have been developed to provide high-performance hardware while maintaining the required accuracy for ML tools. However, one of the main challenges hindering the FPGA-based ML models is their susceptibility to adversarial attacks, such as physical side-channel attacks. In this study, various kinds of countermeasures, including masking and hiding techniques, are examined to mitigate the aforementioned shortcomings and enhance the security of FPGA-based ML systems. In addition to FPGA-based defenses, the advantages of quantum computing for designing circuits to enhance data protection are also elaborated. However, concerning FPGA-based ML models, which are used to defend against physical side-channel attacks, quantum dot cellular automata (QCA) offers a more promising option. Its inherent security, lower power consumption, higher speed, and reduced vulnerability to side-channel leakage make it the best alternative. Therefore, this study emphasizes the implementation of the quantum nature of QCA to protect valuable information against physical side-channel attacks. It also offers quantum masking circuits for protecting sensitive information in machine learning systems, including XOR, adder, and RCA. Furthermore, the presented work advocates for leveraging QCA technology to augment the security of machine learning systems by mitigating the disclosure of sensitive data. The proposed QCA-based masked designs, which include an adder and a ripple carry adder (RCA), pose some qualities, which include a single-layer structure, minimal cell count, and low latency. When compared with the best counterparts among the recommended designs, these designs exhibit significant improvements regarding cell consumption and occupied area, with improvements of 33.3% and 36.6% respectively.},
  archive      = {J_ASOC},
  author       = {Noor Ul Ain and Seyed-Sajad Ahmadpour and Nima Jafari Navimipour and E. Diakina and Sankit R. Kassa},
  doi          = {10.1016/j.asoc.2024.112554},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112554},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Secure quantum‐based adder design for protecting machine learning systems against Side‐Channel attacks},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MobileNetV2-based deep learning architecture with progressive transfer learning for accurate monkeypox detection. <em>ASOC</em>, <em>169</em>, 112553. (<a href='https://doi.org/10.1016/j.asoc.2024.112553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent monkeypox disease outbreak and its serious implications emphasize the urgent necessity for advanced diagnostic tools capable of rapid and accurate disease detection. In response, this study introduces a novel architecture for monkeypox disease identification, leveraging the capabilities of MobileNetV2 combined with progressive transfer learning and innovative model enhancements. The research began by analyzing six pre-trained models, with MobileNetV2 emerging as the most promising due to its balance of efficiency and performance. Using a self-assembled dataset, the model initially achieved an accuracy of 93.79%. Through strategic data augmentation, this performance increased to 97.85%. Further enhancements incorporated a Residual Dilated Spatial Pyramid Integration (ResDSPI) block, boosting feature extraction capabilities and elevating accuracy to 98.83%. The integration of an Efficient Channel Attention (ECA) module propelled the accuracy to an exceptional 99.34%. Each development stage was meticulously designed to ensure systematic improvements in the model’s ability to diagnose monkeypox effectively. The proposed model was rigorously tested on two datasets: the Monkeypox Skin Images Dataset (MSID) and the Monkeypox Skin Lesion Dataset (MSLD). In the binary classification scenario of the MSLD (monkeypox vs. others), our model achieved superior performance, with an accuracy of 95.55%, precision of 95.45%, and sensitivity of 92%. For the multi-class classification task on the MSID, which includes four classes (Chickenpox, Measles, Monkeypox, and Normal), the model achieved an accuracy of 92.95%, precision of 93.09%, and sensitivity of 89.00%. The model’s superior sensitivity and specificity underscore its potential as a critical asset in the surveillance and control of monkeypox outbreaks. These significant advancements not only enhance predictive performance but also mark a major step forward in the field of monkeypox disease image analysis, setting the stage for future applications in combating infectious diseases.},
  archive      = {J_ASOC},
  author       = {Mehdhar S.A.M. Al-Gaashani and Wenbo Xu and Efrem Yohannes Obsie},
  doi          = {10.1016/j.asoc.2024.112553},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112553},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MobileNetV2-based deep learning architecture with progressive transfer learning for accurate monkeypox detection},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-organizing lightweight correlation-aware fuzzy broad learning system for high-dimensional large-scale classification problems. <em>ASOC</em>, <em>169</em>, 112552. (<a href='https://doi.org/10.1016/j.asoc.2024.112552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods have shown outstanding results in various applications. Still, they suffer from time-consuming training and inference due to multiple cascade layers, along with the need for a complete retraining process when encountering new data. Broad Learning Systems (BLS) are novel lightweight emerging learning methods that have been successfully applied to different problems, considered as a paradigm shift. In this paper, a correlated Fuzzy version of BLS (CorFBLS) is proposed that considers the local correlations among input variables in defining each fuzzy rule in either neuro-fuzzy subsystem. CorFBLS transfers the input variables to a new space with orthogonal features to define each fuzzy rule and fuzzy set in this extracted feature space. Random subsets of the training instances are used to construct each neuro-fuzzy subsystem. To decrease the number of calculations along with the number of required fuzzy rules, a random subset of input variables is selected to build each neuro-fuzzy subsystem, making it suitable for high-dimensional problems. Finally, to have a self-organizing system able to encounter new training instances, a new growing algorithm is proposed to add new fuzzy subsystems. The performance of the proposed method based on different metrics including precision, architecture size, memory usage, and processing time is studied and compared with some relevant methods in different high-dimensional and large-scale problems along with some small-scale real-world problems. Based on the comparisons, the proposed method outperforms the previous studies with a more parsimonious structure. For high-dimensional datasets, it achieves total accuracy of 95.32% in MNIST, 91.00% in FMNIST, 70.00% in CIFAR-10, 91.06% in NORB, 85.4% in NoduleMedMNIST3D, 81.27% in AdrenalMedMNIST3D, and 91.6% in VesselMedMNIST3D.},
  archive      = {J_ASOC},
  author       = {Armin Salimi-Badr and Mohammad Mahdi Parchamijalal},
  doi          = {10.1016/j.asoc.2024.112552},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112552},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-organizing lightweight correlation-aware fuzzy broad learning system for high-dimensional large-scale classification problems},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soft cosine and extended cosine adaptation for pre-trained language model semantic vector analysis. <em>ASOC</em>, <em>169</em>, 112551. (<a href='https://doi.org/10.1016/j.asoc.2024.112551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic textual analysis is a natural language processing task that has enjoyed several research contributions towards solving diverse real-life problems. Vector comparison is a core subtask in semantic textual similarity analysis. A plethora of solutions including recent state-of-the-art transformer-based pre-trained language models for transfer learning have focused on using only cosine similarity for embedding evaluation in downstream tasks and ignored other vector comparison methods. To investigate the relative performance of some such ignored measures, this work proposes novel adaptations for soft cosine and extended cosine vector measures. We investigate their performance against the conventional cosine measure, distance-weighted cosine, vector similarity measure, negative Manhattan, and Euclidean distances on downstream semantic textual similarity tasks, under same conditions, for the first time in literature. Adopting transformer-based Universal sentence encoder, SBERT, SRoBERTa, SimCSE, and ST5 for text encoding; the performances of the adapted measures are evaluated on diverse real world datasets using Pearson, Spearman, accuracy and F1 evaluation metrics. Results obtained show that the adapted measures significantly surpass previously reported state-of-the-art cosine similarity-based correlations in several test cases considered.},
  archive      = {J_ASOC},
  author       = {Funebi Francis Ijebu and Yuanchao Liu and Chengjie Sun and Patience Usoro Usip},
  doi          = {10.1016/j.asoc.2024.112551},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112551},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soft cosine and extended cosine adaptation for pre-trained language model semantic vector analysis},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-phase neural networks for feature extraction and ensemble learning for recognizing human health activities. <em>ASOC</em>, <em>169</em>, 112550. (<a href='https://doi.org/10.1016/j.asoc.2024.112550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of smart devices into healthcare has led to the creation of vast amounts of sensor data, which are crucial for advancing various healthcare applications such as elderly care, lifestyle enhancement, and health monitoring. Human Activity Recognition (HAR), which relies on these data, is essential for the success of these applications. While Deep Learning (DL) methods, particularly Convolutional Neural Networks (CNN) and Machine Learning (ML), have been somewhat successful in HAR, they often face performance limitations. These limitations arise from the challenges of extracting complex features from sensor-based HAR data and dealing with noise. Current methods often rely on a single-phase feature extraction process. In contrast, adopting a multi-phase feature extraction approach, which rigorously performs feature extraction across multiple distinct phases, could more effectively address these challenges. To overcome these challenges, we introduce a novel hybrid framework named Dual-Phase Fused Neural Networks with Ensemble Learning (DP-FusedNN-EL), designed to achieve robust feature extraction and enhanced human activity recognition tasks. This model operates in two main stages: dual-phase feature extraction and classification. Initially, it employs two neural networks for feature extraction: a novel Dual-Head Fused CNN for local features and a CNN combined with a Stacked Bidirectional Gated Recurrent Unit and Attention network for local–global features. Subsequently, it utilizes a Dual-Phase Ensemble Learning model for classification, aiming to reduce overfitting by leveraging the strengths of local–global features. We evaluated our DP-FusedNN-EL model on several HAR datasets, achieving remarkable performance with accuracies ranging from 87.47% to 99.66%. These results significantly outperform existing models, demonstrating the effectiveness of the DP-FusedNN-EL model in HAR tasks.},
  archive      = {J_ASOC},
  author       = {Joy Dhar and Kapil Rana and Puneet Goyal and Azadeh Alavi and Rajib Rana and Bao Quoc Vo and Sudeepta Mishra and Sajib Mistry},
  doi          = {10.1016/j.asoc.2024.112550},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112550},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-phase neural networks for feature extraction and ensemble learning for recognizing human health activities},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the performance of countries in COVID-19 management: A data-driven decision-making and clustering. <em>ASOC</em>, <em>169</em>, 112549. (<a href='https://doi.org/10.1016/j.asoc.2024.112549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 outbreak, first reported in Wuhan, China, spread rapidly and endangered human lives and livelihoods globally. Researchers have utilized available tools and facilities to mitigate its impact across dimensions. In this study, we propose a comprehensive, data-driven framework to evaluate periodically 168 countries’ performance, considering four distinct variable categories since the advent of COVID-19. We assess and leverage four clustering methods of K-means, Gaussian Mixture Model (GMM), Density-Based Spatial Clustering of Applications with Noise (DBSCAN), and Spectral, as well as three Multi-Criteria Decision-Making (MCDM) approaches, including Combined Compromise Solution (COCOSO), Grey Relational Analysis (GRA), and Evaluation Based on Distance from Average Solution (EDAS) for ranking the countries. The results are analyzed thoroughly–among the examined factors, “Total Recovered”, “GDP Per capita”, and “Hospital Beds / 1 K” most critically impacted evaluating outcomes, while” Male Smokers”, “Diabetes Prevalence”, and “Cardiovascular Death Rate” are least influential. The novel metric “Medical Waste” also demonstrates more vital than 86 % of existing indicators. Moreover, the findings reveal associations between countries’ development levels and their corresponding cluster assignments. For more precise analysis, we investigate the intra-cluster and inter-cluster approaches, each of which revealed countries’ promotion or degradation regarding rankings within a cluster or transitions between clusters. Finally, appropriate policy-making and management strategies are presented to enhance countries’ preparedness for potential future outbreaks based on the results.},
  archive      = {J_ASOC},
  author       = {Hamed Meraji and Danial Rahimi and Ardavan Babaei and Erfan Babaee Tirkolaee},
  doi          = {10.1016/j.asoc.2024.112549},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112549},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating the performance of countries in COVID-19 management: A data-driven decision-making and clustering},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Align, enhance and read: Scene tibetan text recognition with cross-sequence reasoning. <em>ASOC</em>, <em>169</em>, 112548. (<a href='https://doi.org/10.1016/j.asoc.2024.112548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Market demand has primarily driven scene text recognition to focus on widely spoken languages, such as English and Chinese. Current research put scarce attention on Tibetan, despite its substantial demand and potential applications. To fill this gap, we analyze the characteristics of Tibetan text and point out the corresponding difficulties in text recognition. Moreover, this study explores the performance degradation of existing recognition methods on Tibetan text, and proposes a flexible alignment decoding manner with cross-modal sequence reasoning for Tibetan text. Concretely, our work leverages the information between different modal sequences to learn detailed visual features for Tibetan characters with complex glyphs through cross-modal sequence reasoning. Furthermore, we design an innovative alignment decoding scheme that aligns characters in a dense manner, improving attention quality for the character alignment. Experimental results indicate that the performance of existing text recognition on Tibetan text is far behind that of English or Chinese text. With our design, the recognition on Tibetan text achieves significant improvement.},
  archive      = {J_ASOC},
  author       = {Wenjun Ke and Yutian Liu and Xiaokang Yang and Jianguo Wei and Qingzhi Hou},
  doi          = {10.1016/j.asoc.2024.112548},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112548},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Align, enhance and read: Scene tibetan text recognition with cross-sequence reasoning},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A LLM-driven and motif-informed linearizing graph transformer for web API recommendation. <em>ASOC</em>, <em>169</em>, 112547. (<a href='https://doi.org/10.1016/j.asoc.2024.112547'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth in the number of Web APIs makes it difficult for developers to find the appropriate ones. To tackle this issue, researchers have created various powerful automatic approaches for recommendations. Recently, a range of graph neural networks, drawing inspiration from Transformers, have incorporated global attention to enhance recommendations. However, these approaches still have limitations in terms of processing information between nodes and utilizing other valid information, which reduces their effectiveness in large-scale Web API recommendations. To tackle these problems, this paper introduces a novel positional and semantic encoding method and motif-based linearizing graph Transformer for automatic Web API recommendation. We integrate the semantic information of the nodes into the positional information by using a fine-tuned large language model and graph attention mechanism. Furthermore, we leverage motif information to alter the computational sequence of the Vanilla Transformer, achieving linear time complexity. Experimental results on the two real-world datasets demonstrate the suitability of our model for Web API recommendation, surpassing existing state-of-the-art methods. In summary, our proposed technique exhibits promising results for Web API recommendation and underscores the potential of global attention in this field.},
  archive      = {J_ASOC},
  author       = {Xin Zheng and Guiling Wang and Guiyue Xu and Jianye Yang and Boyang Han and Jian Yu},
  doi          = {10.1016/j.asoc.2024.112547},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112547},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A LLM-driven and motif-informed linearizing graph transformer for web API recommendation},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel cryptographic hash function based on cellular automata and random diffusion model. <em>ASOC</em>, <em>169</em>, 112546. (<a href='https://doi.org/10.1016/j.asoc.2024.112546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of a cryptographic hash algorithm is a crucial task due to its numerous practical applications, such as digital signatures, blockchain, and distributed systems. Constructing a novel and efficient hash algorithm that meets the high security requirements is a challenging endeavor. This study introduces a cryptographic parallel hash algorithm based on cellular automata (CA) and a stochastic diffusion model, referred to as PCASD. The article delves into the rules of cellular automata, classifies 88 types of equivalent class rules, and utilizes random chaotic rules to generate keys for iterative processes. The random diffusion model optimizes parameters to achieve optimal safety performance indicators. The parallel iteration structure allows for simultaneous execution of different branches, ultimately resulting in a hash value. The experimental results demonstrate that the proposed parallel hash algorithm outperforms popular hash functions in terms of randomness, avalanche, information entropy, collision resistance, and efficiency, indicating its practical feasibility.},
  archive      = {J_ASOC},
  author       = {Yijun Yang and Huan Wan and Xiaohu Yan and Ming Zhao and Jianhua Zeng and Bin Li},
  doi          = {10.1016/j.asoc.2024.112546},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112546},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parallel cryptographic hash function based on cellular automata and random diffusion model},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive benefit evaluation of mineral resources development based on dual population constrained multi-objective evolutionary algorithm. <em>ASOC</em>, <em>169</em>, 112545. (<a href='https://doi.org/10.1016/j.asoc.2024.112545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing global demand for mineral resources, the comprehensive benefit evaluation of mineral resources development is playing an increasingly important role in the field of mineral resources development. However, there are few attempts to use multi-objective evolutionary algorithms (MOEAs) to study the comprehensive benefit evaluation problem of mineral resources development. To effectively solve this problem, this study established a multi-objective optimization model for comprehensive benefit evaluation of mineral resources development and proposed a dual-population constrained multi-objective evolutionary algorithm (CMOEA). The algorithm uses two populations to organically combine the efficient convergence of the non-dominated sorting genetic algorithm II (NSGA-II) with the advantage of the adaptive geometry estimation based multi-objective evolutionary algorithm (AGE-MOEA) in estimating the shape of the Pareto front, named co-evolution based on non-dominated sorting genetic algorithm and adaptive geometry estimation (C-NSGA-AGE). 3 test suites are used to verify the performance of the proposed algorithm. Experimental results show that compared with competing algorithms, C-NSGA-AGE has strong competitiveness in most test functions. The proposed method is used to solve the model, and five optimal comprehensive benefit evaluation schemes are given for different objectives. Among them, the maximum benefit is obtained when each objective is developed in a balanced way, which reaches 0.837181.},
  archive      = {J_ASOC},
  author       = {Xuezhi Yue and Yating Cheng and Lanlan Kang and Hu Peng and Yuan Zeng},
  doi          = {10.1016/j.asoc.2024.112545},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112545},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comprehensive benefit evaluation of mineral resources development based on dual population constrained multi-objective evolutionary algorithm},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Small water body extraction in remote sensing with enhanced CNN architecture. <em>ASOC</em>, <em>169</em>, 112544. (<a href='https://doi.org/10.1016/j.asoc.2024.112544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In climate change, the precise acquisition of data about natural resources is essential and poses a significant difficulty in remote sensing. Identifying water resources has become a key focus in addressing the global issue of water scarcity. Using remote sensing techniques to extract water bodies from satellite images is complex. While Convolutional Neural Networks (CNNs) have shown potential in image segmentation, challenges such as unclear boundaries, the requirement for extensive training data, and a high number of trainable parameters persist. To address these challenges, a CNN architecture called ”WaterNet” has been developed by integrating two residual refinement capsules to improve predicted results and ensure continuity in boundary pixels. Incorporating dense layers within the network reduces the total count of trainable parameters, thus alleviating the computational burden. To assess the effectiveness of the proposed method, a comparison was made with several conventional and deep learning approaches. These included the use of NDWI, Unet, Deepunet, Segnet, DeepwaterMap, and Multi-layered Feature Fusion Approach (MFFA). The proposed solution demonstrated superior performance, surpassing all existing approaches, with an outstanding accuracy of 0.97% in extracting water resources from high-resolution satellite imagery. The model is accurate in distinguishing water from other features such as land, ice, clouds, and shadows using sentinel data as input. The uniqueness of this work lies in the development of the ”WaterNet” system tailored for water body extraction, showcasing improved accuracy, efficient context transmission, incorporation of dense layers, utilization of residual refinement modules, and addressing the limitations of conventional approaches. These contributions collectively advance remote sensing and deep learning used in the detection of small bodies of water from imagery captured by satellites.},
  archive      = {J_ASOC},
  author       = {Bazila Farooq and Ankush Manocha},
  doi          = {10.1016/j.asoc.2024.112544},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112544},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Small water body extraction in remote sensing with enhanced CNN architecture},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Web search result diversification by combining global and local document features. <em>ASOC</em>, <em>169</em>, 112543. (<a href='https://doi.org/10.1016/j.asoc.2024.112543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current search engines play a crucial role in information retrieval, shaping how we access and interact with the vast sources of knowledge available online, yet effectively handling ambiguous or broad user queries remains a significant challenge, as these engines typically rank web documents based primarily on relevance. While search result diversification is acknowledged as a key method for enhancing the alignment of results with a user’s implicit intent, existing methods tend to focus on either a global or local perspective of document retrieval. These approaches commonly overlook the dynamic interactions between selected and candidate documents in global strategies and fail to capture the intricate internal correlations among all documents in local assessments. This oversight sets the stage for our proposed MultiViewDIV model, which aims to improve search result diversification through the integration of multi-view feature fusion. MultiViewDIV combines both local and global document diversity features with relevance features between documents and queries. For local diversity, it constructs a novel document relationship graph that incorporates temporal correlations to better understand the inter-document relationships. For global diversity, it introduces a method to assess the dissimilarity among documents and the extent to which they capture the implicit search intent of the query. The model fuses both global and local diversity features into a multilayer perceptron to compute document scores, which encapsulate diversity and relevance of each document, are then used to rank the documents in an optimal manner. Extensive experiments on two public datasets confirm the effectiveness and superiority of MultiViewDIV over existing approaches, achieving relative improvements of 1.07% in α -nDCG, 1.08% in ERR-IA, 0.89% in NRBP, and 1.62% in Pre-IA.},
  archive      = {J_ASOC},
  author       = {Rui Zhu and Shengxiang Zhang and Bo Liu and Qiuyu Tian and Xiaoqing Wu and Ruwen Zhang and Jiuxin Cao and Linfeng Qian},
  doi          = {10.1016/j.asoc.2024.112543},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112543},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Web search result diversification by combining global and local document features},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection for label distribution learning based on neighborhood fuzzy rough sets. <em>ASOC</em>, <em>169</em>, 112542. (<a href='https://doi.org/10.1016/j.asoc.2024.112542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, label distribution learning (LDL) has gained extensive application in actual classification endeavors. Nevertheless, most LDL datasets contain superfluous features that significantly impede the efficacy of classification algorithms and prolong their execution time. In addition, existing feature selection algorithms are usually limited to a specific label assignment paradigm or ignore the overall distribution information of the label significance in LDL. To address the aforementioned issues, this study constructs a novel uncertainty model known as neighborhood fuzzy rough sets, and proposes a LDL feature selection algorithm (LDNF) for better utilization of the overall distribution information of label significance. Specifically, the fuzzy equivalence relation for samples is defined using the correlation of label distributions, and the probabilities of classifying the same category are evaluated. To exploit the overall distribution information, the definitions of the upper and lower approximations are provided with the fuzzy equivalence relation of the label distribution. The experimental results indicate that the performance of the LDNF algorithm is superior to that of five leading-edge comparative algorithms on 10 publicly available real-world datasets. Neighborhood fuzzy rough sets and their applications will be further investigated in the future.},
  archive      = {J_ASOC},
  author       = {Zhixuan Deng and Tianrui Li and Pengfei Zhang and Keyu Liu and Zhong Yuan and Dayong Deng},
  doi          = {10.1016/j.asoc.2024.112542},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112542},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection for label distribution learning based on neighborhood fuzzy rough sets},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating single-cell multi-omics data through self-supervised clustering. <em>ASOC</em>, <em>169</em>, 112541. (<a href='https://doi.org/10.1016/j.asoc.2024.112541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in single-cell sequencing techniques enable individual cells to be sequenced across multiple modalities simultaneously, such as transcriptomics, epigenomics, and proteomics. Integrating multi-omics single-cell data provides a deeper and more comprehensive vision of genomic mechanisms. Due to the huge distribution shift between modalities, current integration methods mostly align the modality through domain adaption or similar strategies. They achieved limited performance likely because the modalities are over-divergent. Here, we propose a novel single-cell multimodal fusion method, scFPN, to improve the learned embedding by a clustering strategy. Specifically, scFPN first embeds each modality data through a feature pyramid network with a modality-specific variational auto-encoder. The learned hierarchical embeddings are then fused and input into a dual self-supervision optimizing module for attracting similar cells and separating dissimilar cells. We conducted comprehensive experiments on recently produced six datasets from different sequencing platforms and demonstrated the superiority of scFPN over a variety of state-of-the-art methods. More importantly, scFPN showed bio-interpretability by marker enrichment analysis from de-noising and imputing raw profiles.},
  archive      = {J_ASOC},
  author       = {Yuansong Zeng and Jianing Chen and Zixiang Pan and Weijiang Yu and Yuedong Yang},
  doi          = {10.1016/j.asoc.2024.112541},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112541},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating single-cell multi-omics data through self-supervised clustering},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of malicious URLs using temporal convolutional network and multi-head self-attention mechanism. <em>ASOC</em>, <em>169</em>, 112540. (<a href='https://doi.org/10.1016/j.asoc.2024.112540'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural Language Processing (NLP) and Deep Learning (DL) have achieved remarkable results in various fields and have also been proven to be effective in detecting phishing webpages. Inspired by the great success of NLP and DL models in phishing detection-related tasks, we examined the application of these techniques in classifying malicious and benign URLs (Uniform Resource Locator). We found that the existing NLP-based solutions mainly used Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) for phishing URL detection. However, CNN performs poorly when handling non-spatial data, while RNN cannot capture the long-distance dependency and has higher computational complexity. To overcome these issues, this paper proposes a phishing detection model based on Temporal Convolutional Network (TCN) to address the limitations of the conventional CNN and/or RNN algorithms. The proposed model used character-level and word-level embedding methods to obtain the feature representations of the input URLs. Then, TCN was employed for feature extraction and further enhanced with Multi-Head Self-Attention (MHSA) mechanism to classify legitimate and phishing websites. We conducted several experiments to validate the performance of the proposed model and measured various evaluation metrics. The obtained results showed that our solution performed better than other baseline models in classifying malicious URLs, achieving an accuracy of 98.78%. This implied the proposed approach provided an effective and efficient solution for detecting phishing URLs.},
  archive      = {J_ASOC},
  author       = {Nguyet Quang Do and Ali Selamat and Ondrej Krejcar and Hamido Fujita},
  doi          = {10.1016/j.asoc.2024.112540},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112540},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detection of malicious URLs using temporal convolutional network and multi-head self-attention mechanism},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision-making technique with neutrosophic Z-rough set approach for sustainable industry evaluation using sine trigonometric operators. <em>ASOC</em>, <em>169</em>, 112539. (<a href='https://doi.org/10.1016/j.asoc.2024.112539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the shortcomings of existing fuzzy decision-making models, the approximation of the neutrosophic Z-number becomes an invaluable tool in complicated circumstances including incomplete and contradictory data. To overcome the challenges of methods like multi-criteria decision-making (MCDM), trapezoidal neutrosophic numbers (TrNNs), and neutrosophic Z-numbers (NZNs), this approach integrates rough set theory, neutrosophic set theory, and Z-numbers to introduce Neutrosophic Z-Rough data. Issues like measurement reliability in TrNNs and continuity difficulties in NZNs can be addressed with the use of sophisticated aggregation techniques and Neutrosophic Z-Rough numbers (NZRNs). Using the periodicity and symmetry of the sine-trigonometric function (STF), this work investigates sine-trigonometric approaches to Neutrosophic Z-Rough numbers. New Rough Neutrosophic Z-number sine-trigonometric aggregation operators are obtained by merging the special features of the STF with the NZRNs. The method’s practical applicability within the NZRN framework is demonstrated by applying these operational notions to the MCDM approach, which supports sustainable decision-making in commercial contexts. In addition to improving the NZRN domain’s decision-making efficiency and rationality, this study guarantees a consistent and trustworthy assessment method. New approaches based on NZRNs are necessary since conventional MCDM frameworks are unable to handle the uncertainties brought forth by Industry 4.0. Results from a battery of tests—including sensitivity analyses, reliability evaluations, and comparison studies show that the suggested method is fresh, trustworthy, and excellent for gauging corporate sustainability.},
  archive      = {J_ASOC},
  author       = {Muhammad Kamran and Nadeem Salamat and Chiranjibe Jana and Qin Xin},
  doi          = {10.1016/j.asoc.2024.112539},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112539},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decision-making technique with neutrosophic Z-rough set approach for sustainable industry evaluation using sine trigonometric operators},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality-aware fuzzy min–max neural networks for dynamic brain network analysis and its application to schizophrenia identification. <em>ASOC</em>, <em>169</em>, 112538. (<a href='https://doi.org/10.1016/j.asoc.2024.112538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic functional connections (dFCs) refer to the observed phenomenon that functional connectivity changes over a short time, which has become a pioneering method in the diagnosis of brain diseases. However, current dynamic brain network analysis methods are insufficient to address the fuzzy information of brain networks caused by the quality issues of rs-fMRI data, and the uncertainty arising from inconsistent data quality across different windows due to missing values and noise in individual windows. These results in unreliable integration of multiple windows. To this end, this paper proposes a dynamic brain network analysis method based on quality-aware fuzzy min–max neural networks (QFMMNet). The individual window of dFCs is treated as a view, and we define three convolution filters to extract features from the brain network under the multi-view learning framework, thereby obtaining multi-view evidence for dFCs. We design the multi-view fuzzy min–max neural networks (MVFMM) based on fuzzy sets to deal with the fuzzy information of the brain network, which takes evidence as input patterns to generate hyperboxes and serves as the classification layer of each view. A quality-aware ensemble module is introduced to deal with uncertainty, which employs Dempster–Shafer Theory (D–S theory) to directly model the uncertainty and evaluate the dynamic quality-aware weighting of each view. At the decision level of model, a weighted fusion strategy is employed for multi-view fusion based on the dynamic quality-aware weighting, thereby the quality of each view is fully considered to improve the trustworthiness of the decision. We verified the effectiveness of QFMMNet through comparison with advanced algorithms on three real schizophrenia datasets, and the results show that QFMMNet significantly improved the classification performance of brain disease diagnosis tasks. The accuracy on the Huaxi, COBRE, and Xiangya datasets reached 87.14%, 86.67%, and 85.27%, respectively.},
  archive      = {J_ASOC},
  author       = {Tao Hou and Weiping Ding and Jiashuang Huang and Shu Jiang and Hongcheng Yao and Tianyi Zhou and Hengrong Ju},
  doi          = {10.1016/j.asoc.2024.112538},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112538},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quality-aware fuzzy min–max neural networks for dynamic brain network analysis and its application to schizophrenia identification},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian learning based elitist nondominated sorting algorithm for a kind of multi-objective integrated production scheduling and transportation problem. <em>ASOC</em>, <em>169</em>, 112537. (<a href='https://doi.org/10.1016/j.asoc.2024.112537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on a kind of multi-objective integrated production scheduling and transportation problem (MIPSTP), which is encountered in many real-life industrial processes. MIPSTP contains a production stage for processing products and a transportation stage for transporting products. In MIPSTP, we consider a dedicated integration of distributed scheduling and transportation regarding the two stages, arising from a practical project for a cooperative iron and steel company. The objective of MIPSTP is to minimize the total completion time of each factory and total transportation cost. To solve the problem, a Bayesian learning-based elitist nondominated sorting algorithm (BLENSA) is proposed. The primary highlights of this work are two-fold: 1) new solution structures of MIPSTP and 2) novel search model of BLENSA. For the solution structures, we propose for the first time the mathematical description of MIPSTP, accounting for several features in applications and so is different from conventional scheduling problems. For the search model of BLENSA, we propose a Bayesian learning-based probability model to learn valuable knowledge about nondominated solutions. Thereby, BLENSA combines the Bayesian learning-based probability model to produce a candidate population (CP) and the nondominated sorting method to generate a main population (MP). Next, we propose a greedy competition strategy to construct MP from CP and MP for the next generation. Results of experiments on 57 test instances and a real-life case study demonstrate the effectiveness and practical values of BLENSA.},
  archive      = {J_ASOC},
  author       = {Zuocheng Li and Ziqi Ding and Bin Qian and Rong Hu and Rongjuan Luo and Ling Wang},
  doi          = {10.1016/j.asoc.2024.112537},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112537},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bayesian learning based elitist nondominated sorting algorithm for a kind of multi-objective integrated production scheduling and transportation problem},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical supervised masked autoencoder: Crafting a better masking strategy and efficient fine-tuning schedule for medical image classification. <em>ASOC</em>, <em>169</em>, 112536. (<a href='https://doi.org/10.1016/j.asoc.2024.112536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, masked autoencoders (MAEs) have displayed great potential in many visual tasks. However, in medical image classification tasks, most human tissue structures are highly similar, and lesion-related regions often account for such a small portion, which causes MAEs’ high-ratio masking strategies likely to overlook these key regions. Meanwhile, inconsistencies between the pre-training phase and the fine-tuning phase lead to problems such as overfitting that hinder the performance and efficiency of MAEs in medical image classification. To address these issues, a Medical Supervised Masked Autoencoder (MSMAE) is proposed in this paper. Specifically, MSMAE designs a supervised attention-driven masking strategy (SAM). Based on the supervision guidance, MSMAE enables the attention map to localize lesion-related areas in medical images accurately. SAM performs accurate masking of medical images through attention maps thereby enabling effective learning of human tissue representations of key lesion areas. Additionally, MSMAE’s expansion of SAM to the fine-tuning stage effectively solves the inconsistency issue between the pre-training and fine-tuning stages of MAEs, which also indirectly increases MSMAE’s attention to secondary lesion areas. Extensive experiments have shown that MSMAE achieves SOTA performance on several official medical datasets for various diseases. Specifically, MSMAE achieves 68.41%, 98.39%, 81.97%, 89.59%, and 99.60% medical image classification accuracies on several datasets such as Messidor-2, BTMD, HAM10000, DeepLesion, and ChestXRay2017, respectively. Moreover, transfer learning for MSMAE demonstrates the promise of MSMAE in medical semantic segmentation tasks. Compared to MAE, MSMAE has 11.2% faster inference time in the fine-tuning phase and 74.08% lower FLOPs.},
  archive      = {J_ASOC},
  author       = {Jiawei Mao and Shujian Guo and Xuesong Yin and Yuanqi Chang and Binling Nie and Yigang Wang},
  doi          = {10.1016/j.asoc.2024.112536},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112536},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Medical supervised masked autoencoder: Crafting a better masking strategy and efficient fine-tuning schedule for medical image classification},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLP-GNAS: Meta-learning-based predictor-assisted genetic neural architecture search system. <em>ASOC</em>, <em>169</em>, 112527. (<a href='https://doi.org/10.1016/j.asoc.2024.112527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks achieve state-of-the-art results on many image classification tasks. However, according to the No Free Lunch Theorem, no single model performs optimally for all the datasets. Hence, the manual development of these models using the hit-and-trial approach requires high computational overhead. Transfer learning mitigates this issue to some extent by transferring knowledge learned from similar domain tasks to target tasks through model architecture and weights. Yet, the model remains designed for the source task, and using the same model on the target task may result in over-parameterization or sub-optimal results. Henceforth, the current work proposes a meta-learning-based predictor-assisted genetic Neural Architecture Search (MLP-GNAS) system to automate the model generation process for plant disease detection tasks to validate its efficacy in real-world scenarios. To this cause, the MLP-GNAS system employs a meta-learning component to recommend the top 3 suitable models for a target dataset, followed by a genetic algorithm-based NAS to fine-tune the recommended model. In addition, the proposed approach uses a CNN-based performance predictor designed to discard models unlikely to surpass the optimal performance recorded thus far. Further, extensive comparative experiments demonstrate that MLP-GNAS-generated models outperform 14 state-of-the-art models on two distinct plant disease datasets with an accuracy of 96.94% and 92.04%. Moreover, further comparison with the manually fine-tuned and NAS-generated models reveals that the proposed system outperforms both to obtain an accuracy of 98.35% and 99.95% on respective datasets.},
  archive      = {J_ASOC},
  author       = {Sahil Verma and Prabhat Kumar and Jyoti Prakash Singh},
  doi          = {10.1016/j.asoc.2024.112527},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112527},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MLP-GNAS: Meta-learning-based predictor-assisted genetic neural architecture search system},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stock selection with intuitionistic fuzzy combined compromise solutions. <em>ASOC</em>, <em>169</em>, 112526. (<a href='https://doi.org/10.1016/j.asoc.2024.112526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the dynamic landscape of stock market investments, decision-making processes are often challenged by uncertainty and imprecision inherent in the financial data. It is important to evaluate the suitability of each stock based on multiple criteria, including market trends, financial indicators, and risk factors. Various approaches can be utilized to determine best option based on different parameters. The ambiguity and uncertainty around a phenomena are handled by fuzzy sets. Intuitionistic fuzzy (IF) set is an extension of fuzzy set used for managing uncertainty in more complex situations when fuzzy sets are unable to produce reliable results. This paper outlines a technique in IF environment to solve the problem of selection of stocks based on financial and other factors. A new IF knowledge measure (IFKM) is proposed to measure the amount of knowledge linked to IF sets. Reliability and utility of introduced knowledge measure is tested using some numerical examples. A new IF accuracy measure (IFAM) is proposed based on the suggested knowledge measure. Proposed IFAM is used to find the patterns similarity of unknown pattern with given pattern. Besides this, a new score function is proposed to compare the IF numbers which can get around the drawbacks of the current scoring functions. A modified Combined compromise solution (CoCoSo) technique is provided to choose the best stock by using the suggested scoring function and the IF accuracy measure. Lastly, to prove the effectiveness of the suggested method, a comparative analysis carries out with the other existing methods.},
  archive      = {J_ASOC},
  author       = {Manish and Satish Kumar},
  doi          = {10.1016/j.asoc.2024.112526},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112526},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stock selection with intuitionistic fuzzy combined compromise solutions},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated model and automatically designed solver for power system restoration. <em>ASOC</em>, <em>169</em>, 112525. (<a href='https://doi.org/10.1016/j.asoc.2024.112525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power system restoration strategies schedule the power plants to re-energize the power network and loads after a blackout. These strategies are of great significance to ensure a resilient power system. Most power system restoration models consider either certain restoration stage(s) or all stages, with each or some of the stages managed independently. They destroy the essential characteristics of the interaction among the restoration stages and lead to sub-optimal solutions. To address this issue, in this paper, we first propose an improved integrated power system restoration (IPSR) model covering the entire restoration process without decoupling and sectionalization. We then develop both mathematical and metaheuristic solvers for the proposed model. In particular, we introduce the automated solver design paradigm to the highly non-linear constrained restoration problem-solving. By the paradigm, we automatically design the metaheuristic solver that gains enhanced performance beyond handcrafted solvers without algorithmic expertise. It is suggested that the automatically designed metaheuristic solver is a promising new option in variable and complicated power system operation scenarios without algorithmic expertise. Finally, a comprehensive case study demonstrates the proposed model’s significance and the proposed solver’s efficiency. Numerical tests show that the proposed IPSR model and solver obtained over 20% lower restoration time than current restoration methods, demonstrating IPSR’s and the proposed solver’s efficiency and effectiveness.},
  archive      = {J_ASOC},
  author       = {Xiaohui Zhao and Xia Li and Qi Zhao and Bai Yan and Yuhui Shi and Jiajin Kang},
  doi          = {10.1016/j.asoc.2024.112525},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112525},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrated model and automatically designed solver for power system restoration},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic hypergraph attention network: Capturing market-wide spatiotemporal dependencies for stock selection. <em>ASOC</em>, <em>169</em>, 112524. (<a href='https://doi.org/10.1016/j.asoc.2024.112524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As more companies choose to go public to raise additional funds, investors are confronted with the challenge of choosing high-quality stocks from a multitude of options to generate returns on investment. In recent years, various deep learning models have been utilized to explore spatiotemporal dependencies through the analysis of historical price indicators of individual stocks and related stocks for stock selection. However, despite advancements in these models, they neglect the dynamic influence of global market-wide information on spatiotemporal dependencies of stocks. Focusing solely on stock-specific dependencies limits their ability to learn comprehensive spatiotemporal representations. To address this limitation, we propose a novel market-wide dynamic hypergraph attention network (MDHAN) for stock selection. The key of MDHAN is introducing an attention-based market-impacted GRU (att-MGRU) and an attention-based market-impacted dynamic hypergraph neural network (att-MHGNN). Specifically, The att-MGRU is a variant of GRU, which employs a fusion gate to capture daily market-wide temporal influences on each stock and a temporal attention mechanism to discern the contributions of historical influences across different trading days. The att-MHGNN first constructs a dynamic market-impacted hypergraph to model the market’s influence on stock relationships, which is then fed into an attention-based hypergraph neural network to extract the spatial dependencies influenced by the market. By conducting experiments on four real world datasets from China and the US, covering a span of four years, we demonstrate that MDHAN significantly outperforms state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Zhipeng Liu and Peibo Duan and Xiaosha Xue and Changsheng Zhang and Wenwei Yue and Bin Zhang},
  doi          = {10.1016/j.asoc.2024.112524},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112524},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic hypergraph attention network: Capturing market-wide spatiotemporal dependencies for stock selection},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source transfer learning for zero-shot structural damage detection. <em>ASOC</em>, <em>169</em>, 112519. (<a href='https://doi.org/10.1016/j.asoc.2024.112519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing generalizable Structural Health Monitoring (SHM) tools for downstream tasks, such as Structural Damage Detection (SDD), is one way to tackle large-scale SHM applications. Such tools become particularly important when dealing with a heterogeneous population of structures lacking prior (damaged and non-damaged) data, a common scenario in large-scale SHM applications. Devising Transfer Learning (TL) mechanisms using representative SHM datasets is a favorable method to address this challenge. Multiple SHM datasets are publicly available but only partially cover the objective representative SHM dataset. The missing parts limit the ability to extract or embed high-level features, resulting in non-generalizable solutions, such as one-to-one (source-to-target) TL models. In the absence of a representative SHM dataset, this study tackles the challenge of accumulating SDD knowledge from sparse, non-similar datasets and integrating them to enable damage detection in unseen target structures with no prior damaged data. The innovation lies in two areas: (1) defining a feature and Domain Adaptation (DA) method that can transfer features between dissimilar structures, and (2) designing an SDD model to learn from these features and a mechanism to combine the learned SDD knowledge from various sources in a zero-shot setting, a necessity in online SHM applications without prior damaged data. To that end, this study proposes a DA-based TL approach and employs ensemble techniques to combine knowledge from parametric models trained on each source dataset, thereby establishing a common ground for fusing data from different structures. The proposed zero-shot Multi-Source TL-SDD technique achieves mean F1 scores of 0.971, 0.959, and 0.922 in three well-known SHM benchmark datasets, without requiring fine-tuning of models pre-trained on source data for the target features. This performance is achieved by integrating signal processing domain knowledge into the DA procedure, outperforming other competitive deep zero-shot SDD techniques trained with the target non-damaged data. Computational complexity analysis is also examined, demonstrating the method satisfying online SHM requirements, a critical feature for real-world applications.},
  archive      = {J_ASOC},
  author       = {Mohammad Hesam Soleimani-Babakamali and Roksana Soleimani-Babakamali and Arash Kashfi-Yeganeh and Kourosh Nasrollahzadeh and Onur Avci and Serkan Kiranyaz and Ertugrul Taciroglu},
  doi          = {10.1016/j.asoc.2024.112519},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112519},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-source transfer learning for zero-shot structural damage detection},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive constrained multi-objective differential evolution algorithm for vehicle routing problem considering crowdsourcing delivery. <em>ASOC</em>, <em>169</em>, 112517. (<a href='https://doi.org/10.1016/j.asoc.2024.112517'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase of logistics orders, the crowdsourcing delivery with part-time drivers is an effective way to solve urban logistics distribution. The crowdsourcing distribution is the vehicle routing problem containing service time windows of customers and delivery time windows of part-time drivers. However, it is challenging to obtain optimal scheduling schemes for the vehicle routing problem considering crowdsourcing (VRP-C). To address this constrained optimization problem, an adaptive constrained multi-objective differential evolution (ACMODE) algorithm is designed in this paper to minimize the travel distance and the driver payment. First, a two-stage initialization method is designed to generate solutions with fewer constraint violations by dual time windows operation. Second, a neighborhood-oriented search strategy is developed to guide searching more feasible regions and avoiding the premature convergence of solutions. Third, a fast selection mechanism based on an improved non-dominated sorting approach is proposed to achieve the tradeoff on objectives and constraints, accelerating the process of optimization. Finally, several numerical simulation experiments explain that the proposed ACMODE algorithm can obtain feasible solutions of the constrained multi-objective optimization problem effectively, and has better performance than some state-of-art algorithms in solving VRP-C.},
  archive      = {J_ASOC},
  author       = {Ying Hou and Yanjie Shen and Honggui Han and Yilin Wu and Yanting Huang},
  doi          = {10.1016/j.asoc.2024.112517},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112517},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive constrained multi-objective differential evolution algorithm for vehicle routing problem considering crowdsourcing delivery},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A repulsive-distance-based maximum diversity selection algorithm for multimodal multiobjective optimization. <em>ASOC</em>, <em>169</em>, 112516. (<a href='https://doi.org/10.1016/j.asoc.2024.112516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal multiobjective optimization problems (MMOPs) are a challenging class of problems. Several advanced evolutionary algorithms have been developed to solve MMOPs, but these algorithms still have some limitations, such as having many parameters, slow convergence speed, and unsatisfactory performance. To solve these problems, we propose a repulsive-distance-based maximum diversity selection algorithm (RMDS) which aims, during environmental selection, to select individuals with the best comprehensive diversity through the repulsive distance. The repulsive distance is the comprehensive Euclidean distance between an individual and selected individuals with the consideration of distribution of individuals in both the decision and objective spaces. RMDS has the following advantages: first, the repulsive distance allows rational selection of well-distributed solutions in the non-parameterized case. Second, the repulsive distance acts both in the decision and objective spaces, so it provides a good balance between the distribution of the solution set in these two spaces. Third, RDMS has a straightforward principle. Experimental results show that RMDS has superior performance incomparison with other well-known multimodal multiobjective evolutionary algorithms on 31 test functions.},
  archive      = {J_ASOC},
  author       = {Qi Deng and Yuan Liu and Shengxiang Yang and Juan Zou and Xijun Li and Yizhang Xia and Jinhua Zheng},
  doi          = {10.1016/j.asoc.2024.112516},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112516},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A repulsive-distance-based maximum diversity selection algorithm for multimodal multiobjective optimization},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing robust acoustic DOA estimation against position errors via fast sparse bayesian learning. <em>ASOC</em>, <em>169</em>, 112499. (<a href='https://doi.org/10.1016/j.asoc.2024.112499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced methods for acoustic direction-of-arrival (DOA) estimation leverage sparse signal recovery techniques, which are valued for providing high-resolution results using a limited number of measurement vectors. Although these technologies are capable, they frequently overlook potential errors in sensor positioning, and their computational complexity is excessive, constraining their practical application. This paper introduces a new self-calibrating Bayesian method designed to address this issue. The approach begins by establishing a hierarchical Bayesian model that exploits the sparsity of both signal and position error vectors. Subsequently, a fast algorithm applies a basis selection strategy to efficiently update all variables. Furthermore, the study also explores off-grid estimation. The empirical results indicate that our method outperforms state-of-the-art methods in both accuracy and computational efficiency.},
  archive      = {J_ASOC},
  author       = {Zonglong Bai and Chenggang Liu and Junyang Zhang and Jianxiang Wang},
  doi          = {10.1016/j.asoc.2024.112499},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112499},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing robust acoustic DOA estimation against position errors via fast sparse bayesian learning},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An extended outranking technique based on spherical fuzzy rough numbers for circular economy business models in small and medium-sized enterprises. <em>ASOC</em>, <em>169</em>, 112496. (<a href='https://doi.org/10.1016/j.asoc.2024.112496'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The circular economic business model is designed to maximize resource use and minimize waste through practices like sharing, leasing, reusing, repairing, refurbishing, and recycling. It promotes keeping materials and products in circulation for as long as possible. Small and medium-sized enterprises play a crucial role in developing economies, significantly contributing to global economic growth through their involvement in imports and commodity pricing. In developing nations, transitioning small and medium-sized enterprises to circular business models is essential for job creation, poverty reduction, innovation, sustainable industrialization, and reducing income disparities, though selecting the optimal model remains challenging. This research study aims to present an outranked multi-criteria group decision making technique, ELimination and Choice Expressing REality, under the consideration of spherical fuzzy rough numbers, to select the best circular business model for such enterprises by which they would make a remarkable change in the economic condition of a developing country. The proposed method integrates spherical fuzzy sets and rough set theory to enhance uncertainty management and preference modeling, enabling more robust evaluations through improved outranking relations. It identifies the best business model by comparing score degrees via concordance and discordance indices, while minimizing subjectivity with spherical fuzzy rough numbers and accounting for satisfaction, dissatisfaction, and neutrality. A case study on selecting the best business model for SMEs validates the proposed technique by averaging forward and reverse rankings to identify the top option along with limitations and future implications.},
  archive      = {J_ASOC},
  author       = {Muhammad Akram and Maheen Sultan and Cengiz Kahraman},
  doi          = {10.1016/j.asoc.2024.112496},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112496},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An extended outranking technique based on spherical fuzzy rough numbers for circular economy business models in small and medium-sized enterprises},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A stage-related online incremental transfer learning-based remaining useful life prediction method of bearings. <em>ASOC</em>, <em>169</em>, 112491. (<a href='https://doi.org/10.1016/j.asoc.2024.112491'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remaining useful life (RUL) prediction of bearings holds significant research value in bearing health management, particularly in the rapidly advancing field of intelligent manufacturing. How to realize the accurate online RUL prediction still faces many challenges, including the diverse degradation trends in bearings, the scarcity of labeled data, and the inconsistencies in the data marginal distributions. This paper proposes a Stage-related Online Incremental Adversarial Domain Adaption (SR-OIADA) transfer learning algorithm for bearing RUL prediction. It employs a dynamic domain adaptation strategy, which integrates incremental learning and transfer learning, facilitating the online acquisition of degradation knowledge aligned with degradation stages. A novel online degradation stage division algorithm is proposed to adaptively detect the health status of online monitored bearings, thereby enhancing the effectiveness of online learning. By incrementally training the deep models in a prediction-to-transfer manner, the model undergoes incremental updates at checkpoints, taking full advantage of new online samples. Experiments are conducted on real rolling bearing datasets to validate the proposed methods, demonstrating excellent predictive performance and highlighting the superiority compare with the non-transfer and offline methods.},
  archive      = {J_ASOC},
  author       = {Weisen Guo and Fang Li and Ping Zhang and Long Luo},
  doi          = {10.1016/j.asoc.2024.112491},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112491},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A stage-related online incremental transfer learning-based remaining useful life prediction method of bearings},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Normalizing flow-based latent space mapping for implicit pattern authentication on mobile devices. <em>ASOC</em>, <em>169</em>, 112469. (<a href='https://doi.org/10.1016/j.asoc.2024.112469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In numerous mobile device applications, safeguarding user privacy through authentication is of paramount importance. Traditional methods employ information-matching-based systems, wherein users authenticate themselves using preset passwords. However, such systems pose a challenge in maintaining privacy if the password is compromised. This study introduces a behavior-based authentication approach that capitalizes on implicit patterns generated by individual users’ device usage habits during password entry on mobile devices, complementing information-based authentication. To optimize this feature, we develop a user-specific latent space mapping framework utilizing a normalizing flow and train a one-class classification machine learning model based on the latent space to enhance authentication performance. Upon applying the proposed methodology to data from 1,000 mobile banking service users, we observed a significant improvement in authentication performance. The integrated error decreased from 21.42% to 9.48%, and the equal error rate reduced from 27.84% to 16.3% compared to the method solely employing a machine learning model in the data space without implementing latent space mapping.},
  archive      = {J_ASOC},
  author       = {Jaehyuk Heo and Jeongseob Kim and Euisuk Chung and Subin Kim and Pilsung Kang and Donghwa Shin and Jinho Shin and Daehee Han},
  doi          = {10.1016/j.asoc.2024.112469},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112469},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Normalizing flow-based latent space mapping for implicit pattern authentication on mobile devices},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection through adaptive sparse learning for scene recognition. <em>ASOC</em>, <em>169</em>, 112439. (<a href='https://doi.org/10.1016/j.asoc.2024.112439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene recognition is an important and challenging task in the field of computer vision. Current research typically focuses on local features in scene images by utilizing pretrained convolutional neural networks (CNN) to obtain a feature dictionary. However, these local features often contain similar features to other scene categories, leading to feature confusion and subsequently low accuracy in scene recognition. Therefore, focusing on local features for scene recognition is controversial. In contrast to existing works, we consider extracting common features within the same category from scene images and using them as discriminative features between different categories. We propose a scene recognition method based on adaptive sparse learning feature selection. This method leverages sparse learning to distinguish the contribution of each feature in the deep features to the classification task, aiming to construct a salient feature dictionary that describes the importance of features in scene images. Subsequently, an adaptive weight optimization method is employed through alternating updates to automatically adjust feature weights, enabling the selection of common features within the same scene category from the compact dictionary. Experimental results on multiple benchmark scene recognition datasets demonstrate that our proposed method is superior to state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Yunyun Sun and Peng Li and Hang Sun and He Xu and Ruchuan Wang},
  doi          = {10.1016/j.asoc.2024.112439},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112439},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection through adaptive sparse learning for scene recognition},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entropy-weighted medoid shift: An automated clustering algorithm for high-dimensional data. <em>ASOC</em>, <em>169</em>, 112347. (<a href='https://doi.org/10.1016/j.asoc.2024.112347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unveiling the intrinsic structure within high-dimensional data presents a significant challenge, particularly when clusters manifest themselves in lower-dimensional subspaces rather than in the full feature space. This complexity is prevalent in real-world datasets, such as text documents and images, which often contain numerous noisy or sparse features. Traditional clustering methods often overlook these latent subspace structures. This paper introduces a novel subspace-based clustering algorithm designed explicitly to address this challenge. Building upon the robust medoid shift framework, we integrate a dimensionality reduction scheme that dynamically projects data onto evolving subspaces determined through entropy-constrained optimization. This approach effectively filters irrelevant information and identifies underlying clusters, optimizing subspace representation while avoiding trivial solutions. Unlike existing methods, our algorithm ensures convergence without necessitating stopping criteria, thereby enabling efficient processing of large datasets. We validate the efficacy of our approach through extensive experiments on synthetic and real-world datasets, demonstrating substantial performance enhancements over state-of-the-art techniques. By explicitly uncovering the underlying subspace structures, our method opens new avenues for effective high-dimensional data clustering and offers valuable insights into complex data environments.},
  archive      = {J_ASOC},
  author       = {Abhishek Kumar and Oladayo S. Ajani and Swagatam Das and Rammohan Mallipeddi},
  doi          = {10.1016/j.asoc.2024.112347},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112347},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Entropy-weighted medoid shift: An automated clustering algorithm for high-dimensional data},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating view location information for multi-view multi-label learning. <em>ASOC</em>, <em>168</em>, 112565. (<a href='https://doi.org/10.1016/j.asoc.2024.112565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-view multi-label learning (MVML), subspace learning provides an effective solution for integrating multi-view data. However, the current learning concentrates on the shared subspace construction process and neglects the exploration of location information, i.e., the view source of the feature, which is inherent to the feature itself. This has somewhat limited the progress of research on MVML. On this basis, we propose a multi-view multi-label learning method based on incorporating view location information (MMVLI) for the first time with reference to the Vision Transformer architecture. Firstly, the common features and special features between views are obtained through the feature extraction. The above two are combined according to the feature dimension and added into the learnable location information encoding matrix named the collaborative matrix. Then, the collaborative matrix is put into Transformer Encoder for self-attention learning to get the final feature matrix for multi-label learning. Extensive comparative experiments with existing state-of-the-art methods on six publicly available multi-view multi-label datasets demonstrate the effectiveness of MMVLI.},
  archive      = {J_ASOC},
  author       = {Jiabao Wang and Yusheng Cheng},
  doi          = {10.1016/j.asoc.2024.112565},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112565},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incorporating view location information for multi-view multi-label learning},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised prototype self-calibration based on hybrid attention contrastive learning for enhanced few-shot action recognition. <em>ASOC</em>, <em>168</em>, 112558. (<a href='https://doi.org/10.1016/j.asoc.2024.112558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The collection and annotation of large-scale video data pose significant challenges, prompting the exploration of few-shot models to recognize unseen actions with limited training samples. However, most existing models rely on complex temporal alignment strategies, neglecting the necessity of constructing accurate class prototypes in action recognition. To address this limitation, we propose an unsupervised prototype self-calibration based on hybrid attention contrastive learning (UPSHC), aiming to refine prototypes through various strategies. Our network comprises three innovative components: a hybrid attention network (HAN), a dual adaptive contrastive learning mechanism (DACL), and an unsupervised prototype self-calibration mechanism (UPSC). The HAN integrates self-attention and cross-attention mechanisms to highlight important spatiotemporal regions, reducing intra-class variations and enhancing inter-class variations. The DACL mechanism constructs prototype-centered and query-centered contrastive losses to deeply explore the similarities between prototypes and samples, improving the model’s sensitivity to subtle inter-class differences. Additionally, DACL employs adaptive margins to dynamically adjust the distances between positive and negative samples, mitigating the unreliability of fixed margins. The UPSC module utilizes unlabeled query samples in an unsupervised manner to optimize target prototypes without requiring additional training. Experimental results on three benchmark datasets demonstrate that UPSHC achieves state-of-the-art performance, highlighting the superiority of our prototype enhancement approach in few-shot action recognition.},
  archive      = {J_ASOC},
  author       = {Yiyuan An and Yingmin Yi and Li Wu and Yuan Cao and Dingsong Zhou and Yiwei Yuan and Bojun Liu and Xianghong Xue and Yankai Li and Chunyi Su},
  doi          = {10.1016/j.asoc.2024.112558},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112558},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised prototype self-calibration based on hybrid attention contrastive learning for enhanced few-shot action recognition},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin-assisted fault diagnosis framework for rolling bearings under imbalanced data. <em>ASOC</em>, <em>168</em>, 112528. (<a href='https://doi.org/10.1016/j.asoc.2024.112528'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of deep learning-based fault diagnosis methods is constrained by the imbalanced data. Recently, many studies have suggested integrating dynamic model responses into the training process to address data imbalances. However, significant distribution discrepancies exist between dynamic model responses and real measured data, resulting in suboptimal performance. To address this challenge, this research proposes a digital twin-assisted framework for rolling bearings fault diagnosis under imbalanced data, which minimizes the distribution discrepancies between dynamic model responses and real measured data through information and feature transfer. Firstly, a Digital Twin-assisted Data Fusion Strategy (DTDFS) is proposed to facilitate information transfer from physical entities to dynamic models, generating digital twin data for data augmentation. Subsequently, a Frequency Filter Subdomain Adaptation Network (FFSAN) is proposed to achieve feature transfer between twin data and measured data. Finally, experimental results and engineering applications demonstrate that the proposed framework significantly outperforms existing imbalanced fault diagnosis methods, which is crucial to the application of deep learning-based fault diagnosis in industrial settings.},
  archive      = {J_ASOC},
  author       = {Zhen Ming and Baoping Tang and Lei Deng and Qichao Yang and Qikang Li},
  doi          = {10.1016/j.asoc.2024.112528},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112528},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Digital twin-assisted fault diagnosis framework for rolling bearings under imbalanced data},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Viable intertwined supply network: Modelling and dynamic analysis using artificial neural networks. <em>ASOC</em>, <em>168</em>, 112503. (<a href='https://doi.org/10.1016/j.asoc.2024.112503'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The viability of intertwined supply networks (ISNs) has recently been studied as a critical topic in operations management. Modelling the viability of ISNs is considered a promising tool to meet the demands of extraordinary events, such as the Russo-Ukrainian War and the COVID-19 pandemic. To enhance the viability of ISNs, the structures of ISN must be modelled, and the behavioral dynamics of interactions between firms in a changing environment should be analyzed. In this study, a trophic chain-based dynamic formulation of ISN viability is presented, and a solution methodology for dynamic analysis of the ISN viability model is designed. The concept of artificial neural networks (ANNs) in ISN analysis is introduced to predict and analyze future behavioral strategies in buyer-supplier relations. The dynamic model of ISN is represented by a system of nonlinear differential equations and described in terms of three dynamic values: suppliers X τ , focal firms Y τ , and market demand Z τ . The stochastic numerical simulations are performed for the dynamics of ISN model by employing ANNs with a scaled conjugate gradient neural network (SCGNN) in a more advanced and efficient manner. Two numerical cases are investigated to evaluate the performance of the proposed approach. The validation, correctness, and reliability of the proposed stochastic SCGNN technique are analyzed by selecting 78 % of the data for training, 12 % for validation, and 10 % for testing. The correctness of the scheme is authenticated through the overlapping of the proposed and state-of-the-art results. Moreover, the statistical analysis is presented graphically in terms of mean square error, state transitions, function fitness, error histograms. The regression coefficient values are calculated as 1 for each scenario presents the perfect model. Finally, a comparison of numerical results, which shows the overlapping is examined and the absolute error is performed between 10 −05 to 10 −07 . The small mean square error performances enhance the correctness of the scheme. These results indicate that the dynamical model can effectively analyze the ISN structures and help researchers and practitioners ensure the survival of supply chains during extraordinary events.},
  archive      = {J_ASOC},
  author       = {Shahid Ahmad Bhat and Tariq Aljuneidi},
  doi          = {10.1016/j.asoc.2024.112503},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112503},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Viable intertwined supply network: Modelling and dynamic analysis using artificial neural networks},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly detection and segmentation in industrial images using multi-scale reverse distillation. <em>ASOC</em>, <em>168</em>, 112502. (<a href='https://doi.org/10.1016/j.asoc.2024.112502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection and segmentation in industrial images are critical tasks requiring robust and precise methodologies. This paper presents the Multi-Scale Reverse Distillation (MSRD) methodology, an innovative improvement of the foundational reverse distillation approach. MSRD leverages autoencoder-based techniques integrated with information at different levels to significantly enhance reconstruction capabilities. A novel module incorporated at the decoder’s end facilitates precise sample reconstruction. The proposed loss function incorporates the reconstruction loss L R e c o n , calculated using structural similarity index measure (SSIM) between the original and reconstructed images, in addition to the knowledge distillation loss L K D . Additionally, the integration of a feature pyramid network improves the spatial coherence of anomaly maps across varying scales, enabling detailed anomaly segmentation. The MSRD method undergoes rigorous evaluation on three public datasets, demonstrating superior performance in both anomaly detection and segmentation. The results highlight MSRD’s adaptability and effectiveness in one-class learning-based applications. This study underscores MSRD’s potential as a powerful tool for industrial anomaly detection, offering significant advancements in AI-driven image analysis.},
  archive      = {J_ASOC},
  author       = {Chien-Liang Liu and Chia-Chen Chung},
  doi          = {10.1016/j.asoc.2024.112502},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112502},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Anomaly detection and segmentation in industrial images using multi-scale reverse distillation},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trainable monte carlo-MLP for cost uncertainty in resilient supply chain optimization with additive manufacturing implementation challenges. <em>ASOC</em>, <em>168</em>, 112501. (<a href='https://doi.org/10.1016/j.asoc.2024.112501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Additive Manufacturing (AM) has the potential to transform Supply Chain (SC) dynamics, but its implementation introduces risks that require careful management. This paper presents an innovative optimization framework for evaluating AM integration within SCs through two policies aimed at creating resilience. By exploring the intersection of AM and Traditional Manufacturing (TM), the study focuses on restructuring SCs for full or partial product production using AM techniques. To enhance SC resilience against material shortages, smart contracts with buffer suppliers are employed. The main objective is to reduce operational and conventional costs while optimizing SC performance. To address cost uncertainty, this research introduces a novel Monte Carlo (MC) and Machine Learning (ML) hybrid approach, termed MCML. This method leverages MCML-Particle Swarm Optimization (MCML-PSO) and MCML-Genetic Algorithm (MCML-GA) for optimization. A real-world case study validates the model, showing that it reduces costs and improves the accuracy of cost uncertainty estimation compared to standalone TM and AM approaches. Various methods, including PSO, GA, MC-PSO, and MC-GA, were evaluated, with MCML-PSO demonstrating the best performance in minimizing total costs. This study highlights the benefits of integrating AM into SCs, emphasizing the importance of precise cost uncertainty estimation. The proposed model offers valuable insights for decision-makers, helping them design resilient and efficient SCs while mitigating the risks associated with AM technology.},
  archive      = {J_ASOC},
  author       = {Pardis Roozkhosh and Mojtaba Ghorbani},
  doi          = {10.1016/j.asoc.2024.112501},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112501},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Trainable monte carlo-MLP for cost uncertainty in resilient supply chain optimization with additive manufacturing implementation challenges},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the performance of CNN models for pneumonia and skin cancer detection using novel fractional activation function. <em>ASOC</em>, <em>168</em>, 112500. (<a href='https://doi.org/10.1016/j.asoc.2024.112500'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel Riemann–Liouville (RL) conformable fractional derivative based Adaptable-Shifted-Fractional-Rectified-Linear-Unit, briefly called R L ASFReLU , and evaluates its efficacy in enhancing the performance of convolutional neural network (CNN) models for pneumonia and skin cancer detection. The study conducts a comprehensive comparative analysis against traditional activation functions and state-of-the-art CNN architectures. The results show that R L ASFReLU consistently outperforms other functions, achieving higher accuracy. Comparative evaluations with various neural network architectures reveal that the model equipped with R L ASFReLU exhibits superior performance despite its simplicity and fewer trainable parameters, highlighting its efficiency and effectiveness. The findings suggest that R L ASFReLU holds promise in improving diagnostic accuracy and efficiency in medical imaging applications, contributing to advancements in healthcare technology and facilitating better patient care. The proposed fractional nonlinear transformation can offer high performance with reduced computational cost, making it practical for deployment in healthcare settings.},
  archive      = {J_ASOC},
  author       = {Meshach Kumar and Utkal Mehta},
  doi          = {10.1016/j.asoc.2024.112500},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112500},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing the performance of CNN models for pneumonia and skin cancer detection using novel fractional activation function},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tourism forecast combination using weighting schemes with flow information among component models. <em>ASOC</em>, <em>168</em>, 112498. (<a href='https://doi.org/10.1016/j.asoc.2024.112498'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecast combination is an effective way to improve the accuracy of tourism demand forecasting. The continuous development of forecast combination methods with high accuracy is inevitable to help tourism practitioners formulate more appropriate management strategies. This study investigated how tourism forecasting accuracy can be improved by treating combination forecasting as a multiple attribute decision making (MADM) problem. The proposed hybrid methods first yield single-model forecasts from grey models without considering the sample size and limiting the available data to satisfy any statistical properties. Given the effectiveness of PROMETHEE in MADM, which applies flows to gauge the intensity of the preference for one alternative over another, the flows among component models in a combination are then used to assess relative weights next. Finally, the flow-based weighting schemes are incorporated into the linear and nonlinear combinations of individual forecasts. After assessing the accuracy of the proposed methods with the inbound tourism demand in Taiwan, the results indicated that the proposed methods involving the integration of the flow-based weighting scheme into the Choquet fuzzy integral performed better than other benchmark forecast combination methods with different model combinations.},
  archive      = {J_ASOC},
  author       = {Yi-Chung Hu},
  doi          = {10.1016/j.asoc.2024.112498},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112498},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tourism forecast combination using weighting schemes with flow information among component models},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adolescent mental health state assessment framework by combining YOLO with random forest. <em>ASOC</em>, <em>168</em>, 112497. (<a href='https://doi.org/10.1016/j.asoc.2024.112497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problems of adolescent mental health are becoming increasingly prominent. The house-tree-person test (HTPT) method can map the psychological state of subjects and has been widely used in clinical testing. However, the HTPT method requires an amount of time for professionals to assess. Based on the HTPT method, how to use artificial intelligence technology to quickly, objectively, and automatically complete mental state assessments has become a new trend. In this paper, a Hybrid of Enhanced YOLO and Random Forest algorithm for adolescent mental health assessment is proposed. Because of the dependence between HTPT feature positions, Bayesian theory is used to enhance YOLO to improve detection accuracy. Among the many features detected by the enhanced YOLO algorithm, RF is used to automatically assess mental state. The method is validated by simulation experiments and actual measurements of university students. Moreover, the simulation results show that the recognition accuracy can reach 92% and the recognition speed can reach the second level. The measured results show that this method can quickly and accurately assess mental state.},
  archive      = {J_ASOC},
  author       = {Min Wan and Sai Zou},
  doi          = {10.1016/j.asoc.2024.112497},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112497},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adolescent mental health state assessment framework by combining YOLO with random forest},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic consensus for uncertain multiple attribute group decision-making problem in belief distribution environment. <em>ASOC</em>, <em>168</em>, 112495. (<a href='https://doi.org/10.1016/j.asoc.2024.112495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of uncertain multiple attribute group decision-making (MAGDM) problems, existing research often focuses on the development of consensus-enhancing algorithms grounded in optimization models. However, this paper takes a stochastic perspective, thoroughly considering the impact of uncertainty on decision-making. And a novel stochastic method to model group consensus is introduced with the listed three components: (1) the concept of stochastic rank analysis based on stochastic belief distribution (BD) is given to measure the uncertainty degree in the original BD matrix, which is then used to assign weights to decision makers (DMs). (2) in uncertain environments, to ensure the effectiveness of consensus from a probabilistic perspective, the stochastic consensus index is proposed by taking both the advantages of the Jensen-Shannon (JS) distance and the hesitant distance between stochastic BDs. Then, the expected acceptable group consensus index is further provided to measure the consensus of original preferences among the group, and (3) finally, to deal with the issue of no consensus information, an optimization model is constructed aimed at achieving an acceptable consensus that can generate recommendation advice for DMs, facilitating the attainment of a consensus. The effectiveness of the proposed method is exemplified through two case studies: purchase of new energy vehicles (NEVs) and a postgraduate interview scenario. Furthermore, sensitivity analysis and comparative analysis are presented to better prove its advantages.},
  archive      = {J_ASOC},
  author       = {Xianchao Dai and Hao Li and Ligang Zhou and Qun Wu},
  doi          = {10.1016/j.asoc.2024.112495},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112495},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stochastic consensus for uncertain multiple attribute group decision-making problem in belief distribution environment},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GRASP with evolutionary path relinking for the conditional p-dispersion problem. <em>ASOC</em>, <em>168</em>, 112494. (<a href='https://doi.org/10.1016/j.asoc.2024.112494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new heuristic method that hybridizes GRASP with Path Relinking to solve the conditional p -Dispersion problem. Given n elements, from which q < n have been already selected, this problem seeks to select p < n additional unselected elements to maximize the minimum dissimilarity among them. The conditional p -dispersion problem models a facility location problem motivated by a real situation faced in many practical settings arising when some facilities have been already located. The algorithm includes a novel proposal based on an efficient interplay between search intensification and diversification provided by the Path Relinking component, and it also incorporates an intelligent way to measure the diversity among solutions. An extensive computational experimentation is carried out to compare the performance of our heuristic with the state of the art method. The comparison shows that our proposal is competitive with the existing method, since it is able to identify 17 best-known values. Additionally, our experimentation includes a real practical case solved for a Spanish company in its expansion process. This case illustrates both the applicability of the conditional p -dispersion model, and the suitability of our algorithm to efficiently solve practical instances.},
  archive      = {J_ASOC},
  author       = {Jesús Sánchez-Oro and Anna Martínez-Gavara and Ana D. López-Sánchez and Rafael Martí and Abraham Duarte},
  doi          = {10.1016/j.asoc.2024.112494},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112494},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GRASP with evolutionary path relinking for the conditional p-dispersion problem},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evolutionary deep learning approach using flexible variable-length dynamic stochastic search for anomaly detection of robot joints. <em>ASOC</em>, <em>168</em>, 112493. (<a href='https://doi.org/10.1016/j.asoc.2024.112493'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is crucial for condition monitoring of robot joints. An increasing number of anomaly detection methods based on deep learning have been investigated. However, since the deep learning architectures for anomaly detection are manually designed by trial and error, the design process is time-consuming, and the designed deep learning architectures may not be optimal for specific anomaly detection tasks. In this paper, a Flexible Variable-Length Dynamic Stochastic Search (FVLDSS) is proposed by designing and embedding the encoding and alignment strategies into the original Dynamic Stochastic Search (DSS). Subsequently, an Evolutionary Deep Learning Approach Using FVLDSS (EDLDSS) is proposed to automatically search for an optimal deep learning architecture for anomaly detection. In EDLDSS, Convolutional Neural Network (CNN) is used as the feature extractor, k -nearest neighbors trained only with normal samples is used as the anomaly detector, and FVLDSS is used to simultaneously optimize the network structure and hyperparameters of CNN. Furthermore, an anomaly detection method based on EDLDSS is developed to detect the anomalies in robot joints, in which S-transform spectrograms of vibration signals normalized by Z-score normalization are used as the input of CNN. To validate the performance of EDLDSS, a condition monitoring system using fiber Bragg grating sensors is first established for the industrial robot to acquire vibration signals from robot joints and conduct three experiments. The statistical results demonstrate the feasibility and satisfactory performance of EDLDSS in handling the anomaly detection problem of robot joints.},
  archive      = {J_ASOC},
  author       = {Qi Liu and Yongchao Yu and Boon Siew Han and Wei Zhou},
  doi          = {10.1016/j.asoc.2024.112493},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112493},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary deep learning approach using flexible variable-length dynamic stochastic search for anomaly detection of robot joints},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging more information for blind stereo super-resolution via large-window cross-attention. <em>ASOC</em>, <em>168</em>, 112492. (<a href='https://doi.org/10.1016/j.asoc.2024.112492'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo image super-resolution aims to reconstruct high-resolution images by effectively utilizing cross-view complementary information from stereo image pairs. A prevalent method in Stereo image super-resolution is the stereo cross-attention module, which allows the model to focus on and integrate relevant features from both the left and right views. Despite its advantages, our analysis using a diagnostic tool called local attribution map (LAM) reveals that current methods exhibit limitations in effectively leveraging this complementary information. To address this issue, we propose the Double Stereo Cross-Attention Module (DSCAM), which utilizes an Overlapping Stereo Cross-Attention (OSCA) mechanism that enhances the integration of cross-view complementary information by using overlapping windows, followed by an additional multiplication step to refine and emphasize the combined features. Additionally, we develop a stereo image degradation model that ensures the consistency of degradation between stereo pairs, accurately simulating the real-world degradation process of stereo images. Extensive experiments have demonstrated that our method achieves visually pleasing results, making it the first to address the problem of stereo image super-resolution in real-world scenarios. The source code is available at https://github.com/nathan66666/LCASSR.git .},
  archive      = {J_ASOC},
  author       = {Weifeng Cao and Xiaoyan Lei and Yong Jiang and Zongfei Bai and Xiaoliang Qian},
  doi          = {10.1016/j.asoc.2024.112492},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112492},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Leveraging more information for blind stereo super-resolution via large-window cross-attention},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correcting translation for non-autoregressive transformer. <em>ASOC</em>, <em>168</em>, 112488. (<a href='https://doi.org/10.1016/j.asoc.2024.112488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Autoregressive Transformer has shown great success in recent years. It generally employs the encoder–decoder framework, where the encoder maps the sentence into hidden representation, and the decoder generates the target tokens simultaneously. Since the theory of non-autoregressive transformer is consistent with the architecture of the encoder, we suppose that it is somewhat wasteful for the encoder to only map input sentence into hidden representation. In this study, we proposed a novel non-autoregressive transformer to fully exploit the capabilities of the encoder. Specifically, in our approach, the encoder not only encodes the input sentence into hidden representation, but also generates the target tokens. Consequently, the decoder is relieved of its responsibility to generate the target tokens, instead of focusing on correcting the sentence produced by the encoder. We evaluate the performance of the proposed non-autoregressive transformer on three widely-used translation tasks. The experimental results illustrate the proposed method can significantly improve the performance of the non-autoregressive transformer , which achieved 27.94 BLEU on WMT14 EN → DE task, 33.96 BLEU on WMT16 EN → RO task, and 33.85 BLEU on IWSLT14 DE → EN.},
  archive      = {J_ASOC},
  author       = {Shuheng Wang and Heyan Huang and Shumin Shi and Dongbai Li and Dongen Guo},
  doi          = {10.1016/j.asoc.2024.112488},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112488},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Correcting translation for non-autoregressive transformer},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical scale awareness for object detection in unmanned aerial vehicle scenes. <em>ASOC</em>, <em>168</em>, 112487. (<a href='https://doi.org/10.1016/j.asoc.2024.112487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing object detection models are typically designed without considering the small-scale context, leading to significant challenges in detecting small objects within Unmanned Aerial Vehicle (UAV) scenes. Therefore, this paper aims to incorporate a novel hierarchical scale-aware module into the neck component of the classical YOLO architecture. This module hierarchically enhances the object features, progressing from small to large scales. Specifically, the proposed Small-Scale Awareness (SSA) module is designed to enhance features from small-scale objects, while the introduced Receptive Field Expansion (RFE) module is responsible for modeling contextual information in a way that expands the receptive field while maintaining feature diversity for large-scale objects. Additionally, in the backbone of our model, a Stack of Non-Linear Mapping (SNM) module is proposed, which utilizes deformable convolutions to fuse feature maps of diverse scales through a cascade of non-linear mapping units, to capture a wide range of contextual and discriminative information. The experimental results on the VisDrone dataset demonstrate that the proposed model outperforms the state-of-the-art models both on the mean Average Precision (mAP) and Average Precision 50 (AP50) metrics. The ablation studies have proved that the proposed modules are beneficial to improve the detection performance of objects in UAV scenes.},
  archive      = {J_ASOC},
  author       = {Shijie Wang and Chaoying Wan and Jinqiang Yan and Silong Li and Tianmeng Sun and Jieru Chi and Guowei Yang and Chenglizhao Chen and Teng Yu},
  doi          = {10.1016/j.asoc.2024.112487},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112487},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical scale awareness for object detection in unmanned aerial vehicle scenes},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent model of pulmonary emphysema detection using adaptive image segmentation and multi-dilated densenet with attention mechanism. <em>ASOC</em>, <em>168</em>, 112483. (<a href='https://doi.org/10.1016/j.asoc.2024.112483'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pulmonary emphysema is a significant factor in lung cancer and Chronic Obstructive Pulmonary Disease (COPD). Traditional pulmonary emphysema detection models may have difficulty in accurately detecting and diagnosing the severity of the disease. So, this work developed a novel pulmonary emphysema detection system with the help of deep learning frameworks. Originally, the significant images are accumulated from the benchmark sources, and fed into the Adaptive Trans-DenseUnet (ATDUnet)-based segmentation model. The ATDUnet model is highly effective in accurately segmenting pulmonary emphysema from the gathered images. Moreover, to enhance the segmentation process, the parameters are tuned in the ATDUnet using the Statistical Solution of the Osprey Optimization Algorithm (SSOOA). Subsequently, the segmented image is given to the pulmonary emphysema classification phase, where the Multi-Dilated DenseNet with Attention Mechanism (MDDNet-AM) is employed. By incorporating an attention mechanism, MDDNet-AM can focus on important features within the image for improved accuracy and efficiency in diagnosis. Finally, the developed model offered the pulmonary emphysema classified outcome. Then, the outcome of the developed model is compared against conventional pulmonary emphysema detection methods, and given the accuracy to be 93.10. Therefore, the result proved that the use of developed advanced technology in pulmonary emphysema detection has shown promising results in the field of respiratory health.},
  archive      = {J_ASOC},
  author       = {Indira Linginani and Muddana A. Lakshmi},
  doi          = {10.1016/j.asoc.2024.112483},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112483},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An intelligent model of pulmonary emphysema detection using adaptive image segmentation and multi-dilated densenet with attention mechanism},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated graph neural network for privacy-preserved supply chain data sharing. <em>ASOC</em>, <em>168</em>, 112475. (<a href='https://doi.org/10.1016/j.asoc.2024.112475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning plays an increasingly important role in supply chain management. Due to privacy and security concerns, enterprises are reluctant to share their raw data, which leads to missing links in supply chains. To address privacy issue and promote data sharing in supply chain, we propose a new federated graph neural network named Isomorphic Federated Graph Neural Network (IFGNN) for supply chain data sharing. IFGNN consists of a server and multiple clients. The server is a lightweight parameter server with an efficient parameter updating algorithm. A client is assigned to each node in the supply chain network. Every supplier client is linked to its first-order neighbors, which means they have supply–demand relationship. The topology of the input network is identical to that of the supplier clients. Experimental results on a newly collected vehicle supply chain dataset show that the performance of IFGNN is close to its centralized counterpart. This work demonstrates that it is feasible to protect supply chain data privacy without a significant loss of prediction accuracy. Federated learning provides a new solution for promoting data sharing and collaborative machine learning in supply chain.},
  archive      = {J_ASOC},
  author       = {Xiaochuan Tang and Yu Wang and Xin Liu and Xiaojun Yuan and Chao Fan and Yanmei Hu and Qiang Miao},
  doi          = {10.1016/j.asoc.2024.112475},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112475},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated graph neural network for privacy-preserved supply chain data sharing},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy grouping-based memetic algorithm for multi-depot multi-UAV power pole inspection. <em>ASOC</em>, <em>168</em>, 112472. (<a href='https://doi.org/10.1016/j.asoc.2024.112472'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power pole inspection is important to maintain the normal operation of electrical system. It usually requires to fly a fleet of unmanned aerial vehicles (UAVs) from multiple depots at the same time to jointly complete the inspection tasks distributed in a wide area, which is a challenging planning problem. In order to address this problem, this work first builds the model of the multi-depot multi-UAV power pole inspection problem with charging stations. After that, a fuzzy grouping-based memetic algorithm named FGATS is proposed to solve the problem. Specifically, a fuzzy grouping strategy is proposed to divide a large-scale problem into multiple small-scale subproblems in order to reduce the complexity of the problem. It continuously adjusts the grouping scheme to enhance the flexibility and effectiveness of the algorithm. Then, a hybrid algorithm combining genetic algorithm and tabu search is designed to jointly optimize the subproblems, ensuring an effective balance between global and local searches. After a certain number of iterations, the problem is re-divided and the populations are re-initialized by the proposed solution update strategy that learns and incorporates historical task-sequence knowledge. This strategy enhances the current optimization process by retaining useful information from previous iterations. Experiments on both artificial terrains and a real terrain verifies the effectiveness of FGATS, with the algorithm’s performance ranking first overall.},
  archive      = {J_ASOC},
  author       = {Xiang-Ling Chen and Ya-Hui Jia and Xiao-Cheng Liao and Wei-Neng Chen},
  doi          = {10.1016/j.asoc.2024.112472},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112472},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy grouping-based memetic algorithm for multi-depot multi-UAV power pole inspection},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An ensemble learning algorithm for optimization of spark ignition engine performance fuelled with methane/hydrogen blends. <em>ASOC</em>, <em>168</em>, 112468. (<a href='https://doi.org/10.1016/j.asoc.2024.112468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing global demand for sustainable and cleaner transportation has led to extensive research on alternative fuels for Internal Combustion (IC) engines. One promising option is the utilization of methane/hydrogen blends in Spark-Ignition (SI) engines due to their potential to reduce Green House Gas (GHG) emissions and improve engine performance. However, the optimal operation of such an engine is challenging due to the interdependence of multiple conflicting objectives, including Brake Mean Effective Pressure (BMEP), Brake Specific Fuel Consumption (BSFC), and nitrogen oxide (NO x ) emissions. This paper proposes an evolutionary optimization algorithm that employs a surrogate model as a fitness function to optimize methane/hydrogen SI engine performance and emissions. To create the surrogate model, we propose a novel ensemble learning algorithm that consists of several base learners. This paper employs ten different learning algorithms diversified via the Wagging method to create a pool of base-learner algorithms. This paper proposes a combinatorial evolutionary pruning algorithm to select an optimal subset of learning algorithms from a pool of base learners for the final ensemble algorithm. Once the base learners are designed, they are incorporated into an ensemble, where their outputs are aggregated using a weighted voting scheme. The weights of these base learners are optimized through a gradient descent algorithm. However, when optimizing a problem using surrogate models, the fitness function is subject to approximation uncertainty. To address this issue, this paper introduces an uncertainty reduction algorithm that performs averaging within a sphere around each solution. Experiments are performed to compare the proposed ensemble learning algorithm to the classical learning algorithms and state-of-the-art ensemble algorithms. Also, the proposed smoothing algorithm is compared with the state-of-the-art evolutionary algorithms. Experimental studies suggest that the proposed algorithms outperform the existing algorithms.},
  archive      = {J_ASOC},
  author       = {Mohammad-H. Tayarani-N. and Amin Paykani},
  doi          = {10.1016/j.asoc.2024.112468},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112468},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble learning algorithm for optimization of spark ignition engine performance fuelled with methane/hydrogen blends},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly-based network intrusion detection using denoising autoencoder and wasserstein GAN synthetic attacks. <em>ASOC</em>, <em>168</em>, 112455. (<a href='https://doi.org/10.1016/j.asoc.2024.112455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection systems face challenges in handling high-dimensional, large-scale, and imbalanced network traffic data. This paper proposes a novel architecture combining a denoising autoencoder (AE) and a Wasserstein generative adversarial network (WGAN) to address these issues. The AE-WGAN model extracts high-representative features and generates realistic synthetic attacks, effectively resolving data imbalance and enhancing anomaly-based intrusion detection. Extensive experiments on NSL-KDD and CICIDS-2017 datasets, using both binary and multiclass classification scenarios with various classifier architectures, demonstrate the model’s superior performance. The proposed approach outperforms state-of-the-art models in accuracy, precision, recall, and F1 score, showing excellent generalization capabilities against unseen attacks. Time complexity analysis reveals computational efficiency while maintaining high-quality synthetic attack generation. This research contributes a robust, efficient, and adaptable framework for intrusion detection, capable of handling modern network traffic complexities and evolving cyber threats.},
  archive      = {J_ASOC},
  author       = {Mohammad Arafah and Iain Phillips and Asma Adnane and Wael Hadi and Mohammad Alauthman and Abedal-Kareem Al-Banna},
  doi          = {10.1016/j.asoc.2024.112455},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112455},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Anomaly-based network intrusion detection using denoising autoencoder and wasserstein GAN synthetic attacks},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential evolution with spherical search algorithm for nonlinear engineering and infectious disease optimization problems. <em>ASOC</em>, <em>168</em>, 112446. (<a href='https://doi.org/10.1016/j.asoc.2024.112446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithms for constrained optimization are popular because of their simplicity and ability to find global solutions. However, they can be computationally demanding and may struggle with identifying feasible points. In this study, we propose a new algorithm, Differential Evolution with Spherical Search (DESS), which combines spherical search to explore feasible regions and differential evolution to exploit promising points. Spherical Search (SS) is a recent algorithm that explores a spherical region around each population member and uses gradient repair to guide the solutions towards the feasible region. We incorporate SS into a multi-operator differential evolution with archiving, which uses hierarchical sorting to handle the constraints. Moreover, DESS implements an interior point method as a local search to obtain more accurate solutions. We also introduce an infeasible operator that allows DESS to examine infeasible regions near the global solution. This is particularly helpful for problems where the global solution lies along the boundary of the feasible region. DESS was tested on 57 benchmark objective functions from recent real-world constrained problems, with dimensions ranging from 2 to 158 and up to 148 constraints. It outperformed 18 state-of-the-art methods, achieving a 98% feasibility rate and finding superior solutions. DESS excelled in complex problems, including power systems, power electronics, and livestock feed optimization. It was also applied to a constrained optimization problem involving an ordinary differential equation, demonstrating its practical relevance and versatility.},
  archive      = {J_ASOC},
  author       = {Jongmin Lee and Renier Mendoza and Victoria May P. Mendoza and Eunok Jung},
  doi          = {10.1016/j.asoc.2024.112446},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112446},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution with spherical search algorithm for nonlinear engineering and infectious disease optimization problems},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic clinical healthcare model for assessing post-pandemic recovery in heart diagnosis. <em>ASOC</em>, <em>168</em>, 112407. (<a href='https://doi.org/10.1016/j.asoc.2024.112407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globally, the occurrence of cardiovascular disease is linked to atherosclerosis in heart vessels due to poor diet or underlying illnesses. Additionally, post-pandemic diseases like COrona VIrus Disease-19 (COVID-19) have accentuated cardiovascular complications, leading to significant fatalities among sufferers. The research addresses the crucial need for monitoring and preventative measures in individuals who have recovered from COVID-19, emphasizing the heightened cardiovascular risks associated with the human population. To safeguard individuals from heart diseases based on the decision-making framework of physicians, the study aimed to develop a clinical decision support system with a novel fuzzy approach. In decision analysis, membership grades alone are not sufficient to analyze objects in the universe. The addition of reference parameters provides freedom to decision makers in selecting these grades. Consequently, the complex spherical fuzzy N-soft set with an associated reference parameter offers a robust approach for modeling such uncertainties. For defuzzification, the Measurement of Alternatives and Ranking according to the Compromise Solution (MARCOS) approach was used. To specify the superiority of weighing approaches based on group decision-making techniques, result validation is established using four distinct methods: equal weight, ROC weight, RS weight, and entropy weight.},
  archive      = {J_ASOC},
  author       = {Subramaniam Pragathi and Samayan Narayanamoorthy and Selvaraj Dhivya and Ranganathan Saraswathy and Vladimir Simic and Dragan Pamucar and Daekook Kang},
  doi          = {10.1016/j.asoc.2024.112407},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {112407},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A systematic clinical healthcare model for assessing post-pandemic recovery in heart diagnosis},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
