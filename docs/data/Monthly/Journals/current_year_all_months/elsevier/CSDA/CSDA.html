<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CSDA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="csda">CSDA - 21</h2>
<ul>
<li><details>
<summary>
(2026). Fast and efficient causal inference in large-scale data via subsampling and projection calibration. <em>CSDA</em>, <em>214</em>, 108281. (<a href='https://doi.org/10.1016/j.csda.2025.108281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the average treatment effect in large-scale datasets faces significant computational and storage challenges. Subsampling has emerged as a critical strategy to mitigate these issues. This paper proposes a novel subsampling method that builds on the G-estimation method offering the double robustness property. The proposed method uses a small subset of data to estimate computationally complex nuisance parameters, while leveraging the full dataset for the computationally simple final estimation. To ensure that the resulting estimator remains first-order insensitive to variations in nuisance parameters, a projection approach is introduced to optimize the estimation of the outcome regression function and treatment regression function such that the Neyman orthogonality conditions are satisfied. It is shown that the resulting estimator is asymptotically normal and achieves the same convergence rate as the full data-based estimator when either the treatment or the outcome models is correctly specified. Additionally, when both models are correctly specified, the proposed estimator achieves the same asymptotic variance as the full data-based estimator. The finite sample performance of the proposed method is demonstrated through simulation studies and an application to birth data, comprising over 30 million observations collected over the past eight years. Numerical results indicate that the proposed estimator is nearly as computationally efficient as the uniform subsampling estimator, while achieving similar estimation efficiency to the full data-based G-estimator.},
  archive      = {J_CSDA},
  author       = {Miaomiao Su},
  doi          = {10.1016/j.csda.2025.108281},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108281},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast and efficient causal inference in large-scale data via subsampling and projection calibration},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An algorithm for estimating threshold boundary regression models. <em>CSDA</em>, <em>214</em>, 108274. (<a href='https://doi.org/10.1016/j.csda.2025.108274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an innovative iterative two-stage algorithm designed for estimating threshold boundary regression (TBR) models. By transforming the non-differentiable least-squares (LS) problem inherent in fitting TBR models into an optimization framework, our algorithm combines the optimization of a weighted classification error function for the threshold model with obtaining LS estimators for regression models. To improve the efficiency and flexibility of TBR model estimation, we integrate the weighted support vector machine (WSVM) as a surrogate method for solving the weighted classification problem. The TBR-WSVM algorithm offers several key advantages over recently developed methods: it eliminates pre-specification requirements for threshold parameters, accommodates flexible estimation of nonlinear threshold boundaries, and streamlines the estimation process. We conducted several simulation studies to illustrate the finite-sample performance of TBR-WSVM. Finally, we demonstrate the practical applicability of the TBR model through a real data analysis.},
  archive      = {J_CSDA},
  author       = {Chih-Hao Chang and Takeshi Emura and Shih-Feng Huang},
  doi          = {10.1016/j.csda.2025.108274},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108274},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An algorithm for estimating threshold boundary regression models},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Rate accelerated inference for integrals of multivariate random functions. <em>CSDA</em>, <em>214</em>, 108273. (<a href='https://doi.org/10.1016/j.csda.2025.108273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computation of integrals is a fundamental task in the analysis of functional data, where the data are typically considered as random elements in a space of squared integrable functions. Effective unbiased estimation and inference procedures are proposed for integrals of uni- and multivariate random functions. Applications to key problems in functional data analysis involving random design points are examined and illustrated. In the absence of noise, the proposed estimates converge faster than the sample mean and standard numerical integration algorithms. The estimator also supports effective inference by generally providing better coverage with shorter confidence and prediction intervals in both noisy and noiseless settings.},
  archive      = {J_CSDA},
  author       = {Valentin Patilea and Sunny G․ W․ Wang},
  doi          = {10.1016/j.csda.2025.108273},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108273},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Rate accelerated inference for integrals of multivariate random functions},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust selection of the number of change-points via FDR control. <em>CSDA</em>, <em>214</em>, 108272. (<a href='https://doi.org/10.1016/j.csda.2025.108272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust quantification of uncertainty regarding the number of change-points presents a significant challenge in data analysis, particularly when employing false discovery rate (FDR) control techniques. Emphasizing the detection of genuine signals while controlling false positives is crucial, especially for identifying shifts in location parameters within flexible distributions. Traditional parametric methods often exhibit sensitivity to outliers and heavy-tailed data. Addressing this limitation, a robust method accommodating diverse data structures is proposed. The approach constructs component-wise sign-based statistics. Leveraging the global symmetry inherent in these statistics enables the derivation of data-driven thresholds suitable for multiple testing scenarios. Method development occurs within the framework of U-statistics, which naturally encompasses existing cumulative sum-based procedures. Theoretical guarantees establish FDR control for the component-wise sign-based method under mild assumptions. Demonstrations of effectiveness utilize simulations with synthetic data and analyses of real data.},
  archive      = {J_CSDA},
  author       = {Hui Chen and Chengde Qian and Qin Zhou},
  doi          = {10.1016/j.csda.2025.108272},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108272},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust selection of the number of change-points via FDR control},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Kernel density estimation with a markov chain monte carlo sample. <em>CSDA</em>, <em>214</em>, 108271. (<a href='https://doi.org/10.1016/j.csda.2025.108271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference relies on the posterior distribution, which is often estimated with a Markov chain Monte Carlo sampler. The sampler produces a dependent stream of variates from the limiting distribution of the Markov chain, the posterior distribution. When one wishes to display the estimated posterior density, a natural choice is the histogram. However, abundant literature has shown that the kernel density estimator is more accurate than the histogram in terms of mean integrated squared error for an i.i.d. sample. With this as motivation, a kernel density estimation method is proposed that is appropriate for the dependence in the Markov chain Monte Carlo output. To account for the dependence, the cross-validation criterion is modified to select the bandwidth in standard kernel density estimation approaches. A data-driven adjustment to the biased cross-validation method is suggested with introducing the integrated autocorrelation time of the kernel. The convergence of the modified bandwidth to the optimal bandwidth is shown by adapting theorems from the time series literature. Simulation studies show that the proposed method finds the bandwidth close to the optimal value, while standard methods lead to smaller bandwidths under Markov chain samples and hence to undersmoothed density estimates. A study with real data shows that the proposed method has a considerably smaller integrated mean squared error than standard methods. The R package KDEmcmc to implement the suggested algorithm is available on the Comprehensive R Archive Network.},
  archive      = {J_CSDA},
  author       = {Hang J. Kim and Steven N. MacEachern and Young Min Kim and Yoonsuh Jung},
  doi          = {10.1016/j.csda.2025.108271},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108271},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Kernel density estimation with a markov chain monte carlo sample},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Measure selection for functional linear model. <em>CSDA</em>, <em>214</em>, 108270. (<a href='https://doi.org/10.1016/j.csda.2025.108270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in modern science have led to an increased prevalence of functional data, which are usually viewed as elements of the space of square-integrable functions L 2 . Core methods in functional data analysis, such as functional principal component analysis, are typically grounded in the Hilbert structure of L 2 and rely on inner products based on integrals with respect to the Lebesgue measure over a fixed domain. A more flexible framework is proposed, where the measure can be arbitrary, allowing natural extensions to unbounded domains and prompting the question of optimal measure choice. Specifically, a novel functional linear model is introduced that incorporates a data-adaptive choice of the measure that defines the space, alongside an enhanced function principal component analysis. Selecting a good measure can improve the model’s predictive performance, especially when the underlying processes are not well-represented when adopting the default Lebesgue measure. Simulations, as well as applications to COVID-19 data and the National Health and Nutrition Examination Survey data, show that the proposed approach consistently outperforms the conventional functional linear model.},
  archive      = {J_CSDA},
  author       = {Su I Iao and Hans-Georg Müller},
  doi          = {10.1016/j.csda.2025.108270},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108270},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Measure selection for functional linear model},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Overview of normal-reference tests for high-dimensional means with implementation in the r package ‘HDNRA’. <em>CSDA</em>, <em>214</em>, 108269. (<a href='https://doi.org/10.1016/j.csda.2025.108269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of testing for equal mean vectors in high-dimensional data poses significant difficulties in statistical inference. Much of the existing literature introduces methods that often rely on stringent regularity conditions for the underlying covariance matrices, enabling asymptotic normality of test statistics. However, this can lead to complications in controlling test size. To address these issues, a new set of tests has emerged, leveraging the normal-reference approach to improve reliability. The latest normal-reference methods for testing equality of mean vectors in high-dimensional samples, potentially with differing covariance structures, are reviewed. The theoretical underpinnings of these tests are revisited, providing a new unified justification for the validity of centralized L 2 -norm-based normal-reference tests (NRTs) by deriving the convergence rate of the distance between the null distribution of the test statistic and its corresponding normal-reference distribution. To facilitate practical application, an R package, HDNRA , is introduced, implementing these NRTs and extending beyond the two-sample problem to accommodate general linear hypothesis testing (GLHT). The package, designed with user-friendliness in mind, achieves efficient computation through a core implemented in C++ using Rcpp , OpenMP , and RcppArmadillo . Examples with real datasets are included, showcasing the application of various tests and providing insights into their practical utility.},
  archive      = {J_CSDA},
  author       = {Pengfei Wang and Tianming Zhu and Jin-Ting Zhang},
  doi          = {10.1016/j.csda.2025.108269},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108269},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Overview of normal-reference tests for high-dimensional means with implementation in the r package ‘HDNRA’},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). High-dimensional subgroup functional quantile regression with panel and dependent data. <em>CSDA</em>, <em>214</em>, 108268. (<a href='https://doi.org/10.1016/j.csda.2025.108268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional additive functional partial linear single-index quantile regression with high-dimensional parameters under subgroup panel data is investigated. Based on spline-based approach, we construct oracle estimators of the unknown parameter and functions, and discuss their consistency with rates and asymptotic normality under α -mixing assumptions. A penalized estimation method by using the SCAD technique is introduced to estimate the additive functions and parameter, enabling variable selection and automatic identification of the number of groups. Hypothesis testing for the parameter is also considered, and the asymptotic distributions of the restricted estimators and the test statistic are derived under both the null and local alternative hypotheses. Simulation studies and real data analysis are conducted to verify the validity of the proposed methods and applications.},
  archive      = {J_CSDA},
  author       = {Xiao-Ge Yu and Han-Ying Liang},
  doi          = {10.1016/j.csda.2025.108268},
  journal      = {Computational Statistics & Data Analysis},
  month        = {2},
  pages        = {108268},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {High-dimensional subgroup functional quantile regression with panel and dependent data},
  volume       = {214},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Discovering causal structures in corrupted data: Frugality in anchored gaussian DAG models. <em>CSDA</em>, <em>213</em>, 108267. (<a href='https://doi.org/10.1016/j.csda.2025.108267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on the recovery of anchored Gaussian directed acyclic graphical (DAG) models to address the challenge of discovering causal or directed relationships among variables in datasets that are either intentionally masked or contaminated due to measurement errors. A main contribution is to relax the existing restrictive identifiability conditions for anchored Gaussian DAG models by introducing the anchored-frugality assumption. This assumption posits that the true graph is the most frugal among those satisfying the possible distributions of the latent and observed variables, thereby making the true Markov equivalent class (MEC) identifiable. The validity of the anchored-frugality assumption is justified using both graph and probability theories, respectively. Another main contribution is the development of the anchored-SP and frugal-PC algorithms. Specifically, the anchored-SP algorithm finds the most frugal graph among all possible graphs satisfying the Markov condition while the frugal-PC algorithm finds the most frugal graph among some graphs. Hence, the frugal-PC algorithm is more computationally feasible, while it requires an additional frugality-faithfulness assumption for soundness. Various simulations support the theoretical findings of this study and demonstrate the practical effectiveness of the proposed algorithm against state-of-the-art algorithms such as ACI, PC, and MMHC. Furthermore, the applications of the proposed algorithm to protein signaling data and breast cancer data illustrate its effectiveness in uncovering relationships among proteins and among cancer-related cell nuclei characteristics.},
  archive      = {J_CSDA},
  author       = {Joonho Shin and Junhyoung Chung and Seyong Hwang and Gunwoong Park},
  doi          = {10.1016/j.csda.2025.108267},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108267},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Discovering causal structures in corrupted data: Frugality in anchored gaussian DAG models},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Estimation of semiparametric probit model based on case-cohort interval-censored failure time data. <em>CSDA</em>, <em>213</em>, 108266. (<a href='https://doi.org/10.1016/j.csda.2025.108266'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of semiparametric probit model is discussed for the situation where one observes interval-censored failure time data arising from case-cohort studies. The probit model has recently attracted some attention for regression analysis of failure time data partly due to the popularity of the normal distribution and its similarity to linear models. Although some methods have been developed in the literature for its estimation, it does not seem to exist an established approach for the situation of case-cohort interval-censored data. To address this, a pseudo-maximum likelihood method is proposed and furthermore, an EM algorithm is developed for its implementation. The resulting estimators of regression parameters are shown to be consistent and asymptotically follow the normal distribution. To assess the empirical performance of the proposed method, a simulation study is conducted and indicates that it works well in practical situations. In addition, it is applied to a set of real data arising from an AIDS clinical trial that motivated this study.},
  archive      = {J_CSDA},
  author       = {Mingyue Du and Ricong Zeng},
  doi          = {10.1016/j.csda.2025.108266},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108266},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of semiparametric probit model based on case-cohort interval-censored failure time data},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Empirical likelihood based bayesian variable selection. <em>CSDA</em>, <em>213</em>, 108258. (<a href='https://doi.org/10.1016/j.csda.2025.108258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical likelihood is a popular nonparametric statistical tool that does not require any distributional assumptions. The possibility of conducting variable selection via Bayesian empirical likelihood is studied both theoretically and empirically. Theoretically, it is shown that when the prior distribution satisfies certain mild conditions, the corresponding Bayesian empirical likelihood estimators are posteriorly consistent and variable selection consistent. As special cases, the prior of Bayesian empirical likelihood LASSO and SCAD satisfy such conditions and thus can identify the non-zero elements of the parameters with probability approaching 1. In addition, it is easy to verify that those conditions are met for other widely used priors such as ridge, elastic net and adaptive LASSO. Empirical likelihood depends on a parameter that needs to be obtained by numerically solving a non-linear equation. Thus, there exists no conjugate prior for the posterior distribution, which causes the slow convergence of the MCMC sampling algorithm in some cases. To solve this problem, an approximation distribution is used as the proposal to enhance the acceptance rate and, therefore, facilitate faster computation. The computational results demonstrate quick convergence for the examples used in the paper. Both simulations and real data analyses are performed to illustrate the advantages of the proposed methods.},
  archive      = {J_CSDA},
  author       = {Yichen Cheng and Yichuan Zhao},
  doi          = {10.1016/j.csda.2025.108258},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108258},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Empirical likelihood based bayesian variable selection},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bagging cross-validated bandwidth selection in nonparametric regression estimation with applications to large-sized samples. <em>CSDA</em>, <em>213</em>, 108257. (<a href='https://doi.org/10.1016/j.csda.2025.108257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-validation is a well-known and widely used bandwidth selection method in nonparametric regression estimation. However, this technique has two remarkable drawbacks: (i) the large variability of the selected bandwidths, and (ii) the inability to provide results in a reasonable time for very large sample sizes. To address these issues, bagged cross-validation bandwidth selectors are investigated. This approach consists in computing the cross-validation bandwidths for a finite number of subsamples and then rescaling the averaged smoothing parameters to the original sample size. Under a random-design regression model, asymptotic expressions up to a second-order for the bias and variance of the leave-one-out cross-validation bandwidth for the Nadaraya–Watson estimator are obtained. Subsequently, the asymptotic bias and variance and the limiting distribution for the bagged cross-validation selector are derived. Suitable choices of the number of subsamples and the subsample size lead to a convergence rate proportional to the inverse square root of the sample size for the bagging cross-validation selector, outperforming the slower rate typically associated with leave-one-out cross-validation. Several simulations and an illustration on a real dataset related to the COVID-19 pandemic show the behavior of our proposal and its better performance, in terms of statistical efficiency and computing time, when compared to leave-one-out cross-validation.},
  archive      = {J_CSDA},
  author       = {Daniel Barreiro-Ures and Ricardo Cao and Mario Francisco-Fernández and Rubén Fernández-Casal},
  doi          = {10.1016/j.csda.2025.108257},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108257},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bagging cross-validated bandwidth selection in nonparametric regression estimation with applications to large-sized samples},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Variable selection in AUC-optimizing classification. <em>CSDA</em>, <em>213</em>, 108256. (<a href='https://doi.org/10.1016/j.csda.2025.108256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing the receiver operating characteristic (ROC) curve is a popular way to evaluate a binary classifier under imbalanced scenarios frequently encountered in practice. A practical approach to constructing a linear binary classifier is presented by simultaneously optimizing the area under the ROC curve (AUC) and selecting informative variables in high dimensions. In particular, the smoothly clipped absolute deviation (SCAD) penalty is employed, and its oracle property is established, which enables the development of a consistent BIC-type information criterion that greatly facilitates the tuning procedure. Both simulated and real data analyses demonstrate the promising performance of the proposed method in terms of AUC optimization and variable selection.},
  archive      = {J_CSDA},
  author       = {Hyungwoo Kim and Seung Jun Shin},
  doi          = {10.1016/j.csda.2025.108256},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108256},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable selection in AUC-optimizing classification},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Estimating a smooth covariance for functional data. <em>CSDA</em>, <em>213</em>, 108255. (<a href='https://doi.org/10.1016/j.csda.2025.108255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis frequently involves estimating a smooth covariance function based on observed data. This estimation is essential for understanding interactions among functions and constitutes a fundamental aspect of numerous advanced methodologies, including functional principal component analysis. Two approaches for estimating smooth covariance functions in the presence of measurement errors are introduced. The first method employs a low-rank approximation of the covariance matrix, while the second ensures positive definiteness via a Cholesky decomposition. Both approaches employ the use of penalized regression to produce smooth covariance estimates and have been validated through comprehensive simulation studies. The practical application of these methods is demonstrated through the examination of average weekly milk yields in dairy cows as well as egg-laying patterns of Mediterranean fruit flies.},
  archive      = {J_CSDA},
  author       = {Uche Mbaka and James Owen Ramsay and Michelle Carey},
  doi          = {10.1016/j.csda.2025.108255},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108255},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimating a smooth covariance for functional data},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Random effects misspecification and its consequences for prediction in generalized linear mixed models. <em>CSDA</em>, <em>213</em>, 108254. (<a href='https://doi.org/10.1016/j.csda.2025.108254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When fitting generalized linear mixed models, choosing the random effects distribution is an important decision. As random effects are unobserved, misspecification of their distribution is a real possibility. Thus, the consequences of random effects misspecification for point prediction and prediction inference of random effects in generalized linear mixed models need to be investigated. A combination of theory, simulation, and a real application is used to explore the effect of using the common normality assumption for the random effects distribution when the correct specification is a mixture of normal distributions, focusing on the impacts on point prediction, mean squared prediction errors, and prediction intervals. Results show that the level of shrinkage for the predicted random effects can differ greatly under the two random effect distributions, and so is susceptible to misspecification. Also, the unconditional mean squared prediction errors for the random effects are almost always larger under the misspecified normal random effects distribution, while results for the mean squared prediction errors conditional on the random effects are more complicated but remain generally larger under the misspecified distribution (especially when the true random effect is close to the mean of one of the component distributions in the true mixture distribution). Results for prediction intervals indicate that the overall coverage probability is, in contrast, not greatly impacted by misspecification. It is concluded that misspecifying the random effects distribution can affect prediction of random effects, and greater caution is recommended when adopting the normality assumption in generalized linear mixed models.},
  archive      = {J_CSDA},
  author       = {Quan Vu and Francis K.C. Hui and Samuel Muller and A.H. Welsh},
  doi          = {10.1016/j.csda.2025.108254},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108254},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Random effects misspecification and its consequences for prediction in generalized linear mixed models},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bayesian optimization sequential surrogate (BOSS) algorithm: Fast bayesian inference for a broad class of bayesian hierarchical models. <em>CSDA</em>, <em>213</em>, 108253. (<a href='https://doi.org/10.1016/j.csda.2025.108253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate Bayesian inference based on Laplace approximation and quadrature has become increasingly popular for its efficiency in fitting latent Gaussian models (LGM). However, many useful models can only be fitted as LGMs if some conditioning parameters are fixed. Such models are termed conditional LGMs, with examples including change-point detection, non-linear regression, and many others. Existing methods for fitting conditional LGMs rely on grid search or sampling-based approaches to explore the posterior density of the conditioning parameters; both require a large number of evaluations of the unnormalized posterior density of the conditioning parameters. Since each evaluation requires fitting a separate LGM, these methods become computationally prohibitive beyond simple scenarios. In this work, the Bayesian Optimization Sequential Surrogate (BOSS) algorithm is introduced, which combines Bayesian optimization with approximate Bayesian inference methods to significantly reduce the computational resources required for fitting conditional LGMs. With orders of magnitude fewer evaluations than those required by the existing methods, BOSS efficiently generates sequential design points that capture the majority of the posterior mass of the conditioning parameters and subsequently yields an accurate surrogate posterior distribution that can be easily normalized. The efficiency, accuracy, and practical utility of BOSS are demonstrated through extensive simulation studies and real-world applications in epidemiology, environmental sciences, and astrophysics.},
  archive      = {J_CSDA},
  author       = {Dayi Li and Ziang Zhang},
  doi          = {10.1016/j.csda.2025.108253},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108253},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian optimization sequential surrogate (BOSS) algorithm: Fast bayesian inference for a broad class of bayesian hierarchical models},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GMM estimation of fixed effects partially linear additive SAR model with space-time correlated disturbances. <em>CSDA</em>, <em>213</em>, 108252. (<a href='https://doi.org/10.1016/j.csda.2025.108252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to study the ubiquitous space-time panel data in real world, a fixed effects partially linear additive spatial autoregressive (SAR) model with space-time correlated disturbances is proposed. Compared to the linear panel model with space-time correlated disturbances, it can simultaneously capture substantial spatial dependence of response, linearity and nonlinearity between response and regressors, spatial and serial correlations of disturbances, and avoid “curse of dimensionality” of nonparametric regression. By using B-splines to fit additive components and constructing linear and quadratic moment conditions which incorporate information in disturbances, the generalized method of moments (GMM) estimators of unknown parameters and additive components are obtained. Under certain regularity assumptions, it is proved that the GMM estimators are consistent and asymptotically normal. Furthermore, the asymptotically efficient best GMM estimators under normality are derived. Monte Carlo simulation and empirical analysis illustrate that the developed estimation method has good finite sample performance and application prospects.},
  archive      = {J_CSDA},
  author       = {Bogui Li and Jianbao Chen},
  doi          = {10.1016/j.csda.2025.108252},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108252},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {GMM estimation of fixed effects partially linear additive SAR model with space-time correlated disturbances},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Inferring the dynamics of quasi-reaction systems via nonlinear local mean-field approximations. <em>CSDA</em>, <em>213</em>, 108251. (<a href='https://doi.org/10.1016/j.csda.2025.108251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter estimation of kinetic rates in stochastic quasi-reaction systems can be challenging, particularly when the time gap between consecutive measurements is large. Local linear approximation approaches account for the stochasticity in the system but fail to capture the intrinsically nonlinear nature of the mean dynamics of the process. Moreover, the mean dynamics of a quasi-reaction system can be described by a system of ODEs, which have an explicit solution only for simple unitary systems. An approximate analytical solution is derived for generic quasi-reaction systems via a first-order Taylor approximation of the hazard rate. This allows a nonlinear forward prediction of the future dynamics given the current state of the system. Predictions and corresponding observations are embedded in a nonlinear least-squares approach for parameter estimation. The performance of the algorithm is compared to existing methods via a simulation study. Besides the generality of the approach in the specification of the quasi-reaction system and the gains in computational efficiency, the results show an improvement in the kinetic rate estimation, particularly for data observed at large time intervals. Additionally, the availability of an explicit solution makes the method robust to stiffness, which is often present in biological systems. Application to Rhesus Macaque data illustrates the use of the method in the study of cell differentiation.},
  archive      = {J_CSDA},
  author       = {Matteo Framba and Veronica Vinciotti and Ernst C. Wit},
  doi          = {10.1016/j.csda.2025.108251},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108251},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Inferring the dynamics of quasi-reaction systems via nonlinear local mean-field approximations},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Sample-specific cooperative learning integrating heterogeneous radiomics and pathomics data. <em>CSDA</em>, <em>213</em>, 108250. (<a href='https://doi.org/10.1016/j.csda.2025.108250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-omics analysis offers unparalleled insights into the interlinked molecular interactions that govern the underlying biological processes. In the era of big data, driven by the emergence of high-throughput technologies, it is possible to gain a more comprehensive and detailed understanding of complex systems. Nevertheless, the challenges lie in developing methods to effectively integrate and analyze this wealth of data. This challenge is even more apparent when the type of -omics data (e.g., pathomics) lacks pixel-to-pixel or region-to-region correspondence across the population. A novel sample-specific cooperative learning framework is introduced, designed to adaptively manage diverse multi-omics data types, even when there is no direct correspondence between regions. The proposed framework is defined for both continuous and categorical outcomes, with theoretical guarantees based on finite samples. Model performance is demonstrated and compared with existing methods using real-world datasets involving proteomics and metabolomics, and radiomics and pathomics.},
  archive      = {J_CSDA},
  author       = {Shih-Ting Huang and Graham A. Colditz and Shu Jiang},
  doi          = {10.1016/j.csda.2025.108250},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108250},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sample-specific cooperative learning integrating heterogeneous radiomics and pathomics data},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). On jeffreys's cardioid distribution. <em>CSDA</em>, <em>213</em>, 108248. (<a href='https://doi.org/10.1016/j.csda.2025.108248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cardioid distribution, despite being one of the fundamental models for circular data, has received limited attention both methodologically and in terms of its implementation in R. To redress these shortcomings, published results on the model are summarized, corrected and extended, and the scope and limitations of the existing support for the model in R identified. A thorough investigation into the performance of trigonometric moment and maximum likelihood based approaches to point and interval estimation of the model's location and concentration parameters is presented, and goodness-of-fit techniques outlined. A suite of reliable R functions is provided for the model's practical application. The application of the proposed inferential methods and R functions is illustrated by an analysis of palaeocurrent cross-bed azimuths.},
  archive      = {J_CSDA},
  author       = {Arthur Pewsey},
  doi          = {10.1016/j.csda.2025.108248},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108248},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On jeffreys's cardioid distribution},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Boosting interaction tree stumps for modeling interactions. <em>CSDA</em>, <em>213</em>, 108247. (<a href='https://doi.org/10.1016/j.csda.2025.108247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating interaction effects is essential for accurately modeling complex underlying relationships in many applications. Often, not only strong predictive performance is desired, but also the interpretability of the resulting model. This need is evident in areas such as epidemiology, in which uncovering the interplay of biological mechanisms is critical for understanding complex diseases. Classical linear models, frequently used for constructing genetic risk scores, fail to capture interaction effects autonomously, while modern machine learning methods such as gradient boosting often produce black-box models that lack interpretability. Existing linear interaction models are largely limited to consider two-way interactions. To address these limitations, a novel statistical learning method, BITS (Boosting Interaction Tree Stumps), is introduced to construct linear models while autonomously detecting and incorporating interaction effects. BITS uses gradient boosting on interaction tree stumps, i.e., decision trees with a single split, where in BITS this split can possibly occur on an interaction term. A branch-and-bound approach is employed in BITS to discard weakly predictive terms. For high-dimensional data, a hybrid search strategy combining greedy and exhaustive approaches is proposed. Regularization techniques are integrated to prevent overfitting and the inclusion of spurious interaction effects. Simulation studies and real data applications demonstrate that BITS produces interpretable models with strong predictive performance. Moreover, in the simulation study, BITS primarily identifies truly influential terms.},
  archive      = {J_CSDA},
  author       = {Michael Lau and Tamara Schikowski and Holger Schwender},
  doi          = {10.1016/j.csda.2025.108247},
  journal      = {Computational Statistics & Data Analysis},
  month        = {1},
  pages        = {108247},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Boosting interaction tree stumps for modeling interactions},
  volume       = {213},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
