<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ARTMED</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="artmed">ARTMED - 158</h2>
<ul>
<li><details>
<summary>
(2025). The interpretable deep learning framework and validation for seizure detection in pediatric electroencephalography: An improved accuracy and performance analysis. <em>ARTMED</em>, <em>170</em>, 103276. (<a href='https://doi.org/10.1016/j.artmed.2025.103276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an interpretable deep learning framework and compares the two novel models. A fully convolutional network with squeeze-and-excitation modules (SE-FCN) is designed to enhance spatial sensitivity and retain temporal resolution. In addition, a transformer-based model (TransNet) is developed to capture temporal and channel-wise dependencies via self-attention. These two models output channel saliency weights to the EEG electrode space and generate heatmaps for inferring potential epileptogenic zones. Deep learning primarily adopts convolutional neural networks (CNNs) or sequence generation networks (SGNs) and faces the limitations. For instance, CNN-based models often lack hierarchical modeling and fail to quantify channel-wise contributions, hindering spatial localization. SGN-based models struggle to capture complex spatiotemporal dependencies and typically lack adaptive attention tailored to electroencephalography (EEG) characters. Epileptic seizure detection is vital for effective clinical intervention and existing methods operated as black boxes, limiting clinical interpretability. This study evaluates the models on the CHB-MIT pediatric EEG dataset using a subject-independent cross-validation protocol. SE-FCN achieves an AUC of 0.89 and accuracy of 86.7 %, while TransNet achieves an AUC of 0.92 and accuracy of 86.4 %. Saliency maps from both models demonstrate high consistency and enable categorization of 22 patients into five groups based on inferred seizure origins.},
  archive      = {J_ARTMED},
  author       = {Yu Zhou and Yuxin Gao and Qiang Li and Ruiheng Wu and Aiping Yang and Ming-Lang Tseng},
  doi          = {10.1016/j.artmed.2025.103276},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103276},
  shortjournal = {Artif. Intell. Med.},
  title        = {The interpretable deep learning framework and validation for seizure detection in pediatric electroencephalography: An improved accuracy and performance analysis},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding the cortical responses to mechanical wrist perturbations: A two-step shared structure NARX method. <em>ARTMED</em>, <em>170</em>, 103273. (<a href='https://doi.org/10.1016/j.artmed.2025.103273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared structure nonlinear autoregressive with exogenous input (NARX) model is a promising tool for exploring cortical responses mechanism to external stimuli, essential for advancing our understanding of brain function and developing methods for direct brain information encoding. In this paper, we proposed a two-step method to overcome limitations in existing method, which neglect data relationships and rely on a greedy search for regression terms, leading to less accurate models. In our approach, data from multiple trials are concatenated, and then the orthogonal forward regression (OFR) algorithm identifies model terms in first step, enhancing inter-trial connections and establishing a preliminary model for each subject. Shared model terms across subjects are then used to construct a general target model. Next, non-shared regression terms that best represent population-level information are identified, using adaptive multi-population genetic algorithms, and use to enhance the target models' descriptive power. Simulations results show significant competitiveness in terms of accuracy as compared to other state-of-the-art methods. When applied to real electroencephalography signals under mechanical disturbance, structural and parameter analysis revealed consistent neural response patterns across subjects, with subject-specific responses likely stemming from muscle feedback. Frequency response analysis further suggests that the brain may generate motor inhibition signals based on sensory inputs to maintain a pre-disturbance resting state. These findings provide valuable insights into cortical response mechanisms and have potential implications for future brain information encoding research.},
  archive      = {J_ARTMED},
  author       = {Nan Zheng and Yurong Li and Wuxiang Shi and Jiyu Tan},
  doi          = {10.1016/j.artmed.2025.103273},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103273},
  shortjournal = {Artif. Intell. Med.},
  title        = {Decoding the cortical responses to mechanical wrist perturbations: A two-step shared structure NARX method},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven dynamic grouping for adaptive clinical trials: Rethinking randomization in precision medicine. <em>ARTMED</em>, <em>170</em>, 103272. (<a href='https://doi.org/10.1016/j.artmed.2025.103272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating artificial intelligence into biomedical human subjects research is transforming traditional experimental paradigms. This perspective introduces the concept of “dynamic grouping,” wherein artificial intelligence (AI) systems continuously reassign participants across experimental conditions based on real-time biomarker data and clinical response patterns. Unlike traditional biomedical research designs that rely on fixed treatment and control groups, dynamic grouping allows participant assignments to evolve throughout the study. We examine the ethical implications, methodological challenges, and research opportunities associated with this paradigm, particularly in clinical trials, precision medicine, and digital therapeutics. To support this analysis, we present three computational simulations that quantify its impact: (i) a heterogeneity simulation demonstrating how patient variability affects the advantage of dynamic grouping, (ii) a statistical power analysis showing potential sample size reductions in adaptive designs, and (iii) a clinical outcome distribution analysis highlighting how dynamic grouping reduces negative treatment outcomes and optimizes patient responses. Our findings suggest that dynamic grouping can improve treatment effectiveness, enhance resource allocation, and increase statistical efficiency, although it also raises new challenges for causal inference, informed consent, and regulatory oversight. As AI continues to reshape medical research, adapting ethical and methodological frameworks will be essential for its responsible implementation.},
  archive      = {J_ARTMED},
  author       = {Madhur Mangalam},
  doi          = {10.1016/j.artmed.2025.103272},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103272},
  shortjournal = {Artif. Intell. Med.},
  title        = {AI-driven dynamic grouping for adaptive clinical trials: Rethinking randomization in precision medicine},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining multi-electrode and multi-wave electroencephalogram based time-interval temporal patterns for improved classification capabilities and explainability. <em>ARTMED</em>, <em>170</em>, 103269. (<a href='https://doi.org/10.1016/j.artmed.2025.103269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-computer interface (BCI) systems, and particularly electroencephalogram (EEG) based BCI systems, have become more widely used in recent years and are utilized in various applications and domains ranging from medicine and marketing to games and entertainment. While different algorithms have been used to analyze EEG data and enable its classification, existing algorithms have two main drawbacks; both their classification and explainability capabilities are limited. Lacking in explainability, they cannot indicate which electrodes and waves led to a classification decision or explain how areas and frequencies of the brain's activity correlate to a specific task. In this study, we propose a novel extension for the time-interval temporal patterns mining algorithms aimed at enhancing the data mining process by enabling a richer set of patterns to be learned from the EEG data, thereby contributing to improved classification and explainability capabilities. The extended algorithm is designed to capture and leverage the unique nature of EEG data by decomposing it into different brain waves and modeling the relations among them and between different electrodes. Our evaluation of the proposed extended algorithm on multiple learning tasks and three EEG datasets demonstrated the extended algorithm's ability to mine richer patterns that improve the classification performance by 4–11 % based on the Area-Under the receiver operating characteristic Curve (AUC) metric, compared to the original version of the algorithm. Moreover, the algorithm was shown to shed light on the areas and frequencies of the brain's activity that are correlated with specific tasks.},
  archive      = {J_ARTMED},
  author       = {Ofir Landau and Nir Nissim},
  doi          = {10.1016/j.artmed.2025.103269},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103269},
  shortjournal = {Artif. Intell. Med.},
  title        = {Mining multi-electrode and multi-wave electroencephalogram based time-interval temporal patterns for improved classification capabilities and explainability},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical multimodal foundation models in clinical diagnosis and treatment: Applications, challenges, and future directions. <em>ARTMED</em>, <em>170</em>, 103265. (<a href='https://doi.org/10.1016/j.artmed.2025.103265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning have significantly revolutionized the field of clinical diagnosis and treatment, offering novel approaches to improve diagnostic precision and treatment efficacy across diverse clinical domains, thus driving the pursuit of precision medicine. The growing availability of multi-organ and multimodal datasets has accelerated the development of large-scale Medical Multimodal Foundation Models (MMFMs). These models, known for their strong generalization capabilities and rich representational power, are increasingly being adapted to address a wide range of clinical tasks, from early diagnosis to personalized treatment strategies. This review offers a comprehensive analysis of recent developments in MMFMs, focusing on three key aspects: datasets, model architectures, and clinical applications. We also explore the challenges and opportunities in optimizing multimodal representations and discuss how these advancements are shaping the future of healthcare by enabling improved patient outcomes and more efficient clinical workflows.},
  archive      = {J_ARTMED},
  author       = {Kai Sun and Siyan Xue and Fuchun Sun and Haoran Sun and Yu Luo and Ling Wang and Siyuan Wang and Na Guo and Lei Liu and Tian Zhao and Xinzhou Wang and Lei Yang and Shuo Jin and Jun Yan and Jiahong Dong},
  doi          = {10.1016/j.artmed.2025.103265},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103265},
  shortjournal = {Artif. Intell. Med.},
  title        = {Medical multimodal foundation models in clinical diagnosis and treatment: Applications, challenges, and future directions},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end solution for out-of-hospital emergency medical dispatch triage based on multimodal and continual deep learning. <em>ARTMED</em>, <em>170</em>, 103264. (<a href='https://doi.org/10.1016/j.artmed.2025.103264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this study was to build a multimodal, multitask predictive model—named E2eDeepEMC 2 —to improve out-of-hospital emergency incident severity assessments while coping with shifts in data distributions over time. We drew on 2 054 694 independent incidents recorded by the Valencian emergency medical dispatch service between 2009 and 2019 (excluding 2013), combining demographic, temporal, clinical and free-text inputs. To handle temporal drift, our model integrates continual learning strategies and comprises three encoder modules (for context, clinical data and text), whose outputs are merged to predict the life-threatening level, admissible response delay and emergency system jurisdiction. Compared with the Valencian Region’s existing in-house triage protocol, E2eDeepEMC 2 achieved absolute F1-score gains of 18.46% for life-threatening level, 25.96% for response delay and 3.63% for jurisdiction. Compared to non-continual learning baselines, it also outperformed them by 3.04%, 9.66% and 0.58%, respectively. Deployment of E2eDeepEMC 2 is currently underway in the Valencian Region, underscoring its practical impact on real-world emergency dispatch decision-making.},
  archive      = {J_ARTMED},
  author       = {Pablo Ferri and Carlos Sáez and Antonio Félix-De Castro and Purificación Sánchez-Cuesta and Juan M. García-Gómez},
  doi          = {10.1016/j.artmed.2025.103264},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103264},
  shortjournal = {Artif. Intell. Med.},
  title        = {An end-to-end solution for out-of-hospital emergency medical dispatch triage based on multimodal and continual deep learning},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LoRA-PT: Low-rank adapting UNETR for hippocampus segmentation using principal tensor singular values and vectors. <em>ARTMED</em>, <em>170</em>, 103254. (<a href='https://doi.org/10.1016/j.artmed.2025.103254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hippocampus is an important brain structure involved in various psychiatric disorders, and its automatic and accurate segmentation is vital for studying these diseases. Recently, deep learning-based methods have made significant progress in hippocampus segmentation. However, training deep neural network models requires substantial computational resources, time, and a large amount of labeled training data, which is frequently scarce in medical image segmentation. To address these issues, we propose LoRA-PT, a novel parameter-efficient fine-tuning (PEFT) method that transfers the pre-trained UNETR model from the BraTS2021 dataset to the hippocampus segmentation task. Specifically, LoRA-PT divides the parameter matrix of the transformer structure into three distinct sizes, yielding three third-order tensors. These tensors are decomposed using tensor singular value decomposition to generate low-rank tensors consisting of the principal singular values and vectors, with the remaining singular values and vectors forming the residual tensor. During fine-tuning, only the low-rank tensors (i.e., the principal tensor singular values and vectors) are updated, while the residual tensors remain unchanged. We validated the proposed method on three public hippocampus datasets, and the experimental results show that LoRA-PT outperformed state-of-the-art PEFT methods in segmentation accuracy while significantly reducing the number of parameter updates. Our source code is available at https://github.com/WangangCheng/LoRA-PT/tree/LoRA-PT .},
  archive      = {J_ARTMED},
  author       = {Guanghua He and Wangang Cheng and Hancan Zhu and Gaohang Yu},
  doi          = {10.1016/j.artmed.2025.103254},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103254},
  shortjournal = {Artif. Intell. Med.},
  title        = {LoRA-PT: Low-rank adapting UNETR for hippocampus segmentation using principal tensor singular values and vectors},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMSupcon: An image fusion-based multi-modal supervised contrastive method for brain tumor diagnosis. <em>ARTMED</em>, <em>170</em>, 103253. (<a href='https://doi.org/10.1016/j.artmed.2025.103253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosis of brain tumors is pivotal for effective treatment, with MRI serving as a commonly used non-invasive diagnostic modality in clinical practices. Fundamentally, brain tumor diagnosis is a type of pattern recognition task that requires the integration of information from multi-modal MRI images. However, existing fusion strategies are hindered by the scarcity of multi-modal imaging samples. In this paper, we propose a new training paradigm tailored for the scenario of multi-modal imaging in brain tumor diagnosis, called multi-modal supervised contrastive learning method (MMSupcon). This method significantly enhances diagnostic accuracy through two key components: multi-modal medical image fusion and multi-modal supervised contrastive loss. First, the fusion component integrates complementary imaging modalities to generate information-rich samples. Second, by introducing fused samples to guide original samples in learning feature consistency or inconsistency among classes, our loss component effectively preserves the integrity of cross-modal information while maintaining the distinctiveness of individual modalities. Finally, MMSupcon is validated on a real-world brain tumor dataset collected from Beijing Tiantan Hospital, achieving state-of-the-art performance. Furthermore, additional experiments on two public BraTS glioma classification datasets also demonstrate our substantial performance improvements. The source code is released at https://github.com/hywang02/MMSupcon .},
  archive      = {J_ARTMED},
  author       = {Haoyu Wang and Jing Zhang and Siying Wu and Haoran Wei and Xun Chen and Yunwei Ou and Xiaoyan Sun},
  doi          = {10.1016/j.artmed.2025.103253},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103253},
  shortjournal = {Artif. Intell. Med.},
  title        = {MMSupcon: An image fusion-based multi-modal supervised contrastive method for brain tumor diagnosis},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathway information on methylation analysis using deep neural network (PROMINENT): An interpretable deep learning method with pathway prior for phenotype prediction using gene-level DNA methylation. <em>ARTMED</em>, <em>170</em>, 103236. (<a href='https://doi.org/10.1016/j.artmed.2025.103236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background DNA methylation is a key epigenetic marker that influences gene expression and phenotype regulation, and is affected by both genetic and environmental factors. Traditional linear regression methods such as elastic nets have been employed to assess the cumulative effects of multiple DNA methylation markers on phenotypes. However, these methods often fail to capture the complex nonlinear nature of the data. Recent deep learning approaches, such as MethylNet, have improved the prediction accuracy but lack interpretability and efficiency. Findings To address these limitations, we introduced P athway Info r mati o n on M ethylat i on Analysis using a Deep Ne ural N e t work (PROMINENT), a novel interpretable deep learning method that integrates gene-level DNA methylation data with biological pathway information for phenotype prediction. PROMINENT enhances interpretability and prediction accuracy by incorporating gene- and pathway-level priors from databases such as Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG). It employs SHapley Additive exPlanations (SHAP) to prioritize significant genes and pathways. Evaluated across various datasets, childhood asthma, idiopathic pulmonary fibrosis (IPF), and first-episode psychosis (FEP)—PROMINENT consistently outperformed existing methods in terms of prediction accuracy and computational efficiency. PROMINENT also identified crucial genes and pathways involved in disease mechanisms. Conclusions PROMINENT represents a significant advancement in leveraging DNA methylation data for phenotype prediction, offering both high accuracy and interpretability within reasonable computational time. This method holds promise for elucidating the epigenetic underpinnings of complex diseases and enhancing the utility of DNA methylation data in biomedical research.},
  archive      = {J_ARTMED},
  author       = {Soyeon Kim and Laizhi Zhang and Yidi Qin and Rebecca I. Caldino Bohn and Hyun Jung Park},
  doi          = {10.1016/j.artmed.2025.103236},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103236},
  shortjournal = {Artif. Intell. Med.},
  title        = {Pathway information on methylation analysis using deep neural network (PROMINENT): An interpretable deep learning method with pathway prior for phenotype prediction using gene-level DNA methylation},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unprepared and overwhelmed: A case for clinician-focused AI education. <em>ARTMED</em>, <em>169</em>, 103252. (<a href='https://doi.org/10.1016/j.artmed.2025.103252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This perspective illustrates the need for improved AI education for clinicians, highlighting gaps in current approaches and technical content. It advocates for the creation of AI guides specifically designed for clinicians integrating case-based learning approaches and led by clinical informaticians. We emphasize the importance of modern medical educational strategies, and reflect on relevance and applicability of AI education, to ensure clinicians are prepared for safe, effective, and efficient AI-driven healthcare. 1–2 Sentence description This position article reflects on the current landscape of AI educational guides for clinicians, identifying gaps in instructional approaches and technical content. We propose the development of case-based AI education modules led by clinical informatics physicians in collaboration with professional societies.},
  archive      = {J_ARTMED},
  author       = {Nadia Siddiqui and Yazan Bouchi and Ellen Kim and Jonathan D. Hron and John Park and John Kang},
  doi          = {10.1016/j.artmed.2025.103252},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103252},
  shortjournal = {Artif. Intell. Med.},
  title        = {Unprepared and overwhelmed: A case for clinician-focused AI education},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical foundations for trustworthy medical imaging: A survey for artificial intelligence researchers. <em>ARTMED</em>, <em>169</em>, 103251. (<a href='https://doi.org/10.1016/j.artmed.2025.103251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence in medical imaging has grown rapidly in the past decade, driven by advances in deep learning and widespread access to computing resources. Applications cover diverse imaging modalities, including those based on electromagnetic radiation (e.g., X-rays), subatomic particles (e.g., nuclear imaging), and acoustic waves (ultrasound). Each modality features and limitations are defined by its underlying physics. However, many artificial intelligence practitioners lack a solid understanding of the physical principles involved in medical image acquisition. This gap hinders leveraging the full potential of deep learning, as incorporating physics knowledge into artificial intelligence systems promotes trustworthiness, especially in limited data scenarios. This work reviews the fundamental physical concepts behind medical imaging and examines their influence on recent developments in artificial intelligence, particularly, generative models and reconstruction algorithms. Finally, we describe physics-informed machine learning approaches to improve feature learning in medical imaging.},
  archive      = {J_ARTMED},
  author       = {Miriam Cobo and David Corral Fontecha and Wilson Silva and Lara Lloret Iglesias},
  doi          = {10.1016/j.artmed.2025.103251},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103251},
  shortjournal = {Artif. Intell. Med.},
  title        = {Physical foundations for trustworthy medical imaging: A survey for artificial intelligence researchers},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TIPs: Tooth instance and pulp segmentation based on hierarchical extraction and fusion of anatomical priors from cone-beam CT. <em>ARTMED</em>, <em>169</em>, 103247. (<a href='https://doi.org/10.1016/j.artmed.2025.103247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate instance segmentation of tooth and pulp from cone-beam computed tomography (CBCT) images is essential but highly challenging due to the pulp’s small structures and indistinct boundaries. To address these critical challenges, we propose TIPs designed for T ooth I nstance and P ulp s egmentation. TIPs initially employs a backbone model to segment a binary mask of the tooth from CBCT images, which is then utilized to derive position prior of the tooth and shape prior of the pulp. Subsequently, we propose the Hierarchical Fusion Mamba models to leverage the strengths of both anatomical priors and CBCT images by extracting and integrating shallow and deep features from Convolution Neural Networks (CNNs) and State Space Sequence Models (SSMs), respectively. This process achieves tooth instance and pulp segmentation, which are then combined to obtain the final pulp instance segmentation. Extensive experiments on CBCT scans from 147 patients demonstrate that TIPs significantly outperforms state-of-the-art methods in terms of segmentation accuracy. Furthermore, we have encapsulated this framework into an openly accessible tool for one-click using. To our knowledge, this is the first toolbox capable of segmentation of tooth and pulp instances, with its performance validated on two external datasets comprising 59 samples from the Toothfairy2 dataset and 48 samples from the STS dataset. These results demonstrate the potential of TIPs as a practical tool to boost clinical workflows in digital dentistry, enhancing the precision and efficiency of dental diagnostics and treatment planning.},
  archive      = {J_ARTMED},
  author       = {Tao Zhong and Yang Ning and Xueyang Wu and Li Ye and Chichi Li and Yu Zhang and Yu Du},
  doi          = {10.1016/j.artmed.2025.103247},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103247},
  shortjournal = {Artif. Intell. Med.},
  title        = {TIPs: Tooth instance and pulp segmentation based on hierarchical extraction and fusion of anatomical priors from cone-beam CT},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EvidenceMap: Learning evidence analysis to unleash the power of small language models for biomedical question answering. <em>ARTMED</em>, <em>169</em>, 103246. (<a href='https://doi.org/10.1016/j.artmed.2025.103246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When addressing professional questions in the biomedical domain, humans typically acquire multiple pieces of information as evidence and engage in multifaceted analysis to provide high-quality answers. Current LLM-based question answering methods lack a detailed definition and learning process for evidence analysis, leading to the risk of error propagation and hallucinations while using evidence. Although increasing the parameter size of LLMs can alleviate these issues, it also presents challenges in training and deployment with limited resources. In this study, we propose EvidenceMap , which aims to enable a lightweight pre-trained language model to explicitly learn multiple aspects of biomedical evidence, including supportive evaluation, logical correlation and content summarization, thereby latently guiding a generative model (around 3B parameters) to provide textual responses. Experimental results demonstrate that our method, learning evidence analysis by fine-tuning a model with only 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in reference-based quality and accuracy, respectively. The code and dataset for reproducing our framework and experiments are available at https://github.com/ZUST-BIT/EvidenceMap .},
  archive      = {J_ARTMED},
  author       = {Chang Zong and Jian Wan and Siliang Tang and Lei Zhang},
  doi          = {10.1016/j.artmed.2025.103246},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103246},
  shortjournal = {Artif. Intell. Med.},
  title        = {EvidenceMap: Learning evidence analysis to unleash the power of small language models for biomedical question answering},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving federated transfer learning for enhanced liver lesion segmentation in PET–CT imaging. <em>ARTMED</em>, <em>169</em>, 103245. (<a href='https://doi.org/10.1016/j.artmed.2025.103245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positron Emission Tomography-Computed Tomography (PET–CT) evolution is critical for liver lesion diagnosis. However, data scarcity, privacy concerns, and cross-institutional imaging heterogeneity impede accurate deep learning model deployment. We propose a Federated Transfer Learning (FTL) framework that integrates federated learning’s privacy-preserving collaboration with transfer learning’s pre-trained model adaptation, enhancing liver lesion segmentation in PET–CT imaging. By leveraging a Feature Co-learning Block (FCB) and privacy-enhancing technologies (DP, HE), our approach ensures robust segmentation without sharing sensitive patient data. (1) A privacy-preserving FTL framework combining federated learning and adaptive transfer learning; (2) A multi-modal FCB for improved PET–CT feature integration; (3) Extensive evaluation across diverse institutions with privacy-enhancing technologies like Differential Privacy (DP) and Homomorphic Encryption (HE). Experiments on simulated multi-institutional PET–CT datasets demonstrate superior performance compared to baselines, with robust privacy guarantees. The FTL framework reduces data requirements and enhances generalizability, advancing liver lesion diagnostics.},
  archive      = {J_ARTMED},
  author       = {Rajesh Kumar and Shaoning Zeng and Jay Kumar and Zakria and Xinfeng Mao},
  doi          = {10.1016/j.artmed.2025.103245},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103245},
  shortjournal = {Artif. Intell. Med.},
  title        = {Privacy-preserving federated transfer learning for enhanced liver lesion segmentation in PET–CT imaging},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of artificial intelligence in liver cancer: A scoping review. <em>ARTMED</em>, <em>169</em>, 103244. (<a href='https://doi.org/10.1016/j.artmed.2025.103244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introduction This review explores the application of Artificial Intelligence (AI) in managing primary liver cancer, focusing on recent advancements. AI, particularly machine learning (ML) and deep learning (DL), shows potential in improving screening, diagnosis, treatment planning, efficacy assessment, prognosis prediction, and follow-up—crucial elements given the high mortality of liver cancer. Methods A systematic search was conducted in the PubMed, Scopus, Embase, and Web of Science databases, focusing on original research published until June 2024 on AI's clinical applications in liver cancer. Studies not relevant or lacking clinical evaluation were excluded. Results Out of 13,122 screened articles, 62 were selected for full review. The studies highlight significant improvements in detecting hepatocellular carcinoma and intrahepatic cholangiocarcinoma through AI. DL models show high sensitivity and specificity, particularly in early detection. In diagnosis, AI models using CT and MRI data improve precision in distinguishing benign from malignant lesions through multimodal data integration. Discussion Recent AI models outperform earlier non-neural network versions, though a gap remains between development and clinical implementation. Many models lack thorough clinical applicability assessments and external validation. Conclusion AI integration in primary liver cancer management is promising but requires rigorous development and validation practices to enhance clinical outcomes fully.},
  archive      = {J_ARTMED},
  author       = {Andrea Chierici and Fabien Lareyre and Antonio Iannelli and Benjamin Salucki and Sébastien Goffart and Lisa Guzzi and Elise Poggi and Hervé Delingette and Juliette Raffort},
  doi          = {10.1016/j.artmed.2025.103244},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103244},
  shortjournal = {Artif. Intell. Med.},
  title        = {Applications of artificial intelligence in liver cancer: A scoping review},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging explainable artificial intelligence for transparent and trustworthy cancer detection systems. <em>ARTMED</em>, <em>169</em>, 103243. (<a href='https://doi.org/10.1016/j.artmed.2025.103243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely detection of cancer is essential for enhancing patient outcomes. Artificial Intelligence (AI), especially Deep Learning (DL), demonstrates significant potential in cancer diagnostics; however, its opaque nature presents notable concerns. Explainable AI (XAI) mitigates these issues by improving transparency and interpretability. This study provides a systematic review of recent applications of XAI in cancer detection, categorizing the techniques according to cancer type, including breast, skin, lung, colorectal, brain, and others. It emphasizes interpretability methods, dataset utilization, simulation environments, and security considerations. The results indicate that Convolutional Neural Networks (CNNs) account for 31 % of model usage, SHAP is the predominant interpretability framework at 44.4 %, and Python is the leading programming language at 32.1 %. Only 7.4 % of studies address security issues. This study identifies significant challenges and gaps, guiding future research in trustworthy and interpretable AI within oncology.},
  archive      = {J_ARTMED},
  author       = {Shiva Toumaj and Arash Heidari and Nima Jafari Navimipour},
  doi          = {10.1016/j.artmed.2025.103243},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103243},
  shortjournal = {Artif. Intell. Med.},
  title        = {Leveraging explainable artificial intelligence for transparent and trustworthy cancer detection systems},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pandemic transition: A review of social media text mining for pandemic transition in the post-vaccination era. <em>ARTMED</em>, <em>169</em>, 103242. (<a href='https://doi.org/10.1016/j.artmed.2025.103242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the post-vaccination phase of the COVID-19 pandemic, surveillance have become critical for sustaining disease control, identifying new variants, and preserving vaccine efficacy. This study explores how social media text mining can support these priorities by providing valuable insights into public sentiment, vaccine hesitancy, and the emergence of novel viral strains. By analyzing online conversations, researchers can gain a deeper understanding of questions and concerns surrounding booster shots, enabling the development of targeted public health initiatives to address vaccine reluctance and promote booster uptake. Moreover, social media data can assist governments in identifying areas with high vaccine hesitancy or low vaccination rates, allowing for the strategic allocation of resources and interventions. Importantly, this study also highlights the potential of social media text mining to serve as an early warning system for new viral variants. By monitoring discussions related to symptoms and outbreaks, researchers can detect risks before they become widespread, informing timely public health responses and mitigation strategies. Complementing these surveillance efforts, the study emphasizes the significance of pattern prediction, which leverages historical data and models to forecast disease dynamics and guide resource allocation. By integrating social media data with epidemiological and clinical information, more accurate and responsive pandemic management strategies can be implemented. Ultimately, this research underscores the critical role of continuous pandemic monitoring and pattern prediction in the post-vaccination phase, enabling evidence-based decision-making and the effective control of infectious diseases. The insights gained from this study can inform the development of robust, data-driven frameworks for pandemic preparedness and response in the aftermath of widespread vaccination campaigns.},
  archive      = {J_ARTMED},
  author       = {Kiarash Bakhshaei and Zahra Rezaei and Mitra Ahmadi and Yaser Mike Banad},
  doi          = {10.1016/j.artmed.2025.103242},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103242},
  shortjournal = {Artif. Intell. Med.},
  title        = {Pandemic transition: A review of social media text mining for pandemic transition in the post-vaccination era},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BIGPN: Biologically informed graph propagational network for plasma proteomic profiling of neurodegenerative biomarkers. <em>ARTMED</em>, <em>169</em>, 103241. (<a href='https://doi.org/10.1016/j.artmed.2025.103241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative diseases involve progressive neuronal dysfunction, requiring identification of specific pathological features for accurate diagnosis. Although cerebrospinal fluid analysis and neuroimaging are commonly employed, their invasiveness and high-cost limit widespread clinical use. In contrast, blood-based biomarkers offer a non-invasive, cost-effective, and accessible alternative. Recent advances in plasma proteomics combined with machine learning (ML) have further improved diagnostic accuracy; however, the integration of underlying biological information remains largely overlooked. Notably, many ML-based plasma proteomic profiling approaches overlook protein-protein interactions (PPI) and the hierarchical structure of molecular pathways. To address these limitations, we propose Biologically Informed Graph Propagational Network (BIGPN), a novel ML model for plasma proteomic profiling of neurodegenerative biomarkers. BIGPN employs graph neural network-based architecture to harness a PPI network and propagates independent effects of proteins through the PPI network, capturing higher-order interactions with global awareness of PPIs. BIGPN then applies a multi-level pathway structure to extract biologically meaningful feature representations, ensuring that the model reflects structured biological mechanisms, and it provides clear explainability of the pathway structure in the context of importance through probabilistically represented parameters. Experimental validation on the UK Biobank dataset demonstrated the superior performance of BIGPN in neurodegenerative risk prediction, outperforming comparison methods. Furthermore, the explainability of BIGPN facilitated detailed analyses of the discriminative significance of synergistic effects, the predictive importance of proteins, and the longitudinal changes in biomarker profiles, reinforcing its clinical relevance. Overall, BIGPN's integration of PPIs and pathway structure addresses critical gaps in ML-based plasma proteomic profiling, offering a powerful approach for improved neurodegenerative disease diagnosis.},
  archive      = {J_ARTMED},
  author       = {Sunghong Park and Dong-gi Lee and Juhyeon Kim and Masaud Shah and Hyunjung Shin and Hyun Goo Woo},
  doi          = {10.1016/j.artmed.2025.103241},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103241},
  shortjournal = {Artif. Intell. Med.},
  title        = {BIGPN: Biologically informed graph propagational network for plasma proteomic profiling of neurodegenerative biomarkers},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Difficulty-aware coupled contour regression network with IoU loss for efficient IVUS delineation. <em>ARTMED</em>, <em>169</em>, 103240. (<a href='https://doi.org/10.1016/j.artmed.2025.103240'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lumen and external elastic lamina contour delineation is crucial for quantitative analyses of intravascular ultrasound (IVUS) images. However, the various artifacts in IVUS images pose substantial challenges for accurate delineation. Existing mask-based methods often produce anatomically implausible contours in artifact-affected images, while contour-based methods suffer from the over-smooth problem within the artifact regions. In this paper, we directly regress the contour pairs instead of mask-based segmentation. A coupled contour representation is adopted to learn a low-dimensional contour signature space, where the embedded anatomical prior enables the model to avoid producing unreasonable results. Further, a PIoU loss is proposed to capture the overall shape of the contour points and maximize the similarity between the regressed contours and manually delineated contours with various irregular shapes, alleviating the over-smooth problem. For the images with severe artifacts, a difficulty-aware training strategy is designed for contour regression, which gradually guides the model focus on hard samples and improves contour localization accuracy. We evaluate the proposed framework on a large IVUS dataset, consisting of 7204 frames from 185 pullbacks. The mean Dice similarity coefficients of the method for the lumen and external elastic lamina are 0.951 and 0.967, which significantly outperforms other state-of-the-art (SOTA) models. All regressed contours in the test images are anatomically plausible. On the public IVUS-2011 dataset, the proposed method attains comparable performance to the SOTA models with the highest processing speed at 100 fps. The code is available at https://github.com/SMU-MedicalVision/ContourRegression .},
  archive      = {J_ARTMED},
  author       = {Yuan Yang and Xu Yu and Wei Yu and Shengxian Tu and Su Zhang and Wei Yang},
  doi          = {10.1016/j.artmed.2025.103240},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103240},
  shortjournal = {Artif. Intell. Med.},
  title        = {Difficulty-aware coupled contour regression network with IoU loss for efficient IVUS delineation},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiplex aggregation combining sample reweight composite network for pathology image segmentation. <em>ARTMED</em>, <em>169</em>, 103239. (<a href='https://doi.org/10.1016/j.artmed.2025.103239'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In digital pathology, nuclei segmentation is a critical task for pathological image analysis, holding significant importance for diagnosis and research. However, challenges such as blurred boundaries between nuclei and background regions, domain shifts between pathological images, and uneven distribution of nuclei pose significant obstacles to segmentation tasks. To address these issues, we propose an innovative Causal inference inspired Diversified aggregation convolution Network named CDNet, which integrates a Diversified Aggregation Convolution (DAC), a Causal Inference Module (CIM) based on causal discovery principles, and a comprehensive loss function. DAC improves the issue of unclear boundaries between nuclei and background regions, and CIM enhances the model’s cross-domain generalization ability. A novel Stable-Weighted Combined loss function was designed that combined the chunk-computed Dice Loss with the Focal Loss and the Causal Inference Loss to address the issue of uneven nuclei distribution. Experimental evaluations on the MoNuSeg, GLySAC, and MoNuSAC datasets demonstrate that CDNet significantly outperforms other models and exhibits strong generalization capabilities. Specifically, CDNet outperforms the second-best model by 0.79% (mIoU) and 1.32% (DSC) on the MoNuSeg dataset, by 2.65% (mIoU) and 2.13% (DSC) on the GLySAC dataset, and by 1.54% (mIoU) and 1.10% (DSC) on the MoNuSAC dataset. Code is publicly available at https://github.com/7FFDW/CDNet .},
  archive      = {J_ARTMED},
  author       = {Dawei Fan and Zhuo Chen and Yifan Gao and Jiaming Yu and Kaibin Li and Yi Wei and Yanping Chen and Riqing Chen and Lifang Wei},
  doi          = {10.1016/j.artmed.2025.103239},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103239},
  shortjournal = {Artif. Intell. Med.},
  title        = {Multiplex aggregation combining sample reweight composite network for pathology image segmentation},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCLDA: Prediction of lncRNA-disease associations based on convolutional block attention module and capsule network. <em>ARTMED</em>, <em>169</em>, 103238. (<a href='https://doi.org/10.1016/j.artmed.2025.103238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several studies have shown that long non-coding RNAs (lncRNAs) influence the biological processes of many diseases, including disease onset, progression, and recovery. Therefore, predicting potential lncRNA-disease associations (LDAs) is crucial for enhancing disease diagnosis and therapy. Compared with biological experimental methods for identifying potential LDAs, computational approaches offer advantages in terms of efficiency and cost-effectiveness. In this study, we introduce a novel deep learning approach, CCLDA, for predicting LDAs. First, we constructed the functional similarity matrix, Gaussian similarity matrix, and sequence similarity matrix for lncRNAs, and the semantic similarity matrix and Gaussian similarity matrix for diseases, applying a matrix fusion process. Then, lncRNA-disease pairs were constructed, and feature extraction was conducted using a multilayer autoencoder (AE). The extracted features were then fed into a capsule network to train the model and generate prediction scores. A Convolutional Attention Module (CBAM) was integrated into the capsule network to assign weights to convolved features in both channel and spatial dimensions, enhancing overall model prediction performance. We compared CCLDA with other models on two datasets, and the results demonstrated that CCLDA outperformed existing LDA prediction methods. Ablation experiments further confirmed the necessity of CCLDA's components, and case studies on both datasets indicated that CCLDA holds significant potential for predicting novel LDAs. Based on these results, CCLDA will be important for research in lncRNA-disease prediction related fields.},
  archive      = {J_ARTMED},
  author       = {Lingyu Meng and Teng Zhang and Yueying Yang and Jiahui Zhang and Jianjun Tan},
  doi          = {10.1016/j.artmed.2025.103238},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103238},
  shortjournal = {Artif. Intell. Med.},
  title        = {CCLDA: Prediction of lncRNA-disease associations based on convolutional block attention module and capsule network},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diagnostic performance of artificial intelligence in detecting and subtyping pediatric medulloblastoma from histopathological images: A systematic review. <em>ARTMED</em>, <em>169</em>, 103237. (<a href='https://doi.org/10.1016/j.artmed.2025.103237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Medulloblastoma is the most prevalent malignant brain tumor in children, requiring timely and precise diagnosis to improve clinical outcomes. Artificial Intelligence (AI) offers a promising avenue to enhance diagnostic accuracy and efficiency in this domain. Objective This systematic review evaluates the performance of AI models in detecting and subtyping medulloblastomas using histopathological images. Methods In this systematic review, we searched seven databases to identify English-language studies assessing AI-based detection or classification of medulloblastomas in patients under 18 years. Two reviewers independently conducted study selection, data extraction, and risk of bias assessment. Results were synthesized narratively. Results Of 3341 records, 15 studies met inclusion criteria. AI models demonstrated strong diagnostic performance, with mean accuracy of 91.3 %, sensitivity of 94.2 %, and specificity of 97.4 %. Support Vector Machines achieved the highest accuracy (96.3 %) and specificity (99.4 %), while K-Nearest Neighbors showed the highest sensitivity (97.1 %). Detection tasks (accuracy 96.1 %, sensitivity 98.5 %) outperformed subtyping tasks (accuracy 87.3 %, sensitivity 91.3 %). Models analyzing images at the architectural level yielded higher accuracy (94.7 %), sensitivity (94.1 %), and specificity (98.2 %) compared to cellular-level analysis. Conclusion AI algorithms show promise in detecting and subtyping medulloblastomas, but the findings are limited by overreliance on one dataset, small sample sizes, limited study numbers, and lack of meta-analysis Future research should develop larger, more diverse datasets and explore advanced approaches like deep learning and foundation models. Techniques (e.g., model ensembling and multimodal data integration) are needed for better multiclass classification. Further reviews are needed to assess AI's role in other pediatric brain tumors.},
  archive      = {J_ARTMED},
  author       = {Hiba Alzoubi and Alaa Abd-alrazaq and Obada Almaabreh and Rawan AlSaad and Sarah Aziz and Rukaya Al-Dafi and Leen Abu Salih and Leen Turani and Sondos Albqowr and Rawan Abu Tarbosh and Batool Abu Alkishik and Rafat Damseh and Arfan Ahmed and Hashem Abu Serhan},
  doi          = {10.1016/j.artmed.2025.103237},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103237},
  shortjournal = {Artif. Intell. Med.},
  title        = {Diagnostic performance of artificial intelligence in detecting and subtyping pediatric medulloblastoma from histopathological images: A systematic review},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving wearable-based seizure prediction by feature fusion using an explainable growing network. <em>ARTMED</em>, <em>169</em>, 103228. (<a href='https://doi.org/10.1016/j.artmed.2025.103228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unpredictability of seizures is highly burdensome for people with epilepsy and their caregivers, with significant impacts on their health, quality of life, and cognitive, social, and emotional well-being. Non-stigmatizing and user-friendly wearable devices may provide information to predict seizures based on physiological data. We propose a patient-agnostic seizure prediction method that identifies group-level patterns across data from multiple patients. We employ a supervised long-short-term network (LSTM) and add an unsupervised deep canonically correlated autoencoder (DCCAE) and 24-hour patterns using time-of-day information. We fuse features from these three techniques using a growing neural network, allowing incremental learning. Our method incorporates all three feature sets and improves prediction accuracy over the baseline LSTM by 7.3%, from 74.4% to 81.7%, when averaged across all patients, and outperforms the LSTM in 84% of patients. Compared to the all-at-once fusion, the growing network improves the accuracy by 9.5%. We report the contributions from different feature sets using Shapley additive explanations (SHAP). We also analyze the impact of preictal data duration, wearable data quality, and clinical variables on the prediction performance. An effective seizure prediction method using wearable devices has the potential to save lives and significantly improve the quality of life for people with epilepsy.},
  archive      = {J_ARTMED},
  author       = {Tanuj Hasija and Maurice Kuschel and Michele Jackson and Stephanie Dailey and Henric Menne and Claus Reinsberger and Solveig Vieluf and Tobias Loddenkemper},
  doi          = {10.1016/j.artmed.2025.103228},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103228},
  shortjournal = {Artif. Intell. Med.},
  title        = {Improving wearable-based seizure prediction by feature fusion using an explainable growing network},
  volume       = {169},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IHE-Net:Hidden feature discrepancy fusion and triple consistency training for semi-supervised medical image segmentation. <em>ARTMED</em>, <em>168</em>, 103229. (<a href='https://doi.org/10.1016/j.artmed.2025.103229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Teacher-Student (TS) networks have become the mainstream frameworks of semi-supervised deep learning, and are widely used in medical image segmentation. However, traditional TSs based on single or homogeneous encoders often struggle to capture the rich semantic details required for complex, fine-grained tasks. To address this, we propose a novel semi-supervised medical image segmentation framework (IHE-Net), which makes good use of the feature discrepancies of two heterogeneous encoders to improve segmentation performance. The two encoders are instantiated by different learning paradigm networks, namely CNN and Transformer/Mamba, respectively, to extract richer and more robust context representations from unlabeled data. On this basis, we propose a simple yet powerful multi-level feature discrepancy fusion module (MFDF), which effectively integrates different modal features and their discrepancies from two heterogeneous encoders. This design enhances the representational capacity of the model through efficient fusion without introducing additional computational overhead. Furthermore, we introduce a triple consistency learning strategy to improve predictive stability by setting dual decoders and adding mixed output consistency. Extensive experimental results on three skin lesion segmentation datasets, ISIC2017, ISIC2018, and PH2, demonstrate the superiority of our framework. Ablation studies further validate the rationale and effectiveness of the proposed method. Code is available at: https://github.com/joey-AI-medical-learning/IHE-Net .},
  archive      = {J_ARTMED},
  author       = {Mengyi Ju and Bing Wang and Zutong Zhao and Shiyin Zhang and Shuo Yang and Zhihong Wei},
  doi          = {10.1016/j.artmed.2025.103229},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103229},
  shortjournal = {Artif. Intell. Med.},
  title        = {IHE-Net:Hidden feature discrepancy fusion and triple consistency training for semi-supervised medical image segmentation},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of natural language processing in improving cancer care: A scoping review with narrative synthesis. <em>ARTMED</em>, <em>168</em>, 103227. (<a href='https://doi.org/10.1016/j.artmed.2025.103227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectives To review studies of Natural Language Processing (NLP) systems that assist in cancer care, explore use cases and summarise current research progress. Methods A scoping review, searching six databases (1) MEDLINE, (2) Embase, (3) IEEE Xplore, (4) ACM Digital Library, (5) Web of Science, and (6) ACL Anthology. Studies were included that reported NLP systems that had been used to improve cancer management by patients or clinicians. Studies were synthesised descriptively and using content analysis. Results Twenty-nine studies were included. Studies mainly applied NLP in mixed cancer types ( n = 10, 34.48 %) and breast cancer ( n = 8, 27.59 %). NLP was used in four main ways: (1) to support patient education and self-management; (2) to improve efficiency in clinical care by summarising, extracting, and categorising data, and supporting record-keeping; (3) to support prevention and early detection of patient problems or cancer recurrence; and (4) to improve cancer treatment by supporting clinicians to make evidence-based treatment decisions. Studies highlighted a wide variety of use cases for NLP technologies in cancer care. However, few technologies have been evaluated within clinical settings, none have been evaluated against clinical outcomes, and none have been implemented into clinical care. Conclusion NLP has the potential to improve cancer care via several mechanisms, including information extraction and classification, which could enable automation and personalization of care processes. Additionally, NLP tools such as chatbots show promise in improving patient communication and support. However, there are deficiencies in the evaluation and clinical integration challenges. Interdisciplinary collaboration between computer scientists and clinicians will be essential if NLP technologies are to fulfil their potential to improve patient experience and outcomes. Registered Protocol: https://doi.org/10.17605/OSF.IO/G9DSR},
  archive      = {J_ARTMED},
  author       = {Mengxuan Sun and Ehud Reiter and Lisa Duncan and Rosalind Adam},
  doi          = {10.1016/j.artmed.2025.103227},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103227},
  shortjournal = {Artif. Intell. Med.},
  title        = {The role of natural language processing in improving cancer care: A scoping review with narrative synthesis},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ontology-based student testing through clinical guidelines: An AI approach. <em>ARTMED</em>, <em>168</em>, 103226. (<a href='https://doi.org/10.1016/j.artmed.2025.103226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On the basis of our 25-year experience with the GLARE (Guideline Acquisition, Representation and Execution) clinical decision support system, we have started to analyze the adoption of computer-interpretable clinical guidelines (CIGs) and AI techniques to train and test medical students about how to act on patients . Moving from decision support to the educational task involves significant research challenges. In this paper, we propose a new facility that supports teachers in the definition of tests, by selecting and hiding to students specific parts of the CIG, and asking students how they would act on the given case study (patient) in the selected parts. Students are provided with a medical ontology to identify proper actions/decisions, and students' proposals are then automatically compared with what the CIG (considered as a “golden standard”) would suggest to do to the patient through knowledge representation and reasoning techniques. Our basic explanation mechanism exploits the medical ontology to show to students the differences (if any) between their proposals and the ones of the CIG.},
  archive      = {J_ARTMED},
  author       = {Alessio Bottrighi and Antonio Maconi and Stefano Nera and Luca Piovesan and Erica Raina and Paolo Terenziani},
  doi          = {10.1016/j.artmed.2025.103226},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103226},
  shortjournal = {Artif. Intell. Med.},
  title        = {Ontology-based student testing through clinical guidelines: An AI approach},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KGDB-DDI:Knowledge graph-based drug background data fusion model for drug–drug interaction prediction. <em>ARTMED</em>, <em>168</em>, 103225. (<a href='https://doi.org/10.1016/j.artmed.2025.103225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combined use of multiple drugs has been widely used in the treatment of many diseases, so accurate prediction of drug–drug interaction (DDI) plays a critical role in ensuring the health of patients’ medication and improving the efficiency of drug development. However, for many existing DDI prediction methods (KGNN, TIGER, DANN-DDI, etc.), They fail to fully utilize the potential information in drug background data, limiting the predictive power of unknown drug interactions. Therefore, this paper proposes a drug interaction model based on knowledge graph and drug background data (KGDB-DDI), which can effectively integrate knowledge graph and drug background data and use the integrated data to predict DDI. We evaluated the model on DrugBank and KEGG datasets, and the AUC and AUPR of the KGDB-DDI model on the DrugBank dataset reached 0.9952. The experimental results show that the KGDB-DDI model outperforms the classic and other state-of-the-art models and has excellent prediction performance. In addition, multi-angle ablation studies further demonstrate its effectiveness and its potential in predicting drug interactions.},
  archive      = {J_ARTMED},
  author       = {Changpeng Zhao and Dongfang Han and Zicheng Zuo and Turdi Tohti},
  doi          = {10.1016/j.artmed.2025.103225},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103225},
  shortjournal = {Artif. Intell. Med.},
  title        = {KGDB-DDI:Knowledge graph-based drug background data fusion model for drug–drug interaction prediction},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMOTS: Partially supervised framework for abdominal multi-organ and tumor segmentation via aspect-aware complementary. <em>ARTMED</em>, <em>168</em>, 103224. (<a href='https://doi.org/10.1016/j.artmed.2025.103224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving precise segmentation of the abdominal multi-organ and pan-cancer tumors is crucial for disease diagnosis and treatment planning in clinical fields such as surgery and radiotherapy. However, the diversity of abdominal organs and tumor types, as well as partially labeled datasets, significantly increases the difficulty in training and improving segmentation accuracy. While existing methods attempt to focus on the segmentation of multiple organs and lesions simultaneously, they mainly concentrate on addressing the issue of missing labels but do not simultaneously consider improvements at the network level. Thus, this paper introduces a cascaded framework, AMOTS. First, to enhance the model’s feature extraction and analysis capabilities in various extreme scenarios, the framework employs a lightweight convolutional network in the first stage to rapidly localize regions of interest, followed by two structurally identical Aspect-Aware Complementary Networks (AACNet) in the second stage for fine segmentation of organs and tumors, respectively. AACNet includes two novel modules: the Directional Separation Focus Module (DSFM) for multi-directional boundary recognition and the Multi-View Slice Cross Attention Module (MVSCM) for global interaction enhancement. Secondly, ambiguity hard mining and pseudo-label supervision strategies are introduced to tackle the challenge posed by label imbalances. This approach progressively enhances the model’s recognition accuracy for unlabeled classes during training. Extensive experiments on large multi-source public datasets (FLARE2023 and MOTS) demonstrate that AMOTS surpasses other methods in segmentation accuracy. Our code is available at www.github.com/zzm3zz/AMOTS .},
  archive      = {J_ARTMED},
  author       = {Zengmin Zhang and Yanjun Peng and Xiaomeng Duan},
  doi          = {10.1016/j.artmed.2025.103224},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103224},
  shortjournal = {Artif. Intell. Med.},
  title        = {AMOTS: Partially supervised framework for abdominal multi-organ and tumor segmentation via aspect-aware complementary},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying signatures of image phenotypes to track treatment response in liver disease. <em>ARTMED</em>, <em>168</em>, 103223. (<a href='https://doi.org/10.1016/j.artmed.2025.103223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifiable image patterns associated with disease progression and treatment response are critical tools for guiding individual treatment, and for developing novel therapies. Here, we show that unsupervised machine learning can identify a pattern vocabulary of liver tissue in magnetic resonance images that quantifies treatment response in diffuse liver disease. Deep clustering networks simultaneously encode and cluster patches of medical images into a low-dimensional latent space to establish a tissue vocabulary. The resulting tissue types capture differential tissue change and its location in the liver associated with treatment response. We demonstrate the utility of the vocabulary in a randomized controlled trial cohort of patients with nonalcoholic steatohepatitis. First, we use the vocabulary to compare longitudinal liver change in a placebo and a treatment cohort. Results show that the method identifies specific liver tissue change pathways associated with treatment and enables a better separation between treatment groups than established non-imaging measures. Moreover, we show that the vocabulary can predict biopsy derived features from non-invasive imaging data. We validate the method in a separate replication cohort to demonstrate the applicability of the proposed method.},
  archive      = {J_ARTMED},
  author       = {Matthias Perkonigg and Nina Bastati and Ahmed Ba-Ssalamah and Peter Mesenbrink and Alexander Goehler and Miljen Martic and Xiaofei Zhou and Michael Trauner and Georg Langs},
  doi          = {10.1016/j.artmed.2025.103223},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103223},
  shortjournal = {Artif. Intell. Med.},
  title        = {Identifying signatures of image phenotypes to track treatment response in liver disease},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reporting guideline for chatbot health advice studies: The CHART statement. <em>ARTMED</em>, <em>168</em>, 103222. (<a href='https://doi.org/10.1016/j.artmed.2025.103222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Chatbot Assessment Reporting Tool (CHART) is a reporting guideline developed to provide reporting recommendations for studies evaluating the performance of generative artificial intelligence (AI)-driven chatbots when summarizing clinical evidence and providing health advice, referred to as Chatbot Health Advice (CHA) studies. CHART was developed in several phases after performing a comprehensive systematic review to identify variation in the conduct, reporting and methodology in CHA studies. Findings from the review were used to develop a draft checklist that was revised through an international, multidisciplinary modified asynchronous Delphi consensus process of 531 stakeholders, three synchronous panel consensus meetings of 48 stakeholders, and subsequent pilot testing of the checklist. CHART includes 12 items and 39 subitems to promote transparent and comprehensive reporting of CHA studies. These include Title (subitem 1a), Abstract/Summary (subitem 1b), Background (subitems 2ab), Model Identifiers (subitem 3ab), Model Details (subitems 4abc), Prompt Engineering (subitems 5ab), Query Strategy (subitems 6abcd), Performance Evaluation (subitems 7ab), Sample Size (subitem 8), Data Analysis (subitem 9a), Results (subitems 10abc), Discussion (subitems 11abc), Disclosures (subitem 12a), Funding (subitem 12b), Ethics (subitem 12c), Protocol (subitem 12d), and Data Availability (subitem 12e). The CHART checklist and corresponding methodological diagram were designed to support key stakeholders including clinicians, researchers, editors, peer reviewers, and readers in reporting, understanding, and interpreting the findings of CHA studies.},
  archive      = {J_ARTMED},
  author       = {The CHART Collaborative and Bright Huo and Gary Collins and David Chartash and Arun Thirunavukarasu and Annette Flanagin and Alfonso Iorio and Giovanni Cacciamani and Xi Chen and Nan Liu and Piyush Mathur and An-Wen Chan and Christine Laine and Daniela Pacella and Michael Berkwits and Stavros A. Antoniou and Jennifer C. Camaradou and Carolyn Canfield and Michael Mittelman and Timothy Feeney and Gordon Guyatt},
  doi          = {10.1016/j.artmed.2025.103222},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103222},
  shortjournal = {Artif. Intell. Med.},
  title        = {Reporting guideline for chatbot health advice studies: The CHART statement},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-based methods for diagnosing and grading diabetic retinopathy: A comprehensive review. <em>ARTMED</em>, <em>168</em>, 103221. (<a href='https://doi.org/10.1016/j.artmed.2025.103221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is a leading cause of blindness worldwide, requiring early detection and accurate grading for effective intervention. Advances in artificial intelligence (AI), computer vision, machine learning, and deep learning (DL) have enabled automated detection and classification of DR through various imaging modalities. This review comprehensively evaluates 91 studies employing AI-based methods in the detection and classification of DR using fundus color photography, optical coherence tomography (OCT), OCT-angiography (OCTA), and fundus fluorescein angiography, providing a holistic understanding of their strengths, challenges, and limitations. Additionally, this review compares the characteristics of 23 public datasets for DR. Across modalities, DL approaches generally outperform traditional methods. Among the studies reviewed, 81% utilized fundus images, followed by 9% using OCT, 6% using OCTA, and 2% incorporating multiple modalities. Regarding classification tasks, 62% used AI for multi-way classification, 28% for binary classification, and 10% incorporated both. The paper concludes with future directions, including explainable AI frameworks, multimodal data integration, and suggested protocols to integrate into existing healthcare workflows.},
  archive      = {J_ARTMED},
  author       = {Ibrahim Saleh and Niveen Nasr El-Den and Mohamed Elsharkawy and Ali Mahmoud and Ashraf Sewelam and Wei Wang and Mohammed Ghazal and Ayman El-Baz},
  doi          = {10.1016/j.artmed.2025.103221},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103221},
  shortjournal = {Artif. Intell. Med.},
  title        = {AI-based methods for diagnosing and grading diabetic retinopathy: A comprehensive review},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical radiology report generation: A systematic review of current deep learning methods, trends, and future directions. <em>ARTMED</em>, <em>168</em>, 103220. (<a href='https://doi.org/10.1016/j.artmed.2025.103220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical radiology reports play a crucial role in diagnosing various diseases, yet generating them manually is time-consuming and burdens clinical workflows. Medical radiology report generation aims to automate this process using deep learning to assist radiologists and reduce patient wait times. This study presents the most comprehensive systematic review to date on deep learning-based MRRG, encompassing recent advances that span traditional architectures to large language models. We focus on available datasets, modeling approaches, and evaluation practices. Following PRISMA guidelines, we retrieved 323 articles from major academic databases and included 78 studies after eligibility screening. We critically analyze key components such as model architectures, loss functions, datasets, evaluation metrics, and optimizers — identifying 22 widely used datasets, 14 evaluation metrics, around 20 loss functions, over 25 visual backbones, and more than 30 textual backbones. To support reproducibility and accelerate future research, we also compile links to modern models, toolkits, and pretrained resources. Our findings provide technical insights and outline future directions to address current limitations, promoting collaboration at the intersection of medical imaging, natural language processing, and deep learning to advance trustworthy AI systems in radiology.},
  archive      = {J_ARTMED},
  author       = {Amaan Izhar and Norisma Idris and Nurul Japar},
  doi          = {10.1016/j.artmed.2025.103220},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103220},
  shortjournal = {Artif. Intell. Med.},
  title        = {Medical radiology report generation: A systematic review of current deep learning methods, trends, and future directions},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mapping the brain: AI-driven radiomic approaches to mental disorders. <em>ARTMED</em>, <em>168</em>, 103219. (<a href='https://doi.org/10.1016/j.artmed.2025.103219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain is the most intricate organ, comprising trillions of synaptic connections and governing every thought, feeling, and action. However, abnormalities in its structure or dysfunction in neural connections can often underpin the development of mental disorders. Mental health conditions affect nearly 1 in 8 people globally, creating a significant challenge for healthcare systems to manage. Advances in neuroimaging and artificial intelligence (AI) hold the potential to transform mental health diagnosis and treatment by enabling the timely detection of these disorders. Radiomics, a technique that extracts quantitative features, has emerged as a promising approach for improving diagnostic accuracy and predicting treatment response. This review explores the current status of radiomics-based applications derived from neuroimaging and AI in addressing various mental disorders categorized under the fifth edition of Diagnostic and Statistical Manual of Mental Disorders (DSM). These include bipolar and anxiety disorders, depressive and neurodevelopmental disorders, schizophrenia spectrum and other psychosis, Post-traumatic stress disorder (PTSD) and Internet Gaming Disorder. The findings highlight the critical role of radiomic features and identify the brain regions associated with each disorder, alongside the tools, algorithms, and methodologies used. While the review also discusses limitations and challenges in radiomics research, it underscores the potential of radiomics and AI to identify significant biomarkers for the precise diagnosis of mental health conditions, as well as to enhance precision in treatment response. The potential of this technology could offer new approaches for the diagnosis and personalized treatment of mental disorders, ultimately improving the well-being of millions people worldwide.},
  archive      = {J_ARTMED},
  author       = {Seraphim S. Moumgiakmas and Eleni Vrochidou and George A. Papakostas},
  doi          = {10.1016/j.artmed.2025.103219},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103219},
  shortjournal = {Artif. Intell. Med.},
  title        = {Mapping the brain: AI-driven radiomic approaches to mental disorders},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-based mining of biomedical literature: Applications for drug repurposing for the treatment of dementia. <em>ARTMED</em>, <em>168</em>, 103218. (<a href='https://doi.org/10.1016/j.artmed.2025.103218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative diseases like Alzheimer's, Parkinson's, and HIV-associated neurocognitive disorder severely impact patients and healthcare systems. While effective treatments remain limited, researchers are actively developing ways to slow progression and improve patient outcomes, requiring innovative approaches to handle huge volumes of new scientific data. To enable the automatic analysis of biomedical data we introduced AGATHA, an effective AI-based literature mining tool that can navigate massive scientific literature databases. The overarching goal of this effort is to adapt AGATHA for drug repurposing by revealing hidden connections between FDA-approved medications and a health condition of interest. Our tool converts the abstracts of peer-reviewed papers from PubMed into multidimensional space where each gene and health condition are represented by specific metrics. We implemented advanced statistical analysis to reveal distinct clusters of scientific terms within the virtual space created using AGATHA-calculated parameters for selected health conditions and genes. Partial Least Squares Discriminant Analysis was employed for categorizing and predicting samples (122 diseases and 20,889 genes) fitted to specific classes. Advanced statistics were employed to build a discrimination model and extract lists of genes specific to each disease class. We focused on repurposing drugs for dementia by identifying dementia-associated genes highly ranked in other disease classes. The method was developed for detection of genes that shared across multiple conditions and classified them based on their roles in biological pathways. This led to the selection of six primary drugs for further study.},
  archive      = {J_ARTMED},
  author       = {Aliaksandra Sikirzhytskaya and Ilya Tyagin and S. Scott Sutton and Michael D. Wyatt and Ilya Safro and Michael Shtutman},
  doi          = {10.1016/j.artmed.2025.103218},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103218},
  shortjournal = {Artif. Intell. Med.},
  title        = {AI-based mining of biomedical literature: Applications for drug repurposing for the treatment of dementia},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapting language models for mental health analysis on social media. <em>ARTMED</em>, <em>168</em>, 103217. (<a href='https://doi.org/10.1016/j.artmed.2025.103217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing research interest focused on identifying traces of mental disorders through social media analysis. These disorders significantly impair millions of individuals’ cognitive and behavioral functions worldwide. Our study aims to advance the understanding of four prevalent mental disorders: Anorexia, Depression, Gambling, and Self-harm. We present a comprehensive framework designed for the domain adaptation of models to analyze and identify signs of these conditions on social media posts. The language models’ adapting strategy consisted of three key stages. First, we gathered and enriched substantial data on the four psychological disorders. Second, we adapted the different models to the language used to discuss mental health concerns on social media. Finally, we employed an adapter to fine-tune the models for multiple classification tasks (specific to each mental health condition). The intuitive idea is to adapt a language model smoothly to each domain. Our work includes a comparative study of different language models under in- and cross-domain conditions. This allows us to, for example, assess the ability of a depression-based language model to detect signs of disorders such as anorexia or self-harm. We show that the resulting mental health models perform well in early risk detection tasks. Additionally, we thoroughly analyze the linguistic qualities of these models by testing their predictive abilities using conventional clinical tools, such as specialized questionnaires. We rigorously examine the models across multiple predictive tasks to provide evidence of the adaptation approach’s robustness and effectiveness. Our evaluation results are promising. They demonstrate that our framework enhances classification performance and competes favorably with state-of-the-art models.},
  archive      = {J_ARTMED},
  author       = {Mario Ezra Aragón and Adrián Pastor López-Monroy and Manuel Montes-y-Gómez and David E. Losada},
  doi          = {10.1016/j.artmed.2025.103217},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103217},
  shortjournal = {Artif. Intell. Med.},
  title        = {Adapting language models for mental health analysis on social media},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward fair medical advice: Addressing and mitigating bias in large language model-based healthcare applications. <em>ARTMED</em>, <em>168</em>, 103216. (<a href='https://doi.org/10.1016/j.artmed.2025.103216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are increasingly deployed in web-based medical advice applications, offering scalable and accessible healthcare solutions. However, their outputs often reflect demographic biases, raising concerns about fairness and equity for vulnerable populations. In this work, we propose FairMed , a framework designed to mitigate biases in LLM-generated medical advice through fine-tuning and prompt engineering strategies. We evaluate FairMed using language-based and content-level metrics across demographic groups on publicly available (MedQA), synthetic (Synthea), and private (CBHS) datasets. Experimental results demonstrate consistent improvements over Llama3 - Med42 , as well as over the zero-shot prompting baseline. For instance, in sentiment analysis for gender groups using MedQA, FairMed with Descriptive Prompting reduces the Statistical Parity Difference (SPD) from 0.0902 to 0.0658, improves the Disparate Impact Ratio from 1.1916 to 1.1566, and decreases the Kullback–Leibler Divergence from 0.0045 to 0.0024. Similarly, in directive language evaluation for gender groups using Synthea, SPD improves from 0.1056 to nearly zero, achieving near-perfect parity. On the CBHS dataset, FairMed with Descriptive Prompting increases Diagnostic Recommendation Divergence (DRD) for race groups from 0.9530 to 0.9848, indicating improved group-specific tailoring, while reducing the Action Disparity Index (ADI) from 0.0857 to 0.0469 and Referral Frequency Parity (RFP) from 0.0791 to 0.0511, reflecting enhanced fairness. These findings highlight FairMed’s effectiveness in addressing demographic disparities and promoting equitable healthcare guidance through web technologies. This framework contributes to building trustworthy and inclusive systems for delivering medical advice by ensuring fairness in sensitive applications.},
  archive      = {J_ARTMED},
  author       = {Haohui Lu and Ye Lin and Zhidong Li and Man Lung Yiu and Yu Gao and Shahadat Uddin},
  doi          = {10.1016/j.artmed.2025.103216},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103216},
  shortjournal = {Artif. Intell. Med.},
  title        = {Toward fair medical advice: Addressing and mitigating bias in large language model-based healthcare applications},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-based approaches for automated vocabulary mapping between SIGTAP and OMOP CDM concepts. <em>ARTMED</em>, <em>168</em>, 103204. (<a href='https://doi.org/10.1016/j.artmed.2025.103204'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of global healthcare systems, integrating diverse medical terminologies and classification systems has become a priority due to the adoption of Electronic Health Record (EHR) systems and the imperative for information exchange between healthcare systems. This study addresses the necessity for mapping between the SIGTAP vocabulary used in Brazilian healthcare systems and the broader medical terms of the OMOP CDM terminologies. Two distinct pipelines are evaluated for the vocabulary mapping process, focusing on two subsets of the SIGTAP vocabulary: medicines and medical procedures. The first pipeline utilizes textual embeddings for semantic similarity evaluation, followed by Large Language Models (LLMs) for correspondences selection through a retrieval-augmented generation (RAG) approach. In the second pipeline, LLM agents employ predefined protocols for vocabulary mapping and query refinement. Our results show comparable performance between pipelines in both the Procedures subset (F 1 of 0.684 versus 0.678), and the Medicines subset (F 1 of 0.846 versus 0.839), indicating the viability of the multi-stage filtering approach. The second pipeline demonstrates an advantage over the first in terms of recall, highlighting the efficacy of dynamic query refinement by the agent. These findings provide evidence that LLM-based methods significantly reduce manual effort required by experts, enabling domain specialists to focus on more challenging cases.},
  archive      = {J_ARTMED},
  author       = {Vinícius João de Barros Vanzin and Dilvan de Abreu Moreira and Ricardo Marcondes Marcacini},
  doi          = {10.1016/j.artmed.2025.103204},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103204},
  shortjournal = {Artif. Intell. Med.},
  title        = {LLM-based approaches for automated vocabulary mapping between SIGTAP and OMOP CDM concepts},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized LLMs framework to support public health financing through probabilistic predictions and uncertainty quantification. <em>ARTMED</em>, <em>168</em>, 103203. (<a href='https://doi.org/10.1016/j.artmed.2025.103203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a systemic problem, public health cannot be addressed without considering other policy dimensions. Hence, a holistic approach across public policy areas is necessary to incorporate Health-for-All values into decision-making. However, such multisectoral interventions require public budgets that are effectively mapped into public health outcomes and indicators of their wider determinants. This budget-tagging procedure is high-cost, given that it is often done manually by domain experts. In this paper, we propose Categorical Perplexity-based Uncertainty Quantification (CPUQ), a novel, cost-effective Large Language Models (LLMs) framework that can be leveraged by policymakers to build budget-to-indicator and indicator-to-indicator mappings. This model-agnostic method employs categorical-style prompts to generate interpretable Bernoulli and categorical distributions for edges in a Text-attributed Graph, which is associated with the descriptions of the budget items and indicators. The prompting strategy proposed provides a novel way to incorporate models’ uncertainty within the final outputs, enhancing accuracy and safety, We find that the budget-to-indicator mapping predicted by the framework aligns effectively with expert annotations, while when prompted to infer indicator-to-indicator networks, CPUQ estimates more nuanced relationships compared to alternative LLMs-based methods. Through our work, we hope to provide valuable insights into the strengths and weaknesses of leveraging LLMs to support public health budget planning, with the aim of promoting the implementation of the Health-for-All agenda across diverse governments and institutions.},
  archive      = {J_ARTMED},
  author       = {Daniele Guariso and Rilwan Adewoyin and Gisela Robles Aguilar and Omar A. Guerrero and Alisha Davies},
  doi          = {10.1016/j.artmed.2025.103203},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103203},
  shortjournal = {Artif. Intell. Med.},
  title        = {A generalized LLMs framework to support public health financing through probabilistic predictions and uncertainty quantification},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDintensity: Addressing imbalanced drug-drug interaction risk levels using pre-trained deep learning model embeddings. <em>ARTMED</em>, <em>168</em>, 103202. (<a href='https://doi.org/10.1016/j.artmed.2025.103202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced datasets have been a persistent challenge in bioinformatics, particularly in the context of drug-drug interaction (DDI) risk level datasets. Such imbalance can lead to biased models that perform poorly on underrepresented classes. To address this issue, one strategy is to construct a balanced dataset, while another involves employing more advanced features and models. In this study, we introduce a novel approach called DDintensity, which leverages pre-trained deep learning models as embedding generators combined with LSTM-attention models to address the imbalance in DDI risk level datasets. We tested embeddings from various domains, including images, graphs, and textual corpus. Among these, embeddings generated by BioGPT achieved the highest performance, with an Area Under the Curve (AUC) of 0.97 and an Area Under the Precision-Recall curve (AUPR) of 0.92. Our model was trained on the DDinter and further validated using the MecDDI dataset. Additionally, case studies on chemotherapeutic drugs, DB00398 (Sorafenib) and DB01204 (Mitoxantrone) used in oncology, were conducted to demonstrate the specificity and effectiveness of the this methods. Our approach demonstrates high scalability across DDI modalities, as well as the discovery of novel interactions. In summary, we introduce DDIntensity as a solution for imbalanced datasets in bioinformatics with pre-trained deep-learning embeddings.},
  archive      = {J_ARTMED},
  author       = {Weidun Xie and Xingjian Chen and Lei Huang and Zetian Zheng and Yuchen Wang and Ruoxuan Zhang and Xiao Zhang and Zhichao Liu and Chengbin Peng and Monika Gullerova and Ka-chun Wong},
  doi          = {10.1016/j.artmed.2025.103202},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103202},
  shortjournal = {Artif. Intell. Med.},
  title        = {DDintensity: Addressing imbalanced drug-drug interaction risk levels using pre-trained deep learning model embeddings},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent systems in medicine and health: The role of AI, trevor a. cohen, vimla l. patel, edward h. shortliffe (Eds.) (2022), [eBook], ISBN: 978-3-031-09108-7. <em>ARTMED</em>, <em>168</em>, 103201. (<a href='https://doi.org/10.1016/j.artmed.2025.103201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ARTMED},
  author       = {Fransiskus Serfian Jogo},
  doi          = {10.1016/j.artmed.2025.103201},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103201},
  shortjournal = {Artif. Intell. Med.},
  title        = {Intelligent systems in medicine and health: The role of AI, trevor a. cohen, vimla l. patel, edward h. shortliffe (Eds.) (2022), [eBook], ISBN: 978-3-031-09108-7},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ContraDTI: Improved drug–target interaction prediction via multi-view contrastive learning. <em>ARTMED</em>, <em>168</em>, 103195. (<a href='https://doi.org/10.1016/j.artmed.2025.103195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug–target interaction (DTI) identification is one of the crucial issues in the field of drug discovery. Machine learning approaches offer efficient ways to address this issue, reducing expensive and time-consuming laboratory experiments. However, the scarcity of annotated drug data with labels restricts supervised machine learning applications to DTI prediction. Drawing inspiration from recent advances in contrastive learning, we present ContraDTI—a novel framework that adopts multi-view contrastive learning to overcome data limitations in this paper. Our model considers the molecular graph of a drug as the main view and the SMILES string of a drug as the side view, employing two types of loss functions for the contrast of the main view and the cross-view alignment between the main and the side views. Extensive experiments on both single-target and multi-target DTI datasets demonstrate that ContraDTI enhances the classification performance of DTI prediction, particularly when labeled data is scarce. ContraDTI can be a powerful tool for DTI prediction in data-limited scenarios. The code of this paper is available at https://github.com/zhiruiliao/ContraDTI .},
  archive      = {J_ARTMED},
  author       = {Zhirui Liao and Lei Xie and Shanfeng Zhu},
  doi          = {10.1016/j.artmed.2025.103195},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103195},
  shortjournal = {Artif. Intell. Med.},
  title        = {ContraDTI: Improved drug–target interaction prediction via multi-view contrastive learning},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering the genetic basis of glioblastoma heterogeneity through multimodal analysis of whole slide images and RNA sequencing data. <em>ARTMED</em>, <em>168</em>, 103191. (<a href='https://doi.org/10.1016/j.artmed.2025.103191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glioblastoma is a highly aggressive form of brain cancer characterized by rapid progression and poor prognosis. Despite advances in treatment, the underlying genetic mechanisms driving this aggressiveness remain poorly understood. In this study, we employed multimodal deep learning approaches to investigate glioblastoma heterogeneity using joint image/RNA-seq analysis. Our results reveal novel genes associated with glioblastoma. By leveraging a combination of whole-slide images and RNA-seq, as well as introducing novel methods to encode RNA-seq data, we identified specific genetic profiles that may explain different patterns of glioblastoma progression. These findings provide new insights into the genetic mechanisms underlying glioblastoma heterogeneity and highlight potential targets for therapeutic intervention. Code and data downloading instructions are available at: https://github.com/ma3oun/gbheterogeneity .},
  archive      = {J_ARTMED},
  author       = {Ahmad Berjaoui and Eduardo Hugo Sanchez and Louis Roussel and Elizabeth Cohen-Jonathan Moyal},
  doi          = {10.1016/j.artmed.2025.103191},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103191},
  shortjournal = {Artif. Intell. Med.},
  title        = {Uncovering the genetic basis of glioblastoma heterogeneity through multimodal analysis of whole slide images and RNA sequencing data},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMEA: A hierarchical medical knowledge graph entity alignment model fusing multi-aspect information. <em>ARTMED</em>, <em>168</em>, 103188. (<a href='https://doi.org/10.1016/j.artmed.2025.103188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical entity alignment is crucial for the integration and reasoning of medical knowledge, aiming to match semantically equivalent entities across different medical knowledge graphs. Unlike entities in general knowledge graphs, medical entities contain rich multi-aspect information, which not only includes structural and attribute information but also additional information such as ontology and descriptions. However, existing entity alignment methods overlook these additional pieces of information and lack exploration into the fusion of multi-aspect information. This leads to less-than-ideal performance in medical entity alignment. To address the aforementioned issues, in this paper, we propose a hierarchical medical knowledge graph entity alignment method, termed HMEA, which integrates multi-aspect information. Firstly, we represent the medical knowledge graph as a hierarchical heterogeneous graph to model the multi-aspect information of medical entities. Secondly, we design different representation learning methods according to the characteristics of multi-aspect information to obtain vector representations of entities in different dimensions. Subsequently, we devise a two-stage multi-aspect knowledge fusion mechanism to dynamically integrate multi-aspect information, enabling mutual complementarity. Finally, we utilize the fused entity vector representations to guide entity alignment. We compare our approach with state-of-the-art baseline models on ten different types of publicly available datasets and further conduct ablation and parameter analyses. Experimental results validate the effectiveness and robustness of the proposed model. In benchmark tests across all datasets, HMEA outperforms the current state-of-the-art methods significantly.},
  archive      = {J_ARTMED},
  author       = {Weiguang Wang and Lijuan Ma and Wei Cai and Haiyan Zhao and Xia Zhang},
  doi          = {10.1016/j.artmed.2025.103188},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103188},
  shortjournal = {Artif. Intell. Med.},
  title        = {HMEA: A hierarchical medical knowledge graph entity alignment model fusing multi-aspect information},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for automatic ICD coding: Review, opportunities and challenges. <em>ARTMED</em>, <em>168</em>, 103187. (<a href='https://doi.org/10.1016/j.artmed.2025.103187'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: The automatic International Classification of Diseases (ICD) coding task assigns unique medical codes to diseases in clinical texts for further data statistics, quality control, billing and other tasks. The efficiency and accuracy of medical code assignment is a significant challenge affecting healthcare. However, in clinical practice, Electronic Health Records (EHRs) data are usually complex, heterogeneous, non-standard and unstructured, and the manual coding process is time-consuming, laborious and error-prone. Traditional machine learning methods struggle to extract significant semantic information from clinical texts accurately, but the latest progress in Deep Learning (DL) has shown promising results to address these issues. Objective: This paper comprehensively reviewed recent advancements in utilizing deep learning for automatic ICD coding, which aimed to reveal prominent challenges and emerging development trends by summarizing and analyzing the model’s year, design motivation, deep neural networks, and auxiliary data. Methods: This review introduced systematic literature on automatic ICD coding methods based on deep learning. We screened 5 online databases, including Web of Science, SpringerLink, PubMed, ACM, and IEEE digital library, and collected 53 published articles related to deep learning-based ICD coding from 2017 to 2023. Results: These deep neural network methods aimed to overcome some challenges, such as lengthy and noisy clinical text, high dimensionality and functional relationships of medical codes, and long-tail label distribution. The Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), attention mechanisms, Transformers, Pre-trained Language Models (PLMs), etc, have become popular to address prominent issues in ICD coding. Meanwhile, introducing medical ontology within the ICD coding system (code description and code hierarchy) and external knowledge (Wikipedia articles, tabular data, Clinical Classification Software (CCS), fine-tuning PLMs based on biomedical corpus, entity recognition and concept extraction) has become an emerging trend for automatic ICD coding. Conclusion: This paper provided a comprehensive review of recent literature on applying deep learning technology to improve medical code assignment from a unique perspective. Multiple neural network methods (CNNs, RNNs, Transformers, PLMs, especially attention mechanisms) have been successfully applied in ICD tasks and achieved excellent performance. Various medical auxiliary data has also proven valuable in enhancing model feature representation and classification performance. Our in-depth and systematic analysis suggested that the automatic ICD coding method based on deep learning has a bright future in healthcare. Finally, we discussed some major challenges and outlined future development directions.},
  archive      = {J_ARTMED},
  author       = {Xiaobo Li and Yijia Zhang and Xiaodi Hou and Shilong Wang and Hongfei Lin},
  doi          = {10.1016/j.artmed.2025.103187},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {103187},
  shortjournal = {Artif. Intell. Med.},
  title        = {Deep learning for automatic ICD coding: Review, opportunities and challenges},
  volume       = {168},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discovering multiple antibiotic resistance phenotypes using diverse top-k subgroup list discovery. <em>ARTMED</em>, <em>167</em>, 103200. (<a href='https://doi.org/10.1016/j.artmed.2025.103200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Antibiotic resistance is one of the major global threats to human health and occurs when antibiotics lose their ability to combat bacterial infections. In this problem, a clinical decision support system could use phenotypes in order to alert clinicians of the emergence of patterns of antibiotic resistance in patients. Patient phenotyping is the task of finding a set of patient characteristics related to a specific medical problem such as the one described in this work. However, a single explanation of a medical phenomenon might be useless in the eyes of a clinical expert and be discarded. The discovery of multiple patient phenotypes for the same medical phenomenon would be useful in such cases. Therefore, in this work, we define the problem of mining diverse top-k phenotypes and propose the EDSLM algorithm, which is based on the Subgroup Discovery technique, the subgroup list model, and the Minimum Description Length principle. Our proposal provides clinicians with a method with which to obtain multiple and diverse phenotypes of a set of patients. We show a real use case of phenotyping in antimicrobial resistance using the well-known MIMIC-III dataset.},
  archive      = {J_ARTMED},
  author       = {Antonio Lopez-Martinez-Carrasco and Hugo M. Proença and Jose M. Juarez and Matthijs van Leeuwen and Manuel Campos},
  doi          = {10.1016/j.artmed.2025.103200},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103200},
  shortjournal = {Artif. Intell. Med.},
  title        = {Discovering multiple antibiotic resistance phenotypes using diverse top-k subgroup list discovery},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online continuous learning of users suicidal risk on social media. <em>ARTMED</em>, <em>167</em>, 103199. (<a href='https://doi.org/10.1016/j.artmed.2025.103199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suicide is a tragedy for family and society. With social media becoming an integral part of people’s life nowadays, assessing suicidal risk based on one’s social media behavior has drawn increasing research attentions. The majority of the works trained a machine learning model to classify user’s suicidal risk severity level in a batch learning setting on the entire training data. This is not a timely and scalable solution in the context of social media where new data arrives sequentially in a stream form. In this study, we formulate and address the continuous suicidal risk assessment problem through a three-layered joint memory network, consisting of a short-term personal memory and long-term personal and global memories. Unlike existing methods that rely on static classification, our model supports real-time, continuous learning from users’ emotional and behavioral dynamics without the need for full retraining. This allows for personalized and adaptive risk tracking over time. We also present a way to continuously capture users’ personal features and integrate them in suicidal risk assessment. The performance on the constructed dataset containing 95 suicidal and 95 non-suicidal social media users shows that 96% of accuracy can be achieved with the proposed method.},
  archive      = {J_ARTMED},
  author       = {Lei Cao and Ling Feng and Yang Ding and Huijun Zhang and Xin Wang and Kaisheng Zeng and Yi Dai},
  doi          = {10.1016/j.artmed.2025.103199},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103199},
  shortjournal = {Artif. Intell. Med.},
  title        = {Online continuous learning of users suicidal risk on social media},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VAE-GANMDA: A microbe-drug association prediction model integrating variational autoencoders and generative adversarial networks. <em>ARTMED</em>, <em>167</em>, 103198. (<a href='https://doi.org/10.1016/j.artmed.2025.103198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional biological experimental methods typically require weeks or even months of experimentation, and the cost of each experiment can reach hundreds or even thousands of dollars, which is quite expensive and time-consuming. To address this, a model called VAE-GANMDA, which integrates variational autoencoders (VAE) and generative adversarial networks (GAN) for predicting microbe-drug associations, has been proposed. Firstly, a heterogeneous network of microbes and drugs is established to enrich the association information. Secondly, by fusing VAE and GAN, the model learns the manifold distribution of data through association features, obtaining nonlinear manifold features. Furthermore, the VAE generation module is improved by integrating the Convolutional Block Attention Module (CBAM) and Gaussian kernel function, enhancing the smooth perception of manifold features, thus endowing VAE with stronger feature extraction capabilities. Then, singular value decomposition (SVD) technique is employed to extract linear features of the data. Finally, by combining linear and nonlinear features, the k-means++ algorithm is used to select balanced and high-quality negative samples for training the MLP classifier. Through performance evaluation, the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC) of VAE-GANMDA reach 0.9724 and 0.9635 respectively, outperforming classical machine learning methods and the majority of deep learning methods. Case studies demonstrate that VAE-GANMDA accurately predicts candidate drugs related to SARS-CoV-2 and candidate microbes related to ciprofloxacin.},
  archive      = {J_ARTMED},
  author       = {Bo Wang and Yang He and Xiaoxin Du and Lei Zhu and Junqi Wang and Tongxuan Wang},
  doi          = {10.1016/j.artmed.2025.103198},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103198},
  shortjournal = {Artif. Intell. Med.},
  title        = {VAE-GANMDA: A microbe-drug association prediction model integrating variational autoencoders and generative adversarial networks},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RobustEMD: Domain robust matching for cross-domain few-shot medical image segmentation. <em>ARTMED</em>, <em>167</em>, 103197. (<a href='https://doi.org/10.1016/j.artmed.2025.103197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot medical image segmentation (FSMIS) aims to perform the limited annotated data learning in the medical image analysis scope. Despite the progress has been achieved, current FSMIS models are all trained and deployed on the same data domain, as is not consistent with the clinical reality that medical imaging data is always across different data domains (e.g. imaging modalities, institutions and equipment sequences). In this paper, we introduce Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) and propose a RobustEMD matching mechanism based on Earth Mover’s Distance (EMD) to enhance cross-domain generalization. Our approach includes three key components: (1) a channel-wise feature decomposition strategy that uniformly divides support and query features into local nodes, (2) a texture structure aware weights generation method that restrains domain-relevant nodes through Sobel-based gradient calculation, and (3) a boundary-aware Hausdorff distance measurement for transportation cost calculation. Extensive experiments across three scenarios (cross-modal, cross-sequence and cross-institution) show that our method significantly outperforms existing approaches. And ablation studies further confirm that each component of our RobustEMD mechanism contributes to the enhanced performance. The experimental outcomes highlight strong generalization capabilities of our model in real-world heterogeneous medical imaging environments. Code is available at https://github.com/YazhouZhu19/RobustEMD .},
  archive      = {J_ARTMED},
  author       = {Yazhou Zhu and Minxian Li and Qiaolin Ye and Shidong Wang and Tong Xin and Haofeng Zhang},
  doi          = {10.1016/j.artmed.2025.103197},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103197},
  shortjournal = {Artif. Intell. Med.},
  title        = {RobustEMD: Domain robust matching for cross-domain few-shot medical image segmentation},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GraphCF: Drug–target interaction prediction via multi-feature fusion with contrastive graph neural network. <em>ARTMED</em>, <em>167</em>, 103196. (<a href='https://doi.org/10.1016/j.artmed.2025.103196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug–target interaction (DTI) is paramount in drug discovery and repurposing, which involves screening for effective candidate drugs by targeting specific proteins. Existing methods often focus on one or two representations of drugs or targets, and little has been explored regarding 3D structures. Moreover, how to capture interactions between multi-modal features comprehensively is also a key issue. A multi-modal interaction fusion method called GraphCF is proposed to overcome these limitations. Specifically, GraphCF uses a MixHop aggregator to gather higher-order neighborhood information between nodes in the DTI topological network and incorporate graph contrastive learning to capture more discriminative 2D representations of drugs and targets. Additionally, GraphCF utilizes convolutional neural networks and graph neural networks to extract the sequence and 3D structural features of drugs and targets, respectively. Then, GraphCF employs a cross-attention-based multi-feature fusion module to facilitate information interaction and fusion among multi-modal feature representations. GraphCF is evaluated and compared with some advanced methods on four public datasets, and the results demonstrate the competitive performance of GraphCF in DTI prediction.},
  archive      = {J_ARTMED},
  author       = {Dianlei Gao and Fei Zhu},
  doi          = {10.1016/j.artmed.2025.103196},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103196},
  shortjournal = {Artif. Intell. Med.},
  title        = {GraphCF: Drug–target interaction prediction via multi-feature fusion with contrastive graph neural network},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Your turn: At home turning angle estimation for parkinson’s disease severity assessment. <em>ARTMED</em>, <em>167</em>, 103194. (<a href='https://doi.org/10.1016/j.artmed.2025.103194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People with Parkinson’s Disease (PD) often experience progressively worsening gait, including changes in how they turn around, as the disease progresses. Existing clinical rating tools are not capable of capturing hour-by-hour variations of PD symptoms, as they are confined to brief assessments within clinic settings, leaving gait performance outside these controlled environments unaccounted for. Measuring turning angles continuously and passively is a component step towards using gait characteristics as sensitive indicators of disease progression in PD. This paper presents a deep learning-based approach to automatically quantify turning angles by extracting 3D skeletons from videos and calculating the rotation of hip and knee joints. We utilise advanced human pose estimation models, Fastpose and Strided Transformer, on a total of 1386 turning video clips from 24 subjects (12 people with PD and 12 healthy control volunteers), trimmed from a PD dataset of unscripted free-living videos in a home-like setting (Turn-REMAP). We also curate a turning video dataset, Turn-H3.6M, from the public Human3.6M human pose benchmark with 3D groundtruth, to further validate our method. Previous gait research has primarily taken place in clinics or laboratories evaluating scripted gait outcomes, but this work focuses on free-living home settings where complexities exist, such as baggy clothing and poor lighting. Due to difficulties in obtaining accurate groundtruth data in a free-living setting, we quantise the angle into the nearest bin 45° based on the manual labelling of expert clinicians. Our method achieves a turning calculation accuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7°, and a weighted precision (WPrec) of 68.3% for Turn-REMAP. On Turn-H3.6M, it achieves an accuracy of 73.5%, an MAE of 18.5°, and a WPrec of 86.2%. This is the first work to explore the use of single monocular camera data to quantify turns by PD patients in a home setting. All data and models are publicly available, providing a baseline for turning parameter measurement to promote future PD gait research.},
  archive      = {J_ARTMED},
  author       = {Qiushuo Cheng and Catherine Morgan and Arindam Sikdar and Alessandro Masullo and Alan Whone and Majid Mirmehdi},
  doi          = {10.1016/j.artmed.2025.103194},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103194},
  shortjournal = {Artif. Intell. Med.},
  title        = {Your turn: At home turning angle estimation for parkinson’s disease severity assessment},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Position of artificial intelligence in healthcare and future perspective. <em>ARTMED</em>, <em>167</em>, 103193. (<a href='https://doi.org/10.1016/j.artmed.2025.103193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) has been used in healthcare with increasing momentum. According to a published report, 6.6 billion dollars were invested in AI healthcare in 2021, and this investment is expected to provide 150 billion dollars of benefit to the USA economy by 2026 (Duchateau and King, 2023 [1]). The future perspective on AI will undoubtedly open new horizons for the healthcare. AI technology in the healthcare field is increasingly popular in the areas of diagnosis, prognosis, classification, therapy, and disease survival prediction. Now that AI has proven its worth, it's already time to re-ask the following three questions according to the fast pace of AI algorithms: 1) Where will AI be positioned in healthcare in the future? 2) What kind of relationship will be defined between doctors, patients and AI? 3) How can we direct AI studies according to the health problems in the world?},
  archive      = {J_ARTMED},
  author       = {Vedat Cicek and Ulas Bagci},
  doi          = {10.1016/j.artmed.2025.103193},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103193},
  shortjournal = {Artif. Intell. Med.},
  title        = {Position of artificial intelligence in healthcare and future perspective},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward responsible artificial intelligence in medicine: Reflections from the australian epilepsy project. <em>ARTMED</em>, <em>167</em>, 103192. (<a href='https://doi.org/10.1016/j.artmed.2025.103192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is a multidisciplinary scientific field that uses machines to solve real-world problems and predict outcomes. Despite the current enthusiasm about AI's potential as a clinical support tool, there is also a growing awareness and concern about the potentially harmful effects of AI. Because AI will likely impact expert-based decision-making in medicine, it is critical to consider the issues that AI raises in medical research. This paper outlines the AI guidelines of the Australian Epilepsy Project. This large-scale platform aims to democratise specialist care in epilepsy and use AI for clinical decision support based on prospective multimodal datasets (MRI, genetic, clinical, and cognitive data) from thousands of people with epilepsy. As AI develops rapidly, we focus on key areas of medical AI identified in the literature, including Trust, Responsibility and Safety. We believe AI is changing medicine, and we believe it is imperative to advance and update our AI guidelines adaptably while preparing for an era of augmented-intelligence-based medicine.},
  archive      = {J_ARTMED},
  author       = {Mangor Pedersen and Heath R. Pardoe and Anton de Weger and Donna Hutchison and David F. Abbott and Karin Verspoor and Graeme D. Jackson and for the Australian Epilepsy Project Investigators},
  doi          = {10.1016/j.artmed.2025.103192},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103192},
  shortjournal = {Artif. Intell. Med.},
  title        = {Toward responsible artificial intelligence in medicine: Reflections from the australian epilepsy project},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lesion boundary detection for skin lesion segmentation based on boundary sensing and CNN-transformer fusion networks. <em>ARTMED</em>, <em>167</em>, 103190. (<a href='https://doi.org/10.1016/j.artmed.2025.103190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional convolutional neural networks often struggle to capture global information and handle ambiguous boundaries during complex skin lesion segmentation tasks. To tackle this challenge, we proposed MPBA-Net, a hybrid network that integrates multi-pooling fusion and boundary-aware refinement. The network integrated Convolutional Neural Network (CNN) and Transformer to generate rich skin lesion feature maps for comprehensive feature extraction. Specifically, we introduced a boundary-aware attention gate (BAAG) module in the Transformer encoder layer and added a boundary cross attention (BCA) module at the end of the network to capture critical skin lesion boundary features. Additionally, we developed a multi-pooling fusion (MPF) module that extracts global multi-scale features by fusing improved Spatial Pyramid (SP) and Atrous Spatial Pyramid Pooling (ASPP). To optimize training, we designed a Point Loss derived from Binary Cross-Entropy (BCE) and combined it with Dice Loss to form a hybrid loss function. This approach not only enhances classification performance but also provides more precise measurement of the similarity between segmentation results and ground truth annotations. Ablation experiments on the ISIC2018 dataset validated the effectiveness of our fusion strategies and network improvements. Comparative experiments on the ISIC2016, ISIC2017, and ISIC2018 datasets showed that the Dice index of MPBA-Net outperformed other comparative segmentation methods in all three datasets, achieving 91.47 %, 87.04 %, and 88.93 %, respectively. Quantitative and qualitative results demonstrate that our method improves skin lesion segmentation accuracy, aiding dermatologists in clinical diagnosis and treatment. Our code is available at https://github.com/FengYuchenGuang/MPBA-Net .},
  archive      = {J_ARTMED},
  author       = {Xuzhen Huang and Yuliang Ma and Xiajin Mei and Zizhuo Wu and Mingxu Sun and Qingshan She},
  doi          = {10.1016/j.artmed.2025.103190},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103190},
  shortjournal = {Artif. Intell. Med.},
  title        = {Lesion boundary detection for skin lesion segmentation based on boundary sensing and CNN-transformer fusion networks},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An anxiety screening framework integrating multimodal data and graph node correlation. <em>ARTMED</em>, <em>167</em>, 103189. (<a href='https://doi.org/10.1016/j.artmed.2025.103189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anxiety disorders are a significant global health concern, profoundly impacting patients’ lives and social functioning while imposing considerable burdens on families and economies. However, current anxiety screening methods face limitations due to cost constraints and cognitive biases, particularly in their inability to deeply model correlations among multidimensional features. They often overlook crucial information inherent in their internal couplings, limiting their accuracy and applicability in clinical diagnostics. To address these challenges, we propose an advanced anxiety screening framework that integrates multimodal data, such as physiological, behavioral, audio, and textual, using a Graph Convolutional Network (GCN). While our framework draws upon existing technologies such as GCN, one-dimensional convolutional neural networks, and gated recurrent units, the uniqueness of our framework lies in how these components are combined to capture complex spatiotemporal relationships and correlations among multimodal features. Experimental results demonstrate the framework’s robust performance, achieving an accuracy of 93.48%, Area Under Curve of 94.58%, precision of 90.00%, sensitivity of 81.82%, specificity of 97.14%, F1 score of 85.71%. Notably, the method remains effective even when questionnaire data is unavailable, underscoring its practicality and reliability. This anxiety screening approach provides a new perspective for early identification and intervention of anxiety symptoms, offering a scientific basis for personalized treatment and prevention through the analysis of multimodal data and graph structures.},
  archive      = {J_ARTMED},
  author       = {Haimiao Mo and Hongjia Wu and Qian Rong and Zhijian Hu and Meng Yi and Peipei Chen},
  doi          = {10.1016/j.artmed.2025.103189},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103189},
  shortjournal = {Artif. Intell. Med.},
  title        = {An anxiety screening framework integrating multimodal data and graph node correlation},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining structural equation modeling analysis with machine learning for early malignancy detection in bethesda category III thyroid nodules. <em>ARTMED</em>, <em>167</em>, 103186. (<a href='https://doi.org/10.1016/j.artmed.2025.103186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atypia of Undetermined Significance (AUS), classified as Category III in the Bethesda Thyroid Cytopathology Reporting System, presents significant diagnostic challenges for clinicians. This study aims to develop a clinical decision support system that integrates structural equation modeling (SEM) and machine learning to predict malignancy in AUS thyroid nodules. The model integrates preoperative clinical data, ultrasonography (USG) findings, and cytopathological and morphometric variables. This retrospective cohort study was conducted between 2011 and 2019 at Karadeniz Technical University (KTU) Farabi Hospital. The dataset included 56 variables derived from 204 thyroid nodules diagnosed via ultrasound-guided fine-needle aspiration biopsy (FNAB) in 183 patients over 18 years. Logistic regression (LR) and SEM were used to identify risk factors for early thyroid cancer detection. Subsequently, machine learning algorithms—including Support Vector Machines (SVM), Naive Bayes (NB), and Decision Trees (DT) were used to construct decision support models. After feature selection with SEM, the SVM model achieved the highest performance, with an accuracy of 82 %, a specificity of 97 %, and an AUC value of 84 %. Additional models were developed for different scenarios, and their performance metrics were compared. Accurate preoperative prediction of malignancy in thyroid nodules is crucial for avoiding unnecessary surgeries. The proposed model supports more informed clinical decision-making by effectively identifying benign cases, thereby reducing surgical risk and improving patient care.},
  archive      = {J_ARTMED},
  author       = {Zeliha Aydın Kasap and Burçin Kurt and Ali Güner and Elif Özsağır and Mustafa Emre Ercin},
  doi          = {10.1016/j.artmed.2025.103186},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103186},
  shortjournal = {Artif. Intell. Med.},
  title        = {Combining structural equation modeling analysis with machine learning for early malignancy detection in bethesda category III thyroid nodules},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting drug-drug interactions: A deep learning approach with GCN-based collaborative filtering. <em>ARTMED</em>, <em>167</em>, 103185. (<a href='https://doi.org/10.1016/j.artmed.2025.103185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of combination drugs among patients is increasing due to effectiveness compared to monotherapies. However, healthcare providers should continue to be concerned about the potential risks associated with patient safety arising from drug-drug interactions (DDIs) when they use combination drugs. Whereas direct physicochemical interactions contribute to certain cases of DDIs, the majority of DDIs occur because one drug modulates enzymes, such as cytochrome P450, responsible for metabolizing another drug. Therefore, drugs that interact with the same family drugs are more likely to interact with each other by mediating specific enzymes. Adapted from techniques used to recommend users with similar interests, we introduce an AI recommendation model with graph convolutional network (GCN) and collaborative filtering that analyzes the connectivity of interacting drugs rather than their chemical structures. This approach deviates from typical classification models by not requiring sampling of undefined interactions as negative samples, allowing the prediction of potential interactions for all unknown drug pairs, circumventing the challenges associated with selecting negative interactions and data imbalance. Our methodology used the DrugBank database (version 5.1.9 released on January 3, 2022), encompassing 4,072 drugs and 1,391,790 drug pairs with interactions. Furthermore, the robustness of the model was verified through a 5-fold validation and external data validation using TWOSIDES data. Notably, our model’s efficacy is established solely through the exploitation of DDI reports, offering a versatile framework capable of accurately predicting interactions among diverse drug types. The source code for this project is distributed on GitHub ( https://github.com/yeonuk-Jeong/DDI-OCF ).},
  archive      = {J_ARTMED},
  author       = {Yeon Uk Jeong and Jeongwhan Choi and Noseong Park and Jae Yong Ryu and Yi Rang Kim},
  doi          = {10.1016/j.artmed.2025.103185},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103185},
  shortjournal = {Artif. Intell. Med.},
  title        = {Predicting drug-drug interactions: A deep learning approach with GCN-based collaborative filtering},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A prior knowledge-supervised fusion network predicts survival after radiotherapy in patients with advanced gastric cancer. <em>ARTMED</em>, <em>167</em>, 103184. (<a href='https://doi.org/10.1016/j.artmed.2025.103184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and objective: Predicting overall survival (OS) for advanced gastric cancer patients after radiotherapy is critical for developing an individualized treatment plan. However, existing studies have focused on gastric cancer CT images with a large amount of redundant information, neglecting the role of physicians’ prior knowledge in guiding gastric cancer CT image information. We propose a multimodal fusion method based on prior knowledge to predict OS after radiotherapy in advanced gastric cancer patients to assist physicians in clinical diagnosis and treatment. Methods: A prior knowledge supervised fusion network (PKSFnet) is proposed. Firstly, PKSFnet uses a novel sampling strategy, which enables the input model data to obtain a complete feature space by analyzing the entire patient data space. Afterwards, under the guidance of the multi-domain feature fusion module (MdFF), multimodal information of patients is adaptively fused and mined to improve the prediction performance. Results: The results of the proposed model are superior to those of other unimodal and multimodal state-of-the-art methods. For the segmented survival time classification task, the AUC, specificity, sensitivity, precision of the proposed model are 0.8397, 0.875, 0.7556, and 0.875, respectively. For the survival risk regression task, the C-index and HR of the proposed model are 0.8574 and 4.658 respectively. Ablation experimental results further demonstrate the impact of each module of the proposed model. Finally, we apply the novel sampling strategy to other deep learning models and achieve significant improvement. Conclusion: The experimental results have demonstrated that the proposed model can effectively predict OS after radiotherapy in patients with advanced gastric cancer, which demonstrate that the proposed model can facilitate the development and application of robust clinical treatment strategies.},
  archive      = {J_ARTMED},
  author       = {Liang Sun and Yongxin Lan and Jian Sun and Pengfei Ji and Hongwei Ge and Ming Cui and Xin Yuan},
  doi          = {10.1016/j.artmed.2025.103184},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103184},
  shortjournal = {Artif. Intell. Med.},
  title        = {A prior knowledge-supervised fusion network predicts survival after radiotherapy in patients with advanced gastric cancer},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interactive prototype learning and self-learning for few-shot medical image segmentation. <em>ARTMED</em>, <em>167</em>, 103183. (<a href='https://doi.org/10.1016/j.artmed.2025.103183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning alleviates the heavy dependence of medical image segmentation on large-scale labeled data, but it shows strong performance gaps when dealing with new tasks compared with traditional deep learning. Existing methods mainly learn the class knowledge of a few known (support) samples and extend it to unknown (query) samples. However, the large distribution differences between the support image and the query image lead to serious deviations in the transfer of class knowledge, which can be specifically summarized as two segmentation challenges: Intra-class inconsistency and Inter-class similarity, blurred and confused boundaries. In this paper, we propose a new interactive prototype learning and self-learning network to solve the above challenges. First, we propose a deep encoding-decoding module to learn the high-level features of the support and query images to build peak prototypes with the greatest semantic information and provide semantic guidance for segmentation. Then, we propose an interactive prototype learning module to improve intra-class feature consistency and reduce inter-class feature similarity by conducting mid-level features-based mean prototype interaction and high-level features-based peak prototype interaction. Last, we propose a query features-guided self-learning module to separate foreground and background at the feature level and combine low-level feature maps to complement boundary information. Our model achieves competitive segmentation performance on benchmark datasets and shows substantial improvement in generalization ability.},
  archive      = {J_ARTMED},
  author       = {Yuhui Song and Chenchu Xu and Boyan Wang and Xiuquan Du and Jie Chen and Yanping Zhang and Shuo Li},
  doi          = {10.1016/j.artmed.2025.103183},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103183},
  shortjournal = {Artif. Intell. Med.},
  title        = {Interactive prototype learning and self-learning for few-shot medical image segmentation},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated analysis of differential intra-chromosomal community interactions: A study of breast cancer. <em>ARTMED</em>, <em>167</em>, 103180. (<a href='https://doi.org/10.1016/j.artmed.2025.103180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to analyze the dynamics of intra-chromosomal interactions when considering multiple high-dimensional epigenetic datasets. A computational approach, differential network analysis in intra-chromosomal community interaction (DNAICI), was proposed here to elucidate these dynamics by integrating Hi-C data with other epigenetic data. DNAICI utilized a novel hyperparameter tuning method, for optimizing the network clustering, to identify valid intra-chromosomal community interactions at different resolutions. The approach was first trained on Hi-C data and other epigenetic data in an untreated and one hour estrogen (E2)-treated breast cancer cell line, MCF7, and uncovered two major types of valid intra-chromosomal community interactions (active/repressive) that resembles the properties of A/B compartments (or open/closed chromatin domains). It was further tested on the breast cancer cell line MCF7 and its corresponding tamoxifen-resistant (TR) derivative, MCF7TR, and identified 515 differentially interacting and expressed genes (DIEGs) within intra-chromosomal community interactions. In silico analysis of these DIEGs revealed that endocrine resistance is among the top biological pathways, suggesting an interacting/looping-mediated mechanism in regulating breast cancer tamoxifen resistance. This novel integrated network analysis approach offers a broad application in diverse biological systems for identifying a biological-context-specific differential community interaction.},
  archive      = {J_ARTMED},
  author       = {Zhihao Yao and Kun Fang and Gege Liu and Magnar Bjørås and Victor X. Jin and Junbai Wang},
  doi          = {10.1016/j.artmed.2025.103180},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103180},
  shortjournal = {Artif. Intell. Med.},
  title        = {Integrated analysis of differential intra-chromosomal community interactions: A study of breast cancer},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label-independent framework for objective evaluation of cosmetic outcome in breast cancer. <em>ARTMED</em>, <em>167</em>, 103179. (<a href='https://doi.org/10.1016/j.artmed.2025.103179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advancements in the field of breast cancer treatment, the assessment of postsurgical cosmetic outcomes has gained increasing significance owing to its substantial impact on patients’ quality of life. However, evaluating breast cosmesis is challenging because of the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, attention-guided denoising diffusion anomaly detection (AG-DDAD), designed to assess breast cosmesis following surgery. The model addresses the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of distillation with no labels and a self-supervised vision transformer, combined with a diffusion model, to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data, predominantly with normal cosmesis, we adopted an unsupervised anomaly detection perspective to automatically score the cosmesis. Real-world data experiments demonstrated the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared with commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers an objective evaluation. Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in terms of accuracy. Beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations.},
  archive      = {J_ARTMED},
  author       = {Sangjoon Park and Yong Bae Kim and Jee Suk Chang and Seo Hee Choi and Hyungjin Chung and Ik Jae Lee and Hwa Kyung Byun},
  doi          = {10.1016/j.artmed.2025.103179},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103179},
  shortjournal = {Artif. Intell. Med.},
  title        = {Label-independent framework for objective evaluation of cosmetic outcome in breast cancer},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic emotion and sentiment modelling of patient-reported experiences. <em>ARTMED</em>, <em>167</em>, 103178. (<a href='https://doi.org/10.1016/j.artmed.2025.103178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient feedback is necessary to assess the extent to which healthcare delivery aligns with public needs and expectations. Surveys provide structured feedback that is readily analysed; however, they are costly, infrequent, and constrained by predefined questions, limiting a comprehensive understanding of patient experience. In contrast, the unstructured nature of online reviews and social-media posts can reveal unique insights into patient perspectives, yet that very lack of structure presents a challenge for analysis. In this study, we present a methodology for interpretable probabilistic modelling of patient emotions from patient-reported experiences. We employ metadata-network topic modelling to uncover key themes in 13,380 patient-reported experiences from Care Opinion (2012-2022) and reveal insightful relationships between these themes and labelled emotions. Our results show positivity and negativity relate most strongly to aspects of patient experience, such as patient-caregiver interactions, rather than clinical outcomes. Patient educational engagement exhibits strong positivity, whereas dismissal and rejection are linked to suicidality and depression. We develop a context-specific probabilistic emotion recommender system that predicts both multi-label emotions and binary sentiments with a Naïve Bayes classifier using topics as predictors. We assess performance with nDCG and Q-measure and achieve an F1 of 0.921, significantly outperforming standard sentiment lexicons. This methodology offers a cost-effective, timely, and transparent approach to harness unconstrained patient-reported feedback, with the potential to augment traditional patient-reported experience collection. Our R package and interactive dashboard make the approach readily accessible for future research and clinical practice applications, enabling hospitals to integrate emotional insights into surveys and tailor care to patient needs. Overall, this study provides a new avenue for understanding and improving patient experience and the quality of healthcare delivery.},
  archive      = {J_ARTMED},
  author       = {Curtis Murray and Lewis Mitchell and Jonathan Tuke and Mark Mackay},
  doi          = {10.1016/j.artmed.2025.103178},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103178},
  shortjournal = {Artif. Intell. Med.},
  title        = {Probabilistic emotion and sentiment modelling of patient-reported experiences},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PhenoLinker: Phenotype-gene link prediction and explanation using heterogeneous graph neural networks. <em>ARTMED</em>, <em>167</em>, 103177. (<a href='https://doi.org/10.1016/j.artmed.2025.103177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The association of a given human phenotype with a genetic variant remains a critical challenge in biomedical research. We present PhenoLinker, a novel graph-based system capable of associating a score to a phenotype-gene relationship by using heterogeneous information networks and a convolutional neural network-based model for graphs, which can provide an explanation for the predictions. Unlike previous approaches, PhenoLinker integrates gene and phenotype attributes, while maintaining explainability through Integrated Gradients. PhenoLinker consistently outperforms existing models in both retrospective and temporal validation tasks. This system can aid in the discovery of new associations and in understanding the consequences of human genetic variation.},
  archive      = {J_ARTMED},
  author       = {Jose L. Mellina Andreu and Luis Bernal and Antonio F. Skarmeta and Mina Ryten and Sara Álvarez and Alejandro Cisterna García and Juan A. Botía},
  doi          = {10.1016/j.artmed.2025.103177},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103177},
  shortjournal = {Artif. Intell. Med.},
  title        = {PhenoLinker: Phenotype-gene link prediction and explanation using heterogeneous graph neural networks},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From black box to clarity: Strategies for effective AI informed consent in healthcare. <em>ARTMED</em>, <em>167</em>, 103169. (<a href='https://doi.org/10.1016/j.artmed.2025.103169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Informed consent is fundamental to ethical medical practice, ensuring that patients understand the procedures they undergo, the associated risks, and available alternatives. The advent of artificial intelligence (AI) in healthcare, particularly in diagnostics, introduces complexities that traditional informed consent forms do not adequately address. AI technologies, such as image analysis and decision-support systems, offer significant benefits but also raise ethical, legal, and practical concerns regarding patient information and autonomy. Main body The integration of AI in healthcare diagnostics necessitates a re-evaluation of current informed consent practices to ensure that patients are fully aware of AI's role, capabilities, and limitations in their care. Existing standards, such as those in the UK's National Health Service and the US, highlight the need for transparency and patient understanding but often fall short when applied to AI. The “black box” phenomenon, where the inner workings of AI systems are not transparent, poses a significant challenge. This lack of transparency can lead to over-reliance or distrust in AI tools by clinicians and patients alike. Additionally, the current informed consent process often fails to provide detailed explanations about AI algorithms, the data they use, and inherent biases. There is also a notable gap in the training and education of healthcare professionals on AI technologies, which impacts their ability to communicate effectively with patients. Ethical and legal considerations, including data privacy and algorithmic fairness, are frequently inadequately addressed in consent forms. Furthermore, integrating AI into clinical workflows presents practical challenges that require careful planning and robust support systems. Conclusion This review proposes strategies for redesigning informed consent forms. These include using plain language, visual aids, and personalised information to improve patient understanding and trust. Implementing continuous monitoring and feedback mechanisms can ensure the ongoing effectiveness of these forms. Future research should focus on developing comprehensive regulatory frameworks and enhancing communication techniques to convey complex AI concepts to patients. By improving informed consent practices, we can uphold ethical standards, foster patient trust, and support the responsible integration of AI in healthcare, ultimately benefiting both patients and healthcare providers.},
  archive      = {J_ARTMED},
  author       = {M. Chau and M.G. Rahman and T. Debnath},
  doi          = {10.1016/j.artmed.2025.103169},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103169},
  shortjournal = {Artif. Intell. Med.},
  title        = {From black box to clarity: Strategies for effective AI informed consent in healthcare},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating patient harm risks: A proposal of requirements for AI in healthcare. <em>ARTMED</em>, <em>167</em>, 103168. (<a href='https://doi.org/10.1016/j.artmed.2025.103168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise Artificial Intelligence (AI), mitigation strategies may be needed to integrate AI-enabled medical software responsibly, ensuring ethical alignment and patient safety. This study examines how to mitigate the key risks identified by the European Parliamentary Research Service (EPRS). For that, we discuss how complementary risk-mitigation requirements may ensure the main aspects of AI in Healthcare: Reliability - Continuous performance evaluation, Continuous usability test, Encryption and use of field-tested libraries, Semantic interoperability -, Transparency - AI passport, eXplainable AI, Data quality assessment, Bias Check -, Traceability - User management, Audit trail, Review of cases -, and Responsibility - Regulation check, Academic use only disclaimer, Clinicians double check -. A survey conducted among 216 Medical ICT professionals (medical doctors, ICT staff and complementary profiles) between March and June 2024 revealed these requirements were perceived positive by all profiles. Responders deemed explainable AI and data quality assessment essential for transparency; audit trail for traceability; and regulatory compliance and clinician double check for responsibility. Clinicians rated the following requirements more relevant ( p < 0.05) than technicians: continuous performance assessment, usability testing, encryption, AI passport, retrospective case review, and academic use check. Additionally, users found the AI passport more relevant for transparency than decision-makers ( p < 0.05). We trust that this proposal can serve as a starting point to endow the future AI systems in medical practice with requirements to ensure their ethical deployment.},
  archive      = {J_ARTMED},
  author       = {Juan M. Garcia-Gomez and Vicent Blanes-Selva and Celia Alvarez Romero and José Carlos de Bartolomé Cenzano and Felipe Pereira Mesquita and Alejandro Pazos and Ascensión Doñate-Martínez},
  doi          = {10.1016/j.artmed.2025.103168},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103168},
  shortjournal = {Artif. Intell. Med.},
  title        = {Mitigating patient harm risks: A proposal of requirements for AI in healthcare},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing colorectal polyp segmentation with TCFMA-net: A transformer-based cross feature and multi-attention network. <em>ARTMED</em>, <em>167</em>, 103167. (<a href='https://doi.org/10.1016/j.artmed.2025.103167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance polyp segmentation in colonoscopy images for early detection and diagnosis of colorectal cancer. The study proposed the Transformer-based cross feature multi-attention network (TCFMA-Net) for polyp segmentation by addressing challenges such as varying polyp sizes and the problem of accurate boundaries. TCFMA-Net utilizes swin transformer-based encoders, a cross-feature enhancer network with multiple cross-feature enhancer blocks, and multi-attention modules integrated within and outside the decoder blocks. This enables comprehensive cross-feature fusion, preserving image clarity and facilitating the flow of information, allowing efficient processing of both low-level and high-level features. TCFMA-Net effectively captures the complexities of polyp size variations and boundaries issues and consistently outperforms existing methods on six benchmark datasets with confidence interval (CI), achieving a Dice score of 92.74 ± 0.10, (CI: 91.92, 94.04), 91.46 ± 0.14 (CI: 91.12, 92.72), and 87.34 ± 0.13, (CI: 86.19, 88.10) on the CVC-ClinicDB, Kvasir-SEG and BKAI-IGH datasets respectively, demonstrating its robustness in diverse polyp segmentation tasks. Generalizability tests also yielded Dice scores of 89.51 ± 0.10, (CI: 88.67, 89.71), 72.91 ± 0.09, (CI: 71.39, 74.14), and 65.83 ± 0.22, (CI: 65.47, 66.52) on the CVC-300, CVC-ColonDB, and Polypgen databases respectively. TCFMA-Net demonstrates superior performance in segmenting polyps across datasets, effectively handling variations in polyp characteristics and demonstrating robust generalization capabilities. This study presents a significant advancement in polyp segmentation methods, offering an accurate and reliable tool for colorectal cancer diagnosis.},
  archive      = {J_ARTMED},
  author       = {Malik Abdul Manan and Jinchao Feng and Shahzad Ahmed and Abdul Raheem},
  doi          = {10.1016/j.artmed.2025.103167},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103167},
  shortjournal = {Artif. Intell. Med.},
  title        = {Enhancing colorectal polyp segmentation with TCFMA-net: A transformer-based cross feature and multi-attention network},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantitative computed tomography imaging classification of cement dust-exposed patients-based kolmogorov-arnold networks. <em>ARTMED</em>, <em>167</em>, 103166. (<a href='https://doi.org/10.1016/j.artmed.2025.103166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Occupational health assessment is critical for detecting respiratory issues caused by harmful exposures, such as cement dust. Quantitative computed tomography (QCT) imaging provides detailed insights into lung structure and function, enhancing the diagnosis of lung diseases. However, its high dimensionality poses challenges for traditional machine learning methods. Methods In this study, Kolmogorov-Arnold networks (KANs) were used for the binary classification of QCT imaging data to assess respiratory conditions associated with cement dust exposure. The dataset comprised QCT images from 609 individuals, including 311 subjects exposed to cement dust and 298 healthy controls. We derived 141 QCT-based variables and employed KANs with two hidden layers of 15 and 8 neurons. The network parameters, including grid intervals, polynomial order, learning rate, and penalty strengths, were carefully fine-tuned. The performance of the model was assessed through various metrics, including accuracy, precision, recall, F1 score, specificity, and the Matthews Correlation Coefficient (MCC). A five-fold cross-validation was employed to enhance the robustness of the evaluation. SHAP analysis was applied to interpret the sensitive QCT features. Results The KAN model demonstrated consistently high performance across all metrics, with an average accuracy of 98.03 %, precision of 97.35 %, recall of 98.70 %, F1 score of 98.01 %, and specificity of 97.40 %. The MCC value further confirmed the robustness of the model in managing imbalanced datasets. The comparative analysis demonstrated that the KAN model outperformed traditional methods and other deep learning approaches, such as TabPFN, ANN, FT-Transformer, VGG19, MobileNets, ResNet101, XGBoost, SVM, random forest, and decision tree. SHAP analysis highlighted structural and functional lung features, such as airway geometry, wall thickness, and lung volume, as key predictors. Conclusion KANs significantly improved the classification of QCT imaging data, enhancing early detection of cement dust-induced respiratory conditions. SHAP analysis supported model interpretability, enhancing its potential for clinical translation in occupational health assessments.},
  archive      = {J_ARTMED},
  author       = {Ngan-Khanh Chau and Woo Jin Kim and Chang Hyun Lee and Kum Ju Chae and Gong Yong Jin and Sanghun Choi},
  doi          = {10.1016/j.artmed.2025.103166},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103166},
  shortjournal = {Artif. Intell. Med.},
  title        = {Quantitative computed tomography imaging classification of cement dust-exposed patients-based kolmogorov-arnold networks},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero- and few-shot named entity recognition and text expansion in medication prescriptions using large language models. <em>ARTMED</em>, <em>167</em>, 103165. (<a href='https://doi.org/10.1016/j.artmed.2025.103165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medication prescriptions in electronic health records (EHR) are often in free-text and may include a mix of languages, local brand names, and a wide range of idiosyncratic formats and abbreviations. Large language models (LLMs) have shown a promising ability to generate text in response to input prompts. We use ChatGPT3.5 to automatically structure and expand medication statements in discharge summaries and thus make them easier to interpret for people and machines. Named Entity Recognition (NER) and Text Expansion (EX) are used with different prompt strategies in a zero- and few-shot setting. 100 medication statements were manually annotated and curated. NER performance was measured by using strict and partial matching. For the EX task, two experts interpreted the results by assessing semantic equivalence between original and expanded statements. The model performance was measured by precision, recall, and F1 score. For NER, the best-performing prompt reached an average F1 score of 0.94 in the test set. For EX, the few-shot prompt showed superior performance among other prompts, with an average F1 score of 0.87. Our study demonstrates good performance for NER and EX tasks in free-text medication statements using ChatGPT3.5. Compared to a zero-shot baseline, a few-shot approach prevented the system from hallucinating, which is essential when processing safety-relevant medication data. We tested ChatGPT3.5-tuned prompts on other LLMs, including ChatGPT4o, Gemini 2.0 Flash, MedLM-1.5-Large, and DeepSeekV3. The findings showed most models outperformed ChatGPT3.5 in NER and EX tasks.},
  archive      = {J_ARTMED},
  author       = {Natthanaphop Isaradech and Andrea Riedel and Wachiranun Sirikul and Markus Kreuzthaler and Stefan Schulz},
  doi          = {10.1016/j.artmed.2025.103165},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103165},
  shortjournal = {Artif. Intell. Med.},
  title        = {Zero- and few-shot named entity recognition and text expansion in medication prescriptions using large language models},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cell-interacting and multi-correcting method for automatic circulating tumor cells detection. <em>ARTMED</em>, <em>167</em>, 103164. (<a href='https://doi.org/10.1016/j.artmed.2025.103164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensitive detection of circulating tumor cells (CTCs) from peripheral blood can serve as an effective tool in the early diagnosis and prognosis of cancer. Many methods based on modern object detectors were proposed in recent years for automatic abnormal cells detection in slide images. Although the modes of these methods can also be applied to the CTCs detection, several practical difficulties lead to suboptimal performance of them, such as accurate capture of CTCs in a large number of mixed cells and identification of CTCs and CTC-like cells with similar visual characteristics. Here, we develop a new cell-interacting and multi-correcting detector called CMD, and apply H&E-stained slide images to detect CTCs automatically for the first time. Specifically, the proposed method incorporates two task-oriented novel modules: (1) a self-attention module for aggregating feature interactions between cells and allowing the model to pay more attention to key abnormal cells, (2) a hard sample mining sampler for progressively correcting predictions of cells with ambiguous classification boundaries. Experiments conducted on a multi-center dataset of 1247 annotated slide images confirm the superiority of our method over state-of-the-art cell detection methods. The results of ablation experiment part also prove the effectiveness of two modules. The source codes of this paper are available at https://github.com/zx333445/CMD .},
  archive      = {J_ARTMED},
  author       = {Xuan Zhang and Rensheng Lai and Ling Bai and Jianxin Ji and Ruihao Qin and Lihong Jiang and Bin Meng and Ying Zhang and Xiaohan Zheng and Yan Wang and Xiang Kui and Liuchao Zhang and Dimin Ning and Liuying Wang and Yujiang Chen and Xinling Wang and Shuang Li and Menglei Hua and Junkai Wang and Yong Cao and Lei Cao},
  doi          = {10.1016/j.artmed.2025.103164},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103164},
  shortjournal = {Artif. Intell. Med.},
  title        = {A cell-interacting and multi-correcting method for automatic circulating tumor cells detection},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ICD code mapping model based on clinical text tree structure. <em>ARTMED</em>, <em>167</em>, 103163. (<a href='https://doi.org/10.1016/j.artmed.2025.103163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development and progress of big data and artificial intelligence technology, the ICD coding problem of electronic medical records has been effectively solved. The deep learning method, which replaces the manual coding method, has improved the quality and efficiency of coding. However, it also faces some challenges, such as poor and fuzzy semantic representation of clinical record text and failure to consider the structural characteristics of clinical records. To address these problems, our study proposed an ICD Coding model ( TR ansformer and TR ee-lstm for I CD C oding, TRIC ), which enables adequate automatic ICD encoding of unstructured clinical records. In this model, the structure and features of clinical records are extracted by the constituency tree model and the transformer based model respectively, and the Tree-lstm model is used to enrich the features. Then bioBERT pre-training model is used to highlight the role of key ICD coding and improve its matching performance. Finally, it is classified by a fully connected neural network classifier to realize the many-to-many mapping between clinical records and ICD codes. On the widely used MIMIC-III full data set and sample data set, the TRIC model is compared with 12 benchmark models. The best results of 0.586, 0.109, 0.989, 0.937 and 0.758 were obtained for MiF, MaF, MiAUC, MaAUC and P@8, respectively, which verified that the TRIC model can effectively improve the quality of ICD automatic coding.},
  archive      = {J_ARTMED},
  author       = {Jingjin Xue and Pengli Lu},
  doi          = {10.1016/j.artmed.2025.103163},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103163},
  shortjournal = {Artif. Intell. Med.},
  title        = {ICD code mapping model based on clinical text tree structure},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECG synthesis for cardiac arrhythmias: Integrating self-supervised learning and generative adversarial networks. <em>ARTMED</em>, <em>167</em>, 103162. (<a href='https://doi.org/10.1016/j.artmed.2025.103162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arrhythmia classifiers relying on supervised deep learning models usually require a substantial amount of labeled clinical data. The distribution of these labels is strictly related to the statistics of cardiovascular diseases among the population, which inherently narrows models’ performance for classification tasks. Furthermore, during acquisition and data retrieval from electronic health records, concerns arise regarding patient anonymization due to stringent clinical policies. We introduce a conditional generative architecture for electrocardiography time series, which integrates self-supervision and generative adversarial principles. Empirical validation confirms the enhancement of morphological plausibility in synthetic data, showcasing its effectiveness in generating realistic signals. We propose a novel model (ECGAN), proving its capability of conditioning the probability distribution of ECG recordings. The proposed methodology is assessed upon various rhythm abnormalities including severe congestive heart failure, myocardial infarction, sinus rhythm, and premature ventricular contractions. Our proposed workflow for synthetic time series assessment demonstrates competitive performance compared to state-of-the-art models, achieving an average improvement of 2.4% in arrhythmia classification accuracy across MIT-BIH, BIDMC, and PTB datasets, while ensuring realistic synthetic data and improving training stability.},
  archive      = {J_ARTMED},
  author       = {Lorenzo Simone and Davide Bacciu and Vincenzo Gervasi},
  doi          = {10.1016/j.artmed.2025.103162},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103162},
  shortjournal = {Artif. Intell. Med.},
  title        = {ECG synthesis for cardiac arrhythmias: Integrating self-supervised learning and generative adversarial networks},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symbolic and hybrid AI for brain tissue segmentation using spatial model checking. <em>ARTMED</em>, <em>167</em>, 103154. (<a href='https://doi.org/10.1016/j.artmed.2025.103154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of 3D medical images, and brain segmentation in particular, is an important topic in neuroimaging and in radiotherapy. Overcoming the current, time consuming, practise of manual delineation of brain tumours and providing an accurate, explainable, and replicable method of segmentation of the tumour area and related tissues is therefore an open research challenge. In this paper, we first propose a novel symbolic approach to brain segmentation and delineation of brain lesions based on spatial model checking . This method has its foundations in the theory of closure spaces, a generalisation of topological spaces, and spatial logics. At its core is a high-level declarative logic language for image analysis, ImgQL, and an efficient spatial model checker, VoxLogicA, exploiting state-of-the-art image analysis libraries in its model checking algorithm. We then illustrate how this technique can be combined with Machine Learning techniques leading to a hybrid AI approach that provides accurate and explainable segmentation results. We show the results of the application of the symbolic approach on several public datasets with 3D magnetic resonance (MR) images. Three datasets are provided by the 2017, 2019 and 2020 international MICCAI BraTS Challenges with 210, 259 and 293 MR images, respectively, and the fourth is the BrainWeb dataset with 20 (synthetic) 3D patient images of the normal brain. We then apply the hybrid AI method to the BraTS 2020 training set. Our segmentation results are shown to be in line with the state-of-the-art with respect to other recent approaches, both from the accuracy point of view as well as from the view of computational efficiency, but with the advantage of them being explainable.},
  archive      = {J_ARTMED},
  author       = {Gina Belmonte and Vincenzo Ciancia and Mieke Massink},
  doi          = {10.1016/j.artmed.2025.103154},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103154},
  shortjournal = {Artif. Intell. Med.},
  title        = {Symbolic and hybrid AI for brain tissue segmentation using spatial model checking},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal inference model for accurate medical diagnosis in coronary artery bypass graft operation. <em>ARTMED</em>, <em>167</em>, 103150. (<a href='https://doi.org/10.1016/j.artmed.2025.103150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary Artery Bypass Grafting (CABG) is the most commonly performed cardiac surgery. Predicting postoperative complication risks for patients undergoing CABG is crucial for medical professionals. Considering the susceptibility of traditional models to confounding factors and the scarcity of medical data, it is necessary to design a model that can truly capture the cause-and-effect relationship between the disease and its underlying causes and achieve high accuracy even with limited data. In this paper, a novel Causal Inference Operation Risk Predictor (CIORP) is proposed. We construct a Structural Causal Model (SCM) that demonstrates how two confounders influence the model’s predictions. Then we utilize the backdoor adjustment strategy to control potential confounders from pre-operative information and non-causal intraoperative data. In parallel, capitalizing on few-shot learning techniques, we initiate pre-training using categories with ample samples to extract essential features. Subsequently, we fine-tuned our model on sparse sets of labeled data, facilitating accurate predictions in scenarios with limited annotated samples. The experimental outcomes demonstrate that our model surpasses most existing methods in the internal Electronic Health Record (EHR) of CABG patients, effectively predicting low cardiac output, new-onset atrial fibrillation, perioperative myocardial infarction, and cardiac arrest or ventricular fibrillation post-operation. Our work effectively mitigates the impact of confounding factors, allowing the model to make accurate predictions with minimal medical data.},
  archive      = {J_ARTMED},
  author       = {Qiyi Zhang and Wei Zhang and Qiang Li and Yunpeng Bai and Weizhi Nie and Keliang Xie},
  doi          = {10.1016/j.artmed.2025.103150},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103150},
  shortjournal = {Artif. Intell. Med.},
  title        = {Causal inference model for accurate medical diagnosis in coronary artery bypass graft operation},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large scale gene set ranking for survival-related gene sets. <em>ARTMED</em>, <em>167</em>, 103149. (<a href='https://doi.org/10.1016/j.artmed.2025.103149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease progression is closely linked to shifts in the expression levels of specific genes within molecular pathways. While gene set enrichment analysis is a widely employed method for identifying key disease markers, it has been underutilized in survival analysis. Here, we introduce a novel computational approach that adapts gene set enrichment analysis for survival analysis. The proposed approach considers a gene set, computes a single-sample gene set enrichment score, and, based on this score, splits the samples into cohorts. It then scores the gene sets by evaluating the differences in survival rates between the resulting cohorts. We aim to find gene sets that can lead to cohorts with significantly different survival probabilities. Utilizing gene expression data from The Cancer Genome Atlas and gene sets from the Molecular Signature Database, our results demonstrate that existing empirical research consistently supports the top gene sets our approach associates with survival prognosis. The proposed method broadens gene set enrichment analysis applications to include information on survival, bridging the gap between alterations in molecular pathways and their implications on survival.},
  archive      = {J_ARTMED},
  author       = {Martin Špendl and Jaka Kokošar and Ela Praznik and Luka Ausec and Miha Štajdohar and Blaž Zupan},
  doi          = {10.1016/j.artmed.2025.103149},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103149},
  shortjournal = {Artif. Intell. Med.},
  title        = {Large scale gene set ranking for survival-related gene sets},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Creating, anonymizing and evaluating the first medical language model pre-trained on dutch electronic health records: MedRoBERTa.nl. <em>ARTMED</em>, <em>167</em>, 103148. (<a href='https://doi.org/10.1016/j.artmed.2025.103148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic Health Records (EHRs) contain written notes by all kinds of medical professionals about all aspects of well-being of a patient. When adequately processed with a Large Language Model (LLM), this enormous source of information can be analyzed quantitatively, which can lead to new insights, for example in treatment development or in patterns of patient recovery. However, the language used in clinical notes is very idiosyncratic, which available generic LLMs have not encountered in their pre-training. They therefore have not internalized an adequate representation of the semantics of this data, which is essential for building reliable Natural Language Processing (NLP) software. This article describes the development of the first domain-specific LLM for Dutch EHRs: MedRoBERTa.nl. We discuss in detail why and how we built our model, pre-training it on the notes in EHRs using different strategies, and how we were able to publish it publicly by thoroughly anonymizing it. We evaluate our model extensively, comparing it to various other LLMs. We also illustrate how our model can be used, discussing various studies that built medical text mining technology on top of our model.},
  archive      = {J_ARTMED},
  author       = {Stella Verkijk and Piek Vossen},
  doi          = {10.1016/j.artmed.2025.103148},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103148},
  shortjournal = {Artif. Intell. Med.},
  title        = {Creating, anonymizing and evaluating the first medical language model pre-trained on dutch electronic health records: MedRoBERTa.nl},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An AI system for continuous knee osteoarthritis severity grading: An anomaly detection inspired approach with few labels. <em>ARTMED</em>, <em>167</em>, 103138. (<a href='https://doi.org/10.1016/j.artmed.2025.103138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnostic accuracy and subjectivity of existing Knee Osteoarthritis (OA) ordinal grading systems has been a subject of on-going debate and concern. Existing automated solutions are trained to emulate these imperfect systems, whilst also being reliant on large annotated databases for fully-supervised training. This work proposes a three stage approach for automated continuous grading of knee OA that is built upon the principles of Anomaly Detection (AD); learning a robust representation of healthy knee X-rays and grading disease severity based on its distance to the centre of normality. In the first stage, SS-FewSOME is proposed, a self-supervised AD technique that learns the ‘normal’ representation, requiring only examples of healthy subjects and < 3 % of the labels that existing methods require. In the second stage, this model is used to pseudo label a subset of unlabelled data as ‘normal’ or ‘anomalous’, followed by denoising of pseudo labels with CLIP. The final stage involves retraining on labelled and pseudo labelled data using the proposed Dual Centre Representation Learning (DCRL) which learns the centres of two representation spaces; normal and anomalous. Disease severity is then graded based on the distance to the learned centres. The proposed methodology outperforms existing techniques by margins of up to 24% in terms of OA detection and the disease severity scores correlate with the Kellgren-Lawrence grading system at the same level as human expert performance. Code available at https://github.com/niamhbelton/SS-FewSOME_Disease_Severity_Knee_Osteoarthritis .},
  archive      = {J_ARTMED},
  author       = {Niamh Belton and Aonghus Lawlor and Kathleen M. Curran},
  doi          = {10.1016/j.artmed.2025.103138},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {103138},
  shortjournal = {Artif. Intell. Med.},
  title        = {An AI system for continuous knee osteoarthritis severity grading: An anomaly detection inspired approach with few labels},
  volume       = {167},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for early detection of chronic kidney disease stages in diabetes patients: A TabNet approach. <em>ARTMED</em>, <em>166</em>, 103153. (<a href='https://doi.org/10.1016/j.artmed.2025.103153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic kidney disease (CKD) poses a significant risk for diabetes patients, often leading to severe complications. Early and accurate CKD stage detection is crucial for timely intervention. However, it remains challenging due to its asymptomatic progression, the oversight of routine CKD tests during diabetes checkups, and limited access to nephrologists. This study aimed to address these challenges by developing a multiclass CKD stage prediction model for diabetes patients using longitudinal data from the Chronic Renal Insufficiency Cohort (CRIC) study. A novel iterative backward feature selection strategy was employed to determine key predictors of the CKD stage. TabNet, an attention-based deep learning architecture, was used to build classification models in complete and simplified categories. The complete model used 31 features, including complex kidney biomarkers, while the simplified model used 15 features readily available from routine checkups. The performance of TabNet was compared against traditional tree-based ensemble methods (XGBoost, random forest, AdaBoost) and a multi-layer perceptron. Model-specific and model-agnostic explainable AI (XAI) techniques were applied to interpret model decisions, enhancing the transparency and clinical applicability of the proposed approach. The TabNet models demonstrated superior performance, achieving 94.06 % and 92.71 % accuracy in cross-validation for the complete and simplified models, respectively, and 91.00 % and 88.00 % accuracy on test sets. XAI analysis identified serum creatinine, cystatin C, sex, and age as the most influential factors in CKD stage classification. The proposed TabNet models offer a robust approach for early CKD severity detection in diabetes patients, potentially improving clinical decision-making and patient outcomes.},
  archive      = {J_ARTMED},
  author       = {Md Nakib Hayat Chowdhury and Mamun Bin Ibne Reaz and Sawal Hamid Md Ali and María Liz Crespo and Shamim Ahmad and Ghassan Maan Salim and Fahmida Haque and Luis Guillermo García Ordóñez and Md. Johirul Islam and Taher Muhammad Mahdee and Kh Shahriya Zaman and Md Shahriar Khan Hemel and Mohammad Arif Sobhan Bhuiyan},
  doi          = {10.1016/j.artmed.2025.103153},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {103153},
  shortjournal = {Artif. Intell. Med.},
  title        = {Deep learning for early detection of chronic kidney disease stages in diabetes patients: A TabNet approach},
  volume       = {166},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal signal integration for enhanced sleep stage classification: Leveraging EOG and 2-channel EEG data with advanced feature extraction. <em>ARTMED</em>, <em>166</em>, 103152. (<a href='https://doi.org/10.1016/j.artmed.2025.103152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an innovative approach to sleep stage classification, leveraging a multi-modal signal integration framework encompassing Electrooculography (EOG) and two-channel electroencephalography (EEG) data. We explore the utility of various feature extraction techniques, including Short-Time Fourier Transform (STFT), Wavelet Transform, and raw signal processing, alongside the utilization of neural networks as feature extractors. This unique combination allows us to harness the benefits of traditional feature extraction methods while capitalizing on the power of neural networks to enhance classification performance. Our comprehensive classifier evaluation encompasses a range of models, including Long Short-Term Memory (LSTM) networks and XGBoost. Remarkably, our results reveal exceptional performance with the XGBoost classifier, achieving an overall accuracy of 84.57 % and a macro-F1 score of 78.21 % on the Sleep-EDF expanded dataset, and an overall accuracy of 86.02 % and a macro-F1 score of 81.96 % on the ISRUC-Sleep dataset. Class-specific accuracies highlight its proficiency, particularly in detecting wake and N2 stages, solidifying its superiority among the classifiers tested. This amalgamation of feature sets, complemented by Principal Component Analysis (PCA) for dimensionality reduction, underscores its significance in yielding top-tier classification outcomes. The integration of traditional feature extraction methods with neural networks as feature extractors creates a robust and comprehensive system for sleep stage classification, offering the advantages of both approaches to enhance the accuracy and reliability of the results.},
  archive      = {J_ARTMED},
  author       = {Mahdi Samaee and Mehran Yazdi and Daniel Massicotte},
  doi          = {10.1016/j.artmed.2025.103152},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {103152},
  shortjournal = {Artif. Intell. Med.},
  title        = {Multi-modal signal integration for enhanced sleep stage classification: Leveraging EOG and 2-channel EEG data with advanced feature extraction},
  volume       = {166},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surgery scheduling based on large language models. <em>ARTMED</em>, <em>166</em>, 103151. (<a href='https://doi.org/10.1016/j.artmed.2025.103151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have shown remarkable potential in various fields. This study explores their application in solving multi-objective combinatorial optimization problems–surgery scheduling problem. Traditional multi-objective optimization algorithms, such as the Non-dominated Sorting Genetic Algorithm II (NSGA-II), often require domain expertise for designing precise operators. Here, we propose LLM-NSGA, where LLMs act as evolutionary optimizers, performing selection, crossover, and mutation operations. Results show that for 40 cases, LLMs can independently generate high-quality solutions from prompts. As problem size increases, LLM-NSGA outperformed traditional approaches like NSGA-II and MOEA/D, achieving average improvements of 5.39 %, 80 %, and 0.42 % in three objectives. While LLM-NSGA provided similar results to EoH, another LLM-based method, it outperformed EoH in overall resource allocation. Additionally, we applied LLMs for hyperparameter optimization, comparing them with Bayesian Optimization and Ant Colony Optimization (ACO). LLMs reduced runtime by an average of 23.68 %, and their generated parameters, validated with NSGA-II, produced better surgery scheduling solutions. This demonstrates that LLMs can not only help traditional algorithms find better solutions but also optimize their parameters efficiently.},
  archive      = {J_ARTMED},
  author       = {Fang Wan and Tao Wang and Kezhi Wang and Yuanhang Si and Julien Fondrevelle and Shuimiao Du and Antoine Duclos},
  doi          = {10.1016/j.artmed.2025.103151},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {103151},
  shortjournal = {Artif. Intell. Med.},
  title        = {Surgery scheduling based on large language models},
  volume       = {166},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speech translation for multilingual medical education leveraged by large language models. <em>ARTMED</em>, <em>166</em>, 103147. (<a href='https://doi.org/10.1016/j.artmed.2025.103147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of large language models (LLMs) to speech translation (ST) or, in general, to machine translation (MT) has recently provided excellent results, superseding conventional encoder–decoder MT systems in the general domain. However, this is not clearly the case when LLMs as MT systems are translating medical-related materials. In this respect, the provision of multilingual training materials for oncology professionals is a goal of the EU project Interact-Europe in which this work was framed. To this end, cross-language technology adapted to the oncology domain was developed, evaluated and deployed for multilingual interspecialty medical education. More precisely, automatic speech recognition (ASR) and MT models were adapted to the oncology domain to translate English pre-recorded training videos, kindly provided by the European School of Oncology (ESO), into French, Spanish, German and Slovene. In this work, three categories of MT models adapted to the medical domain were assessed: bilingual encoder–decoder MT models trained from scratch, pre-trained large multilingual encoder–decoder MT models, and multilingual decoder-only LLMs. The experimental results underline the competitiveness in translation quality of LLMs compared to encoder–decoder MT models. Finally, the ESO speech dataset, comprising roughly 1000 videos and 745 h for the training and evaluation of ASR, MT and ST models, was publicly released for the scientific community.},
  archive      = {J_ARTMED},
  author       = {Jorge Iranzo-Sánchez and Jaume Santamaría-Jordà and Gerard Mas-Mollà and Gonçal V. Garcés Díaz-Munío and Javier Iranzo-Sánchez and Javier Jorge and Joan Albert Silvestre-Cerdà and Adrià Giménez and Jorge Civera and Albert Sanchis and Alfons Juan},
  doi          = {10.1016/j.artmed.2025.103147},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {103147},
  shortjournal = {Artif. Intell. Med.},
  title        = {Speech translation for multilingual medical education leveraged by large language models},
  volume       = {166},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CATI: A medical context-enhanced framework for diagnosis code assignment in the UK biobank study. <em>ARTMED</em>, <em>166</em>, 103136. (<a href='https://doi.org/10.1016/j.artmed.2025.103136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosis codes are standard code format of diseases or medical conditions. This study is aimed at assigning diagnosis codes to patients in large-scale biobanks, particularly addressing the issue of missing codes for some patients. This is crucial for downstream disease-related tasks. While recent methods primarily rely on structured biobank data for code assignment, they often overlook the valuable medical context provided by textual information in the biobanks and hierarchical structure of the disease coding system. To address this gap, we have developed CATI , a medical context-enhanced framework for diagnosis C ode A ssignment by integrating T extual details derived from key features and disease h I erarchy. The study is based on the UK Biobank data and considers Phecodes and ICD-10 codes as standard disease formats. We start by representing ten informative codified features using their formal names and then integrate them into CATI as text embeddings, achieved through prompt tuning on the pre-trained language model BioBERT. Recognizing the hierarchical structure of diagnosis codes, we have developed a novel convolution layer in our method that effectively propagates logits between adjacent diagnosis codes. Evaluation results demonstrate that CATI outperforms existing state-of-the-art methods in terms of both Phecodes and ICD-10 codes, boasting at least a 5.16% improvement in average AUROC for unseen disease codes and an 8.68% rise in average AUPRC for disease codes with training instances ranging in (1000,10000]. This framework contributes to the formation of well-defined cohorts for downstream studies and offers a unique perspective for addressing complex healthcare tasks by incorporating vital medical context.},
  archive      = {J_ARTMED},
  author       = {Yue Shen and Jie Wang and Zhe Wang and Zhihao Shi and Hanzhu Chen and Zheng Wang and Yukang Jiang and Xiaopu Wang and Chuandong Cheng and Xueqin Wang and Hongtu Zhu and Jieping Ye},
  doi          = {10.1016/j.artmed.2025.103136},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {103136},
  shortjournal = {Artif. Intell. Med.},
  title        = {CATI: A medical context-enhanced framework for diagnosis code assignment in the UK biobank study},
  volume       = {166},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning and clinical EEG data for multiple sclerosis: A systematic review. <em>ARTMED</em>, <em>166</em>, 103116. (<a href='https://doi.org/10.1016/j.artmed.2025.103116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Sclerosis (MS) is a chronic neuroinflammatory disease of the Central Nervous System (CNS) in which the body’s immune system attacks and destroys the myelin sheath that protects nerve fibers, leading to a wide range of debilitating symptoms and causing disruption of axonal signal transmission. Accurate prediction, diagnosis, monitoring and treatment (PDMT) of MS are essential to improve patient outcomes. Recent advances in neuroimaging technologies, particularly electroencephalography (EEG), combined with machine learning (ML) techniques — including Deep Learning (DL) models — offer promising avenues for enhancing MS management. This systematic review synthesizes existing research on the application of ML and DL models to EEG data for MS. It explores the methodologies used, with a focus on DL architectures such as Convolutional Neural Networks (CNNs) and hybrid models, and highlights recent advancements in ML techniques and EEG technologies that have significantly improved MS diagnosis and monitoring. The review addresses the challenges and potential biases in using ML-based EEG analysis for MS. Strategies to mitigate these challenges, including advanced preprocessing techniques, diverse training datasets, cross-validation methods, and explainable Artificial Intelligence (AI), are discussed. Finally, the paper outlines potential future applications and trends in ML for MS management. This review underscores the transformative potential of ML-enhanced EEG analysis in improving MS management, providing insights into future research directions to overcome existing limitations and further improve clinical practice.},
  archive      = {J_ARTMED},
  author       = {Badr Mouazen and Ahmed Bendaouia and El Hassan Abdelwahed and Giovanni De Marco},
  doi          = {10.1016/j.artmed.2025.103116},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {103116},
  shortjournal = {Artif. Intell. Med.},
  title        = {Machine learning and clinical EEG data for multiple sclerosis: A systematic review},
  volume       = {166},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing novel dynamic prediction methods for survival time to analyze short-term and long-term progression of alzheimer's disease. <em>ARTMED</em>, <em>165</em>, 103140. (<a href='https://doi.org/10.1016/j.artmed.2025.103140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking and monitoring mild cognitive impairment (MCI) patients to intervene promptly at the imminent onset of Alzheimer's disease (AD) are crucial. However, existing dynamic survival prediction models for the conversion from MCI to AD are mostly based on hazard rates, which are less intuitive to interpret and require adherence to the proportional hazards assumption. To address this, we propose a Bayesian joint model (JM) based on the time scale indicator of the restricted mean survival time (RMST), which can capture the trajectories of multiple longitudinal covariates and dynamically predict the patient time to event. Using Monte Carlo simulation, it can be demonstrated that the JM method has a better prediction performance compared with the static model. To predict the dynamic progression of AD in MCI patients at different stages, based on the landmark (LM) method and the JM method for RMST, we developed an LM-based model for short-term dynamic prediction (LM-ST model) and a JM-based model for long-term dynamic prediction (JM-LT model) utilizing the ADNI database. The internal and external validation results indicate that the predictive performance of the LM-ST and JM-LT models surpasses that of the static RMST model. Additionally, an online web tool for the two dynamic prediction models was created for clinical application. In summary, we propose a novel method and combined it with the existing LM method for AD progression, which improves the predictive power and provides a scientific basis for medical decision-making.},
  archive      = {J_ARTMED},
  author       = {Chengfeng Zhang and Shuyu Chen and Yanjie Wang and Pansheng Xue and Yu Song and Jiaqiao Ren and Derun Zhou and Zheng Chen},
  doi          = {10.1016/j.artmed.2025.103140},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103140},
  shortjournal = {Artif. Intell. Med.},
  title        = {Developing novel dynamic prediction methods for survival time to analyze short-term and long-term progression of alzheimer's disease},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An informed machine learning based environmental risk score for hypertension in european adults. <em>ARTMED</em>, <em>165</em>, 103139. (<a href='https://doi.org/10.1016/j.artmed.2025.103139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The exposome framework seeks to unravel the cumulated effects of environmental exposures on health. However, existing methods struggle with challenges including multicollinearity, non-linearity and confounding. To address these limitations, we introduce SEANN (Summary Effect Adjusted Neural Network) a novel approach that integrates pooled effect sizes—a form of domain knowledge—with neural networks to improve the analysis and interpretation of hypertension risk factors. Methods Based on data from 18,337 adults aged 40-65y participants in the GCAT cohort in Catalonia, covering a diverse selection of 53 environmental factors, we computed two environmental risk scores for hypertension prevalence using deep neural networks. An informed risk score using SEANN, integrating 11 different pooled effect size estimates from meta-analyses, and an agnostic counterpart for comparison. For each score, we computed Shapley values to extract and compare the learnt exposure-outcome relationships from each neural network model. Results The obtained predictive performances were similarly good for the agnostic NN and SEANN (AUC 0.7). However, we demonstrate substantial improvements in the scientific validity of the informed risk score captured relationships. Directly informed variables were closer to their corresponding relationships observed in literature and other non-informed variables were successfully adjusted with their direction of associations more in line with previous studies. The mean delta SHAP distance averaged over all variables of the relationships extracted with both models and those observed in the literature, was 6 times lower with SEANN compared with the agnostic NN. The most influential environmental variables within the informed risk score included smoking intensity, Mediterranean diet adherence, coffee consumption and sedentary behaviour. Conclusions This study demonstrates the added value of SEANN over conventional, purely data-driven machine learning approaches. By aligning learned relationships with established literature-based effect sizes, SEANN improves the disentanglement of exposure effects on hypertension.},
  archive      = {J_ARTMED},
  author       = {Jean-Baptiste Guimbaud and Emilie Calabre and Rafael de Cid and Camille Lassale and Manolis Kogevinas and Léa Maître and Rémy Cazabet},
  doi          = {10.1016/j.artmed.2025.103139},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103139},
  shortjournal = {Artif. Intell. Med.},
  title        = {An informed machine learning based environmental risk score for hypertension in european adults},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Are AI-based surveillance systems for healthcare-associated infections ready for clinical practice? a systematic review and meta-analysis. <em>ARTMED</em>, <em>165</em>, 103137. (<a href='https://doi.org/10.1016/j.artmed.2025.103137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare-associated infections (HAIs) are a global public health concern, imposing significant clinical and financial burdens. Despite advancements, surveillance methods remain largely manual and resource-intensive, often leading to underreporting. In this context, automation, particularly through Artificial Intelligence (AI), shows promise in optimizing clinical workflows. However, adoption challenges persist. This study aims to evaluate the current performance and impact of AI in HAI surveillance, considering technical, clinical, and implementation aspects. We conducted a systematic review of Scopus and Embase databases following PRISMA guidelines. AI-based models' performances, accuracy, AUC, sensitivity, and specificity, were pooled using a random-effect model, stratifying by detected HAI type. Our study protocol was registered in PROSPERO (CRD42024524497). Of 2834 identified citations, 249 studies were reviewed. The performances of AI models were generally high but with significant heterogeneity between HAI types. Overall pooled sensitivity, specificity, AUC, and accuracy were respectively 0.835, 0.899, 0.864, and 0.880. About 35.7 % of studies compared AI system performance with alternative automated or standard-of-care surveillance methods, with most achieving better or comparable results to clinical scores or manual surveillance. <7.6 % explicitly measured AI impact in terms of improved patient outcomes, workload reduction, and cost savings, with the majority finding benefits. Only 30 studies deployed the model in a user-friendly tool, and 9 tested it in real clinical practice. In this systematic review, AI shows promising performance in HAI surveillance, although its routine application in clinical practice remains uncommon. Despite over a decade, retrieved studies offer scant evidence on reducing burden, costs, and resource use. This prevents their potential superiority over traditional or simpler automated surveillance systems from being fully evaluated. Further research is necessary to assess impact, enhance interpretability, and ensure reproducibility.},
  archive      = {J_ARTMED},
  author       = {Claudia Cozzolino and Sofia Mao and Francesco Bassan and Laura Bilato and Linda Compagno and Veronica Salvò and Lorenzo Chiusaroli and Silvia Cocchio and Vincenzo Baldo},
  doi          = {10.1016/j.artmed.2025.103137},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103137},
  shortjournal = {Artif. Intell. Med.},
  title        = {Are AI-based surveillance systems for healthcare-associated infections ready for clinical practice? a systematic review and meta-analysis},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstruction-based approach for chest X-ray image segmentation and enhanced multi-label chest disease classification. <em>ARTMED</em>, <em>165</em>, 103135. (<a href='https://doi.org/10.1016/j.artmed.2025.103135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {U-Net is a commonly used model for medical image segmentation. However, when applied to chest X-ray images that show pathologies, it often fails to include these critical pathological areas in the generated masks. To address this limitation, in our study, we tackled the challenge of precise segmentation and mask generation by developing a novel approach, using CycleGAN, that encompasses the areas affected by pathologies within the region of interest, allowing the extraction of relevant radiomic features linked to pathologies. Furthermore, we adopted a feature selection approach to focus the analysis on the most significant features. The results of our proposed pipeline are promising, with an average accuracy of 92.05% and an average AUC of 89.48% for the multi-label classification of effusion and infiltration acquired from the ChestX-ray14 dataset, using the XGBoost model. Furthermore, applying our methodology to the classification of the 14 diseases in the ChestX-ray14 dataset resulted in an average AUC of 83.12%, outperforming previous studies. This research highlights the importance of effective pathological mask generation and features selection for accurate classification of chest diseases. The promising results of our approach underscore its potential for broader applications in the classification of chest diseases.},
  archive      = {J_ARTMED},
  author       = {Aya Hage Chehade and Nassib Abdallah and Jean-Marie Marion and Mathieu Hatt and Mohamad Oueidat and Pierre Chauvet},
  doi          = {10.1016/j.artmed.2025.103135},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103135},
  shortjournal = {Artif. Intell. Med.},
  title        = {Reconstruction-based approach for chest X-ray image segmentation and enhanced multi-label chest disease classification},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Healing with hierarchy: Hierarchical attention empowered graph neural networks for predictive analysis in medical data. <em>ARTMED</em>, <em>165</em>, 103134. (<a href='https://doi.org/10.1016/j.artmed.2025.103134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In healthcare, predictive analysis using unstructured medical data is crucial for gaining insights into patient conditions and outcomes. However, unstructured data, which contains valuable patient information such as symptoms and medical histories, often presents challenges, including lengthy text sequences and incomplete data. To address these issues, we introduce a new framework named Hierarchical Attention-based Integrated Learning (HAIL), designed to predict in-hospital mortality and the duration of stay in the intensive care unit. HAIL combines hierarchical attention mechanisms with graph neural networks to effectively manage missing data and enhance outcome predictions. Our model iteratively refines embeddings, resulting in a more thorough analysis of electronic health record data. Experimental findings demonstrate a notable performance improvement of 2%–3% across various metrics when compared to existing benchmarks on standard datasets, highlighting HAIL’s effectiveness in time-sensitive clinical decision-making. Additionally, our analysis underscores the significance of patient networks in maintaining the robustness and consistent performance of the HAIL framework.},
  archive      = {J_ARTMED},
  author       = {Shivani Gupta and Saurabh Sharma and Rajesh Sharma and Joydeep Chandra},
  doi          = {10.1016/j.artmed.2025.103134},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103134},
  shortjournal = {Artif. Intell. Med.},
  title        = {Healing with hierarchy: Hierarchical attention empowered graph neural networks for predictive analysis in medical data},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human visual perception-inspired medical image segmentation network with multi-feature compression. <em>ARTMED</em>, <em>165</em>, 103133. (<a href='https://doi.org/10.1016/j.artmed.2025.103133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is crucial for computer-aided diagnosis and treatment planning, directly influencing clinical decision-making. To enhance segmentation accuracy, existing methods typically fuse local, global, and various other features. However, these methods often ignore the negative impact of noise on the results during the feature fusion process. In contrast, certain regions of the human visual system, such as the inferotemporal cortex and parietal cortex, effectively suppress irrelevant noise while integrating multiple features—a capability lacking in current methods. To address this gap, we propose MS-Net, a medical image segmentation network inspired by human visual perception. MS-Net incorporates a multi-feature compression (MFC) module that mimics the human visual system’s processing of complex images, first learning various feature types and subsequently filtering out irrelevant ones. Additionally, MS-Net features a segmentation refinement (SR) module that emulates how physicians segment lesions. This module initially performs coarse segmentation to capture the lesion’s approximate location and shape, followed by a refinement step to achieve precise boundary delineation. Experimental results demonstrate that MS-Net not only attains state-of-the-art segmentation performance across three public datasets but also significantly reduces the number of parameters compared to existing models. Code is available at https://github.com/guangguangLi/MS-Net},
  archive      = {J_ARTMED},
  author       = {Guangju Li and Qinghua Huang and Wei Wang and Longzhong Liu},
  doi          = {10.1016/j.artmed.2025.103133},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103133},
  shortjournal = {Artif. Intell. Med.},
  title        = {Human visual perception-inspired medical image segmentation network with multi-feature compression},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cascade learning in multi-task encoder–decoder networks for concurrent bone segmentation and glenohumeral joint clinical assessment in shoulder CT scans. <em>ARTMED</em>, <em>165</em>, 103131. (<a href='https://doi.org/10.1016/j.artmed.2025.103131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Osteoarthritis is a degenerative condition that affects bones and cartilage, often leading to structural changes, including osteophyte formation, bone density loss, and the narrowing of joint spaces. Over time, this process may disrupt the glenohumeral (GH) joint functionality, requiring a targeted treatment. Various options are available to restore joint functions, ranging from conservative management to surgical interventions, depending on the severity of the condition. This work introduces an innovative deep learning framework to process shoulder CT scans. It features the semantic segmentation of the proximal humerus and scapula, the 3D reconstruction of bone surfaces, the identification of the GH joint region, and the staging of three common osteoarthritic-related conditions: osteophyte formation (OS), GH space reduction (JS), and humeroscapular alignment (HSA). Each condition was stratified into multiple severity stages, offering a comprehensive analysis of shoulder bone structure pathology. The pipeline comprised two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D Arthro-Net for threefold classification. A retrospective dataset of 571 CT scans featuring patients with various degrees of GH osteoarthritic-related pathologies was used to train, validate, and test the pipeline. Root mean squared error and Hausdorff distance median values for 3D reconstruction were 0.22 mm and 1.48 mm for the humerus and 0.24 mm and 1.48 mm for the scapula, outperforming state-of-the-art architectures and making it potentially suitable for a PSI-based shoulder arthroplasty preoperative plan context. The classification accuracy for OS, JS, and HSA consistently reached around 90% across all three categories. The computational time for the entire inference pipeline was less than 15 s, showcasing the framework’s efficiency and compatibility with orthopedic radiology practice. The achieved reconstruction and classification accuracy, combined with the rapid processing time, represent a promising advancement towards the medical translation of artificial intelligence tools. This progress aims to streamline the preoperative planning pipeline, delivering high-quality bone surfaces and supporting surgeons in selecting the most suitable surgical approach according to the unique patient joint conditions.},
  archive      = {J_ARTMED},
  author       = {Luca Marsilio and Davide Marzorati and Matteo Rossi and Andrea Moglia and Luca Mainardi and Alfonso Manzotti and Pietro Cerveri},
  doi          = {10.1016/j.artmed.2025.103131},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103131},
  shortjournal = {Artif. Intell. Med.},
  title        = {Cascade learning in multi-task encoder–decoder networks for concurrent bone segmentation and glenohumeral joint clinical assessment in shoulder CT scans},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning approaches for fine-grained symptom estimation in schizophrenia: A comprehensive review. <em>ARTMED</em>, <em>165</em>, 103129. (<a href='https://doi.org/10.1016/j.artmed.2025.103129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schizophrenia is a severe yet treatable mental disorder, and it is diagnosed using a multitude of primary and secondary symptoms. Diagnosis and treatment for each individual depends on the severity of the symptoms. Therefore, there is a need for accurate, personalised assessments. However, the process can be both time-consuming and subjective; hence, there is a motivation to explore automated methods that can offer consistent diagnosis and precise symptom assessments, thereby complementing the work of healthcare practitioners. Machine Learning has demonstrated impressive capabilities across numerous domains, including medicine; the use of Machine Learning in patient assessment holds great promise for healthcare professionals and patients alike, as it can lead to more consistent and accurate symptom estimation. This survey reviews methodologies utilising Machine Learning for diagnosing and assessing schizophrenia. Contrary to previous reviews that primarily focused on binary classification, this work recognises the complexity of the condition and, instead, offers an overview of Machine Learning methods designed for fine-grained symptom estimation. We cover multiple modalities, namely Medical Imaging, Electroencephalograms and Audio-Visual, as the illness symptoms can manifest in a patient’s pathology and behaviour. Finally, we analyse the datasets and methodologies used in the studies and identify trends, gaps, as opportunities for future research.},
  archive      = {J_ARTMED},
  author       = {Niki Maria Foteinopoulou and Ioannis Patras},
  doi          = {10.1016/j.artmed.2025.103129},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103129},
  shortjournal = {Artif. Intell. Med.},
  title        = {Machine learning approaches for fine-grained symptom estimation in schizophrenia: A comprehensive review},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized aggregation index based collaborative fusion for medical diagnosis. <em>ARTMED</em>, <em>165</em>, 103128. (<a href='https://doi.org/10.1016/j.artmed.2025.103128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is critical for data fusion and its decision applications to define the similarity relationships between data units. Compared with the traditional similarity relationship driven by data, this paper proposes a new concept of generalized aggregation index (GAI) driven by data and knowledge. The concept of GAI defines the cumulative representation of the generalized relationships between data units. The generalized relationship involves not only data attributes but also the decision effect knowledge behind data attributes, and is a more accurate relationship representation. Based on data units' GAIs, a new GAI based collaborative fusion method for multi-source medical data is proposed to get high quality fusion results and precise decision conclusions. In the fusion process, the data units in different datasets attract each other and aggregate into entity subsets for fusion collaboratively based on the data units' attraction capabilities measured by data units' GAIs. The experimental analysis shows that the proposed classical and quantum-inspired GAI based methods can get high quality fusion results and highly precise diagnosis conclusions.},
  archive      = {J_ARTMED},
  author       = {Weimin Peng and Aihong Chen and Wenyuan Huang and Jing Chen and Haitao Xu},
  doi          = {10.1016/j.artmed.2025.103128},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103128},
  shortjournal = {Artif. Intell. Med.},
  title        = {Generalized aggregation index based collaborative fusion for medical diagnosis},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep generative models for physiological signals: A systematic literature review. <em>ARTMED</em>, <em>165</em>, 103127. (<a href='https://doi.org/10.1016/j.artmed.2025.103127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a systematic literature review on deep generative models for physiological signals, particularly electrocardiogram (ECG), electroencephalogram (EEG), photoplethysmogram (PPG) and electromyogram (EMG). Compared to the existing review papers, we present the first review that summarizes the recent state-of-the-art deep generative models. By analyzing the state-of-the-art research related to deep generative models along with their main applications and challenges, this review contributes to the overall understanding of these models applied to physiological signals. Additionally, by highlighting the employed evaluation protocol and the most used physiological databases, this review facilitates the assessment and benchmarking of deep generative models.},
  archive      = {J_ARTMED},
  author       = {Nour Neifar and Afef Mdhaffar and Achraf Ben-Hamadou and Mohamed Jmaiel},
  doi          = {10.1016/j.artmed.2025.103127},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103127},
  shortjournal = {Artif. Intell. Med.},
  title        = {Deep generative models for physiological signals: A systematic literature review},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of ICU readmission prediction models: From statistical methods to deep learning approaches. <em>ARTMED</em>, <em>165</em>, 103126. (<a href='https://doi.org/10.1016/j.artmed.2025.103126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of Intensive Care Unit (ICU) readmission has become a crucial area of research due to the increasing demand for ICU resources and the need to provide timely interventions to critically ill patients. In recent years, several studies have explored the use of statistical, machine learning (ML), and deep learning (DL) models to predict ICU readmission. This review paper presents an extensive overview of these studies and discusses the challenges associated with ICU readmission prediction. We categorize the studies based on the type of model used and evaluate their strengths and limitations. We also discuss the performance metrics used to evaluate the models and their potential clinical applications. In addition, this review explores current methodologies, data usage, and recent advances in interpretability and explainable AI for medical applications, offering insights to guide future research and development in this field. Finally, we identify gaps in the current literature and provide recommendations for future research. Recent advances like ML and DL have moderately improved the prediction of the risk of ICU readmission. However, more progress is needed to reach the precision required to build computerized decision support tools.},
  archive      = {J_ARTMED},
  author       = {Waleed Fathy and Guillaume Emeriaud and Farida Cheriet},
  doi          = {10.1016/j.artmed.2025.103126},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {103126},
  shortjournal = {Artif. Intell. Med.},
  title        = {A comprehensive review of ICU readmission prediction models: From statistical methods to deep learning approaches},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in artificial intelligence for diabetes prediction: Insights from a systematic literature review. <em>ARTMED</em>, <em>164</em>, 103132. (<a href='https://doi.org/10.1016/j.artmed.2025.103132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes mellitus (DM), a prevalent metabolic disorder, has significant global health implications. The advent of machine learning (ML) has revolutionized the ability to predict and manage diabetes early, offering new avenues to mitigate its impact. This systematic review examined 53 articles on ML applications for diabetes prediction, focusing on datasets, algorithms, training methods, and evaluation metrics. Various datasets, such as the Singapore National Diabetic Retinopathy Screening Program, REPLACE-BG, National Health and Nutrition Examination Survey (NHANES), and Pima Indians Diabetes Database (PIDD), have been explored, highlighting their unique features and challenges, such as class imbalance. This review assesses the performance of various ML algorithms, such as Convolutional Neural Networks (CNN), Support Vector Machines (SVM), Logistic Regression, and XGBoost, for the prediction of diabetes outcomes from multiple datasets. In addition, it explores explainable AI (XAI) methods such as Grad-CAM, SHAP, and LIME, which improve the transparency and clinical interpretability of AI models in assessing diabetes risk and detecting diabetic retinopathy. Techniques such as cross-validation, data augmentation, and feature selection are discussed in terms of their influence on the versatility and robustness of the model. Some evaluation techniques involving k-fold cross-validation, external validation, and performance indicators such as accuracy, area under curve, sensitivity, and specificity are presented. The findings highlight the usefulness of ML in addressing the challenges of diabetes prediction, the value of sourcing different data types, the need to make models explainable, and the need to keep models clinically relevant. This study highlights significant implications for healthcare professionals, policymakers, technology developers, patients, and researchers, advocating interdisciplinary collaboration and ethical considerations when implementing ML-based diabetes prediction models. By consolidating existing knowledge, this SLR outlines future research directions aimed at improving diagnostic accuracy, patient care, and healthcare efficiency through advanced ML applications. This comprehensive review contributes to the ongoing efforts to utilize artificial intelligence technology for a better prediction of diabetes, ultimately aiming to reduce the global burden of this widespread disease.},
  archive      = {J_ARTMED},
  author       = {Pir Bakhsh Khokhar and Carmine Gravino and Fabio Palomba},
  doi          = {10.1016/j.artmed.2025.103132},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {103132},
  shortjournal = {Artif. Intell. Med.},
  title        = {Advances in artificial intelligence for diabetes prediction: Insights from a systematic literature review},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring trade-offs in equitable stroke risk prediction with parity-constrained and race-free models. <em>ARTMED</em>, <em>164</em>, 103130. (<a href='https://doi.org/10.1016/j.artmed.2025.103130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent analysis of common stroke risk prediction models showed that performance differs between Black and White subgroups, and that applying standard machine learning methods does not reduce these disparities. There have been calls in the clinical literature to correct such disparities by removing race as a predictor ( i.e. , race-free models). Alternatively, a variety of machine learning methods have been proposed to constrain differences in model predictions between racial groups. In this work, we compare these approaches for equitable stroke risk prediction. We begin by proposing a discrete-time, neural network-based time-to-event model that incorporates a parity constraint designed to make predictions more similar between groups. Using harmonized data from Framingham Offspring, MESA, and ARIC studies, we develop both parity-constrained and unconstrained stroke risk prediction models, then compare their performance with race-free models in a held-out test set and a secondary validation set (REGARDS). Our evaluation includes both intra-group and inter-group performance metrics for right-censored time to event outcomes. Results illustrate a fundamental trade-off in which parity-constrained models must sacrifice intra-group calibration to improve inter-group discrimination performance, while the race-free models strike a balance between the two. Consequently, the choice of model must depend on the potential benefits and harms associated with the intended clinical use. All models as well as code implementing our approach are available in a public repository. More broadly, these results provide a roadmap for development of equitable clinical risk prediction models and illustrate both merits and limitations of a race-free approach.},
  archive      = {J_ARTMED},
  author       = {Matthew Engelhard and Daniel Wojdyla and Haoyuan Wang and Michael Pencina and Ricardo Henao},
  doi          = {10.1016/j.artmed.2025.103130},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {103130},
  shortjournal = {Artif. Intell. Med.},
  title        = {Exploring trade-offs in equitable stroke risk prediction with parity-constrained and race-free models},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy-DDI: A robust fuzzy logic query model for complex drug–drug interaction prediction. <em>ARTMED</em>, <em>164</em>, 103125. (<a href='https://doi.org/10.1016/j.artmed.2025.103125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug–drug interactions (DDI) refer to the compound effects that occur when patients take multiple drugs simultaneously, which may reduce the drug efficacy and even harm the patient’s health. Therefore, DDI prediction is significant for drug development and safe medication. Despite the great efforts of researchers, existing methods mainly focus on predicting interactions between drug pairs, cannot contain more biomedical information, and have poor robustness, limiting their application in real-world scenarios. Therefore, we propose a new robust fuzzy logic query model, Fuzzy-DDI, to predict DDI under various complex conditions. Specifically, Fuzzy-DDI decomposes DDI predictions into relational projections and logical operations on rough sets during inference. Fuzzy logic makes it more fault-tolerant than binary logic models. We explore the reasoning ability of the model in a more realistic and meaningful DDI prediction task with target cell type information and explore the robustness of Fuzzy-DDI in noisy environments and missing sample environments. Experiments on three benchmark datasets show that Fuzzy-DDI significantly outperforms state-of-the-art methods on various DDI prediction tasks, demonstrating its capabilities in inference and robustness. The data and code are available at https://github.com/Cheng0829/Fuzzy-DDI .},
  archive      = {J_ARTMED},
  author       = {Junkai Cheng and Yijia Zhang and Hengyi Zhang and Mingyu Lu},
  doi          = {10.1016/j.artmed.2025.103125},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {103125},
  shortjournal = {Artif. Intell. Med.},
  title        = {Fuzzy-DDI: A robust fuzzy logic query model for complex drug–drug interaction prediction},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IdenBAT: Disentangled representation learning for identity-preserved brain age transformation. <em>ARTMED</em>, <em>164</em>, 103115. (<a href='https://doi.org/10.1016/j.artmed.2025.103115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain age transformation aims to convert reference brain images into synthesized images that accurately reflect the age-specific features of a target age group. The primary objective of this task is to modify only the age-related attributes of the reference image while preserving all other age-irrelevant attributes. However, achieving this goal poses substantial challenges due to the inherent entanglement of various image attributes within features extracted from a backbone encoder, resulting in simultaneous alterations during image generation. To address this challenge, we propose a novel architecture that employs disentangled representation learning for identity-preserved brain age transformation, called IdenBAT. This approach facilitates the decomposition of image features, ensuring the preservation of individual traits while selectively transforming age-related characteristics to match those of the target age group. Through comprehensive experiments conducted on both 2D and full-size 3D brain datasets, our method adeptly converts input images to target age while retaining individual characteristics accurately. Furthermore, our approach demonstrates superiority over existing state-of-the-art regarding performance fidelity. The code is available at: https://github.com/ku-milab/IdenBAT .},
  archive      = {J_ARTMED},
  author       = {Junyeong Maeng and Kwanseok Oh and Wonsik Jung and Heung-Il Suk},
  doi          = {10.1016/j.artmed.2025.103115},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {103115},
  shortjournal = {Artif. Intell. Med.},
  title        = {IdenBAT: Disentangled representation learning for identity-preserved brain age transformation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised nuclei segmentation based on pseudo label correction and uncertainty denoising. <em>ARTMED</em>, <em>164</em>, 103113. (<a href='https://doi.org/10.1016/j.artmed.2025.103113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nuclei segmentation plays a vital role in computer-aided histopathology image analysis. Numerous fully supervised learning approaches exhibit amazing performance relying on pathological image with precisely annotations. Whereas, it is difficult and time-consuming in accurate manual labeling on pathological images. Hence, this paper presents a two-stage weakly supervised model including coarse and fine phases, which can achieve nuclei segmentation on whole slide images using only point annotations. In the coarse segmentation step, Voronoi diagram and K-means cluster results are generated based on the point annotations to supervise the training network. In order to cope with the different imaging conditions, an image adaptive clustering pseudo label algorithm is proposed to adapt the color distribution of different images. A Multi-scale Feature Fusion (MFF) module is designed in the decoder to better fusion the feature outputs. Additionally, to reduce the interference of erroneous cluster label, an Exponential Moving Average for cluster label Correction (EMAC) strategy is proposed. After the first step, an uncertainty estimation pseudo label denoising strategy is introduced to denoise Voronoi diagram and adaptive cluster label. In the fine segmentation step, the optimized labels are used for training to obtain the final predicted probability map. Extensive experiments are performed on MoNuSeg and TNBC public benchmarks, which demonstrate our proposed method is superior to other existing nuclei segmentation methods based on point labels. Codes are available at: https://github.com/SSL-droid/WNS-PLCUD .},
  archive      = {J_ARTMED},
  author       = {Xipeng Pan and Shilong Song and Zhenbing Liu and Huadeng Wang and Lingqiao Li and Haoxiang Lu and Rushi Lan and Xiaonan Luo},
  doi          = {10.1016/j.artmed.2025.103113},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {103113},
  shortjournal = {Artif. Intell. Med.},
  title        = {Weakly supervised nuclei segmentation based on pseudo label correction and uncertainty denoising},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing semantically enhanced self-supervised graph convolution and multi-head attention fusion for herb recommendation. <em>ARTMED</em>, <em>164</em>, 103112. (<a href='https://doi.org/10.1016/j.artmed.2025.103112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Chinese herbal medicine has long been recognized as an effective natural therapy. Recently, the development of recommendation systems for herbs has garnered widespread academic attention, as these systems significantly impact the application of traditional Chinese medicine. However, existing herb recommendation systems are limited by data sparsity, insufficient correlation between prescriptions, and inadequate representation of symptoms and herb characteristics. To address these issues, this paper introduces an approach to herb recommendation based on semantically enhanced self-supervised graph convolution and multi-head attention fusion (BSGAM). This method involves efficient embedding of entities following fine-tuning of BERT; leveraging the attributes of herbs to optimize feature representation through a residual graph convolution network and self-supervised learning; and ultimately employing a multi-head attention mechanism for feature integration and recommendation. Experiments conducted on a publicly available traditional Chinese medicine prescription dataset demonstrate that our method achieves improvements of 6.80%, 7.46%, and 6.60% in F1-Score@5, F1-Score@10, and F1-Score@20, respectively, compared to baseline methods. These results confirm the effectiveness of our approach in enhancing the accuracy of herb recommendations.},
  archive      = {J_ARTMED},
  author       = {Xianlun Tang and Yuze Tang and Xinran Liu and Haochuan Zhang and Xiaoyuan Dang and Ying Wang and Zihui Xu},
  doi          = {10.1016/j.artmed.2025.103112},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {103112},
  shortjournal = {Artif. Intell. Med.},
  title        = {Utilizing semantically enhanced self-supervised graph convolution and multi-head attention fusion for herb recommendation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in kidney biopsy lesion assessment through dense instance segmentation. <em>ARTMED</em>, <em>164</em>, 103111. (<a href='https://doi.org/10.1016/j.artmed.2025.103111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Renal biopsies are the gold standard for the diagnosis of kidney diseases. Lesion scores made by renal pathologists are semi-quantitative and exhibit high inter-observer variability. Automating lesion classification within segmented anatomical structures can provide decision support in quantification analysis, thereby reducing inter-observer variability. Nevertheless, classifying lesions in regions-of-interest (ROIs) is clinically challenging due to (a) a large amount of densely packed anatomical objects, (b) class imbalance across different compartments (at least 3), (c) significant variation in size and shape of anatomical objects and (d) the presence of multi-label lesions per anatomical structure. Existing models cannot address these complexities in an efficient and generic manner. This paper presents an analysis for a generalized solution to datasets from various sources (pathology departments) with different types of lesions. Our approach utilizes two sub-networks: dense instance segmentation and lesion classification. We introduce DiffRegFormer , an end-to-end dense instance segmentation sub-network designed for multi-class, multi-scale objects within ROIs. Combining diffusion models, transformers, and RCNNs, DiffRegFormer is a computational-friendly framework that can efficiently recognize over 500 objects across three anatomical classes, i.e., glomeruli, tubuli, and arteries, within ROIs. In a dataset of 303 ROIs from 148 Jones’ silver-stained renal Whole Slide Images (WSIs), our approach outperforms previous methods, achieving an Average Precision of 52.1% (detection) and 46.8% (segmentation). Moreover, our lesion classification sub-network achieves 89.2% precision and 64.6% recall on 21889 object patches out of the 303 ROIs. Lastly, our model demonstrates direct domain transfer to PAS-stained renal WSIs without fine-tuning.},
  archive      = {J_ARTMED},
  author       = {Zhan Xiong and Junling He and Pieter Valkema and Tri Q. Nguyen and Maarten Naesens and Jesper Kers and Fons J. Verbeek},
  doi          = {10.1016/j.artmed.2025.103111},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {103111},
  shortjournal = {Artif. Intell. Med.},
  title        = {Advances in kidney biopsy lesion assessment through dense instance segmentation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning method for malaria parasite evaluation from microscopic blood smear. <em>ARTMED</em>, <em>163</em>, 103114. (<a href='https://doi.org/10.1016/j.artmed.2025.103114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective Malaria remains a leading cause of global morbidity and mortality, responsible for approximately 5,97,000 deaths according to World Malaria Report 2024. The study aims to systematically review current methodologies for automated analysis of the Plasmodium genus in malaria diagnostics. Specifically, it focuses on computer-assisted methods, examining databases, blood smear types, staining techniques, and diagnostic models used for malaria characterization while identifying the limitations and contributions of recent studies. Methods A systematic literature review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Peer-reviewed and published studies from 2020 to 2024 were retrieved from Web of Science and Scopus. Inclusion criteria focused on studies utilizing deep learning and machine learning models for automated malaria detection from microscopic blood smears. The review considered various blood smear types, staining techniques, and diagnostic models, providing a comprehensive evaluation of the automated diagnostic landscape for malaria. Results The NIH database is the standardized and most widely tested database for malaria diagnostics. Giemsa stained-thin blood smear is the most efficient diagnostic method for the detection and observation of the plasmodium lifecycle. This study has been able to identify three categories of ML models most suitable for digital diagnostic of malaria, i.e., Most Accurate- ResNet and VGG with peak accuracy of 99.12 %, Most Popular- custom CNN-based models used by 58 % of studies, and least complex- CADx model. A few pre and post-processing techniques like Gaussian filter and auto encoder for noise reduction have also been discussed for improved accuracy of models. Conclusion Automated methods for malaria diagnostics show considerable promise in improving diagnostic accuracy and reducing human error. While deep learning models have demonstrated high performance, challenges remain in data standardization and real-world application. Addressing these gaps could lead to more reliable and scalable diagnostic tools, aiding global malaria control efforts.},
  archive      = {J_ARTMED},
  author       = {Abhinav Dahiya and Devvrat Raghuvanshi and Chhaya Sharma and Kamaldeep Joshi and Ashima Nehra and Archana Sharma and Radha Jangra and Parul Badhwar and Renu Tuteja and Sarvajeet S. Gill and Ritu Gill},
  doi          = {10.1016/j.artmed.2025.103114},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {103114},
  shortjournal = {Artif. Intell. Med.},
  title        = {Deep learning method for malaria parasite evaluation from microscopic blood smear},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Histopathology image classification based on semantic correlation clustering domain adaptation. <em>ARTMED</em>, <em>163</em>, 103110. (<a href='https://doi.org/10.1016/j.artmed.2025.103110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been successfully applied to histopathology image classification tasks. However, the performance of deep models is data-driven, and the acquisition and annotation of pathological image samples are difficult, which limit the model's performance. Compared to whole slide images (WSI) of patients, histopathology image datasets of animal models are easier to acquire and annotate. Therefore, this paper proposes an unsupervised domain adaptation method based on semantic correlation clustering for histopathology image classification. The aim is to utilize Minmice model histopathology image dataset to achieve the classification and recognition of human WSIs. Firstly, the multi-scale fused features extracted from the source and target domains are normalized and mapped. In the new feature space, the cosine distance between class centers is used to measure the semantic correlation between categories. Then, the domain centers, class centers, and sample distributions are self-constrainedly aligned. Multi-granular information is applied to achieve cross-domain semantic correlation knowledge transfer between classes. Finally, the probabilistic heatmap is used to visualize the model's prediction results and annotate the cancerous regions in WSIs. Experimental results show that the proposed method has high classification accuracy for WSI, and the annotated result is close to manual annotation, indicating its potential for clinical applications.},
  archive      = {J_ARTMED},
  author       = {Pin Wang and Jinhua Zhang and Yongming Li and Yurou Guo and Pufei Li and Rui Chen},
  doi          = {10.1016/j.artmed.2025.103110},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {103110},
  shortjournal = {Artif. Intell. Med.},
  title        = {Histopathology image classification based on semantic correlation clustering domain adaptation},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Voice analysis in parkinson’s disease - A systematic literature review. <em>ARTMED</em>, <em>163</em>, 103109. (<a href='https://doi.org/10.1016/j.artmed.2025.103109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and aim: Parkinson’s disease is a neurodegenerative disease. It is often diagnosed at an advanced stage, which can influence the control over the illness. Therefore, the possibility of diagnosing Parkinson’s disease at an earlier stage, and possibly prognosticate it, could be an advantage. Given this, a literature review that covers current studies in the field is relevant. Methods: The aim of this study is to present a systematic literature review in which the models used for the diagnosis and prognosis of Parkinson’s disease through voice and speech assessment are elucidated. Three databases were consulted to obtain the studies between 2019 and 2023: SienceDirect, IEEE Xplore and ACM Library . Results: One hundred and six studies were considered eligible, considering the definition of inclusion and exclusion criteria. The vast majority of these studies (94.34%) focus on diagnosing the disease, while the remainder (11.32%) focus on prognosis. Conclusion: Voice analysis for the diagnosis and prognosis of Parkinson’s disease using machine learning techniques can be achieved, with very satisfactory performance results, like is demonstrated in this systematic literature review.},
  archive      = {J_ARTMED},
  author       = {Daniela Xavier and Virginie Felizardo and Beatriz Ferreira and Henriques Zacarias and Mehran Pourvahab and Leonice Souza-Pereira and Nuno M. Garcia},
  doi          = {10.1016/j.artmed.2025.103109},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {103109},
  shortjournal = {Artif. Intell. Med.},
  title        = {Voice analysis in parkinson’s disease - A systematic literature review},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing neural language models for medical concept representation and patient trajectory prediction. <em>ARTMED</em>, <em>163</em>, 103108. (<a href='https://doi.org/10.1016/j.artmed.2025.103108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective representation of medical concepts is crucial for secondary analyses of electronic health records. Neural language models have shown promise in automatically deriving medical concept representations from clinical data. However, the comparative performance of different language models for creating these empirical representations, and the extent to which they encode medical semantics, has not been extensively studied. This study aims to address this gap by evaluating the effectiveness of three popular language models - word2vec, fastText, and GloVe - in creating medical concept embeddings that capture their semantic meaning. By using a large dataset of digital health records, we created patient trajectories and used them to train the language models. We then assessed the ability of the learned embeddings to encode semantics through an explicit comparison with biomedical terminologies, and implicitly by predicting patient outcomes and trajectories with different levels of available information. Our qualitative analysis shows that empirical clusters of embeddings learned by fastText exhibit the highest similarity with theoretical clustering patterns obtained from biomedical terminologies, with a similarity score between empirical and theoretical clusters of 0.88, 0.80, and 0.92 for diagnosis, procedure, and medication codes, respectively. Conversely, for outcome prediction, word2vec and GloVe tend to outperform fastText, with the former achieving AUROC as high as 0.78, 0.62, and 0.85 for length-of-stay, readmission, and mortality prediction, respectively. In predicting medical codes in patient trajectories, GloVe achieves the highest performance for diagnosis and medication codes (AUPRC of 0.45 and of 0.81, respectively) at the highest level of the semantic hierarchy, while fastText outperforms the other models for procedure codes (AUPRC of 0.66). Our study demonstrates that subword information is crucial for learning medical concept representations, but global embedding vectors are better suited for more high-level downstream tasks, such as trajectory prediction. Thus, these models can be harnessed to learn representations that convey clinical meaning, and our insights highlight the potential of using machine learning techniques to semantically encode medical data.},
  archive      = {J_ARTMED},
  author       = {Alban Bornet and Dimitrios Proios and Anthony Yazdani and Fernando Jaume-Santero and Guy Haller and Edward Choi and Douglas Teodoro},
  doi          = {10.1016/j.artmed.2025.103108},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {103108},
  shortjournal = {Artif. Intell. Med.},
  title        = {Comparing neural language models for medical concept representation and patient trajectory prediction},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRExplainer: Quantifiable interpretability in drug response prediction with directed graph convolutional network. <em>ARTMED</em>, <em>163</em>, 103101. (<a href='https://doi.org/10.1016/j.artmed.2025.103101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the response of a cancer cell line to a therapeutic drug is pivotal for personalized medicine. Despite numerous deep learning methods that have been developed for drug response prediction, integrating diverse information about biological entities and predicting the directional response remain major challenges. Here, we propose a novel interpretable predictive model, DRExplainer, which leverages a directed graph convolutional network to enhance the prediction in a directed bipartite network framework. DRExplainer constructs a directed bipartite network integrating multi-omics profiles of cell lines, the chemical structure of drugs and known drug response to achieve directed prediction. Then, DRExplainer identifies the most relevant subgraph to each prediction in this directed bipartite network by learning a mask, facilitating critical medical decision-making. Additionally, we introduce a quantifiable method for model interpretability that leverages a ground truth benchmark dataset curated from biological features. In computational experiments, DRExplainer outperforms state-of-the-art predictive methods and another graph-based explanation method under the same experimental setting. Finally, the case studies further validate the interpretability and the effectiveness of DRExplainer in predictive novel drug response. Our code is available at: https://github.com/vshy-dream/DRExplainer .},
  archive      = {J_ARTMED},
  author       = {Haoyuan Shi and Tao Xu and Xiaodi Li and Qian Gao and Zhiwei Xiong and Junfeng Xia and Zhenyu Yue},
  doi          = {10.1016/j.artmed.2025.103101},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {103101},
  shortjournal = {Artif. Intell. Med.},
  title        = {DRExplainer: Quantifiable interpretability in drug response prediction with directed graph convolutional network},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing diagnosis prediction with adaptive disease representation learning. <em>ARTMED</em>, <em>163</em>, 103098. (<a href='https://doi.org/10.1016/j.artmed.2025.103098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosis prediction predicts which diseases a patient is most likely to suffer from in the future based on their historical electronic health records. The time series model can better capture the temporal progression relationship of patient diseases, but ignores the semantic correlation between all diseases; in fact, multiple diseases that are often diagnosed at the same time reflect hidden patterns that are conducive to diagnosis, so predefined global disease co-occurrence graph can help the model understand disease relationships. But it may contain a lot of noise and ignore the semantic adaptation of the disease under the diagnosis target. To this end, we propose a graph-driven end-to-end framework, named A daptive D isease R epresentation L earning (ADRL), obtain disease representation after learning complex disease relationships, and then use it to improve diagnosis prediction performance. This model introduces an adaptive mechanism to dynamically adjust and optimize disease relationships by performing self-supervised perturbations on a predefined global disease co-occurrence graph, thereby learning a global disease relationship graph that contains complex semantic association information between diseases. The computational burden of adaptive global disease graph can be further alleviated by the proposed SVD-based accelerator. Finally, experimental results on two real-world EHR datasets show that the proposed model outperforms existing models in diagnosis prediction.},
  archive      = {J_ARTMED},
  author       = {Hengliang Cheng and Shibo Li and Tao Shen and Weihua Li},
  doi          = {10.1016/j.artmed.2025.103098},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {103098},
  shortjournal = {Artif. Intell. Med.},
  title        = {Enhancing diagnosis prediction with adaptive disease representation learning},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking mitosis detection: Towards diverse data and feature representation for better domain generalization. <em>ARTMED</em>, <em>163</em>, 103097. (<a href='https://doi.org/10.1016/j.artmed.2025.103097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mitosis detection is one of the fundamental tasks in computational pathology, which is extremely challenging due to the heterogeneity of mitotic cell. Most of the current studies solve the heterogeneity in the technical aspect by increasing the model complexity. However, lacking consideration of the biological knowledge and the complex model design may lead to the overfitting problem while limited the generalizability of the detection model. In this paper, we systematically study the morphological appearances in different mitotic phases as well as the ambiguous non-mitotic cells and identify that balancing the data and feature diversity can achieve better generalizability. Based on this observation, we propose a novel generalizable framework (MitDet) for mitosis detection. The data diversity is considered by the proposed diversity-guided sample balancing (DGSB). And the feature diversity is preserved by inter- and intra- class feature diversity-preserved module (InCDP). Stain enhancement (SE) module is introduced to enhance the domain-relevant diversity of both data and features simultaneously. Extensive experiments have demonstrated that our proposed model outperforms all the state-of-the-art (SOTA) approaches in several popular mitosis detection datasets in both internal and unseen test sets using point annotations only. Comprehensive ablation studies have also proven the effectiveness of the rethinking of data and feature diversity balancing. By analyzing the results quantitatively and qualitatively, we believe that our proposed model not only achieves SOTA performance but also might inspire the future studies in new perspectives. Code is available at https://github.com/linjiatai/MitDet .},
  archive      = {J_ARTMED},
  author       = {Jiatai Lin and Hao Wang and Danyi Li and Jing Wang and Bingchao Zhao and Zhenwei Shi and Changhong Liang and Guoqiang Han and Li Liang and Zaiyi Liu and Chu Han},
  doi          = {10.1016/j.artmed.2025.103097},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {103097},
  shortjournal = {Artif. Intell. Med.},
  title        = {Rethinking mitosis detection: Towards diverse data and feature representation for better domain generalization},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital phenotyping for mental health based on data analytics: A systematic literature review. <em>ARTMED</em>, <em>163</em>, 103094. (<a href='https://doi.org/10.1016/j.artmed.2025.103094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though mental health is a human right, mental disorders still affect millions of people worldwide. Untreated and undertreated mental health conditions may lead to suicide, which generates more than 700,000 deaths annually around the world. The broad adoption of smartphones and wearable devices allowed the recording and analysis of human behaviors in digital devices, which might reveal mental health symptoms. This analysis constitutes digital phenotyping research, referring to frequent and constant measurement of human phenotypes in situ based on data from smartphones and other personal digital devices. Therefore, this article presents a systematic literature review providing a computer science view on data analytics for digital phenotyping in mental health. This study reviewed 5,422 articles from ten academic databases published up to September 2024, generating a final list of 74 studies. The investigated databases are ACM, IEEE Xplore, PsycArticles, PsycInfo, Pubmed, Science Direct, Scopus, Springer, Web of Science, and Wiley. We investigated ten research questions, considering explored data, employed devices, and techniques for data analysis. This review also organizes the application domains and mental health conditions, data analytics techniques, and current research challenges. This study found a growing research interest in digital phenotyping for mental health in recent years. Current approaches still present a high dependence on self-reported measures of mental health status, but there is evidence of the employment of smartphones for leveraging passive data collection. Traditional machine learning techniques are the main explored strategies for analyzing the large amount of collected data. In this regard, published approaches deeply focused on data analysis, generating opportunities concerning the implementation of resources for assisting individuals suffering from mental disorders.},
  archive      = {J_ARTMED},
  author       = {Wesllei Felipe Heckler and Luan Paris Feijó and Juliano Varella de Carvalho and Jorge Luis Victória Barbosa},
  doi          = {10.1016/j.artmed.2025.103094},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {103094},
  shortjournal = {Artif. Intell. Med.},
  title        = {Digital phenotyping for mental health based on data analytics: A systematic literature review},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning based estimation of heart surface potentials. <em>ARTMED</em>, <em>163</em>, 103093. (<a href='https://doi.org/10.1016/j.artmed.2025.103093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiographic imaging (ECGI) aims to noninvasively estimate heart surface potentials starting from body surface potentials. This is classically based on geometric information on the torso and the heart from imaging, which complicates clinical application. In this study, we aim to develop a deep learning framework to estimate heart surface potentials solely from body surface potentials, enabling wider clinical use. The framework introduces two main components: the transformation of 3D torso and heart geometries into standard 2D representations, and the development of a customized deep learning network model. The 2D torso and heart representations maintain a consistent layout across different subjects, making the proposed framework applicable to different torso-heart geometries. With spatial information incorporated in the 2D representations, the torso-heart physiological relationship can be learnt by the network. The deep learning model is based on a Pix2Pix network, adapted to work with 2.5D data in our task, i.e., 2D body surface potential maps (BSPMs) and 2D heart surface potential maps (HSPMs) with time sequential information. We propose a new loss function tailored to this specific task, which uses a cosine similarity and different weights for different inputs. BSPMs and HSPMs from 11 healthy subjects (8 females and 3 males) and 29 idiopathic ventricular fibrillation (IVF) patients (11 females and 18 males) were used in this study. Performance was assessed on a test set by measuring the similarity and error between the output of the proposed model and the solution provided by mainstream ECGI, by comparing HSPMs, the concatenated electrograms (EGMs), and the estimated activation time (AT) and recovery time (RT). The mean of the mean absolute error (MAE) for the HSPMs was 0.012 ± 0.011, and the mean of the corresponding structural similarity index measure (SSIM) was 0.984 ± 0.026. The mean of the MAE for the EGMs was 0.004 ± 0.004, and the mean of the corresponding Pearson correlation coefficient (PCC) was 0.643 ± 0.352. Results suggest that the model is able to precisely capture the structural and temporal characteristics of the HSPMs. The mean of the absolute time differences between estimated and reference activation times was 6.048 ± 5.188 ms, and the mean of the absolute differences for recovery times was 18.768 ± 17.299 ms. Overall, results show similar performance between the proposed model and standard ECGI, exhibiting low error and consistent clinical patterns, without the need for CT/MRI. The model shows to be effective across diverse torso-heart geometries, and it successfully integrates temporal information in the input. This in turn suggests the possible use of this model in cost effective clinical scenarios like patient screening or post-operative follow-up.},
  archive      = {J_ARTMED},
  author       = {Tiantian Wang and Joël M.H. Karel and Niels Osnabrugge and Kurt Driessens and Job Stoks and Matthijs J.M. Cluitmans and Paul G.A. Volders and Pietro Bonizzi and Ralf L.M. Peeters},
  doi          = {10.1016/j.artmed.2025.103093},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {103093},
  shortjournal = {Artif. Intell. Med.},
  title        = {Deep learning based estimation of heart surface potentials},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sum of similarity-regularized squared correlations for enhancing SSVEP detection. <em>ARTMED</em>, <em>162</em>, 103100. (<a href='https://doi.org/10.1016/j.artmed.2025.103100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain-computer interface (BCI) provides a direct control pathway between human brain and external devices. Steady-state visual evoked potential based BCI (SSVEP-BCI) has been proven to be a valuable solution due to its advantages of high information transfer rate (ITR) and minimal calibration requirement. Recently, some methods have been proposed based on calibration-training techniques to compute optimal spatial filters from covariances, and have achieved good detection performance. However, these methods ignore the temporally-varying and spatially-coupled characteristics of the EEG signals, which is essentially an important clue for enhancing ITR. More importantly, existing methods cannot well deal with intrinsic noise components of electroencephalogram (EEG) signals, greatly affecting their detection performance. In this paper, we propose a novel method, termed as S um of S imilarity- R egularized S quared C orrelations (SSRSC), which is extended and regularized from the sum of squared correlations. We simultaneously compute the squared correlations for both calibration data and sine-cosine harmonics templates, and mitigate variations by the similarity regularization. Moreover, we extend the SSRSC by adopting the ranking weighted ensemble strategy, termed as weSSCOR. Extensive experiments have been conducted on two benchmark SSVEP datasets, and the results demonstrated that the proposed SSRSC/weSSRSC can significantly improve accuracy and ITR of SSVEP detection with less calibration data, which has great potential in designing high ITR SSVEP-BCIs with less calibration efforts.},
  archive      = {J_ARTMED},
  author       = {Tian-jian Luo and Tao Wu},
  doi          = {10.1016/j.artmed.2025.103100},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103100},
  shortjournal = {Artif. Intell. Med.},
  title        = {Sum of similarity-regularized squared correlations for enhancing SSVEP detection},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TDMFS: Tucker decomposition multimodal fusion model for pan-cancer survival prediction. <em>ARTMED</em>, <em>162</em>, 103099. (<a href='https://doi.org/10.1016/j.artmed.2025.103099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrated analysis of multimodal data offers a more comprehensive view for cancer survival prediction, yet it faces challenges like computational intensity, overfitting, and challenges in achieving a unified representation due to data heterogeneity. To address the above issues, the first Tucker decomposition multimodal fusion model was hereby proposed for pan-cancer survival prediction (TDMFS). The model employed Tucker decomposition to limit complex tensor parameters during fusion, achieving deep modality integration with reduced computational cost and lower overfitting risk. The individual modality-specific representations were then fully exploited by signal modulation mechanisms in a bilinear pooling decomposition to serve as complementary information for the deep fusion representation. Furthermore, the performance of TDMFS was evaluated using a 5-fold cross-validation method with two modal data, gene expression (GeneExpr), and copy number variation (CNV), for 33 cancers from The Cancer Genome Atlas (TCGA) database. The experiments demonstrated that the proposed TDMFS model achieved an average C-index of 0.757 across 33 cancer datasets, with a C-index exceeding 0.80 on 10 of these datasets. Survival curves for both high and low risk patients plotted on 27 cancer datasets were statistically significant. The TDMFS model demonstrated superior performance in survival prediction, outperforming models like LinearSum and Multimodal Factorisation Higher Order Pooling, making it a valuable asset for advancing clinical cancer research.},
  archive      = {J_ARTMED},
  author       = {Jinchao Chen and Pei Liu and Chen Chen and Ying Su and Enguang Zuo and Min Li and Jiajia Wang and Ziwei Yan and Xinya Chen and Cheng Chen and Xiaoyi Lv},
  doi          = {10.1016/j.artmed.2025.103099},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103099},
  shortjournal = {Artif. Intell. Med.},
  title        = {TDMFS: Tucker decomposition multimodal fusion model for pan-cancer survival prediction},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint segmentation of retinal layers and fluid lesions in optical coherence tomography with cross-dataset learning. <em>ARTMED</em>, <em>162</em>, 103096. (<a href='https://doi.org/10.1016/j.artmed.2025.103096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and objectives Age-related macular degeneration (AMD) is the leading cause of irreversible vision loss among people over 50 years old, which manifests in the retina through various changes of retinal layers and pathological lesions. The accurate segmentation of optical coherence tomography (OCT) image features is crucial for the identification and tracking of AMD. Although the recent developments in deep neural network have brought profound progress in this area, accurately segmenting retinal layers and pathological lesions remains a challenging task because of the interaction between these two tasks. Methods In this study, we propose a three-branch, hierarchical multi-task framework that enables joint segmentation of seven retinal layers and three types of pathological lesions. A regression guidance module is introduced to provide explicit shape guidance between sub-tasks. We also propose a cross-dataset learning strategy to leverage public datasets with partial labels. The proposed framework was evaluated on a clinical dataset consisting of 140 OCT B-scans with pixel-level annotations of seven retinal layers and three types of lesions. Additionally, we compared its performance with the state-of-the-art methods on two public datasets. Results Comprehensive ablation showed that the proposed hierarchical architecture significantly improved performance for most retinal layers and pathological lesions, achieving the highest mean DSC of 76.88 %. The IRF also achieved the best performance with a DSC of 68.15 %. Comparative studies demonstrated that the hierarchical multi-task architecture could significantly enhance segmentation accuracy and outperform state-of-the-art methods. Conclusion The proposed framework could also be generalized to other medical image segmentation tasks with interdependent relationships.},
  archive      = {J_ARTMED},
  author       = {Xiayu Xu and Hualin Wang and Yulei Lu and Hanze Zhang and Tao Tan and Feng Xu and Jianqin Lei},
  doi          = {10.1016/j.artmed.2025.103096},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103096},
  shortjournal = {Artif. Intell. Med.},
  title        = {Joint segmentation of retinal layers and fluid lesions in optical coherence tomography with cross-dataset learning},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised learning from EEG data for epilepsy: A systematic literature review. <em>ARTMED</em>, <em>162</em>, 103095. (<a href='https://doi.org/10.1016/j.artmed.2025.103095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and objectives Epilepsy is a neurological disorder characterized by recurrent epileptic seizures, whose neurophysiological signature is altered electroencephalographic (EEG) activity. The use of artificial intelligence (AI) methods on EEG data can positively impact the management of the disease, significantly improving diagnostic and prognostic accuracy as well as treatment outcomes. Our work aims to systematically review the available literature on the use of unsupervised machine learning methods on EEG data in epilepsy, focusing on methodological and clinical differences in terms of algorithms used and clinical applications. Methods Following the PRISMA guidelines, a systematic literature search was performed in several databases for papers published in the last 10 years. Studies employing both unsupervised and self-supervised methods for the classification of EEG data in epilepsy patients were included. The main outcomes of the study were: (i) to provide an overview of the datasets used as input to train the algorithms; (ii) to identify trends in pre-processing, algorithm architectures, validation, and metrics for performance estimation; (iii) to identify and review the clinical applications of AI in epilepsy patients. Results A total of 108 studies met the inclusion criteria. Of them, 86 (79.6 %) have been published in the last 5 years and 60 (55.5 %) in the last two years. The most used validation methods were: hold-out in 37 (34.2 %), k-fold-cross validation in 35 (32.4 %), and leave-one-out in 19 (17.6 %) studies, respectively. Accuracy, sensitivity, and specificity were the most used performance metrics being reported in 71 (65.7 %), 62 (57.4 %), and 42 (39.8 %) studies, respectively, followed by F1-score (27 studies; 25 %), precision (26 studies; 24 %), area under the curve (25 studies; 23.1 %), and false positive rate (22 studies; 20.3 %). Furthermore, 42 (38.9 %) compared to 63 (58.3 %) studies used individual patient versus multiple patients models, respectively. Finally, concerning the clinical applications of unsupervised learning methods on epilepsy patients, we identified six main fields of interest: seizure detection (69 studies; 63.9 %), seizure prediction (27 studies; 25 %), signal propagation and characterization (2 studies; 1.8 %), seizure localization (4 studies; 3.7 %), and seizure classification (22 studies; 20.3 %), respectively. Conclusion The results of this review suggest that the interest in the use of unsupervised learning methods in epilepsy has significantly increased in recent years. From a methodological perspective, the input EEG datasets used for training and testing the algorithms remain the hardest challenge. From a clinical standpoint, the vast majority of studies addressed seizure detection, prediction, and classification whereas studies focusing on seizure characterization and localization are lacking. Future work that can potentially improve the performance of these algorithms includes the use of context information via reinforcement learning and a focus on model explainability.},
  archive      = {J_ARTMED},
  author       = {Alexandra-Maria Tautan and Alexandra-Georgiana Andrei and Carmelo Luca Smeralda and Giampaolo Vatti and Simone Rossi and Bogdan Ionescu},
  doi          = {10.1016/j.artmed.2025.103095},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103095},
  shortjournal = {Artif. Intell. Med.},
  title        = {Unsupervised learning from EEG data for epilepsy: A systematic literature review},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDMentor: A virtual reality-based intelligent tutoring system for surgical decision making in dentistry. <em>ARTMED</em>, <em>162</em>, 103092. (<a href='https://doi.org/10.1016/j.artmed.2025.103092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background While VR simulation has already had a significant impact on training of psychomotor surgical skills, there is still a lack of work on the use of VR simulation to teach surgical decision making. Since surgical decision making is a cognitive process, a simulation for teaching it must be able to not only accurately simulate the surgical environment but to also represent and reason about the cognitive aspects involved. Materials and methods This paper presents and evaluates SDMentor, a virtual training environment that integrates high-fidelity VR simulation with an intelligent tutoring system for teaching surgical decision making in dentistry. SDMentor provides a virtual dental operating room with 3D stereoscopic graphics and with haptic feedback to realistically render the interaction of dental tools with the patient teeth. The intelligent tutor evaluates the student's actions and generates a variety of tutorial feedback. To evaluate the teaching effectiveness of the system, we carried out a randomized controlled trial in the domain of root canal treatment. Results In all three aspects of scores: situation awareness ability, procedural knowledge, and overall performance; the post-test scores showed significant improvement over the pre-test scores of students in the same group ( P < .05). The students from the experimental group had significantly higher learning gains than the students in the control group (P < .05). Conclusions The integration of high-fidelity VR simulation with intelligent tutoring is a promising approach to teaching surgical decision making and could be useful for teaching decision making in other high-precision psychomotor tasks.},
  archive      = {J_ARTMED},
  author       = {Narumol Vannaprathip and Peter Haddawy and Holger Schultheis and Siriwan Suebnukarn},
  doi          = {10.1016/j.artmed.2025.103092},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103092},
  shortjournal = {Artif. Intell. Med.},
  title        = {SDMentor: A virtual reality-based intelligent tutoring system for surgical decision making in dentistry},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-stage multi-modal learning algorithm with adaptive multimodal fusion for improving multi-label skin lesion classification. <em>ARTMED</em>, <em>162</em>, 103091. (<a href='https://doi.org/10.1016/j.artmed.2025.103091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is frequently occurring and has become a major contributor to both cancer incidence and mortality. Accurate and timely diagnosis of skin cancer holds the potential to save lives. Deep learning-based methods have demonstrated significant advancements in the screening of skin cancers. However, most current approaches rely on a single modality input for diagnosis, thereby missing out on valuable complementary information that could enhance accuracy. Although some multimodal-based methods exist, they often lack adaptability and fail to fully leverage multimodal information. In this paper, we introduce a novel uncertainty-based hybrid fusion strategy for a multi-modal learning algorithm aimed at skin cancer diagnosis. Our approach specifically combines three different modalities: clinical images, dermoscopy images, and metadata, to make the final classification. For the fusion of two image modalities, we employ an intermediate fusion strategy that considers the similarity between clinical and dermoscopy images to extract features containing both complementary and correlated information. To capture the correlated information, we utilize cosine similarity, and we employ concatenation as the means for integrating complementary information. In the fusion of image and metadata modalities, we leverage uncertainty to obtain confident late fusion results, allowing our method to adaptively combine the information from different modalities. We conducted comprehensive experiments using a popular publicly available skin disease diagnosis dataset, and the results of these experiments demonstrate the effectiveness of our proposed method. Our proposed fusion algorithm could enhance the clinical applicability of automated skin lesion classification, offering a more robust and adaptive way to make automatic diagnoses with the help of uncertainty mechanism. Code is available at https://github.com/Zuo-Lihan/CosCatNet-Adaptive_Fusion_Algorithm .},
  archive      = {J_ARTMED},
  author       = {Lihan Zuo and Zizhou Wang and Yan Wang},
  doi          = {10.1016/j.artmed.2025.103091},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103091},
  shortjournal = {Artif. Intell. Med.},
  title        = {A multi-stage multi-modal learning algorithm with adaptive multimodal fusion for improving multi-label skin lesion classification},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperbolic multivariate feature learning in higher-order heterogeneous networks for drug–disease prediction. <em>ARTMED</em>, <em>162</em>, 103090. (<a href='https://doi.org/10.1016/j.artmed.2025.103090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New drug discovery has always been a costly, time-consuming process with a high failure rate. Repurposing existing drugs offers a valuable alternative and reduces the risks associated with developing new drugs. Various experimental methods have been employed to facilitate drug repositioning; however, associations prediction between drugs and diseases through biological experiments is both expensive and time-consuming. Consequently, it is imperative to develop efficient and highly precise computational methods for predicting these associations. Based on this, we propose a drug–disease associations prediction method based on H yperbolic M ultivariate feature L earning in H igh-order H eterogeneous Networks for Drug–Disease Prediction, called H 3 ML. Our approach begins by mining high-order information from protein–disease and drug–protein networks to construct high-order heterogeneous networks. Subsequently, we employ multivariate feature learning to create hyperbolic representations, and then enhance the features of the heterogeneous network. Finally, we utilize a hyperbolic graph attention network in the hyperbolic space to aggregate neighbor information and perform the final prediction task. In addition, we evaluate the performance of H 3 ML by comparing it with some state-of-the-art methods across different datasets. The case study further validate the effectiveness of H 3 ML. Our implementation will be publicly available at: https://github.com/jianruichen/H-3ML .},
  archive      = {J_ARTMED},
  author       = {Jiamin Li and Jianrui Chen and Junjie Huang and Xiujuan Lei},
  doi          = {10.1016/j.artmed.2025.103090},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103090},
  shortjournal = {Artif. Intell. Med.},
  title        = {Hyperbolic multivariate feature learning in higher-order heterogeneous networks for drug–disease prediction},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-driven approaches in antibiotic stewardship programs and optimizing prescription practices: A systematic review. <em>ARTMED</em>, <em>162</em>, 103089. (<a href='https://doi.org/10.1016/j.artmed.2025.103089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Antimicrobial stewardship programs (ASPs) are essential in optimizing the use of antibiotics to address the global concern of antimicrobial resistance (AMR). Artificial intelligence (AI) and machine learning (ML) have emerged as promising tools for enhancing ASPs efficiency by improving antibiotic prescription accuracy, resistance prediction, and dosage optimization. This systematic review evaluated the application of AI-driven ASPs, focusing on their methodologies, outcomes, and challenges. We searched all of the databases in PubMed, Scopus, Web of Science, and Embase using keywords related to “AI” and “antibiotic.” We only included studies that used AI and ML algorithms in ASPs, with the main criteria being empirical antibiotic selection, dose adjustment, and ASP adherence. There were no limits on time, setting, or language. Two authors independently screened studies for inclusion and assessed their risk of bias using the Newcastle Ottawa Scale (NOS) Assessment tool for observational studies. Implementation studies underscored AI's potential for improving antimicrobial stewardship programs. Two studies showed that logistic regression, boosted-tree models, and gradient-boosting machines could effectively describe the difference between patients who needed to change their antibiotic regimen and those who did not. Twenty-four studies have confirmed the role of machine learning in optimizing empirical antibiotic selection, predicting resistance, and enhancing therapy appropriateness, all of which have the potential to reduce mortality rates. Additionally, machine learning algorithms showed promise in optimizing antibiotic dosing, particularly for vancomycin. This systematic review aimed to highlight various AI models, their applications in ASPs, and the resulting impact on healthcare outcomes. Machine learning and AI models effectively enhance antibiotic stewardship by optimizing patient interventions, empirical antibiotic selection, resistance prediction, and dosing. However, it subtly draws attention to the differences between high-income countries (HICs) and low- and middle-income countries (LMICs), highlighting the structural difficulties that LMICs confront while simultaneously highlighting the progress made in HICs.},
  archive      = {J_ARTMED},
  author       = {Hamid Harandi and Maryam Shafaati and Mohammadreza Salehi and Mohammad Mahdi Roozbahani and Keyhan Mohammadi and Samaneh Akbarpour and Ramin Rahimnia and Gholamreza Hassanpour and Yasin Rahmani and Arash Seifi},
  doi          = {10.1016/j.artmed.2025.103089},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103089},
  shortjournal = {Artif. Intell. Med.},
  title        = {Artificial intelligence-driven approaches in antibiotic stewardship programs and optimizing prescription practices: A systematic review},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence non-invasive methods for neonatal jaundice detection: A review. <em>ARTMED</em>, <em>162</em>, 103088. (<a href='https://doi.org/10.1016/j.artmed.2025.103088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neonatal jaundice is a common and potentially fatal health condition in neonates, especially in low and middle income countries, where it contributes considerably to neonatal morbidity and death. Traditional diagnostic approaches, such as Total Serum Bilirubin (TSB) testing, are invasive and could lead to discomfort, infection risk, and diagnostic delays. As a result, there is a rising interest in non-invasive approaches for detecting jaundice early and accurately. An in-depth analysis of non-invasive techniques for detecting neonatal jaundice is presented by this review, exploring several AI-driven techniques, such as Machine Learning (ML) and Deep Learning (DL), which have demonstrated the ability to enhance diagnostic accuracy by evaluating complex patterns in neonatal skin color and other relevant features. It is identified that AI models incorporating variants of neural networks achieve an accuracy rate of over 90% in detecting jaundice when compared to traditional methods. Furthermore, satisfactory outcomes in field settings have been demonstrated by mobile-based applications that use smartphone cameras to estimate bilirubin levels, providing a practical alternative for resource-constrained areas. The potential impact of AI-based solutions on reducing neonatal morbidity and mortality is evaluated by this review, with a focus on real-world clinical challenges, highlighting the effectiveness and practicality of AI-based strategies as an assistive tool in revolutionizing neonatal care through early jaundice diagnosis, while also addressing the ethical and practical implications of integrating these technologies in clinical practice. Future research areas, such as the development of new imaging technologies and the incorporation of wearable sensors for real-time bilirubin monitoring, are recommended by the paper.},
  archive      = {J_ARTMED},
  author       = {Fati Oiza Salami and Muhammad Muzammel and Youssef Mourchid and Alice Othmani},
  doi          = {10.1016/j.artmed.2025.103088},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103088},
  shortjournal = {Artif. Intell. Med.},
  title        = {Artificial intelligence non-invasive methods for neonatal jaundice detection: A review},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving unified information extraction in chinese mental health domain with instruction-tuned LLMs and type-verification component. <em>ARTMED</em>, <em>162</em>, 103087. (<a href='https://doi.org/10.1016/j.artmed.2025.103087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Extracting psychological counseling help-seeker information from unstructured text is crucial for providing effective mental health support. This task involves identifying personal emotions, psychological states, and underlying psychological issues but faces significant challenges. These challenges include the sensitivity of mental health data, the lack of Chinese instruction datasets, and the difficulties large language models (LLMs) encounter with complex natural language understanding tasks. Objective: This study aims to address these challenges by developing a unified information extraction framework for Chinese mental health texts. Specifically, it leverages instruction-tuned LLMs and incorporates a novel type-verification (TV) component to improve performance while minimizing computational demands. Methods: We first constructed a Chinese mental health domain instruction dataset for mental health information extraction using synthetic data generated by ChatGPT, guided by psychology experts. This dataset includes self-reported statements from psychological counseling help-seekers, capturing their personal situations, emotions, thoughts, and experiences. Subsequently, we fine-tuned open-source LLMs on this dataset to perform named entity recognition, relation extraction, and event extraction. To address errors and omissions in the extracted information, we introduced a type-verification component. This component employs a lightweight model with significantly fewer parameters to verify the extracted types. The verification results were then fed back into LLMs for further refinement. Results: Experimental results demonstrate that our framework achieves outstanding performance in mental health information extraction. The type-verification component significantly enhances extraction accuracy while reducing computational resource requirements through the use of a lightweight model. By combining robust instruction-tuned LLMs with an efficient type-verification component, our approach delivers exceptional results. Conclusion: This study presents a novel and efficient framework for tackling the challenges of mental health information extraction in Chinese texts. By integrating instruction-tuned LLMs with a lightweight type-verification component, our approach significantly improves extraction accuracy and computational efficiency. This framework holds promise for supporting scalable, automated mental health support systems, advancing both research and practical applications in the mental health domain.},
  archive      = {J_ARTMED},
  author       = {Zijie Cai and Hui Fang and Jianhua Liu and Ge Xu and Yunfei Long and Yin Guan and Tianci Ke},
  doi          = {10.1016/j.artmed.2025.103087},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103087},
  shortjournal = {Artif. Intell. Med.},
  title        = {Improving unified information extraction in chinese mental health domain with instruction-tuned LLMs and type-verification component},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BDFormer: Boundary-aware dual-decoder transformer for skin lesion segmentation. <em>ARTMED</em>, <em>162</em>, 103079. (<a href='https://doi.org/10.1016/j.artmed.2025.103079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting skin lesions from dermatoscopic images is crucial for improving the quantitative analysis of skin cancer. However, automatic segmentation of skin lesions remains a challenging task due to the presence of unclear boundaries, artifacts, and obstacles such as hair and veins, all of which complicate the segmentation process. Transformers have demonstrated superior capabilities in capturing long-range dependencies through self-attention mechanisms and are gradually replacing CNNs in this domain. However, one of their primary limitations is the inability to effectively capture local details, which is crucial for handling unclear boundaries and significantly affects segmentation accuracy. To address this issue, we propose a novel boundary-aware dual-decoder transformer that employs a single encoder and dual-decoder framework for both skin lesion segmentation and dilated boundary segmentation. Within this model, we introduce a shifted window cross-attention block to build the dual-decoder structure and apply multi-task distillation to enable efficient interaction of inter-task information. Additionally, we propose a multi-scale aggregation strategy to refine the extracted features, ensuring optimal predictions. To further enhance boundary details, we incorporate a dilated boundary loss function, which expands the single-pixel boundary mask into planar information. We also introduce a task-wise consistency loss to promote consistency across tasks. Our method is evaluated on three datasets: ISIC2018, ISIC2017, and PH 2 , yielding promising results with excellent performance compared to state-of-the-art models. The code is available at https://github.com/Yuxuan-Ye/BDFormer .},
  archive      = {J_ARTMED},
  author       = {Zexuan Ji and Yuxuan Ye and Xiao Ma},
  doi          = {10.1016/j.artmed.2025.103079},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103079},
  shortjournal = {Artif. Intell. Med.},
  title        = {BDFormer: Boundary-aware dual-decoder transformer for skin lesion segmentation},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering large language models for automated clinical assessment with generation-augmented retrieval and hierarchical chain-of-thought. <em>ARTMED</em>, <em>162</em>, 103078. (<a href='https://doi.org/10.1016/j.artmed.2025.103078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Understanding and extracting valuable information from electronic health records (EHRs) is important for improving healthcare delivery and health outcomes. Large language models (LLMs) have demonstrated significant proficiency in natural language understanding and processing, offering promises for automating the typically labor-intensive and time-consuming analytical tasks with EHRs. Despite the active application of LLMs in the healthcare setting, many foundation models lack real-world healthcare relevance. Applying LLMs to EHRs is still in its early stage. To advance this field, in this study, we pioneer a generation-augmented prompting paradigm “GAPrompt” to empower generic LLMs for automated clinical assessment, in particular, quantitative stroke severity assessment, using data extracted from EHRs. Methods: The GAPrompt paradigm comprises five components: (i) prompt-driven selection of LLMs, (ii) generation-augmented construction of a knowledge base, (iii) summary-based generation-augmented retrieval (SGAR); (iv) inferencing with a hierarchical chain-of-thought (HCoT), and (v) ensembling of multiple generations. Results: GAPrompt addresses the limitations of generic LLMs in clinical applications in a progressive manner. It efficiently evaluates the applicability of LLMs in specific tasks through LLM selection prompting, enhances their understanding of task-specific knowledge from the constructed knowledge base, improves the accuracy of knowledge and demonstration retrieval via SGAR, elevates LLM inference precision through HCoT, enhances generation robustness, and reduces hallucinations of LLM via ensembling. Experiment results demonstrate the capability of our method to empower LLMs to automatically assess EHRs and generate quantitative clinical assessment results. Conclusion: Our study highlights the applicability of enhancing the capabilities of foundation LLMs in medical domain-specific tasks, i.e. , automated quantitative analysis of EHRs, addressing the challenges of labor-intensive and often manually conducted quantitative assessment of stroke in clinical practice and research. This approach offers a practical and accessible GAPrompt paradigm for researchers and industry practitioners seeking to leverage the power of LLMs in domain-specific applications. Its utility extends beyond the medical domain, applicable to a wide range of fields.},
  archive      = {J_ARTMED},
  author       = {Zhanzhong Gu and Wenjing Jia and Massimo Piccardi and Ping Yu},
  doi          = {10.1016/j.artmed.2025.103078},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103078},
  shortjournal = {Artif. Intell. Med.},
  title        = {Empowering large language models for automated clinical assessment with generation-augmented retrieval and hierarchical chain-of-thought},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finger-aware artificial neural network for predicting arthritis in patients with hand pain. <em>ARTMED</em>, <em>162</em>, 103077. (<a href='https://doi.org/10.1016/j.artmed.2025.103077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arthritis is an inflammatory condition associated with joint damage, the incidence of which is increasing worldwide. In severe cases, arthritis can result in the restriction of joint movement, thereby affecting daily activities; as such, early and accurate diagnosis crucial to ensure effective treatment and management. Advances in imaging technologies used for arthritis diagnosis, particularly Single Photon Emission Computed Tomography/Computed Tomography (SPECT/CT), have enabled the quantitative measurement of joint inflammation using SUV max . To the best of our knowledge, this is the first study to apply deep learning to SUV max to predict the development of hand arthritis. We developed a transformer-based Finger-aware Artificial Neural Network (FANN) to predict arthritis in patients experiencing hand pain, including finger embedding, and to share unique finger-specific information between hands. Compared to conventional machine learning models, the FANN model demonstrated superior performance, achieving an area under the receiver operating characteristic curve of 0.85, accuracy of 0.79, precision of 0.87, recall of 0.79, and F1-score of 0.83. Furthermore, analysis using the SHapley Additive exPlanations (SHAP) algorithm revealed that the FANN predictions were most significantly influenced by the proximal interphalangeal joints of the right hand, in which arthritis is the most clinically prevalent. These findings indicate that the FANN significantly enhances arthritis prediction, representing a promising tool for clinical decision-making in arthritis diagnosis.},
  archive      = {J_ARTMED},
  author       = {Hwa-Ah-Ni Lee and Geun-Hyeong Kim and Seung Park and In Ah Choi and Hyun Woo Kwon and Hansol Moon and Jae Hyun Jung and Chulhan Kim},
  doi          = {10.1016/j.artmed.2025.103077},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {103077},
  shortjournal = {Artif. Intell. Med.},
  title        = {Finger-aware artificial neural network for predicting arthritis in patients with hand pain},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of decision support tools by model order reduction for active endovascular navigation. <em>ARTMED</em>, <em>161</em>, 103080. (<a href='https://doi.org/10.1016/j.artmed.2025.103080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Endovascular therapies enable minimally invasive treatment of vascular pathologies by guiding long tools towards the target area. However, certain pathways, such as the Supra-Aortic Trunks (SATs), present complex trajectories that make navigation challenging. To improve catheterization access to these challenging targets, an active guidewire composed of Shape Memory Alloy has been developed. Our study focuses on navigating this device and associated catheters to reach neurovascular targets via the left carotid artery. In previous work, a finite element model was used to simulate the navigation of the active guidewire and catheters from the aortic arch to the branching of the left carotid artery in patient-specific aortas. However, these numerical simulations are computationally intensive, limiting their feasibility for real-time navigation assistance. To address this, we present the development of numerical charts that enable real-time computation based on high-fidelity FE simulations. These charts predict: (1) the behavior of the active guidewire, and (2) the navigation of the guidewire and catheters within specific anatomical configurations, based on guidewire and navigation parameters. Using the High Order Proper Generalized Decomposition (HOPGD) method, these charts achieve accurate real-time predictions with errors below 5 % and a response time of 10 − 3 seconds, based on a limited number of preliminary high-fidelity computations. These findings could significantly contribute to the development of clinically applicable methods to enhance endovascular procedures and the advance the broader field of neurovascular interventions.},
  archive      = {J_ARTMED},
  author       = {Arif Badrou and Arnaud Duval and Jérôme Szewczyk and Raphaël Blanc and Nicolas Tardif and Nahiène Hamila and Anthony Gravouil and Aline Bel-Brunon},
  doi          = {10.1016/j.artmed.2025.103080},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {103080},
  shortjournal = {Artif. Intell. Med.},
  title        = {Development of decision support tools by model order reduction for active endovascular navigation},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging deep-learning and unconventional data for real-time surveillance, forecasting, and early warning of respiratory pathogens outbreak. <em>ARTMED</em>, <em>161</em>, 103076. (<a href='https://doi.org/10.1016/j.artmed.2025.103076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Controlling re-emerging outbreaks such as COVID-19 is a critical concern to global health. Disease forecasting solutions are extremely beneficial to public health emergency management. This work aims to design and deploy a framework for real-time surveillance, prediction, forecasting, and early warning of respiratory disease. To this end, we selected southern African countries and Canadian provinces, along with COVID-19 and influenza as our case studies. Methodology Six different datasets were collected for different provinces of Canada: number of influenza cases, number of COVID-19 cases, Google Trends, Reddit posts, satellite air quality data, and weather data. Moreover, five different data sources were collected for southern African countries whose COVID-19 number of cases were significantly correlated with each other: number of COVID-19 infections, Google Trends, Wiki Trends, Google News, and satellite air quality data. For each infectious disease, i.e. COVID-19 and Influenza for Canada and COVID-19 for southern African countries, data was processed, scaled, and fed into the deep learning model which included four layers, namely, a Convolutional Neural Network (CNN), a Graph Neural Network (GNN), a Gated Recurrent Unit (GRU), and a linear Neural Network (NN). Hyperparameters were optimized to provide an accurate 56-day-ahead prediction of the number of cases. Result The accuracy of our models in real-time surveillance, prediction, forecasting, and early warning of respiratory diseases are evaluated against state-of-the-art models, through Root Mean Square Error (RMSE), coefficient of determination (R2-score), and correlation coefficient. Our model improves R2-score, RMSE, and correlation by up to 55.98 %, 39.71 %, and 44.47 % for 56 days-ahead COVID-19 prediction in Ontario, 34.87 %, 25.52 %, 50.91 % for 8 weeks-ahead influenza prediction in Quebec, and 51.04 %, 32.04 %, and 28.74 % for 56 days-ahead COVID-19 prediction in South Africa, respectively. Conclusion This work presents a framework that automatically collects data from unconventional sources, and builds an early warning system for COVID-19 and influenza outbreaks. The result is extremely helpful to policy-makers and health officials for preparedness and rapid response against future outbreaks.},
  archive      = {J_ARTMED},
  author       = {Z. Movahedi Nia and L. Seyyed-Kalantari and M. Goitom and B. Mellado and A. Ahmadi and A. Asgary and J. Orbinski and J. Wu and J.D. Kong},
  doi          = {10.1016/j.artmed.2025.103076},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {103076},
  shortjournal = {Artif. Intell. Med.},
  title        = {Leveraging deep-learning and unconventional data for real-time surveillance, forecasting, and early warning of respiratory pathogens outbreak},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical X-ray gastric cancer diagnostic support using refined stochastic data augmentation and hard boundary box training. <em>ARTMED</em>, <em>161</em>, 103075. (<a href='https://doi.org/10.1016/j.artmed.2025.103075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Endoscopy is widely used to diagnose gastric cancer and has a high diagnostic performance, but it must be performed by a physician, which limits the number of people who can be diagnosed. In contrast, gastric X-rays can be taken by radiographers, thus allowing a much larger number of patients to undergo imaging. However, the diagnosis of X-ray images relies heavily on the expertise and experience of physicians, and few machine learning methods have been developed to assist in this process. We propose a novel and practical gastric cancer diagnostic support system for gastric X-ray images that will enable more people to be screened. The system is based on a general deep learning-based object detection model and incorporates two novel techniques: refined probabilistic stomach image augmentation (R-sGAIA) and hard boundary box training (HBBT). R-sGAIA enhances the probabilistic gastric fold region and provides more learning patterns for cancer detection models. HBBT is an efficient training method that improves model performance by allowing the use of unannotated negative (i.e., healthy control) samples, which are typically unusable in conventional detection models. The proposed system achieved a sensitivity (SE) for gastric cancer of 90.2%, higher than that of an expert (85.5%). Under these conditions, two out of five candidate boxes identified by the system were cancerous (precision = 42.5%), with an image processing speed of 0.51 s per image. The system also outperformed methods using the same object detection model and state-of-the-art data augmentation by showing a 5.9-point improvement in the F1 score. In summary, this system efficiently identifies areas for radiologists to examine within a practical time frame, thus significantly reducing their workload.},
  archive      = {J_ARTMED},
  author       = {Hideaki Okamoto and Quan Huu Cap and Takakiyo Nomura and Kazuhito Nabeshima and Jun Hashimoto and Hitoshi Iyatomi},
  doi          = {10.1016/j.artmed.2025.103075},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {103075},
  shortjournal = {Artif. Intell. Med.},
  title        = {Practical X-ray gastric cancer diagnostic support using refined stochastic data augmentation and hard boundary box training},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-based non-invasive imaging technologies for early autism spectrum disorder diagnosis: A short review and future directions. <em>ARTMED</em>, <em>161</em>, 103074. (<a href='https://doi.org/10.1016/j.artmed.2025.103074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism Spectrum Disorder (ASD) is a neurological condition, with recent statistics from the CDC indicating a rising prevalence of ASD diagnoses among infants and children. This trend emphasizes the critical importance of early detection, as timely diagnosis facilitates early intervention and enhances treatment outcomes. Consequently, there is an increasing urgency for research to develop innovative tools capable of accurately and objectively identifying ASD in its earliest stages. This paper offers a short overview of recent advancements in non-invasive technology for early ASD diagnosis, focusing on an imaging modality, structural MRI technique, which has shown promising results in early ASD diagnosis. This brief review aims to address several key questions: (i) Which imaging radiomics are associated with ASD? (ii) Is the parcellation step of the brain cortex necessary to improve the diagnostic accuracy of ASD? (iii) What databases are available to researchers interested in developing non-invasive technology for ASD? (iv) How can artificial intelligence tools contribute to improving the diagnostic accuracy of ASD? Finally, our review will highlight future trends in ASD diagnostic efforts.},
  archive      = {J_ARTMED},
  author       = {Mostafa Abdelrahim and Mohamed Khudri and Ahmed Elnakib and Mohamed Shehata and Kate Weafer and Ashraf Khalil and Gehad A. Saleh and Nihal M. Batouty and Mohammed Ghazal and Sohail Contractor and Gregory Barnes and Ayman El-Baz},
  doi          = {10.1016/j.artmed.2025.103074},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {103074},
  shortjournal = {Artif. Intell. Med.},
  title        = {AI-based non-invasive imaging technologies for early autism spectrum disorder diagnosis: A short review and future directions},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid approach for drug-target interaction predictions in ischemic stroke models. <em>ARTMED</em>, <em>161</em>, 103067. (<a href='https://doi.org/10.1016/j.artmed.2025.103067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple cell death mechanisms are triggered during ischemic stroke and they are interconnected in a complex network with extensive crosstalk, complicating the development of targeted therapies. We therefore propose a novel framework for identifying disease-specific drug-target interaction (DTI), named strokeDTI, to extract key nodes within an interconnected graph network of activated pathways via leveraging transcriptomic sequencing data. Our findings reveal that the drugs a model can predict are highly representative of the characteristics of the database the model is trained on. However, models with comparable performance yield diametrically opposite predictions in real testing scenarios. Our analysis reveals a correlation between the reported literature on drug-target pairs and their binding scores. Leveraging this correlation, we introduced an additional module to assess the predictive validity of our model for each unique target, thereby improving the reliability of the framework's predictions. Our framework identified Cerdulatinib as a potential anti-stroke drug via targeting multiple cell death pathways, particularly necroptosis and apoptosis. Experimental validation in in vitro and in vivo models demonstrated that Cerdulatinib significantly attenuated stroke-induced brain injury via inhibiting multiple cell death pathways, improving neurological function, and reducing infarct volume. This highlights strokeDTI's potential for disease-specific drug-target identification and Cerdulatinib's potential as a potent anti-stroke drug.},
  archive      = {J_ARTMED},
  author       = {Jing-Jie Peng and Yi-Yue Zhang and Rui-Feng Li and Wen-Jun Zhu and Hong-Rui Liu and Hui-Yin Li and Bin Liu and Dong-Sheng Cao and Jun Peng and Xiu-Ju Luo},
  doi          = {10.1016/j.artmed.2025.103067},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {103067},
  shortjournal = {Artif. Intell. Med.},
  title        = {Hybrid approach for drug-target interaction predictions in ischemic stroke models},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implementation of artificial intelligence approaches in oncology clinical trials: A systematic review. <em>ARTMED</em>, <em>161</em>, 103066. (<a href='https://doi.org/10.1016/j.artmed.2025.103066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introduction There is a growing interest in leveraging artificial intelligence (AI) technologies to enhance various aspects of clinical trials. The goal of this systematic review is to assess the impact of implementing AI approaches on different aspects of oncology clinical trials. Methods Pertinent keywords were used to find relevant articles published in PubMed, Scopus, and Google Scholar databases, which described the clinical application of AI approaches. A quality evaluation utilizing a customized checklist specifically adapted was conducted. This study is registered with PROSPERO (CRD42024537153). Results Out of the identified 2833 studies, 72 studies satisfied the inclusion criteria. Clinical Trial Enrollment & Eligibility were among the most commonly studied clinical trial aspects with 30 papers. The prediction of outcomes was covered in 25 studies of which 15 addressed the prediction of patients' survival and 10 addressed the prediction of drug outcomes. The trial design was studied in 10 articles. Three studies addressed each of the personalized treatments and decision-making, while one addressed data management. The results demonstrate using AI in cancer clinical trials has the potential to increase clinical trial enrollment, predict clinical outcomes, improve trial design, enhance personalized treatments, and increase concordance in decision-making. Additionally, automating some areas and tasks, clinical trials were made more efficient, and human error was minimized. Nevertheless, concerns and restrictions related to the application of AI in clinical studies are also noted. Conclusion AI tools have the potential to revolutionize the design, enrollment rate, and outcome prediction of oncology clinical trials.},
  archive      = {J_ARTMED},
  author       = {Marwa Saady and Mahmoud Eissa and Ahmed S. Yacoub and Ahmed B. Hamed and Hassan Mohamed El-Said Azzazy},
  doi          = {10.1016/j.artmed.2025.103066},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {103066},
  shortjournal = {Artif. Intell. Med.},
  title        = {Implementation of artificial intelligence approaches in oncology clinical trials: A systematic review},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalizable normative deep autoencoder for brain morphological anomaly detection: Application to the multi-site StratiBip dataset on bipolar disorder in an external validation framework. <em>ARTMED</em>, <em>161</em>, 103063. (<a href='https://doi.org/10.1016/j.artmed.2024.103063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneity of psychiatric disorders makes researching disorder-specific neurobiological markers an ill-posed problem. Here, we face the need for disease stratification models by presenting a generalizable multivariate normative modelling framework for characterizing brain morphology, applied to bipolar disorder (BD). We used deep autoencoders in an anomaly detection framework, combined for the first time with a confounder removal step that integrates training and external validation. The model was trained with healthy control (HC) data from the human connectome project and applied to multi-site external data of HC and BD individuals. We found that brain deviating scores were greater, more heterogeneous, and with increased extreme values in the BD group, with volumes prominently from the basal ganglia, hippocampus, and adjacent regions emerging as significantly deviating. Similarly, individual brain deviating maps based on modified z scores expressed higher abnormalities occurrences, but their overall spatial overlap was lower compared to HCs. Our generalizable framework enabled the identification of brain deviating patterns differing between the subject and the group levels, a step forward towards the development of more effective and personalized clinical decision support systems and patient stratification in psychiatry.},
  archive      = {J_ARTMED},
  author       = {Inês Won Sampaio and Emma Tassi and Marcella Bellani and Francesco Benedetti and Igor Nenadić and Mary L. Phillips and Fabrizio Piras and Lakshmi Yatham and Anna Maria Bianchi and Paolo Brambilla and Eleonora Maggioni},
  doi          = {10.1016/j.artmed.2024.103063},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {103063},
  shortjournal = {Artif. Intell. Med.},
  title        = {A generalizable normative deep autoencoder for brain morphological anomaly detection: Application to the multi-site StratiBip dataset on bipolar disorder in an external validation framework},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECGEFNet: A two-branch deep learning model for calculating left ventricular ejection fraction using electrocardiogram. <em>ARTMED</em>, <em>160</em>, 103065. (<a href='https://doi.org/10.1016/j.artmed.2024.103065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Left ventricular systolic dysfunction (LVSD) and its severity are correlated with the prognosis of cardiovascular diseases. Early detection and monitoring of LVSD are of utmost importance. Left ventricular ejection fraction (LVEF) is an essential indicator for evaluating left ventricular function in clinical practice, the current echocardiography-based evaluation method is not avaliable in primary care and difficult to achieve real-time monitoring capabilities for cardiac dysfunction. We propose a two-branch deep learning model (ECGEFNet) for calculating LVEF using electrocardiogram (ECG), which holds the potential to serve as a primary medical screening tool and facilitate long-term dynamic monitoring of cardiac functional impairments. It integrates original numerical signal and waveform plots derived from the signals in an innovative manner, enabling joint calculation of LVEF by incorporating diverse information encompassing temporal, spatial and phase aspects. To address the inadequate information interaction between the two branches and the lack of efficiency in feature fusion, we propose the fusion attention mechanism (FAT) and the two-branch feature fusion module (BFF) to guide the learning, alignment and fusion of features from both branches. We assemble a large internal dataset and perform experimental validation on it. The accuracy of cardiac dysfunction screening is 92.3%, the mean absolute error (MAE) in LVEF calculation is 4.57%. The proposed model performs well and outperforms existing basic models, and is of great significance for real-time monitoring of the degree of cardiac dysfunction.},
  archive      = {J_ARTMED},
  author       = {Yiqiu Qi and Guangyuan Li and Jinzhu Yang and Honghe Li and Qi Yu and Mingjun Qu and Hongxia Ning and Yonghuai Wang},
  doi          = {10.1016/j.artmed.2024.103065},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103065},
  shortjournal = {Artif. Intell. Med.},
  title        = {ECGEFNet: A two-branch deep learning model for calculating left ventricular ejection fraction using electrocardiogram},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural architecture search for biomedical image classification: A comparative study across data modalities. <em>ARTMED</em>, <em>160</em>, 103064. (<a href='https://doi.org/10.1016/j.artmed.2024.103064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have significantly advanced medical image classification across various modalities and tasks. However, manually designing these networks is often time-consuming and suboptimal. Neural Architecture Search (NAS) automates this process, potentially finding more efficient and effective models. This study provides a comprehensive comparative analysis of our two NAS methods, PBC-NAS and BioNAS, across multiple biomedical image classification tasks using the MedMNIST dataset. Our experiments evaluate these methods based on classification performance (Accuracy (ACC) and Area Under the Curve (AUC)) and computational complexity (Floating Point Operation Counts). Results demonstrate that BioNAS models slightly outperform PBC-NAS models in accuracy, with BioNAS-2 achieving the highest average accuracy of 0.848. However, PBC-NAS models exhibit superior computational efficiency, with PBC-NAS-2 achieving the lowest average FLOPs of 0.82 GB. Both methods outperform state-of-the-art architectures like ResNet-18 and ResNet-50 and AutoML frameworks such as auto-sklearn, AutoKeras, and Google AutoML. Additionally, PBC-NAS and BioNAS outperform other NAS studies in average ACC results (except MSTF-NAS), and show highly competitive results in average AUC. We conduct extensive ablation studies to investigate the impact of architectural parameters, the effectiveness of fine-tuning, search space efficiency, and the discriminative performance of generated architectures. These studies reveal that larger filter sizes and specific numbers of stacks or modules enhance performance. Fine-tuning existing architectures can achieve nearly optimal results without separating NAS for each dataset. Furthermore, we analyze search space efficiency, uncovering patterns in frequently selected operations and architectural choices. This study highlights the strengths and efficiencies of PBC-NAS and BioNAS, providing valuable insights and guidance for future research and practical applications in biomedical image classification.},
  archive      = {J_ARTMED},
  author       = {Zeki Kuş and Musa Aydin and Berna Kiraz and Alper Kiraz},
  doi          = {10.1016/j.artmed.2024.103064},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103064},
  shortjournal = {Artif. Intell. Med.},
  title        = {Neural architecture search for biomedical image classification: A comparative study across data modalities},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Training and validating a treatment recommender with partial verification evidence. <em>ARTMED</em>, <em>160</em>, 103062. (<a href='https://doi.org/10.1016/j.artmed.2024.103062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Current clinical decision support systems (DSS) are trained and validated on observational data from the clinic in which the DSS is going to be applied. This is problematic for treatments that have already been validated in a randomized clinical trial (RCT), but have not yet been introduced in any clinic. In this work, we report on a method for training and validating the DSS core before introduction to a clinic, using the RCT data themselves. The key challenges we address are of missingness, foremost: missing rationale when assigning a treatment to a patient (the assignment is at random), and missing verification evidence, since the effectiveness of a treatment for a patient can only be verified (ground truth) if the treatment was indeed assigned to the patient — but then the assignment was at random. Materials: We use the data of a multi-armed clinical trial that investigated the effectiveness of single treatments and combination treatments for 240+ tinnitus patients recruited and treated in 5 clinical centres. Methods: To deal with the ‘missing rationale for treatment assignment’ challenge, we re-model the target variable that measures the outcome of interest, in order to suppress the effect of the individual treatment, which was at random, and control on the effect of treatment in general. To deal with missing features for many patients, we use a learning core that is robust to missing features. Further, we build ensembles that parsimoniously exploit the small patient numbers we have for learning. To deal with the ‘missing verification evidence’ challenge, we introduce counterfactual treatment verification , a verification scheme that juxtaposes the effectiveness of the recommendations of our approach to the effectiveness of the RCT assignments in the cases of agreement/disagreement between the two. Results and limitations: We demonstrate that our approach leverages the RCT data for learning and verification, by showing that the DSS suggests treatments that improve the outcome. The results are limited through the small number of patients per treatment; while our ensemble is designed to mitigate this effect, the predictive performance of the methods is affected by the smallness of the data. Outlook: We provide a basis for the establishment of decision supporting routines on treatments that have been tested in RCTs but have not yet been deployed clinically. Practitioners can use our approach to train and validate a DSS on new treatments by simply using the RCT data available to them. More work is needed to strengthen the robustness of the predictors. Since there are no further data available to this purpose, but those already used, the potential of synthetic data generation seems an appropriate alternative.},
  archive      = {J_ARTMED},
  author       = {Vishnu Unnikrishnan and Clara Puga and Miro Schleicher and Uli Niemann and Berthold Langguth and Stefan Schoisswohl and Birgit Mazurek and Rilana Cima and Jose Antonio Lopez-Escamez and Dimitris Kikidis and Eleftheria Vellidou and Rüdiger Pryss and Winfried Schlee and Myra Spiliopoulou},
  doi          = {10.1016/j.artmed.2024.103062},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103062},
  shortjournal = {Artif. Intell. Med.},
  title        = {Training and validating a treatment recommender with partial verification evidence},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fraud detection in healthcare claims using machine learning: A systematic review. <em>ARTMED</em>, <em>160</em>, 103061. (<a href='https://doi.org/10.1016/j.artmed.2024.103061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: Identifying fraud in healthcare programs is crucial, as an estimated 3%–10% of the total healthcare expenditures are lost to fraudulent activities. This study presents a systematic literature review of machine learning techniques applied to fraud detection in health insurance claims. We aim to analyze the data and methodologies documented in the literature over the past two decades, providing insights into research challenges and opportunities. Methods: We identified research studies on health insurance fraud detection using machine learning approaches from databases such as Google Scholar, Springer-Link journals, Elsevier, PubMed, Excerpta Medica Database (EMBASE), Scopus, the Association for Computing Machinery (ACM) Digital Library, and the Institute of Electrical and Electronics Engineers (IEEE) Xplore Digital Library. We included only articles that presented experimental results of machine learning-based approaches applied to healthcare claims. From the reviewed articles, 137 were selected for the final qualitative and quantitative analyses. Results: In recent years, there has been a surge in publications centered on the use of machine learning to detect health insurance fraud. Among these studies, those focused on the detection of fraud committed by healthcare providers was the most prevalent, followed by fraud committed by patients. A wide variety of machine learning algorithms are highlighted in these studies, ranging from unsupervised (41 studies) and supervised methods (94 studies), to hybrid approaches (12 studies). While traditional machine learning approaches remain dominant in this research area, the adoption of advanced deep learning techniques is on the rise. Considering the type of healthcare claims data used, 30 studies utilized private data sources, while the rest used publicly available datasets. Data from 16 countries were utilized, with a majority coming from the United States (96 studies), followed by China (11 studies) and Australia (5 studies). Discussion and Conclusion: Detecting fraud in healthcare claims using machine learning presents several challenges. These include inconsistent data, absence of data standardization and integration, privacy concerns, and a limited number of labeled fraudulent cases to train models on. Future work should focus on enhancing transparency in data preparation, promoting the sharing of fraud investigation outcomes by authorities, and developing benchmark datasets to enhance accessibility and comparability. Furthermore, innovative techniques in data sampling, feature encoding methods for training machine learning models, and exploring the latest advancements in deep learning can significantly advance research in health insurance fraud detection.},
  archive      = {J_ARTMED},
  author       = {Anli du Preez and Sanmitra Bhattacharya and Peter Beling and Edward Bowen},
  doi          = {10.1016/j.artmed.2024.103061},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103061},
  shortjournal = {Artif. Intell. Med.},
  title        = {Fraud detection in healthcare claims using machine learning: A systematic review},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CircWaveDL: Modeling of optical coherence tomography images based on a new supervised tensor-based dictionary learning for classification of macular abnormalities. <em>ARTMED</em>, <em>160</em>, 103060. (<a href='https://doi.org/10.1016/j.artmed.2024.103060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling Optical Coherence Tomography (OCT) images is crucial for numerous image processing applications and aids ophthalmologists in the early detection of macular abnormalities. Sparse representation-based models, particularly dictionary learning (DL), play a pivotal role in image modeling. Traditional DL methods often transform higher-order tensors into vectors and then aggregate them into a matrix, which overlooks the inherent multi-dimensional structure of the data. To address this limitation, tensor-based DL approaches have been introduced. In this study, we present a novel tensor-based DL algorithm, CircWaveDL, for OCT classification, where both the training data and the dictionary are modeled as higher-order tensors. We named our approach CircWaveDL to reflect the use of CircWave atoms for dictionary initialization, rather than random initialization. CircWave has previously shown effectiveness in OCT classification, making it a fitting basis function for our DL method. The algorithm employs CANDECOMP/PARAFAC (CP) decomposition to factorize each tensor into lower dimensions. We then learn a sub-dictionary for each class using its respective training tensor. For testing, a test tensor is reconstructed with each sub-dictionary, and each test B-scan is assigned to the class that yields the minimal residual error. To evaluate the model's generalizability, we tested it across three distinct databases. Additionally, we introduce a new heatmap generation technique based on averaging the most significant atoms of the learned sub-dictionaries. This approach highlights that selecting an appropriate sub-dictionary for reconstructing test B-scans improves reconstructions, emphasizing the distinctive features of different classes. CircWaveDL demonstrated strong generalizability across external validation datasets, outperforming previous classification methods. It achieved accuracies of 92.5 %, 86.1 %, and 89.3 % on datasets 1, 2, and 3, respectively, showcasing its efficacy in OCT image classification.},
  archive      = {J_ARTMED},
  author       = {Roya Arian and Alireza Vard and Rahele Kafieh and Gerlind Plonka and Hossein Rabbani},
  doi          = {10.1016/j.artmed.2024.103060},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103060},
  shortjournal = {Artif. Intell. Med.},
  title        = {CircWaveDL: Modeling of optical coherence tomography images based on a new supervised tensor-based dictionary learning for classification of macular abnormalities},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer model for gait assessments in parkinson's patients using a fuzzy inference model and inertial sensors. <em>ARTMED</em>, <em>160</em>, 103059. (<a href='https://doi.org/10.1016/j.artmed.2024.103059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients with Parkinson's disease (PD) in the moderate and severe stages can present several walk alterations. They can show slow movements and difficulty initiating, varying, or interrupting their gait; freezing; short steps; speed changes; shuffling; little arm swing; and festinating gait. The Movement Disorder Society-Unified Parkinson's Disease Rating Scale (MDS-UPDRS) has a good reputation for uniformly evaluating motor and non-motor aspects of PD. However, the motor clinical assessment depends on visual observations, the results are qualitative, and subtle differences are not identified. This study presents a fuzzy inference model for gait assessments in PD patients with detailed descriptions of signal processing and eight biomechanical indicators computations; as such, other authors can replicate the presented methods. The computer model uses 334 bilateral measurements of 58 Parkinson's patients and 15 healthy control subjects performed over one year. The computer model validations are based on physician evaluations in real-time and post-analysis using an extensive database of videos and signals. The assessment results are explainable, quantitative, and qualitative, increasing their acceptance and use in clinical environments. The computer system design considers three expert motor evaluations, including the PD patients' evolutions; this facilitates correlation with medication doses and appropriate intervals for follow-up medical consultations. The assessments include three qualitative gait conditions of MDS-UPDRS—normal, slight, and mild—as well as a numerical evaluation of up to two decimal places.},
  archive      = {J_ARTMED},
  author       = {Luis Pastor Sánchez-Fernández and Luis Alejandro Sánchez-Pérez and Juan Manuel Martínez-Hernández},
  doi          = {10.1016/j.artmed.2024.103059},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103059},
  shortjournal = {Artif. Intell. Med.},
  title        = {Computer model for gait assessments in parkinson's patients using a fuzzy inference model and inertial sensors},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and use of a denoising convolutional autoencoder for reconstructing electrocardiogram signals at super resolution. <em>ARTMED</em>, <em>160</em>, 103058. (<a href='https://doi.org/10.1016/j.artmed.2024.103058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiogram signals play a pivotal role in cardiovascular diagnostics, providing essential information on electrical hearth activity. However, inherent noise and limited resolution can hinder an accurate interpretation of the recordings. In this paper an advanced Denoising Convolutional Autoencoder designed to process electrocardiogram signals, generating super-resolution reconstructions is proposed; this is followed by in-depth analysis of the enhanced signals. The autoencoder receives a signal window (of 5 s) sampled at 50 Hz (low resolution) as input and reconstructs a denoised super-resolution signal at 500 Hz. The proposed autoencoder is applied to publicly available datasets, demonstrating optimal performance in reconstructing high-resolution signals from very low-resolution inputs sampled at 50 Hz. The results were then compared with current state-of-the-art for electrocardiogram super-resolution, demonstrating the effectiveness of the proposed method. The method achieves a signal-to-noise ratio of 12.20 dB, a mean squared error of 0.0044, and a root mean squared error of 4.86%, which significantly outperforms current state-of-the-art alternatives. This framework can effectively enhance hidden information within signals, aiding in the detection of heart-related diseases.},
  archive      = {J_ARTMED},
  author       = {Ugo Lomoio and Pierangelo Veltri and Pietro Hiram Guzzi and Pietro Liò},
  doi          = {10.1016/j.artmed.2024.103058},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103058},
  shortjournal = {Artif. Intell. Med.},
  title        = {Design and use of a denoising convolutional autoencoder for reconstructing electrocardiogram signals at super resolution},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review on the roles of remote diagnosis in telemedicine system: Coherent taxonomy, insights, recommendations, and open research directions for intelligent healthcare solutions. <em>ARTMED</em>, <em>160</em>, 103057. (<a href='https://doi.org/10.1016/j.artmed.2024.103057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The term ‘remote diagnosis’ in telemedicine describes the procedure wherein medical practitioners diagnose patients remotely by using telecommunications technology. With this method, patients can obtain medical care without having to physically visit a hospital, which can be helpful for people who live in distant places or have restricted mobility. When people in the past had health issues, they were usually sent to the hospital, where they received clinical examinations, diagnoses, and treatment at the facility. Thus, hospitals were overcrowded because of the increase in the number of patients or in the death of some very ill patients given that the completion of medical operations required a significant amount of time. Objective This research aims to provide a literature review study and an in-depth analysis to (1) investigate the procedure and roles of remote diagnosis in telemedicine; (2) review the technical tools and technologies used in remote diagnosis; (3) review the diseases diagnosed remotely in telemedicine; (4) compose a crossover taxonomy among diseases, technologies, and telemedicine; (5) present lists of input variables, vital signs, data and output decisions already applied in remote diagnosis; (6) Summarize the performance assessment measures utilized to assess and validate remote diagnosis models; and (7) identify and categorize open research issues while providing recommendations for future advancements in intelligent remote diagnosis within telemedicine systems. Methods A systematic search was conducted using online libraries for articles published from 1 January 2016 to 13 September 2023 in IEEE, PubMed, Science Direct, Springer, and Web of Science. Notably, searches were limited to articles in the English language. The papers examine remote diagnosis in telemedicine, the technologies employed for this function, and the ramifications of diagnosing patients outside hospital settings. Each selected study was synthesized to furnish proof about the implementation of remote diagnostics in telemedicine. Results A new crossover taxonomy between the most important diagnosed diseases and technologies used for this purpose and their relationship with telemedicine tiers is proposed. The functions executed at each tier are elucidated. Additionally, a compilation of diagnostic technologies is provided. Additionally, open research difficulties, advantages of remote diagnosis in telemedicine, and suggestions for future research prospects that require attention are systematically organized and presented. Conclusions This study reviews the role of remote diagnosis in telemedicine, with a focus on key technologies and current approaches. This study highlights research challenges, provides recommendations for future directions, and addresses research gaps and limitations to provide a clear vision of remote diagnosis in telemedicine. This study emphasizes the advantages of existing research and opens the possibility for new directions and smart healthcare solutions.},
  archive      = {J_ARTMED},
  author       = {Sura Saad Mohsin and Omar H. Salman and Abdulrahman Ahmed Jasim and Mohammed A. Al-Nouman and Ammar Riadh Kairaldeen},
  doi          = {10.1016/j.artmed.2024.103057},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103057},
  shortjournal = {Artif. Intell. Med.},
  title        = {A systematic review on the roles of remote diagnosis in telemedicine system: Coherent taxonomy, insights, recommendations, and open research directions for intelligent healthcare solutions},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransformerLSR: Attentive joint model of longitudinal data, survival, and recurrent events with concurrent latent structure. <em>ARTMED</em>, <em>160</em>, 103056. (<a href='https://doi.org/10.1016/j.artmed.2024.103056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In applications such as biomedical studies, epidemiology, and social sciences, recurrent events often co-occur with longitudinal measurements and a terminal event, such as death. Therefore, jointly modeling longitudinal measurements, recurrent events, and survival data while accounting for their dependencies is critical. While joint models for the three components exist in statistical literature, many of these approaches are limited by heavy parametric assumptions and scalability issues. Recently, incorporating deep learning techniques into joint modeling has shown promising results. However, current methods only address joint modeling of longitudinal measurements at regularly-spaced observation times and survival events, neglecting recurrent events. In this paper, we develop TransformerLSR, a flexible transformer-based deep modeling and inference framework to jointly model all three components simultaneously. TransformerLSR integrates deep temporal point processes into the joint modeling framework, treating recurrent and terminal events as two competing processes dependent on past longitudinal measurements and recurrent event times. Additionally, TransformerLSR introduces a novel trajectory representation and model architecture to potentially incorporate a priori knowledge of known latent structures among concurrent longitudinal variables. We demonstrate the effectiveness and necessity of TransformerLSR through simulation studies and analyzing a real-world medical dataset on patients after kidney transplantation.},
  archive      = {J_ARTMED},
  author       = {Zhiyue Zhang and Yao Zhao and Yanxun Xu},
  doi          = {10.1016/j.artmed.2024.103056},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103056},
  shortjournal = {Artif. Intell. Med.},
  title        = {TransformerLSR: Attentive joint model of longitudinal data, survival, and recurrent events with concurrent latent structure},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concordance-based predictive uncertainty (CPU)-index: Proof-of-concept with application towards improved specificity of lung cancers on low dose screening CT. <em>ARTMED</em>, <em>160</em>, 103055. (<a href='https://doi.org/10.1016/j.artmed.2024.103055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel concordance-based predictive uncertainty (CPU)-Index, which integrates insights from subgroup analysis and personalized AI time-to-event models. Through its application in refining lung cancer screening (LCS) predictions generated by an individualized AI time-to-event model trained with fused data of low dose CT (LDCT) radiomics with patient demographics, we demonstrate its effectiveness, resulting in improved risk assessment compared to the Lung CT Screening Reporting & Data System (Lung-RADS). Subgroup-based Lung-RADS faces challenges in representing individual variations and relies on a limited set of predefined characteristics, resulting in variable predictions. Conversely, personalized AI time-to-event models are hindered by transparency issues and biases from censored data. By measuring the prediction consistency between subgroup analysis and AI time-to-event models, the CPU-Index framework offers a nuanced evaluation of the bias–variance trade-off and improves the transparency and reliability of predictions. Consistency was estimated by the concordance index of subgroup analysis-based similarity rank and model prediction similarity rank. Subgroup analysis-based similarity loss was defined as the sum-of-the-difference between Lung-RADS and feature-level 0-1 loss. Model prediction similarity loss was defined as squared loss. To test our approach, we identified 3,326 patients who underwent LDCT for LCS from 1/1/2015 to 6/30/2020 with confirmation of lung cancer on pathology within one year. For each LDCT image, the lesion associated with a Lung-RADS score was detected using a pretrained deep learning model from Medical Open Network for AI (MONAI), from which radiomic features were extracted. Radiomics were optimally fused with patient demographics via a positional encoding scheme and used to train a neural multi-task logistic regression time-to-event model that predicts malignancy. Performance was maximized when radiomics features were fused with positionally encoded demographic features. In this configuration, our algorithm raised the AUC from 0.81 ± 0.04 to 0.89 ± 0.02. Compared to standard Lung-RADS, our approach reduced the False-Positive-Rate from 0.41 ± 0.02 to 0.30 ± 0.12 while maintaining the same False-Negative-Rate. Our methodology enhances lung cancer risk assessment by estimating prediction uncertainty and adjusting accordingly. Furthermore, the optimal integration of radiomics and patient demographics improved overall diagnostic performance, indicating their complementary nature.},
  archive      = {J_ARTMED},
  author       = {Yuqi Wang and Aarzu Gupta and Fakrul Islam Tushar and Breylon Riley and Avivah Wang and Tina D. Tailor and Stacy Tantum and Jian-Guo Liu and Mustafa R. Bashir and Joseph Y. Lo and Kyle J. Lafata},
  doi          = {10.1016/j.artmed.2024.103055},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103055},
  shortjournal = {Artif. Intell. Med.},
  title        = {Concordance-based predictive uncertainty (CPU)-index: Proof-of-concept with application towards improved specificity of lung cancers on low dose screening CT},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ItpCtrl-AI: End-to-end interpretable and controllable artificial intelligence by modeling radiologists’ intentions. <em>ARTMED</em>, <em>160</em>, 103054. (<a href='https://doi.org/10.1016/j.artmed.2024.103054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using Deep Learning in computer-aided diagnosis systems has been of great interest due to its impressive performance in the general domain and medical domain. However, a notable challenge is the lack of explainability of many advanced models, which poses risks in critical applications such as diagnosing findings in CXR. To address this problem, we propose ItpCtrl-AI, a novel end-to-end interpretable and controllable framework that mirrors the decision-making process of the radiologist . By emulating the eye gaze patterns of radiologists, our framework initially determines the focal areas and assesses the significance of each pixel within those regions. As a result, the model generates an attention heatmap representing radiologists’ attention, which is then used to extract attended visual information to diagnose the findings. By allowing the directional input, our framework is controllable by the user. Furthermore, by displaying the eye gaze heatmap which guides the diagnostic conclusion, the underlying rationale behind the model’s decision is revealed, thereby making it interpretable. In addition to developing an interpretable and controllable framework, our work includes the creation of a dataset, named Diagnosed-Gaze++, which aligns medical findings with eye gaze data. Our extensive experimentation validates the effectiveness of our approach in generating accurate attention heatmaps and diagnoses. The experimental results show that our model not only accurately identifies medical findings but also precisely produces the eye gaze attention of radiologists. The dataset, models, and source code will be made publicly available upon acceptance.},
  archive      = {J_ARTMED},
  author       = {Trong-Thang Pham and Jacob Brecheisen and Carol C. Wu and Hien Nguyen and Zhigang Deng and Donald Adjeroh and Gianfranco Doretto and Arabinda Choudhary and Ngan Le},
  doi          = {10.1016/j.artmed.2024.103054},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103054},
  shortjournal = {Artif. Intell. Med.},
  title        = {ItpCtrl-AI: End-to-end interpretable and controllable artificial intelligence by modeling radiologists’ intentions},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangled global and local features of multi-source data variational autoencoder: An interpretable model for diagnosing IgAN via multi-source raman spectral fusion techniques. <em>ARTMED</em>, <em>160</em>, 103053. (<a href='https://doi.org/10.1016/j.artmed.2024.103053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single Raman spectrum reflects limited molecular information. Effective fusion of the Raman spectra of serum and urine source domains helps to obtain richer feature information. However, most of the current studies on immunoglobulin A nephropathy (IgAN) based on Raman spectroscopy are based on small sample data and low signal-to-noise ratio. If a multi-source data fusion strategy is directly adopted, it may even reduce the accuracy of disease diagnosis. To this end, this paper proposes a data enhancement and spectral optimization method based on variational autoencoders to obtain reconstructed Raman spectra with doubled sample size and improved signal-to-noise ratio. In the diagnosis of IgAN in multi-source domain Raman spectra, this paper builds a global and local feature decoupled variational autoencoder (DMSGL-VAE) model based on multi-source data. First, the statistical features after spectral segmentation are extracted, and the latent variables obtained by the variational encoder are decoupled through the decoupling module. The global representation and local representation obtained represent the global shared information and local unique information of the serum and urine source domains, respectively. Then, the cross-source reconstruction loss and decoupling loss are used to constrain the decoupling, and the effectiveness of the decoupling is proved quantitatively and qualitatively. Finally, the features of different source domains were integrated to diagnose IgAN, and the results were analyzed for important features using the SHapley Additive exPlanations algorithm. The experimental results showed that the AUC value of the DMSGL-VAE model for diagnosing IgAN on the test set was as high as 0.9958. The SHAP algorithm was used to further prove that proteins, hydroxybutyrate, and guanine are likely to be common biological fingerprint substances for the diagnosis of IgAN by serum and urine Raman spectroscopy. In summary, the DMSGL-VAE model designed based on Raman spectroscopy in this paper can achieve rapid, non-invasive, and accurate screening of IgAN in terms of classification performance. And interpretable analysis may help doctors further understand IgAN and make more efficient diagnostic measures in the future.},
  archive      = {J_ARTMED},
  author       = {Wei Shuai and Xuecong Tian and Enguang Zuo and Xueqin Zhang and Chen Lu and Jin Gu and Chen Chen and Xiaoyi Lv and Cheng Chen},
  doi          = {10.1016/j.artmed.2024.103053},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103053},
  shortjournal = {Artif. Intell. Med.},
  title        = {Disentangled global and local features of multi-source data variational autoencoder: An interpretable model for diagnosing IgAN via multi-source raman spectral fusion techniques},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-enabled clinical decision support tools for mental healthcare: A product review. <em>ARTMED</em>, <em>160</em>, 103052. (<a href='https://doi.org/10.1016/j.artmed.2024.103052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The review seeks to promote transparency in the availability of regulated AI-enabled Clinical Decision Support Systems (AI-CDSS) for mental healthcare. From 84 potential products, seven fulfilled the inclusion criteria. The products can be categorized into three major areas: diagnosis of autism spectrum disorder (ASD) based on clinical history, behavioral, and eye-tracking data; diagnosis of multiple disorders based on conversational data; and medication selection based on clinical history and genetic data. We found five scientific articles evaluating the devices' performance and external validity. The average completeness of reporting, indicated by 52 % adherence to the Consolidated Standards of Reporting Trials Artificial Intelligence (CONSORT-AI) checklist, was modest, signaling room for improvement in reporting quality. Our findings stress the importance of obtaining regulatory approval, adhering to scientific standards, and staying up-to-date with the latest changes in the regulatory landscape. Refining regulatory guidelines and implementing effective tracking systems for AI-CDSS could enhance transparency and oversight in the field.},
  archive      = {J_ARTMED},
  author       = {Anne-Kathrin Kleine and Eesha Kokje and Pia Hummelsberger and Eva Lermer and Insa Schaffernak and Susanne Gaube},
  doi          = {10.1016/j.artmed.2024.103052},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103052},
  shortjournal = {Artif. Intell. Med.},
  title        = {AI-enabled clinical decision support tools for mental healthcare: A product review},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of radiological decision errors from longitudinal analysis of gaze and image features. <em>ARTMED</em>, <em>160</em>, 103051. (<a href='https://doi.org/10.1016/j.artmed.2024.103051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging, particularly radiography, is an indispensable part of diagnosing many chest diseases. Final diagnoses are made by radiologists based on images, but the decision-making process is always associated with a risk of incorrect interpretation. Incorrectly interpreted data can lead to delays in treatment, a prescription of inappropriate therapy, or even a completely missed diagnosis. In this context, our study aims to determine whether it is possible to predict diagnostic errors made by radiologists using eye-tracking technology. For this purpose, we asked 4 radiologists with different levels of experience to analyze 1000 images covering a wide range of chest diseases. Using eye-tracking data, we calculated the radiologists’ gaze fixation points and generated feature vectors based on this data to describe the radiologists’ gaze behavior during image analysis. Additionally, we emulated the process of revealing the read images following radiologists’ gaze data to create a more comprehensive picture of their analysis. Then we applied a recurrent neural network to predict diagnostic errors. Our results showed a 0.7755 ROC AUC score, demonstrating a significant potential for this approach in enhancing the accuracy of diagnostic error recognition.},
  archive      = {J_ARTMED},
  author       = {Anna Anikina and Diliara Ibragimova and Tamerlan Mustafaev and Claudia Mello-Thoms and Bulat Ibragimov},
  doi          = {10.1016/j.artmed.2024.103051},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103051},
  shortjournal = {Artif. Intell. Med.},
  title        = {Prediction of radiological decision errors from longitudinal analysis of gaze and image features},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Glaucoma detection: Binocular approach and clinical data in machine learning. <em>ARTMED</em>, <em>160</em>, 103050. (<a href='https://doi.org/10.1016/j.artmed.2024.103050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a multi-modal machine learning method to automate early glaucoma diagnosis. The proposed methodology introduces two novel aspects for automated diagnosis not previously explored in the literature: simultaneous use of ocular fundus images from both eyes and integration with the patient’s additional clinical data. We begin by establishing a baseline, termed monocular mode , which adheres to the traditional approach of considering the data from each eye as a separate instance. We then explore the binocular mode , investigating how combining information from both eyes of the same patient can enhance glaucoma diagnosis accuracy. This exploration employs the PAPILA dataset, comprising information from both eyes, clinical data, ocular fundus images, and expert segmentation of these images. Additionally, we compare two image-derived data modalities: direct ocular fundus images and morphological data from manual expert segmentation. Our method integrates Gradient-Boosted Decision Trees (GBDT) and Convolutional Neural Networks (CNN), specifically focusing on the MobileNet, VGG16, ResNet-50, and Inception models. SHAP values are used to interpret GBDT models, while the Deep Explainer method is applied in conjunction with SHAP to analyze the outputs of convolutional-based models. Our findings show the viability of considering both eyes, which improves the model performance. The binocular approach, incorporating information from morphological and clinical data yielded an AUC of 0.796 ( ± 0 . 003 at a 95% confidence interval), while the CNN, using the same approach (both eyes), achieved an AUC of 0.764 ( ± 0 . 005 at a 95% confidence interval).},
  archive      = {J_ARTMED},
  author       = {Oleksandr Kovalyk-Borodyak and Juan Morales-Sánchez and Rafael Verdú-Monedero and José-Luis Sancho-Gómez},
  doi          = {10.1016/j.artmed.2024.103050},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103050},
  shortjournal = {Artif. Intell. Med.},
  title        = {Glaucoma detection: Binocular approach and clinical data in machine learning},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrinsic-dimension analysis for guiding dimensionality reduction and data fusion in multi-omics data processing. <em>ARTMED</em>, <em>160</em>, 103049. (<a href='https://doi.org/10.1016/j.artmed.2024.103049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-omics data have revolutionized biomedical research by providing a comprehensive understanding of biological systems and the molecular mechanisms of disease development. However, analyzing multi-omics data is challenging due to high dimensionality and limited sample sizes, necessitating proper data-reduction pipelines to ensure reliable analyses. Additionally, its multimodal nature requires effective data-integration pipelines. While several dimensionality reduction and data fusion algorithms have been proposed, crucial aspects are often overlooked. Specifically, the choice of projection space dimension is typically heuristic and uniformly applied across all omics, neglecting the unique high dimension small sample size challenges faced by individual omics. This paper introduces a novel multi-modal dimensionality reduction pipeline tailored to individual views. By leveraging intrinsic dimensionality estimators, we assess the curse-of-dimensionality impact on each view and propose a two-step reduction strategy for significantly affected views, combining feature selection with feature extraction. Compared to traditional uniform reduction pipelines in a crucial and supervised multi-omics analysis setting, our approach shows significant improvement. Additionally, we explore three effective unsupervised multi-omics data fusion methods rooted in the main data fusion strategies to gain insights into their performance under crucial, yet overlooked, settings.},
  archive      = {J_ARTMED},
  author       = {Jessica Gliozzo and Mauricio Soto-Gomez and Valentina Guarino and Arturo Bonometti and Alberto Cabri and Emanuele Cavalleri and Justin Reese and Peter N. Robinson and Marco Mesiti and Giorgio Valentini and Elena Casiraghi},
  doi          = {10.1016/j.artmed.2024.103049},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103049},
  shortjournal = {Artif. Intell. Med.},
  title        = {Intrinsic-dimension analysis for guiding dimensionality reduction and data fusion in multi-omics data processing},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rough hypervolume-driven feature selection with groupwise intelligent sampling for detecting clinical characterization of lupus nephritis. <em>ARTMED</em>, <em>160</em>, 103042. (<a href='https://doi.org/10.1016/j.artmed.2024.103042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systemic lupus erythematosus (SLE) is an autoimmune inflammatory disease. Lupus nephritis (LN) is a major risk factor for morbidity and mortality in SLE. Proliferative and pure membranous LN have different prognoses and may require different treatments. This study proposes a binary rough hypervolume-driven spherical evolution algorithm with groupwise intelligent sampling (bRGSE). The efficient dimensionality reduction capability of the bRGSE is verified across twelve datasets. These datasets are from the public datasets, with feature dimensions ranging from seven hundred to fifty thousand. The experimental results indicate that bRGSE performs better than seven high-performing alternatives. Then, the bRGSE was combined with adaptive boosting (AdaBoost) to form a new model (bRGSE_AdaBoost), which analyzed clinical records collected from 110 patients with LN. Experimental results show that the proposed bRGSE_AdaBoost can identify the most critical indicators, including urine latent blood, white blood cells, endogenous creatinine clearing rate, and age. These indicators may help differentiate between proliferative LN and membranous LN. The proposed bRGSE algorithm is an efficient dimensionality reduction method. The developed bRGSE_AdaBoost model, a computer-aided model, achieved an accuracy of 96.687 % and is expected to provide early warning for the treatment and diagnosis of LN.},
  archive      = {J_ARTMED},
  author       = {Xinsen Zhou and Yi Chen and Ali Asghar Heidari and Huiling Chen and Xiaowei Chen},
  doi          = {10.1016/j.artmed.2024.103042},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103042},
  shortjournal = {Artif. Intell. Med.},
  title        = {Rough hypervolume-driven feature selection with groupwise intelligent sampling for detecting clinical characterization of lupus nephritis},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LCDL: Classification of ICD codes based on disease label co-occurrence dependency and LongFormer with medical knowledge. <em>ARTMED</em>, <em>160</em>, 103041. (<a href='https://doi.org/10.1016/j.artmed.2024.103041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical coding involves assigning codes to clinical free-text documents, specifically medical records that average over 3,000 markers, in order to track patient diagnoses and treatments. This is typically accomplished through manual assignments by healthcare professionals. To improve efficiency and accuracy while reducing the workload on these professionals, researchers have employed a multi-label classification approach. Since the long-tail phenomenon impacts tens of thousands of ICD codes, whereby only a few codes (representative of common diseases) are frequently assigned, while the majority of codes (representative of rare diseases) are infrequently assigned, this paper presents an LCDL model that addresses the challenge at hand by examining the LongFormer pre-trained language model and the disease label co-occurrence map. To enhance the performance of automated medical coding in the biomedical domain, hierarchies with medical knowledge, synonyms and abbreviations are introduced, improving the medical knowledge representation. Test evaluations are extensively conducted on the benchmark dataset MIMIC-III, and obtained the competitive performance compared to the previous state-of-the-art methods.},
  archive      = {J_ARTMED},
  author       = {Yumeng Yang and Hongfei Lin and Zhihao Yang and Yijia Zhang and Di Zhao and Ling Luo},
  doi          = {10.1016/j.artmed.2024.103041},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103041},
  shortjournal = {Artif. Intell. Med.},
  title        = {LCDL: Classification of ICD codes based on disease label co-occurrence dependency and LongFormer with medical knowledge},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic classification of HEp-2 specimens by explainable deep learning and jensen-shannon reliability index. <em>ARTMED</em>, <em>160</em>, 103030. (<a href='https://doi.org/10.1016/j.artmed.2024.103030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Anti-Nuclear Antibodies (ANA) test using Human Epithelial type 2 (HEp-2) cells in the Indirect Immuno-Fluorescence (IIF) assay protocol is considered the gold standard for detecting Connective Tissue Diseases. Computer-assisted systems for HEp-2 image analysis represent a growing field that harnesses the potential offered by novel machine learning techniques to address the classification of HEp-2 images and ANA patterns. In this study, we introduce an innovative platform based on transfer learning with pre-trained deep learning models. This platform combines the power of unsupervised deep description of HEp-2 images, a novel feature selection approach designed for unbalanced datasets, and independent testing using two distinct datasets from different hospitals to tackle cross-hardware compatibility issues. To enhance the trustworthiness of our method, we also present a modified version of gradient-weighted class activation mapping for regional explainability and introduce a new sample quality index based on the Jensen-Shannon divergence to enhance method reliability and quantify sample heterogeneity. The results we provide demonstrate exceptionally high performance in intensity and ANA pattern recognition when compared to state-of-the-art approaches. Our method's ability to eliminate the need for cell segmentation in favor of statistical analysis of the sample makes it applicable, robust, and versatile. Our future work will focus on addressing the challenge of mitotic spindle recognition by expanding our proposed approach to cover mixed patterns.},
  archive      = {J_ARTMED},
  author       = {A. Mencattini and T. Tocci and M. Nuccetelli and M. Pieri and S. Bernardini and E. Martinelli},
  doi          = {10.1016/j.artmed.2024.103030},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103030},
  shortjournal = {Artif. Intell. Med.},
  title        = {Automatic classification of HEp-2 specimens by explainable deep learning and jensen-shannon reliability index},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in diagnosis and prognosis of bacteraemia, bloodstream infection, and sepsis using machine learning: A comprehensive living literature review. <em>ARTMED</em>, <em>160</em>, 103008. (<a href='https://doi.org/10.1016/j.artmed.2024.103008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Blood-related infections are a significant concern in healthcare. They can lead to serious medical complications and even death if not promptly diagnosed and treated. Throughout time, medical research has sought to identify clinical factors and strategies to improve the management of these conditions. The increasing adoption of electronic health records has led to a wealth of electronically available medical information and predictive models have emerged as invaluable tools. This manuscript offers a detailed survey of machine-learning techniques used for the diagnosis and prognosis of bacteraemia, bloodstream infections, and sepsis shedding light on their efficacy, potential limitations, and the intricacies of their integration into clinical practice. Methods: This study presents a comprehensive analysis derived from a thorough search across prominent databases, namely EMBASE, Google Scholar, PubMed, Scopus, and Web of Science, spanning from their inception dates to October 25, 2023. Eligibility assessment was conducted independently by investigators, with inclusion criteria encompassing peer-reviewed articles and pertinent non-peer-reviewed literature. Clinical and technical data were meticulously extracted and integrated into a registry, facilitating a holistic examination of the subject matter. To maintain currency and comprehensiveness, readers are encouraged to contribute manuscript suggestions and/or reports for integration into this living registry. Results: While machine learning (ML) models exhibit promise in advanced disease stages such as sepsis, early stages remain underexplored due to data limitations. Biochemical markers emerge as pivotal predictors during early stages such as bacteraemia, or bloodstream infections, while vital signs assume significance in sepsis prognosis. Integrating temporal trend information into conventional machine learning models appears to enhance performance. Unfortunately, sequential deep learning models face challenges, showing minimal performance improvements and significant drops in external datasets, potentially due to learning missing patterns within the scarce data available rather than understanding disease dynamics. Real-life implementation receives limited attention, as meeting design requirements proves challenging within existing healthcare infrastructure. The data collected in an event-based fashion during clinical practice is insufficient to fully harness the potential of these data-hungry models. Despite limitations, opportunities abound in leveraging flexible models and exploiting real-time non-invasive data collection technologies such as wearable devices or microneedles. Addressing research gaps in early disease stages, harnessing patient history data often underused, and embracing continual diagnostics beyond treatment initiation are crucial for improving healthcare decision-making support and adoption across the entire management pathway. Conclusions: This comprehensive survey illuminates the landscape of ML applications in blood-related infection management, offering insights for future research and clinical practice. Implementing clinical ML-based clinical decision support systems requires balancing research with practical considerations. Current methodologies often lead to complex models lacking transparency and practical validation. Integration into healthcare systems faces regulatory, privacy, and trust challenges. Clear presentations and adherence to standards are essential to boost confidence in machine learning models for real-world healthcare applications.},
  archive      = {J_ARTMED},
  author       = {Hernandez B. and Ming D.K. and Rawson T.M. and Bolton W. and Wilson R. and Vasikasin V. and Daniels J. and Rodriguez-Manzano J. and Davies F.J. and Georgiou P. and Holmes A.H.},
  doi          = {10.1016/j.artmed.2024.103008},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {103008},
  shortjournal = {Artif. Intell. Med.},
  title        = {Advances in diagnosis and prognosis of bacteraemia, bloodstream infection, and sepsis using machine learning: A comprehensive living literature review},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Electronic health records-based identification of newly diagnosed crohn’s disease cases. <em>ARTMED</em>, <em>159</em>, 103032. (<a href='https://doi.org/10.1016/j.artmed.2024.103032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Early diagnosis and treatment of Crohn’s Disease are associated with decreased risk of surgery and complications. However, diagnostic delay is frequently seen in clinical practice. To better understand Crohn’s Disease risk factors and disease indicators, we identified, described, and predicted incident Crohn’s Disease patients based on the Electronic Health Record data of the Mount Sinai Health System. Methods: We developed two phenotyping algorithms based on structured Electronic Health Record data (i.e., coded diagnosis, medication prescription, and healthcare utilization), and a more simple and advanced approach of information extraction from clinical notes, including data between 2011 and 2023. We conducted an ablation study for the classification task using different models, prediction time points, data inputs, text encoding methods, and case-control matching variables. Results: We identified 247 incident Crohn’s Disease cases and 1221 matched controls and validated our cohorts through manual chart review. A second control cohort (n = 1235) was created without matching on race. Gastrointestinal symptoms were significantly overrepresented in cases at least 180 days before the first coded Crohn’s Disease diagnosis. Adding text-based features to the clinical prediction models increased their overall performances. However, adding race as a matching variable had more effects on the model performance than the choice of modeling algorithm or input data, with an area under the receiver operating characteristic difference of 0.09 between the best-performing models. Conclusion: We demonstrate the feasibility of identifying newly diagnosed Crohn’s Disease patients within a United States health system using Electronic Health Records. For the predictive modeling task, cases and controls were distinguished only with modest performance, even though various state-of-the-art methods were applied based on features from structured and unstructured data. Our findings suggest the benefit of adding information from clinical notes in a supervised or unsupervised manner for cohort creation and predictive modeling.},
  archive      = {J_ARTMED},
  author       = {Susanne Ibing and Julian Hugo and Florian Borchert and Linea Schmidt and Caroline Benson and Allison A. Marshall and Colleen Chasteau and Ujunwa Korie and Diana Paguay and Jan Philipp Sachs and Bernhard Y. Renard and Judy H. Cho and Erwin P. Böttinger and Ryan C. Ungaro},
  doi          = {10.1016/j.artmed.2024.103032},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103032},
  shortjournal = {Artif. Intell. Med.},
  title        = {Electronic health records-based identification of newly diagnosed crohn’s disease cases},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-task learning U-net model for end-to-end HEp-2 cell image analysis. <em>ARTMED</em>, <em>159</em>, 103031. (<a href='https://doi.org/10.1016/j.artmed.2024.103031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Antinuclear Antibody ( ANA ) testing is pivotal to help diagnose patients with a suspected autoimmune disease. The Indirect Immunofluorescence ( IIF ) microscopy performed with human epithelial type 2 (HEp-2) cells as the substrate is the reference method for ANA screening. It allows for the detection of antibodies binding to specific intracellular targets, resulting in various staining patterns that should be identified for diagnosis purposes. In recent years, there has been an increasing interest in devising deep learning methods for automated cell segmentation and classification of staining patterns, as well as for other tasks related to this diagnostic technique (such as intensity classification). However, little attention has been devoted to architectures aimed at simultaneously managing multiple interrelated tasks, via a shared representation. In this paper, we propose a deep neural network model that extends U-Net in a Multi-Task Learning (MTL) fashion, thus offering an end-to-end approach to tackle three fundamental tasks of the diagnostic procedure, i.e., HEp-2 cell specimen intensity classification, specimen segmentation, and pattern classification. The experiments were conducted on one of the largest publicly available datasets of HEp-2 images. The results showed that the proposed approach significantly outperformed the competing state-of-the-art methods for all the considered tasks.},
  archive      = {J_ARTMED},
  author       = {Gennaro Percannella and Umberto Petruzzello and Francesco Tortorella and Mario Vento},
  doi          = {10.1016/j.artmed.2024.103031},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103031},
  shortjournal = {Artif. Intell. Med.},
  title        = {A multi-task learning U-net model for end-to-end HEp-2 cell image analysis},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDDINet: Enhancing drug–drug interaction prediction via information flow and consensus constrained multi-graph contrastive learning. <em>ARTMED</em>, <em>159</em>, 103029. (<a href='https://doi.org/10.1016/j.artmed.2024.103029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting drug–drug interactions (DDIs) is crucial for understanding and preventing adverse drug reactions (ADRs). However, most existing methods inadequately explore the interactive information between drugs in a self-supervised manner, limiting our comprehension of drug–drug associations. This paper introduces EDDINet : E nhancing D rug- D rug I nteraction Prediction via Information Flow and Consensus-Constrained Multi-Graph Contrastive Learning for precise DDI prediction. We first present a cross-modal information-flow mechanism to integrate diverse drug features, enriching the structural insights conveyed by the drug feature vector. Next, we employ contrastive learning to filter various biological networks, enhancing the model’s robustness. Additionally, we propose a consensus regularization framework that collaboratively trains multi-view models, producing high-quality drug representations. To unify drug representations derived from different biological information, we utilize an attention mechanism for DDI prediction. Extensive experiments demonstrate that EDDINet surpasses state-of-the-art unsupervised models and outperforms some supervised baseline models in DDI prediction tasks. Our approach shows significant advantages and holds promising potential for advancing DDI research and improving drug safety assessments. Our codes are available at: https://github.com/95LY/EDDINet_code .},
  archive      = {J_ARTMED},
  author       = {Hong Wang and Luhe Zhuang and Yijie Ding and Prayag Tiwari and Cheng Liang},
  doi          = {10.1016/j.artmed.2024.103029},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103029},
  shortjournal = {Artif. Intell. Med.},
  title        = {EDDINet: Enhancing drug–drug interaction prediction via information flow and consensus constrained multi-graph contrastive learning},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAPPA: Enhancing prognosis prediction in primary aldosteronism post-adrenalectomy using graph-based modeling. <em>ARTMED</em>, <em>159</em>, 103028. (<a href='https://doi.org/10.1016/j.artmed.2024.103028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and objective Predicting postoperative prognosis is vital for clinical decision making in patients undergoing adrenalectomy (ADX). This study introduced GAPPA, a novel GNN-based approach, to predict post-ADX outcomes in patients with unilateral primary aldosteronism (UPA). The objective was to leverage the intricate dependencies between clinico-biochemical features and clinical outcomes using GNNs integrated into a bipartite graph structure to enhance prognostic prediction accuracy. Methods We conceptualized prognostic prediction as a link prediction task on a bipartite graph, with nodes representing patients, clinico-biochemical features, and clinical outcomes, and edges denoting the connections between them. GAPPA utilizes GNNs to capture these dependencies and seamlessly integrates the outcome predictions into a graph structure. This approach was evaluated using a dataset of 640 patients with UPA who underwent unilateral ADX (uADX) between 1990 and 2022. We conducted a comparative analysis using repeated stratified five-fold cross-validation and paired t -tests to evaluate the performance of GAPPA against conventional machine learning methods and previous studies across various metrics. Results GAPPA significantly outperformed conventional machine learning methods and previous studies ( p < 0.05) across various metrics. It achieved F1-score, accuracy, sensitivity, and specificity of 71.3 % ± 3.1 %, 71.1 % ± 3.4 %, 69.9 % ± 4.3 %, and 72.4 % ± 7.2 %, respectively, with an AUC of 0.775 ± 0.030. We also investigated the impact of different initialization schemes on GAPPA outcome-edge embeddings, highlighting their robustness and stability. Conclusion GAPPA aids in preoperative prognosis assessment and facilitates patient counseling, contributing to prognostic prediction and advancing the applications of GNNs in the biomedical domain.},
  archive      = {J_ARTMED},
  author       = {Pei-Yan Li and Yu-Wen Huang and Vin-Cent Wu and Jeff S. Chueh and Chi-Shin Tseng and Chung-Ming Chen},
  doi          = {10.1016/j.artmed.2024.103028},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103028},
  shortjournal = {Artif. Intell. Med.},
  title        = {GAPPA: Enhancing prognosis prediction in primary aldosteronism post-adrenalectomy using graph-based modeling},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating synthetic clinical text with local large language models to identify misdiagnosed limb fractures in radiology reports. <em>ARTMED</em>, <em>159</em>, 103027. (<a href='https://doi.org/10.1016/j.artmed.2024.103027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) demonstrate impressive capabilities in generating human-like content and have much potential to improve the performance and efficiency of healthcare. An important application of LLMs is to generate synthetic clinical reports that could alleviate the burden of annotating and collecting real-world data in training AI models. Meanwhile, there could be concerns and limitations in using commercial LLMs to handle sensitive clinical data. In this study, we examined the use of open-source LLMs as an alternative to generate synthetic radiology reports to supplement real-world annotated data. We found LLMs hosted locally can achieve similar performance compared to ChatGPT and GPT-4 in augmenting training data for the downstream report classification task of identifying misdiagnosed fractures. We also examined the predictive value of using synthetic reports alone for training downstream models, where our best setting achieved more than 90 % of the performance using real-world data. Overall, our findings show that open-source, local LLMs can be a favourable option for creating synthetic clinical reports for downstream tasks.},
  archive      = {J_ARTMED},
  author       = {Jinghui Liu and Bevan Koopman and Nathan J. Brown and Kevin Chu and Anthony Nguyen},
  doi          = {10.1016/j.artmed.2024.103027},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103027},
  shortjournal = {Artif. Intell. Med.},
  title        = {Generating synthetic clinical text with local large language models to identify misdiagnosed limb fractures in radiology reports},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable machine learning for time-to-event prediction in medicine and healthcare. <em>ARTMED</em>, <em>159</em>, 103026. (<a href='https://doi.org/10.1016/j.artmed.2024.103026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-to-event prediction, e.g. cancer survival analysis or hospital length of stay, is a highly prominent machine learning task in medical and healthcare applications. However, only a few interpretable machine learning methods comply with its challenges. To facilitate a comprehensive explanatory analysis of survival models, we formally introduce time-dependent feature effects and global feature importance explanations. We show how post-hoc interpretation methods allow for finding biases in AI systems predicting length of stay using a novel multi-modal dataset created from 1235 X-ray images with textual radiology reports annotated by human experts. Moreover, we evaluate cancer survival models beyond predictive performance to include the importance of multi-omics feature groups based on a large-scale benchmark comprising 11 datasets from The Cancer Genome Atlas (TCGA). Model developers can use the proposed methods to debug and improve machine learning algorithms, while physicians can discover disease biomarkers and assess their significance. We contribute open data and code resources to facilitate future work in the emerging research direction of explainable survival analysis.},
  archive      = {J_ARTMED},
  author       = {Hubert Baniecki and Bartlomiej Sobieski and Patryk Szatkowski and Przemyslaw Bombinski and Przemyslaw Biecek},
  doi          = {10.1016/j.artmed.2024.103026},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103026},
  shortjournal = {Artif. Intell. Med.},
  title        = {Interpretable machine learning for time-to-event prediction in medicine and healthcare},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-powered image analysis: A paradigm shift in infectious disease detection. <em>ARTMED</em>, <em>159</em>, 103025. (<a href='https://doi.org/10.1016/j.artmed.2024.103025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global burden of infectious diseases significantly affects mortality rates, with their varying symptoms making it challenging to assess and determine the severity of infections. Different countries face unique challenges related to these diseases. This study introduces innovative Artificial Intelligence (AI) based methodologies to enhance diagnostic accuracy through the analysis of medical imagery. It achieves this by developing a mathematical model capable of identifying potential infectious diseases from images, utilizing a Multi-Criteria Decision-Making (MCDM) framework. This cutting-edge approach combines Hypersoft Set (HSS) within a fuzzy context, pioneering in AI-driven diagnostic processes. The decision-making process might suggest actions such as isolation, quarantine in either domestic settings or specialized facilities, or admission to a hospital for further treatment. The use of visual aids in this research not only improves understanding but also highlights the effectiveness and significance of the proposed methods. The foundational theory and the results from this novel approach demonstrate its potential for widespread application in fields like machine learning, deep learning, and pattern recognition, indicating a significant stride in the fight against infectious diseases through advanced diagnostic techniques.},
  archive      = {J_ARTMED},
  author       = {Muhammad Ahsan and Robertas Damaševičius},
  doi          = {10.1016/j.artmed.2024.103025},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103025},
  shortjournal = {Artif. Intell. Med.},
  title        = {Artificial intelligence-powered image analysis: A paradigm shift in infectious disease detection},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimal data poisoning attack in federated learning for medical image classification: An attacker perspective. <em>ARTMED</em>, <em>159</em>, 103024. (<a href='https://doi.org/10.1016/j.artmed.2024.103024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The privacy-sensitive nature of medical image data is often bounded by strict data sharing regulations that necessitate the need for novel modeling and analysis techniques. Federated learning (FL) enables multiple medical institutions to collectively train a deep neural network without sharing sensitive patient information. In addition, FL uses its collaborative approach to address challenges related to the scarcity and non-uniform distribution of heterogeneous medical domain data. Nevertheless, the data-opaque nature and distributed setup make FL susceptible to data poisoning attacks. There are diverse FL data poisoning attacks for classification models on natural image data in the literature. But their primary focus is on the impact of the attack and they do not consider the attack budget and attack visibility. The attack budget is essential for adversaries to optimize resource utilization in real-world scenarios, which determines the number of manipulations or perturbations they can apply. Simultaneously, attack visibility is crucial to ensure covert execution, allowing attackers to achieve their objectives without triggering detection mechanisms. Generally, an attacker’s aim is to create maximum attack impact with minimal resources and low visibility. So, considering these three entities can effectively comprehend the adversary’s perspective in designing an attack for real-world scenarios. Further, data poisoning attacks on medical images are challenging compared to natural images due to the subjective nature of medical data. Hence, we develop an attack with a low budget, low visibility, and high impact for medical image classification in FL. We propose a federated learning attention guided minimal attack (FL-AGMA), that uses class attention maps to identify specific medical image regions for perturbation. We introduce image distortion degree (IDD) as a metric to assess the attack budget. Also, we develop a feedback mechanism to regulate the attack coefficient for low attack visibility. Later, we optimize the attack budget by adaptively changing the IDD based on attack visibility. We extensively evaluate three large-scale datasets, namely, Covid-chestxray, Camelyon17, and HAM10000, covering three different data modalities. We observe that our FL-AGMA method has resulted in 44.49% less test accuracy with only 24% of IDD attack budget and lower attack visibility compared to the other attacks.},
  archive      = {J_ARTMED},
  author       = {K. Naveen Kumar and C. Krishna Mohan and Linga Reddy Cenkeramaddi and Navchetan Awasthi},
  doi          = {10.1016/j.artmed.2024.103024},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103024},
  shortjournal = {Artif. Intell. Med.},
  title        = {Minimal data poisoning attack in federated learning for medical image classification: An attacker perspective},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMHGNN: Double multi-view heterogeneous graph neural network framework for drug-target interaction prediction. <em>ARTMED</em>, <em>159</em>, 103023. (<a href='https://doi.org/10.1016/j.artmed.2024.103023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of drug-target interactions (DTIs) plays a crucial role in drug discovery. Compared with traditional experimental methods that are labor-intensive and time-consuming, computational methods for drug-target interactions prediction are more popular in recent years. Conventional computational methods almost simply view heterogeneous network constructed by the drug-related and protein-related dataset instead of comprehensively exploring drug-protein pair (DPP) information. To address this limitation, we proposed a D ouble M ulti-view H eterogeneous G raph N eural N etwork framework for drug-target interaction prediction (DMHGNN). In DMHGNN, one multi-view heterogeneous graph neural network is based on meta-paths and denoising autoencoder for protein-, drug-related heterogeneous network learning, and another multi-view heterogeneous graph neural network is based on multi-channel graph convolutional network for drug-protein pair similarity network learning. First, a meta-path-based graph encoder with the attention mechanism is used for substructure learning of complex relationships from heterogeneous network constructed by proteins, drugs, side-effects and diseases, obtaining key information that is easy to be ignored in global learning of heterogeneous networks, and multi-source neighbouring features for drugs and proteins are learned from heterogeneous network via denoising auto-encoder model. Then, multi-view graphs of drug-protein pairs (DPPs) including the topology graph, semantics graph and collaborative graph with shared weights are constructed, and the multi-channel graph convolutional network (GCN) is utilized to learn the deep representation of DPPs. Finally, a multi-layer fully connection network is trained to predict drug-target interactions. Experiments have demonstrated its effectiveness and better performance than state-of-the-art methods.},
  archive      = {J_ARTMED},
  author       = {Qiao Ning and Yue Wang and Yaomiao Zhao and Jiahao Sun and Lu Jiang and Kaidi Wang and Minghao Yin},
  doi          = {10.1016/j.artmed.2024.103023},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103023},
  shortjournal = {Artif. Intell. Med.},
  title        = {DMHGNN: Double multi-view heterogeneous graph neural network framework for drug-target interaction prediction},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anatomical prior-based vertebral landmark detection for spinal disorder diagnosis. <em>ARTMED</em>, <em>159</em>, 103011. (<a href='https://doi.org/10.1016/j.artmed.2024.103011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of fundamental ways to interpret spine images, detection of vertebral landmarks is an informative prerequisite for further diagnosis and management of spine disorders such as scoliosis and fractures. Most existing machine learning-based methods for automatic vertebral landmark detection suffer from overlapping landmarks or abnormally long distances between nearby landmarks against anatomical priors, and thus lack sufficient reliability and interpretability. To tackle the problem, this paper systematically utilizes anatomical prior knowledge in vertebral landmark detection. We explicitly formulate anatomical priors of the spine, related to distances among vertebrae and spatial order within the spine, and integrate these geometrical constraints within training loss, inference procedure, and evaluation metrics. First, we introduce an anatomy-constraint loss to regularize the training process with the aforementioned contextual priors explicitly. Second, we propose a simple-yet-effective anatomy-aided inference procedure by employing sequential prediction rather than a parallel counterpart. Third, we provide novel anatomy-related metrics to quantitatively evaluate to which extent landmark predictions follow the anatomical priors, as is not reflected within the widely-used landmark localization error metric. We employ the localization framework on 1410 anterior–posterior radiographic images. Compared with competitive baseline models, we achieve superior landmark localization accuracy and comparable Cobb angle estimation for scoliosis assessment. Ablation studies demonstrate the effectiveness of designed components on the decrease of localization error and improvement of anatomical plausibility. Additionally, we exhibit effective generalization performance by transferring our detection method onto sagittal 2-D slices of CT scans and boost the performance of downstream compression fracture classification at vertebra-level.},
  archive      = {J_ARTMED},
  author       = {Yukang Yang and Yu Wang and Tianyu Liu and Miao Wang and Ming Sun and Shiji Song and Wenhui Fan and Gao Huang},
  doi          = {10.1016/j.artmed.2024.103011},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103011},
  shortjournal = {Artif. Intell. Med.},
  title        = {Anatomical prior-based vertebral landmark detection for spinal disorder diagnosis},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
