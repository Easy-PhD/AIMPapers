<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ARTMED</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="artmed">ARTMED - 7</h2>
<ul>
<li><details>
<summary>
(2026). LkaM-PTM: Predicting PTM sites through multimodal protein features from capturing cross-field information. <em>ARTMED</em>, <em>171</em>, 103297. (<a href='https://doi.org/10.1016/j.artmed.2025.103297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-translational modification (PTM) site prediction is significant for a deeper understanding of biological processes and disease mechanisms. Most existing studies on predicting PTM sites focus on specific PTM types and lack universality. Additionally, few methods consider combining the advantages of sequence information, pre-trained information, and structural information. A multimodal adaptive PTM sites prediction method based on 1D Large Kernel Attention (1D-LKA) is thereby proposed, named LkaM-PTM. Specifically, LkaM-PTM fuses sequence representations from the SeqNet, pretrained representations from the PLMNet, and structural representations from the StruNet in the encoder part. Firstly, SeqNet utilizes 1D-LKA adaptively to capture cross-field information, combined with the feature reusability of DenseNet to extract local features of the sequence. Next, PLMNet captures global features from the protein language model ProtBert through a multilayer perceptron (MLP). Subsequently, StruNet compresses and reconstruct structural information of global, amino acid, and atomic granularity through a Stacked Autoencoder (SAE) to extract low-dimensional structural representations. Ultimately, the three feature sets are subsequently concatenated into a unified multimodal representation and fed into a MLP-based decoder for final PTM sites prediction. Experimental results show that LkaM-PTM outperforms the state-of-the-art approaches in the accuracy of predicting PTM sites. The source code and data for LkaM-PTM are available at https://github.com/tony-soochow/LkaM-PTM .},
  archive      = {J_ARTMED},
  author       = {Mengjie Liu and Fei Zhu},
  doi          = {10.1016/j.artmed.2025.103297},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103297},
  shortjournal = {Artif. Intell. Med.},
  title        = {LkaM-PTM: Predicting PTM sites through multimodal protein features from capturing cross-field information},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-annotation agreement and prediction consistency networks: Improving semi-supervised segmentation of medical images with ambiguous boundaries. <em>ARTMED</em>, <em>171</em>, 103289. (<a href='https://doi.org/10.1016/j.artmed.2025.103289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation annotations exhibit variations among experts due to the ambiguous boundaries of segmented objects and backgrounds in medical images. Although using multiple annotations for each image in the fully-supervised setting has been extensively studied for training deep models, obtaining a large amount of multi-annotated data is challenging due to the substantial time and manpower costs required for segmentation annotations, resulting in most images lacking any annotations. To address this, we propose Multi-annotated Semi-supervised Ensemble Networks (MSE-Nets) for learning segmentation from limited multi-annotated and abundant unannotated data. Specifically, we introduce the Network Pairwise Consistency Enhancement (NPCE) module and Multi-Network Pseudo Supervised (MNPS) module to enhance MSE-Nets for the segmentation task by considering two major factors: (1) to optimize the utilization of all accessible multi-annotated data, the NPCE separates (dis)agreement annotations of multi-annotated data at the pixel level and handles agreement and disagreement annotations in different ways; (2) to mitigate the introduction of imprecise pseudo-labels, the MNPS extends the training data by leveraging consistent pseudo-labels from unannotated data. Finally, we improve confidence calibration by averaging the predictions of base networks. Experiments on the ISIC dataset show that we reduced the demand for multi-annotated data by 97.75% and narrowed the gap with the best fully-supervised baseline to just a Jaccard index of 3.7%. Furthermore, compared to other semi-supervised methods that rely only on a single annotation or a combined fusion approach, the comprehensive experimental results on ISIC and RIGA datasets demonstrate the superior performance of our proposed method in medical image segmentation with ambiguous boundaries.},
  archive      = {J_ARTMED},
  author       = {Shuai Wang and Tengjin Weng and Jingyi Wang and Kai Zhao and Yang Shen and Zhidong Zhao and Yixiu Liu and Pengfei Jiao and Zhiming Cheng and Yaoqi Sun and Yaqi Wang},
  doi          = {10.1016/j.artmed.2025.103289},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103289},
  shortjournal = {Artif. Intell. Med.},
  title        = {Multi-annotation agreement and prediction consistency networks: Improving semi-supervised segmentation of medical images with ambiguous boundaries},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-modal medical image synthesis via dual-branch wavelet encoding and deformable feature interaction. <em>ARTMED</em>, <em>171</em>, 103287. (<a href='https://doi.org/10.1016/j.artmed.2025.103287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal medical imaging is essential in disease diagnosis and treatment planning, as it provides complementary anatomical and pathological information. However, the limitations of scan time, image corruption, and differences in imaging protocols often result in certain modalities being missing or unavailable, severely limiting the application of multi-modal data. Although existing medical image synthesis methods have made notable progress, it remains challenging to synergistically extract local details and global contextual information from multi-modal inputs, as well as to effectively fuse complementary features across modalities. To address these problems, this paper proposes a novel dual-branch wavelet encoding and deformable feature interaction generative adversarial network (DWFI-GAN), for synthesizing missing modalities from available ones. Specifically, in the generator, we design a dual-branch wavelet encoder and introduce a wavelet multi-scale downsampling (Wavelet-MS-Down) module, in order to efficiently extract multi-scale local features and global contextual information at the downsampling stage. To better fuse information across modalities, we design the deformable cross-attention feature fusion (DCFF) module to interactively fuse features from different source images at multiple scales. Additionally, at the bottleneck of the generator, we design an episodic bottleneck structure. This structure employs intermittent injection of frequency-space enhanced (FSE) modules, which enriches the scale diversity of the fusion features from both frequency and spatial domain perspectives. Experiments on two medical imaging datasets show that DWFI-GAN outperforms several state-of-the-art methods in both qualitative and quantitative comparisons, with ablation studies further validating each module's contribution to fusion and detail enhancement. The code is publicly available at https://github.com/xuefengjia227/DWFI-GAN .},
  archive      = {J_ARTMED},
  author       = {Xuefeng Jia and Biyuan Li and Jinying Ma and Chunjie Lv and Xiao Tian and Lianhao Huo and Mengyao Lun},
  doi          = {10.1016/j.artmed.2025.103287},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103287},
  shortjournal = {Artif. Intell. Med.},
  title        = {Multi-modal medical image synthesis via dual-branch wavelet encoding and deformable feature interaction},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An explainable three dimensional framework to uncover learning patterns: A unified look in variable sulci recognition. <em>ARTMED</em>, <em>171</em>, 103286. (<a href='https://doi.org/10.1016/j.artmed.2025.103286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significant features identified in a representative subset of the dataset during the learning process of an artificial intelligence model are referred to as a ‘global’ explanation. Three-dimensional (3D) global explanations are crucial in neuroimaging, where a complex representational space demands more than basic two-dimensional interpretations. However, current studies in the literature often lack the accuracy, comprehensibility, and 3D global explanations needed in neuroimaging and beyond. To address this gap, we developed an explainable artificial intelligence (XAI) 3D-Framework capable of providing accurate, low-complexity global explanations. We evaluated the framework using various 3D deep learning models trained on a well-annotated cohort of 596 structural MRIs. The binary classification task focused on detecting the presence or absence of the paracingulate sulcus (PCS), a highly variable brain structure associated with psychosis. Our framework integrates statistical features (Shape) and XAI methods (GradCam and SHAP) with dimensionality reduction, ensuring that explanations reflect both model learning and cohort-specific variability. By combining Shape, GradCam, and SHAP, our framework reduces inter-method variability, enhancing the faithfulness and reliability of global explanations. These robust explanations facilitated the identification of critical sub-regions, including the posterior temporal and internal parietal regions, as well as the cingulate region and thalamus, suggesting potential genetic or developmental influences. For the first time, this XAI 3D-Framework leverages global explanations to uncover the broader developmental context of specific cortical features. This approach advances the fields of deep learning and neuroscience by offering insights into normative brain development and atypical trajectories linked to mental illness, paving the way for more reliable and interpretable AI applications in neuroimaging.},
  archive      = {J_ARTMED},
  author       = {Michail Mamalakis and Héloïse de Vareilles and Atheer Al-Manea and Samantha C. Mitchell and Ingrid Agartz and Lynn Egeland Mørch-Johnsen and Jane Garrison and Jon Simons and Pietro Lio and John Suckling and Graham K. Murray},
  doi          = {10.1016/j.artmed.2025.103286},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103286},
  shortjournal = {Artif. Intell. Med.},
  title        = {An explainable three dimensional framework to uncover learning patterns: A unified look in variable sulci recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SpineCLUE: Automatic vertebrae identification using contrastive learning and uncertainty estimation. <em>ARTMED</em>, <em>171</em>, 103285. (<a href='https://doi.org/10.1016/j.artmed.2025.103285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertebrae identification in arbitrary fields-of-view plays a crucial role in diagnosing spine disease. Most spine CT contain only local regions, such as the neck, chest, and abdomen. Existing spine-level methods, which rely on a priori on the specific number of target vertebrae, are less able to cope with this challenge. In this paper, we propose a three-stage vertebra-level method to address the challenges in 3D CT vertebrae identification with arbitrary fields-of-view. In order to integrate contextual prior information during identification, rather than identifying independently at the vertebrae-level, we perform the vertebrae localization, segmentation and identification tasks sequentially, thus making effective use of anatomical prior information about the vertebrae throughout the process. Specifically, to improve the stability of localization and prevent failures caused by abnormal vertebral positions in 3D space, we introduce a dual-factor density clustering algorithm to acquire localization information for individual vertebrae, thereby facilitating the subsequent segmentation and recognition processes. In addition, to tackle the issue of inter-class similarity and intra-class variability, we pretrain our identification network by using a supervised contrastive learning method. To further optimize the identification results, we estimated the uncertainty of the classification network and utilized the message fusion module to combine the uncertainty scores, while aggregating global information about the spine. Our method achieves state-of-the-art results on the VerSe20 challenge benchmark.},
  archive      = {J_ARTMED},
  author       = {Sheng Zhang and Hongxuan Li and Minheng Chen and Mingying Li and Miao Liu and Junxian Wu and Cheng Xue and Youyong Kong},
  doi          = {10.1016/j.artmed.2025.103285},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103285},
  shortjournal = {Artif. Intell. Med.},
  title        = {SpineCLUE: Automatic vertebrae identification using contrastive learning and uncertainty estimation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Automation or augmentation? the impact of artificial intelligence's technological characteristics on usage intention in medical staff. <em>ARTMED</em>, <em>171</em>, 103283. (<a href='https://doi.org/10.1016/j.artmed.2025.103283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is revolutionizing clinical practice. As medical AI becomes increasingly a part of regular practice and procedures, a deeper understanding of the AI usage intention of medical staff is of great value. By incorporating technology-task fit and self-determination theory, this study developed a conceptual model through which to identify and observe the effects of medical AI automation and augmentation on the psychological needs (i.e., autonomy, relatedness, and competence), technology-task fit, and AI usage intention of medical staff. Using cross-sectional data from 400 Chinese medical staff, a partial least squares structural equation model (PLS-SEM) analysis showed that medical AI automation correlated positively with perceived autonomy and competence, while augmentation correlated positively with perceived autonomy, competence, and relatedness. Mediation analysis indicated that perceived autonomy, competence, and technology-task fit were sequential mediators in the association between medical AI automation and usage intention; perceived autonomy, competence, relatedness, and technology-task fit were sequential mediators in the link between medical AI augmentation and usage intention. These findings exemplify the automation–augmentation paradox in AI research, offering insight into the AI usage intention of medical staff and the underlying psychological mechanisms involved. Strategies to facilitate medical staff's clinical AI usage intention stemming from a human-centered perspective are also proposed.},
  archive      = {J_ARTMED},
  author       = {Xiqian Zou and Shuang Chen},
  doi          = {10.1016/j.artmed.2025.103283},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103283},
  shortjournal = {Artif. Intell. Med.},
  title        = {Automation or augmentation? the impact of artificial intelligence's technological characteristics on usage intention in medical staff},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Investigation of synonym expansion and self-alignment pretraining for enhancing human phenotype ontology concept recognition. <em>ARTMED</em>, <em>171</em>, 103282. (<a href='https://doi.org/10.1016/j.artmed.2025.103282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of Human Phenotype Ontology (HPO) terms from biomedical text is crucial for disease diagnosis and analysis. However, traditional named entity recognition (NER) methods often fall short by overlooking synonyms and entity variants, resulting in reduced accuracy and coverage. To address these limitations, we introduce three strategic enhancements to improve HPO concept recognition. The first strategy augments the training data by incorporating HPO synonyms at the instance level, which enhances the model’s ability to recognize diverse phenotypic expressions. The second strategy introduces HPOBERT, a semantic approach that models HPO synonyms through self-aligned pretraining. This method closely aligns synonym representations while effectively distinguishing them from non-synonyms, thereby improving the model’s ability to differentiate between concepts. The third strategy integrates both the instance level and semantic approach. We evaluated these enhancement strategies on four clinical text datasets annotated with HPO concepts. The results demonstrate significant improvements in classification accuracy, recall, and both micro and macro F1 scores. Additionally, our enhancement strategies showed strong performance in Named Entity Normalization (NEN) after NER, accurately linking recognized HPO concepts to standardized knowledge bases. Specifically, we observe improvements of 2.44% and 4.38% in NEN-F on the gold and silver standard datasets, respectively, highlighting the effectiveness of our approach. The source code is available at https://github.com/ZhuLab-Fudan/HPOTagger .},
  archive      = {J_ARTMED},
  author       = {Weiqi Zhai and Rongze Jiang and Xiaodi Huang and Junyi Bian and Shanfeng Zhu},
  doi          = {10.1016/j.artmed.2025.103282},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {103282},
  shortjournal = {Artif. Intell. Med.},
  title        = {Investigation of synonym expansion and self-alignment pretraining for enhancing human phenotype ontology concept recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
