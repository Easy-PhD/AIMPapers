<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CVIU</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cviu">CVIU - 227</h2>
<ul>
<li><details>
<summary>
(2025). Enhancing action recognition by leveraging the hierarchical structure of actions and textual context. <em>CVIU</em>, <em>262</em>, 104560. (<a href='https://doi.org/10.1016/j.cviu.2025.104560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach to improve action recognition by exploiting the hierarchical organization of actions and by incorporating contextualized textual information, including location and previous actions, to reflect the action’s temporal context . To achieve this, we introduce a transformer architecture tailored for action recognition that employs both visual and textual features. Visual features are obtained from RGB and optical flow data, while text embeddings represent contextual information. Furthermore, we define a joint loss function to simultaneously train the model for both coarse- and fine-grained action recognition, effectively exploiting the hierarchical nature of actions. To demonstrate the effectiveness of our method, we extend the Toyota Smarthome Untrimmed (TSU) dataset by incorporating action hierarchies, resulting in the Hierarchical TSU dataset , a hierarchical dataset designed for monitoring activities of the elderly in home environments. An ablation study assesses the performance impact of different strategies for integrating contextual and hierarchical data. Experimental results demonstrate that the proposed method consistently outperforms SOTA methods on the Hierarchical TSU dataset, Assembly101 and IkeaASM, achieving over a 17% improvement in top-1 accuracy.},
  archive      = {J_CVIU},
  author       = {Manuel Benavent-Lledo and David Mulero-Pérez and David Ortiz-Perez and Jose Garcia-Rodriguez and Antonis Argyros},
  doi          = {10.1016/j.cviu.2025.104560},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104560},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enhancing action recognition by leveraging the hierarchical structure of actions and textual context},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffExplainer: Towards cross-modal global explanations with diffusion models. <em>CVIU</em>, <em>262</em>, 104559. (<a href='https://doi.org/10.1016/j.cviu.2025.104559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present DiffExplainer , a novel framework that, leveraging language-vision models, enables multimodal global explainability. DiffExplainer employs diffusion models conditioned on optimized text prompts, synthesizing images that maximize class outputs and hidden features of a classifier, thus providing a visual tool for explaining decisions. Moreover, the analysis of generated visual descriptions allows for automatic identification of biases and spurious features, as opposed to traditional methods that often rely on manual intervention. The cross-modal transferability of language-vision models also enables the possibility to describe decisions in a more human-interpretable way, i.e., through text. We conduct comprehensive experiments demonstrating the effectiveness of DiffExplainer on (1) the generation of high-quality images explaining model decisions, surpassing existing activation maximization methods, and (2) the automated identification of biases and spurious features.},
  archive      = {J_CVIU},
  author       = {Matteo Pennisi and Giovanni Bellitto and Simone Palazzo and Isaak Kavasidis and Mubarak Shah and Concetto Spampinato},
  doi          = {10.1016/j.cviu.2025.104559},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104559},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DiffExplainer: Towards cross-modal global explanations with diffusion models},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for automatic breast density classification in magnetic resonance imaging. <em>CVIU</em>, <em>262</em>, 104558. (<a href='https://doi.org/10.1016/j.cviu.2025.104558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and Objective: Breast density classification is a critical factor in assessing breast cancer risk, and most existing methods rely on mammography. This study proposes a method for automatic breast density classification using T2-weighted MRI images, following the ACR BI-RADS criteria. Methods: The proposed method involves creating total and border masks based on MRI scans, excluding the chest area, computing the percentage of periglandular fat, and classifying density into “Dense” and “Not Dense”. Experimental validation was conducted using 136 MRI exams, including cases with prosthetic implants, mastectomy, nodulectomy, and tumorectomy. Results: The method achieved an Accuracy of 0.86, a Precision of 0.96, a Specificity of 0.97, and a Sensitivity of 0.76. These results highlight the robustness of the method and demonstrate the potential of MRI-based density classification as a complementary tool to mammography. Conclusions: The findings provide new insights for breast imaging practices and assist radiologists in clinical practice. Future developments will focus on extending the classification to the original four BI-RADS classes.},
  archive      = {J_CVIU},
  author       = {Simona Correra and Francesco Mercaldo and Vittoria Nardone and Giulia Varriano and Dalila De Lucia and Maria Chiara Brunese and Antonella Santone and Corrado Caiazzo},
  doi          = {10.1016/j.cviu.2025.104558},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104558},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A method for automatic breast density classification in magnetic resonance imaging},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffuseDoc: Document geometric rectification via diffusion model. <em>CVIU</em>, <em>262</em>, 104554. (<a href='https://doi.org/10.1016/j.cviu.2025.104554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document images captured by sensors often suffer from intricate geometric distortions, hindering readability and impeding downstream document analysis tasks. While deep learning-based methods for document geometric rectification have shown promising results, their training heavily relies on high quality ground truth for the mapping field, resulting in challenging and expensive dataset creation. To address this issue, we propose DiffuseDoc, a novel framework for document image geometric rectification based on the diffusion model. Unlike existing methods, the training process of DiffuseDoc only requires pairs of distorted and distortion-free images, eliminating the need for ground truth mapping field supervision. Specifically, DiffuseDoc consists of two primary components: the geometric rectification module and the conditional diffusion module. By jointly training the two components, the rectification results are optimized while simultaneously learning the latent feature distribution of the distortion-free image. Also, we contribute the DocReal dataset, comprising document images captured by diverse high-resolution sensors in real-world scenarios, alongside their corresponding scanned versions. Extensive evaluations demonstrate that DiffuseDoc achieves state-of-the-art performance on both the Doc-U-Net benchmark and DocReal datasets.},
  archive      = {J_CVIU},
  author       = {Wenfei Xiong and Huabing Zhou and Yanduo Zhang and Tao Lu and Jiayi Ma},
  doi          = {10.1016/j.cviu.2025.104554},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104554},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DiffuseDoc: Document geometric rectification via diffusion model},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PlanarTrack: A high-quality and challenging benchmark for large-scale planar object tracking. <em>CVIU</em>, <em>262</em>, 104553. (<a href='https://doi.org/10.1016/j.cviu.2025.104553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planar tracking has drawn increasing interest owing to its key roles in robotics and augmented reality. Despite recent great advancement, further development of planar tracking, particularly in the deep learning era, is largely limited compared to generic tracking due to the lack of large-scale platforms. To mitigate this, we propose PlanarTrack , a large-scale high-quality and challenging benchmark for planar tracking. Specifically, PlanarTrack consists of 1150 sequences with over 733K frames, including 1000 short-term and 150 new long-term videos, which enables comprehensive evaluation of short- and long-term tracking performance. All videos in PlanarTrack are recorded in unconstrained conditions from the wild, which makes PlanarTrack challenging but more realistic for real-world applications. To ensure high-quality annotations, each video frame is manually annotated by four corner points with multi-round meticulous inspection and refinement. To enhance target diversity of PlanarTrack, we only capture a unique target in one sequence, which is different from existing benchmarks. To our best knowledge, PlanarTrack is by far the largest and most diverse and challenging dataset dedicated to planar tracking. To understand performance of existing methods on PlanarTrack and to provide a comparison for future research, we evaluate 10 representative planar trackers with extensive comparison and in-depth analysis. Our evaluation reveals that, unsurprisingly, the top planar trackers heavily degrade on the challenging PlanarTrack, which indicates more efforts are required for improving planar tracking. Moreover, we derive a variant named PlanarTrack BB from PlanarTrack for generic tracking. Evaluation with 15 generic trackers shows that, surprisingly, our PlanarTrack BB is even more challenging than several popular generic tracking benchmarks, and more attention should be paid to dealing with planar targets, though they are rigid. Our data and results will be released at https://github.com/HengLan/PlanarTrack},
  archive      = {J_CVIU},
  author       = {Yifan Jiao and Xinran Liu and Xiaoqiong Liu and Xiaohui Yuan and Heng Fan and Libo Zhang},
  doi          = {10.1016/j.cviu.2025.104553},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104553},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {PlanarTrack: A high-quality and challenging benchmark for large-scale planar object tracking},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel-aware feature mining network for Visible–Infrared person re-identification. <em>CVIU</em>, <em>262</em>, 104552. (<a href='https://doi.org/10.1016/j.cviu.2025.104552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible–Infrared Person Re-identification (VI-ReID) aims to match the identities of pedestrians captured by non-overlapping cameras in both visible and infrared modalities. The key to overcoming the VI-ReID challenge lies in extracting diverse modality-shared features. Current methods mainly focus on channel-level operations during data preprocessing, with the aim of expanding the dataset. However, these methods often overlook the complex relationships among channel features, leading to insufficient utilization of unique information in each channel. To address this issue, we propose the Channel-Aware Feature Mining Network (CAFMNet) to improve VI-ReID effectiveness. Specifically, we design three core modules: a Channel-Level Feature Optimization (CLFO) module, which captures channel-level key features for identity recognition and directly extracts identity-relevant information at the channel level; a Channel-Level Feature Refinement (CLFR) module, which enhances channel-level features while retaining useful information—addressing the irrelevant content in initially extracted features; a Multi-Dimensional Feature Optimization (MDFO) module, which comprehensively processes multi-dimensional feature information to enhance the model’s ability to understand and describe input data. Extensive experiments on the SYSU-MM01 and LLCM datasets demonstrate that our CAFMNet outperforms existing approaches in terms of VI-ReID effectiveness. The code is available at https://github.com/cobeibei/CAFMNet-1 .},
  archive      = {J_CVIU},
  author       = {Pengxia Li and Zhonghao Du and Linhui Zhang and Yanyi Lv and Yujie Liu},
  doi          = {10.1016/j.cviu.2025.104552},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104552},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Channel-aware feature mining network for Visible–Infrared person re-identification},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCESS-net: Semantic consistency enhancement and segment selection network for audio–visual event localization. <em>CVIU</em>, <em>262</em>, 104551. (<a href='https://doi.org/10.1016/j.cviu.2025.104551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a central task in multi-modal learning, audio–visual event localization seeks to identify consistent event information within visual–audio segments and to identify event categories. Some works do not consider the impact of visual features on audio features and the issue of segment information loss in selecting semantically consistent segments. To address the aforementioned issues, we introduce a network that enhances the multi-task learning performance of visual–audio modalities and resolves the semantic inconsistency present in audio–visual segments by employing bi-directional collaborative guided attention and semantic consistency enhancement. Firstly, we introduce a bi-directional collaborative guided attention module, which integrates multi-modal linear pooling and spatial-channel attention to bolster the semantic information of both audio and visual features across the visual-guided audio attention and audio-guided visual attention pathways. Subsequently, we propose an innovative multi-modal similarity learning model that addresses the issue of information loss during the filtering of low-similarity segments, which is a common problem in existing approaches. By incorporating multi-modal feature random masking, this model is capable of learning robust audio–visual relationships. Lastly, we capture global semantic information across the entire video in the temporal dimension, and enhance semantic consistency of events by using differential semantics between global semantics and audio–visual segment semantics. The experimental results on the AVE dataset indicate that our network has achieved superior performance.},
  archive      = {J_CVIU},
  author       = {Jichen Gao and Suiping Zhou and Hang Yu and Chenyang Li and Xiaoxi Hu},
  doi          = {10.1016/j.cviu.2025.104551},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104551},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SCESS-net: Semantic consistency enhancement and segment selection network for audio–visual event localization},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSFENet: Multi-scale filter-enhanced network architecture for digital image forgery trace localization. <em>CVIU</em>, <em>262</em>, 104550. (<a href='https://doi.org/10.1016/j.cviu.2025.104550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of image editing technologies, forensic analysis for detecting malicious image manipulations has become a critical research topic. While existing deep learning-based forgery localization methods have demonstrated promising results, they face three fundamental limitations: (1) heavy reliance on large-scale annotated datasets, (2) computationally intensive training processes, and (3) insufficient capability in capturing diverse forgery traces. To address these challenges, we present MSFENet (Multi-Scale Filter-Enhanced Network), a novel framework that synergistically integrates multiple forensic filters for comprehensive forgery detection. Our approach introduces three key innovations: First, we employ a multi-filter feature extraction module that combines NoisePrint++, SRM, and Bayar Conv to capture complementary forensic traces, including noise patterns, texture inconsistencies, and boundary artifacts. Second, we introduce a dual-branch multi-scale encoder that effectively preserves both local and global manipulation characteristics. Third, we design two novel components: the Coordinate Attention-based Cross-modal Feature Rectification (CAFR) module, which adaptively recalibrates feature representations across different modalities and learns the complementary properties of different extracted features, and the Multi-Scale Selective Fusion (MSF) module, which intelligently integrates discriminative features while suppressing redundant information. Extensive experiments on six benchmark datasets demonstrate the superiority of MSFENet. Our method achieves state-of-the-art performance, with F1-score improvements of 6.36%, 0.84%, 6.22%, and 48.8% on Casiav1, COVER, IMD20, and DSO-1, respectively, compared to existing methods.},
  archive      = {J_CVIU},
  author       = {Min Mao and Ge Jiao and Wanhui Gao and Jixun Ye},
  doi          = {10.1016/j.cviu.2025.104550},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104550},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MSFENet: Multi-scale filter-enhanced network architecture for digital image forgery trace localization},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RefineHOS: A high-performance hand–object segmentation with fine-grained spatial features. <em>CVIU</em>, <em>262</em>, 104548. (<a href='https://doi.org/10.1016/j.cviu.2025.104548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of hands and interacting objects is a critical challenge in computer vision, attributed to complex factors like mutual occlusion, finger self-similarity, and high hand movement flexibility. To tackle these issues, we present RefineHOS, an innovative framework for precise pixel-level segmentation of hands and interacting objects. Based on RefineMask, RefineHOS features substantial architectural optimizations for hand–object interaction scenarios. Specifically, we designed an Augmentation Feature Pyramid Path module (AFPN) integrated with a Dual Attention module (DAM) in the backbone to capture multi-scale feature information of hands and interacting objects. Additionally, we enhanced segmentation performance by introducing a Triplet Attention module (TAM) to optimize both the mask head and the semantic head. We also presented a new Boundary Refinement Module (BRM), utilizing an iterative subdivision approach to enhance the precision of boundary details in the segmentation results. Extensive experiments on multiple benchmark datasets (including VISOR, Ego-HOS, and ENIGMA-51) show that our method achieves state-of-the-art performance. To comprehensively evaluate segmentation quality, we introduce Boundary Average Precision (Boundary AP) as a key metric to complement existing benchmark segmentation metrics.},
  archive      = {J_CVIU},
  author       = {Wenrun Wang and Jianwu Dang and Yangping Wang and Rui Pan},
  doi          = {10.1016/j.cviu.2025.104548},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104548},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {RefineHOS: A high-performance hand–object segmentation with fine-grained spatial features},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VMM: Video-music mamba for generating background music from videos. <em>CVIU</em>, <em>262</em>, 104545. (<a href='https://doi.org/10.1016/j.cviu.2025.104545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the task of video background music generation, focusing on symbolic music generation. We propose a hybrid modeling approach that jointly considers both long-term and short-term musical structures. While existing research predominantly focuses on long-term patterns in music, the critical role of short-term structures in shaping musical expressiveness has been largely overlooked. Short-term structures provide intrinsic logic and emotional tension through dynamic variations and immediacy, synergizing with long-term structures to form cohesive musical narratives. To this end, we propose the VMM model, which innovatively integrates Mamba and Transformer: the Mamba module is designed to captures long-term dependencies via state space modeling, while the Transformer decodes short-term interactions among local notes through self-attention mechanisms. To further enhance multimodal understanding, we introduce Video-Music Generation Framework (VMGF), which incorporates a Switch Schedule mechanism that dynamically selects fusion strategies between video features and chord features during training and mitigates multimodal representation conflicts through gradient control. Experiments demonstrate that VMM achieves state-of-the-art performance in symbolic music generation tasks, excelling in metrics such as structural coherence and emotional consistency, thereby validating its leading capabilities in the field of symbolic music generation. Code and generated samples are available at https://jiajunxiii.github.io/VMM-Video-Music-Mamba .},
  archive      = {J_CVIU},
  author       = {Jiajun Xu and Zixiang Lu and Ping Gao and Qiguang Miao and Kun Xie},
  doi          = {10.1016/j.cviu.2025.104545},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104545},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {VMM: Video-music mamba for generating background music from videos},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XLITE-unet: Extremely light and efficient deep learning architecture with selective atrous and axial depthwise convolution for image segmentation. <em>CVIU</em>, <em>262</em>, 104543. (<a href='https://doi.org/10.1016/j.cviu.2025.104543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has achieved an exceptional success in image analysis making it easier to quickly and efficiently understand their content. Although deep learning models have shown exceptional performance in image segmentation, their computational demands restrict the range of devices capable of executing these models. This paper introduces a computationally efficient deep neural network named Extremely Light Unet (XLITE-UNET) as a solution to this challenge. Through the use of axial depthwise and selective atrous convolutions within Unet convolution blocks and the substitution of skip connections with a tiny channel attention (TCA) module, a lightweight and efficient deep convolutional neural network architecture is introduced in this paper. Experimental results on several benchmark datasets including COVID-19 CT scans, Data Science Bowl 2018, Lung Segmentation, PH2 dataset, and CVC-ClinicDB show that our model surpasses existing heavyweights and lightweights segmentation models in several aspects such as: (a) Low computational cost, with floating points operation (FLOP) less than 1.6M; (b) reduced parameter count; (c) best accuracy in cases of small size segments; and (d) compact model size, enabling storage on smaller memory devices.},
  archive      = {J_CVIU},
  author       = {Cezar Mbiethieu and Norbert Tsopze and Engelbert Mephu-Nguifo},
  doi          = {10.1016/j.cviu.2025.104543},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104543},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {XLITE-unet: Extremely light and efficient deep learning architecture with selective atrous and axial depthwise convolution for image segmentation},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating ConvNeXt and vision transformers for enhancing facial age estimation. <em>CVIU</em>, <em>262</em>, 104542. (<a href='https://doi.org/10.1016/j.cviu.2025.104542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age estimation from facial images is a complex and multifaceted challenge in computer vision. In this study, we present a novel hybrid architecture that combines ConvNeXt, a state-of-the-art advancement of convolutional neural networks (CNNs), with Vision Transformers (ViT). While each model independently delivers excellent performance on a variety of tasks, their integration leverages the complementary strengths of the CNNs’ localized feature extraction capabilities and the Transformers’ global attention mechanisms. Our proposed ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior performance in terms of mean absolute error (MAE). To address computational constraints, we leverage pre-trained models and systematically explore different configurations, using linear layers and advanced regularization techniques to optimize the architecture. Comprehensive ablation studies highlight the critical role of individual components and training strategies, and in particular emphasize the importance of adapted attention mechanisms within the CNN framework to improve the model’s focus on age-relevant facial features. The results show that the ConvNeXt-ViT hybrid not only outperforms traditional methods, but also provides a robust foundation for future advances in age estimation and related visual tasks. This work underscores the transformative potential of hybrid architectures and represents a promising direction for the seamless integration of CNNs and transformers to address complex computer vision challenges.},
  archive      = {J_CVIU},
  author       = {G. Maroun and S.E. Bekhouche and J. Charafeddine and F. Dornaika},
  doi          = {10.1016/j.cviu.2025.104542},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104542},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Integrating ConvNeXt and vision transformers for enhancing facial age estimation},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time fusion of stereo vision and hyperspectral imaging for objective decision support during surgery. <em>CVIU</em>, <em>262</em>, 104541. (<a href='https://doi.org/10.1016/j.cviu.2025.104541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a real-time stereo hyperspectral imaging (stereo-HSI) system for intraoperative tissue and organ analysis that integrates multispectral snapshot imaging with stereo vision to support clinical decision-making. The system visualize both RGB and high-dimensional spectral data while simultaneously reconstructing 3D surfaces, offering a compact, non-contact solution for seamless integration into surgical workflows. A modular processing pipeline enables robust demosaicing, spectral and spatial fusion, and pixel-wise medical assessment, including perfusion and tissue classification. Our spectral warping algorithm leverages a custom learned mapping, our white-balance network method is the first for snapshot MSI cameras, and our fusion CNN employs spectral-attention modules to exploit the rich hyperspectral domain. Clinical feasibility was demonstrated in 57 surgical procedures, including kidney transplantation, parotidectomy, and neck dissection, achieving high spatial and spectral resolution under standard surgical lighting conditions. The system enables visualization of oxygenation and tissue composition in real-time, offering surgeons a novel tool for image-guided interventions. This study establishes the stereo-HSI platform as a clinically viable and effective method for enhancing intraoperative insight and surgical precision.},
  archive      = {J_CVIU},
  author       = {Eric L. Wisotzky and Jost Triller and Michael Knoke and Brigitta Globke and Anna Hilsmann and Peter Eisert},
  doi          = {10.1016/j.cviu.2025.104541},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104541},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Real-time fusion of stereo vision and hyperspectral imaging for objective decision support during surgery},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Student gaze target estimation based on depth transformation on dual-view classroom images. <em>CVIU</em>, <em>262</em>, 104533. (<a href='https://doi.org/10.1016/j.cviu.2025.104533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing gaze target estimation methods fail to adequately consider depth information, primarily focusing on 2D image features while neglecting the inherent 3D spatial context that could enhance global context modeling in classroom environments. To address this limitation, we propose a depth-aware gaze target estimation framework specifically designed for classroom scenarios. Our approach consists of three key components: First, a depth estimation module is developed to handle feature information degradation. Second, we design a dual-view depth transformation method to project students’ gaze cones onto the target frame. Third, we introduce a context-aware pyramid feature extraction (CPFE) module that generates multiscale high-level feature representations to strengthen global context modeling. We also construct two datasets (MPMOCS and DVSEG) for our tasks. Experimental results on these datasets demonstrate that our method achieves significant improvements in both single-view and dual-view gaze target estimation tasks.},
  archive      = {J_CVIU},
  author       = {Xiaolong Zhang and Haonan Miao and Peizheng Zhao and Yuqi Sun and Fang Nan and Saberi Morteza and Yaqiang Wu and Feng Tian},
  doi          = {10.1016/j.cviu.2025.104533},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104533},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Student gaze target estimation based on depth transformation on dual-view classroom images},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards 4D human video stylization. <em>CVIU</em>, <em>262</em>, 104532. (<a href='https://doi.org/10.1016/j.cviu.2025.104532'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a first step towards 4D (3D space and time) human video stylization, which addresses style transfer, novel view synthesis, and human animation within a unified framework. While numerous video stylization methods have been developed, they are typically restricted to rendering images in specific viewpoints of the input video, lacking the capability to generalize to novel views and novel poses in dynamic scenes. To overcome these limitations, we leverage Neural Radiance Fields (NeRFs) to represent and stylize videos within a single framework. Our method involves simultaneously representing the human subject and the surrounding scene using two NeRFs. This dual representation facilitates the animation of human subjects across various poses and novel viewpoints. A key innovation is our introduction of a geometry-guided tri-plane representation, which significantly boosts the efficiency and robustness of the feature representation compared to direct tri-plane optimization. Stylization is performed within the NeRF rendered feature space, which can reduce the computational burden compared to applying style transformation to the feature vector of sampled points. Extensive experiments demonstrate that the proposed method strikes a superior balance between stylized textures and temporal coherence, surpassing existing approaches. Furthermore, our framework uniquely extends its capabilities to accommodate novel poses and viewpoints, making it a versatile tool for creative human video stylization. The source code and results will be available at this github site . The stylized videos are available in this Youtube video .},
  archive      = {J_CVIU},
  author       = {Tiantian Wang and Xinxin Zuo and Fangzhou Mu and Jian Wang and Ming-Hsuan Yang},
  doi          = {10.1016/j.cviu.2025.104532},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104532},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Towards 4D human video stylization},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCENE-net: Geometric induction for interpretable and low-resource 3D pole detection with group-equivariant non-expansive operators. <em>CVIU</em>, <em>262</em>, 104531. (<a href='https://doi.org/10.1016/j.cviu.2025.104531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces SCENE-Net, a novel low-resource, white-box model that serves as a compelling proof-of-concept for 3D point cloud segmentation. At its core, SCENE-Net employs Group Equivariant Non-Expansive Operators (GENEOs), a mechanism that leverages geometric priors for enhanced object identification. Our contribution extends the theoretical landscape of geometric learning, highlighting the utility of geometric observers as intrinsic biases in analyzing 3D environments. Through empirical testing and efficiency analysis, we demonstrate the performance of SCENE-Net in detecting power line supporting towers, a key application in forest fire prevention. Our results showcase the superior accuracy and resilience of our model to label noise, achieved with minimal computational resources—this instantiation of SCENE-Net has only eleven trainable parameters—thereby marking a significant step forward in trustworthy machine learning applied to 3D scene understanding. Our code is available in: https://github.com/dlavado/scene-net .},
  archive      = {J_CVIU},
  author       = {Diogo Lavado and Alessandra Micheletti and Giovanni Bocchi and Patrizio Frosini and Cláudia Soares},
  doi          = {10.1016/j.cviu.2025.104531},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104531},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SCENE-net: Geometric induction for interpretable and low-resource 3D pole detection with group-equivariant non-expansive operators},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic deep multi-label image data augmentation based on self-paced learning. <em>CVIU</em>, <em>262</em>, 104530. (<a href='https://doi.org/10.1016/j.cviu.2025.104530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification is crucial in image recognition. However, data class imbalance can lead to poor performance in classification algorithms, particularly for minority classes, thereby impacting overall accuracy. Existing augmentation methods for minority class data typically adopt a dataset-wide approach, failing to enhance minority class samples based on the model’s performance for each class during training. In this paper, we propose an end-to-end Self-paced Deep Multi-label data Augmentation (SDMA) method which is capable of dynamically generating image data for minority classes in multi-label classification. Our method selects a batch of relatively simple samples as the training set during each training iteration. The number of samples from each category in this batch reflects the model’s training performance on those categories. By generating data for the minority classes within these samples, we can dynamically generate samples that better meet the model’s needs based on its training progress. Additionally, to mitigate the impact of noisy data in the generated set, we compute the predicted similarity of the generated data with the seed and reference instances, excluding low-similarity data from influencing the model. Extensive experiments on multi-label datasets demonstrate that SDMA is competitive compared to other state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Bin Yu and Wei Li and Chen Zhang and Wenjie Mao and Yu Xie},
  doi          = {10.1016/j.cviu.2025.104530},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104530},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dynamic deep multi-label image data augmentation based on self-paced learning},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCNeXt: An effective self-supervised stereo depth estimation approach. <em>CVIU</em>, <em>262</em>, 104529. (<a href='https://doi.org/10.1016/j.cviu.2025.104529'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth Estimation plays a crucial role in recent applications in robotics, autonomous vehicles, and augmented reality. These scenarios commonly operate under constraints imposed by computational power. Stereo image pairs offer an effective solution for depth estimation since it only needs to estimate the disparity of pixels in image pairs to determine the depth in a known rectified system. Due to the difficulty in acquiring reliable ground-truth depth data across diverse scenarios, self-supervised techniques emerge as a solution, particularly when large unlabeled datasets are available. We propose a novel self-supervised convolutional approach that outperforms existing state-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) while balancing computational cost. The proposed CCNeXt architecture employs a modern CNN feature extractor with a novel windowed epipolar cross-attention module in the encoder, complemented by a comprehensive redesign of the depth estimation decoder. Our experiments demonstrate that CCNeXt achieves competitive metrics on the KITTI Eigen Split test data while being 10.18 × faster than the current best model and achieves state-of-the-art results in all metrics in the KITTI Eigen Split Improved Ground Truth and Driving Stereo datasets when compared to recently proposed techniques. To ensure complete reproducibility, our project is accessible at https://github.com/alelopes/CCNext .},
  archive      = {J_CVIU},
  author       = {Alexandre Lopes and Roberto Souza and Helio Pedrini},
  doi          = {10.1016/j.cviu.2025.104529},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104529},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CCNeXt: An effective self-supervised stereo depth estimation approach},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to recognize correctly completed procedure steps in egocentric assembly videos through spatio-temporal modeling. <em>CVIU</em>, <em>262</em>, 104528. (<a href='https://doi.org/10.1016/j.cviu.2025.104528'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedure step recognition (PSR) aims to identify all correctly completed steps and their sequential order in videos of procedural tasks. The existing state-of-the-art models rely solely on detecting assembly object states in individual video frames. By neglecting temporal features, model robustness and accuracy are limited, especially when objects are partially occluded. To overcome these limitations, we propose Spatio-Temporal Occlusion-Resilient Modeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework for PSR that leverages both spatial and temporal features. The assembly state detection stream operates effectively with unobstructed views of the object, while the spatio-temporal stream captures both spatial and temporal features to recognize step completions even under partial occlusion. This stream includes a spatial encoder, pre-trained using a novel weakly supervised approach to capture meaningful spatial representations, and a transformer-based temporal encoder that learns how these spatial features relate over time. STORM-PSR is evaluated on the MECCANO and IndustReal datasets, reducing the average delay between actual and predicted assembly step completions by 11.2% and 26.1%, respectively, compared to prior methods. We demonstrate that this reduction in delay is driven by the spatio-temporal stream, which does not rely on unobstructed views of the object to infer completed steps. The code for STORM-PSR, along with the newly annotated MECCANO labels, is made publicly available at https://timschoonbeek.github.io/stormpsr .},
  archive      = {J_CVIU},
  author       = {Tim J. Schoonbeek and Shao-Hsuan Hung and Dan Lehman and Hans Onvlee and Jacek Kustra and Peter H.N. De With and Fons Van der Sommen},
  doi          = {10.1016/j.cviu.2025.104528},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104528},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning to recognize correctly completed procedure steps in egocentric assembly videos through spatio-temporal modeling},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer-aided design of personalized occlusal positioning splints using multimodal 3D data. <em>CVIU</em>, <em>262</em>, 104527. (<a href='https://doi.org/10.1016/j.cviu.2025.104527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital technology plays a crucial role in designing customized medical devices, such as occlusal splints, commonly used in the management of disorders of the stomatognathic system. This methodological proof-of-concept study presents a computer-aided approach for designing and evaluating occlusal positioning splints. The primary aim is to demonstrate the feasibility and geometric accuracy of the proposed method at the preclinical stage. In this approach, a three-dimensional splint is generated using a transformation matrix to represent the therapeutic mandibular position. An experienced operator defines this position using a virtual patient model reconstructed from intraoral scans, CBCT, 3D facial scans, and a digitized plaster model. We introduce a novel method for generating splints that reproduces occlusal conditions in the therapeutic position and resolves surface conflicts through virtual embossing. The process for obtaining transformation matrices using dental tools and intraoral devices commonly employed in dental and laboratory workflows is described, and the geometric accuracy of both designed and printed splints is evaluated using profile and surface deviation analysis. The method supports reproducible, patient-specific splint fabrication and provides a transparent foundation for future validation studies, supporting multimodal image registration and quantification of occlusal discrepancies in research settings.},
  archive      = {J_CVIU},
  author       = {Agnieszka Anna Tomaka and Leszek Luchowski and Michał Tarnawski and Dariusz Pojda},
  doi          = {10.1016/j.cviu.2025.104527},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104527},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Computer-aided design of personalized occlusal positioning splints using multimodal 3D data},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDC-net: A novel selective dilated convolution network for medical images segmentation. <em>CVIU</em>, <em>262</em>, 104526. (<a href='https://doi.org/10.1016/j.cviu.2025.104526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is a critical technology that enables medical professional personnels to accurately identify lesions, organs, and other key structures from various medical images. In this area, deep learning-based methods have emerged as the mainstream approach for medical image segmentation. However, the inherent variability in lesion size and spatial dimensions still presents significant challenges for achieving precise medical segmentation, as current dilated convolution-based approaches exhibit limitations in effectively capturing and processing multi-scale pathological features. In order to overcome the shortcomings of the current techniques, this paper proposes a novel deep neural network (DNN) architecture, referred to as selective dilated convolution network (SDC-Net), to address the limitations of existing methods in multi-scale feature extraction, detail preservation, and feature fusion. Within the SDC-Net, the selective dilated convolution modules (SDCMs), the wavelet convolution module (WCM), and the feature fusion modules (FFMs) are proposed: The SDCMs capture multi-scale contextual information through cascaded dilated convolutions and dynamic weight adjustment. The WCM extracts multi-spectral features with wavelet transforms, enhancing the capture of complex shapes and boundaries. The FFMs optimize feature fusion with attention mechanisms, improving segmentation accuracy. Through carefully designed comparative experiments, the SDC-Net is shown to outperform the state-of-the-art methods including CE-Net, CPF-Net and Rolling U-Net, with the metrics of Dice, Average Symmetric Surface Distance (ASSD), and mean Intersection-over-Union (mIoU) reaching 94.55%, 0.40 pix and 89.57%, 95.21%, 0.29 pix and 91.11%, and 92.05%, 0.59 pix and 86.37%, on three publicly available medical image datasets: ISIC-2018 (melanoma dermoscopy), Kvasir-SEG (gastrointestinal disease polyp endoscopy) and Figshare (brain tumor magnetic resonance scans), respectively.},
  archive      = {J_CVIU},
  author       = {Chaoqun Ma and Rongsheng Cui and Feng Liu and Chunli Cai},
  doi          = {10.1016/j.cviu.2025.104526},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104526},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SDC-net: A novel selective dilated convolution network for medical images segmentation},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D2PCFN: Dual domain progressive cross-fusion network for remote sensing image pansharpening. <em>CVIU</em>, <em>262</em>, 104525. (<a href='https://doi.org/10.1016/j.cviu.2025.104525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution multispectral (HRMS) image generation through pansharpening requires effective integration of spatial details from panchromatic (PAN) images and spectral information from low-resolution multispectral (LRMS) images. Existing methods often overlook interactions between deep features across different depths and modalities, resulting in spectral distortion and loss of spatial detail. To address this, we propose a dual domain progressive cross-fusion network (D2PCFN) that progressively integrates features in both spatial and frequency domains. The network consists of a dual-branch feature generation module (DBFGM) for deep feature extraction, a dual domain cross-fusion module (D2CFM) for cross-interaction between spatial and frequency representations, and a deep feature reconstruction module (DFRM) for synthesizing high-quality outputs. Extensive experiments on GaoFen-2, QuickBird, WorldView-3, and WorldView-2 datasets demonstrate that our method achieves state-of-the-art accuracy, with average gains of 1.77% in SAM, 1.70% in ERGAS, 0.89% in PSNR, and 1.37% in HQNR over leading methods. Both quantitative and qualitative results confirm the effectiveness and generalization ability of the proposed D2PCFN. Source code will also be shared on https://github.com/MysterYxby/D2PCFN -website link after publication.},
  archive      = {J_CVIU},
  author       = {Biyun Xu and Yan Zheng and Suleman Mazhar and Zhenghua Huang},
  doi          = {10.1016/j.cviu.2025.104525},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104525},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {D2PCFN: Dual domain progressive cross-fusion network for remote sensing image pansharpening},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning multiscale residual prototypes and global–local correspondence for video anomaly detection. <em>CVIU</em>, <em>262</em>, 104524. (<a href='https://doi.org/10.1016/j.cviu.2025.104524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the powerful generalization ability of deep learning, existing video anomaly detection methods based on reconstruction or prediction can reconstruct not only normal but also abnormal frames. In response to this issue, numerous methods have been suggested to learn prototypes of normal events to suppress the generalization ability of networks towards anomalies. However, the majority of prototypes in these methods are single-scaling, which leads to an incomplete representation of the intricate complexity of normal events. Additionally, current methods tend to overemphasize global information while neglecting the importance of local information and its correspondence with local information. To address the aforementioned issues, we propose to learn Multiscale Residual Prototypes and Global–Local Correspondence (MRP-GLC) for video anomaly detection. Specifically, we introduce a Multiscale Residual Prototypes Fusion (MRPF) module that enables the learning and integration of multi-scale prototypes of normal events. The utilization of a residual design in this module ensures that the prototypes of normal events are learned while retaining the respective properties of each individual normal event. To facilitate global–local feature correspondence, we adopt a dual-encoder architecture to learn both local and global features, which are subsequently aggregated through our global–local fusion module and fed into the decoder for future frame prediction. The global–local fusion module leverages global features to prioritize attention on local features, enabling the capture of correspondence between global and local information. Extensive experiments conducted on three public datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_CVIU},
  author       = {Yongting Hu and Yuanhong Zhong and Jinkai Li and Xin Wang},
  doi          = {10.1016/j.cviu.2025.104524},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104524},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning multiscale residual prototypes and global–local correspondence for video anomaly detection},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MOSAIC: A multi-view 2.5D organ slice selector with cross-attentional reasoning for anatomically-aware CT localization in medical organ segmentation. <em>CVIU</em>, <em>262</em>, 104522. (<a href='https://doi.org/10.1016/j.cviu.2025.104522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and accurate multi-organ segmentation from abdominal CT volumes is a fundamental challenge in medical image analysis. Existing 3D segmentation approaches are computationally and memory intensive, often processing entire volumes that contain many anatomically irrelevant slices. Meanwhile, 2D methods suffer from class imbalance and lack cross-view contextual awareness. To address these limitations, we propose a novel, anatomically-aware slice selector pipeline that reduces input volume prior to segmentation. Our unified framework introduces a vision-language model (VLM) for cross-view organ presence detection using fused tri-slice (2.5D) representations from axial, sagittal, and coronal planes. Our proposed model acts as an “expert” in anatomical localization, reasoning over multi-view representations to selectively retain slices with high structural relevance. This enables spatially consistent filtering across orientations while preserving contextual cues. More importantly, since standard segmentation metrics such as Dice or IoU fail to measure the spatial precision of such slice selection, we introduce a novel metric, Slice Localization Concordance (SLC), which jointly captures anatomical coverage and spatial alignment with organ-centric reference slices. Unlike segmentation-specific metrics, SLC provides a model-agnostic evaluation of localization fidelity. Our model offers substantial improvement gains against several baselines across all organs, demonstrating both accurate and reliable organ-focused slice filtering. These results show that our method enables efficient and spatially consistent organ filtering, thereby significantly reducing downstream segmentation cost while maintaining high anatomical fidelity.},
  archive      = {J_CVIU},
  author       = {Hania Ghouse and Muzammil Behzad},
  doi          = {10.1016/j.cviu.2025.104522},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104522},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MOSAIC: A multi-view 2.5D organ slice selector with cross-attentional reasoning for anatomically-aware CT localization in medical organ segmentation},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-aligned distillation for dense object detection via refined semantic guidance and distribution consistency. <em>CVIU</em>, <em>262</em>, 104519. (<a href='https://doi.org/10.1016/j.cviu.2025.104519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Distillation (KD), as an efficient model compression technology, has been widely used in object detection tasks. However, existing distillation methods generally lack refined knowledge guidance for student models, making it difficult to meet the high requirements of dense object detection for fine-grained perception capabilities. In this paper, we propose a feature-aligned knowledge distillation method based on refined semantic guidance and distribution consistency, aiming to improve the performance of lightweight student models in dense object detection. Specifically, we first design a Lightweight Semantic-Guided Feature Imitation (LSFI) module that efficiently extracts multi-level semantic information across spatial and channel dimensions from the teacher’s feature maps at low computational cost, offering detailed and discriminative feature to the student. Furthermore, to alleviate the structural distribution gap between student and teacher features, we introduce Multi-Metric Distribution Consistency Knowledge Distillation (MDC-KD) that constructs consistency constraints from both global statistical correlation and local structural similarity, further enhancing the student’s feature alignment ability. Extensive experiments on MS COCO 2017, PASCAL VOC and Cityscapes datasets demonstrate that our method achieves significant performance improvements across various detectors and heterogeneous backbones, while substantially reducing the number of parameters and computational complexity of the teacher model.},
  archive      = {J_CVIU},
  author       = {Ximing Li and Xiaoguang Di and Maozhen Liu and Shaoxun Ye},
  doi          = {10.1016/j.cviu.2025.104519},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104519},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Feature-aligned distillation for dense object detection via refined semantic guidance and distribution consistency},
  volume       = {262},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IP-CAM: Class activation mapping based on importance weights and principal-component weights for better and simpler visual explanations. <em>CVIU</em>, <em>261</em>, 104523. (<a href='https://doi.org/10.1016/j.cviu.2025.104523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual explanations of deep neural networks (DNNs) have gained considerable importance in deep learning due to the lack of interpretability, which constrains human trust in DNNs. This paper proposes a new gradient-free class activation map (CAM) architecture called importance principal-component CAM (IP-CAM). The architecture not only improves the prediction accuracy of networks but also provides simpler and more reliable visual explanations. It adds importance weight layers before the classifier and assigns an importance weight to each activation map. After fine-tuning, it selects images with the highest prediction score for each class, performs principal component analysis (PCA) on activation maps of all channels, and regards the eigenvector of the first principal component as principal-component weights for that class. The final saliency map is obtained by linearly combining the activation maps, importance weights and principal-component weights. IP-CAM is evaluated on the ILSVRC 2012 dataset and RSD46-WHU dataset, whose results show that IP-CAM performs better than most previous CAM variants in recognition and localization tasks. Finally, the method is applied as a tool for interpretability, and the results illustrate that IP-CAM effectively unveils the decision-making process of DNNs through saliency maps.},
  archive      = {J_CVIU},
  author       = {Wenyi Zhang and Haoran Zhang and Xisheng Zhang and Xiaohua Shen and Lejun Zou},
  doi          = {10.1016/j.cviu.2025.104523},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104523},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {IP-CAM: Class activation mapping based on importance weights and principal-component weights for better and simpler visual explanations},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSBATN: Multi-stage boundary-aware transformer network for action segmentation in untrimmed surgical videos. <em>CVIU</em>, <em>261</em>, 104521. (<a href='https://doi.org/10.1016/j.cviu.2025.104521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding actions within surgical workflows is critical for evaluating post-operative outcomes and enhancing surgical training and efficiency. Capturing and analysing long sequences of actions in surgical settings is challenging due to the inherent variability in individual surgeon approaches, which are shaped by their expertise and preferences. This variability complicates the identification and segmentation of distinct actions with ambiguous boundary start and end points. The traditional models, such as MS-TCN, which rely on large receptive fields, cause over-segmentation or under-segmentation, where distinct actions are incorrectly aligned. To address these challenges, we propose the Multi-Stage Boundary-Aware Transformer Network (MSBATN) with hierarchical sliding window attention to improve action segmentation. Our approach effectively manages the complexity of varying action durations and subtle transitions by accurately identifying start and end action boundaries in untrimmed surgical videos. MSBATN introduces a novel unified loss function that optimises action classification and boundary detection as interconnected tasks. Unlike conventional binary boundary detection methods, our innovative boundary weighing mechanism leverages contextual information to precisely identify action boundaries. Extensive experiments on three challenging surgical datasets demonstrate that MSBATN achieves state-of-the-art performance, with superior F1 scores at 25% and 50% thresholds and competitive results across other metrics.},
  archive      = {J_CVIU},
  author       = {Rezowan Shuvo and M.S. Mekala and Eyad Elyan},
  doi          = {10.1016/j.cviu.2025.104521},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104521},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MSBATN: Multi-stage boundary-aware transformer network for action segmentation in untrimmed surgical videos},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SVFFNet: A scale-aware voxel flow fusion network for video prediction. <em>CVIU</em>, <em>261</em>, 104520. (<a href='https://doi.org/10.1016/j.cviu.2025.104520'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video prediction is a challenging task due to the potential for various motion scales in the complex scene. The diversity of motion scales stems from the time-variant and object-dependent motion magnitudes, as well as the multiple image resolutions across datasets. However, the vast majority of frame forecasting networks do not distinguish between treatment of different motion scales. Therefore, their receptive field is normally insufficient to capture larger-scale motions. Those that do, often yield significant local distortions in the predicted images. The reasons lie in their fixed choice of scale factors and lack of cross-scale interaction between motion features. In this work, we propose a Scale-Aware Voxel Flow Fusion Network (SVFFNet) to address the motion scale inconsistency problem and fully integrate multi-scale feature. This network consists of a set of flow estimation blocks, each block containing a selector module and a fusion module. The selector module adaptively selects the appropriate scale-processing branch for the input frames, thus facilitating acquisition of more refined features for large-scale motion. The fusion module then combines these features with the original motion information via an attention mechanism, preserving the actually existing structural details. Experimental results on four widely used benchmark datasets demonstrate that our method outperforms previously published baselines for video prediction. The code is available at: https://github.com/zyaojlu/SVFFNet .},
  archive      = {J_CVIU},
  author       = {Yao Zhou and Jinpeng Wei and Xueyong Zhang and Yusong Zhai and Jian Wei},
  doi          = {10.1016/j.cviu.2025.104520},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104520},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SVFFNet: A scale-aware voxel flow fusion network for video prediction},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalization-preserving adaptation of vision-language models for open-vocabulary segmentation. <em>CVIU</em>, <em>261</em>, 104518. (<a href='https://doi.org/10.1016/j.cviu.2025.104518'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in large-scale Vision-Language Models (VLMs) has significantly advanced open-vocabulary segmentation. Previous works typically either generate class-agnostic masks and classify them with frozen VLMs, or align the mask generator features with VLM text features. These approaches face challenges of weak spatial discrimination ability of frozen VLMs and poor generalization due to unreliable vision-language alignment. This paper introduces a novel Generalization-Preserving Adaptation (GPA) of VLMs for open-vocabulary segmentation. GPA enhances the spatial discrimination capability of pre-trained VLMs through an efficient fine-tuning scheme, which incorporates a spatial adaptation module comprising spatial dependency modeling and low-rank feature modulation for preserving the feature space. Additionally, GPA proposes a context-aware feature aggregation module to extract mask features better aligned with the VLM features for mask classification. It performs decoupled context modeling that generates object-agnostic contextualized feature map and object-specific classification maps for accentuating discriminative and contextual clues. By maintaining the original VLM feature distribution for vision-language alignment, GPA effectively preserves the generalization capabilities of VLMs while enhancing segmentation performance. Extensive experiments on multiple open-vocabulary panoptic and semantic segmentation benchmarks demonstrate both superior effectiveness and generalization capabilities compared to previous works.},
  archive      = {J_CVIU},
  author       = {Zhen Chen and Hao Tang and Shiliang Zhang},
  doi          = {10.1016/j.cviu.2025.104518},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104518},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Generalization-preserving adaptation of vision-language models for open-vocabulary segmentation},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond geometry: The power of texture in interpretable 3D person ReID. <em>CVIU</em>, <em>261</em>, 104517. (<a href='https://doi.org/10.1016/j.cviu.2025.104517'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents FusionTexReIDNet, a robust framework for 3D person re-identification that uniquely leverages UVTexture to enhance both performance and explainability. Unlike existing 3D person ReID approaches that simply overlay textures on point clouds, our method exploits the full potential of UVTexture through its high resolution and normalized coordinate properties. The framework consists of two main streams: a UVTexture stream that processes appearance features and a 3D stream that handles geometric information. These streams are fused through an effective combination of KNN, attribute-based, and explainable re-ranking strategies. Our approach introduces explainability to 3D person ReID through the visualization of activation maps on UVTextures, providing insights into the model’s decision-making process by highlighting discriminative regions. By incorporating the Intersection-Alignment Score derived from activation maps and visible clothing masks, we further improve the ReID accuracy. Extensive experiments demonstrate that FusionTexReIDNet achieves state-of-the-art performance across various scenarios, with Rank-1 accuracies of 98.5% and 89.7% Rank-1 on benchmark datasets, while providing interpretable results through its explainable component.},
  archive      = {J_CVIU},
  author       = {Huy Nguyen and Kien Nguyen and Akila Pemasiri and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1016/j.cviu.2025.104517},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104517},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Beyond geometry: The power of texture in interpretable 3D person ReID},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic dual and efficient additive attention network for no-reference image quality assessment. <em>CVIU</em>, <em>261</em>, 104516. (<a href='https://doi.org/10.1016/j.cviu.2025.104516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {No-Reference Image Quality Assessment (NR-IQA) aims to evaluate the perceptual quality of images in alignment with human subjective judgments. However, most existing NR-IQA methods, while striving for high accuracy, often neglect computational complexity. To address this challenge, we propose a Synergistic Spatial and Channel and Efficient Additive Attention Network for NR-IQA. In our approach, we first employ a feature extraction module to derive features rich in both distortion and semantic information. Subsequently, we introduce a spatial-channel synergistic attention mechanism to enhance feature representations across spatial and channel dimensions. This attention module focuses on the most salient regions of the image and modulates feature responses accordingly, enabling the network to emphasize critical distortions and semantic features pertinent to perceptual quality assessment. Specifically, the spatial attention mechanism identifies significant regions that substantially contribute to quality perception, while the channel attention mechanism adjusts the importance of each feature channel, ensuring effective utilization of spatial and channel-specific information. Furthermore, to enhance the model’s robustness, we incorporate an Efficient Additive Attention mechanism alongside a Multi-scale Feed-forward Network, designed to reduce computational costs without compromising performance. Finally, a dual-branch structure for patch-weighted quality prediction is employed to derive the final quality score based on the weighted scores of individual patches. Extensive experimental evaluations on four widely used benchmark datasets demonstrate that the proposed method surpasses several state-of-the-art NR-IQA approaches in both performance and computational efficiency.},
  archive      = {J_CVIU},
  author       = {Zhou Fang and Baiming Feng and Ning Li},
  doi          = {10.1016/j.cviu.2025.104516},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104516},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Synergistic dual and efficient additive attention network for no-reference image quality assessment},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative caption generation with heuristic guidance for enhancing knowledge-based visual question answering. <em>CVIU</em>, <em>261</em>, 104515. (<a href='https://doi.org/10.1016/j.cviu.2025.104515'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of large language models (LLMs) has significantly advanced Knowledge-based Visual Question Answering (KBVQA) by reducing the reliance on external knowledge bases. Traditional methods often generate captions in a single pass, which can struggle with complex questions due to difficulty in precisely identifying key visual components. This challenge undermines the reasoning capabilities of LLMs, which require accurate, semantically aligned captions to answer complex questions effectively. To address this limitation, we propose ICGHG I terative C aption G eneration with H euristic G uidance, a novel framework that refines captions iteratively. Our approach incorporates a dynamic loop where captions are continuously refined based on heuristic feedback from a set of candidate answers and the question itself, ensuring that the final caption provides accurate semantic alignment with both the visual content and the question. By leveraging this iterative process, ICGHG mitigates common issues such as hallucinations and improves the quality of the generated captions. Extensive experiments on OK-VQA, A-OKVQA, and FVQA datasets demonstrate that ICGHG significantly outperforms existing methods, achieving 57.5%, 60.2%, and 69.4% accuracy on their respective test sets, setting new benchmarks in KBVQA accuracy.},
  archive      = {J_CVIU},
  author       = {Fengyuan Liu and Zhongjian Hu and Peng Yang and Xingyu Liu},
  doi          = {10.1016/j.cviu.2025.104515},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104515},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Iterative caption generation with heuristic guidance for enhancing knowledge-based visual question answering},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KD-mamba: Selective state space models with knowledge distillation for trajectory prediction. <em>CVIU</em>, <em>261</em>, 104499. (<a href='https://doi.org/10.1016/j.cviu.2025.104499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory prediction is a key component of intelligent mobility systems and human–robot interaction. The inherently stochastic nature of human behavior, coupled with external environmental influences, poses significant challenges for long-term prediction. However, existing approaches struggle to effectively model spatial interactions and accurately predict long-term destinations, while their high computational demands limit real-world applicability. To address these limitations, this paper presents KD-Mamba, the Selective State Space Models with Knowledge Distillation for trajectory prediction. The model incorporates the U-CMamba module, which features a U-shaped encoder–decoder architecture. By integrating convolutional neural networks (CNN) with the Mamba mechanism, this module effectively captures local spatial interactions and global contextual information of human motion patterns. Subsequently, we introduce a Bi-Mamba module, which captures long-term dependencies in human movement, ensuring a more accurate representation of trajectory dynamics. Knowledge distillation strengthens both modules by facilitating knowledge transfer across diverse scenarios. Compared to transformer-based approaches, KD-Mamba reduces computational complexity from quadratic to linear. Extensive experimental results from two real-world trajectory datasets indicate that KD-Mamba outperforms the existing mainstream baselines. The proposed method provides insights into the application of trajectory prediction in human-in-the-loop assistive systems.},
  archive      = {J_CVIU},
  author       = {Shaokang Cheng and Sourav Das and Shiru Qu and Lamberto Ballan},
  doi          = {10.1016/j.cviu.2025.104499},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104499},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {KD-mamba: Selective state space models with knowledge distillation for trajectory prediction},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gloss-free sign language translation: An unbiased evaluation of progress in the field. <em>CVIU</em>, <em>261</em>, 104498. (<a href='https://doi.org/10.1016/j.cviu.2025.104498'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign Language Translation (SLT) aims to automatically convert visual sign language videos into spoken language text and vice versa. While recent years have seen rapid progress, the true sources of performance improvements often remain unclear. Do reported performance gains come from methodological novelty, or from the choice of a different backbone, training optimizations, hyperparameter tuning, or even differences in the calculation of evaluation metrics? This paper presents a comprehensive study of recent gloss-free SLT models by re-implementing key contributions in a unified codebase. We ensure fair comparison by standardizing preprocessing, video encoders, and training setups across all methods. Our analysis shows that many of the performance gains reported in the literature often diminish when models are evaluated under consistent conditions, suggesting that implementation details and evaluation setups play a significant role in determining results. We make the codebase publicly available here 1 to support transparency and reproducibility in SLT research.},
  archive      = {J_CVIU},
  author       = {Ozge Mercanoglu Sincan and Jian He Low and Sobhan Asasi and Richard Bowden},
  doi          = {10.1016/j.cviu.2025.104498},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104498},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Gloss-free sign language translation: An unbiased evaluation of progress in the field},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage attribute-guided dual attention network for fine-grained fashion retrieval. <em>CVIU</em>, <em>261</em>, 104497. (<a href='https://doi.org/10.1016/j.cviu.2025.104497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained clothing retrieval is essential for intelligent shopping and personalized recommendation systems. However, conventional methods often fail to capture subtle attribute variations. This paper proposes a novel two-stage attribute-guided dual attention network. The network combines global and local feature extraction with Attribute-aware Multi-Scale Spatial Attention (AMSA) and Attribute-guided Dynamic Channel Attention (ADCA). AMSA captures attribute-specific spatial details at multiple scales, while ADCA dynamically adjusts channel importance based on attribute embeddings, enabling precise attribute-level similarity modeling. A multi-level joint loss function further optimizes both global and local representations and enhances feature alignment. Experiments on FashionAI and the self-built FGDress dataset show that the proposed method achieves mAP scores of 66.01% and 73.98%, respectively, outperforming baseline approaches. Attribute-level analysis confirms robust recognition of both well-defined and challenging attributes. These results validate the practicality and generalizability of the proposed framework, with promising applications in personalized recommendation, fashion trend analysis, and design evaluation.},
  archive      = {J_CVIU},
  author       = {Bo Pan and Jun Xiang and Ning Zhang and Ruru Pan},
  doi          = {10.1016/j.cviu.2025.104497},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104497},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Two-stage attribute-guided dual attention network for fine-grained fashion retrieval},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JSF: A joint spatial-frequency domain network for low-light image enhancement. <em>CVIU</em>, <em>261</em>, 104496. (<a href='https://doi.org/10.1016/j.cviu.2025.104496'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The enhancement of low-light images remains a prominent focus in the field of image processing. The degree of lightness significantly influences vision-based intelligent recognition and analysis. Departing from conventional methods, this paper proposes an innovative joint spatial-frequency domain network for low-light image enhancement, referred to as JSF. In the spatial domain, brightness is optimized through the amalgamation of global and local information. In the frequency domain, noise is reduced and details are amplified using Fourier Transformation to carry out amplitude and phase enhancement. Additionally, the enhanced results from the aforementioned domains are fused by linear and nonlinear stretching. To validate the effectiveness of JSF, this paper presents both qualitative and quantitative comparison results, demonstrating its superiority over several existing state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Yahong Wu and Feng Liu and Rong Wang},
  doi          = {10.1016/j.cviu.2025.104496},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104496},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {JSF: A joint spatial-frequency domain network for low-light image enhancement},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FCNet: A feature complementary network for nighttime flare removal. <em>CVIU</em>, <em>261</em>, 104495. (<a href='https://doi.org/10.1016/j.cviu.2025.104495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nighttime image flare removal is a very challenging task due to the presence of various types of unfavorable degrading effects, including glare, shimmer, streak and saturated blobs. Most of the existing methods focus on the spatial domain and limited perception field, resulting in incomplete flare removal and severe artifacts. To address these challenges, we propose a two-stage feature complementary network for nighttime flare removal, which is used for flare perception and removal, respectively. In the first stage, a Spatial-Frequency Complementary Module (SFCM) is designed to perceive the flare region from different domains to get a mask of the flare. In the second stage, the flare mask and image are fed into the Spatial-Frequency Complementary Gating Module (SFCGM) to preserve the background information, while removing the flares from different angles and restoring the detailed features. Finally the flare and non-flare regions are modeled by the Flare Interactive Module (FIM) to refine the flare regions at a fine-grained level to suppress the artifact problem. Extensive experiments on Flare 7K++ validate the superiority of the proposed approach over state-of-the-arts, both qualitatively and quantitatively.},
  archive      = {J_CVIU},
  author       = {Kejing Qi and Bo Wang and Chongyi Li},
  doi          = {10.1016/j.cviu.2025.104495},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104495},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FCNet: A feature complementary network for nighttime flare removal},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GRE-net: A forgery image detection framework based on gradient feature and reconstruction error. <em>CVIU</em>, <em>261</em>, 104494. (<a href='https://doi.org/10.1016/j.cviu.2025.104494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous technological breakthroughs in Generative Adversarial Networks (GANs) and diffusion models, remarkable progress has been achieved in the field of image generation. These technologies enable the creation of highly realistic images, thereby intensifying the risk of spreading fake information. However, traditional image detectors face a growing challenge of inadequate generalization capabilities when confronted with images generated by models that were not included during the training phase. To tackle this challenge, we introduce a novel detection framework, named GRE-Net (Network integrating Gradient and Reconstruction Error), which extracts gradient feature through the DPG module and calculates the reconstruction error utilizing the DIRE method. By integrating these two aspects into a comprehensive feature representation, GRE-Net effectively detects the authenticity of images. Specifically, we devise a dual-branch model that leverages the proposed DPG (Discriminator of ProjectedGAN to extract Gradient) module to extract gradient feature from images and concurrently employs the DIRE (DIffusion Reconstruction Error) method to obtain the diffusion reconstruction error of images. By fusing the features extracted from these two modules as a universal representation, we describe the artifacts produced by generative models, crafting a comprehensive detector capable of identifying both GAN-generated and diffusion model-generated images. Notably, the DPG approach utilizes the discriminator of ProjectedGAN as an intermediary bridge, mapping all data into the gradient domain. This transformation process effectively captures the intrinsic feature differences during the image generation process. Subsequently, the gradient feature are fed into a classifier to achieve efficient discrimination between authentic and fake images. To validate the efficacy of our proposed detector, we conducted evaluations on a dataset comprising images generated by ten diverse diffusion models and GANs. Extensive experiments demonstrate that our detector exhibits stronger generalization capabilities and higher robustness, rendering it suitable for real-world generated image detection tasks.},
  archive      = {J_CVIU},
  author       = {Wenqing Wu and Xinyi Shi and Jinghai Ai and Xiaodong Wang},
  doi          = {10.1016/j.cviu.2025.104494},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104494},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {GRE-net: A forgery image detection framework based on gradient feature and reconstruction error},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating forgetting in the adaptation of CLIP for few-shot classification. <em>CVIU</em>, <em>261</em>, 104493. (<a href='https://doi.org/10.1016/j.cviu.2025.104493'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapter-style efficient transfer learning has demonstrated outstanding performance in fine-tuning vision-language models, especially in scenarios with limited data. However, existing methods fail to effectively balance the prior knowledge acquired during the pre-training process and the training samples. To address this problem, we propose a method called Mitigating Forgetting in the Adaptation (MiFA) of CLIP. MiFA first employs class prototypes to represent the most prominent features of a class, and these prototypes provide a robust initialization for the classifier. To overcome the forgetting of prior knowledge, MiFA then leverages a memory module that retains the initial parameters and the parameters of training history by creating a memory weight through momentum. The weight is used to initialize a new classification layer, which, along with the original layer, guides each other to balance prior knowledge and feature adaptation. Similarly, in the text processing branch, a parallel initialization strategy is adopted to ensure that the model’s performance is improved. Text features are employed to initialize a text classification layer, and CLIP logits help prevent excessive forgetting of useful text information. Extensive experiments have demonstrated the effectiveness of our method.},
  archive      = {J_CVIU},
  author       = {Jiale Cao and Yuanheng Liu and Zhong Ji and Jingren Liu and Aiping Yang and Yanwei Pang},
  doi          = {10.1016/j.cviu.2025.104493},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104493},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Mitigating forgetting in the adaptation of CLIP for few-shot classification},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint multi-dimensional dynamic attention and transformer for general image restoration. <em>CVIU</em>, <em>261</em>, 104491. (<a href='https://doi.org/10.1016/j.cviu.2025.104491'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outdoor images often suffer from severe degradation due to rain, haze, and noise, impairing image quality and challenging high-level tasks. Current image restoration methods struggle to handle complex degradation while maintaining efficiency. This paper introduces a novel image restoration architecture that combines multi-dimensional dynamic attention and self-attention within a U-Net framework. To leverage the global modeling capabilities of transformers and the local modeling capabilities of convolutions, we integrate sole CNNs in the encoder–decoder and sole transformers in the latent layer. Additionally, we design convolutional kernels with selected multi-dimensional dynamic attention to capture diverse degraded inputs efficiently. A transformer block with transposed self-attention further enhances global feature extraction while maintaining efficiency. Extensive experiments demonstrate that our method achieves a better balance between performance and computational complexity across five image restoration tasks: deraining, deblurring, denoising, dehazing, and enhancement, as well as superior performance for high-level vision tasks. The source code will be available at https://github.com/House-yuyu/MDDA-former .},
  archive      = {J_CVIU},
  author       = {Huan Zhang and Xu Zhang and Nian Cai and Jianglei Di and Yun Zhang},
  doi          = {10.1016/j.cviu.2025.104491},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104491},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Joint multi-dimensional dynamic attention and transformer for general image restoration},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LAM-YOLO: Drones-based small object detection on lighting-occlusion attention mechanism YOLO. <em>CVIU</em>, <em>261</em>, 104489. (<a href='https://doi.org/10.1016/j.cviu.2025.104489'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drone-based target detection presents inherent challenges, including the high density and overlap of targets in drone images, as well as the blurriness of targets under varying lighting conditions, which complicates accurate identification. Traditional methods often struggle to detect numerous small, densely packed targets against complex backgrounds. To address these challenges, we propose LAM-YOLO, an object detection model specifically designed for drone-based applications. First, we introduce a light-occlusion attention mechanism to enhance the visibility of small targets under diverse lighting conditions. Additionally, we incorporate Involution modules to improve feature layer interactions. Second, we employ an improved SIB-IoU as the regression loss function to accelerate model convergence and enhance localization accuracy. Finally, we implement a novel detection strategy by introducing two auxiliary detection heads to better identify smaller-scale targets. Our quantitative results demonstrate that LAM-YOLO outperforms methods such as Faster R-CNN, YOLOv11, and YOLOv12 in terms of mAP@0.5 and mAP@0.5:0.95 on the VisDrone2019 public dataset. Compared to the original YOLOv8, the average precision increases by 7.1%. Additionally, the proposed SIB-IoU loss function not only accelerates convergence speed during training but also improves average precision compared to the traditional loss function.},
  archive      = {J_CVIU},
  author       = {Yuchen Zheng and Yuxin Jing and Jufeng Zhao and Guangmang Cui},
  doi          = {10.1016/j.cviu.2025.104489},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104489},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LAM-YOLO: Drones-based small object detection on lighting-occlusion attention mechanism YOLO},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Camera pose in SfT and NRSfM under isometric and weaker deformation models. <em>CVIU</em>, <em>261</em>, 104488. (<a href='https://doi.org/10.1016/j.cviu.2025.104488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera pose is a very natural concept in 3D vision in the rigid setting. It is however much more difficult to work with in deformable settings. Consequently, numerous deformable reconstruction methods simply ignore camera pose. We analyse the concept of pose in deformable settings and prove that it is unconstrained with the existing formulations, properly justifying the existing pose-less methods reconstructing structure only. We explain this result intuitively by the impossibility to define an intrinsic coordinate frame to a general deforming object. The proposed analysis uses the isometric deformation model and extends to the weaker models including conformality and equiareality We propose a novel prior to rescue camera pose estimation in deformable settings, which attributes the deforming object’s dominant rigid-body motion to the camera. We show that adding this prior to any existing formulation fully constrains camera pose and leads to elegant two-step solution methods, involving deformable structure reconstruction using a base method in the first step, and absolute orientation or Procrustes analysis in the second step. We derive the proposed approach for the template-based and template-less settings, respectively implemented using Shape-from-Template (SfT) and Non-Rigid Structure-from-Motion (NRSfM) as base methods and validate them experimentally, showing that the computed pose is qualitatively and quantitatively plausible.},
  archive      = {J_CVIU},
  author       = {Adrien Bartoli and Agniva Sengupta},
  doi          = {10.1016/j.cviu.2025.104488},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104488},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Camera pose in SfT and NRSfM under isometric and weaker deformation models},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-granularity balance learning for long-tailed image classification. <em>CVIU</em>, <em>261</em>, 104469. (<a href='https://doi.org/10.1016/j.cviu.2025.104469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In long-tailed datasets, the training of deep neural network-based models faces challenges, where the model may become biased towards the head classes with abundant training data, resulting in poor performance on tail classes with limited samples. Most current methods employ contrastive learning to learn more balanced representations by finding the class center. However, these methods use class centers to address local imbalance within a mini-batch, they overlook the global imbalance between batches throughout an epoch, caused by the long-tailed distribution of the dataset. In this paper, we propose bi-granularity balance learning to address the two-layer imbalance. We decouple the attraction–repulsion term in contrastive loss into two independent components: global and local balance. The global balance component focuses on capturing semantic information from different perspectives of the image and shifting learning attention from the head classes to the tail classes in the global perspective. The local balance component aims to learn inter-class separability from the local perspective. The proposed method efficiently learns the intra-class compactness and inter-class separability in long-tailed model training and improves the performance of the long-tailed model. Experimental results show that the proposed method achieves competitive performance on long-tailed benchmarks such as CIFAR-10/100-LT, TinyImageNet-LT, and iNaturalist 2018.},
  archive      = {J_CVIU},
  author       = {Ning Ren and Xiaosong Li and Yanxia Wu and Yan Fu},
  doi          = {10.1016/j.cviu.2025.104469},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104469},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Bi-granularity balance learning for long-tailed image classification},
  volume       = {261},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive regional guidance for attention map semantics in text-to-image diffusion models. <em>CVIU</em>, <em>260</em>, 104492. (<a href='https://doi.org/10.1016/j.cviu.2025.104492'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have shown remarkable success in image generation tasks. However, accurately interpreting and translating the semantic meaning of input text into coherent visuals remains a significant challenge. We observe that existing approaches often rely on enhancing attention maps in a pixel-based or patch-based manner, which can lead to issues such as non-contiguous regions, unintended region leakage, eventually causing attention maps with limited semantic richness, degrade output quality. To address these limitations, we propose CoRe Diffusion, a novel method that provides comprehensive regional guidance throughout the generation process. Our approach introduces a region-assignment mechanism coupled with a tailored optimization strategy, enabling attention maps to better capture and express semantic information of concepts. Additionally, we incorporate mask guidance during the denoising steps to mitigate region leakage. Through extensive comparisons with state-of-the-art methods and detailed visual analyses, we demonstrate that our approach achieves superior performance, offering a more faithful image generation framework with semantically accurate procedure. Furthermore, our framework offers flexibility by supporting both automatic region assignment and user-defined spatial inputs as conditional guidance, enhancing its adaptability for diverse applications.},
  archive      = {J_CVIU},
  author       = {Haoxuan Wu and Lai-Man Po and Xuyuan Xu and Kun Li and Yuyang Liu and Zeyu Jiang},
  doi          = {10.1016/j.cviu.2025.104492},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104492},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Comprehensive regional guidance for attention map semantics in text-to-image diffusion models},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-aware graph reasoning network for image manipulation localization. <em>CVIU</em>, <em>260</em>, 104490. (<a href='https://doi.org/10.1016/j.cviu.2025.104490'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution networks continue to be the dominant approach in current research on image manipulation localization. However, their inherent sensory field limitation results in the network primarily focusing on local feature extraction, which largely ignores the importance of long-range contextual information. To overcome this limitation, researchers have proposed methods such as multi-scale feature fusion, self-attention mechanisms, and pyramid pooling. Nevertheless, these methods frequently encounter difficulties in feature coupling between tampered and non-tampered regions in practice, which constrains the model performance. To address these issues, we propose an innovative edge-aware graph reasoning network (EGRNet). The core advantage of this network is that it can effectively enhance the similarity of features within the tampered region while weakening the information interaction between the tampered region and non-tampered region, thus enabling the precise localization of the tampered region. The network employs dual-stream encoders, comprising an RGB encoder and an SRM encoder, to extract visual and noise features, respectively. It then utilizes spatial pyramid graph reasoning to fuse features at different scales. The graph reasoning architecture comprises two components: the Cross Graph Convolution Feature Fusion Module (CGCFFM) and the Edge-aware Graph Attention Module (EGAM). CGCFFM realizes adaptive fusion of visual and noise features by performing cross spatial graph convolution and channel graph convolution operations. The integration of two-stream features is facilitated by graph convolution, which enables the mapping of the features to a novel low-dimensional space. This space is capable of modeling long-range contextual relationships. EGAM applies binary edge information to the graph attention adjacency matrix. This enables the network to explicitly model the inconsistency between the tampered region and non-tampered region. EGAM effectively suppresses the interference of the non-tampered region to the tampered region, thereby improves the network’s ability to recognize tampered regions. To validate the performance of EGRNet, extensive experiments are conducted on several challenging benchmark datasets. The experimental results indicate that our method outperforms current state-of-the-art image manipulation localization methods in both qualitative and quantitative evaluations. Furthermore, EGRNet demonstrated strong adaptability to different types of tampering and robustness to various attacks.},
  archive      = {J_CVIU},
  author       = {Ruyi Bai},
  doi          = {10.1016/j.cviu.2025.104490},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104490},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Edge-aware graph reasoning network for image manipulation localization},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive bias learning via gradient-based reweighting and constrained pruning for robust visual question answering. <em>CVIU</em>, <em>260</em>, 104484. (<a href='https://doi.org/10.1016/j.cviu.2025.104484'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) presents significant challenges in cross-modal reasoning due to susceptibility to dataset biases, spurious correlations, and shortcuts learning, which undermine model robustness. While ensemble methods mitigate bias via joint optimization of a bias model and a target model during training, their efficacy remains limited by suboptimal bias exploitation and model capacity imbalances. To address this, we propose the Adaptive Bias Learning Network (ABLNet), a novel framework that systematically enhances bias capture for improved generalization. Our approach introduces two key innovations: (1) Gradient-driven sample reweighting, which quantifies per-sample bias magnitude via training gradients and prioritizes low-bias samples to refine bias model training; (2) Constrained network pruning, deliberately restricting bias model capacity to amplify its focus on bias patterns. Extensive evaluations on VQA-CPv1, VQA-CPv2, and VQA-v2 benchmarks confirm our ABLNet’s superiority, demonstrating generalizability across diverse question types. The code will be released at https://github.com/runminwang/ABLNet .},
  archive      = {J_CVIU},
  author       = {Zukun Wan and Runmin Wang and Xingdong Song and Juan Xu and Xiaofei Cao and Jielei Hei and Shengrong Yuan and Yajun Ding and Changxin Gao},
  doi          = {10.1016/j.cviu.2025.104484},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104484},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive bias learning via gradient-based reweighting and constrained pruning for robust visual question answering},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing ambiguous speech emotion recognition through spatial–temporal parallel network with label correction strategy. <em>CVIU</em>, <em>260</em>, 104483. (<a href='https://doi.org/10.1016/j.cviu.2025.104483'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition is of great significance for improving the human–computer interaction experience. However, traditional methods based on hard labels have difficulty dealing with the ambiguity of emotional expression. Existing studies alleviate this problem by redefining labels, but still rely on the subjective emotional expression of annotators and fail to consider the truly ambiguous speech samples without dominant labels fully. To solve the problems of insufficient expression of emotional labels and ignoring ambiguous undominantly labeled speech samples, we propose a label correction strategy that uses a model with exact sample knowledge to modify inappropriate labels for ambiguous speech samples, integrating model training with emotion cognition, and considering the ambiguity without dominant label samples. It is implemented on a spatial–temporal parallel network, which adopts a temporal pyramid pooling (TPP) to process the variable-length features of speech to improve the recognition efficiency of speech emotion. Through experiments, it has been shown that ambiguous speech after label correction has a more promoting effect on the recognition performance of speech emotions.},
  archive      = {J_CVIU},
  author       = {Chenquan Gan and Daitao Zhou and Kexin Wang and Qingyi Zhu and Deepak Kumar Jain and Vitomir Štruc},
  doi          = {10.1016/j.cviu.2025.104483},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104483},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Optimizing ambiguous speech emotion recognition through spatial–temporal parallel network with label correction strategy},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EntroFormer: An entropy-based sparse vision transformer for real-time semantic segmentation. <em>CVIU</em>, <em>260</em>, 104482. (<a href='https://doi.org/10.1016/j.cviu.2025.104482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image semantic segmentation plays a fundamental role in a wide range of pixel-level scene understanding tasks. State-of-the-art segmentation methods often leverage sparse attention mechanisms to identify informative patches for modeling long-range dependencies, significantly reducing the computational complexity of Vision Transformers. Most of these methods focus on selecting regions that are highly relevant to the queries, achieving strong performance in tasks like classification and object detection. However, in the semantic segmentation task, current sparse attention methods are limited by their query-based focus, overlooking the importance of interactions between different objects. In this paper, we propose Sparse Entropy Attention (SEA) to select regions with higher informational content for long-range dependency capture. Specifically, the information entropy of each region is computed to assess its uncertainty in semantic prediction. Regions with high information entropy are considered informative and selected to explore sparse global semantic dependencies. Based on SEA, we present an entropy-based sparse Vision Transformer (EntroFormer) network for real-time semantic segmentation. EntroFormer integrates sparse global semantic features with dense local ones, enhancing the network’s ability to capture both the interaction of image contents and specific semantics. Experimental results show that the proposed real-time network outperforms state-of-the-art methods with similar parameters and computational costs on the Cityscapes, COCO-Stuff, and Bdd100K datasets. Ablation studies further demonstrate that SEA outperforms other sparse attention mechanisms in semantic segmentation.},
  archive      = {J_CVIU},
  author       = {Zhiyan Wang and Song Wang and Lin Yuanbo Wu and Deyin Liu and Lei Gao and Lin Qi and Guanghui Wang},
  doi          = {10.1016/j.cviu.2025.104482},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104482},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {EntroFormer: An entropy-based sparse vision transformer for real-time semantic segmentation},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive DETR: A framework with dynamic sampling points and feature-guided adaptive attention updates. <em>CVIU</em>, <em>260</em>, 104481. (<a href='https://doi.org/10.1016/j.cviu.2025.104481'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, DETR-based models have advanced object detection but still face key challenges: the encoder’s high complexity and limited adaptability, and the decoder’s slow convergence due to query initialization. We propose Adaptive DETR, a framework with dynamic sampling and adaptive feature encoding. First, we design an attention update strategy that computes weights based on image features, enhancing detection accuracy. Second, we enable dynamic adjustment of sampling points in deformable attention, improving adaptability in complex scenes. Finally, we optimize the decoder by performing attention between bounding-box and semantic queries during initialization, effectively injecting semantics, accelerating convergence, and improving localization. Experiments on COCO, UAVDT, VisDrone, and RSOD confirm that Adaptive DETR achieves superior accuracy and generalization with improved efficiency.},
  archive      = {J_CVIU},
  author       = {Botao Li and Huguang Yang and Chenglong Xia and Han Zheng and Aziguli Wulamu and Taohong Zhang},
  doi          = {10.1016/j.cviu.2025.104481},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104481},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive DETR: A framework with dynamic sampling points and feature-guided adaptive attention updates},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal vs. unimodal approaches to uncertainty in 3D image segmentation under distribution shifts. <em>CVIU</em>, <em>260</em>, 104473. (<a href='https://doi.org/10.1016/j.cviu.2025.104473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has been widely adopted across sectors, yet its application in medical imaging remains challenging due to distribution shifts in real-world data. Deployed models often encounter samples that differ from the training dataset, particularly in the health domain, leading to performance issues. This limitation hinders the expressiveness and reliability of deep learning models in health applications. Thus, it becomes crucial to identify methods capable of producing reliable uncertainty estimation in the context of distribution shifts in the health sector. In this paper, we explore the feasibility of using cutting-edge Bayesian and non-Bayesian methods to detect distributionally shifted samples, aiming to achieve reliable and trustworthy diagnostic predictions in segmentation task. Specifically, we compare three distinct uncertainty estimation methods, each designed to capture either unimodal or multimodal aspects in the posterior distribution. Our findings demonstrate that methods capable of addressing multimodal characteristics in the posterior distribution, offer more dependable uncertainty estimates. This research contributes to enhancing the utility of deep learning in healthcare, making diagnostic predictions more robust and trustworthy.},
  archive      = {J_CVIU},
  author       = {Masoumeh Javanbakhat and Md Tasnimul Hasan and Cristoph Lippert},
  doi          = {10.1016/j.cviu.2025.104473},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104473},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multimodal vs. unimodal approaches to uncertainty in 3D image segmentation under distribution shifts},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCANet: A cross-scale context aggregation network for UAV object detection. <em>CVIU</em>, <em>260</em>, 104472. (<a href='https://doi.org/10.1016/j.cviu.2025.104472'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of deep learning technology, Unmanned Aerial Vehicle (UAV) object detection demonstrates significant potential across various fields. However, multi-scale object variations and complex environmental interference in UAV images present considerable challenges. This paper proposes a new UAV object detection network named Cross-scale Context Aggregation Network (CCANet), which contains Multi-scale Convolution Aggregation Darknet (MCADarknet) and Cross-scale Context Aggregation Feature Pyramid Network (CCA-FPN). First, MCADarknet serves as a multi-scale feature extraction network. It employs parallel multi-scale convolutional kernels and depth-wise strip convolution techniques to expand the network’s receptive field, extracting feature maps at four different scales layer by layer. Second, to address interference in complex scenes, a Context Enhanced Fusion method enhances the interaction between adjacent features extracted by MCADarknet and higher-level features to form intermediate features. Finally, CCA-FPN employs a cross-scale fusion strategy to deeply integrate shallow, intermediate, and deep feature information, thereby enhancing object representation in complex scenarios. Experimental results indicate that CCANet performs well on three public datasets. In particular, mAP 50 and mAP 50 − 95 can reach 47.4% and 29.4% respectively on the VisDrone dataset. Compared to the baseline model, it achieves improvements of 6.2% and 4.3%.},
  archive      = {J_CVIU},
  author       = {Lei Shang and Qihan He and Huan Lei and Wenyuan Yang},
  doi          = {10.1016/j.cviu.2025.104472},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104472},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CCANet: A cross-scale context aggregation network for UAV object detection},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EGLC: Enhancing global localization capability for medical image segmentation. <em>CVIU</em>, <em>260</em>, 104471. (<a href='https://doi.org/10.1016/j.cviu.2025.104471'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation plays a vital role in computer-aided diagnosis and treatment planning. Traditional convolutional networks excel at capturing local patterns, while Transformer-based models are effective at modeling global context. We observe that this advantage arises from the global model’s sensitivity to boundary information, whereas local modeling tends to focus on regional consistency. Based on this insight, we propose EGLC, a novel global-local collaborative segmentation framework. During global modeling, we progressively discard inattentive patches and apply wavelet transform to extract multi-frequency boundary features. These boundary features are then used as guidance to enhance local representations. To implement this strategy, we introduce a new encoder, Boundary PVT, which incorporates both global semantics and boundary cues. In the decoding phase, we design a Reverse Progressive Locality Decoder to redirect attention to the peripheral edges of the lesion, thereby improving boundary delineation. Extensive experiments on multiple public medical image datasets demonstrate that our EGLC framework consistently outperforms existing state-of-the-art methods, especially in preserving fine-grained boundary details. The proposed approach offers a promising direction for precise and robust medical image segmentation.},
  archive      = {J_CVIU},
  author       = {Yulong Wan and Dongming Zhou and Ran Yan},
  doi          = {10.1016/j.cviu.2025.104471},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104471},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {EGLC: Enhancing global localization capability for medical image segmentation},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics-guided human interaction generation via motion diffusion model. <em>CVIU</em>, <em>260</em>, 104470. (<a href='https://doi.org/10.1016/j.cviu.2025.104470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising diffusion model significantly boosts the generation of two-person interactions conditioned on textual descriptions. However, due to the complexity of interactions and the diversity of textual descriptions, motion generation still faces two critical challenges: The self-induced motion and the increasing error accumulation with more denoised steps. To address these issues, we propose a novel Physics-guided human Interaction generation framework based on motion diffusion model, named PhyInter. It can synthesize contextually appropriate motion, automatically learn the dynamic states of the other participant without additional annotation, and also optimize the errors of generation by guiding the next denoising diffusion step. Specifically, PhyInter integrates physical principles from two perspectives: (1) Defining a stochastic differential equation based on human kinematics to model the physical states of interaction; (2) Employing an interactive attention module to share physical information between intra- and inter-human motions. Additionally, we design a sampling strategy to facilitate motion generation and avoid unnecessary computation, ensuring realistic, physically-plausible interactions. Extensive experiments demonstrate that our method surpasses previous approaches on the InterHuman dataset, achieving the state-of-the-art performance.},
  archive      = {J_CVIU},
  author       = {Dahua Gao and Wenlong Wang and Xinyu Liu and Yuxi Hu and Danhua Liu},
  doi          = {10.1016/j.cviu.2025.104470},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104470},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Physics-guided human interaction generation via motion diffusion model},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified learning for image–text alignment via multi-scale feature fusion. <em>CVIU</em>, <em>260</em>, 104468. (<a href='https://doi.org/10.1016/j.cviu.2025.104468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval, particularly image–text retrieval, aims to achieve efficient matching and retrieval between images and text. With the continuous advancement of deep learning technologies, numerous innovative models and algorithms have emerged. However, existing methods still face some limitations: (1) Most models overly focus on either global or local correspondences, failing to fully integrate global and local information; (2) They typically emphasize cross-modal similarity optimization while neglecting the relationships among samples within the same modality; (3) They struggle to effectively handle noise in image–text pairs, negatively impacting model performance due to noisy negative samples. To address these challenges, this paper proposes a dual-branch structured model that combines global and local matching—Momentum-Augmented Transformer Encoder (MATE). The model aligns closely with human cognitive processes by integrating global and local features and leveraging an External Spatial Attention aggregation (ESA) mechanism and a Multi-modal Fusion Transformer Encoder, significantly enhancing feature representation capabilities. Furthermore, this work introduces a Hard Enhanced Contrastive Triplet Loss (HECT Loss), which effectively optimizes the model’s ability to distinguish positive and negative samples. A self-supervised learning method based on momentum distillation is also employed to further improve image–text matching performance. The experimental results demonstrate that the MATE model outperforms the vast majority of existing state-of-the-art methods on both Flickr30K and MS-COCO datasets. The code is available at https://github.com/wangmeng-007/MATE/tree/master .},
  archive      = {J_CVIU},
  author       = {Jing Zhou and Meng Wang},
  doi          = {10.1016/j.cviu.2025.104468},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104468},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unified learning for image–text alignment via multi-scale feature fusion},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Representation learning of point cloud upsampling in global and local inputs. <em>CVIU</em>, <em>260</em>, 104467. (<a href='https://doi.org/10.1016/j.cviu.2025.104467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, point cloud upsampling has been widely applied in tasks such as 3D reconstruction and object recognition. This study proposed a novel framework, ReLPU, which enhances upsampling performance by explicitly learning from both global and local structural features of point clouds. Specifically, we extracted global features from uniformly segmented inputs (Average Segments) and local features from patch-based inputs of the same point cloud. These two types of features were processed through parallel autoencoders, fused, and then fed into a shared decoder for upsampling. This dual-input design improved feature completeness and cross-scale consistency, especially in sparse and noisy regions. Our framework was applied to several state-of-the-art autoencoder-based networks and validated on standard datasets. Experimental results demonstrated consistent improvements in geometric fidelity and robustness. In addition, saliency maps confirmed that parallel global-local learning significantly enhanced the interpretability and performance of point cloud upsampling.},
  archive      = {J_CVIU},
  author       = {Tongxu Zhang and Bei Wang},
  doi          = {10.1016/j.cviu.2025.104467},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104467},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Representation learning of point cloud upsampling in global and local inputs},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive illumination and noise-free detail recovery via visual decomposition for low-light image enhancement. <em>CVIU</em>, <em>260</em>, 104466. (<a href='https://doi.org/10.1016/j.cviu.2025.104466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing low-light image enhancement methods often struggle with precise brightness control and frequently introduce noise during the enhancement process. To address these limitations, we propose BVILLIE, a novel biologically inspired visual model. BVILLIE employs a visual decomposition network that separates low-light images into low-frequency and high-frequency components, with the low-frequency path focused on brightness management and the high-frequency path enhancing details without amplifying noise. In the low-frequency path, inspired by the biological visual system’s adaptive response to varying light conditions, BVILLIE incorporates a custom-designed luminance curve based on the Naka–Rushton equation. This equation models the nonlinear response of retinal neurons to light intensity, simulating human perceptual adaptation to different brightness levels. Additionally, a convolutional enhancement module corrects color shifts resulting from luminance adjustments. In the high-frequency path, an innovative fusion module integrates a preliminary denoiser with an adaptive enhancement mechanism to improve detail preservation and texture refinement. Extensive experiments across multiple benchmark datasets demonstrate that BVILLIE significantly outperforms state-of-the-art techniques. For instance, on the LOLv2-Real dataset, BVILLIE achieves a PSNR of 25.335 dB, SSIM of 0.866, LPIPS of 0.106, and LOE of 0.208. These results, consistently observed across various metrics, highlight BVILLIE’s superior performance in terms of image quality, perceptual similarity, preservation of lightness order, detail enhancement, and noise suppression.},
  archive      = {J_CVIU},
  author       = {Tianqi Li and Pingping Liu and Qiuzhan Zhou and Tongshun Zhang},
  doi          = {10.1016/j.cviu.2025.104466},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104466},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive illumination and noise-free detail recovery via visual decomposition for low-light image enhancement},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PConvSRGAN: Real-world super-resolution reconstruction with pure convolutional networks. <em>CVIU</em>, <em>260</em>, 104465. (<a href='https://doi.org/10.1016/j.cviu.2025.104465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) reconstruction technology faces numerous challenges in real-world applications: image degradation types are diverse, complex, and unknown; the diversity of imaging devices increases the complexity of image degradation in the super-resolution reconstruction process; SR requires substantial computational resources, especially with the latest significantly effective Transformer-based SR methods. To address these issues, we improved the ESRGAN model by implementing the following: first, a probabilistic degradation model was added to simulate the degradation process, preventing overfitting to specific degradations; second, BiFPN was introduced in the generator to fuse multi-scale features; lastly, inspired by the ConvNeXt network, the discriminator was redesigned as a pure convolutional network built entirely from standard CNN modules, which matches Transformer performance across various aspects. Experimental results demonstrate that our approach achieves the best PI and LPIPS performance compared to state-of-the-art SR methods, with PSNR,SSIM and NIQE being on par. Visualization results show that our method not only generates natural SR images but also excels in restoring structures.},
  archive      = {J_CVIU},
  author       = {Zuopeng Zhao and Yumeng Gao and Bingbing Min and Xiaoran Miao and Jianfeng Hu and Ying Liu and Kanyaphakphachsorn Pharksuwan},
  doi          = {10.1016/j.cviu.2025.104465},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104465},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {PConvSRGAN: Real-world super-resolution reconstruction with pure convolutional networks},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context perturbation: A consistent alignment approach for domain adaptive semantic segmentation. <em>CVIU</em>, <em>260</em>, 104464. (<a href='https://doi.org/10.1016/j.cviu.2025.104464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Adaptive Semantic Segmentation (DASS) aims to adapt a pre-trained segmentation model from a labeled source domain to an unlabeled target domain. Previous approaches usually address the domain gap by consistency regularization which is implemented based on the augmented data. However, as the augmentations are often performed at the input level with simple linear transformations, the feature representations suffer limited perturbation from these augmented views. As a result, they are not effective for cross-domain consistency learning. In this work, we propose a new augmentation method, namely contextual augmentation, and combine it with contrastive learning approaches from both the pixel and class levels to achieve consistency regularization. We term this methodology as Context Perturbation for DASS (CoPDASeg). Specifically, contextual augmentation first combines domain information by class mix and then randomly crops two patches with an overlapping region. To achieve consistency regularization with the two augmented patches, we focus on both pixel and class perspectives and propose two parallel contrastive learning paradigms ( i.e. , pixel-level contrastive learning and class-level contrastive learning). The former aligns the pixel-to-pixel feature representations, and later aligns class prototypes across domains. Experimental results on representative benchmarks ( i.e. , GTA5 → Cityscapes and SYNTHIA → Cityscapes ) demonstrate that CoPDASeg improves the segmentation performance over state-of-the-arts by a large margin.},
  archive      = {J_CVIU},
  author       = {Meiqin Liu and Zilin Wang and Chao Yao and Yao Zhao and Wei Wang and Yunchao Wei},
  doi          = {10.1016/j.cviu.2025.104464},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104464},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Context perturbation: A consistent alignment approach for domain adaptive semantic segmentation},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GaitBranch: A multi-branch refinement model combined with frame-channel attention mechanism for gait recognition. <em>CVIU</em>, <em>260</em>, 104463. (<a href='https://doi.org/10.1016/j.cviu.2025.104463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately representing human motion in video-based gait recognition is challenging due to the difficulty in obtaining an ideal gait silhouette sequence that captures comprehensive information. To address this challenge, we propose GaitBranch, a novel method that emphasizes local key information of human motion in different layers of the neural network. It divides the neural network into multiple branches using the multi-branch refinement (MBR) module and extracts local key frames from various body parts through the frame-channel attention mechanism (FCAM) to form a comprehensive representation of human motion patterns. GaitBranch achieves high gait recognition accuracy on the CASIA-B (98.6%, 96.1%, and 85.5% for normal walking, carrying a bag, and wearing a coat conditions), OU-MVLP (92.3%), and GREW (79.8%) datasets, demonstrating its robustness across different environments. Ablation experiments confirm the efficacy of our method and demonstrate that the performance gains result from the optimized model structure rather than simply increasing parameters.},
  archive      = {J_CVIU},
  author       = {Huakang Li and Yidan Qiu and Huimin Zhao and Jin Zhan and Rongjun Chen and Jinchang Ren and Ying Gao and Wing W.Y. Ng},
  doi          = {10.1016/j.cviu.2025.104463},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104463},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {GaitBranch: A multi-branch refinement model combined with frame-channel attention mechanism for gait recognition},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale spatio-temporal fusion network for video dehazing. <em>CVIU</em>, <em>260</em>, 104462. (<a href='https://doi.org/10.1016/j.cviu.2025.104462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video dehazing aims to restore high-resolution and high-contrast haze-free frames, which is crucial in engineering applications such as intelligent traffic monitoring systems. These monitoring systems heavily rely on clear visual information to ensure accurate decision-making and reliable operation. However, despite significant advances achieved by deep learning methods, they still face challenges when dealing with diverse real-world scenarios. To address these issues, we propose a Multi-Scale Spatio-Temporal Fusion Network (MSTF-Net), a novel framework designed to enhance video dehazing performance in complex engineering environments. Specifically, the MainAux Encoder integrates multi-source information through a progressively enhanced feature fusion mechanism, improving the representation of both global dynamics and local details. Furthermore, the Spatio-Temporal Adaptive Fusion (STAF) module ensures robust temporal consistency and spatial clarity by leveraging multi-level spatio-temporal information fusion. To evaluate our framework, we constructed a challenging dataset named “DarkRoad”, which includes low-light, uneven lighting, and dynamic outdoor scenarios, addressing the key limitations of existing datasets in video dehazing tasks. Extensive experiments demonstrate that MSTF-Net achieves state-of-the-art performance, excelling particularly in applications requiring high clarity, strong contrast, and detailed preservation, providing a reliable solution to video dehazing problems in practical engineering scenarios.},
  archive      = {J_CVIU},
  author       = {Qingru Zhang and Guorong Chen and Yixuan Zhang and Jinmei Zhang and Shaofeng Liu and Jian Wang},
  doi          = {10.1016/j.cviu.2025.104462},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104462},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multiscale spatio-temporal fusion network for video dehazing},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DBiSeNet: Dual bilateral segmentation network for real-time semantic segmentation. <em>CVIU</em>, <em>260</em>, 104461. (<a href='https://doi.org/10.1016/j.cviu.2025.104461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bilateral networks have shown effectiveness and efficiency for real-time semantic segmentation. However, the single bilateral architecture exhibits limitations in capturing multi-scale feature representations and addressing misalignment issues during spatial and contextual feature fusion, thereby constraining segmentation accuracy. To address these challenges, we propose a novel dual bilateral segmentation network (DBiSeNet) that incorporates an additional bilateral branch into the original architecture. The additional (high-scale) bilateral operating at high resolution to preserve fine-grained details and responsible for thin object prediction, while the original (low-scale) bilateral maintains an enlarged receptive field to capture global context for large object segmentation. Furthermore, we introduce an aligned and refined feature fusion module to mitigate feature misalignment within each bilateral branch. To optimize the final prediction, we design a dual prediction fusion module that utilizes the low-scale segmentation results as a baseline and adaptively incorporates complementary information from high-scale predictions. Extensive experiments on the Cityscapes and CamVid datasets validate the effectiveness of DBiSeNet in achieving an optimal balance between accuracy and inference speed. In particular, on a single RTX3090 GPU, DBiSeNet2 yields 75.6% mIoU at 225.9 FPS on Cityscapes test set and 75.7% mIoU at 203.4 FPS on CamVid test set.},
  archive      = {J_CVIU},
  author       = {Xiaobo Hu and Hongbo Zhu and Ning Su and Taosheng Xu},
  doi          = {10.1016/j.cviu.2025.104461},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104461},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DBiSeNet: Dual bilateral segmentation network for real-time semantic segmentation},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAVE: Segment audio-visual easy way using the segment anything model. <em>CVIU</em>, <em>260</em>, 104460. (<a href='https://doi.org/10.1016/j.cviu.2025.104460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-visual segmentation (AVS) primarily aims to accurately detect and pinpoint sound elements in visual contexts by predicting pixel-level segmentation masks. To address this task effectively, it is essential to thoroughly consider both the data and model aspects. This study introduces a streamlined approach, SAVE, which directly modifies the pretrained segment anything model (SAM) for the AVS task. By integrating an image encoder adapter within the transformer blocks for improved dataset-specific information capture and introducing a residual audio encoder adapter to encode audio features as a sparse prompt, our model achieves robust audio-visual fusion and interaction during encoding. Our method enhances the training and inference speeds by reducing the input resolution from 1024 to 256 pixels while still surpassing the previous state-of-the-art (SOTA) in performance. Extensive experiments validated our approach, indicating that our model significantly outperforms other SOTA methods. Additionally, utilizing the pretrained model on synthetic data enhances performance on real AVSBench data, attaining mean intersection over union (mIoU) of 84.59 on the S4 (V1S) subset and 70.28 on the MS3 (V1M) set with image inputs of 256 pixels. This performance increases to 86.16 mIoU on the S4 (V1S) and 70.83 mIoU on the MS3 (V1M) with 1024-pixel inputs. These findings show that simple adaptations of pretrained models can enhance AVS and support real-world applications.},
  archive      = {J_CVIU},
  author       = {Khanh-Binh Nguyen and Chae Jung Park},
  doi          = {10.1016/j.cviu.2025.104460},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104460},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SAVE: Segment audio-visual easy way using the segment anything model},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust cross-image adversarial watermark with JPEG resistance for defending against deepfake models. <em>CVIU</em>, <em>260</em>, 104459. (<a href='https://doi.org/10.1016/j.cviu.2025.104459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread convenience of generative models has exacerbated the misuse of attribute-editing-based Deepfake technologies, leading to the proliferation of illegally generated content that severely threatens personal privacy and security. Existing proactive defense strategies mitigate Deepfake attacks by embedding imperceptible adversarial watermarks into the spatial-domain of protected images. However, spatial-domain adversarial watermarks are inherently sensitive to lossy compression operations, which significantly degrades their defense efficacy. To address this limitation, we propose a frequency-domain cross-image adversarial watermark generation scheme to enhance robustness toward JPEG compression. In the proposed method, the adversarial watermark training process is migrated to the frequency domain using a differentiable JPEG module, which explicitly simulates the impact of quantization and compression on perturbation distributions. Furthermore, a fusion module is incorporated to coordinate watermark distributions across images, thereby enhancing the generalization of the defense. Experimental results demonstrate that the generated adversarial watermarks exhibit strong robustness against JPEG compression and effectively disrupt the outputs of Deepfake models. Moreover, the proposed scheme can be directly applied to diverse facial images without retraining, thereby providing reliable protection for real-world image application scenarios.},
  archive      = {J_CVIU},
  author       = {Zhiyu Lin and Hanbin Lin and Liqiang Lin and Shuwu Chen and Xiaolong Liu},
  doi          = {10.1016/j.cviu.2025.104459},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104459},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Robust cross-image adversarial watermark with JPEG resistance for defending against deepfake models},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PakSign: Advancing dynamic pakistani sign language recognition with a novel skeleton-based dataset and graph-enhanced architectures. <em>CVIU</em>, <em>260</em>, 104458. (<a href='https://doi.org/10.1016/j.cviu.2025.104458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign Language Recognition (SLR) is a critical yet complex task in pattern recognition and computer vision due to the visual-gestural nature of sign languages. While regional variants like American, British, and Chinese Sign Languages have seen significant research advancements, Pakistani Sign Language (PSL) remains underexplored, mostly limited to static Urdu alphabet recognition rather than dynamic gestures used in daily communication. The scarcity of large-scale PSL datasets further hinders the training of deep learning models, which require extensive data. This work addresses these gaps by introducing a novel skeleton-based PSL dataset comprising over 1280 pose sequences of 52 Urdu signs, each performed five times by five different signers. We detail the data collection protocol and evaluate lightweight, pose-based baseline models using a K-fold cross-validation protocol. Furthermore, we propose Efficient-Sign, a novel recognition pipeline with two variants: B0, achieving a 2.28% accuracy gain with 35.37% fewer FLOPs and 63.55% fewer parameters, and B4, yielding a 3.48% accuracy improvement and 14.95% fewer parameters when compared to state-of-the-art model. We also conduct cross-dataset evaluations on widely-used benchmarks such as WLASL-100 and MINDS-Libras, where Efficient-Sign maintains competitive accuracy with substantially fewer parameters and computational overhead. These results confirm the model’s generalizability and robustness across diverse sign languages and signer populations. This work contributes significantly by providing a publicly available pose-based PSL dataset, strong baseline evaluations, and an efficient architecture for benchmarking future research, marking a critical advancement in dynamic PSL recognition and establishing a foundation for scalable, real-world SLR systems.},
  archive      = {J_CVIU},
  author       = {Neelma Naz and Maheen Salman and Fiza Ayub and Zawata Afnan Asif and Sara Ali},
  doi          = {10.1016/j.cviu.2025.104458},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104458},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {PakSign: Advancing dynamic pakistani sign language recognition with a novel skeleton-based dataset and graph-enhanced architectures},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parametric kernels for artifact mitigation in patch-based image aggregation using generative models. <em>CVIU</em>, <em>260</em>, 104457. (<a href='https://doi.org/10.1016/j.cviu.2025.104457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in artificial intelligence applications have highlighted the effectiveness of generative models for domain transfer, image enhancement and simulation. However, when applied to large-scale gigapixel images, the use of traditional patch-based image aggregation methods introduces checkerboard or blocking artifacts, which compromises image quality and fidelity. In this paper, we propose a parametric kernel that is specifically designed to target the underlying grid structure to mitigate these artifacts. With the use of adjustable zero-padding and linear-padding parameters, our kernel provides fine control over the fusion process by combining a central area of constant weights with border regions of gradually decreasing weights. The proposed method was validated using three medical imaging modalities (digital pathology, fluorescence microscopy and ultrasound imaging) for different generative model tasks. The results showed statistically significant improvements (p < 0.0001) in artifact removal when compared to state-of-the-art methods. Quantitative analysis revealed improvements in fusion quality measures for digital pathology, fluorescence microscopy and ultrasound imaging of 7.1%, 27.4% and 20.0%, respectively. Additionally, expert evaluators confirmed superior visual quality and reduced artifacts in blind assessments of reconstructed images, with our method achieving significantly higher scores across all modalities. Our method is versatile, compatible with various generative models and can be easily adjusted by modifying kernel parameters. This kernel-based approach significantly advances the quality of synthesized medical images, directly supporting more reliable clinical assessment and automated analysis.},
  archive      = {J_CVIU},
  author       = {Nicola Michielli and Francesco Marzola and Francesco Branciforti and Kristen M. Meiburger and Alessandro Gambella and Massimo Salvi},
  doi          = {10.1016/j.cviu.2025.104457},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104457},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Parametric kernels for artifact mitigation in patch-based image aggregation using generative models},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniMultNet: Action recognition method based on multi-scale feature fusion and video-text constraint guidance. <em>CVIU</em>, <em>260</em>, 104456. (<a href='https://doi.org/10.1016/j.cviu.2025.104456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods in action recognition primarily focus on the extraction of local and global features but neglect their complementarity. Moreover, the difference in spatial distribution between visual and textual features frequently results in information loss in mainstream fusion methods. To address these issues, this paper proposes an action recognition method based on multi-scale feature fusion and video-text constraint guidance (UniMultNet), designed to ensure the effective fusion of local and global features as well as a tight coupling of visual and textual information. The UniMultNet consists of two principal components: the Local-Global Feature Fusion Block (LGFB) and the Cross-Modal Adaptive Constraint Fusion Module (ACCF). The LGFB utilizes a self-attention mechanism to aggregate multi-scale information and capture correlations between local and global features, while the ACCF employs constraint learning strategies to learn global representations of visual-textual interactions, thereby supervising the learning process of visual features. Extensive experiments on several benchmark datasets (MOD-20, HMDB-51, UCF-101, and Something V1 & V2) demonstrate that UniMultNet achieves significant improvements in accuracy, ranging from 0.3% to 1.04%, compared to state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Qiuhong Tian and Fei Zeng and Junxiao Ning and Lizao Zhang},
  doi          = {10.1016/j.cviu.2025.104456},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104456},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {UniMultNet: Action recognition method based on multi-scale feature fusion and video-text constraint guidance},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive margin for unsupervised domain adaptation without source data. <em>CVIU</em>, <em>260</em>, 104455. (<a href='https://doi.org/10.1016/j.cviu.2025.104455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) methods aim to transfer the knowledge acquired from labeled source data to unlabeled target data. However, these methods are often inefficient and impractical due to concerns related to data privacy and memory storage. As a result, source-free domain adaptation (SFDA) was introduced as a solution, which involves deploying a well-trained source model to the target domain, while the source data are unavailable for optimization. Existing pseudo-label based SFDA methods suffer from two issues: (1) they do not well leverage the discriminating power of the model at the early step of the training; (2) they do not well prevent memorization of the noisy labels at the late step of the training. In this paper, we propose a novel method called AM-SFDA to address SFDA issue via A daptive M argin. AM-SFDA combines the information maximization and the commonly used standard cross-entropy loss, which can make the source and target outputs closer. Furthermore, inspired by the early-learning phenomenon, we propose to prevent the memorization of the noisy samples, where large values are assigned to the samples with moderate margins, and small values are assigned to the samples with small margins. Extensive experiments on several source-free benchmarks under different settings illustrate that AM-SFDA exceeds the existing state-of-the-art SFDA methods successfully.},
  archive      = {J_CVIU},
  author       = {Ziyun Cai and Yawen Huang and Tengfei Zhang and Changhui Hu and Xiao-Yuan Jing},
  doi          = {10.1016/j.cviu.2025.104455},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104455},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive margin for unsupervised domain adaptation without source data},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Made-in: An immersive human-in-the-loop analytics platform for enhancing creative processes in fashion. <em>CVIU</em>, <em>260</em>, 104454. (<a href='https://doi.org/10.1016/j.cviu.2025.104454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fashion industry is undergoing a digital transformation, driven by growing demands for sustainability, personalization and immersive experiences. In this paper, we present Made-In (Multimodal and Collaborative Artificial Intelligence for the Design of Inclusive and Sustainable Fashion): an immersive, human-in-the-loop analytics system designed to support fashion professionals in exploring, comparing and contextualizing product data across digital and social platforms. Unlike generative or simulation-based approaches, Made-In provides creative decision support by aggregating real-world data from luxury brand websites and social media. This enables designers and merchandisers to make informed, context-aware choices. The system comprises three core modules: a 3D configurator for visualizing product assortments; a collection grid interface for the comparative analysis of e-commerce data; and a social media trend detector based on deep learning pipelines for image classification, object detection and color clustering. Two curated datasets, one derived from Instagram and the other from fashion e-tailers, provide the system with analytics. A user study with domain experts confirms the platform’s usability and relevance for trend forecasting, sustainability evaluation and visual merchandising strategy. The results demonstrate that Made-In effectively bridges the gap between data analytics and human creativity in fashion, offering a scalable solution that aligns with EU goals for digital sustainability and inclusivity.},
  archive      = {J_CVIU},
  author       = {Emanuele Balloni and Rocco Pietrini and Michele Sasso and Emanuele Frontoni and Marina Paolanti},
  doi          = {10.1016/j.cviu.2025.104454},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104454},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Made-in: An immersive human-in-the-loop analytics platform for enhancing creative processes in fashion},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crack segmentation in roads using synthetic data and RGB-D data fusion. <em>CVIU</em>, <em>260</em>, 104452. (<a href='https://doi.org/10.1016/j.cviu.2025.104452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we use deep learning on the task of crack segmentation using a novel data fusion approach with RGB-D data. We use an existing architecture with DeepLabV3 and synthetic data to address the issue of limited availability for real-world data. The synthetic data is generated with Blender and BlenSor to accurately model the real-world crack scenarios. We train the model with a mixture of real-world data and synthetic data and evaluate it on a real-world dataset. The results show significant improvements over baseline models that only use the RGB data when evaluated with the IoU and F1-score. This demonstrates the success of using synthetic data for crack segmentation with data fusion and suggests a promising direction for future crack detection research to provide increased accuracy in automated maintenance and monitoring applications.},
  archive      = {J_CVIU},
  author       = {Benedict Marsh and Ruiheng Wu},
  doi          = {10.1016/j.cviu.2025.104452},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104452},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Crack segmentation in roads using synthetic data and RGB-D data fusion},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEAF: Unveiling two sides of the same coin in semi-supervised facial expression recognition. <em>CVIU</em>, <em>260</em>, 104451. (<a href='https://doi.org/10.1016/j.cviu.2025.104451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning has emerged as a promising approach to tackle the challenge of label scarcity in facial expression recognition (FER) task. However, current state-of-the-art methods primarily focus on one side of the coin, i.e., generating high-quality pseudo-labels , while overlooking the other side: enhancing expression-relevant representations . In this paper, we unveil both sides of the coin by proposing a unified framework termed hierarchica L d E coupling A nd F using (LEAF) to coordinate expression-relevant representations and pseudo-labels for semi-supervised FER. LEAF introduces a hierarchical expression-aware aggregation strategy that operates at three levels: semantic, instance, and category. (1) At the semantic and instance levels, LEAF decouples representations into expression-agnostic and expression-relevant components, and adaptively fuses them using learnable gating weights. (2) At the category level, LEAF assigns ambiguous pseudo-labels by decoupling predictions into positive and negative parts, and employs a consistency loss to ensure agreement between two augmented views of the same image. Extensive experiments on benchmark datasets demonstrate that by unveiling and harmonizing both sides of the coin, LEAF outperforms state-of-the-art semi-supervised FER methods, effectively leveraging both labeled and unlabeled data. Moreover, the proposed expression-aware aggregation strategy can be seamlessly integrated into existing semi-supervised frameworks, leading to significant performance gains. Our code is available at https://github.com/zfkarl/LEAF .},
  archive      = {J_CVIU},
  author       = {Fan Zhang and Zhi-Qi Cheng and Jian Zhao and Xiaojiang Peng and Xuelong Li},
  doi          = {10.1016/j.cviu.2025.104451},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104451},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LEAF: Unveiling two sides of the same coin in semi-supervised facial expression recognition},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Training-free diffusion for controlling illumination conditions in images. <em>CVIU</em>, <em>260</em>, 104450. (<a href='https://doi.org/10.1016/j.cviu.2025.104450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel approach to illumination manipulation in diffusion models, addressing the gap in conditional image generation with a focus on lighting conditions. While most of methods employ ControlNet and its variants to address the illumination-aware guidance in diffusion models. In contrast, We conceptualize the diffusion model as a black-box image render and strategically decompose its energy function in alignment with the image formation model. Our method effectively separates and controls illumination-related properties during the generative process. It generates images with realistic illumination effects, including cast shadow, soft shadow, and inter-reflections. Remarkably, it achieves this without the necessity for learning intrinsic decomposition, finding directions in latent space, or undergoing additional training with new datasets.},
  archive      = {J_CVIU},
  author       = {Xiaoyan Xing and Tao Hu and Jan Hendrik Metzen and Konrad Groh and Sezer Karaoglu and Theo Gevers},
  doi          = {10.1016/j.cviu.2025.104450},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104450},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Training-free diffusion for controlling illumination conditions in images},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FCEGNet: Feature calibration and edge-guided MLP decoder network for RGB-D semantic segmentation. <em>CVIU</em>, <em>260</em>, 104448. (<a href='https://doi.org/10.1016/j.cviu.2025.104448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The references from depth image data provide rich geometric information for traditional RGB semantic segmentation, which effectively improves the performance of semantic segmentation. However, during the process of feature fusion, there are feature biases between RGB features and depth features, which negatively affect cross-modal feature fusion. In this paper, we propose a novel RGB-D network, FCEGNet, consisting of a Feature Calibration Interaction Module (FCIM), a Three-Stream Fusion Extraction Module(TFEM), and an edge-guided MLP decoder. FCIM processes features in different orientations and scales by balancing features across modalities, and exchanges spatial information to allow RGB and depth features to be calibrated and interact with cross-modal features. TFEM performs feature extraction on cross-modal features and combines them with unimodal features to improve the accuracy of enhanced semantic understanding and fine-grained recognition. Dual-stream edge guidance module (DEGM) is designed in the edge-guided MLP decoder to protect the consistency and disparity of cross-modal features while enhancing the edge information and preserving the spatial information, which helps to obtain more accurate segmentation results. Experimental results on the RGB-D dataset show that the proposed FCFGNet is superior and more efficient than several state-of-the-art methods. The generalised validation of FCEGNet on the RGB-T semantic segmentation dataset also achieves better results.},
  archive      = {J_CVIU},
  author       = {Yiming Lu and Bin Ge and Chenxing Xia and Xu Zhu and Mengge Zhang and Mengya Gao and Ningjie Chen and Jianjun Hu and Junjie Zhi},
  doi          = {10.1016/j.cviu.2025.104448},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104448},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FCEGNet: Feature calibration and edge-guided MLP decoder network for RGB-D semantic segmentation},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separable distributed local thickness algorithm for efficient morphological characterization of terapixel-scale volume images. <em>CVIU</em>, <em>260</em>, 104443. (<a href='https://doi.org/10.1016/j.cviu.2025.104443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thickness of structures is a fundamental quantity in the morphometric study of volumetric datasets. The local thickness transform allows to quantify the local structure thickness in a model-independent and direct way. However, till now implementations of local thickness algorithms suffer from very high computational cost when applied to multi-dimensional image data with structure sizes extending over several orders of magnitude. We show that this bottleneck can be addressed, albeit on the expense of increased memory requirements, thus rendering the problem as a typical case example of a time-memory trade-off. We propose a new separable algorithm for local thickness transformation with a potential performance increase of up to several orders of magnitude. By distributing the processing into smaller tasks, the algorithm can be executed on a computer cluster in a fully parallelized manner as well as on a computer with limited memory by executing each of the tasks sequentially. We further propose an approximation of the original local thickness and show that an additional increase in computation speed is possible, at the expense of error that is often negligible. To demonstrate the performance of the algorithm, we use computer-generated images as well as a real terapixel-scale tomographic volume. All the algorithms are published in the open source pi2 software package as well as integrated in the popular ImageJ image processing framework.},
  archive      = {J_CVIU},
  author       = {Arttu Miettinen and Elena Borisova and Marco Stampanoni and Goran Lovric},
  doi          = {10.1016/j.cviu.2025.104443},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104443},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Separable distributed local thickness algorithm for efficient morphological characterization of terapixel-scale volume images},
  volume       = {260},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPDiff: Enhancing prior-guided diffusion model for real-world image super-resolution. <em>CVIU</em>, <em>259</em>, 104453. (<a href='https://doi.org/10.1016/j.cviu.2025.104453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion Models (DMs) have achieved promising success in Real-world Image Super-Resolution (Real-ISR), where they reconstruct High-Resolution (HR) images from available Low-Resolution (LR) counterparts with unknown degradation by leveraging pre-trained Text-to-Image (T2I) diffusion models. However, due to the randomness nature of DMs and the severe degradation commonly presented in LR images, most DMs-based Real-ISR methods neglect the structure-level and semantic information, which results in reconstructed HR images suffering not only from important edge missing, but also from undesired regional information confusion. To tackle these challenges, we propose an Enhancing Prior-guided Diffusion model (EPDiff) for Real-ISR, which leverages high-frequency priors and semantic guidance to generate reconstructed images with realistic details. Firstly, we design a Guide Adapter (GA) module that extracts latent texture and edge features from LR images to provide high-frequency priors. Subsequently, we introduce a Semantic Prompt Extractor (SPE) that generates high-quality semantic prompts to enhance image understanding. Additionally, we build a Feature Rectify ControlNet (FRControlNet) to refine feature modulation, enabling realistic detail generation. Extensive experiments demonstrate that the proposed EPDiff outperforms state-of-the-art methods on both synthetic and real-world datasets.},
  archive      = {J_CVIU},
  author       = {Detian Huang and Miaohua Ruan and Yaohui Guo and Zhenzhen Hu and Huanqiang Zeng},
  doi          = {10.1016/j.cviu.2025.104453},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104453},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {EPDiff: Enhancing prior-guided diffusion model for real-world image super-resolution},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-overlapping image stitching method for reconstruction of page in ancient chinese books. <em>CVIU</em>, <em>259</em>, 104449. (<a href='https://doi.org/10.1016/j.cviu.2025.104449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic stitching of page images in ancient Chinese books plays an important role in preservation and transmission of cultural heritage, significantly diminishing the need for manual intervention. Current methods accomplish image stitching based on their overlapping area and struggle with the ancient book pages without overlapping areas. To overcome this hurdle, this study proposes a novel deep learning based method to accurately stitch ancient book pages, which contains three key steps. Firstly, aiming to locate stitching seams precisely, a semantic segmentation model is exploited to predict the thickness masks of page images, and the non-overlapping pages can be stitched by cropping the thickness areas. Secondly, a novel multi-rule page stitching module with two creative page alignment methods is designed to align elements along the stitching seams. Lastly, the proposed method encompasses a self-assessment module, which judiciously selects the optimal stitched outcome from the multiple probable outputs generated by the multi-rule page stitching module. Experimental results demonstrate that the proposed method achieves superior performance in automatic stitching of ancient book pages. The stitching results on over 140 pages from three different ancient books show an accuracy of 82.18%, with 37.75% improvements over existing methods. This method provides a foundation for the automatic digitization of ancient Chinese books, showing significant potential applications in the field of automatic character recognition for historical documents.},
  archive      = {J_CVIU},
  author       = {Yizhou Lan and Daoyuan Zheng and Qingwu Hu and Shaohua Wang and Shunli Wang and Tong Yue and Jiayuan Li},
  doi          = {10.1016/j.cviu.2025.104449},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104449},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A non-overlapping image stitching method for reconstruction of page in ancient chinese books},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic BIQA: Median randomized smoothing for certified blind image quality assessment. <em>CVIU</em>, <em>259</em>, 104447. (<a href='https://doi.org/10.1016/j.cviu.2025.104447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most modern No-Reference Image-Quality Assessment (NR-IQA) metrics are based on neural networks vulnerable to adversarial attacks. Although some empirical defenses for IQA metrics were proposed, they do not provide theoretical guarantees and may be vulnerable to adaptive attacks. This work focuses on developing a provably robust no-reference IQA metric. The proposed DMS-IQA method is based on randomized Median Smoothing combined with an additional convolution denoiser with ranking loss to improve the SROCC and PLCC scores of the defended IQA metric. We theoretically show that the output of the defended IQA metric changes by no more than a predefined delta for all input perturbations bounded by a given l 2 norm. Compared with two prior methods on three datasets, our method exhibited superior SROCC and PLCC scores while maintaining comparable certified guarantees. We also experimentally demonstrate that embedding the DMS-IQA defended quality metric into the training of image processing algorithms can yield benefits, but it requires extra computational resources. We made the code available on GitHub.},
  archive      = {J_CVIU},
  author       = {Ekaterina Shumitskaya and Mikhail Pautov and Dmitriy Vatolin and Anastasia Antsiferova},
  doi          = {10.1016/j.cviu.2025.104447},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104447},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Stochastic BIQA: Median randomized smoothing for certified blind image quality assessment},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of smart walker and augmented reality on gait parameters of a patient with spinocerebellar ataxia: Case report. <em>CVIU</em>, <em>259</em>, 104446. (<a href='https://doi.org/10.1016/j.cviu.2025.104446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ataxia is a neurological condition that impairs mobility and independence in daily activities. To mitigate the symptoms, patients often seek physical therapy interventions. However, these therapies can be challenging for some individuals, depending on their level of independence, and patients may experience pain and frustration due to repetitive tasks. To address these limitations, rehabilitation robots, such as the Smart Walker (SW), can be tailored to an individual’ s degree of independence, while Augmented Reality (AR) systems can enhance patient engagement and motivation. However, the use of AR may also lead to adverse effects, such as restrictions in gait patterns and the potential of cybersickness symptoms. In this context, this paper presents a case report of a patient with ataxia to evaluate the effects of the SW and AR in three tasks: Physiotherapist-Assisted Gait (PAG), Walker-Assisted Gait (WAG), and Augmented Reality Walker-Assisted Gait (ARWAG). The results show that the use of the SW in WAG led to improvements in gait parameters, including a 27% increase in step length and a 19% increase in hip excursion in the sagittal plane. In ARWAG, these improvements were even greater, with a 58% increase in step length and a 43% increase in hip excursion in the sagittal plane. No cybersickness symptoms were observed during the ARWAG. Additionally, among all tasks, the patient expressed a preference for the ARWAG, indicating that the combination of SW and AR holds potential benefits for assisting ataxia patients in physical therapy interventions.},
  archive      = {J_CVIU},
  author       = {Matheus Loureiro and Janine Valentino and Weslley Oliveira and Fabiana Machado and Arlindo Elias and Ricardo Mello and Arnaldo Leal and Anselmo Frizera},
  doi          = {10.1016/j.cviu.2025.104446},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104446},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Effects of smart walker and augmented reality on gait parameters of a patient with spinocerebellar ataxia: Case report},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Style transfer with diffusion models for synthetic-to-real domain adaptation. <em>CVIU</em>, <em>259</em>, 104445. (<a href='https://doi.org/10.1016/j.cviu.2025.104445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation models trained on synthetic data often perform poorly on real-world images due to domain gaps, particularly in adverse conditions where labeled data is scarce. Yet, recent foundation models enable to generate realistic images without any training. This paper proposes to leverage such diffusion models to improve the performance of vision models when learned on synthetic data. We introduce two novel techniques for semantically consistent style transfer using diffusion models: Class-wise Adaptive Instance Normalization and Cross-Attention ( CACTI ) and its extension with selective attention Filtering ( CACTI F ). CACTI applies statistical normalization selectively based on semantic classes, while CACTI F further filters cross-attention maps based on feature similarity, preventing artifacts in regions with weak cross-attention correspondences. Our methods transfer style characteristics while preserving semantic boundaries and structural coherence, unlike approaches that apply global transformations or generate content without constraints. Experiments using GTA5 as source and Cityscapes/ACDC as target domains show that our approach produces higher quality images with lower FID scores and better content preservation. Our work demonstrates that class-aware diffusion-based style transfer effectively bridges the synthetic-to-real domain gap even with minimal target domain data, advancing robust perception systems for challenging real-world applications. The source code is available at: https://github.com/echigot/cactif .},
  archive      = {J_CVIU},
  author       = {Estelle Chigot and Dennis G. Wilson and Meriem Ghrib and Thomas Oberlin},
  doi          = {10.1016/j.cviu.2025.104445},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104445},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Style transfer with diffusion models for synthetic-to-real domain adaptation},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S2DNet: A self-supervised deraining network using monocular videos. <em>CVIU</em>, <em>259</em>, 104444. (<a href='https://doi.org/10.1016/j.cviu.2025.104444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rainy conditions degrade the visual quality of images, thus presenting significant challenges for various vision-based downstream tasks. Traditional deraining approaches often rely on supervised learning methods requiring large, paired datasets of rainy and clean images. However, due to the dynamic and complex nature of rain, compiling such datasets is challenging and often insufficient for training robust models. As a result, researchers often resort to synthetic datasets. However, synthetic datasets have limitations because they often lack realism, can introduce biases, and seldom capture the diversity of real rain scenes. We propose a self-supervised method for image deraining using monocular videos that leverages the fact that rain moves spatially across frames, independently of the static elements in a scene, thus enabling isolation of rain-affected regions. We utilize depth information from the target frame and the camera’s relative pose (translations and rotations) across frames to achieve scene alignment. We apply a view-synthesis constraint that warps features from adjacent frames to the target frame, which enables us to generate pseudo-ground truth images by selecting clean pixels from the warped frame. The pseudo-clean images thus generated are effectively leveraged by our network to remove rain from images in a self-supervised manner without the need for a real rain paired dataset which is difficult to capture. Extensive evaluations on diverse real-world rainy datasets demonstrate that our approach achieves state-of-the-art performance in real image deraining, outperforming existing unsupervised methods.},
  archive      = {J_CVIU},
  author       = {Aman Kumar and Aditya Mohan and A.N. Rajagopalan},
  doi          = {10.1016/j.cviu.2025.104444},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104444},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {S2DNet: A self-supervised deraining network using monocular videos},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedVLP: Visual-aware latent prompt generation for multimodal federated learning. <em>CVIU</em>, <em>259</em>, 104442. (<a href='https://doi.org/10.1016/j.cviu.2025.104442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies indicate that prompt learning based on CLIP-like models excels in a variety of image recognition and detection tasks, consequently, it has been applied in Multimodal Federated Learning (MMFL). Federated Prompt Learning (FPL), as a technical branch of MMFL, enables clients and servers to exchange prompts rather than model parameters during communication to address challenges such as data heterogeneity and high training costs. Many existing FPL methods rely heavily on pre-trained visual-language models, making it difficult for them to handle new and real specialized domain data. To further boost the generalization ability of FPL without compromising the personalization of clients, we propose a novel framework that generates prompts guided by visual semantics to better handle specialized and small-scale data. In our approach, each client generates visual-aware latent prompts using a Fusion Encoder and an IE-Module, enabling the learning of fine-grained knowledge. Through federated computation, clients collaboratively maintain a global prompt, allowing the learning of coarse-grained knowledge. FedVLP removes the dependency on manually designed prompt templates and demonstrates superior performance across seven datasets, including CIFAR-10, CIFAR-100, Caltech-101, FLIndustry-100, and others.},
  archive      = {J_CVIU},
  author       = {Hao Pan and Xiaoli Zhao and Yuchen Jiang and Lipeng He and Bingquan Wang and Yincan Shu},
  doi          = {10.1016/j.cviu.2025.104442},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104442},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FedVLP: Visual-aware latent prompt generation for multimodal federated learning},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PDCNet: A lightweight and efficient robotic grasp detection framework via partial convolution and knowledge distillation. <em>CVIU</em>, <em>259</em>, 104441. (<a href='https://doi.org/10.1016/j.cviu.2025.104441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving detection accuracy complicates robotic grasp models, which makes deploying them on resource-constrained edge AI devices more challenging. Although various lightweight strategies have been proposed, directly designing compact networks may not be optimal, as balancing accuracy and model size is challenging. This paper proposes a lightweight grasp detection framework, PDCNet. In response to this problem, we optimize the interplay between computational demands and detection performance. The method integrates Partial Convolution (PConv) for efficient feature extraction, Discrete Wavelet Transform (DWT) for enhancing frequency-domain feature representation, and a Cross-Stage Fusion (CSF) strategy for optimizing the utilization of multi-scale features. A Quality-Enhanced Huber Loss Function (Q-Huber) is also introduced to improve the network’s sensitivity to vital grasp localities. Finally, the teacher–student framework distills expertise into a compact student model. Comprehensive evaluations were conducted using the public datasets to demonstrate that PDCNet achieves detection accuracies of 98.7%, 95.8%, and 97.1% on Cornell, Jacquard and Jacquard_V2 datasets respectively, while maintaining minimal parameters and high computational efficiency. Real-world experiments on an embedded edge AI device further validate the capability of PDCNet to perform accurate grasp detection under limited computational resources.},
  archive      = {J_CVIU},
  author       = {Yanshu Jiang and Yanze Fang and Liwei Deng},
  doi          = {10.1016/j.cviu.2025.104441},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104441},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {PDCNet: A lightweight and efficient robotic grasp detection framework via partial convolution and knowledge distillation},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for absolute pose regression based on cascaded attention modules. <em>CVIU</em>, <em>259</em>, 104440. (<a href='https://doi.org/10.1016/j.cviu.2025.104440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The absolute camera pose regression estimates the position and orientation of the camera solely based on captured RGB images. However, current single-image techniques often lack robustness, resulting in significant outliers. To address the issues of pose regressors in repetitive textures and dynamic blur scenarios, this paper proposes an absolute pose regression method based on cascaded attention modules. This network integrates global and local information through cascaded attention modules and then employs a dual-stream attention module to reduce the impact of dynamic objects and lighting changes on localization performance by constructing dual-channel dependencies. Specifically, the cascaded attention modules guide the model to focus on the relationships between global and local features and establish long-range channel dependencies, enabling the network to learn richer multi-scale feature representations. Additionally, a dual-stream attention module is introduced to further enhance feature representation by closely associating spatial and channel dimensions. This method is evaluated and analyzed on various indoor and outdoor datasets, with our method reducing the median position error and orientation error to 0.19 m/ 7 . 44 ° on 7-Scenes and 7.09 m/ 1 . 45 ° on RobotCar, demonstrating that the proposed method can significantly improve localization performance. Ablation studies on multiple categories further verify the effectiveness of the proposed modules.},
  archive      = {J_CVIU},
  author       = {Xiaogang Song and Junjie Tang and Kaixuan Yang and Weixuan Guo and Xiaofeng Lu and Xinhong Hei},
  doi          = {10.1016/j.cviu.2025.104440},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104440},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A method for absolute pose regression based on cascaded attention modules},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaptDiff: Adaptive diffusion learning for low-light image enhancement. <em>CVIU</em>, <em>259</em>, 104439. (<a href='https://doi.org/10.1016/j.cviu.2025.104439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering details obscured by noise from low-light images is a challenging task. Recent diffusion models have achieved relatively promising results in low-level vision tasks. However, there are still two issues: (1) under non-uniform illumination conditions, the low-light image cannot be restored with high quality, and (2) the models have limited generalization capabilities. To solve these problems, this paper proposes an Adaptive Enhancement Algorithm guided by a Multi-scale Structural Diffusion (AdaptDiff). AdaptDiff employs adaptive high-order mapping curves (AHMC) for pixel-by-pixel mapping of the image during the diffusion process, thereby adjusting the brightness levels between different regions within the image. In addition, a multi-scale structural guidance approach (MSGD) is proposed as an implicit bias, informing the intermediate layers of the model about the structural characteristics of the image, facilitating more effective restoration of clear images. Guiding the diffusion direction through structural information is conducive to maintaining good performance of the model even when faced with data that it has not previously encountered. Extensive experiments on popular benchmarks show that AdaptDiff achieves superior performance and efficiency.},
  archive      = {J_CVIU},
  author       = {Xiaotao Shao and Guipeng Zhang and Yan Shen and Boyu Zhang and Zhongli Wang and Yanlong Sun},
  doi          = {10.1016/j.cviu.2025.104439},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104439},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {AdaptDiff: Adaptive diffusion learning for low-light image enhancement},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution-aware contrastive learning for domain adaptation in 3D LiDAR segmentation. <em>CVIU</em>, <em>259</em>, 104438. (<a href='https://doi.org/10.1016/j.cviu.2025.104438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of 3D LiDAR point clouds is very important for applications like autonomous driving and digital twins of cities. However, current deep learning models suffer from a significant generalization gap. Unsupervised Domain Adaptation methods have recently emerged to tackle this issue. While domain-invariant feature learning using Maximum Mean Discrepancy has shown promise for images due to its simplicity, its application remains unexplored in outdoor mobile mapping point clouds. Moreover, previous methods do not consider the class information, which can lead to suboptimal adaptation performance. We propose a new approach—Contrastive Maximum Mean Discrepancy—to maximize intra-class domain alignment and minimize inter-class domain discrepancy, and integrate it into a 3D semantic segmentation model for LiDAR point clouds. The evaluation of our method with large-scale UDA datasets shows that it surpasses state-of-the-art UDA approaches for 3D LiDAR point clouds. CMMD is a promising UDA approach with strong potential for point cloud semantic segmentation.},
  archive      = {J_CVIU},
  author       = {Lamiae El Mendili and Sylvie Daniel and Thierry Badard},
  doi          = {10.1016/j.cviu.2025.104438},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104438},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Distribution-aware contrastive learning for domain adaptation in 3D LiDAR segmentation},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRDT-based knowledge synchronisation in an internet of robotics things ecosystem for ambient assisted living. <em>CVIU</em>, <em>259</em>, 104437. (<a href='https://doi.org/10.1016/j.cviu.2025.104437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating IoT and assistive robots in the design of Ambient Assisted Living (AAL) frameworks has proven to be a useful solution for monitoring and assisting elderly people at home. As a way to manage the information captured and assess the person’s condition, respond to emergencies, promote physical or cognitive exercises, etc., these systems can also integrate a Virtual Caregiver (VC). Given the diversity of technologies deployed in such an AAL framework, deciding how to manage knowledge appropriately can be complex. This paper proposes to organise the AAL framework as a distributed system, i.e., as a collection of autonomous software agents that provide users with a single coherent response. In this distributed system, agents are deployed locally and handle replicas of the knowledge model. The problem of merging these replicas into a consistent representation, therefore arises.The δ -CRDT (Conflict-free Replicated Data Type) synchronisation mechanism is employed to ensure the eventual consistency with low communication overhead. To manage the dynamics of the AAL ecosystem, the δ -CRDT is combined with the publish/subscribe interaction protocol. In this way, the performance of the IoT, the robot and the VC, through the functionalities that depend on them, is efficiently adapted to changes in the context. To demonstrate the validity of the proposal, two use cases have been designed in which a collaborative response from the system is required. The first one deals with a possible fall of the user at home, while the second one deals with the problem of helping the person move small objects around the flat. The measured values of latency or consistency in the data show that the proposal works satisfactorily.},
  archive      = {J_CVIU},
  author       = {José Galeas and Alberto Tudela and Óscar Pons and Juan Pedro Bandera and Antonio Bandera and Pablo Bustos},
  doi          = {10.1016/j.cviu.2025.104437},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104437},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CRDT-based knowledge synchronisation in an internet of robotics things ecosystem for ambient assisted living},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UM-mamba: An efficient U-network with medical visual state space for medical image segmentation. <em>CVIU</em>, <em>259</em>, 104436. (<a href='https://doi.org/10.1016/j.cviu.2025.104436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing computationally efficient network architectures remains a persistent necessity in medical image segmentation. Lately, State Space Models (SSMs) are emerging in the field of deep learning and gradually becoming effective basic building layers (or blocks) for constructing deep networks. SSMs not only effectively capture long-distance dependencies but also maintain linear computational complexity relative to input sizes. However, the non-sequential structure of 2D images limits its application in visual tasks. To solve this problem, this paper designs a Medical Visual State Space (MVSS) block with 2D Spiral Selective Scanning (SSS2D) module as the core, and constructs a U-shaped medical image segmentation network called UM-Mamba. The SSS2D module traverses the samples through four spiral scanning paths, which makes up for the deficiency of Mamba architecture in the non-sequential structure of 2D images. We conduct experiments on the Kvasir-SEG and ISIC2018 datasets, and achieve the best results in Dice, IoU and MAE by fine-tuning, which proves that UM-Mamba has the leading level in the experimental datasets.},
  archive      = {J_CVIU},
  author       = {Hejian Chen and Qing Liu and Zhongming Fu and Li Liu},
  doi          = {10.1016/j.cviu.2025.104436},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104436},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {UM-mamba: An efficient U-network with medical visual state space for medical image segmentation},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous hand gesture recognition: Benchmarks and methods. <em>CVIU</em>, <em>259</em>, 104435. (<a href='https://doi.org/10.1016/j.cviu.2025.104435'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we review the existing benchmarks for continuous gesture recognition, e.g., the online analysis of hand movements over time to detect and recognize meaningful gestures from a specific dictionary. Focusing on human–computer interaction scenarios, we classify these benchmarks based on input data types, gesture dictionaries, and evaluation metrics. Specific metrics for the continuous recognition task are crucial for understanding how effectively gestures are spotted in real time within input streams. We also discuss the most effective detection and classification methods proposed for these benchmarks. Our findings indicate that the number and quality of publicly available datasets remain limited, and evaluation methodologies for continuous recognition are not yet standardized. These issues highlight the need for new benchmarks that reflect real-world usage conditions and can support the development of best practices in gesture-based interface design.},
  archive      = {J_CVIU},
  author       = {Marco Emporio and Amirpouya Ghasemaghaei and Joseph J. Laviola Jr. and Andrea Giachetti},
  doi          = {10.1016/j.cviu.2025.104435},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104435},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Continuous hand gesture recognition: Benchmarks and methods},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPKDB-net: A salient-part pose keypoints-based dual-branch network for repetitive action counting. <em>CVIU</em>, <em>259</em>, 104434. (<a href='https://doi.org/10.1016/j.cviu.2025.104434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of deep learning, the field of repetitive action counting is gradually gaining notice from many researchers. Extraction of pose keypoints using human pose estimation networks is proven to be an effective pose-level method. However, the existing pose-level methods have some drawbacks, for example, ignoring the fact that occlusion and unfavourable viewing angles in videos lead to affect the accuracy of pose keypoints extraction. To overcome these problems, we propose a simple but efficient Salient-Part Pose Keypoints-Based Dual-Branch Network (SPKDB-Net). Specifically, we design a dual-branch input channel consisting of a global-based and a salient-part input branch. The global-based input branch is used to input the pose keypoints of the whole body extracted by the human pose estimation network, and the salient-part input branch is used to input the salient-part pose keypoints ( i.e. , head, shoulders, and hands). The second branch acts as an auxiliary to the first branch, thus effectively addressing the influence of external factors. In addition, we propose a DFEPM-Module that obtains long-distance dependency between pose keypoints through the attention mechanism, and obtains salient local features fused by the attention mechanism through convolution. Eventually, extensive experiments on the challenging RepCount-pose, UCFRep-pose and Countix-Fitness-pose benchmarks show that our proposed SPKDB-Net achieves state-of-the-art performance.},
  archive      = {J_CVIU},
  author       = {Jinying Wu and Jun Li and Qiming Li},
  doi          = {10.1016/j.cviu.2025.104434},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104434},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SPKDB-net: A salient-part pose keypoints-based dual-branch network for repetitive action counting},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-graph meta matching correction for noisy graph matching. <em>CVIU</em>, <em>259</em>, 104433. (<a href='https://doi.org/10.1016/j.cviu.2025.104433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant advancements have been made in image feature point matching within the context of deep graph matching. However, keypoint annotations in images can be inaccurate due to various issues such as occlusion, changes in viewpoint, or poor recognizability, leading to noisy correspondence. To address this limitation, we propose a novel Meta Matching Correction for noisy Graph Matching (MCGM), which introduces meta-learning to mitigate noisy correspondence for the first time. Specifically, we design a Meta Correcting Network (MCN) that integrates global features and geometric consistency information of graphs to generate confidence scores for nodes and edges. Based on the scores, MCN adaptively adjusts and penalizes the noisy assignments, enhancing the model’s ability to handle noisy correspondence. We conduct joint training of the main network and MCN to achieve dynamic correction through a bi-level optimization framework. Experimental evaluations on three public benchmark datasets demonstrate that our proposed method delivers robust performance improvements over state-of-the-art graph matching solutions and exhibits excellent stability when handling images under complex conditions.},
  archive      = {J_CVIU},
  author       = {Fangkai Li and Feiyu Pan and Wenjia Meng and Haoliang Sun and Xiushan Nie and Yilong Yin and Xiankai Lu},
  doi          = {10.1016/j.cviu.2025.104433},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104433},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cross-graph meta matching correction for noisy graph matching},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking the sparse mask learning mechanism in sparse convolution for object detection on drone images. <em>CVIU</em>, <em>259</em>, 104432. (<a href='https://doi.org/10.1016/j.cviu.2025.104432'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although sparse convolutional neural networks have achieved significant progress in fast object detection on high-resolution drone images, the research community has yet to pay enough attention to the great potential of prior knowledge (i.e., local contextual information) in UAV imagery for assisting sparse masks to improve detector performance. Such prior knowledge is beneficial for object detection in complex drone imagery, as tiny objects may be mistakenly detected or even missed entirely without referencing the local context surrounding them. In this paper, we take these priors into account and propose a crucial region learning strategy for sparse masks to boost object detection performance. Specifically, we extend the mask region from the feature region of the objects to their surrounding local context region and introduce a method for selecting and evaluating this local context region. Furthermore, we propose a novel mask-matching constraint to replace the mask activation ratio constraint, thereby enhancing object localization accuracy. We extensively evaluate our method across various detectors on two UAV benchmarks: VisDrone and UAVDT. By leveraging our mask learning strategy, the state-of-the-art sparse convolutional framework achieves higher detection gains with a faster detection speed, demonstrating its significant superiority.},
  archive      = {J_CVIU},
  author       = {Yixuan Li and Pengnian Wu and Meng Zhang},
  doi          = {10.1016/j.cviu.2025.104432},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104432},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Rethinking the sparse mask learning mechanism in sparse convolution for object detection on drone images},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective CNN and transformer fusion network for camouflaged object detection. <em>CVIU</em>, <em>259</em>, 104431. (<a href='https://doi.org/10.1016/j.cviu.2025.104431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflage object detection aims to identify concealed objects in images. Global context and local spatial details are crucial for this task. Convolutional neural network (CNN) excels at capturing fine-grained local features, while Transformer is adept at modeling global contextual information. To leverage their respective strengths, we propose a novel CNN-Transformer fusion network (CTF-Net) for COD to achieve more accurate detection. Our approach employs parallel CNN and Transformer branches as an encoder to extract complementary features. We then propose a cross-domain fusion module (CDFM) to fuse these features with cross-modulation. Additionally, we develop a boundary-aware module (BAM) that combines low-level edge details with high-level global context to extract camouflaged object edge features. Furthermore, we design a feature enhancement module (FEM) to mitigate background and noise interference during cross-layer feature fusion, thereby highlighting camouflaged object regions for precise predictions. Extensive experiments show that CTF-Net outperforms the existing 16 state-of-the-art methods on four widely-used COD datasets. Especially, compared with all the comparison models, CTF-Net significantly improves the performance by ∼ 5.1% (F-measure) on the NC4K dataset, showing that CTF-Net could accurately detect camouflaged objects. Our code is publicly available at https://github.com/zcc0616/CTF-Net .},
  archive      = {J_CVIU},
  author       = {Dongdong Zhang and Chunping Wang and Huiying Wang and Qiang Fu and Zhaorui Li},
  doi          = {10.1016/j.cviu.2025.104431},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104431},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An effective CNN and transformer fusion network for camouflaged object detection},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive context mining for camouflaged object detection with scribble supervision. <em>CVIU</em>, <em>259</em>, 104430. (<a href='https://doi.org/10.1016/j.cviu.2025.104430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to find objects hidden in their surroundings, which has attracted extensive attention in recent years. Although fully supervised COD methods have made considerable progress in performance, they rely heavily on expensive pixel-level annotations. Scribble-based weakly supervised methods can effectively alleviate this problem, but they struggle to fully understand complex COD tasks and achieve outstanding performance due to limited information in the training data. In this paper, inspired by the human visual mechanism, we propose a novel framework for graffiti-based COD, named SCNet. This framework focuses on learning multi-scale context-aware features and employs a two-stage strategy for efficient detection. Specifically, we first adopt the improved Pyramid Visual Transformer (PVTv2) model as the backbone to extract multi-scale global contextual information. A neighbor-interactive decoder (NID) is then designed to coarsely localize potential object regions. Further, a refinement module (RM) is introduced to facilitate multi-scale information interaction and contextual information mining to refine the object regions. In addition, adaptive local camouflage coherence (ALCC) loss is devised to enhance the network’s adaptability to different complex scenarios. Experimental results on three benchmark COD datasets show that SCNet, which utilizes only scribble annotations without any pre- or post-processing, not only outperforms six state-of-the-art weakly supervised methods, but even surpasses some fully supervised COD methods. Moreover, SCNet achieves promising results in a COD-related task (polyp segmentation). The results of our method are available at https://github.com/zcc0616/SCNet .},
  archive      = {J_CVIU},
  author       = {Dongdong Zhang and Chunping Wang and Huiying Wang and Qiang Fu and Zhaorui Li},
  doi          = {10.1016/j.cviu.2025.104430},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104430},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive context mining for camouflaged object detection with scribble supervision},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel spatial–temporal feature analysis for generic event boundary detection in videos. <em>CVIU</em>, <em>259</em>, 104429. (<a href='https://doi.org/10.1016/j.cviu.2025.104429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generic event boundary detection (GEBD) aims to split video into chunks at a broad and diverse set of actions as humans naturally perceive event boundaries. In this study, we propose an approach that leverages multilevel spatial–temporal features to construct a framework for localizing generic events in videos. Our method capitalizes on the correlation between neighbor frames, employing a hierarchy of spatial and temporal features to create a comprehensive representation. Specifically, features from multiple spatial dimensions of a pre-trained ResNet-50 are combined with diverse temporal views, generating a multilevel spatial–temporal feature map. This map facilitates the calculation of similarities between neighbor frames, which are then projected to build a multilevel spatial–temporal similarity feature vector. Subsequently, a decoder employing 1D convolution operations deciphers these similarities, incorporating their temporal relationships to estimate boundary scores effectively. Extensive experiments conducted on the GEBD benchmark dataset demonstrate the superior performance of our system and its variants, outperforming state-of-the-art approaches. Furthermore, additional experiments conducted on the TAPOS dataset, comprising long-form videos with Olympic sport actions, reaffirm the efficacy of our proposed methodology compared to existing techniques.},
  archive      = {J_CVIU},
  author       = {Van Thong Huynh and Seungwon Kim and Hyung-Jeong Yang and Soo-Hyung Kim},
  doi          = {10.1016/j.cviu.2025.104429},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104429},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multilevel spatial–temporal feature analysis for generic event boundary detection in videos},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Light-YOLO: A lightweight and high-performance network for detecting small obstacles on roads at night. <em>CVIU</em>, <em>259</em>, 104428. (<a href='https://doi.org/10.1016/j.cviu.2025.104428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of detecting small obstacles and model portability, this study proposes a lightweight, high-precision, and high-speed small obstacle detection network at nighttime road environments referred to as Light-YOLO. First, the SPDConvMobileNetV3 feature extraction network is introduced, which significantly reduces the total number of parameters while enhancing the ability to capture small obstacle details. Next, to make the network more focused on small obstacles at nighttime conditions, a loss function called Wise-IoU is incorporated, which is more suitable to low-quality images. Finally, to improve overall model performance without increasing the total number of parameters, a parameter-free attention mechanism (SimAM) is integrated. By comparing the publicly available data with the self-built dataset, the experimental results show that Light-YOLO achieves a mean average precision ( m A P 50 ) of 97.1% while maintaining a high image processing speed. Additionally, compared to other advanced models in the same series, Light-YOLO has fewer parameters, a smaller computational load (GFLOPs), and reduced model weight (Best.pt). Overall, Light-YOLO strikes a balance between lightweight design, accuracy, and speed, making it more suitable for hardware-constrained devices.},
  archive      = {J_CVIU},
  author       = {Dan Huang and Guangyin Zhang and Zixu Li and Keying Liu and Wenguang Luo},
  doi          = {10.1016/j.cviu.2025.104428},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104428},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Light-YOLO: A lightweight and high-performance network for detecting small obstacles on roads at night},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GraPLUS: Graph-based placement using semantics for image composition. <em>CVIU</em>, <em>259</em>, 104427. (<a href='https://doi.org/10.1016/j.cviu.2025.104427'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present GraPLUS (Graph-based Placement Using Semantics), a novel framework for plausible object placement in images that leverages scene graphs and large language models. Our approach uniquely combines graph-structured scene representation with semantic understanding to determine contextually appropriate object positions. The framework employs GPT-2 to transform categorical node and edge labels into rich semantic embeddings that capture both definitional characteristics and typical spatial contexts, enabling a nuanced understanding of object relationships and placement patterns. GraPLUS achieves a placement accuracy of 92.1% and an FID score of 28.83 on the OPA dataset, outperforming state-of-the-art methods by 8.3% while maintaining competitive visual quality. In human evaluation studies involving 964 samples assessed by 38 participants, our method was preferred in 51.8% of cases, significantly outperforming previous approaches (25.8% and 22.4% for the next best methods). The framework’s key innovations include: (i) leveraging pre-trained scene graph models that transfer knowledge from other domains, eliminating the need to train feature extraction parameters from scratch, (ii) edge-aware graph neural networks that process scene semantics through structured relationships, (iii) a cross-modal attention mechanism that aligns categorical embeddings with enhanced scene features, and (iv) a multiobjective training strategy incorporating semantic consistency constraints. Extensive experiments demonstrate GraPLUS’s superior performance in both placement plausibility and spatial precision, with particular strengths in maintaining object proportions and contextual relationships across diverse scene types.},
  archive      = {J_CVIU},
  author       = {Mir Mohammad Khaleghi and Mehran Safayani and Abdolreza Mirzaei},
  doi          = {10.1016/j.cviu.2025.104427},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104427},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {GraPLUS: Graph-based placement using semantics for image composition},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FAR-AMTN: Attention multi-task network for face attribute recognition. <em>CVIU</em>, <em>259</em>, 104426. (<a href='https://doi.org/10.1016/j.cviu.2025.104426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the generalization performance of Multi-Task Networks (MTN) in Face Attribute Recognition (FAR), it is crucial to share relevant information across multiple related prediction tasks effectively. Traditional MTN methods create shared low-level modules and distinct high-level modules, causing an exponential increase in model parameters with the addition of tasks. This approach also limits feature interaction at the high level, hindering the exploration of semantic relations among attributes, thereby affecting generalization negatively. In response, this study introduces FAR-AMTN, a novel Attention Multi-Task Network for FAR. It incorporates a Weight-Shared Group-Specific Attention (WSGSA) module with shared parameters to minimize complexity while improving group feature representation. Furthermore, a Cross-Group Feature Fusion (CGFF) module is utilized to foster interactions between attribute groups, enhancing feature learning. A Dynamic Weighting Strategy (DWS) is also introduced for synchronized task convergence. Experiments on the CelebA and LFWA datasets demonstrate that the proposed FAR-AMTN demonstrates superior accuracy with significantly fewer parameters compared to existing models.},
  archive      = {J_CVIU},
  author       = {Gong Gao and Zekai Wang and Xianhui Liu and Weidong Zhao},
  doi          = {10.1016/j.cviu.2025.104426},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104426},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FAR-AMTN: Attention multi-task network for face attribute recognition},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving a segment anything model for segmenting low-quality medical images via an adapter. <em>CVIU</em>, <em>259</em>, 104425. (<a href='https://doi.org/10.1016/j.cviu.2025.104425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase in large models, segmentation foundation models have greatly improved the segmentation results of medical images. However, these foundation models often yield unsatisfactory segmentation results because their training data rarely involve low-quality images. In medical imaging, issues such as considerable noise or poor image resolution are common due to imaging equipment. Using these segmentation foundation models on such images produces poor results. To address this challenge, we utilize a low-quality perception adapter to improve the capabilities of segmentation foundation models, specifically in terms of handling low-quality medical images. First, the low-quality perception adapter distills the intrinsic statistical features from images compromised by noise or reduced clarity. These intrinsic features are aligned with textual-level attributes by employing contrastive learning. Then, we use a text-vision progressive fusion strategy, starting with multilevel text–image fusion to incorporate multimodal information. Next, we incorporate visual features from the underlying segmentation foundation model. Finally, a carefully designed decoder predicts the segmented mask. The low-quality perception adapter reduces the impacts of blur and noise on the developed model, while text-based contrastive learning, along with multimodal fusion, bridge the semantic gap. Experiments demonstrate that the proposed model significantly improves segmentation accuracy on noisy or blurry medical images, with gains up to 24.6% in mIoU and 13.6% in pixel accuracy over state-of-the-art methods across multiple datasets.},
  archive      = {J_CVIU},
  author       = {Can Bai and Jie Wang and Xianjun Han and Zijian Wu},
  doi          = {10.1016/j.cviu.2025.104425},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104425},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Improving a segment anything model for segmenting low-quality medical images via an adapter},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffMatter: Different frequency fusion for trimap-free image matting via edge detection. <em>CVIU</em>, <em>259</em>, 104424. (<a href='https://doi.org/10.1016/j.cviu.2025.104424'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matting extracts the foreground from the target image by predicting the alpha transparency of the foreground. Existing methods rely on constraints such as trimaps to distinguish the foreground from the background in the image, which, while improving accuracy, inevitably incurs significant costs. This paper proposes a trimap-free automatic matting method that highlights the foreground area through edge detection. To address the domain adaptation issues of edge information and the fine-grained features required for matting task, we designed a plug-and-play D ifferent F requency F usion module according to the paradigm of characteristic enhancement, feature fusion, and information integration to effectively combine high-frequency components with low-frequency counterparts and propose a matting model, DiffMatter. Specifically, we designed texture highlighting and semantic enhancement modules for high-frequency and low-frequency information during the characteristic enhancement phase. For feature fusion, we employed cross-fusion operations, and in the information integration phase, we integrated information across spatial and channel dimensions. Additionally, to compensate for the shortcoming of transformer in capturing local information, we construct an attention embedding module and propose a cross-aware module to utilize channel and spatial information, respectively, to enhance representational capability. Experimental results on the Composition-1k, Distinctions, 646, and real-world AIM-500 datasets demonstrate that our model outperforms competing methods, achieving a balance between performance and computational efficiency. Furthermore, our different frequency fusion module enhances several state-of-the-art matting models. The code will be publicly released.},
  archive      = {J_CVIU},
  author       = {Anming Sun and Junjie Chang and Guilin Yao},
  doi          = {10.1016/j.cviu.2025.104424},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104424},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DiffMatter: Different frequency fusion for trimap-free image matting via edge detection},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring black-box adversarial attacks on interpretable deep learning systems. <em>CVIU</em>, <em>259</em>, 104423. (<a href='https://doi.org/10.1016/j.cviu.2025.104423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have empirically demonstrated that neural network interpretability is susceptible to malicious manipulations. However, existing attacks on Interpretable Deep Learning Systems (IDLSes) predominantly focus on the white-box setting, which is impractical for real-world applications. In this paper, we present the first attempt to attack IDLSes in more challenging and realistic black-box settings. We introduce a novel framework called Dual Black-box Adversarial Attack (DBAA) which can generate adversarial examples that are misclassified as the target class, while maintaining interpretations similar to their benign counterparts. In our method, adversarial examples are generated via black-box adversarial attacks and then refined using ADV-Plugin, a novel approach proposed in this paper, which employs single-pixel perturbation and an adaptive step-size algorithm to enhance explanation similarity with benign samples while preserving adversarial properties. We conduct extensive experiments on multiple datasets (CIFAR-10, ImageNet, and Caltech-101) and various combinations of classifiers and interpreters, comparing our approach against five baseline methods. Empirical results indicate that DBAA is comparable to regular adversarial attacks in compromising classifiers and significantly enhances interpretability deception. Specifically, DBAA achieves Intersection over Union (IoU) scores exceeding 0.5 across all interpreters, approximately doubling the performance of regular attacks, while concurrently reducing the average ℓ 2 distance between its attribution maps and those of benign samples by about 50%.},
  archive      = {J_CVIU},
  author       = {Yike Zhan and Baolin Zheng and Dongxin Liu and Boren Deng and Xu Yang},
  doi          = {10.1016/j.cviu.2025.104423},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104423},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Exploring black-box adversarial attacks on interpretable deep learning systems},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STDepth: Leveraging semantic-textural information in transformers for self-supervised monocular depth estimation. <em>CVIU</em>, <em>259</em>, 104422. (<a href='https://doi.org/10.1016/j.cviu.2025.104422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised monocular depth estimation, relying solely on monocular or stereo video for supervision, plays an important role in computer vision. The encoder backbone generates features at various stages, and each stage exhibits distinct properties. However, conventional methods fail to take full advantage of these distinctions and apply the same processing to features from different stages, lacking the adaptability required for aggregating the unique information in features. In this research, we replace convolutional neural networks (CNNs) with a Transformer as the encoder backbone, intending to enhance the model’s ability to encode long-range spatial dependencies. Furthermore, we introduce a semantic-textural decoder (STDec) to emphasize local critical regions and more effectively process intricate details. The STDec incorporates two principal modules: (1) the global feature recalibration (GFR) module, which performs a comprehensive analysis of the scene structure using high-level features, and recalibrates features in the spatial dimension through semantic information, and (2) the detail focus (DF) module is employed in low-level features to capture texture details precisely. Additionally, we propose an innovative multi-arbitrary-scale reconstruction loss (MAS Loss) function to fully exploit the depth estimation network’s capabilities. The extensive experimental results demonstrate that our method achieves state-of-the-art performance on the KITTI dataset. Moreover, our models demonstrate remarkable generalization ability when applied to the Make3D and NYUv2 datasets. The code is publicly available at: https://github.com/xagao/STDepth .},
  archive      = {J_CVIU},
  author       = {Xuanang Gao and Bingchao Wang and Zhiwei Ning and Jie Yang and Wei Liu},
  doi          = {10.1016/j.cviu.2025.104422},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104422},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {STDepth: Leveraging semantic-textural information in transformers for self-supervised monocular depth estimation},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ERTFNet: Enhanced RGB-T fusion network for semantic segmentation by integrating thermal edge features. <em>CVIU</em>, <em>259</em>, 104421. (<a href='https://doi.org/10.1016/j.cviu.2025.104421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is crucial for computer vision, especially in the field of autonomous driving. RGB-Thermal (RGB-T) fusion networks enhance semantic segmentation accuracy in road scenes. However, most existing methods employ the same module structure to extract features from both RGB and thermal images, and all the obtained features are subsequently fused, neglecting the unique characteristics of each modality. Nevertheless, the fused thermal features may introduce noise and redundancy into the network, which is capable of segmenting objects well solely using RGB images. As a result, the performance and accuracy of the approach are limited in complex scenarios. To address this problem, a novel method named Enhanced RGB-T Fusion Network (ERTFNet) is proposed by adopting the encoder–decoder design concept. The constructed encoder in ERTFNet can obtain fused features by combining the extracted edge features from thermal images with RGB image features processed by an attention mechanism. Then, the feature map is restored by a general decoder. Additionally, we introduce the spatial edge constraints during the training stage to further enhance the model’s ability to capture image details and improve both prediction accuracy and boundary clarity. Experiments on two public datasets, compared with existing methods, show that the proposed method can obtain more clear visual contours and higher prediction accuracy.},
  archive      = {J_CVIU},
  author       = {Hanqi Yin and Liguo Zhang and Yiming Sun and Guisheng Yin},
  doi          = {10.1016/j.cviu.2025.104421},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104421},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {ERTFNet: Enhanced RGB-T fusion network for semantic segmentation by integrating thermal edge features},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep semantic segmentation for drivable area detection on unstructured roads. <em>CVIU</em>, <em>259</em>, 104420. (<a href='https://doi.org/10.1016/j.cviu.2025.104420'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drivable area detection on unstructured roads is crucial for autonomous driving, as it provides path planning constraints for end-to-end models and enhances driving safety. This paper proposes a deep learning approach for drivable area detection on unstructured roads using semantic segmentation. The deep learning approach is based on the DeepLabv3+ network and incorporates a Unit Attention Module following the Atrous Spatial Pyramid Pooling Module in the encoder. The Unit Attention Module combines a dual attention module and a spatial attention module. It enhances the adaptive weighting of semantic information in key channels and spatial locations, thereby improving the overall segmentation accuracy of drivable areas on unstructured roads. Evaluations on the India Driving Dataset demonstrate that the proposed network consistently surpasses most comparative methods, achieving a mean IoU of 85.99% and a mean pixel accuracy of 92.01%.},
  archive      = {J_CVIU},
  author       = {Xiangjun Mo and Yonghui Feng and Yihe Liu},
  doi          = {10.1016/j.cviu.2025.104420},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104420},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep semantic segmentation for drivable area detection on unstructured roads},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSCA: A few-shot segmentation framework driven by multi-scale cross-attention and information extraction. <em>CVIU</em>, <em>259</em>, 104419. (<a href='https://doi.org/10.1016/j.cviu.2025.104419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-Shot Semantic Segmentation (FSS) aims to achieve precise pixel-level segmentation of target objects in query images using only a small number of annotated support images. The main challenge lies in effectively capturing and transferring critical information from support samples while establishing fine-grained semantic associations between query and support images to improve segmentation accuracy. However, existing methods struggle with spatial alignment issues caused by intra-class variations and inter-class visual similarities, and they fail to fully integrate high-level and low-level decoder features. To address these limitations, we propose a novel framework based on cross-scale interactive attention mechanisms. This framework employs a hybrid mask-guided multi-scale feature fusion strategy, constructing a cross-scale attention network that spans from local details to global context. It dynamically enhances target region representation and alleviates spatial misalignment issues. Furthermore, we design a hierarchical multi-axis decoding architecture that progressively integrates multi-resolution feature pathways, enabling the model to focus on semantic associations within foreground regions. Experimental results show that our Multi-Scale Cross-Attention (MSCA) model performs exceptionally well on the PASCAL-5i and COCO-20i benchmark datasets, achieving highly competitive results. Notably, the model contains only 1.86 million learnable parameters, demonstrating its efficiency and practical applicability.},
  archive      = {J_CVIU},
  author       = {Zhihao Ren and Shengning Lu and Xinhua Wang and Yaoming Liu and Yong Liang},
  doi          = {10.1016/j.cviu.2025.104419},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104419},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MSCA: A few-shot segmentation framework driven by multi-scale cross-attention and information extraction},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cascading attention enhancement network for RGB-D indoor scene segmentation. <em>CVIU</em>, <em>259</em>, 104411. (<a href='https://doi.org/10.1016/j.cviu.2025.104411'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network based Red, Green, Blue, and Depth (RGB-D) image semantic segmentation for indoor scenes has attracted increasing attention, because of its great potentiality of extracting semantic information from RGB-D images. However, the challenge it brings lies in how to effectively fuse features from RGB and depth images within the neural network architecture. The technical approach of feature aggregation has evolved from the early integration of RGB color images and depth images to the current cross-attention fusion, which enables the features of different RGB channels to be fully integrated with ones of the depth image. However, noises and useless feature for segmentation are inevitably propagated between feature layers during the period of feature aggregation, thereby affecting the accuracy of segmentation results. In this paper, for indoor scenes, a cascading attention enhancement network (CAENet) is proposed with the aim of progressively refining the semantic features of RGB and depth images layer by layer, consisting of four modules: a channel enhancement module (CEM), an adaptive aggregation of spatial attention (AASA), an adaptive aggregation of channel attention (AACA), and a triple-path fusion module (TFM). In encoding stage, CEM complements RGB features with depth features at the end of each layer, in order to effectively revise RGB features for the next layer. At the end of encoding stage, AASA module combines low-level and high-level RGB semantic features by their spatial attention, and AACA module fuses low-level and high-level depth semantic features by their channel attention. The combined RGB and depth semantic features are fused into one and fed into the decoding stage, which consists of triple-path fusion modules (TFMs) combining low-level RGB and depth semantic features and decoded high-level semantic features. The TFM outputs multi-scale feature maps that encapsulate both rich semantic information and fine-grained details, thereby augmenting the model’s capacity for accurate per-pixel semantic label prediction. The proposed CAENet achieves mIoU of 52.0% on NYUDv2 and 48.3% on SUNRGB-D datasets, outperforming recent RGB-D segmentation methods.},
  archive      = {J_CVIU},
  author       = {Xu Tang and Songyang Cen and Zhanhao Deng and Zejun Zhang and Yan Meng and Jianxiao Xie and Changbing Tang and Weichuan Zhang and Guanghui Zhao},
  doi          = {10.1016/j.cviu.2025.104411},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104411},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cascading attention enhancement network for RGB-D indoor scene segmentation},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal graph neural network based child action recognition using data-efficient methods: A systematic analysis. <em>CVIU</em>, <em>259</em>, 104410. (<a href='https://doi.org/10.1016/j.cviu.2025.104410'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents implementations on child activity recognition (CAR) using spatial–temporal graph neural network (ST-GNN)-based deep learning models with the skeleton modality. Prior implementations in this domain have predominantly utilized CNN, LSTM, and other methods, despite the superior performance potential of graph neural networks. To the best of our knowledge, this study is the first to use an ST-GNN model for child activity recognition employing both in-the-lab, in-the-wild, and in-the-deployment skeleton data. To overcome the challenges posed by small publicly available child action datasets, transfer learning methods such as feature extraction and fine-tuning were applied to enhance model performance. As a principal contribution, we developed an ST-GNN-based skeleton modality model that, despite using a relatively small child action dataset, achieved superior performance (94.81%) compared to implementations trained on a significantly larger (x10) adult action dataset (90.6%) for a similar subset of actions. With ST-GCN-based feature extraction and fine-tuning methods, accuracy improved by 10%–40% compared to vanilla implementations, achieving a maximum accuracy of 94.81%. Additionally, implementations with other ST-GNN models demonstrated further accuracy improvements of 15%–45% over the ST-GCN baseline. The results on activity datasets empirically demonstrate that class diversity, dataset size, and careful selection of pre-training datasets significantly enhance accuracy. In-the-wild and in-the-deployment implementations confirm the real-world applicability of above approaches, with the ST-GNN model achieving 11 FPS on streaming data. Finally, preliminary evidence on the impact of graph expressivity and graph rewiring on accuracy of small dataset-based models is provided, outlining potential directions for future research. The codes are available at https://github.com/sankamohotttala/ST_GNN_HAR_DEML .},
  archive      = {J_CVIU},
  author       = {Sanka Mohottala and Asiri Gawesha and Dharshana Kasthurirathna and Pradeepa Samarasinghe and Charith Abhayaratne},
  doi          = {10.1016/j.cviu.2025.104410},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104410},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Spatio-temporal graph neural network based child action recognition using data-efficient methods: A systematic analysis},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCTD: A CNN-transformer hybrid for precise object detection in UAV aerial imagery. <em>CVIU</em>, <em>259</em>, 104409. (<a href='https://doi.org/10.1016/j.cviu.2025.104409'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in UAV imagery poses substantial challenges due to severe object scale variation, dense distributions of small objects, complex backgrounds, and arbitrary orientations. These factors, compounded by high inter-class similarity and large intra-class variation caused by multi-scale targets, occlusion, and environmental interference, make aerial object detection fundamentally different from conventional scenes. Existing methods often struggle to capture global semantic information effectively and tend to overlook critical issues such as feature loss during downsampling, information redundancy, and inconsistency in cross-level feature interactions. To address these limitations, this paper proposes a hybrid CNN-Transformer-based detector, termed HCTD, specifically designed for UAV image analysis. The proposed framework integrates three novel modules: (1) a Feature Filtering Module (FFM) that enhances discriminative responses and suppresses background noise through dual global pooling (max and average) strategies; (2) a Convolutional Additive Self-attention Feature Interaction (CASFI) module that replaces dot-product attention with a lightweight additive fusion of spatial and channel interactions, enabling efficient global context modeling at reduced computational cost; and (3) a Global Context Flow Feature Pyramid Network (GC2FPN) that facilitates multi-scale semantic propagation and alignment to improve small-object detection robustness. Extensive experiments on the VisDrone2019 dataset demonstrate that HCTD-R18 and HCTD-R50 achieve 38.2%/43.7% AP 50 , 23.1%/24.6% AP 75 , and 13.9%/14.7% AP S respectively. Additionally, the TIDE toolkit is employed to analyze the absolute and relative contributions of six error types, providing deeper insight into the effectiveness of each module and offering valuable guidance for future improvements. The code is available at: https://github.com/Mundane-X/HCTD .},
  archive      = {J_CVIU},
  author       = {Hongcheng Xue and Zhan Tang and Yuantian Xia and Longhe Wang and Lin Li},
  doi          = {10.1016/j.cviu.2025.104409},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104409},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {HCTD: A CNN-transformer hybrid for precise object detection in UAV aerial imagery},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SASFNet: Soft-edge awareness and spatial-attention feedback deep network for blind image deblurring. <em>CVIU</em>, <em>259</em>, 104408. (<a href='https://doi.org/10.1016/j.cviu.2025.104408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a camera is used to capture moving objects in natural scenes, the obtained images will be degraded to varying degrees due to camera shaking and object displacement, which is called motion blurring. Moreover, the complexity of natural scenes makes the image motion deblurring more challenging. Now, there are two crucial problems in Deep Learning-based methods for blind motion deblurring: (1) how to restore sharp images with fine textures, and (2) how to improve the generalization of the model. In this paper, we propose Soft-edge Awareness and Spatial-attention Feedback deep Network (SASFNet) to restore sharp images. First, we restore images with fine textures using a soft-edge assist mechanism. This mechanism uses the soft edge extraction network to map the fine edge information from the blurred image to assist the model to restore the high-quality clear image. Second, for the generalization of the model, we propose feedback mechanism with attention. Similar to course learning, feedback mechanism imitates the human learning process, learning from easy to difficult to restore sharp images, which not only refines the restored features, but also brings better generalization. To evaluate the model, we use the GoPro dataset for model training and validity testing, and the Realblur dataset to test the generalization of the model. Experiments show that our proposed SASFNet can not only restore sharp images that are more in line with human perception, but also has good generalization.},
  archive      = {J_CVIU},
  author       = {Jing Cheng and Kaibing Zhang and Jiahui Hou and Yuhong Zhang and Guang Shi},
  doi          = {10.1016/j.cviu.2025.104408},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104408},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SASFNet: Soft-edge awareness and spatial-attention feedback deep network for blind image deblurring},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive reverse attention network for image inpainting detection and localization. <em>CVIU</em>, <em>259</em>, 104407. (<a href='https://doi.org/10.1016/j.cviu.2025.104407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting is originally presented to restore damaged image areas, but it might be maliciously used for object removal that change image semantic content. This easily leads to serious public confidence crises. Up to present, image inpainting forensics works have achieved remarkable results, but they usually ignore or fail to capture subtle artifacts near object boundary, resulting in inaccurate object mask localization. To address this issue, we propose a Progressive Reverse Attention Network (PRA-Net) for image inpainting detection and localization. Different from the traditional convolutional neural networks (CNN) structure, PRA-Net follows an encoder and decoder architecture. The encoder leverages features at different scales with dense cross-connections to locate inpainted regions and generates global map with our designed multi-scale extraction module. A reverse attention module is used as the backbone of the decoder to progressively refine the details of predictions. Experimental results show that PRA-Net achieves accurate image inpainting localization and desirable robustness.},
  archive      = {J_CVIU},
  author       = {Shuai Liu and Jiyou Chen and Xiangling Ding and Gaobo Yang},
  doi          = {10.1016/j.cviu.2025.104407},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104407},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Progressive reverse attention network for image inpainting detection and localization},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSGN:CLIP-driven semantic guidance network for clothes-changing person re-identification. <em>CVIU</em>, <em>259</em>, 104406. (<a href='https://doi.org/10.1016/j.cviu.2025.104406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clothes-Changing Person Re-identification (CCReID) aims to match identities across images of individuals in different attires. Due to the significant appearance variations caused by clothing changes, distinguishing the same identity becomes challenging, while the differences between distinct individuals are often subtle. To address this, we reduce the impact of clothing information on identity judgment by introducing linguistic modalities. Considering CLIP’s (Contrastive Language-Image Pre-training) ability to align high-level semantic information with visual features, we propose a CLIP-driven Semantic Guidance Network (CSGN), which consists of a Multi-Description Generator (MDG), a Visual Semantic Steering module (VSS), and a Heterogeneous Semantic Fusion loss (HSF). Specifically, to mitigate the color sensitivity of CLIP’s text encoder, we design the MDG to generate pseudo-text in both RGB and grayscale modalities, incorporating a combined loss function for text-image mutuality. This helps reduce the encoder’s bias towards color. Additionally, to improve the CLIP visual encoder’s ability to extract identity-independent features, we construct the VSS, which combines ResNet and ViT feature extractors to enhance visual feature extraction. Finally, recognizing the complementary nature of semantics in heterogeneous descriptions, we use HSF, which constrains visual features by focusing not only on pseudo-text derived from RGB but also on pseudo-text derived from grayscale, thereby mitigating the influence of clothing information. Experimental results show that our method outperforms existing state-of-the-art approaches.},
  archive      = {J_CVIU},
  author       = {Yang Lu and Bin Ge and Chenxing Xia and Junming Guan},
  doi          = {10.1016/j.cviu.2025.104406},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104406},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CSGN:CLIP-driven semantic guidance network for clothes-changing person re-identification},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast self-supervised 3D mesh object retrieval for geometric similarity. <em>CVIU</em>, <em>259</em>, 104405. (<a href='https://doi.org/10.1016/j.cviu.2025.104405'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital 3D models play a pivotal role in engineering, entertainment, education, and various domains. However, the search and retrieval of these models have not received adequate attention compared to other digital assets like documents and images. Traditional supervised methods face challenges in scalability due to the impracticality of creating large, labeled collections of 3D objects. In response, this paper introduces a self-supervised approach to generate efficient embeddings for 3D mesh objects, facilitating ranked retrieval of similar objects. The proposed method employs a straightforward representation of mesh objects and utilizes an encoder–decoder architecture to learn the embedding. Extensive experiments demonstrate the competitiveness of our approach compared to supervised methods, showcasing its scalability across diverse object collections. Notably, the method exhibits transferability across datasets, implying its potential for broader applicability beyond the training dataset. The robustness and generalization capabilities of the proposed method are substantiated through experiments conducted on varied datasets. These findings underscore the efficacy of the approach in capturing underlying patterns and features, independent of dataset-specific nuances. This self-supervised framework offers a promising solution for enhancing the search and retrieval of 3D models, addressing key challenges in scalability and dataset transferability.},
  archive      = {J_CVIU},
  author       = {Kajal Sanklecha and Prayushi Mathur and P.J. Narayanan},
  doi          = {10.1016/j.cviu.2025.104405},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104405},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Fast self-supervised 3D mesh object retrieval for geometric similarity},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum redundancy pruning for network compression. <em>CVIU</em>, <em>259</em>, 104404. (<a href='https://doi.org/10.1016/j.cviu.2025.104404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Filter pruning has become one of the most powerful methods for model compression in recent years. However, existing pruning methods often rely on predefined layer-wise pruning ratios or computationally expensive search processes, leading to suboptimal architectures and high computational overhead. To address these limitations, we propose a novel pruning method, termed Maximum Redundancy Pruning (MRP), which consists of Redundancy Measurement by Community Detection (RMCD) and Structural Redundancy Pruning (SRP). We first demonstrate a Role-Information (RI) hypothesis based on the link between social networks and convolutional neural networks through empirical study. Based on that, RMCD is proposed to obtain the level of redundancy for each layer, enabling adaptive pruning without predefined layer-wise ratios. In addition, we introduce SRP to obtain a sub-network with the optimal architecture according to the redundancy of each layer obtained by RMCD. Specifically, we recalculate the redundancy of each layer at each iteration and then remove the most replaceable filters in the most redundant layer until a target compression ratio is achieved. This approach automatically determines the optimal layer-wise pruning ratios, avoiding the limitations of uniform pruning or expensive architecture search. We show that our proposed MRP method can reduce the model size for ResNet-110 by up to 52.4% and FLOPs by up to 50.3% on CIFAR-10 while actually improving the original accuracy by 1.04% after retraining the networks.},
  archive      = {J_CVIU},
  author       = {Chang Gao and Jiaqi Wang and Liping Jing},
  doi          = {10.1016/j.cviu.2025.104404},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104404},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Maximum redundancy pruning for network compression},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing vision–language contrastive representation learning using domain knowledge. <em>CVIU</em>, <em>259</em>, 104403. (<a href='https://doi.org/10.1016/j.cviu.2025.104403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual representation learning plays a key role in solving medical computer vision tasks. Recent advances in the literature often rely on vision–language models aiming to learn the representation of medical images from the supervision of paired captions in a label-free manner. The training of such models is however very data/time intensive and the alignment strategies involved in the contrastive loss functions may not capture the full richness of information carried by inter-data relationships. We assume here that considering expert knowledge from the medical domain can provide solutions to these problems during model optimization. To this end, we propose a novel knowledge-augmented vision–language contrastive representation learning framework consisting of the following steps: (1) Modeling the hierarchical relationships between various medical concepts using expert knowledge and medical images in a dataset through a knowledge graph, followed by translating each node into a knowledge embedding; And (2) integrating knowledge embeddings into a vision–language contrastive learning framework, either by introducing an additional alignment loss between visual and knowledge embeddings or by relaxing binary constraints of vision–language alignment using knowledge embeddings. Our results demonstrate that the proposed solution achieves competitive performances against state-of-the-art approaches for downstream tasks while requiring significantly less training data. Our code is available at https://github.com/Wxy-24/KL-CVR .},
  archive      = {J_CVIU},
  author       = {Xiaoyang Wei and Camille Kurtz and Florence Cloppet},
  doi          = {10.1016/j.cviu.2025.104403},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104403},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enhancing vision–language contrastive representation learning using domain knowledge},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous conditional video synthesis by neural processes. <em>CVIU</em>, <em>259</em>, 104387. (<a href='https://doi.org/10.1016/j.cviu.2025.104387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different conditional video synthesis tasks, such as frame interpolation and future frame prediction, are typically addressed individually by task-specific models, despite their shared underlying characteristics. Additionally, most conditional video synthesis models are limited to discrete frame generation at specific integer time steps. This paper presents a unified model that tackles both challenges simultaneously. We demonstrate that conditional video synthesis can be formulated as a neural process, where input spatio-temporal coordinates are mapped to target pixel values by conditioning on context spatio-temporal coordinates and pixel values. Our approach leverages a Transformer-based non-autoregressive conditional video synthesis model that takes the implicit neural representation of coordinates and context pixel features as input. Our task-specific models outperform previous methods for future frame prediction and frame interpolation across multiple datasets. Importantly, our model enables temporal continuous video synthesis at arbitrary high frame rates, outperforming the previous state-of-the-art. The source code and video demos for our model are available at https://npvp.github.io .},
  archive      = {J_CVIU},
  author       = {Xi Ye and Guillaume-Alexandre Bilodeau},
  doi          = {10.1016/j.cviu.2025.104387},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104387},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Continuous conditional video synthesis by neural processes},
  volume       = {259},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting few-shot point cloud segmentation with intra-class correlation and iterative prototype fusion. <em>CVIU</em>, <em>258</em>, 104393. (<a href='https://doi.org/10.1016/j.cviu.2025.104393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of 3D point clouds is often limited by the challenge of obtaining labeled data. Few-shot point cloud segmentation methods, which can learn previously unseen categories, help reduce reliance on labeled datasets. However, existing methods are susceptible to correlation noise and suffer from significant discrepancies between support prototypes and query features. To address these issues, we first introduce an intra-class correlation enhancement module for filtering correlation noise driven by inter-class similarity and intra-class diversity. Second, to better represent the target classes, we propose an iterative prototype fusion module that adapts the query point cloud feature space, mitigating the problem of object variations in the support set and query set. Extensive experiments on S3DIS and ScanNet benchmark datasets demonstrate that our approach achieves competitive performance with state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Xindan Zhang and Ying Li and Xinnian Zhang},
  doi          = {10.1016/j.cviu.2025.104393},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104393},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Boosting few-shot point cloud segmentation with intra-class correlation and iterative prototype fusion},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HVQ-VAE: Variational auto-encoder with hyperbolic vector quantization. <em>CVIU</em>, <em>258</em>, 104392. (<a href='https://doi.org/10.1016/j.cviu.2025.104392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector quantized-variational autoencoder (VQ-VAE) and its variants have made significant progress in creating discrete latent space via learning a codebook. Previous works on VQ-VAE have focused on discrete latent spaces in Euclidean or in spherical spaces. This paper studies the geometric prior of hyperbolic spaces as a way to improve the learning capacity of VQ-VAE. That being said, working with the VQ-VAE in the hyperbolic space is not without difficulties, and the benefits of using hyperbolic space as the geometric prior for the latent space have never been studied in VQ-VAE. We bridge this gap by developing the VQ-VAE with hyperbolic vector quantization. To this end, we propose the hyperbolic VQ-VAE (HVQ-VAE), which learns the latent embedding of data and the codebook in the hyperbolic space. Specifically, we endow the discrete latent space in the Poincaré ball, such that the clustering algorithm can be formulated and optimized in the Poincaré ball. Thorough experiments against various baselines are conducted to evaluate the superiority of the proposed HVQ-VAE empirically. We show that HVQ-VAE enjoys better image reconstruction, effective codebook usage, and fast convergence than baselines. We also present evidence that HVQ-VAE outperforms VQ-VAE in low-dimensional latent space.},
  archive      = {J_CVIU},
  author       = {Shangyu Chen and Pengfei Fang and Mehrtash Harandi and Trung Le and Jianfei Cai and Dinh Phung},
  doi          = {10.1016/j.cviu.2025.104392},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104392},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {HVQ-VAE: Variational auto-encoder with hyperbolic vector quantization},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TEMSA:Text enhanced modal representation learning for multimodal sentiment analysis. <em>CVIU</em>, <em>258</em>, 104391. (<a href='https://doi.org/10.1016/j.cviu.2025.104391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis aims to identify human emotions by leveraging multimodal information, including language, visual, and audio data. Most existing models focus on extracting common features across modalities or simply integrating heterogeneous multimodal data. However, such approaches often overlook the unique representation advantages of individual modalities, as they treat all modalities equally and use bidirectional information transfer mechanisms. This can lead to information redundancy and feature conflicts. To address this challenge, we propose a Text-Enhanced Modal Representation Learning Model (TEMSA), which builds robust and unified multimodal representations through the design of text-guided pairwise cross-modal mapping modules. Specifically, TEMSA employs a text-guided multi-head cross-attention mechanism to embed linguistic information into the emotion-related representation learning of non-linguistic modalities, thereby enhancing the representations of visual and audio modalities. In addition to preserving consistent information through cross-modal mapping, TEMSA also incorporates text-guided reconstruction modules, which leverage text-enhanced non-linguistic modal features to decouple modality-specific representations from non-linguistic modalities. This dual representation learning framework captures inter-modal consistent information through cross-modal mapping, and extracts modal difference information through intra-modal decoupling, thus improving the understanding of cross-modal affective associations. The experimental results on the CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets demonstrate that TEMSA achieves superior performance, highlighting the critical role of text-guided cross-modal and intra-modal representation learning in multimodal sentiment analysis.},
  archive      = {J_CVIU},
  author       = {Jingwen Chen and Shuxiang Song and Yumei Tan and Haiying Xia},
  doi          = {10.1016/j.cviu.2025.104391},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104391},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {TEMSA:Text enhanced modal representation learning for multimodal sentiment analysis},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient human–object-interaction (EHOI) detection via interaction label coding and conditional decision. <em>CVIU</em>, <em>258</em>, 104390. (<a href='https://doi.org/10.1016/j.cviu.2025.104390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–Object Interaction (HOI) detection is a fundamental task in image understanding. While deep-learning-based HOI methods provide high performance in terms of mean Average Precision (mAP), they are computationally expensive and opaque in training and inference processes. An Efficient HOI (EHOI) detector is proposed in this work to strike a good balance between detection performance, inference complexity, and mathematical transparency. EHOI is a two-stage method. In the first stage, it leverages a frozen object detector to localize the objects and extract various features as intermediate outputs. In the second stage, the first-stage outputs predict the interaction type using the XGBoost classifier. Our contributions include the application of error correction codes (ECCs) to encode rare interaction cases, which reduces the model size and the complexity of the XGBoost classifier in the second stage. Additionally, we provide a mathematical formulation of the relabeling and decision-making process. Apart from the architecture, we present qualitative results to explain the functionalities of the feedforward modules. Experimental results demonstrate the advantages of ECC-coded interaction labels and the excellent balance of detection performance and complexity of the proposed EHOI method. The codes are available: https://github.com/keevin60907/EHOI---Efficient-Human-Object-Interaction-Detector .},
  archive      = {J_CVIU},
  author       = {Tsung-Shan Yang and Yun-Cheng Wang and Chengwei Wei and Suya You and C.-C. Jay Kuo},
  doi          = {10.1016/j.cviu.2025.104390},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104390},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Efficient human–object-interaction (EHOI) detection via interaction label coding and conditional decision},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale feature fusion based SAM for high-quality few-shot medical image segmentation. <em>CVIU</em>, <em>258</em>, 104389. (<a href='https://doi.org/10.1016/j.cviu.2025.104389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying the Segmentation Everything Model (SAM) to the field of medical image segmentation is a great challenge due to the significant differences between natural and medical images. Direct fine-tuning of SAM using medical images requires a large amount of exhaustively annotated medical image data. This paper aims to propose a new method, High-quality Few-shot Segmentation Everything Model (HF-SAM), to address these issues and achieve efficient medical image segmentation. We proposed HF-SAM, which requires only a small number of medical images for model training and does not need precise medical cues for fine-tuning SAM. HF-SAM employs Low-rank adaptive (LoRA) technology to fine-tune SAM by leveraging the lack of large local details in the image embedding of SAM’s mask decoder and the complementarity between high-level global and low-level local features. Additionally, we propose an Adaptive Weighted Feature Fusion Module (AWFFM) and a two-step skip-feature fusion decoding process. The AWFFM integrates low-level local information into high-level global features without suppressing global information, while the two-step skip-feature fusion decoding process enhances SAM’s ability to capture fine-grained information and local details. Experimental results show that HF-SAM achieves Dice scores of 79.50% on the Synapse dataset and 88.68% on the ACDC dataset. These results outperform existing traditional methods, semi-supervised methods, and other SAM variants in few-shot medical image segmentation. By combining low-rank adaptive technology and the adaptive weighted feature fusion module, HF-SAM effectively addresses the adaptability issues of SAM in medical image segmentation and demonstrates excellent segmentation performance with few samples. This method provides a new solution for the field of medical image segmentation and holds significant application value. The code of HF-SAM is available at https://github.com/1683194873xrn/HF-SAM .},
  archive      = {J_CVIU},
  author       = {Shangwang Liu and Ruonan Xu},
  doi          = {10.1016/j.cviu.2025.104389},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104389},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-scale feature fusion based SAM for high-quality few-shot medical image segmentation},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EUN: Enhanced unlearnable examples generation approach for privacy protection. <em>CVIU</em>, <em>258</em>, 104388. (<a href='https://doi.org/10.1016/j.cviu.2025.104388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of artificial intelligence, the importance of protecting user privacy has become increasingly prominent. Unlearnable examples prevent deep learning models from learning semantic features in images by adding perturbations or noise that are imperceptible to the human eye. Existing perturbation generation methods are not robust to defense methods or are only robust to one defense method. To address this problem, we propose an enhanced perturbation generation method for unlearnable examples. This method generates the perturbation by performing a class-wise convolution on the image and changing a pixel in the local position of the image. This method is robust to multiple defense methods. In addition, by adjusting the order of global position convolution and local position pixel change of the image, variants of the method were generated and analyzed. We have tested our method on a variety of datasets with a variety of models, and compared with 6 perturbation generation methods. The results demonstrate that the clean test accuracy of the enhanced perturbation generation method for unlearnable examples is still less than 35% when facing defense methods such as image shortcut squeezing, adversarial training, and adversarial augmentation. It outperforms existing perturbation generation methods in many aspects, and is 20% lower than CUDA and OPS, two excellent perturbation generation methods, under several parameter settings.},
  archive      = {J_CVIU},
  author       = {Xiaotian Chen and Yang Xu and Sicong Zhang and Jiale Yan and Weida Xu and Xinlong He},
  doi          = {10.1016/j.cviu.2025.104388},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104388},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {EUN: Enhanced unlearnable examples generation approach for privacy protection},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AODGCN: Adaptive object detection with attention-guided dynamic graph convolutional network. <em>CVIU</em>, <em>258</em>, 104386. (<a href='https://doi.org/10.1016/j.cviu.2025.104386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various classifiers based on convolutional neural networks have been successfully applied to image classification in object detection. However, object detection is much more sophisticated and most classifiers used in this context exhibit limitations in capturing contextual information, particularly in scenarios with complex backgrounds or occlusions. Additionally, they lack spatial awareness, resulting in the loss of spatial structure and inadequate modeling of object details and context. In this paper, we propose an adaptive object detection approach using an attention-guided dynamic graph convolutional network (AODGCN). AODGCN represents images as graphs, enabling the capture of object properties such as connectivity, proximity, and hierarchical relationships. Attention mechanisms guide the model to focus on informative regions, highlighting relevant features while suppressing background information. This attention-guided approach enhances the model’s ability to capture discriminative features. Furthermore, the dynamic graph convolutional network (D-GCN) adjusts the receptive field size and weight coefficients based on object characteristics, enabling adaptive detection of objects with varying sizes. The achieved results demonstrate the effectiveness of AODGCN on the MS-COCO 2017 dataset, with a significant improvement of 1.6% in terms of mean average precision (mAP) compared to state-of-the-art algorithms.},
  archive      = {J_CVIU},
  author       = {Meng Zhang and Yina Guo and Haidong Wang and Hong Shangguan},
  doi          = {10.1016/j.cviu.2025.104386},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104386},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {AODGCN: Adaptive object detection with attention-guided dynamic graph convolutional network},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equipping sketch patches with context-aware positional encoding for graphic sketch representation. <em>CVIU</em>, <em>258</em>, 104385. (<a href='https://doi.org/10.1016/j.cviu.2025.104385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When benefiting graphic sketch representation with sketch drawing orders, recent studies have linked sketch patches as graph edges by drawing orders in accordance to a temporal-based nearest neighboring strategy. However, such constructed graph edges may be unreliable, since the contextual relationships between patches may be inconsistent with the sequential positions in drawing orders, due to variants of sketch drawings. In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for sketch learning. We introduce a sinusoidal absolute PE to embed the sequential positions in drawing orders, and a learnable relative PE to encode the unseen contextual relationships between patches. Both types of PEs never attend the construction of graph edges, but are injected into graph nodes to cooperate with the visual patterns captured from patches. After linking nodes by semantic proximity, during message aggregation via graph convolutional networks, each node receives both semantic features from patches and contextual information from PEs from its neighbors, which equips local patch patterns with global contextual information, further obtaining drawing-order-enhanced sketch representations. Experimental results indicate that our method significantly improves sketch healing and controllable sketch synthesis.},
  archive      = {J_CVIU},
  author       = {Sicong Zang and Zhijun Fang},
  doi          = {10.1016/j.cviu.2025.104385},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104385},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Equipping sketch patches with context-aware positional encoding for graphic sketch representation},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uniss-MDF: A multidimensional face dataset for assessing face analysis on the move. <em>CVIU</em>, <em>258</em>, 104384. (<a href='https://doi.org/10.1016/j.cviu.2025.104384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multidimensional 2D–3D face analysis has demonstrated a strong potential for human identification in several application domains. The combined, synergic use of 2D and 3D data from human faces can counteract typical limitations in 2D face recognition, while improving both accuracy and robustness in identification. On the other hand, current mobile devices, often equipped with depth cameras and high performance computing resources, offer a powerful and practical tool to better investigate new models to jointly process real 2D and 3D face data. However, recent concerns related to privacy of individuals and the collection, storage and processing of personally identifiable biometric information have diminished the availability of public face recognition datasets. Uniss-MDF (Uniss-MultiDimensional Face) represents the first collection of combined 2D–3D data of human faces captured with a mobile device. Over 76,000 depth images and videos are captured from over 100 subjects, in both controlled and uncontrolled conditions, over two sessions. The features of Uniss-MDF are extensively compared with existing 2D–3D face datasets. The reported statistics underscore the value of the dataset as a versatile resource for researchers in face recognition on the move and for a wide range of applications. Notably, it is the sole 2D–3D facial dataset using data from a mobile device that includes both 2D and 3D synchronized sequences acquired in controlled and uncontrolled conditions. The Uniss-MDF dataset and the proposed experimental protocols with baseline results provide a new platform to compare processing models for novel research avenues in advanced face analysis on the move.},
  archive      = {J_CVIU},
  author       = {Pietro Ruiu and Marinella Iole Cadoni and Andrea Lagorio and Seth Nixon and Filippo Casu and Massimo Farina and Mauro Fadda and Giuseppe A. Trunfio and Massimo Tistarelli and Enrico Grosso},
  doi          = {10.1016/j.cviu.2025.104384},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104384},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Uniss-MDF: A multidimensional face dataset for assessing face analysis on the move},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grad-CAM: The impact of large receptive fields and other caveats. <em>CVIU</em>, <em>258</em>, 104383. (<a href='https://doi.org/10.1016/j.cviu.2025.104383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in complexity of deep learning models demands explanations that can be obtained with methods like Grad-CAM. This method computes an importance map for the last convolutional layer relative to a specific class, which is then upsampled to match the size of the input. However, this final step assumes that there is a spatial correspondence between the last feature map and the input, which may not be the case. We hypothesize that, for models with large receptive fields, the feature spatial organization is not kept during the forward pass, which may render the explanations devoid of meaning. To test this hypothesis, common architectures were applied to a medical scenario on the public VinDr-CXR dataset, to a subset of ImageNet and to datasets derived from MNIST. The results show a significant dispersion of the spatial information, which goes against the assumption of Grad-CAM, and that explainability maps are affected by this dispersion. Furthermore, we discuss several other caveats regarding Grad-CAM, such as feature map rectification, empty maps and the impact of global average pooling or flatten layers. Altogether, this work addresses some key limitations of Grad-CAM which may go unnoticed for common users, taking one step further in the pursuit for more reliable explainability methods.},
  archive      = {J_CVIU},
  author       = {Rui Santos and João Pedrosa and Ana Maria Mendonça and Aurélio Campilho},
  doi          = {10.1016/j.cviu.2025.104383},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104383},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Grad-CAM: The impact of large receptive fields and other caveats},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSPPNet: Cascade space pyramid pooling network for object detection. <em>CVIU</em>, <em>258</em>, 104377. (<a href='https://doi.org/10.1016/j.cviu.2025.104377'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time object detection, as an important research direction in the field of computer vision, aims to achieve fast and accurate object detection. However, many current methods fail to achieve a balance between speed, parameters, and accuracy. To alleviate this problem, in this paper, we construct a novel cascade spatial pyramid pooling network (CSPPNet) for object detection. In particular, we first propose a cascade feature fusion (CFF) module, which combines the novel cascade cross-layer structure and GSConv convolution to lighten the existing necking structure and improve the detection accuracy of the model without adding a large number of parameters. In addition, in order to alleviate the loss of feature detail information due to max pooling, we further propose the nest space pooling (NSP) module, which combines nest feature fusion with max pooling operations to improve the fusion performance of local feature information with global feature information. Experimental results show that our CSPPNet is competitive, achieving 43.1% AP on the MS-COCO 2017 test-dev dataset.},
  archive      = {J_CVIU},
  author       = {Yafeng Liu and Yongsheng Dong},
  doi          = {10.1016/j.cviu.2025.104377},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104377},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CSPPNet: Cascade space pyramid pooling network for object detection},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute-guided relevance propagation for interpreting image classifier based on deep neural networks. <em>CVIU</em>, <em>258</em>, 104370. (<a href='https://doi.org/10.1016/j.cviu.2025.104370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have emerged as powerful tools for addressing complex and varied problems, achieving remarkable success across numerous AI domains. Despite their effectiveness, the inherent complexity of deep learning models makes them considered black boxes, reducing their interpretability and reliability. To address this challenge, we propose a novel approach called Attribute-guided Relevance Propagation (ARP). ARP enhances the interpretability of deep learning models by learning attributes from specific layers within a pre-trained image classifier and integrating these attributes into saliency maps. This integration not only improves the saliency maps but also identifies and provides example images related to key regions reflected in the maps. We validate the efficacy of ARP through both quantitative and qualitative evaluations, employing widely recognized image classifiers such as ResNet-50 and ViT trained on the benchmark datasets.},
  archive      = {J_CVIU},
  author       = {Seung-Ho Han and Ho-Jin Choi},
  doi          = {10.1016/j.cviu.2025.104370},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104370},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Attribute-guided relevance propagation for interpreting image classifier based on deep neural networks},
  volume       = {258},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral image classification using hybrid convolutional-based cross-patch retentive network. <em>CVIU</em>, <em>257</em>, 104382. (<a href='https://doi.org/10.1016/j.cviu.2025.104382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformer (ViT) is a widely used method to capture long-distance dependencies and has demonstrated remarkable results in classifying hyperspectral images (HSIs). Nevertheless, the fundamental component of ViT, self-attention, has difficulty striking a balance between global modeling and high computational complexity across entire input sequences. Recently, the Retentive Network (RetNet) was developed to address this issue, claiming to be more scalable and efficient than standard transformers. However, RetNet struggles to capture local features such as traditional transformers. This paper proposes a RetNet-based novel hybrid convolutional-based cross-patch retentive network (HCCRN). The proposed HCCRN model comprises a hybrid convolutional-based feature extraction (HCFE) module, a weighted feature tokenization module, and a cross-patch retentive network (CRN) module. The HCFE architecture combines four 2D convolutional layers and residual connections with a 3D convolutional layer to extract high-level fused spatial–spectral information and capture low-level spectral features. This hybrid method solves the vanishing gradient issue and comprehensively represents intricate spatial–spectral interactions by enabling hierarchical learning of spectral context and spatial dependencies. To further maximize processing efficiency, the acquired spatial–spectral data are transformed into semantic tokens by the tokenization module, which feeds them into the CRN module. CRN enriches feature representations and increases accuracy by utilizing a multi-head cross-patch retention mechanism to capture numerous semantic relations between input tokens. Extensive experiments on three benchmark datasets have shown that the proposed HCCRN architecture significantly outperforms state-of-the-art methods. It reduces computation time and increases classification accuracy, demonstrating its generalizability and robustness in the HSIC task.},
  archive      = {J_CVIU},
  author       = {Rajat Kumar Arya and Rohith Peddi and Rajeev Srivastava},
  doi          = {10.1016/j.cviu.2025.104382},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104382},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Hyperspectral image classification using hybrid convolutional-based cross-patch retentive network},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STELA: Spatial–temporal enhanced learning with an anatomical graph transformer for 3D human pose estimation. <em>CVIU</em>, <em>257</em>, 104381. (<a href='https://doi.org/10.1016/j.cviu.2025.104381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have led to remarkable performance improvements in 3D human pose estimation by capturing global dependencies between joints in spatial and temporal aspects. To leverage human body topology information, attempts have been made to incorporate graph representation within a transformer architecture. However, they neglect spatial–temporal anatomical knowledge inherent in the human body, without considering the implicit relationships of non-connected joints. Furthermore, they disregard the movement patterns between joint trajectories, concentrating on the trajectories of individual joints. In this paper, we propose Spatial–Temporal Enhanced Learning with an Anatomical graph transformer (STELA) to aggregate the spatial–temporal global relationships and intricate anatomical relationships between joints. It consists of Global Self-attention (GS) and Anatomical Graph-attention (AG) branches. GS learns long-range dependencies between all joints across entire frames. AG focuses on the anatomical relationships of the human body in the spatial–temporal aspect using skeleton and motion pattern graphs. Extensive experiments demonstrate that STELA outperforms state-of-the-art approaches with an average of 41% fewer parameters, reducing MPJPE by an average of 2.7 mm on Human3.6M and 1.5 mm on MPI-INF-3DHP.},
  archive      = {J_CVIU},
  author       = {Jian Son and Jiho Lee and Eunwoo Kim},
  doi          = {10.1016/j.cviu.2025.104381},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104381},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {STELA: Spatial–temporal enhanced learning with an anatomical graph transformer for 3D human pose estimation},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater image quality evaluation via deep meta-learning: Dataset and objective method. <em>CVIU</em>, <em>257</em>, 104380. (<a href='https://doi.org/10.1016/j.cviu.2025.104380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The degradation of underwater image quality due to complex environments affects the effectiveness of the application, making accurate quality assessment crucial. However, existing Underwater Image Quality Assessment (UIQA) methods lack sufficient reliable data. To address this, we construct the DART2024 dataset, containing 1,000 raw images and 10,000 distorted images generated by 10 enhancement methods, covering diverse underwater scenarios. We propose a novel UIQA method that weights original images via gradient maps, highlights details, constructs a multi-scale deep neural network with perception, fusion, and prediction modules to describe quality characteristics, and designs a meta-learning framework for rapid adaptation to unknown distortions. The experimental results show that DART2024 is credible and meets the training requirements. Our method outperforms SOTA approaches in accuracy, stability, and convergence speed on DART2024 and other underwater datasets. It also shows higher applicability on natural scene datasets. The dataset and source code for the proposed method can be made available at https://github.com/dart-into/DART2024 .},
  archive      = {J_CVIU},
  author       = {Tianhai Chen and Xichen Yang and Tianshu Wang and Nengxin Li and Shun Zhu and Xiaobo Shen},
  doi          = {10.1016/j.cviu.2025.104380},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104380},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Underwater image quality evaluation via deep meta-learning: Dataset and objective method},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly-aware self-supervised feature learning for weakly supervised video anomaly detection. <em>CVIU</em>, <em>257</em>, 104379. (<a href='https://doi.org/10.1016/j.cviu.2025.104379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised video anomaly detection (WSVAD) aims to achieve frame-level anomaly detection utilizing video-level labeled training data. Most of the WSVAD methods typically focus on developing various anomaly detectors and employ the models pretrained on general large-scale databases ( e.g. Kinetics-400) for video feature extraction. However, such features are pretrained in a supervised manner to recognize significantly distinct human actions, making them sub-optimal for the WSVAD task, which involves detecting more complex events including subtle human and non-human motions. Unlike previous efforts, in this work, we address WSVAD from another fundamental perspective of feature learning, and considering the anomaly attributes, we propose an anomaly-aware self-supervised learning (SSL) based method. Specifically, we design a series of pretext tasks, including temporal order verification, speed prediction, arrow of time prediction, and abrupt change detection, to directly pretrain the model on the anomaly datasets, thus rendering features tailored for the WSVAD task. Moreover, to deal with the limitation of the existing methods which adapt the pretrained features to anomaly detection by utilizing top- k anomaly-scored samples, we present a hard instance mining strategy (HIMS) to additionally explore valuable cues from unused non-top- k ones, enhancing the discriminability between normal and abnormal instances. Experimental results clearly demonstrate that our method outperforms state-of-the-art counterparts on the UCF-Crime, ShanghaiTech and XD-Violence benchmark datasets, highlighting its effectiveness.},
  archive      = {J_CVIU},
  author       = {Zhen Yang and Guodong Wang and Yuanfang Guo and Xiuguo Bao and Di Huang},
  doi          = {10.1016/j.cviu.2025.104379},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104379},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Anomaly-aware self-supervised feature learning for weakly supervised video anomaly detection},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guided progressive learning for room layout estimation: From pixel-level embeddings to refined depth maps. <em>CVIU</em>, <em>257</em>, 104378. (<a href='https://doi.org/10.1016/j.cviu.2025.104378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The room layout captures the global structure of an indoor scene. Estimating the 3D layout of a room from an RGB image requires recovering the spatial information of the dominant indoor planes. However, directly learning the plane parameters or layout depth map is inherently ill-posed and prone to errors. In this paper, we introduce a guided progressive learning framework that systematically builds layout understanding through progressively complex representations. Our approach first learns pixel-level plane embeddings to capture the positional information of the dominant planes within the room. Building on these embeddings, we then learn the planar depth parameters for each pixel. Using these parameters, we generate a coarse layout depth map through pixel-wise calculations. To enhance the accuracy of this depth map, we employ a refinement module that not only produces a refined layout depth map but also establishes geometric connections between the pixel-level outputs. During inference, we select the optimal layout hypothesis based on the predicted plane embeddings, planar depth parameters, and layout depth map. The proposed guided progressive learning framework achieves state-of-the-art performance on both 3D and 2D layout estimation metrics, demonstrating its robustness and precision.},
  archive      = {J_CVIU},
  author       = {Weidong Zhang and Kang Cheng and Ying Liu and Yu Hao},
  doi          = {10.1016/j.cviu.2025.104378},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104378},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Guided progressive learning for room layout estimation: From pixel-level embeddings to refined depth maps},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classroom teacher behavior analysis: The TBU dataset and performance evaluation. <em>CVIU</em>, <em>257</em>, 104376. (<a href='https://doi.org/10.1016/j.cviu.2025.104376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classroom videos are objective records of teaching behaviors, which provide evidence for teachers’ teaching reflection and evaluation. The intelligent identification, tracking and description of teacher teaching behavior based on classroom videos have become a research hotspot in the field of intelligent education to understand the teaching process of teachers. Although the recent attempts propose several promising directions for the analysis of teaching behavior, the existing public datasets are still insufficient to meet the need for these potential solutions due to lack of varied classroom environment, fine-grained teaching scene behavior data. To address this, we analyzed the influencing factors of teacher behavior and related video datasets, and constructed a diverse, scenario-specific, and multi-task dataset named TBU for Teacher Behavior Understanding. The TBU contains 37,026 high-quality teaching behavior clips, 9422 annotated teaching behavior clips with precise time boundaries, and 6098 teacher teaching behavior description clips annotated with multi-level atomic action labels of fine-grained behavior, spatial location, and interactive objects in four education stages. We performed a comprehensive statistical analysis of TBU and summarized the behavioral characteristics of teachers at different educational stages. Additionally, we systematically investigated representative methods for three video understanding tasks on TBU: behavior recognition, behavior detection, and behavior description, providing a benchmark for the research towards a more comprehensive understanding of teaching video data. Considering the specificity of classroom scenarios and the needs of teaching behavior analysis, we put forward new requirements for the existing baseline methods. We believe that TBU can facilitate in-depth research on classroom teacher teaching video analysis. TBU is available at: https://github.com/cai-KU/TBU .},
  archive      = {J_CVIU},
  author       = {Ting Cai and Yu Xiong and Chengyang He and Chao Wu and Linqin Cai},
  doi          = {10.1016/j.cviu.2025.104376},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104376},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Classroom teacher behavior analysis: The TBU dataset and performance evaluation},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional neural network framework for deepfake detection: A diffusion-based approach. <em>CVIU</em>, <em>257</em>, 104375. (<a href='https://doi.org/10.1016/j.cviu.2025.104375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly advancing domain of synthetic media, DeepFakes emerged as a potent tool for misinformation and manipulation. Nevertheless, the engineering challenge lies in detecting such content to ensure information integrity. Recent artificial intelligence contributions in deepfake detection have mainly concentrated around sophisticated convolutional neural network models, which derive insights from facial biometrics, including multi-attentional and multi-view mechanisms, pairwise/siamese, distillation learning technique and facial-geometry approaches. In this work, we consider a new diffusion-based neural network approach, rather than directly analyzing deepfake images for inconsistencies. Motivated by the considerable property of diffusion procedure of unveiling anomalies, we employ diffusion of the inherent structure of deepfake images, seeking for patterns throughout this process. Specifically, the proposed diffusion network, iteratively adds noise to the input image until it almost becomes pure noise. Subsequently, a convolutional neural network extracts features from the final diffused state, as well as from all transient states of the diffusion process. The comprehensive experimental analysis demonstrates the efficacy and adaptability of the proposed model, validating its robustness against a wide range of deepfake detection models, being a promising artificial intelligence tool for DeepFake detection.},
  archive      = {J_CVIU},
  author       = {Emmanuel Pintelas and Ioannis E. Livieris},
  doi          = {10.1016/j.cviu.2025.104375},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104375},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Convolutional neural network framework for deepfake detection: A diffusion-based approach},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Translation-classification loss for SAR image understanding with deep learning. <em>CVIU</em>, <em>257</em>, 104374. (<a href='https://doi.org/10.1016/j.cviu.2025.104374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SAR-to-optical translator networks are especially used to overcome the lack of optical images under cloudy conditions. Those translations being used for downstream tasks, they require the reconstruction of reliable patterns with respect to the underlying objects. In this paper, we propose a novel training strategy to account for land-cover complexity through a conjoint Translation-Classification Loss (TCL). The proposed loss evaluates the classifiability of translated images with a pre-trained land-cover classifier by assessing the reliability of its predictions and the relevance of its extracted hidden features. This new loss is applied to nine translators from the literature and to a tenth architecture introduced in the paper. Experiments show that applying the TCL not only improves the credibility of structures, patterns and textures but it also allows for better class discrimination and transitions while avoiding unreliable hallucinated artifacts produced by standard losses in adversarial approaches.},
  archive      = {J_CVIU},
  author       = {Antoine Bralet and Abdourrahmane M. Atto and Jocelyn Chanussot and Emmanuel Trouvé},
  doi          = {10.1016/j.cviu.2025.104374},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104374},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Translation-classification loss for SAR image understanding with deep learning},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RAFNet: Rotation-aware anchor-free framework for geospatial object detection. <em>CVIU</em>, <em>257</em>, 104373. (<a href='https://doi.org/10.1016/j.cviu.2025.104373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in remote sensing images plays a crucial role in applications such as disaster monitoring, and urban planning. However, detecting small and rotated objects in complex backgrounds remains a significant challenge. Traditional anchor-based methods, which rely on preset anchor boxes with fixed sizes and aspect ratios, face three core limitations: geometric mismatch (difficulty adapting to rotated objects and feature confusion caused by dense anchor boxes), missed detection of small objects (feature loss due to the decoupling between anchor boxes and feature map strides), and parameter sensitivity (requiring complex anchor box combinations for multi-scale targets). To address these challenges, this paper proposes an anchor-free detection framework, RAFNet, integrating three key innovations: Mona Swin Transformer as the backbone to enhance feature extraction, Rotated Feature Pyramid Network (Rotated FPN) for rotation-aware feature representation, and Local Importance-based Attention (LIA) mechanism to focus on critical regions and improve object feature representation. Extensive experiments on the DOTA1.0 dataset demonstrate that RAFNet achieves a mean Average Precision (mAP) of 74.91, outperforming baseline models by 3.24%, with significant improvements in challenging categories such as helicopters (+32.5% AP) and roundabouts (+4% AP). The model achieves the mAP of 30.29% on the STAR dataset, validating its high adaptability and robustness in generalization tasks. These results highlight the effectiveness of the proposed method in detecting small, rotated objects in complex scenes. RAFNet offers a more flexible, efficient, and generalizable solution for remote sensing object detection, underscoring the great potential of anchor-free approaches in this field.},
  archive      = {J_CVIU},
  author       = {Liwei Deng and Yangyang Tan and Songyu Chen},
  doi          = {10.1016/j.cviu.2025.104373},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104373},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {RAFNet: Rotation-aware anchor-free framework for geospatial object detection},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local gaussian ensemble for arbitrary-scale image super-resolution. <em>CVIU</em>, <em>257</em>, 104372. (<a href='https://doi.org/10.1016/j.cviu.2025.104372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In arbitrary-scale image super-resolution (SR), the local coordinate information is pivotal to enhancing performance through local ensemble. The previous method local implicit image function (LIIF) reconstructs pixels by using multi-layer perceptron (MLP), then refines each pixel by a weighted summation of nearby pixels (also called local ensemble), where the weight depends on the distances between the query pixel and the nearby pixels. Since the distances are fixed, so is the weighting mechanism, limiting the effectiveness of local ensemble. Furthermore, the weighted summation involves repeated reconstructions, increasing the computational cost. Orthogonal position encoding SR (OPE-SR) reduces pixel reconstruction complexity using orthogonal position encoding. However, it still relies on LIIF’s local ensemble method. Additionally, lacking scale information, OPE-SR demonstrates unstable performance across various datasets and scale factors. In this paper, we propose to conduct local ensemble in feature domain, and we present a new ensemble method, the local Gaussian ensemble (LGE), to utilize the local coordinate information more flexibly and efficiently. Specifically, we introduce learnable anisotropic 2D Gaussians for each query coordinate in the SR image, transforming normalized coordinates of nearby features into multiple Gaussian weights to effectively ensemble local features. Then a scale-aware deep MLP is applied only once for pixel reconstruction. Extensive experiments demonstrate that our LGE significantly reduces computational costs during both training and inference while delivering performance comparable to the existing local ensemble method. Moreover, our method consistently outperforms the existing parameter-free approach in terms of efficiency and stability across various benchmark datasets and scale factors.},
  archive      = {J_CVIU},
  author       = {Chuan Chen and Weiwei Wang and Xixi Jia and Xiangchu Feng and Hanjia Wei},
  doi          = {10.1016/j.cviu.2025.104372},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104372},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Local gaussian ensemble for arbitrary-scale image super-resolution},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Egocentric and exocentric methods: A short survey. <em>CVIU</em>, <em>257</em>, 104371. (<a href='https://doi.org/10.1016/j.cviu.2025.104371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Egocentric vision captures the scene from the point of view of the camera wearer while exocentric vision captures the overall scene context. Jointly modeling ego and exo views is crucial to developing next-generation AI agents. The community has regained interest in the field of egocentric vision. While the third-person view and first-person have been thoroughly investigated, very few works aim to study both synchronously. Exocentric videos contain many relevant signals that are transferrable to egocentric videos. This paper provides a timely overview of works combining egocentric and exocentric visions, a very new but promising research topic. We describe in detail the datasets and present a survey of the key applications of ego-exo joint learning, where we identify the most recent advances. With the presentation of the current status of the progress, we believe this short but timely survey will be valuable to the broad video-understanding community, particularly when multi-view modeling is critical.},
  archive      = {J_CVIU},
  author       = {Anirudh Thatipelli and Shao-Yuan Lo and Amit K. Roy-Chowdhury},
  doi          = {10.1016/j.cviu.2025.104371},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104371},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Egocentric and exocentric methods: A short survey},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A vector quantized masked autoencoder for audiovisual speech emotion recognition. <em>CVIU</em>, <em>257</em>, 104362. (<a href='https://doi.org/10.1016/j.cviu.2025.104362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder–decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.},
  archive      = {J_CVIU},
  author       = {Samir Sadok and Simon Leglaive and Renaud Séguier},
  doi          = {10.1016/j.cviu.2025.104362},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104362},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A vector quantized masked autoencoder for audiovisual speech emotion recognition},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning temporal-aware representation for controllable interventional radiology imaging. <em>CVIU</em>, <em>257</em>, 104360. (<a href='https://doi.org/10.1016/j.cviu.2025.104360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interventional Radiology Imaging (IRI) is essential for evaluating cerebral vascular anatomy by providing sequential images of both arterial and venous blood flow. In IRI, the low frame rate (4 fps) during acquisition can lead to discontinuities and flickering, whereas higher frame rates are associated with increased radiation exposure. Nevertheless, under complex blood flow conditions, it becomes necessary to increase the frame rate to 15 fps for the second sampling. Previous methods relied solely on fixed frame interpolation to mitigate discontinuities and flicker. However, owing to frame rate constraints, they were ineffective in addressing the high radiation issues arising from complex blood flow conditions. In this study, we introduce a novel approach called Temporally Controllable Network (TCNet), which innovatively applies controllable frame interpolation techniques to IRI for the first time. Our method effectively tackles the issues of discontinuity and flickering arising from low frame rates and mitigates the radiation concerns linked to higher frame rates during second sampling. Our method emphasizes synthesizing intermediate frame features via a Temporal-Aware Representation Learning (TARL) module and optimizes this process through bilateral optical flow supervision for accurate optical flow estimation. Additionally, to enhance the depiction of blood vessel motion and breathing nuances, we introduce an implicit function module for refining motion cues in videos. Our experiments reveal that TCNet successfully generate videos at clinically appropriate frame rates, significantly improving the reconstruction of blood flow and respiratory patterns. We will publicly release our code and datasets.},
  archive      = {J_CVIU},
  author       = {Wei Si and Zhaolin Zheng and Zhewei Huang and Xi-Ming Xu and Ruijue Wang and Ji-Gang Bao and Qiang Xiong and Xiantong Zhen and Jun Xu},
  doi          = {10.1016/j.cviu.2025.104360},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104360},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning temporal-aware representation for controllable interventional radiology imaging},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extensions in channel and class dimensions for attention-based knowledge distillation. <em>CVIU</em>, <em>257</em>, 104359. (<a href='https://doi.org/10.1016/j.cviu.2025.104359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As knowledge distillation technology evolves, it has bifurcated into three distinct methodologies: logic-based, feature-based, and attention-based knowledge distillation. Although the principle of attention-based knowledge distillation is more intuitive, its performance lags behind the other two methods. To address this, we systematically analyze the advantages and limitations of traditional attention-based methods. In order to optimize these limitations and explore more effective attention information, we expand attention-based knowledge distillation in the channel and class dimensions, proposing Spatial Attention-based Knowledge Distillation with Channel Attention (SAKD-Channel) and Spatial Attention-based Knowledge Distillation with Class Attention (SAKD-Class). On CIFAR-100, with ResNet8 × 4 as the student model, SAKD-Channel improves Top-1 validation accuracy by 1.98%, and SAKD-Class improves it by 3.35% compared to traditional distillation methods. On ImageNet, using ResNet18, these two methods improve Top-1 validation accuracy by 0.55% and 0.17%, respectively, over traditional methods. We also conduct extensive experiments to investigate the working mechanisms and application conditions of channel and class dimensions knowledge distillation, providing new theoretical insights for attention-based knowledge transfer.},
  archive      = {J_CVIU},
  author       = {Liangtai Zhou and Weiwei Zhang and Banghui Zhang and Yufeng Guo and Junhuang Wang and Xiaobin Li and Jianqing Zhu},
  doi          = {10.1016/j.cviu.2025.104359},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104359},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Extensions in channel and class dimensions for attention-based knowledge distillation},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep reinforcement active learning method for multi-label image classification. <em>CVIU</em>, <em>257</em>, 104351. (<a href='https://doi.org/10.1016/j.cviu.2025.104351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning is a widely used method for addressing the high cost of sample labeling in deep learning models and has achieved significant success in recent years. However, most existing active learning methods only focus on single-label image classification and have limited application in the context of multi-label images. To address this issue, we propose a novel, multi-label active learning approach based on a reinforcement learning strategy. The proposed approach introduces a reinforcement active learning framework that accounts for the expected error reduction in multi-label images, making it adaptable to multi-label classification models. Additionally, we develop a multi-label reinforcement active learning module (MLRAL), which employs an actor-critic strategy and proximal policy optimization algorithm (PPO). Our state and reward functions consider multi-label correlations to accurately evaluate the potential impact of unlabeled samples on the current model state. We conduct experiments on various multi-label image classification tasks, including the VOC 2007, MS-COCO, NUS-WIDE and ODIR. We also compare our method with multiple classification models, and experimental results show that our method outperforms existing approaches on various tasks, demonstrating the superiority and effectiveness of the proposed method.},
  archive      = {J_CVIU},
  author       = {Qing Cai and Ran Tao and Xiufen Fang and Xiurui Xie and Guisong Liu},
  doi          = {10.1016/j.cviu.2025.104351},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104351},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A deep reinforcement active learning method for multi-label image classification},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot object detection via synthetic features with optimal transport. <em>CVIU</em>, <em>257</em>, 104350. (<a href='https://doi.org/10.1016/j.cviu.2025.104350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot object detection aims to simultaneously localize and classify the objects in an image with limited training samples. Most existing few-shot object detection methods focus on extracting the features of a few samples of novel classes, which can lack diversity. Consequently, they may not sufficiently capture the data distribution. To address this limitation, we propose a novel approach that trains a generator to produce synthetic data for novel classes. Still, directly training a generator on the novel class is ineffective due to the scarcity of novel data. To overcome this issue, we leverage the large-scale dataset of base classes by training a generator that captures the data variations of the dataset. Specifically, we train the generator with an optimal transport loss that minimizes the distance between the real and synthetic data distributions, which encourages the generator to capture data variations in base classes. We then transfer the captured variations to novel classes by generating synthetic data with the trained generator. Extensive experiments on benchmark datasets demonstrate that the proposed method outperforms the state of the art.},
  archive      = {J_CVIU},
  author       = {Anh-Khoa Nguyen Vu and Thanh-Toan Do and Vinh-Tiep Nguyen and Tam Le and Minh-Triet Tran and Tam V. Nguyen},
  doi          = {10.1016/j.cviu.2025.104350},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104350},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Few-shot object detection via synthetic features with optimal transport},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local consistency guidance: Personalized stylization method of face video. <em>CVIU</em>, <em>257</em>, 104339. (<a href='https://doi.org/10.1016/j.cviu.2025.104339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face video stylization aims to transform real face videos into specific reference styles. Although image stylization has achieved remarkable results, maintaining continuity and accurately preserving original facial expressions in video stylization remains a significant challenge. This work introduces a novel approach for face video stylization that ensures consistent quality across the entire video by leveraging local consistency. Specifically, the framework builds upon existing diffusion models and employs local consistency as a guiding principle. It integrates a Local-Cross Attention (LCA) module to maintain style consistency between frames and a Local Style Transfer (LST) module to ensure seamless video continuity. Comparative experiments were conducted, along with qualitative and quantitative analyses using frame consistency, SSIM, FID, LPIPS, user studies, and flow similarity parameters. An ablation experiment section is also included. The experimental results demonstrate that the proposed approach effectively achieves continuous video stylization by applying local consistency guidance. Additionally, the Local Consistency Guidance (LCG) method shows strong performance in achieving continuous video stylization. After extensive investigation, this work achieves state-of-the-art results in the field of video stylization. Further information is available on the project homepage at https://lcgfacevideostylization.github.io/github.io/ .},
  archive      = {J_CVIU},
  author       = {Wancheng Feng and Yingchao Liu and Jiaming Pei and Guangliang Cheng and Lukun Wang},
  doi          = {10.1016/j.cviu.2025.104339},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104339},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Local consistency guidance: Personalized stylization method of face video},
  volume       = {257},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MF-LPR2: Multi-frame license plate image restoration and recognition using optical flow. <em>CVIU</em>, <em>256</em>, 104361. (<a href='https://doi.org/10.1016/j.cviu.2025.104361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {License plate recognition (LPR) is important for traffic law enforcement, crime investigation, and surveillance. However, license plate areas in dash cam images often suffer from low resolution, motion blur, and glare, which make accurate recognition challenging. Existing generative models that rely on pretrained priors cannot reliably restore such poor quality images, frequently introducing severe artifacts and distortions. To address this issue, we propose a novel multi-frame license plate restoration and recognition framework, MF-LPR 2 , which addresses ambiguities in poor quality images by aligning and aggregating neighboring frames instead of relying on pretrained knowledge. To achieve accurate frame alignment, we employ a state-of-the-art optical flow estimator in conjunction with carefully designed algorithms that detect and correct erroneous optical flow estimations by leveraging the spatio-temporal consistency inherent in license plate image sequences. Our approach enhances both image quality and recognition accuracy while preserving the evidential content of the input images. In addition, we constructed a novel Realistic LPR (RLPR) dataset to evaluate MF-LPR 2 . The RLPR dataset contains 200 pairs of low-quality license plate image sequences and high-quality pseudo ground-truth images, reflecting the complexities of real-world scenarios. In experiments, MF-LPR 2 outperformed eight recent restoration models in terms of PSNR, SSIM, and LPIPS by significant margins. In recognition, MF-LPR 2 achieved an accuracy of 86.44%, outperforming both the best single-frame LPR (16.18%) and the multi-frame LPR (82.55%) among the eleven baseline models. The results of ablation studies confirm that our filtering and refinement algorithms significantly contribute to these improvements.},
  archive      = {J_CVIU},
  author       = {Kihyun Na and Junseok Oh and Youngkwan Cho and Bumjin Kim and Sungmin Cho and Jinyoung Choi and Injung Kim},
  doi          = {10.1016/j.cviu.2025.104361},
  journal      = {Computer Vision and Image Understanding},
  month        = {5},
  pages        = {104361},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MF-LPR2: Multi-frame license plate image restoration and recognition using optical flow},
  volume       = {256},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modality mixer exploiting complementary information for multi-modal action recognition. <em>CVIU</em>, <em>256</em>, 104358. (<a href='https://doi.org/10.1016/j.cviu.2025.104358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the distinctive characteristics of sensors, each modality exhibits unique physical properties. For this reason, in the context of multi-modal action recognition, it is important to consider not only the overall action content but also the complementary nature of different modalities. In this paper, we propose a novel network, named Modality Mixer (M-Mixer) network, which effectively leverages and incorporates the complementary information across modalities with the temporal context of actions for action recognition. A key component of our proposed M-Mixer is the Multi-modal Contextualization Unit (MCU), a simple yet effective recurrent unit. Our MCU is responsible for temporally encoding a sequence of one modality ( e . g ., RGB) with action content features of other modalities ( e . g ., depth and infrared modalities). This process encourages M-Mixer network to exploit global action content and also to supplement complementary information of other modalities. Furthermore, to extract appropriate complementary information regarding to the given modality settings, we introduce a new module, named Complementary Feature Extraction Module (CFEM). CFEM incorporates separate learnable query embeddings for each modality, which guide CFEM to extract complementary information and global action content from the other modalities. As a result, our proposed method outperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and NW-UCLA datasets. Moreover, through comprehensive ablation studies, we further validate the effectiveness of our proposed method.},
  archive      = {J_CVIU},
  author       = {Sumin Lee and Sangmin Woo and Muhammad Adi Nugroho and Changick Kim},
  doi          = {10.1016/j.cviu.2025.104358},
  journal      = {Computer Vision and Image Understanding},
  month        = {5},
  pages        = {104358},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Modality mixer exploiting complementary information for multi-modal action recognition},
  volume       = {256},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty estimation using boundary prediction for medical image super-resolution. <em>CVIU</em>, <em>256</em>, 104349. (<a href='https://doi.org/10.1016/j.cviu.2025.104349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image super-resolution can be performed by several deep learning frameworks. However, as the safety of each patient is of primary concern, having models with a high degree of population level accuracy is not enough. Instead of a one size fits all approach, there is a need to measure the reliability and trustworthiness of such models from the point of view of personalized healthcare and precision medicine. Hence, in this paper, we propose a novel approach to predict a range of super-resolved (SR) images that any generative super-resolution model may yield for a given low-resolution (LR) image using residual image prediction. Providing multiple images within the suggested lower and upper bound increases the probability of finding an exact match to the high-resolution (HR) image. To further compare models and provide reliability scores, we estimate the coverage and uncertainty of the models and check if coverage can be improved at the cost of increasing uncertainty. Experimental results on lung CT scans from LIDC-IDRI and Radiopedia COVID-19 CT Images Segmentation datasets show that our models, BliMSR and MoMSGAN, provide the best HR and SR coverage at different levels of residual attention with a comparatively lower uncertainty. We believe our model agnostic approach to uncertainty estimation for generative medical imaging is the first of its kind and would help clinicians decide on the trustworthiness of any super-resolution model in a generalized manner while providing alternate SR images with enhanced details for better diagnosis for each individual patient.},
  archive      = {J_CVIU},
  author       = {Samiran Dey and Partha Basuchowdhuri and Debasis Mitra and Robin Augustine and Sanjoy Kumar Saha and Tapabrata Chakraborti},
  doi          = {10.1016/j.cviu.2025.104349},
  journal      = {Computer Vision and Image Understanding},
  month        = {5},
  pages        = {104349},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Uncertainty estimation using boundary prediction for medical image super-resolution},
  volume       = {256},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure perception and edge refinement network for monocular depth estimation. <em>CVIU</em>, <em>256</em>, 104348. (<a href='https://doi.org/10.1016/j.cviu.2025.104348'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation is fundamental for scene understanding and visual downstream tasks. In recent years, with the development of deep learning, increasing complex networks and powerful mechanisms have significantly improved the performance of monocular depth estimation. Nevertheless, predicting dense pixel depths from a single RGB image remains challenging due to the ill-posed issues and inherent ambiguity. Two unresolved issues persist: (1) Depth features are limited in perceiving the scene structure accurately, leading to inaccurate region estimation. (2) Low-level features, which are rich in details, are not fully utilized, causing the missing of details and ambiguous edges. The crux to accurate dense depth restoration is to efficiently handle global scene structure as well as local details. To solve these two issues, we propose the Scene perception and Edge refinement network for Monocular Depth Estimation (SE-MDE). Specifically, we carefully design a depth-enhanced encoder (DEE) to effectively perceive the overall structure of the scene while refining the feature responses of different regions. Meanwhile, we introduce a dense edge-guided network (DENet) that maximizes the utilization of low-level features to enhance the depth of details and edges. Extensive experiments validate the effectiveness of our method, with several experimental results on the NYU v2 indoor dataset and KITTI outdoor dataset demonstrate the state-of-the-art performance of the proposed method.},
  archive      = {J_CVIU},
  author       = {Shuangquan Zuo and Yun Xiao and Xuanhong Wang and Hao Lv and Hongwei Chen},
  doi          = {10.1016/j.cviu.2025.104348},
  journal      = {Computer Vision and Image Understanding},
  month        = {5},
  pages        = {104348},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Structure perception and edge refinement network for monocular depth estimation},
  volume       = {256},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial style mixup and improved temporal alignment for cross-domain few-shot action recognition. <em>CVIU</em>, <em>255</em>, 104341. (<a href='https://doi.org/10.1016/j.cviu.2025.104341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Domain Few-Shot Action Recognition (CDFSAR) aims at transferring knowledge from base classes to novel ones with limited labeled data, under distribution shift between base (source domain) and novel (target domain) classes. This paper addresses the issues of insufficient style coverage for the target domain and potential temporal misalignment with chronological order in existing methods. To mitigate distribution shifts across domains, we propose an Adversarial Style Mixup (ASM) module, which enriches the diversity of style distributions covering the target domain. ASM mixes up source and target domain styles through statistical means and variances, with the adversarially learned mixup ratio and style noise. On the other hand, we design an Improved Temporal Alignment (ITA) module to address the issue of temporal misalignment between videos. In the proposed ITA, keyframes are extracted as priors for better temporal alignment with a temporal mixer to reduce the misalignment noise. Extensive experiments on video action recognition datasets demonstrates the superiority of our method compared with the state of the arts for the challenging problem of CDFSAR. Ablation study validates that both the proposed ASM and ITA modules contribute to performance improvement by style distribution expansion and keyframe-based temporal alignment.},
  archive      = {J_CVIU},
  author       = {Kaiyan Cao and Jiawen Peng and Jiaxin Chen and Xinyuan Hou and Andy J. Ma},
  doi          = {10.1016/j.cviu.2025.104341},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {104341},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adversarial style mixup and improved temporal alignment for cross-domain few-shot action recognition},
  volume       = {255},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Syntactically and semantically enhanced captioning network via hybrid attention and POS tagging prompt. <em>CVIU</em>, <em>255</em>, 104340. (<a href='https://doi.org/10.1016/j.cviu.2025.104340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning has become a thriving research area, with current methods relying on static visuals or motion information. However, videos contain a complex interplay between multiple objects with unique temporal patterns. Traditional techniques struggle to capture this intricate connection, leading to inaccurate captions due to the gap between video features and generated text. Analyzing these temporal variations and identifying relevant objects remains a challenge. This paper proposes SySCapNet, a novel deep-learning architecture for video captioning, designed to address this limitation. SySCapNet effectively captures objects involved in motions and extracts spatio-temporal action features. This information, along with visual features and motion data, guides the caption generation process. We introduce a groundbreaking hybrid attention module that leverages both visual saliency and spatio-temporal dynamics to extract highly detailed and semantically meaningful features. Furthermore, we incorporate part-of-speech tagging to guide the network in disambiguating words and understanding their grammatical roles. Extensive evaluations on benchmark datasets demonstrate that SySCapNet achieves superior performance compared to existing methods. The generated captions are not only informative but also grammatically correct and rich in context, surpassing the limitations of basic AI descriptions.},
  archive      = {J_CVIU},
  author       = {Deepali Verma and Tanima Dutta},
  doi          = {10.1016/j.cviu.2025.104340},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {104340},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Syntactically and semantically enhanced captioning network via hybrid attention and POS tagging prompt},
  volume       = {255},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FrTrGAN: Single image dehazing using the frequency component of transmission maps in the generative adversarial network. <em>CVIU</em>, <em>255</em>, 104336. (<a href='https://doi.org/10.1016/j.cviu.2025.104336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazy images, particularly in outdoor scenes, have reduced visibility due to atmospheric particles, making image dehazing a critical task for enhancing visual clarity. The main challenges in image dehazing involve accurately detecting and removing haze while preserving fine details and maintaining overall image quality. Many existing dehazing methods struggle with varying haze conditions, often compromising the structural and perceptual integrity of the restored images. In this paper, we introduce FrTrGAN, a framework for single-image dehazing that leverages the frequency components of transmission maps. This novel framework addresses these challenges by integrating the Fourier Transform within an enhanced CycleGAN architecture. Unlike traditional spatial-domain dehazing methods, FrTrGAN operates in the frequency domain, where it isolates low-frequency haze components – responsible for blurring fine details – and removes them more precisely. The Inverse Fourier Transform is then applied to map the refined data back to the spatial domain, ensuring that the resulting images maintain clarity, sharpness, and structural integrity. We evaluate our method on multiple datasets, including HSTS, SOTS Outdoor, O-Haze, I-Haze, D-Hazy, BeDDE and Dense-Haze using PSNR and SSIM for quantitative performance assessment. Additionally, we include results based on non-referential metrics such as FADE, SSEQ, BRISQUE and NIQE to further evaluate the perceptual quality of the dehazed images. The results demonstrate that FrTrGAN significantly outperforms existing methods while effectively restoring both frequency components and perceptual image quality. This comprehensive evaluation highlights the robustness of FrTrGAN in diverse haze conditions and underscores the effectiveness of a frequency-domain approach to image dehazing, laying the groundwork for future advancements in the field.},
  archive      = {J_CVIU},
  author       = {Pulkit Dwivedi and Soumendu Chakraborty},
  doi          = {10.1016/j.cviu.2025.104336},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {104336},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FrTrGAN: Single image dehazing using the frequency component of transmission maps in the generative adversarial network},
  volume       = {255},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hexagonal mesh-based neural rendering for real-time rendering and fast reconstruction. <em>CVIU</em>, <em>255</em>, 104335. (<a href='https://doi.org/10.1016/j.cviu.2025.104335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although recent neural rendering-based methods can achieve high-quality geometry and realistic rendering results in multi-view reconstruction, they incur a heavy computational burden on rendering and training, which limits their application scenarios. To address these challenges, we propose an effective mesh-based neural rendering approach which leverages the characteristic of meshes being able to achieve real-time rendering. Besides, a coarse-to-fine scheme is introduced to efficiently extract the initial mesh so as to significantly reduce the reconstruction time. More importantly, we suggest a hexagonal mesh model to preserve surface regularity by constraining the second-order derivatives of its vertices, where only low level of positional encoding is engaged for neural rendering. Experiments show that our approach significantly reduces the rendering time from several tens of seconds to 0.05s compared to methods based on implicit representation. And it can quickly achieve state-of-the-art results in novel view synthesis and reconstruction. Our full implementation will be made publicly available at https://github.com/FuchengSu/FastMesh .},
  archive      = {J_CVIU},
  author       = {Yisu Zhang and Jianke Zhu and Lixiang Lin},
  doi          = {10.1016/j.cviu.2025.104335},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {104335},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Hexagonal mesh-based neural rendering for real-time rendering and fast reconstruction},
  volume       = {255},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic anchor: Density map guided small object detector for tiny persons. <em>CVIU</em>, <em>255</em>, 104325. (<a href='https://doi.org/10.1016/j.cviu.2025.104325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the application of aerial and space-based equipments, such as drones in the search and rescue process, there is an increasing demand on the detection of small and even tiny human targets. However, most existing detectors rely on generating smaller and denser anchors for small target detection, which introduces a high number of redundant negative anchor samples. To alleviate this issue, we propose a novel density map-guided tiny person detector with dynamic anchor. Specifically, we elaborately design an Anchor Proposals Mask (APM) module to effectively eliminate negative anchor samples and adaptively adjust anchor distribution with the guidance of density maps produced by Density Map Generator (DMG). To promote the quality of the density map, we develop a Multi-Scale Feature Distillation (MSFD) module and incorporate the Focal Inverse Distance Transform (FIDT) map to conduct knowledge distillation for DMG with the assistance of the crowd counting network. Extensive experiments on the TinyPerson and VisDrone datasets demonstrate that our method significantly enhances the performance of two-stage detectors in terms of average precision (AP) and average recall (AR) while effectively reducing the impact of negative anchor boxes.},
  archive      = {J_CVIU},
  author       = {Xingzhou Xu and Zhaoyong Mao and Xin Wang and Qinhao Tu and Junge Shen},
  doi          = {10.1016/j.cviu.2025.104325},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {104325},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dynamic anchor: Density map guided small object detector for tiny persons},
  volume       = {255},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial and temporal beliefs for mistake detection in assembly tasks. <em>CVIU</em>, <em>254</em>, 104338. (<a href='https://doi.org/10.1016/j.cviu.2025.104338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assembly tasks, as an integral part of daily routines and activities, involve a series of sequential steps that are prone to error. This paper proposes a novel method for identifying ordering mistakes in assembly tasks based on knowledge-grounded beliefs. The beliefs comprise spatial and temporal aspects, each serving a unique role. Spatial beliefs capture the structural relationships among assembly components and indicate their topological feasibility. Temporal beliefs model the action preconditions and enforce sequencing constraints. Furthermore, we introduce a learning algorithm that dynamically updates and augments the belief sets online. To evaluate, we first test our approach in deducing predefined rules on synthetic data based on industry assembly. We also verify our approach on the real-world Assembly101 dataset, enhanced with annotations of component information. Our framework achieves superior performance in detecting ordering mistakes under both synthetic and real-world settings, highlighting the effectiveness of our approach.},
  archive      = {J_CVIU},
  author       = {Guodong Ding and Fadime Sener and Shugao Ma and Angela Yao},
  doi          = {10.1016/j.cviu.2025.104338},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104338},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Spatial and temporal beliefs for mistake detection in assembly tasks},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Establishing a unified evaluation framework for human motion generation: A comparative analysis of metrics. <em>CVIU</em>, <em>254</em>, 104337. (<a href='https://doi.org/10.1016/j.cviu.2025.104337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of generative artificial intelligence for human motion generation has expanded rapidly, necessitating a unified evaluation framework. This paper presents a detailed review of eight evaluation metrics for human motion generation, highlighting their unique features and shortcomings. We propose standardized practices through a unified evaluation setup to facilitate consistent model comparisons. Additionally, we introduce a novel metric that assesses diversity in temporal distortion by analyzing warping diversity, thereby enhancing the evaluation of temporal data. We also conduct experimental analyses of three generative models using two publicly available datasets, offering insights into the interpretation of each metric in specific case scenarios. Our goal is to offer a clear, user-friendly evaluation framework for newcomers, complemented by publicly accessible code: https://github.com/MSD-IRIMAS/Evaluating-HMG .},
  archive      = {J_CVIU},
  author       = {Ali Ismail-Fawaz and Maxime Devanne and Stefano Berretti and Jonathan Weber and Germain Forestier},
  doi          = {10.1016/j.cviu.2025.104337},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104337},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Establishing a unified evaluation framework for human motion generation: A comparative analysis of metrics},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lifelong visible–infrared person re-identification via replay samples domain-modality-mix reconstruction and cross-domain cognitive network. <em>CVIU</em>, <em>254</em>, 104328. (<a href='https://doi.org/10.1016/j.cviu.2025.104328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapting statically-trained models to the incessant influx of data streams poses a pivotal research challenge. Concurrently, visible and infrared person re-identification (VI-ReID) offers an all-day surveillance mode to advance intelligent surveillance and elevate public safety precautions. Hence, we are pioneering a more fine-grained exploration of the lifelong VI-ReID task at the camera level, aiming to imbue the learned models with the capabilities of lifelong learning and memory within the continuous data streams. This task confronts dual challenges of cross-modality and cross-domain variations. Thus, in this paper, we proposed a Domain-Modality-Mix (DMM) based replay samples reconstruction strategy and Cross-domain Cognitive Network (CDCN) to address those challenges. Firstly, we establish an effective and expandable baseline model based on residual neural networks. Secondly, capitalizing on the unexploited potential knowledge of a memory bank that archives diverse replay samples, we enhance the anti-forgetting ability of our model by the Domain-Modality-Mix strategy, which devising a cross-domain, cross-modal image-level replay sample reconstruction, effectively alleviating catastrophic forgetting induced by modality and domain variations. Finally, guided by the Chunking Theory in cognitive psychology, we designed a Cross-domain Cognitive Network, which incorporates a camera-aware, expandable graph convolutional cognitive network to facilitate adaptive learning of intra-modal consistencies and cross-modal similarities within continuous cross-domain data streams. Extensive experiments demonstrate that our proposed method has remarkable adaptability and robust resistance to forgetting and outperforms multiple state-of-the-art methods in comparative assessments of the performance of LVI-ReID. The source code of our designed method is at https://github.com/SWU-CS-MediaLab/DMM-CDCN .},
  archive      = {J_CVIU},
  author       = {Xianyu Zhu and Guoqiang Xiao and Michael S. Lew and Song Wu},
  doi          = {10.1016/j.cviu.2025.104328},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104328},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Lifelong visible–infrared person re-identification via replay samples domain-modality-mix reconstruction and cross-domain cognitive network},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigating social contexts: A transformer approach to relationship recognition. <em>CVIU</em>, <em>254</em>, 104327. (<a href='https://doi.org/10.1016/j.cviu.2025.104327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing interpersonal relationships is essential for enabling human–computer systems to understand and engage effectively with social contexts. Compared to other computer vision tasks, Interpersonal relation recognition requires an higher semantic understanding of the scene, ranging from large background context to finer clues. We propose a transformer based model that attends to each person pair relation in an image reaching state of the art performances on a classical benchmark dataset People in Social Context (PISC). Our solution differs from others as it makes no use of a separate GNN but relies instead on transformers alone. Additionally, we explore the impact of incorporating additional supervision from occupation labels on relationship recognition performance and we extensively ablate different architectural parameters and loss choices. Furthermore, we compare our model with a recent Large Multimodal Model (LMM) to precisely assess the zero-shot capabilities of such general models over highly specific tasks. Our study contributes to advancing the state of the art in social relationship recognition and highlights the potential of transformer-based models in capturing complex social dynamics from visual data.},
  archive      = {J_CVIU},
  author       = {Lorenzo Berlincioni and Luca Cultrera and Marco Bertini and Alberto Del Bimbo},
  doi          = {10.1016/j.cviu.2025.104327},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104327},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Navigating social contexts: A transformer approach to relationship recognition},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient feature selection for pre-trained vision transformers. <em>CVIU</em>, <em>254</em>, 104326. (<a href='https://doi.org/10.1016/j.cviu.2025.104326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handcrafted layer-wise vision transformers have demonstrated remarkable performance in image classification. However, their high computational cost limits their practical applications. In this paper, we first identify and highlight the data-independent feature redundancy in pre-trained Vision Transformer (ViT) models. Based on this observation, we explore the feasibility of searching for the best substructure within the original pre-trained model. To this end, we propose EffiSelecViT, a novel pruning method aimed at reducing the computational cost of ViTs while preserving their accuracy. EffiSelecViT introduces importance scores for both self-attention heads and Multi-Layer Perceptron (MLP) neurons in pre-trained ViT models. L1 regularization is applied to constrain and learn these scores. In this simple way, components that are crucial for model performance are assigned higher scores, while those with lower scores are identified as less important and subsequently pruned. Experimental results demonstrate that EffiSelecViT can prune DeiT-B to retain only 64% of FLOPs while maintaining accuracy. This efficiency-accuracy trade-off is consistent across various ViT architectures. Furthermore, qualitative analysis reveals enhanced information expression in the pruned models, affirming the effectiveness and practicality of EffiSelecViT. The code is available at https://github.com/ZJ6789/EffiSelecViT .},
  archive      = {J_CVIU},
  author       = {Lan Huang and Jia Zeng and Mengqiang Yu and Weiping Ding and Xingyu Bai and Kangping Wang},
  doi          = {10.1016/j.cviu.2025.104326},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104326},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Efficient feature selection for pre-trained vision transformers},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain tumor image segmentation based on shuffle transformer-dynamic convolution and inception dilated convolution. <em>CVIU</em>, <em>254</em>, 104324. (<a href='https://doi.org/10.1016/j.cviu.2025.104324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of brain tumors is essential for accurate clinical diagnosis and effective treatment. Convolutional neural networks (CNNs) have improved brain tumor segmentation with their excellent performance in local feature modeling. However, they still face the challenge of unpredictable changes in tumor size and location, because it cannot be effectively matched by CNN-based methods with local and regular receptive fields. To overcome these obstacles, we propose brain tumor image segmentation based on shuffle transformer-dynamic convolution and inception dilated convolution that captures and adapts different features of tumors through multi-scale feature extraction. Our model combines Shuffle Transformer-Dynamic Convolution (STDC) to capture both fine-grained and contextual image details so that it helps improve localization accuracy. In addition, the Inception Dilated Convolution(IDConv) module solves the problem of significant changes in the size of brain tumors, and then captures the information of different size of object. The multi-scale feature aggregation(MSFA) module integrates features from different encoder levels, which contributes to enriching the scale diversity of input patches and enhancing the robustness of segmentation. The experimental results conducted on the BraTS 2019, BraTS 2020, BraTS 2021, and MSD BTS datasets indicate that our model outperforms other state-of-the-art methods in terms of accuracy.},
  archive      = {J_CVIU},
  author       = {Lifang Zhou and Ya Wang},
  doi          = {10.1016/j.cviu.2025.104324},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104324},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Brain tumor image segmentation based on shuffle transformer-dynamic convolution and inception dilated convolution},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental few-shot instance segmentation without fine-tuning on novel classes. <em>CVIU</em>, <em>254</em>, 104323. (<a href='https://doi.org/10.1016/j.cviu.2025.104323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many current incremental few-shot object detection and instance segmentation methods necessitate fine-tuning on novel classes, which presents difficulties when training newly emerged classes on devices with limited computational power. In this paper, a finetune-free incremental few-shot instance segmentation method is proposed. Firstly, a novel weight generator (NWG) is proposed to map the embeddings of novel classes to their respective true centers. Then, the limitations of cosine similarity on novel classes with few samples are analyzed, and a simple yet effective improvement called the piecewise function for similarity calculation (PFSC) is proposed. Lastly, a probability dependency method (PD) is designed to mitigate the impact on the performance of base classes after registering novel classes. The comparative experimental results show that the proposed model outperforms existing finetune-free methods much more on MS COCO and VOC datasets, and registration of novel classes has almost no negative impact on the base classes. Therefore, the model exhibits excellent performance and the proposed finetune-free idea enables it to learn novel classes directly through inference on devices with limited computational power.},
  archive      = {J_CVIU},
  author       = {Luofeng Zhang and Libo Weng and Yuanming Zhang and Fei Gao},
  doi          = {10.1016/j.cviu.2025.104323},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104323},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Incremental few-shot instance segmentation without fine-tuning on novel classes},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint image-instance spatial–temporal attention for few-shot action recognition. <em>CVIU</em>, <em>254</em>, 104322. (<a href='https://doi.org/10.1016/j.cviu.2025.104322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot Action Recognition (FSAR) constitutes a crucial challenge in computer vision, entailing the recognition of actions from a limited set of examples. Recent approaches mainly focus on employing image-level features to construct temporal dependencies and generate prototypes for each action category. However, a considerable number of these methods utilize mainly image-level features that incorporate background noise and focus insufficiently on real foreground (action-related instances), thereby compromising the recognition capability, particularly in the few-shot scenario. To tackle this issue, we propose a novel joint Image-Instance level Spatial–temporal attention approach (I 2 ST) for Few-shot Action Recognition. The core concept of I 2 ST is to perceive the action-related instances and integrate them with image features via spatial–temporal attention. Specifically, I 2 ST consists of two key components: Action-related Instance Perception and Joint Image-Instance Spatial–temporal Attention. Given the basic representations from the feature extractor, the Action-related Instance Perception is introduced to perceive action-related instances under the guidance of a text-guided segmentation model. Subsequently, the Joint Image-Instance Spatial–temporal Attention is used to construct the feature dependency between instances and images. To enhance the prototype representations of different categories of videos, a pair of spatial–temporal attention sub-modules is introduced to combine image features and instance embeddings across both temporal and spatial dimensions, and a global fusion sub-module is utilized to aggregate global contextual information, then robust action video prototypes can be formed. Finally, based on the video prototype, a Global–Local Prototype Matching is performed for reliable few-shot video matching. In this manner, our proposed I 2 ST can effectively exploit the foreground instance-level cues and model more accurate spatial–temporal relationships for the complex few-shot video recognition scenarios. Extensive experiments across standard few-shot benchmarks demonstrate that the proposed framework outperforms existing methods and achieves state-of-the-art performance under various few-shot settings.},
  archive      = {J_CVIU},
  author       = {Zefeng Qian and Chongyang Zhang and Yifei Huang and Gang Wang and Jiangyong Ying},
  doi          = {10.1016/j.cviu.2025.104322},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104322},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Joint image-instance spatial–temporal attention for few-shot action recognition},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). View-to-label: Multi-view consistency for self-supervised monocular 3D object detection. <em>CVIU</em>, <em>254</em>, 104320. (<a href='https://doi.org/10.1016/j.cviu.2025.104320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For autonomous vehicles, driving safely is highly dependent on the capability to correctly perceive the environment in the 3D space, hence the task of 3D object detection represents a fundamental aspect of perception. While 3D sensors deliver accurate metric perception, monocular approaches enjoy cost and availability advantages that are valuable in a wide range of applications. Unfortunately, training monocular methods requires a vast amount of annotated data. To compensate for this need, we propose a novel approach to self-supervise 3D object detection purely from RGB video sequences, leveraging geometric constraints and weak labels. Unlike other approaches that exploit additional sensors during training, our method relies on the temporal continuity of video sequences. A supervised pre-training on synthetic data produces initial plausible 3D boxes, then our geometric and photometrically grounded losses provide a strong self-supervision signal that allows the model to be fine-tuned on real data without labels. Our experiments on Autonomous Driving benchmark datasets showcase the effectiveness and generality of our approach and the competitive performance compared to other self-supervised approaches.},
  archive      = {J_CVIU},
  author       = {Issa Mouawad and Nikolas Brasch and Fabian Manhardt and Federico Tombari and Francesca Odone},
  doi          = {10.1016/j.cviu.2025.104320},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104320},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {View-to-label: Multi-view consistency for self-supervised monocular 3D object detection},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mandala simplification: Sacred symmetry meets minimalism. <em>CVIU</em>, <em>254</em>, 104319. (<a href='https://doi.org/10.1016/j.cviu.2025.104319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mandalas, intricate artistic designs with radial symmetry, are imbued with a timeless allure that transcends cultural boundaries. Found in various cultures and spiritual traditions worldwide, mandalas hold profound significance as symbols of unity, wholeness, and spiritual transformation. At the heart of mandalas lies the concept of sacred symmetry, a timeless principle that resonates with the deepest realms of human consciousness. However, in handcrafted mandalas, symmetry often falls short of perfection, necessitating refinement to evoke harmony and balance. With this in mind, we introduce a computational approach aimed at capturing the all-round symmetry of mandalas through minimalist principles. By leveraging innovative geometric and graph-theoretic tools and an interactive twin atlas, this approach streamlines parameter domains to achieve the revered state of sacred symmetry, epitomizing harmonious balance. This is especially beneficial when dealing with handcrafted mandalas of subpar quality, necessitating concise representations for tasks like mandala editing, recreation, atlas building, and referencing. Experimental findings and related results demonstrate the effectiveness of the proposed methodology.},
  archive      = {J_CVIU},
  author       = {Tusita Sarkar and Preetam Chayan Chatterjee and Partha Bhowmick},
  doi          = {10.1016/j.cviu.2025.104319},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104319},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Mandala simplification: Sacred symmetry meets minimalism},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MultiFire20K: A semi-supervised enhanced large-scale UAV-based benchmark for advancing multi-task learning in fire monitoring. <em>CVIU</em>, <em>254</em>, 104318. (<a href='https://doi.org/10.1016/j.cviu.2025.104318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective fire detection and response are crucial to minimizing the widespread damage and loss caused by fires in both urban and natural environments. While advancements in Computer Vision have enhanced fire detection and response, progress in UAV-based monitoring remains limited due to the lack of comprehensive datasets. This study introduces the MultiFire20K dataset, comprising 20,500 diverse aerial fire images with annotations for fire classification, environment classification, and separate segmentation masks for both fire and smoke, specifically designed to support multi-task learning. Due to limited labeled data in remote sensing, a semi-supervised approach for generating pseudo-labels for fire and smoke masks is explored which takes into consideration the environment of the event. We experimented with various segmentation architectures backbone models to generate reliable pseudo-label masks. Benchmarks were established by evaluating models on fire classification, environment classification, and the segmentation of both fire and smoke, and comparing these results to those obtained from multi-task models. Our study highlights the substantial advantages of a multi-task approach in fire monitoring, particularly in improving fire and smoke segmentation through shared knowledge during training. This enhanced efficiency, combined with the conservation of memory and computational resources, makes the multi-task framework superior for real-time applications, especially when compared to using separate models for each individual task. We anticipate that our dataset and benchmark results will encourage further research in fire surveillance, advancing fire detection and prevention methods.},
  archive      = {J_CVIU},
  author       = {Demetris Shianios and Panayiotis Kolios and Christos Kyrkou},
  doi          = {10.1016/j.cviu.2025.104318},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104318},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MultiFire20K: A semi-supervised enhanced large-scale UAV-based benchmark for advancing multi-task learning in fire monitoring},
  volume       = {254},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When super-resolution meets camouflaged object detection: A comparison study. <em>CVIU</em>, <em>253</em>, 104321. (<a href='https://doi.org/10.1016/j.cviu.2025.104321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution (SR) and camouflage object detection (COD) are two prominent topics in the field of computer vision, with various joint applications. However, in previous work, these two areas were often studied in isolation. In this paper, we conduct a comprehensive comparative evaluation of both for the first time. Specifically, we benchmark different super-resolution methods on commonly used COD datasets while also evaluating the robustness of different COD models using COD data processed by SR methods. Experiments reveal challenges in preserving semantic information due to differences in targets and features between the two domains. COD relies on extracting semantic information from low-resolution images to identify camouflage targets. There is a risk of losing or distorting important semantic details during the application of SR techniques. Balancing the enhancement of spatial resolution with the preservation of semantic information is crucial for maintaining the accuracy of COD algorithms. Therefore, we propose a new SR model called Dilated Super-resolution (DISR) to enhance SR performance on COD, achieving state-of-the-art results on five commonly used SR datasets. The Urban100 x4 dataset task improved by 0.38 dB. Using low-resolution images processed by DISR for COD tasks can enhance target visibility and significantly improve the performance of COD tasks. Our goal is to leverage the synergies between these two domains, draw insights from the complementarity of techniques in both fields, and provide insights and inspiration for future research in both communities.},
  archive      = {J_CVIU},
  author       = {Juan Wen and Shupeng Cheng and Weiyan Hou and Luc Van Gool and Radu Timofte},
  doi          = {10.1016/j.cviu.2025.104321},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104321},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {When super-resolution meets camouflaged object detection: A comparison study},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental few-shot instance segmentation via feature enhancement and prototype calibration. <em>CVIU</em>, <em>253</em>, 104317. (<a href='https://doi.org/10.1016/j.cviu.2025.104317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental few-shot instance segmentation (iFSIS) aims to detect and segment instances of novel classes with only a few training samples, while maintaining performance on base classes without revisiting base class data. iMTFA, a representative iFSIS method, offers a flexible approach for adding novel classes. Its key mechanism involves generating novel class weights by normalizing and averaging embeddings obtained from K -shot novel instances. However, relying on such a small sample size often leads to insufficient representation of the real class distribution, which in turn results in biased weights for the novel classes. Furthermore, due to the absence of novel fine-tuning, iMTFA tends to predict potential novel class foregrounds as background, which exacerbates the bias in the generated novel class weights. To overcome these limitations, we propose a simple but effective iFSIS method, named Enhancement and Calibration-based iMTFA (EC-iMTFA). Specifically, we first design an embedding enhancement and aggregation (EEA) module, which enhances the feature diversity of each novel instance embedding before generating novel class weights. We then design a novel prototype calibration (NPC) module that leverages the well-calibrated base class and background weights in the classifier to enhance the discriminability of novel class prototypes. In addition, a simple weight preprocessing (WP) mechanism is designed based on NPC to improve the calibration process further. Extensive experiments on COCO and VOC datasets demonstrate that EC-iMTFA outperforms iMTFA in terms of iFSIS and iFSOD performance, stability, and efficiency without requiring novel fine-tuning. Moreover, EC-iMTFA achieves competitive results compared to recent state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Weixiang Gao and Caijuan Shi and Rui Wang and Ao Cai and Changyu Duan and Meiqin Liu},
  doi          = {10.1016/j.cviu.2025.104317},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104317},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Incremental few-shot instance segmentation via feature enhancement and prototype calibration},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cartoon character recognition based on portrait style fusion. <em>CVIU</em>, <em>253</em>, 104316. (<a href='https://doi.org/10.1016/j.cviu.2025.104316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a cartoon character recognition method using portrait characteristics to address the problem of copyright protection in cartoon works. The proposed recognition framework is derived from content-based retrieval mechanism, achieving an effective solution for copyright identification of cartoon characters. This research has two core contributions. The first is that we propose an ECA-based residual attention module to improve cartoon character feature learning ability. Cartoon character images typically have fewer details and texture information, and inter-channel information interaction can more effectively extract cartoon features. The second is a style transfer-based cartoon character construction mechanism, which is proposed to create a simulated plagiarized cartoon character dataset by fusing portrait style and content. Comparative experiments demonstrate that the proposed model effectively improves detection accuracy. Finally, we validate the effectiveness and feasibility of the model by retrieving plagiarized versions of cartoon characters.},
  archive      = {J_CVIU},
  author       = {De Li and Zhenyi Jin and Xun Jin},
  doi          = {10.1016/j.cviu.2025.104316},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104316},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cartoon character recognition based on portrait style fusion},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring plain ViT features for multi-class unsupervised visual anomaly detection. <em>CVIU</em>, <em>253</em>, 104308. (<a href='https://doi.org/10.1016/j.cviu.2025.104308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work studies a challenging and practical issue known as multi-class unsupervised anomaly detection (MUAD). This problem requires only normal images for training while simultaneously testing both normal and anomaly images across multiple classes. Existing reconstruction-based methods typically adopt pyramidal networks as encoders and decoders to obtain multi-resolution features, often involving complex sub-modules with extensive handcraft engineering. In contrast, a plain Vision Transformer (ViT) showcasing a more straightforward architecture has proven effective in multiple domains, including detection and segmentation tasks. It is simpler, more effective, and elegant. Following this spirit, we explore the use of only plain ViT features for MUAD. We first abstract a Meta-AD concept by synthesizing current reconstruction-based methods. Subsequently, we instantiate a novel ViT-based ViTAD structure, designed incrementally from both global and local perspectives. This model provide a strong baseline to facilitate future research. Additionally, this paper uncovers several intriguing findings for further investigation. Finally, we comprehensively and fairly benchmark various approaches using seven metrics and their average. Utilizing a basic training regimen with only an MSE loss, ViTAD achieves state-of-the-art results and efficiency on MVTec AD, VisA, and Uni-Medical datasets. E.g ., achieving 85.4 mAD that surpasses UniAD by +3.0 for the MVTec AD dataset, and it requires only 1.1 h and 2.3G GPU memory to complete model training on a single V100 that can serve as a strong baseline to facilitate the development of future research. Full code is available at https://zhangzjn.github.io/projects/ViTAD/ .},
  archive      = {J_CVIU},
  author       = {Jiangning Zhang and Xuhai Chen and Yabiao Wang and Chengjie Wang and Yong Liu and Xiangtai Li and Ming-Hsuan Yang and Dacheng Tao},
  doi          = {10.1016/j.cviu.2025.104308},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104308},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Exploring plain ViT features for multi-class unsupervised visual anomaly detection},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale riemannian meta-optimization via subspace adaptation. <em>CVIU</em>, <em>253</em>, 104306. (<a href='https://doi.org/10.1016/j.cviu.2025.104306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Riemannian meta-optimization provides a promising approach to solving non-linear constrained optimization problems, which trains neural networks as optimizers to perform optimization on Riemannian manifolds. However, existing Riemannian meta-optimization methods take up huge memory footprints in large-scale optimization settings, as the learned optimizer can only adapt gradients of a fixed size and thus cannot be shared across different Riemannian parameters. In this paper, we propose an efficient Riemannian meta-optimization method that significantly reduces the memory burden for large-scale optimization via a subspace adaptation scheme. Our method trains neural networks to individually adapt the row and column subspaces of Riemannian gradients, instead of directly adapting the full gradient matrices in existing Riemannian meta-optimization methods. In this case, our learned optimizer can be shared across Riemannian parameters with different sizes. Our method reduces the model memory consumption by six orders of magnitude when optimizing an orthogonal mainstream deep neural network ( e.g. ResNet50). Experiments on multiple Riemannian tasks show that our method can not only reduce the memory consumption but also improve the performance of Riemannian meta-optimization.},
  archive      = {J_CVIU},
  author       = {Peilin Yu and Yuwei Wu and Zhi Gao and Xiaomeng Fan and Yunde Jia},
  doi          = {10.1016/j.cviu.2025.104306},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104306},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Large-scale riemannian meta-optimization via subspace adaptation},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial semi-supervised domain adaptation for semantic segmentation: A new role for labeled target samples. <em>CVIU</em>, <em>253</em>, 104305. (<a href='https://doi.org/10.1016/j.cviu.2025.104305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial learning baselines for domain adaptation (DA) approaches in the context of semantic segmentation are under explored in semi-supervised framework. These baselines involve solely the available labeled target samples in the supervision loss. In this work, we propose to enhance their usefulness on both semantic segmentation and the single domain classifier neural networks. We design new training objective losses for cases when labeled target data behave as source samples or as real target samples. The underlying rationale is that considering the set of labeled target samples as part of source domain helps reducing the domain discrepancy and, hence, improves the contribution of the adversarial loss. To support our approach, we consider a complementary method that mixes source and labeled target data, then applies the same adaptation process. We further propose an unsupervised selection procedure using entropy to optimize the choice of labeled target samples for adaptation. We illustrate our findings through extensive experiments on the benchmarks GTA5, SYNTHIA, and Cityscapes. The empirical evaluation highlights competitive performance of our proposed approach.},
  archive      = {J_CVIU},
  author       = {Marwa Kechaou and Mokhtar Z. Alaya and Romain Hérault and Gilles Gasso},
  doi          = {10.1016/j.cviu.2025.104305},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104305},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adversarial semi-supervised domain adaptation for semantic segmentation: A new role for labeled target samples},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-modal explainability approach for human-aware robots in multi-party conversation. <em>CVIU</em>, <em>253</em>, 104304. (<a href='https://doi.org/10.1016/j.cviu.2025.104304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The addressee estimation (understanding to whom somebody is talking) is a fundamental task for human activity recognition in multi-party conversation scenarios. Specifically, in the field of human–robot interaction, it becomes even more crucial to enable social robots to participate in such interactive contexts. However, it is usually implemented as a binary classification task, restricting the robot’s capability to estimate whether it was addressed or not, which limits its interactive skills. For a social robot to gain the trust of humans, it is also important to manifest a certain level of transparency and explainability. Explainable artificial intelligence thus plays a significant role in the current machine learning applications and models, to provide explanations for their decisions besides excellent performance. In our work, we (a) present an addressee estimation model with improved performance in comparison with the previous state-of-the-art; (b) further modify this model to include inherently explainable attention-based segments; (c) implement the explainable addressee estimation as part of a modular cognitive architecture for multi-party conversation in an iCub robot; (d) validate the real-time performance of the explainable model in multi-party human–robot interaction; (e) propose several ways to incorporate explainability and transparency in the aforementioned architecture; and (f) perform an online user study to analyze the effect of various explanations on how human participants perceive the robot.},
  archive      = {J_CVIU},
  author       = {Iveta Bečková and Štefan Pócoš and Giulia Belgiovine and Marco Matarese and Omar Eldardeer and Alessandra Sciutti and Carlo Mazzola},
  doi          = {10.1016/j.cviu.2025.104304},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104304},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A multi-modal explainability approach for human-aware robots in multi-party conversation},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monocular per-object distance estimation with masked object modeling. <em>CVIU</em>, <em>253</em>, 104303. (<a href='https://doi.org/10.1016/j.cviu.2025.104303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Per-object distance estimation is critical in surveillance and autonomous driving, where safety is crucial. While existing methods rely on geometric or deep supervised features, only a few attempts have been made to leverage self-supervised learning. In this respect, our paper draws inspiration from Masked Image Modeling (MiM) and extends it to multi-object tasks . While MiM focuses on extracting global image-level representations, it struggles with individual objects within the image. This is detrimental for distance estimation, as objects far away correspond to negligible portions of the image. Conversely, our strategy, termed Masked Object Modeling ( MoM ), enables a novel application of masking techniques. In a few words, we devise an auxiliary objective that reconstructs the portions of the image pertaining to the objects detected in the scene. The training phase is performed in a single unified stage, simultaneously optimizing the masking objective and the downstream loss ( i.e ., distance estimation). We evaluate the effectiveness of MoM on a novel reference architecture (DistFormer) on the standard KITTI, NuScenes, and MOTSynth datasets. Our evaluation reveals that our framework surpasses the SoTA and highlights its robust regularization properties. The MoM strategy enhances both zero-shot and few-shot capabilities, from synthetic to real domain. Finally, it furthers the robustness of the model in the presence of occluded or poorly detected objects.},
  archive      = {J_CVIU},
  author       = {Aniello Panariello and Gianluca Mancusi and Fedy Haj Ali and Angelo Porrello and Simone Calderara and Rita Cucchiara},
  doi          = {10.1016/j.cviu.2025.104303},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104303},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Monocular per-object distance estimation with masked object modeling},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mask prior generation with language queries guided networks for referring image segmentation. <em>CVIU</em>, <em>253</em>, 104296. (<a href='https://doi.org/10.1016/j.cviu.2025.104296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of Referring Image Segmentation (RIS) is to generate a pixel-level mask to accurately segment the target object according to its natural language expression. Previous RIS methods ignore exploring the significant language information in both the encoder and decoder stages, and simply use an upsampling-convolution operation to obtain the prediction mask, resulting in inaccurate visual object locating. Thus, this paper proposes a Mask Prior Generation with Language Queries Guided Network (MPG-LQGNet). In the encoder of MPG-LQGNet, a Bidirectional Spatial Alignment Module (BSAM) is designed to realize the bidirectional fusion for both vision and language embeddings, generating additional language queries to understand both the locating of targets and the semantics of the language. Moreover, a Channel Attention Fusion Gate (CAFG) is designed to enhance the exploration of the significance of the cross-modal embeddings. In the decoder of the MPG-LQGNet, the Language Query Guided Mask Prior Generator (LQPG) is designed to utilize the generated language queries to activate significant information in the upsampled decoding features, obtaining the more accurate mask prior that guides the final prediction. Extensive experiments on RefCOCO series datasets show that our method consistently improves over state-of-the-art methods. The source code of our MPG-LQGNet is available at https://github.com/SWU-CS-MediaLab/MPG-LQGNet .},
  archive      = {J_CVIU},
  author       = {Jinhao Zhou and Guoqiang Xiao and Michael S. Lew and Song Wu},
  doi          = {10.1016/j.cviu.2025.104296},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104296},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Mask prior generation with language queries guided networks for referring image segmentation},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond shadows and light: Odyssey of face recognition for social good. <em>CVIU</em>, <em>253</em>, 104293. (<a href='https://doi.org/10.1016/j.cviu.2025.104293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition technology, though undeniably transformative in its technical evolution, remains conspicuously underleveraged in humanitarian endeavors. This survey highlights its latent utility in addressing critical societal exigencies, ranging from the expeditious identification of disaster-afflicted individuals to locating missing children. We investigate technical complexities arising from facial feature degradation, aging, occlusions, and low-resolution images. These issues are frequently encountered in real-world scenarios. We provide a comprehensive review of state-of-the-art models and relevant datasets, including a meta-analysis of existing and curated collections such as the newly introduced Web and Generated Injured Faces (WGIF) dataset. Our evaluation encompasses the performance of current face recognition algorithms in real-world scenarios, exemplified by a case study on the Balasore train accident in India. By examining factors such as the impact of aging on facial features and the limitations of traditional models in handling low-quality or occluded images, we showcase the complexities inherent in applying face recognition for societal good. We discuss future research directions, emphasizing the need for interdisciplinary collaborations and innovative methodologies to enhance the adaptability and robustness of face recognition systems in humanitarian contexts. Through detailed case studies, we provide insights into the effectiveness of current methods and identify key areas for improvement. Our goal is to encourage the development of specialized face recognition models for social welfare applications, contributing to timely and accurate identification in critical situations.},
  archive      = {J_CVIU},
  author       = {Chiranjeev Chiranjeev and Muskan Dosi and Shivang Agarwal and Jyoti Chaudhary and Pranav Pant and Mayank Vatsa and Richa Singh},
  doi          = {10.1016/j.cviu.2025.104293},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104293},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Beyond shadows and light: Odyssey of face recognition for social good},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Luminance prior guided low-light 4C catenary image enhancement. <em>CVIU</em>, <em>253</em>, 104287. (<a href='https://doi.org/10.1016/j.cviu.2025.104287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In scenarios characterized by inadequate fill lighting, catenary images captured by railway power supply 4C monitoring equipment often exhibit a phenomenon of low light, which can pose significant challenges for accurately detecting anomalies in the equipment. This, in turn, has ramifications for the smooth operation, timely maintenance, and overall safety assurance of railway systems. Recognizing this critical issue, our study introduces an innovative dual-branch priori-guided enhancement method specifically tailored for low-light catenary images obtained through powered 4C monitoring equipment. Within the multi-scale branch of our method, we leverage the powerful capabilities of convolutional neural networks (CNNs) along with the self-attention mechanism to effectively extract both local and global features from the images. This dual focus allows our model to capture intricate details and broader contextual information, enhancing its ability to understand and enhance the images. Concurrently, the pixel-wise branch of our method is designed to estimate enhancement parameters at the pixel level, enabling an adaptive and iterative enhancement process. This fine-grained approach ensures that each pixel in the image is optimized based on its unique characteristics and context, leading to more nuanced and accurate enhancements. To further inform and constrain our enhancement process, we conduct a statistical analysis of the average light intensity of images under both normal and low-light conditions. By examining the differences and correlations between image brightness under these varying light conditions, we derive statistical priors that are integrated into our method. These priors serve as valuable guidance for our model, helping it to make more informed decisions during the enhancement process. Moreover, to mitigate the challenges associated with obtaining labeled data, we adopt an unsupervised model training strategy. This approach allows our method to learn and improve without the need for extensive and costly labeling efforts, making it more practical and scalable for real-world applications. Experimental results demonstrate the superiority of our proposed method when compared to state-of-the-art approaches for low-light catenary image enhancement. Our method improves the visual quality of the images, ultimately contributing to the safety and efficiency of railway operations.},
  archive      = {J_CVIU},
  author       = {Zhenhua Xue and Jun Luo and Zhenlin Wei},
  doi          = {10.1016/j.cviu.2025.104287},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {104287},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Luminance prior guided low-light 4C catenary image enhancement},
  volume       = {253},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-preserved point-based human avatar. <em>CVIU</em>, <em>252</em>, 104307. (<a href='https://doi.org/10.1016/j.cviu.2025.104307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enable realistic experience in AR/VR and digital entertainment, we present the first point-based human avatar model that embodies the entirety expressive range of digital humans. Specifically, we employ two MLPs to model pose-dependent deformation and linear skinning (LBS) weights. The representation of appearance relies on a decoder and the features attached to each point. In contrast to alternative implicit approaches, the oriented points representation not only provides a more intuitive way to model human avatar animation but also significantly reduces the computational time on both training and inference. Moreover, we propose a novel method to transfer semantic information from the SMPL-X model to the points, which enables to better understand human body movements. By leveraging the semantic information of points, we can facilitate virtual try-on and human avatar composition through exchanging the points of same category across different subjects. Experimental results demonstrate the efficacy of our presented method. Our implementation is publicly available at https://github.com/l1346792580123/spa .},
  archive      = {J_CVIU},
  author       = {Lixiang Lin and Jianke Zhu},
  doi          = {10.1016/j.cviu.2025.104307},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104307},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Semantic-preserved point-based human avatar},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASK_LOSS guided non-end-to-end image denoising network based on multi-attention module with bias rectified linear unit and absolute pooling unit. <em>CVIU</em>, <em>252</em>, 104302. (<a href='https://doi.org/10.1016/j.cviu.2025.104302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based image denoising algorithms have demonstrated superior denoising performance but suffer from loss of details and excessive smoothing of edges after denoising. In addition, these denoising models often involve redundant calculations, resulting in low utilization rates and poor generalization capabilities. To address these challenges, we proposes an Non-end-to-end Multi-Attention Denoising Network (N-ete MADN). Firstly, we propose a Bias Rectified Linear Unit (BReLU) to replace ReLU as the activation function, which provides enhanced flexibility and expanded activation range without additional computation, constructing a Feature Extraction Unit (FEU) with depth-wise convolutions (DConv). Then an Absolute Pooling Unit (AbsPooling-unit) is proposed to consist Channel Attention Block(CAB), Spatial Attention Block(SAB) and Channel Similarity Attention Block (CSAB) , which are integrated into a Multi-Attention Module (MAM). CAB and SAB aim to enhance the model’s focus on key information respectively in the spatial dimension and the channel dimension, while CSAB aims to improve the model’s ability to detect similar features. Finally, the MAM is utilized to construct a Multi-Attention Denoising Network (MADN). Then a mask loss function (MASK_LOSS) and a compound multi-stage denoising network called Non-end-to-end Multi-Attention Denoising Network (N-ete MADN) based on the loss and MADN are proposed, which aim to handle the image with rich edge information, providing enhanced protection for edges and facilitating the reconstruction of edge information after image denoising. This framework enhances learning capacity and efficiency, effectively addressing edge detail loss challenges in denoising tasks. Experimental results on both synthetic several datasets demonstrate that our model can achieve the state-of-the-art denoising performance with low computational costs.},
  archive      = {J_CVIU},
  author       = {Jing Zhang and Jingcheng Yu and Zhicheng Zhang and Congyao Zheng and Yao Le and Yunsong Li},
  doi          = {10.1016/j.cviu.2025.104302},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104302},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MASK_LOSS guided non-end-to-end image denoising network based on multi-attention module with bias rectified linear unit and absolute pooling unit},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fake news detection based on BERT multi-domain and multi-modal fusion network. <em>CVIU</em>, <em>252</em>, 104301. (<a href='https://doi.org/10.1016/j.cviu.2025.104301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive growth of the Internet has simplified communication, making the detection and annotation of fake news on social media increasingly critical. Leveraging existing studies, this work introduces the Fake News Detection Based on BERT Multi-domain and Multi-modal Fusion Network (BMMFN). This framework utilizes the BERT model to transform text content of fake news into textual vectors, while image features are extracted using the VGG-19 model. A multimodal fusion network is developed, factoring in text-image correlations and interactions through joint matrices that enhance the integration of information across modalities. Additionally, a multidomain classifier is incorporated to align multimodal features from various events within a unified feature space. The performance of this model is confirmed through experiments on Weibo and Twitter datasets, with results indicating that the BMMFN model surpasses contemporary state-of-the-art models in several metrics, thereby effectively enhancing the detection of fake news.},
  archive      = {J_CVIU},
  author       = {Kai Yu and Shiming Jiao and Zhilong Ma},
  doi          = {10.1016/j.cviu.2025.104301},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104301},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Fake news detection based on BERT multi-domain and multi-modal fusion network},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RelFormer: Advancing contextual relations for transformer-based dense captioning. <em>CVIU</em>, <em>252</em>, 104300. (<a href='https://doi.org/10.1016/j.cviu.2025.104300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense captioning aims to detect regions in images and generate natural language descriptions for each identified region. For this task, contextual modeling is crucial for generating accurate descriptions since regions in the image could interact with each other. Previous efforts primarily focused on the modeling between categorized object regions, which are extracted by pre-trained object detectors, e.g ., Fast R-CNN. However, they overlook the contextual modeling for non-object regions, e.g ., sky, rivers, and grass, commonly referred to as “stuff”. In this paper, we propose the RelFormer framework to enhance the contextual relation modeling of Transformer-based dense captioning. Specifically, we design a clip-assisted region feature extraction module to extract rich contextual features of regions, involving stuff regions. We then introduce a straightforward relation encoder based on self-attention to effectively model relationships between regional features. To accurately extract candidate regions in dense images while minimizing redundant proposals, we further introduce the amplified decay non-maximum-suppression, which amplifies the decay degree of the redundant proposals so that they can be removed while reserving the detection of the small regions under a low confidence threshold. The experimental results indicate that by enhancing contextual interactions, our model exhibits a good understanding of regions and attains state-of-the-art performance on dense captioning tasks. Our method achieves 17.52% mAP on VG V1.0, 16.59% on VG V1.2, and 15.49% on VG-COCO. Code is available at https://github.com/Wykay/Relformer .},
  archive      = {J_CVIU},
  author       = {Weiqi Jin and Mengxue Qu and Caijuan Shi and Yao Zhao and Yunchao Wei},
  doi          = {10.1016/j.cviu.2025.104300},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104300},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {RelFormer: Advancing contextual relations for transformer-based dense captioning},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic scene understanding through advanced object context analysis in image. <em>CVIU</em>, <em>252</em>, 104299. (<a href='https://doi.org/10.1016/j.cviu.2025.104299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in computer vision have primarily concentrated on interpreting visual data, often overlooking the significance of contextual differences across various regions within images. In contrast, our research introduces a model for indoor scene recognition that pivots towards the ‘attention’ paradigm. This model views attention as a response to the stimulus image properties, suggesting that focus is ‘pulled’ towards the most visually salient zones within an image, as represented in a saliency map. Attention is directed towards these zones based on uninterpreted semantic features of the image, such as luminance contrast, color, shape, and edge orientation. This neurobiologically plausible and computationally tractable approach offers a more nuanced understanding of scenes by prioritizing zones solely based on their image properties. The proposed model enhances scene understanding through an in-depth analysis of the object context in images. Scene recognition is achieved by extracting features from selected regions of interest within individual image frames using patch-based object detection techniques, thus generating distinctive feature descriptors for the identified objects of interest. The resulting feature descriptors are then subjected to semantic embedding, which uses distributed representations to transform the sparse feature vectors into dense semantic vectors within a learned latent space. This enables subsequent classification tasks by machine learning models trained on embedded semantic representations. This model was evaluated on three image datasets: UIUC Sports-8, PASCAL VOC - Visual Object Classes, and a proprietary image set created by the authors. Compared to state-of-the-art methods, this paper presents a more robust approach to the abstraction and generalization of interior scenes. This approach has demonstrated superior accuracy with our novel model over existing models. Consequently, this has led to an improvement in the classification of scenes in the selected indoor environments. Our code is published here: https://github.com/sebastianlop8/Semantic-Scene-Object-Context-Analysis.git},
  archive      = {J_CVIU},
  author       = {Luis Hernando Ríos González and Sebastián López Flórez and Alfonso González-Briones and Fernando de la Prieta},
  doi          = {10.1016/j.cviu.2025.104299},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104299},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Semantic scene understanding through advanced object context analysis in image},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative neural painting. <em>CVIU</em>, <em>252</em>, 104298. (<a href='https://doi.org/10.1016/j.cviu.2025.104298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of painting fosters creativity and rational planning. However, existing generative AI mostly focuses on producing visually pleasant artworks, without emphasizing the painting process. We introduce a novel task, Collaborative Neural Painting (CNP) , to facilitate collaborative art painting generation between users and agents. Given any number of user-input brushstrokes as the context or just the desired object class , CNP should produce a sequence of strokes supporting the completion of a coherent painting. Importantly, the process can be gradual and iterative, so allowing users’ modifications at any phase until the completion. Moreover, we propose to solve this task using a painting representation based on a sequence of parametrized strokes, which makes it easy both editing and composition operations. These parametrized strokes are processed by a Transformer-based architecture with a novel attention mechanism to model the relationship between the input strokes and the strokes to complete. We also propose a new masking scheme to reflect the interactive nature of CNP and adopt diffusion models as the basic learning process for its effectiveness and diversity in the generative field. Finally, to develop and validate methods on the novel task, we introduce a new dataset of painted objects and an evaluation protocol to benchmark CNP both quantitatively and qualitatively. We demonstrate the effectiveness of our approach and the potential of the CNP task as a promising avenue for future research. Project page and code: this https URL .},
  archive      = {J_CVIU},
  author       = {Nicola Dall’Asen and Willi Menapace and Elia Peruzzo and Enver Sangineto and Yiming Wang and Elisa Ricci},
  doi          = {10.1016/j.cviu.2025.104298},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104298},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Collaborative neural painting},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing human pose estimation through deep learning approaches: An overview. <em>CVIU</em>, <em>252</em>, 104297. (<a href='https://doi.org/10.1016/j.cviu.2025.104297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the everyday IoT ecosystem, many devices and systems are interconnected in an intelligent living environment to create a comfortable and efficient living space. In this scenario, approaches based on automatic recognition of actions and events can support fully autonomous digital assistants and personalized services. A pivotal component in this domain is “Human Pose Estimation”, which plays a critical role in action recognition for a wide range of applications, including home automation, healthcare, safety, and security. These systems are designed to detect human actions and deliver customized real-time responses and support. Selecting an appropriate technique for Human Pose Estimation is crucial to enhancing these systems for various applications. This choice hinges on the specific environment and can be categorized on the basis of whether the technique is designed for images or videos, single-person or multi-person scenarios, and monocular or multiview inputs. A comprehensive overview of recent research outcomes is essential to showcase the evolution of the research area, along with its underlying principles and varied application domains. Key benchmarks across these techniques are suitable and provide valuable insights into their performance. Hence, the paper summarizes these benchmarks, offering a comparative analysis of the techniques. As research in this field continues to evolve, it is critical for researchers to stay up to date with the latest developments and methodologies to promote further innovations in the field of pose estimation research. Therefore, this comprehensive overview presents a thorough examination of the subject matter, encompassing all pertinent details. Its objective is to equip researchers with the knowledge and resources necessary to investigate the topic and effectively retrieve all relevant information necessary for their investigations.},
  archive      = {J_CVIU},
  author       = {Gaetano Dibenedetto and Stefanos Sotiropoulos and Marco Polignano and Giuseppe Cavallo and Pasquale Lops},
  doi          = {10.1016/j.cviu.2025.104297},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104297},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Comparing human pose estimation through deep learning approaches: An overview},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating degradation estimation in light field spatial super-resolution. <em>CVIU</em>, <em>252</em>, 104295. (<a href='https://doi.org/10.1016/j.cviu.2025.104295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in light field super-resolution (SR) have yielded impressive results. In practice, however, many existing methods are limited by assuming fixed degradation models, such as bicubic downsampling, which hinders their robustness in real-world scenarios with complex degradations. To address this limitation, we present LF-DEST, an effective blind L ight F ield SR method that incorporates explicit D egradation Est imation to handle various degradation types. LF-DEST consists of two primary components: degradation estimation and light field restoration. The former concurrently estimates blur kernels and noise maps from low-resolution degraded light fields, while the latter generates super-resolved light fields based on the estimated degradations. Notably, we introduce a modulated and selective fusion module that intelligently combines degradation representations with image information, effectively handling diverse degradation types. We conduct extensive experiments on benchmark datasets, demonstrating that LF-DEST achieves superior performance across various degradation scenarios in light field SR. The implementation code is available at https://github.com/zeyuxiao1997/LF-DEST .},
  archive      = {J_CVIU},
  author       = {Zeyu Xiao and Zhiwei Xiong},
  doi          = {10.1016/j.cviu.2025.104295},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104295},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Incorporating degradation estimation in light field spatial super-resolution},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to mask and permute visual tokens for vision transformer pre-training. <em>CVIU</em>, <em>252</em>, 104294. (<a href='https://doi.org/10.1016/j.cviu.2025.104294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of self-supervised pre-training has emerged as a promising approach to enhance the performance of many different visual tasks. In this context, recent approaches have employed the Masked Image Modeling paradigm, which pre-trains a backbone by reconstructing visual tokens associated with randomly masked image patches. This masking approach, however, introduces noise into the input data during pre-training, leading to discrepancies that can impair performance during the fine-tuning phase. Furthermore, input masking neglects the dependencies between corrupted patches, increasing the inconsistencies observed in downstream fine-tuning tasks. To overcome these issues, we propose a new self-supervised pre-training approach, named Masked and Permuted Vision Transformer (MaPeT), that employs autoregressive and permuted predictions to capture intra-patch dependencies. In addition, MaPeT employs auxiliary positional information to reduce the disparity between the pre-training and fine-tuning phases. In our experiments, we employ a fair setting to ensure reliable and meaningful comparisons and conduct investigations on multiple visual tokenizers, including our proposed k -CLIP which directly employs discretized CLIP features. Our results demonstrate that MaPeT achieves competitive performance on ImageNet, compared to baselines and competitors under the same model setting. We release an implementation of our code and models at https://github.com/aimagelab/MaPeT .},
  archive      = {J_CVIU},
  author       = {Lorenzo Baraldi and Roberto Amoroso and Marcella Cornia and Lorenzo Baraldi and Andrea Pilzer and Rita Cucchiara},
  doi          = {10.1016/j.cviu.2025.104294},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104294},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning to mask and permute visual tokens for vision transformer pre-training},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DM-align: Leveraging the power of natural language instructions to make changes to images. <em>CVIU</em>, <em>252</em>, 104292. (<a href='https://doi.org/10.1016/j.cviu.2025.104292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based semantic image editing assumes the manipulation of an image using a natural language instruction. Although recent works are capable of generating creative and qualitative images, the problem is still mostly approached as a black box sensitive to generating unexpected outputs. Therefore, we propose a novel model to enhance the text-based control of an image editor by explicitly reasoning about which parts of the image to alter or preserve. It relies on word alignments between a description of the original source image and the instruction that reflects the needed updates, and the input image. The proposed Diffusion Masking with word Alignments (DM-Align) allows the editing of an image in a transparent and explainable way. It is evaluated on a subset of the Bison dataset and a self-defined dataset dubbed Dream. When comparing to state-of-the-art baselines, quantitative and qualitative results show that DM-Align has superior performance in image editing conditioned on language instructions, well preserves the background of the image and can better cope with long text instructions.},
  archive      = {J_CVIU},
  author       = {Maria-Mihaela Trusca and Tinne Tuytelaars and Marie-Francine Moens},
  doi          = {10.1016/j.cviu.2025.104292},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104292},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DM-align: Leveraging the power of natural language instructions to make changes to images},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rebalanced supervised contrastive learning with prototypes for long-tailed visual recognition. <em>CVIU</em>, <em>252</em>, 104291. (<a href='https://doi.org/10.1016/j.cviu.2025.104291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, data often follows a long-tailed distribution, resulting in head classes receiving more attention while tail classes are frequently overlooked. Although supervised contrastive learning (SCL) performs well on balanced datasets, it struggles to distinguish features between tail classes in the latent space when dealing with long-tailed data. To address this issue, we propose Rebalanced Supervised Contrastive Learning (ReCL), which can effectively enhance the separability of tail classes features. Compared with two state-of-the-art methods, Contrastive Learning based hybrid networks (Hybrid-SC) and Targeted Supervised Contrastive Learning (TSC), ReCL has two distinctive characteristics: (1) ReCL enhances the clarity of classification boundaries between tail classes by encouraging samples to align more closely with their corresponding prototypes. (2) ReCL does not require targets generation, thereby conserving computational resources. Our method significantly improves the recognition of tail classes, demonstrating competitive accuracy across multiple long-tailed datasets. Our code has been uploaded to https://github.com/cxh981110/ReCL .},
  archive      = {J_CVIU},
  author       = {Xuhui Chang and Junhai Zhai and Shaoxin Qiu and Zhengrong Sun},
  doi          = {10.1016/j.cviu.2025.104291},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104291},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Rebalanced supervised contrastive learning with prototypes for long-tailed visual recognition},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based moving object segmentation for underwater videos using semi-supervised learning. <em>CVIU</em>, <em>252</em>, 104290. (<a href='https://doi.org/10.1016/j.cviu.2025.104290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving object segmentation (MOS) using passive underwater image processing is an important technology for monitoring marine habitats. It aids marine biologists studying biological oceanography and the associated fields of chemical, physical, and geological oceanography to understand marine organisms. Dynamic backgrounds due to marine organisms like algae and seaweed, and improper illumination of the environment pose challenges in detecting moving objects in the scene. Previous graph-learning methods have shown promising results in MOS, but are mostly limited to terrestrial surface videos such as traffic video surveillance. Traditional object modeling fails in underwater scenes, due to fish shape and color degradation in motion and the lack of extensive underwater datasets for deep-learning models. Therefore, we propose a semi-supervised graph-learning approach (GraphMOS-U) to segment moving objects in underwater environments. Additionally, existing datasets were consolidated to form the proposed Teleost Fish Classification Dataset, specifically designed for fish classification tasks in complex environments to avoid unseen scenes, ensuring the replication of the transfer learning process on a ResNet-50 backbone. GraphMOS-U uses a six-step approach with transfer learning using Mask R-CNN and a ResNet-50 backbone for instance segmentation, followed by feature extraction using optical flow, visual saliency, and texture. After concatenating these features, a k -NN Graph is constructed, and graph node classification is applied to label objects as foreground or background. The foreground nodes are used to reconstruct the segmentation map of the moving object from the scene. Quantitative and qualitative experiments demonstrate that GraphMOS-U outperforms state-of-the-art algorithms, accurately detecting moving objects while preserving fine details. The proposed method enables the use of graph-based MOS algorithms in underwater scenes.},
  archive      = {J_CVIU},
  author       = {Meghna Kapoor and Wieke Prummel and Jhony H. Giraldo and Badri Narayan Subudhi and Anastasia Zakharova and Thierry Bouwmans and Ankur Bansal},
  doi          = {10.1016/j.cviu.2025.104290},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104290},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Graph-based moving object segmentation for underwater videos using semi-supervised learning},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint generating terminal correction imaging method for modular LED integral imaging systems. <em>CVIU</em>, <em>252</em>, 104279. (<a href='https://doi.org/10.1016/j.cviu.2025.104279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integral imaging has garnered significant attention in 3D display technology due to its potential for high-quality visualization. However, elemental images in integral imaging systems usually suffer from misalignment due to the mechanical or human-induced assembly within the lens arrays, leading to undesirable display quality. This paper introduces a novel Joint-Generating Terminal Correction Imaging (JGTCI) approach tailored for large-scale, modular LED integral imaging systems to address the misalignment between the optical centers of physical lens arrays and the camera in generated elemental image arrays. Specifically, we propose: (1) a high-sensitivity calibration marker to enhance alignment precision by accurately matching lens centers to the central points of elemental images; (2) a partitioned calibration strategy that supports independent calibration of display sections, enabling seamless system expansion without recalibrating previously adjusted regions; and (3) a calibration setup where markers are strategically placed near the lens focal length, ensuring optimal pixel coverage in the camera frame for improved accuracy. Extensive experimental results demonstrate that our JGTCI approach significantly enhances 3D display accuracy, extends the viewing angle, and improves the scalability and practicality of modular integral imaging systems, outperforming recent state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Tianshu Li and Shigang Wang},
  doi          = {10.1016/j.cviu.2025.104279},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104279},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Joint generating terminal correction imaging method for modular LED integral imaging systems},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guided image filtering-conventional to deep models: A review and evaluation study. <em>CVIU</em>, <em>252</em>, 104278. (<a href='https://doi.org/10.1016/j.cviu.2025.104278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, guided image filtering (GIF) has emerged as a successful edge-preserving smoothing technique designed to remove noise while retaining important edges and structures in images. By leveraging a well-aligned guidance image as the prior, GIF has become a valuable tool in various visual applications, offering a balance between edge preservation and computational efficiency. Despite the significant advancements and the development of numerous GIF variants, there has been limited effort to systematically review and evaluate the diverse methods within this research community. To address this gap, this paper offers a comprehensive survey of existing GIF variants, covering both conventional and deep learning-based models. Specifically, we begin by introducing the basic formulation of GIF and its fast implementations. Next, we categorize the GIF follow-up methods into three main categories: local methods, global methods and deep learning-based methods. Within each category, we provide a new sub-taxonomy to better illustrate the motivations behind their design, as well as their contributions and limitations. We then conduct experiments to compare the performance of representative methods, with an analysis of qualitative and quantitative results that reveals several insights into the current state of this research area. Finally, we discuss unresolved issues in the field of GIF and highlight some open problems for further research.},
  archive      = {J_CVIU},
  author       = {Weimin Yuan and Yinuo Wang and Cai Meng and Xiangzhi Bai},
  doi          = {10.1016/j.cviu.2025.104278},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104278},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Guided image filtering-conventional to deep models: A review and evaluation study},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrigendum to “LightSOD: Towards lightweight and efficient network for salient object detection” [J. comput. vis. imag. underst. 249 (2024) 104148]. <em>CVIU</em>, <em>252</em>, 104277. (<a href='https://doi.org/10.1016/j.cviu.2024.104277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CVIU},
  author       = {Thien-Thu Ngo and Hoang Ngoc Tran and Md. Delowar Hossain and Eui-Nam Huh},
  doi          = {10.1016/j.cviu.2024.104277},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104277},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Corrigendum to “LightSOD: Towards lightweight and efficient network for salient object detection” [J. comput. vis. imag. underst. 249 (2024) 104148]},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Illumination-aware and structure-guided transformer for low-light image enhancement. <em>CVIU</em>, <em>252</em>, 104276. (<a href='https://doi.org/10.1016/j.cviu.2024.104276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we proposed a novel illumination-aware and structure-guided transformer that achieves efficient image enhancement by focusing on brightness degradation and precise high-frequency guidance. Specifically, low-light images often contain numerous regions with similar brightness levels but different spatial locations. However, existing attention mechanisms only compute self-attention using channel dimensions or fixed-size spatial blocks, which limits their ability to capture long-range features, making it challenging to achieve satisfactory image restoration quality. At the same time, the details of low-light images are mostly hidden in the darkness. However, existing models often give equal attention to both high-frequency and smooth regions, which makes it difficult to capture the details of deep degradation, resulting in blurry recovered image details. On the one hand, we introduced a dynamic brightness multi-domain self-attention mechanism that selectively focuses on spatial features within dynamic ranges and incorporates frequency domain information. This approach allows the model to capture both local details and global features, restoring global brightness while paying closer attention to regions with similar degradation. On the other hand, we proposed a global maximum gradient search strategy to guide the model’s attention towards high-frequency detail regions, thereby achieving a more fine-grained restored image. Extensive experiments on various benchmark datasets demonstrate that our method achieves state-of-the-art performance.},
  archive      = {J_CVIU},
  author       = {Guodong Fan and Zishu Yao and Min Gan},
  doi          = {10.1016/j.cviu.2024.104276},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104276},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Illumination-aware and structure-guided transformer for low-light image enhancement},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SSL-rehab: Assessment of physical rehabilitation exercises through self-supervised learning of 3D skeleton representations. <em>CVIU</em>, <em>251</em>, 104275. (<a href='https://doi.org/10.1016/j.cviu.2024.104275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rehabilitation aims to assist individuals in recovering or enhancing functions that have been lost or impaired due to injury, illness, or disease. The automatic assessment of physical rehabilitation exercises offers a valuable method for patient supervision, complementing or potentially substituting traditional clinical evaluations. However, acquiring large-scale annotated datasets presents challenges, prompting the need for self-supervised learning and transfer learning in the rehabilitation domain. Our proposed approach integrates these two strategies through Low-Rank Adaptation (LoRA) for both pretraining and fine-tuning. Specifically, we train a foundation model to learn robust 3D skeleton features that adapt to varying levels of masked motion complexity through a three-stage process. In the first stage, we apply a high masking ratio to a subset of joints, using a transformer-based architecture with a graph embedding layer to capture fundamental motion features. In the second stage, we reduce the masking ratio and expand the model’s capacity to learn more intricate motion patterns and interactions between joints. Finally, in the third stage, we further lower the masking ratio to enable the model to refine its understanding of detailed motion dynamics, optimizing its overall performance. During the second and third stages, LoRA layers are incorporated to extract unique features tailored to each masking level, ensuring efficient adaptation without significantly increasing the model size. Fine-tuning for downstream tasks shows that the model performs better when different masked motion levels are utilized. Through extensive experiments conducted on the publicly available KIMORE and UI-PRMD datasets, we demonstrate the effectiveness of our approach in accurately evaluating the execution quality of rehabilitation exercises, surpassing state-of-the-art performance across all metrics. Our project page is available online .},
  archive      = {J_CVIU},
  author       = {Ikram Kourbane and Panagiotis Papadakis and Mihai Andries},
  doi          = {10.1016/j.cviu.2024.104275},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104275},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SSL-rehab: Assessment of physical rehabilitation exercises through self-supervised learning of 3D skeleton representations},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cleanness-navigated-contamination network: A unified framework for recovering regional degradation. <em>CVIU</em>, <em>251</em>, 104274. (<a href='https://doi.org/10.1016/j.cviu.2024.104274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration from regional degradation has long been an important and challenging task. The key to contamination removal is recovering the contents of the corrupted regions with the guidance of the non-corrupted regions. Due to the inadequate long-range modeling, the CNN-based approaches cannot thoroughly investigate the information from non-corrupted regions, resulting in distorted visuals with artificial traces between different regions. To address this issue, we propose a novel Cleanness-Navigated-Contamination Network (CNCNet), which is a unified framework for recovering regional image contamination, such as shadow, flare, and other regional degradation. Our method mainly consists of two components: a contamination-oriented adaptive normalization (COAN) module and a contamination-aware aggregation with transformer (CAAT) module based on the contamination region mask. Under the guidance of the contamination mask, the COAN module formulates the statistics from the non-corrupted region and adaptively applies them to the corrupted region for region-wise restoration. The CAAT module utilizes the region mask to precisely guide the restoration of each contaminated pixel by considering the highly relevant pixels from the contamination-free regions for global pixel-wise restoration. Extensive experiments in both shadow removal tasks and flare removal tasks show that our network framework achieves superior restoration performance.},
  archive      = {J_CVIU},
  author       = {Qianhao Yu and Naishan Zheng and Jie Huang and Feng Zhao},
  doi          = {10.1016/j.cviu.2024.104274},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104274},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cleanness-navigated-contamination network: A unified framework for recovering regional degradation},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian splatting with NeRF-based color and opacity. <em>CVIU</em>, <em>251</em>, 104273. (<a href='https://doi.org/10.1016/j.cviu.2024.104273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of neural networks to capture the intricacies of 3D objects. NeRFs excel at producing strikingly sharp novel views of 3D objects by encoding the shape and color information within neural network weights. Recently, numerous generalizations of NeRFs utilizing generative models have emerged, expanding their versatility. In contrast, Gaussian Splatting (GS) offers a similar render quality with faster training and inference as it does not need neural networks to work. It encodes information about the 3D objects in the set of Gaussian distributions that can be rendered in 3D similarly to classical meshes. Unfortunately, GS is difficult to condition since its representation is fully explicit. To mitigate the caveats of both models, we propose a hybrid model Viewing Direction Gaussian Splatting (VDGS) that uses GS representation of the 3D object’s shape and NeRF-based encoding of opacity. Our model uses Gaussian distributions with trainable positions (i.e., means of Gaussian), shape (i.e., the covariance of Gaussian), opacity, and a neural network that takes Gaussian parameters and viewing direction to produce changes in the said opacity.As a result, our model better describes shadows, light reflections, and the transparency of 3D objects without adding additional texture and light components.},
  archive      = {J_CVIU},
  author       = {Dawid Malarz and Weronika Smolak-Dyżewska and Jacek Tabor and Sławomir Tadeja and Przemysław Spurek},
  doi          = {10.1016/j.cviu.2024.104273},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104273},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Gaussian splatting with NeRF-based color and opacity},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised vision transformers for semantic segmentation. <em>CVIU</em>, <em>251</em>, 104272. (<a href='https://doi.org/10.1016/j.cviu.2024.104272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a fundamental task in computer vision and it is a building block of many other vision applications. Nevertheless, semantic segmentation annotations are extremely expensive to collect, so using pre-training to alleviate the need for a large number of labeled samples is appealing. Recently, self-supervised learning (SSL) has shown effectiveness in extracting strong representations and has been widely applied to a variety of downstream tasks. However, most works perform sub-optimally in semantic segmentation because they ignore the specific properties of segmentation: (i) the need of pixel level fine-grained understanding; (ii) with the assistance of global context understanding; (iii) both of the above achieve with the dense self-supervisory signal. Based on these key factors, we introduce a systematic self-supervised pre-training framework for semantic segmentation, which consists of a hierarchical encoder–decoder architecture MEVT for generating high-resolution features with global contextual information propagation and a self-supervised training strategy for learning fine-grained semantic features. In our study, our framework shows competitive performance compared with other main self-supervised pre-training methods for semantic segmentation on COCO-Stuff, ADE20K, PASCAL VOC, and Cityscapes datasets. e.g., MEVT achieves the advantage in linear probing by +1.3 mIoU on PASCAL VOC.},
  archive      = {J_CVIU},
  author       = {Xianfan Gu and Yingdong Hu and Chuan Wen and Yang Gao},
  doi          = {10.1016/j.cviu.2024.104272},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104272},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Self-supervised vision transformers for semantic segmentation},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YES: You should examine suspect cues for low-light object detection. <em>CVIU</em>, <em>251</em>, 104271. (<a href='https://doi.org/10.1016/j.cviu.2024.104271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in low-light conditions presents substantial challenges, particularly the issue we define as “low-light object-background cheating”. This phenomenon arises from uneven lighting, leading to blurred and inaccurate object edges. Most existing methods focus on basic feature enhancement and addressing the gap between normal-light and synthetic low-light conditions. However, they often overlook the complexities introduced by uneven lighting in real-world environments. To address this, we propose a novel low-light object detection framework, You Examine Suspect (YES), comprising two key components: the Optical Balance Enhancer (OBE) and the Entanglement Attenuation Module (EAM). The OBE emphasizes “balance” by employing techniques such as inverse tone mapping, white balance, and gamma correction to recover details in dark regions while adjusting brightness and contrast without introducing noise. The EAM focuses on “disentanglement” by analyzing both object regions and surrounding areas affected by lighting variations and integrating multi-scale contextual information to clarify ambiguous features. Extensive experiments on ExDark and Dark Face datasets demonstrate the superior performance of proposed YES, validating its effectiveness in low-light object detection tasks. The code will be available at https://github.com/Regina971/YES .},
  archive      = {J_CVIU},
  author       = {Shu Ye and Wenxin Huang and Wenxuan Liu and Liang Chen and Xiao Wang and Xian Zhong},
  doi          = {10.1016/j.cviu.2024.104271},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104271},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {YES: You should examine suspect cues for low-light object detection},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlocal gaussian scale mixture modeling for hyperspectral image denoising. <em>CVIU</em>, <em>251</em>, 104270. (<a href='https://doi.org/10.1016/j.cviu.2024.104270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent nonlocal sparsity methods have gained significant attention in hyperspectral image (HSI) denoising. These methods leverage the nonlocal self-similarity (NSS) prior to group similar full-band patches into nonlocal full-band groups, followed by enforcing a sparsity constraint, usually through soft-thresholding or hard-thresholding operators, on each nonlocal full-band group. However, in these methods, given that real HSI data are non-stationary and affected by noise, the variances of the sparse coefficients are unknown and challenging to accurately estimate from the degraded HSI, leading to suboptimal denoising performance. In this paper, we propose a novel nonlocal Gaussian scale mixture (NGSM) approach for HSI denoising, which significantly enhances the estimation accuracy of both the variances of the sparse coefficients and the unknown sparse coefficients. To reduce spectral redundancy, a global spectral low-rank (LR) prior is integrated with the NGSM model and consolidated into a variational framework for optimization. Extensive experimental results demonstrate that the proposed NGSM algorithm achieves convincing improvements over many state-of-the-art HSI denoising methods, both in quantitative and visual evaluations, while offering exceptional computational efficiency.},
  archive      = {J_CVIU},
  author       = {Ling Ding and Qiong Wang and Yin Poo and Xinggan Zhang},
  doi          = {10.1016/j.cviu.2024.104270},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104270},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Nonlocal gaussian scale mixture modeling for hyperspectral image denoising},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASELMAR: Active and semi-supervised learning-based framework to reduce multi-labeling efforts for activity recognition. <em>CVIU</em>, <em>251</em>, 104269. (<a href='https://doi.org/10.1016/j.cviu.2024.104269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manual annotation of unlabeled data for model training is expensive and time-consuming, especially for visual datasets requiring domain-specific experience for multi-labeling, such as video records generated in hospital settings. There is a need to build frameworks to reduce human labeling efforts while improving training performance. Semi-supervised learning is widely used to generate predictions for unlabeled samples in a partially labeled datasets. Active learning can be used with semi-supervised learning to annotate unlabeled samples to reduce the sampling bias due to the label predictions. We developed the aselmar framework based on active and semi-supervised learning techniques to reduce the time and effort associated with multi-labeling of unlabeled samples for activity recognition. aselmar (i) categorizes the predictions for unlabeled data based on the confidence level in predictions using fixed and adaptive threshold settings, (ii) applies a label verification procedure for the samples with the ambiguous prediction, and (iii) retrains the model iteratively using samples with their high-confidence predictions or manual annotations. We also designed a software tool to guide domain experts in verifying ambiguous predictions. We applied aselmar to recognize eight selected activities from our trauma resuscitation video dataset and evaluated their performance based on the label verification time and the mean ap score metric. The label verification required by aselmar was 12.1% of the manual annotation effort for the unlabeled video records. The improvement in the mean ap score was 5.7% for the first iteration and 8.3% for the second iteration with the fixed threshold-based method compared to the baseline model. The p-values were below 0.05 for the target activities. Using an adaptive-threshold method, aselmar achieved a decrease in ap score deviation, implying an improvement in model robustness. For a speech-based case study, the word error rate decreased by 6.2%, and the average transcription factor increased 2.6 times, supporting the broad applicability of ASELMAR in reducing labeling efforts from domain experts.},
  archive      = {J_CVIU},
  author       = {Aydin Saribudak and Sifan Yuan and Chenyang Gao and Waverly V. Gestrich-Thompson and Zachary P. Milestone and Randall S. Burd and Ivan Marsic},
  doi          = {10.1016/j.cviu.2024.104269},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104269},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {ASELMAR: Active and semi-supervised learning-based framework to reduce multi-labeling efforts for activity recognition},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). As-global-as-possible stereo matching with sparse depth measurement fusion. <em>CVIU</em>, <em>251</em>, 104268. (<a href='https://doi.org/10.1016/j.cviu.2024.104268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently lauded methodologies of As-Global-As-Possible (AGAP) and Sparse Depth Measurement Fusion (SDMF) have emerged as celebrated solutions for tackling the issue of stereo matching. AGAP addresses the congenital shortcomings of Semi-Global-Matching (SGM) in terms of streaking effects, while SDMF leverages active depth sensors to boost disparity computation. In this paper, these two methods are intertwined for attaining superior disparity estimation. Random sparse Depth measurements are fused with Diffusion-Based Fusion to update AGAP’s matching costs. Then, Neighborhood-Based Fusion refines the cost further, leveraging the previous results. Ultimately, the segment-based disparity refinement strategy is utilized for handling outliers and mismatched pixels to achieve final disparity results. Performance evaluations on various stereo datasets demonstrate that the proposed algorithm not only surpasses other challenging stereo matching algorithms but also achieves near real-time efficiency. It is worth pointing out that our proposal surprisingly outperforms most of the deep learning based stereo matching algorithms on Middlebury v.3 online evaluation system, despite not utilizing any learning-based techniques, further validating its superiority and practicality.},
  archive      = {J_CVIU},
  author       = {Peng Yao and Haiwei Sang},
  doi          = {10.1016/j.cviu.2024.104268},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104268},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {As-global-as-possible stereo matching with sparse depth measurement fusion},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV-based person re-identification: A survey of UAV datasets, approaches, and challenges. <em>CVIU</em>, <em>251</em>, 104261. (<a href='https://doi.org/10.1016/j.cviu.2024.104261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) has gained significant interest due to growing public safety concerns that require advanced surveillance and identification mechanisms. While most existing ReID research relies on static surveillance cameras, the use of Unmanned Aerial Vehicles (UAVs) for surveillance has recently gained popularity. Noting the promising application of UAVs in ReID, this paper presents a comprehensive overview of UAV-based ReID, highlighting publicly available datasets, key challenges, and methodologies. We summarize and consolidate evaluations conducted across multiple studies, providing a unified perspective on the state of UAV-based ReID research. Despite their limited size and diversity, We underscore current datasets’ importance in advancing UAV-based ReID research. The survey also presents a list of all available approaches for UAV-based ReID. The survey presents challenges associated with UAV-based ReID, including environmental conditions, image quality issues, and privacy concerns. We discuss dynamic adaptation techniques, multi-model fusion, and lightweight algorithms to leverage ground-based person ReID datasets for UAV applications. Finally, we explore potential research directions, highlighting the need for diverse datasets, lightweight algorithms, and innovative approaches to tackle the unique challenges of UAV-based person ReID.},
  archive      = {J_CVIU},
  author       = {Yousaf Albaluchi and Biying Fu and Naser Damer and Raghavendra Ramachandra and Kiran Raja},
  doi          = {10.1016/j.cviu.2024.104261},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104261},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {UAV-based person re-identification: A survey of UAV datasets, approaches, and challenges},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local optimization cropping and boundary enhancement for end-to-end weakly-supervised segmentation network. <em>CVIU</em>, <em>251</em>, 104260. (<a href='https://doi.org/10.1016/j.cviu.2024.104260'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the performance of weakly-supervised semantic segmentation(WSSS) has significantly increased. It usually employs image-level labels to generate Class Activation Map (CAM) for producing pseudo-labels, which greatly reduces the cost of annotation. Since CNN cannot fully identify object regions, researchers found that Vision Transformers (ViT) can complement the deficiencies of CNN by better extracting global contextual information. However, ViT also introduces the problem of over-smoothing. Great progress has been made in recent years to solve the over-smoothing problem, yet two issues remain. The first issue is that the high-confidence regions in the network-generated CAM still contain areas irrelevant to the class. The second issue is the inaccuracy of CAM boundaries, which contain a small portion of background regions. As we know, the precision of label boundaries is closely tied to excellent segmentation performance. In this work, to address the first issue, we propose a local optimized cropping module (LOC). By randomly cropping selected regions, we allow the local class tokens to be contrasted with the global class tokens. This method facilitates enhanced consistency between local and global representations. To address the second issue, we design a boundary enhancement module (BE) that utilizes an erasing strategy to re-train the image, increasing the network’s extraction of boundary information and greatly improving the accuracy of CAM boundaries, thereby enhancing the quality of pseudo labels. Experiments on the PASCAL VOC dataset show that the performance of our proposed LOC-BE Net outperforms multi-stage methods and is competitive with end-to-end methods. On the PASCAL VOC dataset, our method achieves a CAM mIoU of 74.2% and a segmentation mIoU of 73.1%. On the COCO2014 dataset, our method achieves a CAM mIoU of 43.8% and a segmentation mIoU of 43.4%. Our code has been open sourced: https://github.com/whn786/LOC-BE/tree/main .},
  archive      = {J_CVIU},
  author       = {Weizheng Wang and Chao Zeng and Haonan Wang and Lei Zhou},
  doi          = {10.1016/j.cviu.2024.104260},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104260},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Local optimization cropping and boundary enhancement for end-to-end weakly-supervised segmentation network},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Full-body virtual try-on using top and bottom garments with wearing style control. <em>CVIU</em>, <em>251</em>, 104259. (<a href='https://doi.org/10.1016/j.cviu.2024.104259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various studies have been proposed to synthesize realistic images for image-based virtual try-on, but most of them are limited to replacing a single item on a given model, without considering wearing styles. In this paper, we address the novel problem of full-body virtual try-on with multiple garments by introducing a new benchmark dataset and an image synthesis method. Our Fashion-TB dataset provides comprehensive clothing information by mapping fashion models to their corresponding top and bottom garments, along with semantic region annotations to represent the structure of the garments. WGF-VITON, the single-stage network we have developed, generates full-body try-on images using top and bottom garments simultaneously. Instead of relying on preceding networks to estimate intermediate knowledge, modules for garment transformation and image synthesis are integrated and trained through end-to-end learning. Furthermore, our method proposes Wearing-guide scheme to control the wearing styles in the synthesized try-on images. Through various experiments, for the full-body virtual try-on task, WGF-VITON outperforms state-of-the-art networks in both quantitative and qualitative evaluations with an optimized number of parameters while allowing users to control the wearing styles of the output images. The code and data are available at https://github.com/soonchanpark/WGF-VITON .},
  archive      = {J_CVIU},
  author       = {Soonchan Park and Jinah Park},
  doi          = {10.1016/j.cviu.2024.104259},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104259},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Full-body virtual try-on using top and bottom garments with wearing style control},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal dynamic interlaced network for 3D human pose estimation in video. <em>CVIU</em>, <em>251</em>, 104258. (<a href='https://doi.org/10.1016/j.cviu.2024.104258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent transformer-based methods have achieved excellent performance in 3D human pose estimation. The distinguishing characteristic of transformer lies in its equitable treatment of each token, encoding them independently. When applied to the human skeleton, transformer regards each joint as an equally significant token. This can lead to a lack of clarity in the extraction of connection relationships between joints, thus affecting the accuracy of relationship information. In addition, transformer also treats each frame of temporal sequences equally. This design can introduce a lot of redundant information in short frames with frequent action changes, which can have a negative impact on learning temporal correlations. To alleviate the above issues, we propose an end-to-end framework, a Spatio-Temporal Dynamic Interlaced Network (S-TDINet), including a dynamic spatial GCN encoder (DSGCE) and an interlaced temporal transformer encoder (ITTE). In the DSGCE module, we design three adaptive adjacency matrices to model spatial correlation from static and dynamic perspectives. In the ITTE module, we introduce a global–local interlaced mechanism to mitigate potential interference from redundant information in fast motion scenarios, thereby achieving more accurate temporal correlation modeling. Finally, we conduct extensive experiments and validate the effectiveness of our approach on two widely recognized benchmark datasets: Human3.6M and MPI-INF-3DHP.},
  archive      = {J_CVIU},
  author       = {Feiyi Xu and Jifan Wang and Ying Sun and Jin Qi and Zhenjiang Dong and Yanfei Sun},
  doi          = {10.1016/j.cviu.2024.104258},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104258},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Spatio-temporal dynamic interlaced network for 3D human pose estimation in video},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based dense event grounding with relative positional encoding. <em>CVIU</em>, <em>251</em>, 104257. (<a href='https://doi.org/10.1016/j.cviu.2024.104257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Sentence Grounding (TSG) in videos aims to localize a temporal moment from an untrimmed video that is relevant to a given query sentence. Most existing methods focus on addressing the problem of single sentence grounding. Recently, researchers proposed a new Dense Event Grounding (DEG) problem by extending the single event localization to a multi-event localization, where the temporal moments of multiple events described by multiple sentences are retrieved. In this paper, we introduce an effective proposal-based approach to solve the DEG problem. A Relative Sentence Interaction (RSI) module using graph neural network is proposed to model the event relationship by introducing a temporal relative positional encoding to learn the relative temporal order information between sentences in a dense multi-sentence query. In addition, we design an Event-contextualized Cross-modal Interaction (ECI) module to tackle the lack of global information from other related events when fusing visual and sentence features. Finally, we construct an Event Graph (EG) with intra-event edges and inter-event edges to model the relationship between proposals in the same event and proposals in different events to further refine their representations for final localizations. Extensive experiments on ActivityNet-Captions and TACoS datasets show the effectiveness of our solution.},
  archive      = {J_CVIU},
  author       = {Jianxiang Dong and Zhaozheng Yin},
  doi          = {10.1016/j.cviu.2024.104257},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104257},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Graph-based dense event grounding with relative positional encoding},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DA2: Distribution-agnostic adaptive feature adaptation for one-class classification. <em>CVIU</em>, <em>251</em>, 104256. (<a href='https://doi.org/10.1016/j.cviu.2024.104256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-class classification (OCC), i.e., identifying whether an example belongs to the same distribution as the training data, is essential for deploying machine learning models in the real world. Adapting the pre-trained features on the target dataset has proven to be a promising paradigm for improving OCC performance. Existing methods are constrained by assumptions about the training distribution. This contradicts the real scenario where the data distribution is unknown. In this work, we propose a simple d istribution- a gnostic a daptive feature adaptation method ( DA 2 ). The core idea is to adaptively cluster the features of every class tighter depending on the property of the data. We rely on the prior that the augmentation distributions of intra-class samples overlap, then align the features of different augmentations of every sample by a non-contrastive method. We find that training a random initialized predictor degrades the pre-trained backbone in the non-contrastive method. To tackle this problem, we design a learnable symmetric predictor and initialize it based on the eigenspace alignment theory. Benchmarks, the proposed challenging near-distribution experiments substantiate the capability of our method in various data distributions. Furthermore, we find that utilizing DA 2 can immensely mitigate the long-standing catastrophic forgetting in feature adaptation of OCC. Code will be released upon acceptance.},
  archive      = {J_CVIU},
  author       = {Zilong Zhang and Zhibin Zhao and Xingwu Zhang and Xuefeng Chen},
  doi          = {10.1016/j.cviu.2024.104256},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104256},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DA2: Distribution-agnostic adaptive feature adaptation for one-class classification},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive semantic guidance network for video captioning. <em>CVIU</em>, <em>251</em>, 104255. (<a href='https://doi.org/10.1016/j.cviu.2024.104255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning aims to describe video content using natural language, and effectively integrating information of visual and textual is crucial for generating accurate captions. However, we find that the existing methods over-rely on the language-prior information about the text acquired by training, resulting in the model tending to output high-frequency fixed phrases. In order to solve the above problems, we extract high-quality semantic information from multi-modal input and then build a semantic guidance mechanism to adapt to the contribution of visual semantics and text semantics to generate captions. We propose an Adaptive Semantic Guidance Network (ASGNet) for video captioning. The ASGNet consists of a Semantic Enhancement Encoder (SEE) and an Adaptive Control Decoder (ACD). Specifically, the SEE helps the model obtain high-quality semantic representations by exploring the rich semantic information from visual and textual. The ACD dynamically adjusts the contribution weights of semantics about visual and textual for word generation, guiding the model to adaptively focus on the correct semantic information. These two modules work together to help the model overcome the problem of over-reliance on language priors, resulting in more accurate video captions. Finally, we conducted extensive experiments on commonly used video captioning datasets. MSVD and MSR-VTT reached the state-of-the-art, and YouCookII also achieved good performance. These experiments fully verified the advantages of our method.},
  archive      = {J_CVIU},
  author       = {Yuanyuan Liu and Hong Zhu and Zhong Wu and Sen Du and Shuning Wu and Jing Shi},
  doi          = {10.1016/j.cviu.2024.104255},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104255},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive semantic guidance network for video captioning},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RS3Lip: Consistency for remote sensing image classification on part embeddings using self-supervised learning and CLIP. <em>CVIU</em>, <em>251</em>, 104254. (<a href='https://doi.org/10.1016/j.cviu.2024.104254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tackling domain and class generalization challenges remains a significant hurdle in the realm of remote sensing (RS). Recently, large-scale pre-trained vision-language models (VLMs), exemplified by CLIP, have showcased impressive zero-shot and few-shot generalization capabilities through extensive contrastive training. Existing literature emphasizes prompt learning as a means of enriching prompts with both domain and content information, particularly through smaller learnable projectors, thereby addressing multi-domain data challenges perceptibly. Along with this, it is observed that CLIP’s vision encoder fails to generalize well on the puzzled or corrupted RS images. In response, we propose a novel solution utilizing self-supervised learning (SSL) to ensure consistency for puzzled RS images in domain generalization (DG). This approach strengthens visual features, facilitating the generation of domain-invariant prompts. Our proposed RS 3 Lip, trained with small projectors featuring few layers, complements the pre-trained CLIP. It incorporates SSL and inpainting losses for visual features, along with a consistency loss between the features of SSL tasks and textual features. Empirical findings demonstrate that RS 3 Lip consistently outperforms state-of-the-art prompt learning methods across five benchmark optical remote sensing datasets, achieving improvements of at least by 1.3% in domain and class generalization tasks.},
  archive      = {J_CVIU},
  author       = {Ankit Jha and Mainak Singha and Avigyan Bhattacharya and Biplab Banerjee},
  doi          = {10.1016/j.cviu.2024.104254},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104254},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {RS3Lip: Consistency for remote sensing image classification on part embeddings using self-supervised learning and CLIP},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building extraction from remote sensing images with deep learning: A survey on vision techniques. <em>CVIU</em>, <em>251</em>, 104253. (<a href='https://doi.org/10.1016/j.cviu.2024.104253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building extraction from remote sensing images is a hot topic in the fields of computer vision and remote sensing. In recent years, driven by deep learning, the accuracy of building extraction has been improved significantly. This survey offers a review of recent deep learning-based building extraction methods, systematically covering concepts like representation learning, efficient data utilization, multi-source fusion, and polygonal outputs, which have been rarely addressed in previous surveys comprehensively, thereby complementing existing research. Specifically, we first briefly introduce the relevant preliminaries and the challenges of building extraction with deep learning. Then we construct a systematic and instructive taxonomy from two perspectives: (1) representation and learning-oriented perspective and (2) input and output-oriented perspective. With this taxonomy, the recent building extraction methods are summarized. Furthermore, we introduce the key attributes of extensive publicly available benchmark datasets, the performance of some state-of-the-art models and the free-available products. Finally, we prospect the future research directions from three aspects.},
  archive      = {J_CVIU},
  author       = {Yuan Yuan and Xiaofeng Shi and Junyu Gao},
  doi          = {10.1016/j.cviu.2024.104253},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104253},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Building extraction from remote sensing images with deep learning: A survey on vision techniques},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial intensity awareness for robust object detection. <em>CVIU</em>, <em>251</em>, 104252. (<a href='https://doi.org/10.1016/j.cviu.2024.104252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Like other computer vision models, object detectors are vulnerable to adversarial examples (AEs) containing imperceptible perturbations. These AEs can be generated with multiple intensities and then used to attack object detectors in real-world scenarios. One of the most effective ways to improve the robustness of object detectors is adversarial training (AT), which incorporates AEs into the training process. However, while previous AT-based models have shown certain robustness against adversarial attacks of a pre-specific intensity, they still struggle to maintain robustness when defending against adversarial attacks with multiple intensities. To address this issue, we propose a novel robust object detection method based on adversarial intensity awareness. We first explore potential schema to define the relationship between the neglected intensity information and actual evaluation metrics in AT. Then, we propose the sequential intensity loss (SI Loss) to represent and leverage the neglected intensity information in the AEs. Specifically, SI Loss deploys a sequential adaptive strategy to transform intensity into concrete learnable metrics in a discrete and cumulative manner. Additionally, a boundary smoothing algorithm is introduced to mitigate the influence of some particular AEs that challenging to be divided into a certain intensity level. Extensive experiments on PASCAL VOC and MS-COCO datasets substantially demonstrate the superior performance of our method over other defense methods against multi-intensity adversarial attacks.},
  archive      = {J_CVIU},
  author       = {Jikang Cheng and Baojin Huang and Yan Fang and Zhen Han and Zhongyuan Wang},
  doi          = {10.1016/j.cviu.2024.104252},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104252},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adversarial intensity awareness for robust object detection},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-domain conditional prior network for water-related optical image enhancement. <em>CVIU</em>, <em>251</em>, 104251. (<a href='https://doi.org/10.1016/j.cviu.2024.104251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water-related optical image enhancement improves the perception of information for human and machine vision, facilitating the development and utilization of marine resources. Due to the absorption and scattering of light in different water media, water-related optical images typically suffer from color distortion and low contrast. However, existing enhancement methods struggle to accurately simulate the imaging process in real underwater environments. To model and invert the degradation process of water-related optical images, we propose a Multi-domain Conditional Prior Network (MCPN) based on color vector prior and spectrum vector prior for enhancing water-related optical images. MCPN captures color, luminance, and structural priors across different feature spaces, resulting in a lightweight architecture that enhances water-related optical images while preserving critical information fidelity. Specifically, MCPN includes a modulated network, and a conditional network comprises two conditional units. The modulated network is a lightweight Convolutional Neural Network responsible for image reconstruction and local feature refinement. To avoid feature loss from multiple extractions, the Gaussian Conditional Unit (GCU) extracts atmospheric light and color shift information from the input image to form color prior vectors. Simultaneously, incorporating the Fast Fourier Transform, the Spectrum Conditional Unit (SCU) extracts scene brightness and structure to form spectrum prior vectors. These prior vectors are embedded into the modulated network to guide the image reconstruction. MCPN utilizes a PAL-based weighted Selective Supervision (PSS) strategy, selectively adjusting learning weights for images with excessive artificial noise. Experimental results demonstrate that MCPN outperforms existing methods, achieving excellent performance on the UIEB dataset. The PSS also shows fine feature matching in downstream applications.},
  archive      = {J_CVIU},
  author       = {Tianyu Wei and Dehuan Zhang and Zongxin He and Rui Zhou and Xiangfu Meng},
  doi          = {10.1016/j.cviu.2024.104251},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104251},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-domain conditional prior network for water-related optical image enhancement},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pruning networks at once via nuclear norm-based regularization and bi-level optimization. <em>CVIU</em>, <em>251</em>, 104247. (<a href='https://doi.org/10.1016/j.cviu.2024.104247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most network pruning methods focus on identifying redundant channels from pre-trained models, which is inefficient due to its three-step process: pre-training, pruning and fine-tuning, and reconfiguration. In this paper, we propose a pruning-from-scratch framework that unifies these processes into a single approach. We introduce nuclear norm-based regularization to maintain the representational capacity of large networks during pruning. Combining this with MACs-based regularization enhances the performance of the pruned network at the target compression rate. Our bi-level optimization approach simultaneously improves pruning efficiency and representation capacity. Experimental results show that our method achieves 75.4% accuracy on ImageNet without a pre-trained network, using only 41% of the original model’s computational cost. It also attains 0.5% higher performance in compressing the SSD network for object detection. Furthermore, we analyze the effects of nuclear norm-based regularization.},
  archive      = {J_CVIU},
  author       = {Donghyeon Lee and Eunho Lee and Jaehyuk Kang and Youngbae Hwang},
  doi          = {10.1016/j.cviu.2024.104247},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104247},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Pruning networks at once via nuclear norm-based regularization and bi-level optimization},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UATST: Towards unpaired arbitrary text-guided style transfer with cross-space modulation. <em>CVIU</em>, <em>251</em>, 104246. (<a href='https://doi.org/10.1016/j.cviu.2024.104246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing style transfer methods usually utilize style images to represent the target style. Since style images need to be prepared in advance and are confined to existing artworks, these methods are limited in flexibility and creativity. Compared with images, language is a more natural, common, and flexible way for humans to transmit information. Therefore, a better choice is to utilize text descriptions instead of style images to represent the target style. To this end, we propose a novel U npaired A rbitrary T ext-guided S tyle T ransfer ( UATST ) framework, which can render arbitrary photographs in the style of arbitrary text descriptions with one single model. To the best of our knowledge, this is the first model that achieves Arbitrary-Text-Per-Model with unpaired training data. In detail, we first use a pre-trained VGG network to map the content image into the VGG feature space, and use a pre-trained CLIP text encoder to map the text description into the CLIP feature space. Then we introduce a cross-space modulation module to bridge these two feature spaces, so that the content and style information in two different spaces can be seamlessly and adaptively combined for stylization. In addition, to learn better style representations, we introduce a new CLIP-based style contrastive loss to our model. Extensive qualitative and quantitative experiments verify the effectiveness and superiority of our method.},
  archive      = {J_CVIU},
  author       = {Haibo Chen and Lei Zhao},
  doi          = {10.1016/j.cviu.2024.104246},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104246},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {UATST: Towards unpaired arbitrary text-guided style transfer with cross-space modulation},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From bias to balance: Leverage representation learning for bias-free MoCap solving. <em>CVIU</em>, <em>251</em>, 104241. (<a href='https://doi.org/10.1016/j.cviu.2024.104241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion Capture (MoCap) is still dominated by optical MoCap as it remains the gold standard. However, the raw captured data even from such systems suffer from high-frequency noise and errors sourced from ghost or occluded markers. To that end, a post-processing step is often required to clean up the data, which is typically a tedious and time-consuming process. Some studies tried to address these issues in a data-driven manner, leveraging the availability of MoCap data. However, there is a high-level data redundancy in such data, as the motion cycle is usually comprised of similar poses (e.g. standing still). Such redundancies affect the performance of those methods, especially in the rarer poses. In this work, we address the issue of long-tailed data distribution by leveraging representation learning. We introduce a novel technique for imbalanced regression that does not require additional data or labels. Our approach uses a Mahalanobis distance-based method for automatically identifying rare samples and properly reweighting them during training, while at the same time, we employ high-order interpolation algorithms to effectively sample the latent space of a Variational Autoencoder (VAE) to generate new tail samples. We prove that the proposed approach can significantly improve the results, especially in the tail samples, while at the same time is a model-agnostic method and can be applied across various architectures.},
  archive      = {J_CVIU},
  author       = {Georgios Albanis and Nikolaos Zioulis and Spyridon Thermos and Anargyros Chatzitofis and Kostas Kolomvatsos},
  doi          = {10.1016/j.cviu.2024.104241},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104241},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {From bias to balance: Leverage representation learning for bias-free MoCap solving},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D pose nowcasting: Forecast the future to improve the present. <em>CVIU</em>, <em>251</em>, 104233. (<a href='https://doi.org/10.1016/j.cviu.2024.104233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technologies to enable safe and effective collaboration and coexistence between humans and robots have gained significant importance in the last few years. A critical component useful for realizing this collaborative paradigm is the understanding of human and robot 3D poses using non-invasive systems. Therefore, in this paper, we propose a novel vision-based system leveraging depth data to accurately establish the 3D locations of skeleton joints. Specifically, we introduce the concept of Pose Nowcasting, denoting the capability of the proposed system to enhance its current pose estimation accuracy by jointly learning to forecast future poses. The experimental evaluation is conducted on two different datasets, providing accurate and real-time performance and confirming the validity of the proposed method on both the robotic and human scenarios.},
  archive      = {J_CVIU},
  author       = {Alessandro Simoni and Francesco Marchetti and Guido Borghi and Federico Becattini and Lorenzo Seidenari and Roberto Vezzani and Alberto Del Bimbo},
  doi          = {10.1016/j.cviu.2024.104233},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104233},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {3D pose nowcasting: Forecast the future to improve the present},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Action assessment in rehabilitation: Leveraging machine learning and vision-based analysis. <em>CVIU</em>, <em>251</em>, 104228. (<a href='https://doi.org/10.1016/j.cviu.2024.104228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-hip replacement rehabilitation often depends on exercises under medical supervision. Yet, the lack of therapists, financial limits, and inconsistent evaluations call for a more user-friendly, accessible approach. Our proposed solution is a scalable, affordable system based on computer vision, leveraging machine learning and 2D cameras to provide tailored monitoring. This system is designed to address the shortcomings of conventional rehab methods, facilitating effective healthcare at home. The system’s key feature is the use of DTAN deep learning approach to synchronize exercise data over time, which guarantees precise analysis and evaluation. We also introduce a ‘Golden Feature’—a spatio-temporal element that embodies the essential movement of the exercise, serving as the foundation for aligning signals and identifying crucial exercise intervals. The system employs automated feature extraction and selection, offering valuable insights into the execution of exercises and enhancing the system’s precision. Moreover, it includes a multi-label ML model that not only predicts exercise scores but also forecasts therapists’ feedback for exercises performed partially. Performance of the proposed system is shown to be predict exercise scores with accuracy between 82% and 95%. Due to the automatic feature selection, and alignment methods, the proposed framework is easily scalable to additional exercises.},
  archive      = {J_CVIU},
  author       = {Alaa Kryeem and Noy Boutboul and Itai Bear and Shmuel Raz and Dana Eluz and Dorit Itah and Hagit Hel-Or and Ilan Shimshoni},
  doi          = {10.1016/j.cviu.2024.104228},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {104228},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Action assessment in rehabilitation: Leveraging machine learning and vision-based analysis},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SANet: Selective aggregation network for unsupervised object re-identification. <em>CVIU</em>, <em>250</em>, 104232. (<a href='https://doi.org/10.1016/j.cviu.2024.104232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in unsupervised object re-identification have witnessed remarkable progress, which usually focuses on capturing fine-grained semantic information through partitioning or relying on auxiliary networks for optimizing label consistency. However, incorporating extra complex partitioning mechanisms and models leads to non-negligible optimization difficulties, resulting in limited performance gains. To address these problems, this paper presents a Selective Aggregation Network (SANet) to obtain high-quality features and labels for unsupervised object re-identification, which explores primitive fine-grained information of large-scale pre-trained models such as CLIP and designs customized modifications. Specifically, we propose an adaptive selective aggregation module that chooses a set of tokens based on CLIP’s attention scores to aggregate discriminative global features. Built upon the representations output by the adaptive selective aggregation module, we design a dynamic weighted clustering algorithm to obtain accurate confidence-weighted pseudo-class centers for contrastive learning. In addition, a dual confidence judgment strategy is introduced to refine and correct the pseudo-labels by assigning three categories of samples through their noise degree. By this means, the proposed SANet enables discriminative feature extraction and clustering refinement for more precise classification without complex architectures such as feature partitioning or auxiliary models. Extensive experiments on existing standard unsupervised object re-identification benchmarks, including Market1501, MSMT17, and Veri776, demonstrate the effectiveness of the proposed SANet method, and SANet achieves state-of-the-art results over other strong competitors.},
  archive      = {J_CVIU},
  author       = {Minghui Lin and Jianhua Tang and Longbin Fu and Zhengrong Zuo},
  doi          = {10.1016/j.cviu.2024.104232},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104232},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SANet: Selective aggregation network for unsupervised object re-identification},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RetSeg3D: Retention-based 3D semantic segmentation for autonomous driving. <em>CVIU</em>, <em>250</em>, 104231. (<a href='https://doi.org/10.1016/j.cviu.2024.104231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LiDAR semantic segmentation is one of the crucial tasks for scene understanding in autonomous driving. Recent trends suggest that voxel- or fusion-based methods obtain improved performance. However, the fusion-based methods are computationally expensive. On the other hand, the voxel-based methods uniformly employ local operators (e.g., 3D SparseConv) without considering the varying-density property of LiDAR point clouds, which result in inferior performance, specifically on far away sparse points due to limited receptive field. To tackle this issue, we propose novel retention block to capture long-range dependencies, maintain the receptive field of far away sparse points and design RetSeg3D , a retention-based 3D semantic segmentation model for autonomous driving. Instead of vanilla attention mechanism to model long-range dependencies, inspired by RetNet, we design cubic window multi-scale retentive self-attention (CW-MSRetSA) module with bidirectional and 3D explicit decay mechanism to introduce 3D spatial distance related prior information into the model to improve not only the receptive field but also the model capacity. Our novel retention block maintains the receptive field which significantly improve the performance of far away sparse points. We conduct extensive experiments and analysis on three large-scale datasets: SemanticKITTI, nuScenes and Waymo. Our method not only outperforms existing methods on far away sparse points but also on close and medium distance points and efficiently runs in real time at 52.1 FPS on a RTX 4090 GPU.},
  archive      = {J_CVIU},
  author       = {Gopi Krishna Erabati and Helder Araujo},
  doi          = {10.1016/j.cviu.2024.104231},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104231},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {RetSeg3D: Retention-based 3D semantic segmentation for autonomous driving},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open-set domain adaptation with visual-language foundation models. <em>CVIU</em>, <em>250</em>, 104230. (<a href='https://doi.org/10.1016/j.cviu.2024.104230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) has proven to be very effective in transferring knowledge obtained from a source domain with labeled data to a target domain with unlabeled data. Owing to the lack of labeled data in the target domain and the possible presence of unknown classes, open-set domain adaptation (ODA) has emerged as a potential solution to identify these classes during the training phase. Although existing ODA approaches aim to solve the distribution shifts between the source and target domains, most methods fine-tuned ImageNet pre-trained models on the source domain with the adaptation on the target domain. Recent visual-language foundation models (VLFM), such as Contrastive Language-Image Pre-Training (CLIP), are robust to many distribution shifts and, therefore, should substantially improve the performance of ODA. In this work, we explore generic ways to adopt CLIP, a popular VLFM, for ODA. We investigate the performance of zero-shot prediction using CLIP, and then propose an entropy optimization strategy to assist the ODA models with the outputs of CLIP. The proposed approach achieves state-of-the-art results on various benchmarks, demonstrating its effectiveness in addressing the ODA problem.},
  archive      = {J_CVIU},
  author       = {Qing Yu and Go Irie and Kiyoharu Aizawa},
  doi          = {10.1016/j.cviu.2024.104230},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104230},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Open-set domain adaptation with visual-language foundation models},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale adaptive skeleton transformer for action recognition. <em>CVIU</em>, <em>250</em>, 104229. (<a href='https://doi.org/10.1016/j.cviu.2024.104229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer has demonstrated remarkable performance in various computer vision tasks. However, its potential is not fully explored in skeleton-based action recognition. On one hand, existing methods primarily utilize fixed function or pre-learned matrix to encode position information, while overlooking the sample-specific position information. On the other hand, these approaches focus on single-scale spatial relationships, while neglecting the discriminative fine-grained and coarse-grained spatial features. To address these issues, we propose a Multi-Scale Adaptive Skeleton Transformer (MSAST), including Adaptive Skeleton Position Encoding Module (ASPEM), Multi-Scale Embedding Module (MSEM), and Adaptive Relative Location Module (ARLM). ASPEM decouples spatial–temporal information in the position encoding procedure, which acquires inherent dependencies of skeleton sequences. ASPEM is also designed to be dependent on input tokens, which can learn sample-specific position information. The MSEM employs multi-scale pooling to generate multi-scale tokens that contain multi-grained features. Then, the spatial transformer captures multi-scale relations to address the subtle differences between various actions. Another contribution of this paper is that ARLM is presented to mine suitable location information for better recognition performance. Extensive experiments conducted on three benchmark datasets demonstrate that the proposed model achieves Top-1 accuracy of 94.9%/97.5% on NTU-60 C-Sub/C-View, 88.7%/91.6% on NTU-120 X-Sub/X-Set and 97.4% on NW-UCLA, respectively.},
  archive      = {J_CVIU},
  author       = {Xiaotian Wang and Kai Chen and Zhifu Zhao and Guangming Shi and Xuemei Xie and Xiang Jiang and Yifan Yang},
  doi          = {10.1016/j.cviu.2024.104229},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104229},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-scale adaptive skeleton transformer for action recognition},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2S-SGCN: A two-stage stratified graph convolutional network model for facial landmark detection on 3D data. <em>CVIU</em>, <em>250</em>, 104227. (<a href='https://doi.org/10.1016/j.cviu.2024.104227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Landmark Detection (FLD) algorithms play a crucial role in numerous computer vision applications, particularly in tasks such as face recognition, head pose estimation, and facial expression analysis. While FLD on images has long been the focus, the emergence of 3D data has led to a surge of interest in FLD on it due to its potential applications in various fields, including medical research. However, automating FLD in this context presents significant challenges, such as selecting suitable network architectures, refining outputs for precise landmark localization and optimizing computational efficiency. In response, this paper presents a novel approach, the 2-Stage Stratified Graph Convolutional Network ( 2S-SGCN ), which addresses these challenges comprehensively. The first stage aims to detect landmark regions using heatmap regression, which leverages both local and long-range dependencies through a stratified approach. In the second stage, 3D landmarks are precisely determined using a new post-processing technique, namely MSE-over-mesh . 2S-SGCN ensures both efficiency and suitability for resource-constrained devices. Experimental results on 3D scans from the public Facescape and Headspace datasets, as well as on point clouds derived from FLAME meshes collected in the DAD-3DHeads dataset, demonstrate that the proposed method achieves state-of-the-art performance across various conditions. Source code is accessible at https://github.com/gfacchi-dev/CVIU-2S-SGCN .},
  archive      = {J_CVIU},
  author       = {Jacopo Burger and Giorgio Blandano and Giuseppe Maurizio Facchi and Raffaella Lanzarotti},
  doi          = {10.1016/j.cviu.2024.104227},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104227},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {2S-SGCN: A two-stage stratified graph convolutional network model for facial landmark detection on 3D data},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual stage semantic information based generative adversarial network for image super-resolution. <em>CVIU</em>, <em>250</em>, 104226. (<a href='https://doi.org/10.1016/j.cviu.2024.104226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has revolutionized image super-resolution, yet challenges persist in preserving intricate details and avoiding overly smooth reconstructions. In this work, we introduce a novel architecture, the Residue and Semantic Feature-based Dual Subpixel Generative Adversarial Network (RSF-DSGAN), which emphasizes the critical role of semantic information in addressing these issues. The proposed generator architecture is designed with two sequential stages: the Premier Residual Stage and the Deuxième Residual Stage. These stages are concatenated to form a dual-stage upsampling process, substantially augmenting the model’s capacity for feature learning. A central innovation of our approach is the integration of semantic information directly into the generator. Specifically, feature maps derived from a pre-trained network are fused with the primary feature maps of the first stage, enriching the generator with high-level contextual cues. This semantic infusion enhances the fidelity and sharpness of reconstructed images, particularly in preserving object details and textures. Inter- and intra-residual connections are employed within these stages to maintain high-frequency details and fine textures. Additionally, spectral normalization is introduced in the discriminator to stabilize training. Comprehensive evaluations, including visual perception and mean opinion scores, demonstrate that RSF-DSGAN, with its emphasis on semantic information, outperforms current state-of-the-art super-resolution methods.},
  archive      = {J_CVIU},
  author       = {Shailza Sharma and Abhinav Dhall and Shikhar Johri and Vinay Kumar and Vivek Singh},
  doi          = {10.1016/j.cviu.2024.104226},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104226},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dual stage semantic information based generative adversarial network for image super-resolution},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scene-cGAN: A GAN for underwater restoration and scene depth estimation. <em>CVIU</em>, <em>250</em>, 104225. (<a href='https://doi.org/10.1016/j.cviu.2024.104225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their wide scope of application, the development of underwater models for image restoration and scene depth estimation is not a straightforward task due to the limited size and quality of underwater datasets, as well as variations in water colours resulting from attenuation, absorption and scattering phenomena in the water column. To address these challenges, we present an all-in-one conditional generative adversarial network (cGAN) called Scene-cGAN. Our cGAN is a physics-based multi-domain model designed for image dewatering, restoration and depth estimation. It comprises three generators and one discriminator. To train our Scene-cGAN, we use a multi-term loss function based on uni-directional cycle-consistency and a novel dataset. This dataset is constructed from RGB-D in-air images using spectral data and concentrations of water constituents obtained from real-world water quality surveys. This approach allows us to produce imagery consistent with the radiance and veiling light corresponding to representative water types. Additionally, we compare Scene-cGAN with current state-of-the-art methods using various datasets. Results demonstrate its competitiveness in terms of colour restoration and its effectiveness in estimating the depth information for complex underwater scenes.},
  archive      = {J_CVIU},
  author       = {Salma González-Sabbagh and Antonio Robles-Kelly and Shang Gao},
  doi          = {10.1016/j.cviu.2024.104225},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104225},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Scene-cGAN: A GAN for underwater restoration and scene depth estimation},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing scene text detectors with realistic text image synthesis using diffusion models. <em>CVIU</em>, <em>250</em>, 104224. (<a href='https://doi.org/10.1016/j.cviu.2024.104224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text detection techniques have garnered significant attention due to their wide-ranging applications. However, existing methods have a high demand for training data, and obtaining accurate human annotations is labor-intensive and time-consuming. As a solution, researchers have widely adopted synthetic text images as a complementary resource to real text images during pre-training. Yet there is still room for synthetic datasets to enhance the performance of scene text detectors. We contend that one main limitation of existing generation methods is the insufficient integration of foreground text with the background. To alleviate this problem, we present the Diff usion Model based Text Generator ( DiffText ), a pipeline that utilizes the diffusion model to seamlessly blend foreground text regions with the background’s intrinsic features. Additionally, we propose two strategies to generate visually coherent text with fewer spelling errors. With fewer text instances, our produced text images consistently surpass other synthetic data in aiding text detectors. Extensive experiments on detecting horizontal, rotated, curved, and line-level texts demonstrate the effectiveness of DiffText in producing realistic text images. Code is available at: https://github.com/99Franklin/DiffText .},
  archive      = {J_CVIU},
  author       = {Ling Fu and Zijie Wu and Yingying Zhu and Yuliang Liu and Xiang Bai},
  doi          = {10.1016/j.cviu.2024.104224},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104224},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enhancing scene text detectors with realistic text image synthesis using diffusion models},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised co-generation of foreground–background segmentation from text-to-image synthesis. <em>CVIU</em>, <em>250</em>, 104223. (<a href='https://doi.org/10.1016/j.cviu.2024.104223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-Image (T2I) synthesis is a challenging task requiring modelling both textual and image domains and their relationship. The substantial improvement in image quality achieved by recent works has paved the way for numerous applications such as language-aided image editing, computer-aided design, text-based image retrieval, and training data augmentation. In this work, we ask a simple question: Along with realistic images, can we obtain any useful by-product ( e.g. foreground/background or multi-class segmentation masks, detection labels) in an unsupervised way that will also benefit other computer vision tasks and applications?. In an attempt to answer this question, we explore generating realistic images and their corresponding foreground/background segmentation masks from the given text. To achieve this, we experiment the concept of co-segmentation along with GAN. Specifically, a novel GAN architecture called Co-Segmentation Inspired GAN (COS-GAN) is proposed that generates two or more images simultaneously from different noise vectors and utilises a spatial co-attention mechanism between the image features to produce realistic segmentation masks for each of the generated images. The advantages of such an architecture are two-fold: (1) The generated segmentation masks can be used to focus on foreground and background exclusively to improve the quality of generated images, and (2) the segmentation masks can be used as a training target for other tasks, such as object localisation and segmentation. Extensive experiments conducted on CUB, Oxford-102, and COCO datasets show that COS-GAN is able to improve visual quality and generate reliable foreground/background masks for the generated images.},
  archive      = {J_CVIU},
  author       = {Yeruru Asrar Ahmed and Anurag Mittal},
  doi          = {10.1016/j.cviu.2024.104223},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104223},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unsupervised co-generation of foreground–background segmentation from text-to-image synthesis},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging vision-language prompts for real-world image restoration and enhancement. <em>CVIU</em>, <em>250</em>, 104222. (<a href='https://doi.org/10.1016/j.cviu.2024.104222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant advancements have been made in image restoration methods aimed at removing adverse weather effects. However, due to natural constraints, it is challenging to collect real-world datasets for adverse weather removal tasks. Consequently, existing methods predominantly rely on synthetic datasets, which struggle to generalize to real-world data, thereby limiting their practical utility. While some real-world adverse weather removal datasets have emerged, their design, which involves capturing ground truths at a different moment, inevitably introduces interfering discrepancies between the degraded images and the ground truths. These discrepancies include variations in brightness, color, contrast, and minor misalignments. Meanwhile, real-world datasets typically involve complex rather than singular degradation types. In many samples, degradation features are not overt, which poses immense challenges to real-world adverse weather removal methodologies. To tackle these issues, we introduce the recently prominent vision-language model, CLIP, to aid in the image restoration process. An expanded and fine-tuned CLIP model acts as an ‘expert’, leveraging the image priors acquired through large-scale pre-training to guide the operation of the image restoration model. Additionally, we generate a set of pseudo-ground-truths on sequences of degraded images to further alleviate the difficulty for the model in fitting the data. To imbue the model with more prior knowledge about degradation characteristics, we also incorporate additional synthetic training data. Lastly, the progressive learning and fine-tuning strategies employed during training enhance the model’s final performance, enabling our method to surpass existing approaches in both visual quality and objective image quality assessment metrics.},
  archive      = {J_CVIU},
  author       = {Yanyan Wei and Yilin Zhang and Kun Li and Fei Wang and Shengeng Tang and Zhao Zhang},
  doi          = {10.1016/j.cviu.2024.104222},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104222},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Leveraging vision-language prompts for real-world image restoration and enhancement},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leaf cultivar identification via prototype-enhanced learning. <em>CVIU</em>, <em>250</em>, 104221. (<a href='https://doi.org/10.1016/j.cviu.2024.104221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leaf cultivar identification, as a typical task of ultra-fine-grained visual classification (UFGVC), is facing a huge challenge due to the high similarity among different varieties. In practice, an instance may be related to multiple varieties to varying degrees, especially in the UFGVC datasets. However, deep learning methods trained on one-hot labels fail to reflect patterns shared across categories and thus perform poorly on this task. As an analogy to natural language processing (NLP), by capturing the co-relation between labels, label embedding can select the most informative words and neglect irrelevant ones when predicting different labels. Based on this intuition, we propose a novel method named Prototype-enhanced Learning (PEL), which is predicated on the assumption that label embedding encoded with the inter-class relationships would force the image classification model to focus on discriminative patterns. In addition, a new prototype update module is put forward to learn inter-class relations by capturing label semantic overlap and iteratively update prototypes to generate continuously enhanced soft targets. Prototype-enhanced soft labels not only contain original one-hot label information, but also introduce rich inter-category semantic association information, thus providing more effective supervision for deep model training. Extensive experimental results on 7 public datasets show that our method can significantly improve the performance on the task of ultra-fine-grained visual classification. The code is available at https://github.com/YIYIZH/PEL .},
  archive      = {J_CVIU},
  author       = {Yiyi Zhang and Zhiwen Ying and Ying Zheng and Cuiling Wu and Nannan Li and Fangfang Wang and Jun Wang and Xianzhong Feng and Xiaogang Xu},
  doi          = {10.1016/j.cviu.2024.104221},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104221},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Leaf cultivar identification via prototype-enhanced learning},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seam estimation based on dense matching for parallax-tolerant image stitching. <em>CVIU</em>, <em>250</em>, 104219. (<a href='https://doi.org/10.1016/j.cviu.2024.104219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image stitching with large parallax poses a significant challenge in the field of computer vision. Existing seam-based approaches attempt to address parallax artifacts by stitching images along seams. However, issues such as object mismatches, disappearances, and duplications still arise occasionally, primarily due to inaccurate alignment of dense pixels or inappropriate seam estimation methods. In this paper, we propose a robust seam-based parallax-tolerant image stitching method that leverages dense flow estimation from state-of-the-art approaches. Firstly, we develop a seam estimation method that does not require pre-estimation of image warping model. Instead, it directly estimates the seam by measuring the local smoothness of the optical flow field and incorporating a penalty term for duplications. Subsequently, we design an iterative algorithm that utilizes the location of estimated seam to solve a spatial smooth warping model and eliminate outlier corresponding pairs. By employing this approach, we effectively address the intertwined challenges of estimating the warping model and seam. Experiment on real-world images shows that our proposed method achieves superior local alignment accuracy near the stitching seam and outperforms other state-of-the-art techniques on visual stitching result. Code is available at https://github.com/zhihao0512/dense-matching-image-stitching .},
  archive      = {J_CVIU},
  author       = {Zhihao Zhang and Jie He and Mouquan Shen and Xianqiang Yang},
  doi          = {10.1016/j.cviu.2024.104219},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104219},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Seam estimation based on dense matching for parallax-tolerant image stitching},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced local multi-windows attention network for lightweight image super-resolution. <em>CVIU</em>, <em>250</em>, 104217. (<a href='https://doi.org/10.1016/j.cviu.2024.104217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the global self-attention mechanism can capture long-distance dependencies well, Transformer-based methods have achieved remarkable performance in many vision tasks, including single-image super-resolution (SISR). However, there are strong local self-similarities in images, if the global self-attention mechanism is still used for image processing, it may lead to excessive use of computing resources on parts of the image with weak correlation. Especially in the high-resolution large-size image, the global self-attention will lead to a large number of redundant calculations. To solve this problem, we propose the Enhanced Local Multi-windows Attention Network (ELMA), which contains two main designs. First, different from the traditional self-attention based on square window partition, we propose a Multi-windows Self-Attention (M-WSA) which uses a new window partitioning mechanism to obtain different types of local long-distance dependencies. Compared with original self-attention mechanisms commonly used in other SR networks, M-WSA reduces computational complexity and achieves superior performance through analysis and experiments. Secondly, we propose a Spatial Gated Network (SGN) as a feed-forward network, which can effectively overcome the problem of intermediate channel redundancy in traditional MLP, thereby improving the parameter utilization and computational efficiency of the network. Meanwhile, SGN introduces spatial information into the feed-forward network that traditional MLP cannot obtain. It can better understand and use the spatial structure information in the image, and enhances the network performance and generalization ability. Extensive experiments show that ELMA achieves competitive performance compared to state-of-the-art methods while maintaining fewer parameters and computational costs.},
  archive      = {J_CVIU},
  author       = {Yanheng Lv and Lulu Pan and Ke Xu and Guo Li and Wenbo Zhang and Lingxiao Li and Le Lei},
  doi          = {10.1016/j.cviu.2024.104217},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104217},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enhanced local multi-windows attention network for lightweight image super-resolution},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilevel progressive homography estimation via correlative region-focused transformer. <em>CVIU</em>, <em>250</em>, 104209. (<a href='https://doi.org/10.1016/j.cviu.2024.104209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel correlative region-focused transformer for accurate homography estimation by a bilevel progressive architecture. Existing methods typically consider the entire image features to establish correlations for a pair of input images, but irrelevant regions often introduce mismatches and outliers. In contrast, our network effectively mitigates the negative impact of irrelevant regions through a bilevel progressive homography estimation architecture. Specifically, in the outer iteration, we progressively estimate the homography matrix at different feature scales; in the inner iteration, we dynamically extract correlative regions and progressively focus on their corresponding features from both inputs. Moreover, we develop a quadtree attention mechanism based on the transformer to explicitly capture the correspondence between the input images, localizing and cropping the correlative regions for the next iteration. This progressive training strategy enhances feature consistency and enables precise alignment with comparable inference rates. Extensive experiments on qualitative and quantitative comparisons show that the proposed method exhibits competitive alignment results while reducing the mean average corner error (MACE) on the MS-COCO dataset compared to previous methods, without increasing additional parameter cost.},
  archive      = {J_CVIU},
  author       = {Qi Jia and Xiaomei Feng and Wei Zhang and Yu Liu and Nan Pu and Nicu Sebe},
  doi          = {10.1016/j.cviu.2024.104209},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {104209},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Bilevel progressive homography estimation via correlative region-focused transformer},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
