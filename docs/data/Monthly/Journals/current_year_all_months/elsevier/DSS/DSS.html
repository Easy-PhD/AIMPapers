<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DSS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dss">DSS - 4</h2>
<ul>
<li><details>
<summary>
(2026). AI nudging and decision quality: Evidence from randomized experiments in online recommendation setting. <em>DSS</em>, <em>200</em>, 114565. (<a href='https://doi.org/10.1016/j.dss.2025.114565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the impacts of AI nudging on customer purchase decisions. Digital nudging is a well-established technique used to alter people's behaviors in a predictable way. With the rapid development of Artificial Intelligence/Machine Learning (AI/ML) and the widespread integration of the “black box” algorithm in the digital choice architecture, personalized targeting nudges can vastly influence individual and collective behaviors and lead to undesired consequences. AI nudge refers to the situation when human outsources developing and implementing nudges to AI/ML systems. Drawing upon the literature on nudge and recommendation agents/systems in IS, this study investigated the impact of two types of recommendation badges on user decision quality: AI nudge (e.g., Amazon's Choice ) and non-AI nudge (e.g., Best Seller ). We found that these two badges can lead to different user perceptions of transparency and thus affect the choice confidence of product selection. In addition, the effect of perceived transparency on choice confidence is contingent upon the mismatch/match between the recommendation and users' preferences, with perceived transparency exerting significantly higher influence on choice confidence in the preference match condition. We tested our research model using a randomized experiment and post-task survey data collected from 837 US-based college students with online shopping experience. This is the first empirical study examining the impact of AI nudging on user decision-making on e-commerce platforms and will contribute to the nudge literature and biased recommendation research in IS. The study also brings ethical implications to the use of AI/ML models and calls for careful oversight on delegating the power of nudging to AI in guiding online user behavior.},
  archive      = {J_DSS},
  author       = {Yuxiao Luo and Nanda Kumar and Adel Yazdanmehr},
  doi          = {10.1016/j.dss.2025.114565},
  journal      = {Decision Support Systems},
  month        = {1},
  pages        = {114565},
  shortjournal = {Decis. Supp. Syst.},
  title        = {AI nudging and decision quality: Evidence from randomized experiments in online recommendation setting},
  volume       = {200},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Leveraging large language models for enhanced process model comprehension. <em>DSS</em>, <em>200</em>, 114563. (<a href='https://doi.org/10.1016/j.dss.2025.114563'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Business Process Management (BPM), effectively comprehending process models is crucial yet poses significant challenges, particularly as organizations scale and processes become more complex. This paper introduces a novel framework utilizing the advanced capabilities of Large Language Models (LLMs) to enhance the comprehension of complex process models. We present different methods for abstracting business process models into a format accessible to LLMs, and we implement advanced prompting strategies specifically designed to optimize LLM performance within our framework. Additionally, we present a tool, AIPA, that implements our proposed framework and allows for conversational process querying. We evaluate our framework and tool through: i) an automatic evaluation comparing different LLMs, model abstractions, and prompting strategies; ii) a qualitative analysis assessing the ability to identify critical quality issues in process models; and iii) a user study designed to assess AIPA’s effectiveness comprehensively. Results demonstrate our framework’s ability to improve the comprehension and understanding of process models, pioneering new pathways for integrating AI technologies into the BPM field.},
  archive      = {J_DSS},
  author       = {Humam Kourani and Alessandro Berti and Jasmin Hennrich and Wolfgang Kratsch and Robin Weidlich and Chiao-Yun Li and Ahmad Arslan and Wil M.P. van der Aalst and Daniel Schuster},
  doi          = {10.1016/j.dss.2025.114563},
  journal      = {Decision Support Systems},
  month        = {1},
  pages        = {114563},
  shortjournal = {Decis. Supp. Syst.},
  title        = {Leveraging large language models for enhanced process model comprehension},
  volume       = {200},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Artificial intelligence agents or human agents? impact of online customer service agents on crowdfunding performance. <em>DSS</em>, <em>200</em>, 114562. (<a href='https://doi.org/10.1016/j.dss.2025.114562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Artificial Intelligence (AI) agents are being increasingly deployed in crowdfunding platforms to address labor shortages, knowledge about their scope and limits is still limited. Across a secondary data analysis and three experiments (total N = 1027), we reveal that AI (vs. human) agents are more effective in reward-based (vs. donation-based) crowdfunding. This effect can be parallelly mediated by perceptions of warmth and competence, with AI agents evoking higher competence but weaker warmth perceptions. Importantly, anthropomorphic AI agents serve as an effective intervention to alleviate AI's negative impact on donation-based crowdfunding by enhancing warmth perceptions. Finally, we show that human agents outperform AI agents in boosting donation-based funding performance only for those with an interdependent versus independent self-construal. Overall, these findings expand the theoretical framework on AI applications in crowdfunding and offer actionable insights for fundraisers and platform operators to optimize agent deployment.},
  archive      = {J_DSS},
  author       = {Wei Wang and Yao Tong and Jian Mou},
  doi          = {10.1016/j.dss.2025.114562},
  journal      = {Decision Support Systems},
  month        = {1},
  pages        = {114562},
  shortjournal = {Decis. Supp. Syst.},
  title        = {Artificial intelligence agents or human agents? impact of online customer service agents on crowdfunding performance},
  volume       = {200},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Decoding LLMs' verbal deception in online reviews. <em>DSS</em>, <em>200</em>, 114529. (<a href='https://doi.org/10.1016/j.dss.2025.114529'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of fake online reviews, a long-standing threat to platform trust, is now exacerbated by large language models (LLMs) capable of generating highly convincing deceptive text. Understanding the linguistic strategies LLMs employ is crucial for developing effective mitigation. To address this gap, we develop an explainable artificial intelligence (XAI)-based computational framework, grounded in deception detection theories, to analyze and distinguish the deceptive patterns of LLMs. A core component of our methodology is a novel Turing-style test designed for LLM-generated online reviews. When applied to three purpose-built datasets, our framework not only achieves high detection accuracy for both human-authored fakes (96.57 %) and LLM-generated fakes (96.13 %)—substantially outperforming two current general-purpose detectors—but also indicates that LLMs possess a human-level deceptive capability (metric gaps <0.72 %). The analysis reveals that while cues related to cognitive load and perceptual-contextual details are powerful discriminators for both human and machine deception, certainty uniquely signals LLM-generated text, whereas emotion is a primary predictor only for human fakes. These findings support a central dissociation hypothesis between linguistic generation and cognitive representation: LLM deception is characterized by strategies like surface-level fluency, content realism without experiential grounding, and positivity bias. This study probes the mechanistic differences between human and machine deception, delivers a robust computational detection framework, and advances the theoretical discourse on AI's capacity for deceit.},
  archive      = {J_DSS},
  author       = {Yinghui Huang and Jinyi Zhou and Wanghao Dong and Weiqing Li and Maomao Chi and Changbin Jiang and Weijun Wang and Shasha Deng},
  doi          = {10.1016/j.dss.2025.114529},
  journal      = {Decision Support Systems},
  month        = {1},
  pages        = {114529},
  shortjournal = {Decis. Supp. Syst.},
  title        = {Decoding LLMs' verbal deception in online reviews},
  volume       = {200},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
