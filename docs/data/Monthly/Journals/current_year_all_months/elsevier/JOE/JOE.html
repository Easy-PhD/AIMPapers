<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JOE</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="joe">JOE - 133</h2>
<ul>
<li><details>
<summary>
(2025). Structural periodic vector autoregressions. <em>JOE</em>, <em>252</em>, 106099. (<a href='https://doi.org/10.1016/j.jeconom.2025.106099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While seasonality inherent to raw macroeconomic data is commonly removed by seasonal adjustment techniques before it is used for structural inference, this may distort valuable information in the data. As an alternative method to commonly used structural vector autoregressions (SVARs) for seasonally adjusted data, we propose to model potential periodicity in seasonally unadjusted (raw) data directly by structural periodic vector autoregressions (SPVARs). This approach does not only allow for periodically time-varying intercepts, but also for periodic autoregressive parameters and innovations variances. As this larger flexibility leads to an increased number of parameters, we propose linearly constrained estimation techniques. Moreover, based on SPVARs, we provide two novel identification schemes and propose a general framework for impulse response analyses that allows for direct consideration of seasonal patterns. We provide asymptotic theory for SPVAR estimators and impulse responses under flexible linear restrictions and introduce a test for seasonality in impulse responses. For the construction of confidence intervals, we discuss several residual-based (seasonal) bootstrap methods and prove their bootstrap consistency under different assumptions. A real data application shows that useful information about the periodic structure in the data may be lost when relying on common seasonal adjustment methods.},
  archive      = {J_JOE},
  author       = {Daniel Dzikowski and Carsten Jentsch},
  doi          = {10.1016/j.jeconom.2025.106099},
  journal      = {Journal of Econometrics},
  month        = {11},
  pages        = {106099},
  shortjournal = {J. Econ.},
  title        = {Structural periodic vector autoregressions},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Misspecification-robust bootstrap t-test for irrelevant factor in linear stochastic discount factor models. <em>JOE</em>, <em>252</em>, 106097. (<a href='https://doi.org/10.1016/j.jeconom.2025.106097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the applicability of the bootstrap approach to test for irrelevant risk factors that are potentially useless in misspecified linear stochastic discount factor (SDF) models. In the literature, the misspecification-robust inference with useless factors is known to give rise to nonstandard limiting distributions bounded stochastically to compute critical values. We show how and to what extent the wild bootstrap yields a more accurate approximation of the distribution of t -statistics when testing for an unpriced factor in the context of linear SDF models. Simulation experiments and empirical tests are also used to document the relevance of the bootstrap method.},
  archive      = {J_JOE},
  author       = {Antoine A. Djogbenou and Ulrich Hounyo},
  doi          = {10.1016/j.jeconom.2025.106097},
  journal      = {Journal of Econometrics},
  month        = {11},
  pages        = {106097},
  shortjournal = {J. Econ.},
  title        = {Misspecification-robust bootstrap t-test for irrelevant factor in linear stochastic discount factor models},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On-line detection of changes in the shape of intraday volatility curves. <em>JOE</em>, <em>252</em>, 106089. (<a href='https://doi.org/10.1016/j.jeconom.2025.106089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise an on-line detector for temporal instability in the shape of average intraday volatility curves under a general semimartingale setup for the price-volatility dynamics. We adopt a block-based strategy to estimate volatility nonparametrically from the intraday observations over local time windows with asymptotically shrinking size. Our detector then tracks sequential changes in running means of the intraday volatility curve estimates. Asymptotic size and power properties of the detector follow from a weak form invariance principle, which is established under the strong mixing condition aligned with our semimartingale setup. Simulation and empirical results demonstrate good finite-sample performance of the proposed detection method.},
  archive      = {J_JOE},
  author       = {Torben G. Andersen and Yingwen Tan and Viktor Todorov and Zhiyuan Zhang},
  doi          = {10.1016/j.jeconom.2025.106089},
  journal      = {Journal of Econometrics},
  month        = {11},
  pages        = {106089},
  shortjournal = {J. Econ.},
  title        = {On-line detection of changes in the shape of intraday volatility curves},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High dimensional factor analysis with weak factors. <em>JOE</em>, <em>252</em>, 106086. (<a href='https://doi.org/10.1016/j.jeconom.2025.106086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the principal components (PC) estimator for high dimensional approximate factor models with weak factors in that the factor loading ( Λ 0 ) scales sublinearly in the number N of cross-section units, i.e., Λ 0 ⊤ Λ 0 / N α is positive definite in the limit for some α ∈ ( 0 , 1 ) . While the consistency and asymptotic normality of these estimates are by now well known when the factors are strong, i.e., α = 1 , the statistical properties for weak factors remain less explored. Here, we show that the PC estimator maintains consistency and asymptotic normality for any α ∈ ( 0 , 1 ) , provided suitable conditions regarding the dependence structure in the noise are met. This complements earlier result by Onatski (2012) that the PC estimator is inconsistent when α = 0 , and the more recent work by Bai and Ng (2023) who established the asymptotic normality of the PC estimator when α ∈ ( 1 / 2 , 1 ) . Our proof strategy integrates the traditional eigendecomposition-based approach for factor models with leave-one-out analysis similar in spirit to those used in matrix completion and other settings. This combination allows us to deal with factors weaker than the former and at the same time relax the incoherence and independence assumptions often associated with the later.},
  archive      = {J_JOE},
  author       = {Jungjun Choi and Ming Yuan},
  doi          = {10.1016/j.jeconom.2025.106086},
  journal      = {Journal of Econometrics},
  month        = {11},
  pages        = {106086},
  shortjournal = {J. Econ.},
  title        = {High dimensional factor analysis with weak factors},
  volume       = {252},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Support vector decision making. <em>JOE</em>, <em>251</em>, 106087. (<a href='https://doi.org/10.1016/j.jeconom.2025.106087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper develops a support vector machine (SVM) for binary decision-making within a utility framework. Given an information set, a decision-maker first predicts a binary outcome and then selects a binary action based on this prediction to maximize expected utility, where the utility function can depend on the action taken, observable covariates, and the binary outcome subsequently realized. The proposed maximum utility SVM differs from the traditional SVM in four key aspects. First, as a conceptual innovation, it incorporates the optimal cutoff function as a separate and special covariate. Second, there is a sign restriction on this special covariate. Third, it accounts for the dependence of the utility-induced loss on both the covariates and the binary outcome. Finally, it allows the margin to differ across different classes of outcomes. The paper proves that the proposed method is Bayes-consistent under the maximum utility criterion and establishes a finite-sample generalization bound. A simulation study shows that the proposed method outperforms existing methods under the data-generating processes considered in the literature.},
  archive      = {J_JOE},
  author       = {Yixiao Sun},
  doi          = {10.1016/j.jeconom.2025.106087},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106087},
  shortjournal = {J. Econ.},
  title        = {Support vector decision making},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification and inference for semiparametric single index transformation models. <em>JOE</em>, <em>251</em>, 106084. (<a href='https://doi.org/10.1016/j.jeconom.2025.106084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a semiparametric single index model in which the dependent variable is subject to a nonparametric transformation. The model has the form G 0 ( Y ) = g 0 ( X ⊤ θ 0 ) + e , where X is a random vector of regressors, Y is the dependent variable and e is the random noise, the monotonic function G 0 , the smooth function g 0 and the index vector θ 0 are all unknown. This model is quite general in the sense that it nests many popular regression models as special cases. We first propose identification strategies for the three unknown quantities, based on which estimators are then constructed. The kernel density weighted average derivative estimator of δ (proportional to θ 0 ) has a V -statistic representation and its asymptotical normality is established under the small bandwidth asymptotics. The kernel estimator of the transformation function G 0 is a functional of the conditional distribution estimator of Y given X ⊤ θ 0 and is shown to be n -consistent and asymptotically normal. The sieve estimator of g 0 is shown to enjoy the standard nonparametric asymptotic properties. A specification test for the single index structure and extension to allow for endogeneous regressors are also developed. In addition, data-driven choices of the smoothing parameters are discussed. Simulation results illustrate the nice finite sample performance of the proposed estimators and specification test. An empirical application to studying the impact of family income on child achievement demonstrates the practical merits of the proposed model.},
  archive      = {J_JOE},
  author       = {Yingqian Lin and Yundong Tu},
  doi          = {10.1016/j.jeconom.2025.106084},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106084},
  shortjournal = {J. Econ.},
  title        = {Identification and inference for semiparametric single index transformation models},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning based residuals in non-linear factor models: Precision matrix estimation of returns with low signal-to-noise ratio. <em>JOE</em>, <em>251</em>, 106083. (<a href='https://doi.org/10.1016/j.jeconom.2025.106083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a consistent estimator and rate of convergence for the precision matrix of asset returns in large portfolios using a non-linear factor model within the deep learning framework. Our estimator remains valid even in low signal-to-noise ratio environments typical for financial markets and is compatible with the weak factor framework. Our theoretical analysis establishes uniform bounds on expected estimation risk based on deep neural networks for an expanding number of assets. Additionally, we provide a new consistent data-dependent estimator of error covariance in deep neural networks. Our models demonstrate superior accuracy in extensive simulations and the empirical application.},
  archive      = {J_JOE},
  author       = {Mehmet Caner and Maurizio Daniele},
  doi          = {10.1016/j.jeconom.2025.106083},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106083},
  shortjournal = {J. Econ.},
  title        = {Deep learning based residuals in non-linear factor models: Precision matrix estimation of returns with low signal-to-noise ratio},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shrinkage estimation of spatial panel data models with multiple structural breaks and a multifactor error structure. <em>JOE</em>, <em>251</em>, 106082. (<a href='https://doi.org/10.1016/j.jeconom.2025.106082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates spatial panel data models with a multifactor error structure and multiple structural breaks occurring in the coefficients of both spatial lagged and explanatory variables. While extensive research has addressed cross-sectional dependence in panel data, including approaches that integrate spatial and factor structures within a single framework, few studies account for time-varying model parameters and achieving consistent estimation remains a significant challenge. To address the dual challenges of endogeneity and time heterogeneity, we propose a novel penalized generalized method of moments estimation with common correlated effects (PGMM-CCEX). Specifically, this method addresses the endogeneity issue by utilizing the cross-sectional averages of regressors as factor proxies when constructing the internal instrumental variables, while employing adaptive group fused Lasso to detect multiple structural breaks. The PGMM-CCEX method consistently estimates both the number of breaks and their locations. Furthermore, the post-PGMM-CCEX regime-specific coefficient estimates are consistent and asymptotically follow a normal distribution. Notably, the method remains valid even when factor loadings vary over time, whether synchronously or asynchronously with the parameters of interest. Monte Carlo simulations confirm the satisfactory finite-sample performance of the proposed PGMM-CCEX method. Finally, we apply our method to analyze cross-country economic growth across 106 countries from 1970 to 2019, revealing the time-varying influence of key economic factors on growth dynamics.},
  archive      = {J_JOE},
  author       = {Siqi Dai and Yongmiao Hong and Haiqi Li and Chaowen Zheng},
  doi          = {10.1016/j.jeconom.2025.106082},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106082},
  shortjournal = {J. Econ.},
  title        = {Shrinkage estimation of spatial panel data models with multiple structural breaks and a multifactor error structure},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On regression-adjusted imputation estimators of average treatment effects. <em>JOE</em>, <em>251</em>, 106080. (<a href='https://doi.org/10.1016/j.jeconom.2025.106080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imputing missing potential outcomes using an estimated regression function is a natural idea for estimating causal effects. In the literature, estimators that combine imputation and regression adjustments are believed to be comparable to augmented inverse probability weighting. Accordingly, people for a long time conjectured that such estimators, while avoiding directly constructing the weights, are also doubly robust (Imbens, 2004; Stuart, 2010). Generalizing an earlier result of the authors (Lin et al., 2023), this paper formalizes this conjecture, showing that a large class of regression-adjusted imputation methods are indeed doubly robust for estimating average treatment effects. In addition, they are provably semiparametrically efficient as long as both the density and regression models are correctly specified. Notable examples of imputation methods covered by our theory include kernel matching, (weighted) nearest neighbor matching, local linear matching, and (honest) random forests.},
  archive      = {J_JOE},
  author       = {Zhexiao Lin and Fang Han},
  doi          = {10.1016/j.jeconom.2025.106080},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106080},
  shortjournal = {J. Econ.},
  title        = {On regression-adjusted imputation estimators of average treatment effects},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile regression with group-level treatments. <em>JOE</em>, <em>251</em>, 106079. (<a href='https://doi.org/10.1016/j.jeconom.2025.106079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To study distributional effects of group level treatments, Chetverikov et al. (2016) proposed a grouped instrumental variables quantile regression estimator, a quantile extension of the Hausman and Taylor’s 1981 instrumental variables estimator for panel data. However, their approach only allows for heterogenous distributional effects of group-level treatments that correspond to individual-level unobserved characteristics, but not group-level unobserved characteristics. In this article, we propose a quantile regression model that allows for heterogenous distributional effects of group-level treatments associated with both individual-level and group-level unobserved characteristics. We propose two-step quantile regression and instrumental variables quantile regression estimators, depending on whether the group-level treatments are correlated with the group-level unobserved characteristics. Large sample properties are presented and simulation results indicate our estimators perform well in finite samples. We apply our method to study the impact of Chinese import competition on the U.S. local wage distribution, and uncover both significant individual-level and group-level heterogeneity of the group treatment effects.},
  archive      = {J_JOE},
  author       = {Songnian Chen},
  doi          = {10.1016/j.jeconom.2025.106079},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106079},
  shortjournal = {J. Econ.},
  title        = {Quantile regression with group-level treatments},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial panel data models with structural change. <em>JOE</em>, <em>251</em>, 106078. (<a href='https://doi.org/10.1016/j.jeconom.2025.106078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial panel data models have gained widespread application in social sciences, particularly in economics, due to their ability to capture spatial dependencies. However, existing studies on spatial models typically rely on the assumption of parameter stability, which may be overly restrictive given the well-documented prevalence of structural changes in economic relationships. This paper addresses this limitation by proposing and analyzing spatial panel data models that explicitly incorporate structural change. The primary focus of this paper is on a static spatial autoregressive (SAR) panel data model, which we estimate using the quasi-maximum likelihood (QML) method. We establish a comprehensive asymptotic theory for the QML estimators, including proofs of consistency, convergence rates, and limiting distributions, under a large- N and large- T framework. Furthermore, we extend our theoretical framework in two important directions: dynamic spatial panel data models and settings with large- N and fixed- T . Additionally, we develop hypothesis testing procedures for detecting structural change, proposing three sup-type test statistics for this purpose. To validate our theoretical results, we conduct extensive Monte Carlo simulations, which demonstrate that the QML estimators perform well in finite samples. These findings underscore the practical relevance and robustness of our proposed methodology.},
  archive      = {J_JOE},
  author       = {Luya Wang and Kunpeng Li},
  doi          = {10.1016/j.jeconom.2025.106078},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106078},
  shortjournal = {J. Econ.},
  title        = {Spatial panel data models with structural change},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bregman model averaging for forecast combination. <em>JOE</em>, <em>251</em>, 106076. (<a href='https://doi.org/10.1016/j.jeconom.2025.106076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a unified model averaging (MA) approach for a broad class of forecasting targets. This approach is established by minimizing an asymptotic risk based on the expected Bregman divergence of a combined forecast, relative to the optimal forecast of the forecasting target, under local(-to-zero) asymptotics. It can be flexibly applied to develop effective MA methods across various forecasting contexts, including but not limited to univariate and multivariate mean forecasting, volatility forecasting, probabilistic forecasting, and density forecasting. As illustrative examples, we present a series of simulation experiments and empirical cases that demonstrate strong numerical performance of our approach in forecasting.},
  archive      = {J_JOE},
  author       = {Yi-Ting Chen and Chu-An Liu and Jiun-Hua Su},
  doi          = {10.1016/j.jeconom.2025.106076},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106076},
  shortjournal = {J. Econ.},
  title        = {Bregman model averaging for forecast combination},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified test for regression discontinuity designs. <em>JOE</em>, <em>251</em>, 106074. (<a href='https://doi.org/10.1016/j.jeconom.2025.106074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic tests for regression discontinuity design face a size-control problem. We document a massive over-rejection of the diagnostic restriction among empirical studies in the top five economics journals. At least one diagnostic test was rejected for 19 out of 59 studies, whereas less than 5% of the collected 787 tests rejected the null hypotheses. In other words, one-third of the studies rejected at least one of their diagnostic tests, whereas their underlying identifying restrictions appear plausible. Multiple testing causes this problem because the median number of tests per study was as high as 12. The critical issue is the lack of a formal method to dismiss possibly false rejections caused by the multiple testing problem. Therefore, we offer unified tests to overcome the size-control problem. Our procedure is based on the new joint asymptotic normality of local polynomial mean and density estimates. In simulation studies, our unified tests outperformed the Bonferroni correction. We implement the procedure as an R package rdtest with two empirical examples in its vignettes.},
  archive      = {J_JOE},
  author       = {Koki Fusejima and Takuya Ishihara and Masayuki Sawada},
  doi          = {10.1016/j.jeconom.2025.106074},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106074},
  shortjournal = {J. Econ.},
  title        = {A unified test for regression discontinuity designs},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An order-invariant score-driven dynamic factor model. <em>JOE</em>, <em>251</em>, 106073. (<a href='https://doi.org/10.1016/j.jeconom.2025.106073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel score-driven dynamic factor model designed for filtering cross-sectional co-movements in panels of time series. The model is formulated using elliptical distribution for noise terms, allowing the update of the time-varying parameter to be potentially nonlinear and robust to outliers. We derive stochastic properties of time series generated by the model, such as stationarity and ergodicity, and establish the invertibility of the filter. We prove that the identification of the factors and loadings is achieved by incorporating an orthogonality constraint on the loadings, which is invariant to the order of the series in the panel. Given the nonlinearity of the constraint, we propose exploiting a maximum likelihood estimation on Stiefel manifolds. This approach ensures that the identification constraint is satisfied numerically, enabling joint estimation of the static and time-varying parameters. Furthermore, the asymptotic properties of the constrained estimator are derived. In a series of Monte Carlo experiments, we find evidence of appropriate finite sample properties of the estimator and resulting score filter for the time-varying parameters. We demonstrate the empirical usefulness of our factor model in constructing indices of economic activity from a set of macroeconomic and financial variables during the period 1981–2022. An empirical application highlights the importance of robustness, particularly in the presence of V-shaped recessions, such as the COVID-19 recession.},
  archive      = {J_JOE},
  author       = {Mariia Artemova},
  doi          = {10.1016/j.jeconom.2025.106073},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106073},
  shortjournal = {J. Econ.},
  title        = {An order-invariant score-driven dynamic factor model},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast on-line changepoint detection using heavily-weighted CUSUM and veto-based decision rules. <em>JOE</em>, <em>251</em>, 106071. (<a href='https://doi.org/10.1016/j.jeconom.2025.106071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study on-line changepoint detection in the context of a linear regression model, developing two novel contributions. Firstly, we propose a class of heavily weighted statistics based on the CUSUM process of the regression residuals, which are specifically designed to ensure timely detection of breaks occurring early on during the monitoring horizon. Secondly, we develop a class of composite statistics using different weighting schemes; the decision rule to mark a changepoint is based on the largest statistic across the various weights, thus effectively working like a “veto-based” voting mechanism, which ensures fast detection irrespective of the location of the changepoint. Our theory is derived under a very general form of weak dependence, thus being able to apply our tests to virtually all time series encountered in economics, finance, and other applied sciences. Monte Carlo simulations and an application to financial data show that our methodologies are able to control the procedure-wise Type I Error, and have short detection delays in the presence of breaks.},
  archive      = {J_JOE},
  author       = {Fabrizio Ghezzi and Eduardo Rossi and Lorenzo Trapani},
  doi          = {10.1016/j.jeconom.2025.106071},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106071},
  shortjournal = {J. Econ.},
  title        = {Fast on-line changepoint detection using heavily-weighted CUSUM and veto-based decision rules},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Factor-guided estimation of large covariance matrix function with conditional functional sparsity. <em>JOE</em>, <em>251</em>, 106070. (<a href='https://doi.org/10.1016/j.jeconom.2025.106070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the fundamental task of estimating covariance matrix functions for high-dimensional functional data/functional time series. We consider two functional factor structures encompassing either functional factors with scalar loadings or scalar factors with functional loadings, and postulate functional sparsity on the covariance of idiosyncratic errors after taking out the common unobserved factors. To facilitate estimation, we rely on the spiked matrix model and its functional generalization, and derive some novel asymptotic identifiability results, based on which we develop DIGIT and FPOET estimators under two functional factor models, respectively. Both estimators involve performing associated eigenanalysis to estimate the covariance of common components, followed by adaptive functional thresholding applied to the residual covariance. We also develop functional information criteria for model selection with theoretical guarantees. The convergence rates of involved estimated quantities are respectively established for DIGIT and FPOET estimators. Numerical studies including extensive simulations and a real data application on functional portfolio allocation are conducted to examine the finite-sample performance of the proposed methodology.},
  archive      = {J_JOE},
  author       = {Dong Li and Xinghao Qiao and Zihan Wang},
  doi          = {10.1016/j.jeconom.2025.106070},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106070},
  shortjournal = {J. Econ.},
  title        = {Factor-guided estimation of large covariance matrix function with conditional functional sparsity},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High dimensional binary choice model with unknown heteroskedasticity or instrumental variables. <em>JOE</em>, <em>251</em>, 106069. (<a href='https://doi.org/10.1016/j.jeconom.2025.106069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new method for estimating high-dimensional binary choice models. We consider a semiparametric model that places no distributional assumptions on the error term, allows for heteroskedastic errors, and permits endogenous regressors. Our approaches extend the special regressor estimator originally proposed by Lewbel (2000). This estimator becomes impractical in high-dimensional settings due to the curse of dimensionality associated with high-dimensional conditional density estimation. To overcome this challenge, we introduce an innovative data-driven dimension reduction method for nonparametric kernel estimators, which constitutes the main contribution of this work. The method combines distance covariance-based screening with cross-validation (CV) procedures, making special regressor estimation feasible in high dimensions. Using this new feasible conditional density estimator, we address variable and moment (instrumental variable) selection problems for these models. We apply penalized least squares (LS) and generalized method of moments (GMM) estimators with an L 1 penalty. A comprehensive analysis of the oracle and asymptotic properties of these estimators is provided. Finally, through Monte Carlo simulations and an empirical study on the migration intentions of rural Chinese residents, we demonstrate the effectiveness of our proposed methods in finite sample settings.},
  archive      = {J_JOE},
  author       = {Fu Ouyang and Thomas T. Yang},
  doi          = {10.1016/j.jeconom.2025.106069},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106069},
  shortjournal = {J. Econ.},
  title        = {High dimensional binary choice model with unknown heteroskedasticity or instrumental variables},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taking advantage of biased proxies for forecast evaluation. <em>JOE</em>, <em>251</em>, 106068. (<a href='https://doi.org/10.1016/j.jeconom.2025.106068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper rehabilitates biased proxies for the assessment of the predictive accuracy of competing forecasts. By relaxing the ubiquitous assumption of proxy unbiasedness adopted in the theoretical and empirical literature, we show how to optimally combine (possibly) biased proxies to maximize the probability of inferring the ranking that would be obtained using the true latent variable, a property that we dub proxy reliability. Our procedure still preserves the robustness of the loss function, in the sense of Patton (2011b), and allows testing for equal predictive accuracy, as in Diebold and Mariano (1995). We demonstrate the usefulness of the method with compelling empirical applications on GDP growth, financial market volatility forecasting, and sea surface temperature of the Niño 3.4 region.},
  archive      = {J_JOE},
  author       = {Giuseppe Buccheri and Roberto Renò and Giorgio Vocalelli},
  doi          = {10.1016/j.jeconom.2025.106068},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106068},
  shortjournal = {J. Econ.},
  title        = {Taking advantage of biased proxies for forecast evaluation},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust estimation for dynamic spatial autoregression models with nearly optimal rates. <em>JOE</em>, <em>251</em>, 106065. (<a href='https://doi.org/10.1016/j.jeconom.2025.106065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial autoregression has been extensively studied in various applications, yet its robust estimation methods have received limited attention. In this work, we introduce two dynamic spatial autoregression (DSAR) models aimed at capturing temporal trends and depicting the asymmetric network effects of the units. For both DSAR models, we propose a truncated Yule–Walker estimation method, which is tailored to achieve robust estimation in the presence of heavy-tailed data. Additionally, we extend this robust estimation procedure to a constrained estimation framework using the Dantzig selector, enabling the identification of sparse network effects observed in real-world applications. Theoretically, the minimax optimality of proposed estimators is derived under certain conditions on the weighting matrix. Empirical studies, including an analysis of financial contagion in the Chinese stock market and the dynamics of live streaming popularity, demonstrate the practical efficacy of our methods.},
  archive      = {J_JOE},
  author       = {Yin Lu and Chunbai Tao and Di Wang and Gazi Salah Uddin and Libo Wu and Xuening Zhu},
  doi          = {10.1016/j.jeconom.2025.106065},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106065},
  shortjournal = {J. Econ.},
  title        = {Robust estimation for dynamic spatial autoregression models with nearly optimal rates},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sieve estimation of state-varying factor models. <em>JOE</em>, <em>251</em>, 106064. (<a href='https://doi.org/10.1016/j.jeconom.2025.106064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a sieve approach to estimate state-varying factor models, where the factor loadings vary over specific state variables. Our methodology consists of a two-step estimation procedure for the parameters of interest. In the first step, we achieve preliminary consistent estimates of the factors and factor loadings via nuclear norm regularization (NNR). In the second step, we perform post-NNR iterative least squares estimations for the factors and factor loadings. We establish the asymptotic properties of these estimators. Based on the estimation theory, we propose a test for the null hypothesis of constant factor loadings and examine the asymptotic properties of the test statistic. Monte Carlo simulations demonstrate the favorable performance of the proposed estimation procedure and testing method in finite samples. An application to a U.S. macroeconomic dataset suggests potential state-dependency within the U.S. economy.},
  archive      = {J_JOE},
  author       = {Liangjun Su and Sainan Jin and Xia Wang},
  doi          = {10.1016/j.jeconom.2025.106064},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106064},
  shortjournal = {J. Econ.},
  title        = {Sieve estimation of state-varying factor models},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general test for functional inequalities. <em>JOE</em>, <em>251</em>, 106063. (<a href='https://doi.org/10.1016/j.jeconom.2025.106063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a nonparametric test for general functional inequalities that include conditional moment inequalities as a special case. It is shown that the test controls size uniformly over a large class of distributions for observed data, importantly allowing for general forms of time series dependence. New results on uniform growing dimensional Gaussian coupling for general mixingale processes are developed for this purpose, which readily accommodate most applications in economics and finance. The proposed method is applied in a portfolio evaluation context to test for “all-weather” portfolios with uniformly superior conditional Sharpe ratio functions.},
  archive      = {J_JOE},
  author       = {Jia Li and Zhipeng Liao and Wenyu Zhou},
  doi          = {10.1016/j.jeconom.2025.106063},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106063},
  shortjournal = {J. Econ.},
  title        = {A general test for functional inequalities},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural conformal inference for jump diffusion processes. <em>JOE</em>, <em>251</em>, 106061. (<a href='https://doi.org/10.1016/j.jeconom.2025.106061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference for jump diffusion processes (JDPs) remains challenging due to intractable transition densities and the latency of jump times and intensities. This paper introduces Neural Conformal Inference for JDPs (NCoin-JDP), a novel likelihood-free approach that leverages the power of deep neural networks (DNNs). NCoin-JDP bypasses the limitations of traditional methods by establishing a direct mapping between observed data and model parameters using a DNN. This approach eliminates the discretization errors inherent in likelihood-based methods, leading to more accurate inference. Despite the black-box nature of DNNs, we establish the asymptotic theory to quantify the approximation error of our algorithm. Additionally, we calibrate the uncertainty of our estimations using conformal prediction, providing theoretical guarantees of equivalence with the Bayesian posterior. NCoin-JDP demonstrates competitive performance compared to state-of-the-art methods. We showcase its effectiveness through numerical simulations and apply it to real-world data (S&P 500 and NASDAQ, 1993–2024) to investigate the impact of COVID-19 on the US economy. All numerical studies are reproducible in https://github.com/anonymous1116/NCoin-JDP .},
  archive      = {J_JOE},
  author       = {Hyeong Jin Hyun and Xiao Wang},
  doi          = {10.1016/j.jeconom.2025.106061},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106061},
  shortjournal = {J. Econ.},
  title        = {Neural conformal inference for jump diffusion processes},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Layered policy analysis in program evaluation using the marginal treatment effect. <em>JOE</em>, <em>251</em>, 106060. (<a href='https://doi.org/10.1016/j.jeconom.2025.106060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a unified approach to derive sharp bounds on conventional policy parameters when the instrumental variables (IVs) are potentially invalid. Using a vine copula approach, we propose a novel characterization of the identified sets for the marginal treatment effect (MTE) and the policy-relevant treatment effect (PRTE) parameters. Our method has various advantages: First, it explicitly demonstrates how imposing different IV-related assumptions with different credibility levels affects the MTE and PRTE’s identified set. Second, it provides a basis for testing model specifications and hypotheses about various imperfect IV-related assumptions. Third, it provides a tractable way to inform policy choices in the presence of uncertainty of the validity of identifying assumptions. Our approach enlarges the MTE framework’s scope by showing how it can be used to inform policy decisions even when valid instruments are not available.},
  archive      = {J_JOE},
  author       = {Ismael Mourifié and Yuanyuan Wan},
  doi          = {10.1016/j.jeconom.2025.106060},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106060},
  shortjournal = {J. Econ.},
  title        = {Layered policy analysis in program evaluation using the marginal treatment effect},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative analysis of two-way fixed effects estimators in staggered treatment designs. <em>JOE</em>, <em>251</em>, 106059. (<a href='https://doi.org/10.1016/j.jeconom.2025.106059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-way fixed effects (TWFE) is a flexible and widely used approach for estimating treatment effects, and several TWFE estimators have been proposed for staggered treatment designs. This paper focuses on the extended TWFE estimator, introduced by Borusyak et al. (2024) and Wooldridge (2021), and compares it with alternative TWFE estimators. The main contribution is the derivation of an equation that connects the extended TWFE estimator with the difference-in-differences estimator. This equivalence provides a transparent decomposition of the components of the extended TWFE estimand. The results show that the extended TWFE estimand consists of two distinct components: one that captures meaningful comparisons and a residual term. The paper outlines the assumptions required to identify treatment effects. In line with previous literature, the findings show that the extended TWFE estimator relies on a parallel trends assumption that extends across multiple periods. Additionally, illustrative examples compare the TWFE estimators under violations of the parallel trends assumption. The results suggest that no single estimator outperforms the others. The choice of the TWFE estimator depends on the parameter of interest and the characteristics of the empirical application.},
  archive      = {J_JOE},
  author       = {Jhordano Aguilar-Loyo},
  doi          = {10.1016/j.jeconom.2025.106059},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106059},
  shortjournal = {J. Econ.},
  title        = {A comparative analysis of two-way fixed effects estimators in staggered treatment designs},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High frequency factor analysis with partially observable factors. <em>JOE</em>, <em>251</em>, 106058. (<a href='https://doi.org/10.1016/j.jeconom.2025.106058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a novel factor structure – Partially Observable Factor Model – where both observable factors and latent factors exist in the model simultaneously. Such factor structure can make sure both interpretability and goodness-of-fit at the same time. Necessary estimation methodologies for this partially observable factor model are developed in this paper for the high frequency data. The proposed estimation methodology is robust to jumps, microstructure noise and asynchronous observation times simultaneously. When the observable factors are exogenous, we provide the estimation theory for the integrated eigenvalues of the residual covariance matrix, which including the bias-corrected estimator, central limit theorem and asymptotic variance estimator. As a result, the asymptotic normality of the bias-corrected estimator can be applied to test the existence of the latent factors. When the observable factors are endogenous, we propose a novel framework of high frequency unsupervised exogenous component learning (HF-UECL), which can help people quantify the contributions of the observable factors into the latent factors. This is the first work on high frequency instrumental variables, and it can be regard as a necessary and non-trivial extension of the Projected-PCA in the world of continuous-time model. Statistical inferences have been established for the loadings of the observable factors onto the latent factors. Monte Carlo simulation demonstrates the validity of our estimation methodologies. Empirical study demonstrates that (i) in the exogenous setting, the latent factors significantly exist in the residual process of the high frequency regression; (ii) in the endogenous setting, the correlations between the observable factors and latent factors do exist significantly.},
  archive      = {J_JOE},
  author       = {Dachuan Chen and Wenqi Lu and Siyu Xie},
  doi          = {10.1016/j.jeconom.2025.106058},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106058},
  shortjournal = {J. Econ.},
  title        = {High frequency factor analysis with partially observable factors},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global identification, estimation and inference of structural impulse response functions in factor models: A unified framework. <em>JOE</em>, <em>251</em>, 106057. (<a href='https://doi.org/10.1016/j.jeconom.2025.106057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a theory for the global identification, estimation and inference of impulse response functions (IRFs) in structural factor models (SFMs). We examine the impact of normalization choices on IRF identification and propose to use identification restrictions robust to such choices. A new theorem is established to address IRF identification under both recursive and nonrecursive schemes in SFMs. Moreover, we develop two new estimators for structural IRFs under principal component normalization and establish their asymptotic distributions. We also propose a test for overidentifying restrictions. Simulation results demonstrate the validity of the asymptotic approximations and the favorable finite-sample properties of the overidentification test. To illustrate the flexibility of our methodology, we employ a hybrid identification scheme and analyze the dynamic effects of oil shocks using a US dataset.},
  archive      = {J_JOE},
  author       = {Xu Han},
  doi          = {10.1016/j.jeconom.2025.106057},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106057},
  shortjournal = {J. Econ.},
  title        = {Global identification, estimation and inference of structural impulse response functions in factor models: A unified framework},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast computation of exact confidence intervals for randomized experiments with binary outcomes. <em>JOE</em>, <em>251</em>, 106056. (<a href='https://doi.org/10.1016/j.jeconom.2025.106056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a randomized experiment with binary outcomes, exact confidence intervals for the average causal effect of the treatment can be computed through a series of permutation tests. This approach requires minimal assumptions and is valid for all sample sizes, as it does not rely on large-sample approximations such as those implied by the central limit theorem. We show that these confidence intervals can be found in O ( n log n ) permutation tests in the case of balanced designs, where the treatment and control groups have equal sizes, and O ( n 2 ) permutation tests in the general case. Prior to this work, the most efficient known constructions required O ( n 2 ) such tests in the balanced case (Li and Ding, 2016), and O ( n 4 ) tests in the general case (Rigdon and Hudgens, 2015). Our results thus facilitate exact inference as a viable option for randomized experiments far larger than those accessible by previous methods. We also generalize our construction to produce confidence intervals for other causal estimands, including the relative risk ratio and odds ratio, yielding similar computational gains.},
  archive      = {J_JOE},
  author       = {P.M. Aronow and Haoge Chang and Patrick Lopatto},
  doi          = {10.1016/j.jeconom.2025.106056},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106056},
  shortjournal = {J. Econ.},
  title        = {Fast computation of exact confidence intervals for randomized experiments with binary outcomes},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized lee bounds. <em>JOE</em>, <em>251</em>, 106055. (<a href='https://doi.org/10.1016/j.jeconom.2025.106055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lee (2009) is a common approach to bound the average causal effect in the presence of selection bias, assuming the treatment effect on selection has the same sign for all subjects. This paper generalizes Lee bounds to allow the sign of this effect to be identified by pretreatment covariates, relaxing the standard (unconditional) monotonicity to its conditional analog. Asymptotic theory for generalized Lee bounds is proposed in low-dimensional smooth and high-dimensional sparse designs. The paper also generalizes Lee bounds to accommodate multiple outcomes. Focusing on JobCorps job training program, I first show that unconditional monotonicity is unlikely to hold, and then demonstrate the use of covariates to tighten the bounds.},
  archive      = {J_JOE},
  author       = {Vira Semenova},
  doi          = {10.1016/j.jeconom.2025.106055},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106055},
  shortjournal = {J. Econ.},
  title        = {Generalized lee bounds},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bernstein-type inequalities and nonparametric estimation under near-epoch dependence. <em>JOE</em>, <em>251</em>, 106054. (<a href='https://doi.org/10.1016/j.jeconom.2025.106054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main contributions of this paper are twofold. First, we derive Bernstein-type inequalities for irregularly spaced data under near-epoch dependent (NED) conditions and deterministic domain-expanding-infill (DEI) asymptotics. By introducing the concept of “effective dimension” to describe the geometric structure of sampled locations, we illustrate – unlike previous research – that the sharpness of these inequalities is affected by this effective dimension. To our knowledge, ours is the first study to report this phenomenon and show Bernstein-type inequalities under deterministic DEI asymptotics. This work represents a direct generalization of the work of Xu and Lee (2018), thus marking an important contribution to the topic. As a corollary, we derive a Bernstein-type inequality for irregularly spaced α -mixing random fields under DEI asymptotics. Our second contribution is to apply these inequalities to explore the attainability of optimal convergence rates for the local linear conditional mean estimator under algebraic NED conditions. Our results illustrate how the effective dimension affects assumptions of dependence. This finding refines the results of Jenish (2012) and extends the work of Hansen (2008), Vogt (2012), Chen and Christensen (2015) and Li, Lu, and Linton (2012).},
  archive      = {J_JOE},
  author       = {Zihao Yuan and Martin Spindler},
  doi          = {10.1016/j.jeconom.2025.106054},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106054},
  shortjournal = {J. Econ.},
  title        = {Bernstein-type inequalities and nonparametric estimation under near-epoch dependence},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bias correction for quantile regression estimators. <em>JOE</em>, <em>251</em>, 106053. (<a href='https://doi.org/10.1016/j.jeconom.2025.106053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the bias of classical quantile regression and instrumental variable quantile regression estimators. While being asymptotically first-order unbiased, these estimators can have non-negligible second-order biases. We derive a higher-order stochastic expansion of these estimators using empirical process theory. Based on this expansion, we derive an explicit formula for the second-order bias and propose a feasible bias correction procedure that uses finite-difference estimators of the bias components. The proposed bias correction method performs well in simulations. We provide an empirical illustration using Engel’s classical data on household food expenditure.},
  archive      = {J_JOE},
  author       = {Grigory Franguridi and Bulat Gafarov and Kaspar Wüthrich},
  doi          = {10.1016/j.jeconom.2025.106053},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106053},
  shortjournal = {J. Econ.},
  title        = {Bias correction for quantile regression estimators},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hedonic prices and quality adjusted price indices powered by AI. <em>JOE</em>, <em>251</em>, 106052. (<a href='https://doi.org/10.1016/j.jeconom.2025.106052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop empirical models that efficiently process large amounts of unstructured product data (text, images, prices, quantities) to produce accurate hedonic price estimates and derived indices. To achieve this, we generate abstract product attributes (or “features”) from descriptions and images using deep neural networks. These attributes are then used to estimate the hedonic price function. To demonstrate the effectiveness of this approach, we apply the models to Amazon’s data for first-party apparel sales, and estimate hedonic prices. The resulting models have a very high out-of-sample predictive accuracy, with R 2 ranging from 80% to 90%. Finally, we construct the AI-based hedonic Fisher price index, chained at the year-over-year frequency, and contrast it with the CPI and other electronic indices.},
  archive      = {J_JOE},
  author       = {P. Bajari and Z. Cen and V. Chernozhukov and M. Manukonda and S. Vijaykumar and J. Wang and R. Huerta and J. Li and L. Leng and G. Monokroussos and S. Wang},
  doi          = {10.1016/j.jeconom.2025.106052},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106052},
  shortjournal = {J. Econ.},
  title        = {Hedonic prices and quality adjusted price indices powered by AI},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic theory of the best-choice rerandomization using the mahalanobis distance. <em>JOE</em>, <em>251</em>, 106049. (<a href='https://doi.org/10.1016/j.jeconom.2025.106049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rerandomization, a design that utilizes pretreatment covariates and improves their balance between different treatment groups, has received attention recently in both theory and practice. From a survey by Bruhn and McKenzie (2009), there are at least two types of rerandomization that are used in practice: the first rerandomizes the treatment assignment until covariate imbalance is below a prespecified threshold; the second randomizes the treatment assignment multiple times and chooses the one with the best covariate balance. In this paper we will consider the second type of rerandomization, namely the best-choice rerandomization, whose theory and inference are still lacking in the literature. In particular, we will focus on the best-choice rerandomization that uses the Mahalanobis distance to measure covariate imbalance, which is one of the most commonly used imbalance measure for multivariate covariates and is invariant to affine transformations of covariates. We will study the large-sample repeatedly sampling properties of the best-choice rerandomization, allowing both the number of covariates and the number of tried complete randomizations to increase with the sample size. We show that the asymptotic distribution of the difference-in-means estimator is more concentrated around the true average treatment effect under rerandomization than under the complete randomization, and propose large-sample accurate confidence intervals for rerandomization that are shorter than that for the completely randomized experiment. We further demonstrate that, with moderate number of covariates and with the number of tried randomizations increasing polynomially with the sample size, the best-choice rerandomization can achieve the ideally optimal precision that one can expect even with perfectly balanced covariates. The developed theory and methods for rerandomization are also illustrated using real field experiments.},
  archive      = {J_JOE},
  author       = {Yuhao Wang and Xinran Li},
  doi          = {10.1016/j.jeconom.2025.106049},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106049},
  shortjournal = {J. Econ.},
  title        = {Asymptotic theory of the best-choice rerandomization using the mahalanobis distance},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust residual-based test for structural changes in factor models. <em>JOE</em>, <em>251</em>, 106042. (<a href='https://doi.org/10.1016/j.jeconom.2025.106042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an easy-to-implement residual-based specification testing procedure for detecting structural changes in factor models, which is powerful against both smooth and abrupt structural changes with unknown break dates. The proposed test is robust to the over-specified number of factors, and serially and cross-sectionally correlated error processes. A new central limit theorem is given for the quadratic forms of panel data with dependence over both dimensions, thereby filling a gap in the literature. We establish the asymptotic properties of the proposed test statistic, and accordingly develop a simulation-based scheme to select critical value in order to improve finite sample performance. Through extensive simulations and a real-world application, we confirm our theoretical results and demonstrate that the proposed test exhibits desirable size and power in practice.},
  archive      = {J_JOE},
  author       = {Bin Peng and Liangjun Su and Yayi Yan},
  doi          = {10.1016/j.jeconom.2025.106042},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106042},
  shortjournal = {J. Econ.},
  title        = {A robust residual-based test for structural changes in factor models},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate stochastic volatility models based on generalized fisher transformation. <em>JOE</em>, <em>251</em>, 106041. (<a href='https://doi.org/10.1016/j.jeconom.2025.106041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling multivariate stochastic volatility (MSV) can pose significant challenges, particularly when both variances and covariances are time-varying. In this study, we tackle these complexities by introducing novel MSV models based on the generalized Fisher transformation (GFT) proposed by Archakov and Hansen (2021). Our model exhibits remarkable flexibility, ensuring the positive-definiteness of the variance–covariance matrix, and disentangling the driving forces of volatilities and correlations. To conduct Bayesian analysis of the models, we employ a Particle Gibbs Ancestor Sampling (PGAS) method, facilitating efficient Bayesian model comparisons. Furthermore, we extend our MSV model to cover leverage effects and incorporate realized measures. Our simulation studies demonstrate that the proposed method performs well for our GFT-based MSV model. Furthermore, empirical studies based on equity returns show that the MSV models outperform alternative specifications in both in-sample and out-of-sample performances.},
  archive      = {J_JOE},
  author       = {Han Chen and Yijie Fei and Jun Yu},
  doi          = {10.1016/j.jeconom.2025.106041},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106041},
  shortjournal = {J. Econ.},
  title        = {Multivariate stochastic volatility models based on generalized fisher transformation},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-varying vector error-correction models: Estimation and inference. <em>JOE</em>, <em>251</em>, 106035. (<a href='https://doi.org/10.1016/j.jeconom.2025.106035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a time-varying vector error-correction model that allows for different time series behaviors (e.g., unit-root and locally stationary processes) to interact with each other and co-exist. From a practical perspective, this framework can be used to estimate shifts in the predictability of non-stationary variables, and test whether economic theories hold periodically, etc. We first develop a time-varying Granger Representation Theorem, which facilitates the establishment of an asymptotic theory for the model, and then propose estimation and inferential methods for both short-run and long-run coefficients. We also propose an information criterion to estimate the lag length, a singular-value ratio test to determine the cointegration rank, and a hypothesis test to examine the parameter stability. Finally, we extend the framework to allow for unknown structural breaks in either cointegration relationship or time-varying coefficient functions. To validate the theoretical findings, we conduct extensive simulations, and demonstrate the empirical relevance by testing the present value model for stock returns.},
  archive      = {J_JOE},
  author       = {Jiti Gao and Bin Peng and Yayi Yan},
  doi          = {10.1016/j.jeconom.2025.106035},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106035},
  shortjournal = {J. Econ.},
  title        = {Time-varying vector error-correction models: Estimation and inference},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel matrix factor model. <em>JOE</em>, <em>251</em>, 106033. (<a href='https://doi.org/10.1016/j.jeconom.2025.106033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale matrix data has been widely discovered and continuously studied in various fields recently. We propose a multilevel matrix factor model considering the existence of multi level factor structure in matrix time series. This model incorporates both global factors influencing all matrix time series and local factors confined to impact specific matrix time series. Asymptotic properties are established to ensure the consistency of our procedure for estimating factor loading matrices. To demonstrate the finite-sample performance of our estimation, we present comprehensive simulation results. Finally, we apply our model to an empirical analysis of eight indexes, including return, trading volume, and trading value, across 200 stocks from ten distinct industries.},
  archive      = {J_JOE},
  author       = {Yuteng Zhang and Yongchang Hui and Junrong Song and Shurong Zheng},
  doi          = {10.1016/j.jeconom.2025.106033},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106033},
  shortjournal = {J. Econ.},
  title        = {Multilevel matrix factor model},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution regression with censored selection. <em>JOE</em>, <em>251</em>, 106030. (<a href='https://doi.org/10.1016/j.jeconom.2025.106030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chernozhukov, Fernández-Val, and Luo (2023, CFL (2023) hereafter) considered a distribution regression model subject to sample selection with a binary selection mechanism. In this paper, we show how to identify and estimate a semi-parametric distribution regression model subject to a censored selection rule. With censored selection, we do not need to impose the usual outcome exclusion restriction or exclusion of the level of the latent selection variable from the selection sorting function for model identification, unlike CFL (2023). We propose new semiparametric estimators and corresponding inference procedures for model parameters and related functional parameters. We apply our method to investigate wage inequality in the UK for the period 1978–2000 using the Family Expenditure Survey (FES) data. Our findings reveal that (i) the selection sorting exclusion and outcome exclusion restrictions imposed by CFL (2023) are rejected; (ii) there is negative selection into work at most quantile levels for females, but not for males; (iii) in contrast to CFL (2023), our selection sorting effect pattern does not offer clear evidence on assortative matching or glass ceiling in the UK labor market; (iv) the latent gender wage gaps after correcting for selection bias are about 25%–50% of CFL (2023)’s estimates, and are also significantly smaller than the observed wage gaps; (v) similar to CFL (2023), there exists some strong evidence on gender discrimination in the UK labor market.},
  archive      = {J_JOE},
  author       = {Songnian Chen and Nianqing Liu and Hanghui Zhang},
  doi          = {10.1016/j.jeconom.2025.106030},
  journal      = {Journal of Econometrics},
  month        = {9},
  pages        = {106030},
  shortjournal = {J. Econ.},
  title        = {Distribution regression with censored selection},
  volume       = {251},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On changepoint detection in functional data using empirical energy distance. <em>JOE</em>, <em>250</em>, 106023. (<a href='https://doi.org/10.1016/j.jeconom.2025.106023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel family of test statistics to detect the presence of changepoints in a sequence of dependent, possibly multivariate, functional-valued observations. Our approach allows to test for a very general class of changepoints, including the “classical” case of changes in the mean, and even changes in the whole distribution. Our statistics are based on a generalisation of the empirical energy distance; we propose weighted functionals of the energy distance process, which are designed in order to enhance the ability to detect breaks occurring at sample endpoints. The limiting distribution of the maximally selected version of our statistics requires only the computation of the eigenvalues of the covariance function, thus being readily implementable in the most commonly employed packages, e.g. R . We show that, under the alternative, our statistics are able to detect changepoints occurring even very close to the beginning/end of the sample. In the presence of multiple changepoints, we propose a binary segmentation algorithm to estimate the number of breaks and the locations thereof. Simulations show that our procedures work very well in finite samples. We complement our theory with applications to financial and temperature data.},
  archive      = {J_JOE},
  author       = {B. Cooper Boniece and Lajos Horváth and Lorenzo Trapani},
  doi          = {10.1016/j.jeconom.2025.106023},
  journal      = {Journal of Econometrics},
  month        = {7},
  pages        = {106023},
  shortjournal = {J. Econ.},
  title        = {On changepoint detection in functional data using empirical energy distance},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference for large dimensional factor models under general missing data patterns. <em>JOE</em>, <em>250</em>, 106022. (<a href='https://doi.org/10.1016/j.jeconom.2025.106022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes the inferential theory for the least squares estimation of large factor models with missing data. We propose a unified framework for asymptotic analysis of factor models that covers a wide range of missing patterns, including heterogenous random missing, selection on covariates/factors/loadings, block/staggered missing, mixed frequency and ragged edge. We establish the average convergence rates of the estimated factor space and loading space, the limit distributions of the estimated factors and loadings, as well as the limit distributions of the estimated average treatment effects and the parameter estimates in the factor-augmented regressions. These results allow us to impute the unbalanced panel appropriately or make inference for the heterogenous treatment effects. For computation, we can use the nuclear norm regularized estimator as the initial value for the EM algorithm and iterate until convergence. Empirically, we apply our method to test the average treatment effects of partisan alignment on grant allocation in UK.},
  archive      = {J_JOE},
  author       = {Liangjun Su and Fa Wang},
  doi          = {10.1016/j.jeconom.2025.106022},
  journal      = {Journal of Econometrics},
  month        = {7},
  pages        = {106022},
  shortjournal = {J. Econ.},
  title        = {Inference for large dimensional factor models under general missing data patterns},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Realized candlestick wicks. <em>JOE</em>, <em>250</em>, 106014. (<a href='https://doi.org/10.1016/j.jeconom.2025.106014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel nonparametric estimator of integrated variance by summing up the squared wick lengths of intraday candlesticks over a fixed time interval. The proposed wick-based estimator is robust to short-lived extreme price movements, such as gradual jumps and flash crashes. We investigate the asymptotic properties of the proposed estimator, and show that its asymptotic variance is about four times smaller than the state-of-the-art differenced-return volatility (DV) estimator. We also develop a Hausman-type test for the presence of both jumps and episodic extreme price movements. Monte Carlo simulations and empirical applications further validate the practical reliability of our proposed estimator.},
  archive      = {J_JOE},
  author       = {Yifan Li and Ingmar Nolte and Sandra Nolte and Shifan Yu},
  doi          = {10.1016/j.jeconom.2025.106014},
  journal      = {Journal of Econometrics},
  month        = {7},
  pages        = {106014},
  shortjournal = {J. Econ.},
  title        = {Realized candlestick wicks},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic treatment effect estimation with interactive fixed effects and short panels. <em>JOE</em>, <em>250</em>, 106013. (<a href='https://doi.org/10.1016/j.jeconom.2025.106013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the estimation and inference of dynamic average treatment effect parameters when parallel trends holds conditional on interactive fixed effects and where units enter into treatment at different time periods. Our proposed generalized method of moments estimator consists of two parts: first, we estimate the unobserved time effects by applying the fixed- T consistent quasi-long-differencing estimator of Ahn et al., (2013) to the never-treated group. Second, we estimate the interactive fixed effects for treated groups post-treatment to recover their unobserved counterfactual outcomes then subtract this quantity from the observed outcomes and average over group membership to estimate the Average Treatment Effect on the Treated. We also demonstrate the robustness of two-way fixed effects to certain parallel trends violations and describe how to test for consistency. We investigate the effect of Walmart openings on local economic conditions and demonstrate that our methods ameliorate pre-trend violations commonly found in the literature. We also provide statistical software to implement our estimator in Julia and R .},
  archive      = {J_JOE},
  author       = {Nicholas L. Brown and Kyle Butts},
  doi          = {10.1016/j.jeconom.2025.106013},
  journal      = {Journal of Econometrics},
  month        = {7},
  pages        = {106013},
  shortjournal = {J. Econ.},
  title        = {Dynamic treatment effect estimation with interactive fixed effects and short panels},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dimension-agnostic change point detection. <em>JOE</em>, <em>250</em>, 106012. (<a href='https://doi.org/10.1016/j.jeconom.2025.106012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change point testing for high-dimensional data has attracted a lot of attention in statistics, econometrics and machine learning owing to the emergence of high-dimensional data with structural breaks from many fields. In practice, when the dimension is less than the sample size but is not small, it is often unclear whether a method that is tailored to high-dimensional data or simply a classical method that is developed and justified for low-dimensional data is preferred. In addition, the methods designed for low-dimensional data may not work well in the high-dimensional environment and vice versa. In this paper, we propose a dimension-agnostic testing procedure targeting a single change point in the mean of a multivariate weakly dependent time series. Specifically, we can show that the limiting null distribution for our test statistic is the same regardless of the dimensionality and the magnitude of cross-sectional dependence. The power analysis is also conducted to understand the large sample behavior of the proposed test. Through Monte Carlo simulations and a real data illustration, we demonstrate that the finite sample results strongly corroborate the theory and suggest that the proposed test can be used as a benchmark for change-point detection of time series of low, medium, and high dimensions with complex cross-sectional and temporal dependence.},
  archive      = {J_JOE},
  author       = {Hanjia Gao and Runmin Wang and Xiaofeng Shao},
  doi          = {10.1016/j.jeconom.2025.106012},
  journal      = {Journal of Econometrics},
  month        = {7},
  pages        = {106012},
  shortjournal = {J. Econ.},
  title        = {Dimension-agnostic change point detection},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple and computationally trivial estimator for grouped fixed effects models. <em>JOE</em>, <em>250</em>, 106011. (<a href='https://doi.org/10.1016/j.jeconom.2025.106011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new fixed effects estimator for linear panel data models with clustered time patterns of unobserved heterogeneity. The method avoids non-convex and combinatorial optimization by combining a preliminary consistent estimator of the slope coefficient, an agglomerative pairwise-differencing clustering of cross-sectional units, and a pooled ordinary least squares regression. Asymptotic guarantees are established in a framework where T can grow at any power of N , as both N and T approach infinity. Unlike most existing approaches, the proposed estimator is computationally straightforward and does not require a known upper bound on the number of groups. As existing approaches, this method leads to a consistent estimation of well-separated groups and an estimator of common parameters asymptotically equivalent to the infeasible regression controlling for the true groups. An application revisits the statistical association between income and democracy.},
  archive      = {J_JOE},
  author       = {Martin Mugnier},
  doi          = {10.1016/j.jeconom.2025.106011},
  journal      = {Journal of Econometrics},
  month        = {7},
  pages        = {106011},
  shortjournal = {J. Econ.},
  title        = {A simple and computationally trivial estimator for grouped fixed effects models},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite- and large-sample inference for ranks using multinomial data with an application to ranking political parties. <em>JOE</em>, <em>250</em>, 106010. (<a href='https://doi.org/10.1016/j.jeconom.2025.106010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is common to rank different categories by means of preferences that are revealed through data on choices. A prominent example is the ranking of political candidates or parties using the estimated share of support each one receives in surveys or polls about political attitudes. Since these rankings are computed using estimates of the share of support rather than the true share of support, there may be considerable uncertainty concerning the true ranking of the political candidates or parties. In this paper, we consider the problem of accounting for such uncertainty by constructing confidence sets for the rank of each category. We consider both the problem of constructing marginal confidence sets for the rank of a particular category as well as simultaneous confidence sets for the ranks of all categories. A distinguishing feature of our analysis is that we exploit the multinomial structure of the data to develop confidence sets that are valid in finite samples. We additionally develop confidence sets using the bootstrap that are valid only approximately in large samples. We use our methodology to rank political parties in Australia using data from the 2019 Australian Election Survey. We find that our finite-sample confidence sets are informative across the entire ranking of political parties, even in Australian territories with few survey respondents and/or with parties that are chosen by only a small share of the survey respondents. In contrast, the bootstrap-based confidence sets may sometimes be considerably less informative. These findings motivate us to compare these methods in an empirically-driven simulation study, in which we conclude that our finite-sample confidence sets often perform better than their large-sample, bootstrap-based counterparts, especially in settings that resemble our empirical application.},
  archive      = {J_JOE},
  author       = {Sergei Bazylik and Magne Mogstad and Joseph P. Romano and Azeem M. Shaikh and Daniel Wilhelm},
  doi          = {10.1016/j.jeconom.2025.106010},
  journal      = {Journal of Econometrics},
  month        = {7},
  pages        = {106010},
  shortjournal = {J. Econ.},
  title        = {Finite- and large-sample inference for ranks using multinomial data with an application to ranking political parties},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pairwise valid instruments. <em>JOE</em>, <em>250</em>, 106009. (<a href='https://doi.org/10.1016/j.jeconom.2025.106009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding valid instruments is difficult. We propose Validity Set Instrumental Variable (VSIV) estimation, a method for estimating local average treatment effects (LATEs) in heterogeneous causal effect models when the instruments are partially invalid. We consider settings with pairwise valid instruments, that is, instruments that are valid for a subset of instrument value pairs. VSIV estimation exploits testable implications of instrument validity to remove invalid pairs and provides estimates of the LATEs for all remaining pairs, which can be aggregated into a single parameter of interest using researcher-specified weights. We show that the proposed VSIV estimators are asymptotically normal under weak conditions and remove or reduce the asymptotic bias relative to standard LATE estimators (that is, LATE estimators that do not use testable implications to remove invalid variation). We evaluate the finite sample properties of VSIV estimation in application-based simulations and apply our method to estimate the returns to college education using parental education as an instrument.},
  archive      = {J_JOE},
  author       = {Zhenting Sun and Kaspar Wüthrich},
  doi          = {10.1016/j.jeconom.2025.106009},
  journal      = {Journal of Econometrics},
  month        = {7},
  pages        = {106009},
  shortjournal = {J. Econ.},
  title        = {Pairwise valid instruments},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Faster estimation of dynamic discrete choice models using index invertibility. <em>JOE</em>, <em>250</em>, 106004. (<a href='https://doi.org/10.1016/j.jeconom.2025.106004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many estimators of dynamic discrete choice models with persistent unobserved heterogeneity have desirable statistical properties but are computationally intensive. In this paper we propose a method to quicken estimation for a broad class of dynamic discrete choice problems by exploiting semiparametric index restrictions. Specifically, we propose an estimator for models whose reduced form parameters are invertible functions of one or more linear indices (Ahn et al., 2018) , a property we term index invertibility. We establish that index invertibility implies a set of equality constraints on the model parameters. Our proposed estimator uses the equality constraints to decrease the dimension of the optimization problem, thereby generating computational gains. Our main result shows that the proposed estimator is asymptotically equivalent to the unconstrained, computationally heavy estimator. In addition, we provide a series of results on the number of independent index restrictions on the model parameters, providing theoretical guidance on the extent of computational gains. Finally, we demonstrate the advantages of our approach via Monte Carlo simulations.},
  archive      = {J_JOE},
  author       = {Jackson Bunting and Takuya Ura},
  doi          = {10.1016/j.jeconom.2025.106004},
  journal      = {Journal of Econometrics},
  month        = {7},
  pages        = {106004},
  shortjournal = {J. Econ.},
  title        = {Faster estimation of dynamic discrete choice models using index invertibility},
  volume       = {250},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Limit theory for local polynomial estimation of functional coefficient models with possibly integrated regressors. <em>JOE</em>, <em>249</em>, 106007. (<a href='https://doi.org/10.1016/j.jeconom.2025.106007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limit theory for functional coefficient cointegrating regression was recently found to be considerably more complex than earlier understood. The issues were explained and correct limit theory derived for the kernel weighted local level estimator in Phillips and Wang (2023b). The present paper provides complete limit theory for the general kernel weighted local p th order polynomial estimators of the functional coefficient and the coefficient derivatives. Both stationary and nonstationary regressors are allowed. Implications for bandwidth selection are discussed. An adaptive procedure to select the fit order p is proposed and found to work well. A robust t -ratio is constructed following the new limit theory, which corrects and improves the usual t -ratio in the literature. The robust t -ratio is valid and works well regardless of the properties of the regressors, thereby providing a unified procedure to compute the t -ratio and facilitating practical inference. Testing constancy of the functional coefficient is also considered. Finite sample studies are provided that corroborate the new asymptotic theory.},
  archive      = {J_JOE},
  author       = {Ying Wang and Peter C.B. Phillips},
  doi          = {10.1016/j.jeconom.2025.106007},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {106007},
  shortjournal = {J. Econ.},
  title        = {Limit theory for local polynomial estimation of functional coefficient models with possibly integrated regressors},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating coefficient-by-coefficient breaks in panel data models. <em>JOE</em>, <em>249</em>, 106005. (<a href='https://doi.org/10.1016/j.jeconom.2025.106005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When estimating structural breaks, existing econometric methods adopt an approach in which either all parameters change simultaneously, or they remain the same. In this paper, we consider the estimation of panel data models when an unknown subset of coefficients is subject to breaks. The challenge lies in estimating the breaks for each coefficient. To tackle this, we propose a new estimator for panel data, the “Coefficient-by-Coefficient Lasso” break estimator. This estimator is derived by penalizing the coefficients with a fused penalty and using component-wise adaptive weights. We present this estimator for two scenarios: those with homogeneous breaks and those with heterogeneous breaks. We show that the method identifies the number and dates of breaks for all coefficients with high probability and that the post-selection estimator is asymptotically normal. We examine the small-sample properties of the method through a Monte Carlo study and further apply it to analyze the influence of socioeconomic factors on crime.},
  archive      = {J_JOE},
  author       = {Yousef Kaddoura},
  doi          = {10.1016/j.jeconom.2025.106005},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {106005},
  shortjournal = {J. Econ.},
  title        = {Estimating coefficient-by-coefficient breaks in panel data models},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-sectional dependence in idiosyncratic volatility. <em>JOE</em>, <em>249</em>, 106003. (<a href='https://doi.org/10.1016/j.jeconom.2025.106003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an econometric framework for analyzing cross-sectional dependence in the idiosyncratic volatilities of assets using high frequency data. We first consider the estimation of standard measures of dependence in the idiosyncratic volatilities such as covariances and correlations. Naive estimators of these measures are biased due to the use of the error-laden estimates of idiosyncratic volatilities. We provide bias-corrected estimators and the relevant asymptotic theory. Next, we introduce an idiosyncratic volatility factor model, in which we decompose the variation in idiosyncratic volatilities into two parts: the variation related to the systematic factors such as the market volatility, and the residual variation. Again, naive estimators of the decomposition are biased, and we provide bias-corrected estimators. We also provide the asymptotic theory that allows us to test whether the residual (non-systematic) components of the idiosyncratic volatilities exhibit cross-sectional dependence. We apply our methodology to the S&P 100 index constituents, and document strong cross-sectional dependence in their idiosyncratic volatilities. We consider two different sets of idiosyncratic volatility factors, and find that neither can fully account for the cross-sectional dependence in idiosyncratic volatilities. For each model, we map out the network of dependencies in residual (non-systematic) idiosyncratic volatilities across all stocks.},
  archive      = {J_JOE},
  author       = {Ilze Kalnina and Kokouvi Tewou},
  doi          = {10.1016/j.jeconom.2025.106003},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {106003},
  shortjournal = {J. Econ.},
  title        = {Cross-sectional dependence in idiosyncratic volatility},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive quantile regressions with persistent and heteroskedastic predictors: A powerful 2SLS testing approach. <em>JOE</em>, <em>249</em>, 106002. (<a href='https://doi.org/10.1016/j.jeconom.2025.106002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop new tests for predictability at a given quantile, based on the Lagrange Multiplier [LM] principle, in the context of quantile regression [QR] models which allow for persistent and endogenous predictors driven by heteroskedastic errors. Of the extant predictive QR tests in the literature, only the moving blocks bootstrap implementation, due to Fan and Lee (2019) , of the Wald-type test of Lee (2016) can allow for conditionally heteroskedastic errors in the context of a QR model with persistent predictors. In common with all other tests in the literature these tests cannot, however, allow for unconditionally heteroskedastic behaviour in the errors. The LM-based approach we adopt in this paper is obtained from a simple auxiliary linear test regression which facilitates inference based on established instrumental variable methods. We demonstrate that, as a result, the tests we develop, based on either conventional or heteroskedasticity-consistent standard errors in the auxiliary regression, are robust under the null hypothesis of no predictability to conditional heteroskedasticity and to unconditional heteroskedasticity in the errors driving the predictors, with no need for bootstrap implementation. We also propose tests for joint predictability across a set of multiple distinct quantiles. Simulation results for both conditionally and unconditionally heteroskedastic errors highlight the superior finite sample properties of our proposed LM tests over the tests of Lee (2016) and Fan and Lee (2019) and the recent variable addition tests of Cai et al. (2023). An empirical application to the equity premium for the S&P 500 highlights the practical usefulness of our proposed tests, uncovering significant evidence of predictability in the left and right tails of the returns distribution for a number of predictors containing information on market or firm risk.},
  archive      = {J_JOE},
  author       = {Matei Demetrescu and Paulo M.M. Rodrigues and A.M. Robert Taylor},
  doi          = {10.1016/j.jeconom.2025.106002},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {106002},
  shortjournal = {J. Econ.},
  title        = {Predictive quantile regressions with persistent and heteroskedastic predictors: A powerful 2SLS testing approach},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic theory for two-way clustering. <em>JOE</em>, <em>249</em>, 106001. (<a href='https://doi.org/10.1016/j.jeconom.2025.106001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proves a new central limit theorem for a sample that exhibits two-way dependence and heterogeneity across clusters. Statistical inference for situations with both two-way dependence and cluster heterogeneity has thus far been an open issue. The existing theory for two-way clustering inference requires identical distributions across clusters (implied by the so-called separate exchangeability assumption). Yet no such homogeneity requirement is needed in the existing theory for one-way clustering. The new result therefore theoretically justifies the view that two-way clustering is a more robust version of one-way clustering, consistent with applied practice. In an application to linear regression, I show that a standard plug-in variance estimator is valid for inference.},
  archive      = {J_JOE},
  author       = {Luther Yap},
  doi          = {10.1016/j.jeconom.2025.106001},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {106001},
  shortjournal = {J. Econ.},
  title        = {Asymptotic theory for two-way clustering},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference for deprivation profiles in a binary setting. <em>JOE</em>, <em>249</em>, 106000. (<a href='https://doi.org/10.1016/j.jeconom.2025.106000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper addresses the issue of comparing deprivation distributions when the severity of deprivation is measured by a sum of (weighted) binary variables. To accomplish this task, it provides a graphical tool, the Three I’s of Deprivation (TID) curve, which summarises the incidence, intensity and inequality aspects of deprivation in a society and is the natural counterpart to the TIP curve widely used in income poverty analysis. Uncertainty around the estimated deprivation curves is assessed by simultaneous confidence bands. A dominance hypothesis test is presented to facilitate the comparison and ordering of TID curves across groups and over time. A rank-dependent multi-deprivation index consistent with the TID ordering is calculated and confidence intervals are developed. As a substantive illustration, the evolution of material and social deprivation across European countries over the period of the pandemic outbreak is analysed.},
  archive      = {J_JOE},
  author       = {Maria Grazia Pittau and Pier Luigi Conti and Roberto Zelli},
  doi          = {10.1016/j.jeconom.2025.106000},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {106000},
  shortjournal = {J. Econ.},
  title        = {Inference for deprivation profiles in a binary setting},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile prediction with factor-augmented regression: Structural instability and model uncertainty. <em>JOE</em>, <em>249</em>, 105999. (<a href='https://doi.org/10.1016/j.jeconom.2025.105999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantile regression is an effective tool in modeling data with heterogeneous conditional distribution. This paper considers the time-varying coefficient quantile predictive regression with factor-augmented predictors, to capture smooth structural changes and incorporate high-dimensional data information in prediction simultaneously. Uniform consistency of the local linear quantile coefficient estimators is established under misspecification. To further improve the forecast accuracy, a novel time-varying model averaging based on local forward-validation is developed. The averaging estimator is shown to be asymptotically optimal in the sense of minimizing out-of-sample forecast risk function. Furthermore, the weight selection consistency and the asymptotic distribution of the averaging coefficient estimator are established. Numerical results from simulations and a real data application to forecasting U.S. inflation demonstrate the nice performance of the averaging estimators.},
  archive      = {J_JOE},
  author       = {Yundong Tu and Siwei Wang},
  doi          = {10.1016/j.jeconom.2025.105999},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105999},
  shortjournal = {J. Econ.},
  title        = {Quantile prediction with factor-augmented regression: Structural instability and model uncertainty},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regret analysis in threshold policy design. <em>JOE</em>, <em>249</em>, 105998. (<a href='https://doi.org/10.1016/j.jeconom.2025.105998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Threshold policies are decision rules that assign treatments based on whether an observable characteristic exceeds a certain threshold. They are widespread across multiple domains, including welfare programs, taxation, and clinical medicine. This paper examines the problem of designing threshold policies using experimental data, when the goal is to maximize the population welfare. First, I characterize the regret – a measure of policy optimality – of the Empirical Welfare Maximizer (EWM) policy, popular in the literature. Next, I introduce the Smoothed Welfare Maximizer (SWM) policy, which improves the EWM’s regret convergence rate under an additional smoothness condition. The two policies are compared by studying how differently their regrets depend on the population distribution, and investigating their finite sample performances through Monte Carlo simulations. In many contexts, the SWM policy guarantees larger welfare than the EWM. An empirical illustration demonstrates how the treatment recommendations of the two policies may differ in practice.},
  archive      = {J_JOE},
  author       = {Federico Crippa},
  doi          = {10.1016/j.jeconom.2025.105998},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105998},
  shortjournal = {J. Econ.},
  title        = {Regret analysis in threshold policy design},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subjective expectations and demand for contraception. <em>JOE</em>, <em>249</em>, 105997. (<a href='https://doi.org/10.1016/j.jeconom.2025.105997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-quarter of married, fertile-age women in Sub-Saharan Africa report not wanting a pregnancy and yet do not practice contraception. We collect detailed data on the subjective beliefs of married, adult women not wanting a pregnancy and estimate a structural model of contraceptive choices. Both our structural model and a validation exercise using an exogenous shock to beliefs show that correcting women’s beliefs about pregnancy risk absent contraception can increase use considerably. Our structural estimates further indicate that costly interventions like eliminating supply constraints would only modestly increase contraceptive use, while confirming the importance of partners’ preferences highlighted in related literature.},
  archive      = {J_JOE},
  author       = {Grant Miller and Áureo de Paula and Christine Valente},
  doi          = {10.1016/j.jeconom.2025.105997},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105997},
  shortjournal = {J. Econ.},
  title        = {Subjective expectations and demand for contraception},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Limit theory and inference in non-cointegrated functional coefficient regression. <em>JOE</em>, <em>249</em>, 105996. (<a href='https://doi.org/10.1016/j.jeconom.2025.105996'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional coefficient (FC) cointegrating regressions offer empirical investigators flexibility in modeling economic relationships by introducing covariates that influence the direction and intensity of comovement among nonstationary time series. FC regression models are also useful when formal cointegration is absent, in the sense that the equation errors may themselves be nonstationary, but where the nonstationary series display well-defined FC linkages that can be meaningfully interpreted as correlation measures involving the covariates. The present paper proposes new nonparametric estimators for such FC regression models where the nonstationary series display linkages that enable consistent estimation of the correlation measures between them. Specifically, we develop n -consistent estimators for the functional coefficient and establish their asymptotic distributions, which involve mixed normal limits that facilitate inference. Two novel features that appear in the limit theory are (i) the need for non-diagonal matrix normalization due to the presence of stationary and nonstationary components in the regression; and (ii) random bias elements that appear in the asymptotic distribution of the kernel estimators, again resulting from the nonstationary regression components. Numerical studies reveal that the proposed estimators achieve significant efficiency improvements compared to the estimators suggested in earlier work by Sun et al. (2011). Easily implementable specification tests with standard chi-square asymptotics are suggested to check for constancy of the functional coefficient. These tests are shown to have faster divergence rate under local alternatives and enjoy superior performance in simulations than tests proposed in Gan et al. (2014). An empirical application based on the quantity theory of money is included, illustrating the practical use of correlated but non-cointegrated regression relations.},
  archive      = {J_JOE},
  author       = {Ying Wang and Peter C.B. Phillips and Yundong Tu},
  doi          = {10.1016/j.jeconom.2025.105996},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105996},
  shortjournal = {J. Econ.},
  title        = {Limit theory and inference in non-cointegrated functional coefficient regression},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised factor modeling for high-dimensional linear time series. <em>JOE</em>, <em>249</em>, 105995. (<a href='https://doi.org/10.1016/j.jeconom.2025.105995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by Tucker tensor decomposition, this paper imposes low-rank structures to the column and row spaces of coefficient matrices in a multivariate infinite-order vector autoregression (VAR), which leads to a supervised factor model with two factor modelings being conducted to responses and predictors simultaneously. Interestingly, the stationarity condition implies an intrinsic weak group sparsity mechanism of infinite-order VAR, and hence a rank-constrained group Lasso estimation is considered for high-dimensional linear time series. Its non-asymptotic properties are discussed by balancing the estimation, approximation and truncation errors. Moreover, an alternating gradient descent algorithm with hard-thresholding is designed to search for high-dimensional estimates, and its theoretical justifications, including statistical and convergence analysis, are also provided. Theoretical and computational properties of the proposed methodology are verified by simulation experiments, and the advantages over existing methods are demonstrated by analyzing US quarterly macroeconomic variables.},
  archive      = {J_JOE},
  author       = {Feiqing Huang and Kexin Lu and Yao Zheng and Guodong Li},
  doi          = {10.1016/j.jeconom.2025.105995},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105995},
  shortjournal = {J. Econ.},
  title        = {Supervised factor modeling for high-dimensional linear time series},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model averaging prediction for possibly nonstationary autoregressions. <em>JOE</em>, <em>249</em>, 105994. (<a href='https://doi.org/10.1016/j.jeconom.2025.105994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an alternative to model selection (MS), this paper considers model averaging (MA) for integrated autoregressive processes of infinite order (AR( ∞ )). We derive a uniformly asymptotic expression for the mean squared prediction error (MSPE) of the averaging prediction with fixed weights and then propose a Mallows-type criterion to select the data-driven weights that minimize the MSPE asymptotically. We show that the proposed MA estimator and its variants, Shibata and Akaike MA estimators, are asymptotically optimal in the sense of achieving the lowest possible MSPE. We further demonstrate that MA can provide significant MSPE reduction over MS in the algebraic-decay case. These theoretical findings are extended to integrated AR( ∞ ) models with deterministic time trends and are supported by Monte Carlo simulations and real data analysis.},
  archive      = {J_JOE},
  author       = {Tzu-Chi Lin and Chu-An Liu},
  doi          = {10.1016/j.jeconom.2025.105994},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105994},
  shortjournal = {J. Econ.},
  title        = {Model averaging prediction for possibly nonstationary autoregressions},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Huber principal component analysis for large-dimensional factor models. <em>JOE</em>, <em>249</em>, 105993. (<a href='https://doi.org/10.1016/j.jeconom.2025.105993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor models have been widely used in economics and finance. However, the heavy-tailed nature of macroeconomic and financial data is often neglected in statistical analysis. To address this issue, we propose a robust approach to estimate factor loadings and scores by minimizing the Huber loss function, which is motivated by the equivalence between conventional Principal Component Analysis (PCA) and the constrained least squares method in the factor model. We provide two algorithms that use different penalty forms. The first algorithm involves an element-wise-type Huber loss minimization, solved by an iterative Huber regression algorithm. The second algorithm, which we refer to as Huber PCA, minimizes the ℓ 2 -norm-type Huber loss and performs PCA on the weighted sample covariance matrix. We examine the theoretical minimizer of the element-wise Huber loss function and demonstrate that it has the same convergence rate as conventional PCA when the idiosyncratic errors have bounded second moments. We also derive their asymptotic distributions under mild conditions. Moreover, we suggest a consistent model selection criterion that relies on rank minimization to estimate the number of factors robustly. We showcase the benefits of the proposed two algorithms through extensive numerical experiments and a real macroeconomic data example. An R package named “ HDRFA ” 1 has been developed to conduct the proposed robust factor analysis.},
  archive      = {J_JOE},
  author       = {Yong He and Lingxiao Li and Dong Liu and Wen-Xin Zhou},
  doi          = {10.1016/j.jeconom.2025.105993},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105993},
  shortjournal = {J. Econ.},
  title        = {Huber principal component analysis for large-dimensional factor models},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile granger causality in the presence of instability. <em>JOE</em>, <em>249</em>, 105992. (<a href='https://doi.org/10.1016/j.jeconom.2025.105992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new framework for assessing Granger causality in quantiles in unstable environments, for a fixed quantile or over a continuum of quantile levels. Our proposed test statistics are consistent against fixed alternatives, they have nontrivial power against local alternatives, and they are pivotal in certain important special cases. In addition, we show the validity of a bootstrap procedure when asymptotic distributions depend on nuisance parameters. Monte Carlo simulations reveal that the proposed test statistics have correct empirical size and high power, even in absence of structural breaks. Moreover, a procedure providing additional insight into the timing of Granger causal regimes based on our new tests is proposed. Finally, an empirical application in energy economics highlights the applicability of our method as the new tests provide stronger evidence of Granger causality.},
  archive      = {J_JOE},
  author       = {Alexander Mayer and Dominik Wied and Victor Troster},
  doi          = {10.1016/j.jeconom.2025.105992},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105992},
  shortjournal = {J. Econ.},
  title        = {Quantile granger causality in the presence of instability},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjustments with many regressors under covariate-adaptive randomizations. <em>JOE</em>, <em>249</em>, 105991. (<a href='https://doi.org/10.1016/j.jeconom.2025.105991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our paper discovers a new trade-off of using regression adjustments (RAs) in causal inference under covariate-adaptive randomizations (CARs). On one hand, RAs can improve the efficiency of causal estimators by incorporating information from covariates that are not used in the randomization. On the other hand, RAs can degrade estimation efficiency due to their estimation errors, which are not asymptotically negligible when the number of regressors is of the same order as the sample size. Ignoring the estimation errors of RAs may result in serious over-rejection of causal inference under the null hypothesis. To address the issue, we construct a new ATE estimator by optimally linearly combining the estimators with and without RAs. We then develop a unified inference theory for this estimator under CARs. It has two features: (1) the Wald test based on it achieves the exact asymptotic size under the null hypothesis, regardless of whether the number of covariates is fixed or diverges no faster than the sample size; and (2) it guarantees weak efficiency improvement over estimators both with and without RAs.},
  archive      = {J_JOE},
  author       = {Liang Jiang and Liyao Li and Ke Miao and Yichong Zhang},
  doi          = {10.1016/j.jeconom.2025.105991},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105991},
  shortjournal = {J. Econ.},
  title        = {Adjustments with many regressors under covariate-adaptive randomizations},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bootstrap based asymptotic refinements for high-dimensional nonlinear models. <em>JOE</em>, <em>249</em>, 105977. (<a href='https://doi.org/10.1016/j.jeconom.2025.105977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider penalized extremum estimation of a high-dimensional, possibly nonlinear model that is sparse in the sense that most of its parameters are zero but some are not. We use the SCAD penalty function, which provides model selection consistent and oracle efficient estimates under suitable conditions. However, asymptotic approximations based on the oracle model can be inaccurate with the sample sizes found in many applications. This paper gives conditions under which the bootstrap, based on estimates obtained through SCAD penalization with thresholding, provides asymptotic refinements of size O ( n − 2 ) for the error in the rejection (coverage) probability of a symmetric hypothesis test (confidence interval) and O ( n − 1 ) for the error in the rejection (coverage) probability of a one-sided or equal tailed test (confidence interval). The results of Monte Carlo experiments show that the bootstrap can provide large reductions in errors in rejection and coverage probabilities. The bootstrap is consistent, though it does not necessarily provide asymptotic refinements, if some parameters are close but not equal to zero. Random-coefficients logit and probit models and nonlinear moment models are examples of models to which the procedure applies.},
  archive      = {J_JOE},
  author       = {Joel L. Horowitz and Ahnaf Rafi},
  doi          = {10.1016/j.jeconom.2025.105977},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105977},
  shortjournal = {J. Econ.},
  title        = {Bootstrap based asymptotic refinements for high-dimensional nonlinear models},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor time series imputation through tensor factor modelling. <em>JOE</em>, <em>249</em>, 105974. (<a href='https://doi.org/10.1016/j.jeconom.2025.105974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose tensor time series imputation when the missing pattern in the tensor data can be general, as long as any two data positions along a tensor fibre are both observed for enough time points. The method is based on a tensor time series factor model with Tucker decomposition of the common component. One distinguished feature of the tensor time series factor model used is that there can be weak factors in the factor loading matrix for each mode. This reflects reality better when real data can have weak factors which drive only groups of observed variables, for instance, a sector factor in a financial market driving only stocks in a particular sector. Using the data with missing entries, asymptotic normality is derived for rows of estimated factor loadings, while consistent covariance matrix estimation enables us to carry out inferences. As a first in the literature, we also propose a ratio-based estimator for the rank of the core tensor under general missing patterns. Rates of convergence are spelt out for the imputations from the estimated tensor factor models. Simulation results show that our imputation procedure works well, with asymptotic normality and corresponding inferences also demonstrated. Re-imputation performances are also gauged when we demonstrate that using slightly larger rank then estimated gives superior re-imputation performances. A Fama–French portfolio example with matrix returns and an OECD data example with matrix of economic indicators are presented and analysed, showing the efficacy of our imputation approach compared to direct vector imputation.},
  archive      = {J_JOE},
  author       = {Zetai Cen and Clifford Lam},
  doi          = {10.1016/j.jeconom.2025.105974},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105974},
  shortjournal = {J. Econ.},
  title        = {Tensor time series imputation through tensor factor modelling},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation and uniform inference in sparse high-dimensional additive models. <em>JOE</em>, <em>249</em>, 105973. (<a href='https://doi.org/10.1016/j.jeconom.2025.105973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel method to construct uniformly valid confidence bands for a nonparametric component f 1 in the sparse additive model Y = f 1 ( X 1 ) + … + f p ( X p ) + ɛ in a high-dimensional setting. Our method integrates sieve estimation into a high-dimensional Z-estimation framework, facilitating the construction of uniformly valid confidence bands for the target component f 1 . To form these confidence bands, we employ a multiplier bootstrap procedure. Additionally, we provide rates for the uniform lasso estimation in high dimensions, which may be of independent interest. Through simulation studies, we demonstrate that our proposed method delivers reliable results in terms of estimation and coverage, even in small samples.},
  archive      = {J_JOE},
  author       = {Philipp Bach and Sven Klaassen and Jannis Kueck and Martin Spindler},
  doi          = {10.1016/j.jeconom.2025.105973},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105973},
  shortjournal = {J. Econ.},
  title        = {Estimation and uniform inference in sparse high-dimensional additive models},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When structural break meets threshold effect: Factor analysis under structural instabilities. <em>JOE</em>, <em>249</em>, 105972. (<a href='https://doi.org/10.1016/j.jeconom.2025.105972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural instability has been one of the central research questions in economics and finance over many decades. This paper systematically investigates structural instabilities in high dimensional factor models, which portray both structural breaks and threshold effects simultaneously. The observed high dimensional time series are concatenated at an unknown number of break points, while they are described by multiple threshold factor models that are heterogeneous between any two consecutive subsamples. Both joint and sequential procedures for estimating the break points are developed based on the second moment of the pseudo factor estimates that fully ignore the structural instabilities. In each separated subsample, the group Lasso approach recently proposed by Ma and Tu (2023b) is adopted to efficiently identify the threshold factor structure. An information criterion is further proposed to determine the number of break points, which also serves the purpose to distinguish the two types of instabilities. Theoretical properties of the proposed estimators are established, and their finite sample performance is evaluated in Monte Carlo simulations. An empirical application to the U.S. financial market dataset demonstrates the consequences when structural break meets threshold effect in factor analysis.},
  archive      = {J_JOE},
  author       = {Chenchen Ma and Yundong Tu},
  doi          = {10.1016/j.jeconom.2025.105972},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105972},
  shortjournal = {J. Econ.},
  title        = {When structural break meets threshold effect: Factor analysis under structural instabilities},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large confirmatory dynamic factor model for stock market returns in different time zones. <em>JOE</em>, <em>249</em>, 105971. (<a href='https://doi.org/10.1016/j.jeconom.2025.105971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a confirmatory dynamic factor model for a large number of stocks whose returns are observed daily across multiple time zones. The model has a global factor and a continental factor that both drive the individual stock return series. We propose two estimators of the model: a quasi-maximum likelihood estimator (QML-just-identified), and an improved estimator based on an Expectation Maximization (EM) algorithm (QML-all-res). Our estimators are consistent and asymptotically normal under the large approximate factor model setting. In particular, the asymptotic distributions of QML-all-res are the same as those of the infeasible OLS estimators that treat factors as known and utilize all the restrictions on the parameters of the model. We apply the model to MSCI equity indices of 42 developed and emerging markets, and find that most markets are more integrated when the CBOE Volatility Index (VIX) is high.},
  archive      = {J_JOE},
  author       = {Oliver B. Linton and Haihan Tang and Jianbin Wu},
  doi          = {10.1016/j.jeconom.2025.105971},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105971},
  shortjournal = {J. Econ.},
  title        = {A large confirmatory dynamic factor model for stock market returns in different time zones},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning for economic policy. <em>JOE</em>, <em>249</em>, 105970. (<a href='https://doi.org/10.1016/j.jeconom.2025.105970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Themed Issue Machine Learning for Economic Policy consists of 12 papers at the intersection of machine learning, nontraditional data sources and economic policymaking. We will introduce the Themed Issue and review its contributions.},
  archive      = {J_JOE},
  author       = {Maryam Haghighi and Andreas Joseph and George Kapetanios and Christopher Kurz and Michele Lenza and Juri Marcucci},
  doi          = {10.1016/j.jeconom.2025.105970},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105970},
  shortjournal = {J. Econ.},
  title        = {Machine learning for economic policy},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On time-varying panel data models with time-varying interactive fixed effects. <em>JOE</em>, <em>249</em>, 105960. (<a href='https://doi.org/10.1016/j.jeconom.2025.105960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a time-varying (TV) panel data model with interactive fixed effects where both the coefficients and factor loadings are allowed to change smoothly over time. We propose a local version of the least squares and principal component method to estimate the TV coefficients, TV factor loadings, and common factors simultaneously. We provide a bias-corrected local least squares estimator for the TV coefficients and establish the limiting distributions and uniform convergence of the bias-corrected coefficient estimators, estimated factors, and factor loadings in the large N and large T framework. Based on the estimates, we propose three test statistics to gauge possible sources of TV features. We establish the limit null distributions and the asymptotic local power properties of our tests. Simulations are conducted to evaluate the finite sample performance of our estimates and tests. We apply our theoretical results to analyze the Phillips curve using the U.S. state-level unemployment rates and nominal wages, and document significant TV behavior in both the slope coefficient and factor loadings.},
  archive      = {J_JOE},
  author       = {Xia Wang and Sainan Jin and Yingxing Li and Junhui Qian and Liangjun Su},
  doi          = {10.1016/j.jeconom.2025.105960},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105960},
  shortjournal = {J. Econ.},
  title        = {On time-varying panel data models with time-varying interactive fixed effects},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiplicative factor model for volatility. <em>JOE</em>, <em>249</em>, 105959. (<a href='https://doi.org/10.1016/j.jeconom.2025.105959'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facilitated with high-frequency observations, we introduce a remarkably parsimonious one-factor volatility model that offers a novel perspective for comprehending daily volatilities of a large number of stocks. Specifically, we propose a multiplicative volatility factor (MVF) model, where stock daily variance is represented by a common variance factor and a multiplicative idiosyncratic component. We demonstrate compelling empirical evidence supporting our model and provide statistical properties for two simple estimation methods. The MVF model reflects important properties of volatilities, applies to both individual stocks and portfolios, can be easily estimated, and leads to exceptional predictive performance in both US stocks and global equity indices.},
  archive      = {J_JOE},
  author       = {Yi Ding and Robert Engle and Yingying Li and Xinghua Zheng},
  doi          = {10.1016/j.jeconom.2025.105959},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105959},
  shortjournal = {J. Econ.},
  title        = {Multiplicative factor model for volatility},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized estimation of finite mixture models. <em>JOE</em>, <em>249</em>, 105958. (<a href='https://doi.org/10.1016/j.jeconom.2025.105958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Economists often model unobserved heterogeneity using finite mixtures. In practice, the number of mixture components is rarely known. Model parameters lack point-identification if the estimation includes too many components, thus invalidating the classic properties of maximum likelihood estimation. I propose a penalized likelihood method to estimate finite mixtures with an unknown number of components. The resulting Order-Selection-Consistent Estimator (OSCE) consistently estimates the true number of components and achieves oracle efficiency. This paper extends penalized estimation to models without point-identification and to mixtures with growing number of components. I apply the OSCE to estimate players’ rationality levels in a coordination game.},
  archive      = {J_JOE},
  author       = {Sofya Budanova},
  doi          = {10.1016/j.jeconom.2025.105958},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105958},
  shortjournal = {J. Econ.},
  title        = {Penalized estimation of finite mixture models},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-dimensional heterogeneous panel data models with multi-level interactive fixed effects. <em>JOE</em>, <em>249</em>, 105957. (<a href='https://doi.org/10.1016/j.jeconom.2025.105957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a three-dimensional (3D) panel data model with heterogeneous slope coefficients and multi-level interactive fixed effects consisting of latent global factors and two types of local factors. Our model nests many commonly used 3D panel data models. We propose an iterative estimation procedure that relies on initial consistent estimators obtained through a novel defactored approach. We study the asymptotic properties of our estimators and show that our iterative estimators of the slope coefficients are “oracle efficient” in the sense that they are asymptotically equivalent to those when the factors were known. Some specification testing issues are also considered. Monte Carlo simulations demonstrate that our estimators and tests perform well in finite samples. We apply our new method to the international trade dataset.},
  archive      = {J_JOE},
  author       = {Sainan Jin and Xun Lu and Liangjun Su},
  doi          = {10.1016/j.jeconom.2025.105957},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105957},
  shortjournal = {J. Econ.},
  title        = {Three-dimensional heterogeneous panel data models with multi-level interactive fixed effects},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification and estimation of a search model with heterogeneous consumers and firms. <em>JOE</em>, <em>249</em>, 105956. (<a href='https://doi.org/10.1016/j.jeconom.2025.105956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model of nonsequential consumer search where consumers and firms differ in search and production costs respectively. We characterize the equilibrium of the game. We first show the distribution of search cost can be identified by market shares and prices. Subsequently, we identify the production cost distribution using a similar strategy to Guerre, Perrigne and Vuong (2000) as the firms’ decision problems resemble bidders’ problems in a particular procurement auction. We prove the firm’s cost density can be estimated at the same convergence rate as the optimal rate in Guerre et al. uniformly over any fixed subset on the interior of the support. The uniform convergence rate over any expanding support is slower due to a pole in the price pdf that is a feature of the equilibrium. Our simulation study confirms the theoretical features of the model. Our identification and convergence rate results also apply to two generalizations of the baseline search model that allow for: (i) vertically differentiated products; (ii) an intermediary. We apply the latter model to study loan search using UK mortgage data.},
  archive      = {J_JOE},
  author       = {Mateusz Myśliwski and May Rostom and Fabio Sanches and Daniel Silva Jr and Sorawoot Srisuma},
  doi          = {10.1016/j.jeconom.2025.105956},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105956},
  shortjournal = {J. Econ.},
  title        = {Identification and estimation of a search model with heterogeneous consumers and firms},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simple subvector inference on sharp identified set in affine models. <em>JOE</em>, <em>249</em>, 105952. (<a href='https://doi.org/10.1016/j.jeconom.2025.105952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a regularized support function estimator for bounds on components of the parameter vector in the case in which the identified set is a polygon. The proposed regularized estimator has three important properties: (i) it has a uniform asymptotic Gaussian limit in the presence of flat faces in the absence of redundant (or overidentifying) constraints (or vice versa); (ii) the bias from regularization does not enter the first-order limiting distribution; (iii) the estimator remains consistent for sharp (non-enlarged) identified set for the individual components even in the non-regular case. These properties are used to construct uniformly valid confidence sets for an element θ 1 of a parameter vector θ ∈ R d that is partially identified by affine moment equality and inequality conditions. The proposed confidence sets can be computed as a solution to a small number of linear and convex quadratic programs, leading to a substantial decrease in computation time and guarantees a global optimum. As a result, the method provides a uniformly valid inference in applications in which the dimension of the parameter space, d , and the number of inequalities, k , were previously computationally unfeasible ( d , k = 100 ). The proposed approach can be extended to construct confidence sets for intersection bounds, to construct joint polygon-shaped confidence sets for multiple components of θ , and to find the set of solutions to a linear program. Inference for coefficients in the linear IV regression model with an interval outcome is used as an illustrative example.},
  archive      = {J_JOE},
  author       = {Bulat Gafarov},
  doi          = {10.1016/j.jeconom.2025.105952},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105952},
  shortjournal = {J. Econ.},
  title        = {Simple subvector inference on sharp identified set in affine models},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Central bank mandates and monetary policy stances: Through the lens of federal reserve speeches. <em>JOE</em>, <em>249</em>, 105948. (<a href='https://doi.org/10.1016/j.jeconom.2025.105948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Federal Reserve System has an institutional mandate to pursue price stability and maximum sustainable employment; however, it remains unclear whether it can also pursue secondary objectives. The academic literature has largely argued that it should not. We characterize the Fed’s interpretation of its mandate using state-of-the-art methods from natural language processing, including a collection of large language models (LLMs) that we modify for enhanced performance on central bank texts. We apply these methods and models to a comprehensive corpus of Fed speeches delivered between 1960 and 2022. We find that the Fed perceives financial stability to be the most important policy concern that is not directly enumerated in its mandate, especially in times when the debt-to-GDP ratio is high, but does not generally treat it as a separate policy objective. In its policy discourse, it has frequently discussed the use of monetary policy to achieve financial stability, which we demonstrate generates movements in asset prices, even after rigorously controlling for macroeconomic and financial variables.},
  archive      = {J_JOE},
  author       = {Christoph Bertsch and Isaiah Hull and Robin L. Lumsdaine and Xin Zhang},
  doi          = {10.1016/j.jeconom.2025.105948},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105948},
  shortjournal = {J. Econ.},
  title        = {Central bank mandates and monetary policy stances: Through the lens of federal reserve speeches},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Themed issue: Quantile regression and data heterogeneity. <em>JOE</em>, <em>249</em>, 105946. (<a href='https://doi.org/10.1016/j.jeconom.2024.105946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Xiaohong Chen and Xuming He},
  doi          = {10.1016/j.jeconom.2024.105946},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105946},
  shortjournal = {J. Econ.},
  title        = {Themed issue: Quantile regression and data heterogeneity},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning who to nudge: Causal vs predictive targeting in a field experiment on student financial aid renewal. <em>JOE</em>, <em>249</em>, 105945. (<a href='https://doi.org/10.1016/j.jeconom.2024.105945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many settings, interventions may be more effective for some individuals than for others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use “nudges” to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predicted probability. Nonetheless, targeting on low baseline outcomes is common in practice, for example when treatment effects are difficult to estimate. We propose hybrid approaches that incorporate the strengths of predictive approaches (accurate estimation) and causal approaches (correct criterion). We show that targeting intermediate baseline outcomes is most effective in our application, while targeting based on low baseline outcomes is detrimental. In one year of the experiment, nudging all students improved early filing by an average of 6.4 percentage points over a baseline average of 37%, and we estimate that targeting half of the students using our preferred policy attains around 75% of this benefit.},
  archive      = {J_JOE},
  author       = {Susan Athey and Niall Keleher and Jann Spiess},
  doi          = {10.1016/j.jeconom.2024.105945},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105945},
  shortjournal = {J. Econ.},
  title        = {Machine learning who to nudge: Causal vs predictive targeting in a field experiment on student financial aid renewal},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating time-varying networks for high-dimensional time series. <em>JOE</em>, <em>249</em>, 105941. (<a href='https://doi.org/10.1016/j.jeconom.2024.105941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore time-varying networks for high-dimensional locally stationary time series, using the large VAR model framework with both the transition and (error) precision matrices evolving smoothly over time. Two types of time-varying graphs are investigated: one containing directed edges of Granger causality linkages, and the other containing undirected edges of partial correlation linkages. Under the sparse structural assumption, we propose a penalised local linear method with time-varying weighted group LASSO to jointly estimate the transition matrices and identify their significant entries, and a time-varying CLIME method to estimate the precision matrices. The estimated transition and precision matrices are then used to determine the time-varying network structures. Under some mild conditions, we derive the theoretical properties of the proposed estimates including the consistency and oracle properties. In addition, we extend the methodology and theory to cover highly-correlated large-scale time series, for which the sparsity assumption becomes invalid and we allow for common factors before estimating the factor-adjusted time-varying networks. We provide extensive simulation studies and an empirical application to a large U.S. macroeconomic dataset to illustrate the finite-sample performance of our methods.},
  archive      = {J_JOE},
  author       = {Jia Chen and Degui Li and Yu-Ning Li and Oliver Linton},
  doi          = {10.1016/j.jeconom.2024.105941},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105941},
  shortjournal = {J. Econ.},
  title        = {Estimating time-varying networks for high-dimensional time series},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for smoothed quantile regression with streaming data. <em>JOE</em>, <em>249</em>, 105924. (<a href='https://doi.org/10.1016/j.jeconom.2024.105924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the problem of conducting valid statistical inference for quantile regression with streaming data. The main difficulties are that the quantile regression loss function is non-smooth and it is often infeasible to store the entire dataset in memory, rendering traditional methodologies ineffective. We introduce a fully online updating method for statistical inference in smoothed quantile regression with streaming data to overcome these issues. Our main contributions are twofold. First, for low-dimensional data, we present an incremental updating algorithm to obtain the smoothed quantile regression estimator with the streaming data set. The proposed estimator allows us to construct asymptotically exact statistical inference procedures. Second, within the realm of high-dimensional data, we develop an online debiased lasso procedure to accommodate the special sparse structure of streaming data. The proposed online debiased approach is updated with only the current data and summary statistics of historical data and corrects an approximation error term from online updating with streaming data. Furthermore, theoretical results such as estimation consistency and asymptotic normality are established to justify its validity in both settings. Our findings are supported by simulation studies and illustrated through applications to Seoul’s bike-sharing demand data and index fund data.},
  archive      = {J_JOE},
  author       = {Jinhan Xie and Xiaodong Yan and Bei Jiang and Linglong Kong},
  doi          = {10.1016/j.jeconom.2024.105924},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105924},
  shortjournal = {J. Econ.},
  title        = {Statistical inference for smoothed quantile regression with streaming data},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Satellites turn “concrete”: Tracking cement with satellite data and neural networks. <em>JOE</em>, <em>249</em>, 105923. (<a href='https://doi.org/10.1016/j.jeconom.2024.105923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper exploits daily infrared images taken from satellites to track economic activity in advanced and emerging countries. We first develop a framework to read, clean, and exploit satellite images. Our algorithm uses the laws of physics (Planck's law) and machine learning to detect the heat produced by cement plants in activity. This allows us to monitor in real-time whether a cement plant is working. Using this on around 1,000 plants, we construct a satellite-based index. We show that using this satellite index outperforms benchmark models and alternative indicators for nowcasting the production of the cement industry as well as the activity in the construction sector. Comparing across methods, neural networks appear to yield more accurate predictions as they allow to exploit the granularity of our dataset. Overall, combining satellite images and machine learning can help policymakers to take informed and swift economic policy decisions by nowcasting accurately and in real-time economic activity.},
  archive      = {J_JOE},
  author       = {Alexandre d'Aspremont and Simon Ben Arous and Jean-Charles Bricongne and Benjamin Lietti and Baptiste Meunier},
  doi          = {10.1016/j.jeconom.2024.105923},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105923},
  shortjournal = {J. Econ.},
  title        = {Satellites turn “concrete”: Tracking cement with satellite data and neural networks},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval quantile correlations with applications to testing high-dimensional quantile effects. <em>JOE</em>, <em>249</em>, 105922. (<a href='https://doi.org/10.1016/j.jeconom.2024.105922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose interval quantile correlation and interval quantile partial correlation to measure the association between two random variables over an interval of quantile levels. We construct efficient estimators for the proposed correlations, and establish their asymptotic properties under the null and alternative hypotheses. We further use the interval quantile partial correlation to test for the significance of covariate effects in high-dimensional quantile regression when a subset of covariates are controlled. We calculate marginal interval quantile partial correlations for each covariate, then aggregate them to construct a sum-type test statistic. The null distribution of our proposed test statistic is asymptotically standard normal. We use extensive simulations and an application to illustrate that our proposed test, which pools information across an interval of quantile levels to enhance power performances, is very effective in detecting quantile effects.},
  archive      = {J_JOE},
  author       = {Yaowu Zhang and Yeqing Zhou and Liping Zhu},
  doi          = {10.1016/j.jeconom.2024.105922},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105922},
  shortjournal = {J. Econ.},
  title        = {Interval quantile correlations with applications to testing high-dimensional quantile effects},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mind your language: Market responses to central bank speeches. <em>JOE</em>, <em>249</em>, 105921. (<a href='https://doi.org/10.1016/j.jeconom.2024.105921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Central bank communication between meetings often moves markets, but researchers have traditionally paid less attention to it. Using a dataset of U.S. Federal Reserve speeches, we develop supervised multimodal natural language processing methods to identify how monetary policy news affect bond and stock market volatility and tail risk through implied changes in forecasts of GDP, inflation, and unemployment. We find that forecast revisions derived from FOMC-member speech can help explain volatility and tail risk in both equity and bond markets. Speeches from Chairs tend to produce larger forecast revisions and unconditionally raise volatility and tail risk, but their economic signals can calm markets (reduce volatility and tail risk). There is some evidence that a speaker’s monetary policy views may affect the impact of implied forecast revisions after conditioning on GDP growth.},
  archive      = {J_JOE},
  author       = {Maximilian Ahrens and Deniz Erdemlioglu and Michael McMahon and Christopher J. Neely and Xiye Yang},
  doi          = {10.1016/j.jeconom.2024.105921},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105921},
  shortjournal = {J. Econ.},
  title        = {Mind your language: Market responses to central bank speeches},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference on time series nonparametric conditional moment restrictions using nonlinear sieves. <em>JOE</em>, <em>249</em>, 105920. (<a href='https://doi.org/10.1016/j.jeconom.2024.105920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies estimation of and inference on dynamic nonparametric conditional moment restrictions of high dimensional variables for weakly dependent data, where the unknown functions of endogenous variables can be approximated via nonlinear sieves such as neural networks and Gaussian radial bases. The true unknown functions and their sieve approximations are allowed to be in general weighted function spaces with unbounded supports, which is important for time series data. Under some regularity conditions, the optimally weighted general nonlinear sieve quasi-likelihood ratio (GN-QLR) statistic for the expectation functional of unknown function is asymptotically Chi-square distributed regardless whether the functional could be estimated at a root- n rate or not, and the estimated expectation functional is asymptotically efficient if it is root- n estimable. Our general theories are applied to two important examples: (1) estimating the value function and the off-policy evaluation in reinforcement learning (RL); and (2) estimating the averaged partial mean and averaged partial derivative of dynamic nonparametric quantile instrumental variable (NPQIV) models. We demonstrate the finite sample performance of our optimal inference procedure on averaged partial derivative of a dynamic NPQIV model in simulation studies.},
  archive      = {J_JOE},
  author       = {Xiaohong Chen and Yuan Liao and Weichen Wang},
  doi          = {10.1016/j.jeconom.2024.105920},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105920},
  shortjournal = {J. Econ.},
  title        = {Inference on time series nonparametric conditional moment restrictions using nonlinear sieves},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Paying over the odds at the end of the fiscal year. evidence from ukraine. <em>JOE</em>, <em>249</em>, 105903. (<a href='https://doi.org/10.1016/j.jeconom.2024.105903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Governments are the largest buyers in most countries. They tend to operate annually expiring budgets and spend disproportionately large amounts at year-end. This paper is the first to investigate whether supplier firms increase prices at year-end to benefit from this behaviour. We develop a novel method using neural networks to estimate firms margins from the bidding behaviour of other firms in procurement auctions. We use a dataset of Ukrainian government procurement between 2017 and 2021 to document significantly higher prices and supplier profit margins at year-end. We demonstrate how results change depending on the type of good, the length of the buyer–supplier relationship, and the impact of Covid-19. Finally, we provide policy suggestions on how funds could be saved.},
  archive      = {J_JOE},
  author       = {Margaryta Klymak and Stuart Baumann},
  doi          = {10.1016/j.jeconom.2024.105903},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105903},
  shortjournal = {J. Econ.},
  title        = {Paying over the odds at the end of the fiscal year. evidence from ukraine},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How do firms’ financial conditions influence the transmission of monetary policy? a non-parametric local projection approach. <em>JOE</em>, <em>249</em>, 105886. (<a href='https://doi.org/10.1016/j.jeconom.2024.105886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do monetary policy shocks affect firm investment? This paper provides new evidence on US non-financial firms and a novel non-parametric framework based on random forests. The key advantage of the methodology is that it does not impose any assumptions on how the effect of shocks varies across firms thereby allowing for general forms of heterogeneity in the transmission of shocks. My estimates suggest that there exists a threshold in the level of firm risk above which monetary policy is much less effective. Additionally, there is no evidence that the effect of policy varies with firm risk for the 75% of firms in the sample with higher risk. The proposed methodology is a generalization of local projections and nests several common local projection specifications, including linear and nonlinear.},
  archive      = {J_JOE},
  author       = {Livia Paranhos},
  doi          = {10.1016/j.jeconom.2024.105886},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105886},
  shortjournal = {J. Econ.},
  title        = {How do firms’ financial conditions influence the transmission of monetary policy? a non-parametric local projection approach},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Central bank communication on social media: What, to whom, and how?. <em>JOE</em>, <em>249</em>, 105869. (<a href='https://doi.org/10.1016/j.jeconom.2024.105869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study answers three questions about central bank communication on Twitter : what was communicated, who were listeners, and how they reacted. Using various natural language processing techniques, we identify the main topics discussed by the Fed and major audiences. While the Fed tweets talking about central banking topics attract greater attention from Twitter users, only the extensive margin is economically meaningful. Among all groups of users, the media accounts and economists are most active in engaging with the Fed, especially when discussing central banking-related issues. We also show that information extracted from the tweets can provide a real-time, qualitative diagnostic for inflation expectations and some reaction of these Twitter-based inflation expectations to policy action and communication.},
  archive      = {J_JOE},
  author       = {Yuriy Gorodnichenko and Tho Pham and Oleksandr Talavera},
  doi          = {10.1016/j.jeconom.2024.105869},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105869},
  shortjournal = {J. Econ.},
  title        = {Central bank communication on social media: What, to whom, and how?},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiway empirical likelihood. <em>JOE</em>, <em>249</em>, 105861. (<a href='https://doi.org/10.1016/j.jeconom.2024.105861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a general methodology to conduct statistical inference for observations indexed by multiple sets of entities. We propose a novel multiway empirical likelihood statistic that converges to a chi-square distribution under the non-degenerate case, where corresponding Hoeffding type decomposition is dominated by linear terms. Our methodology is related to the notion of jackknife empirical likelihood but the leave-out pseudo values are constructed by leaving out columns or rows. We further develop a modified version of our multiway empirical likelihood statistic, which converges to a chi-square distribution regardless of the degeneracy, and discuss its desirable higher-order property in a simplified setup. The proposed methodology is illustrated by several important econometric problems, such as bipartite network, generalized estimating equations, and three-way observations.},
  archive      = {J_JOE},
  author       = {Harold D. Chiang and Yukitoshi Matsushita and Taisuke Otsu},
  doi          = {10.1016/j.jeconom.2024.105861},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105861},
  shortjournal = {J. Econ.},
  title        = {Multiway empirical likelihood},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient quantile covariate adjusted response adaptive experiments. <em>JOE</em>, <em>249</em>, 105857. (<a href='https://doi.org/10.1016/j.jeconom.2024.105857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In program evaluation studies, understanding the heterogeneous distributional impacts of a program beyond the average effect is crucial. Quantile treatment effect (QTE) provides a natural measure to capture such heterogeneity. While much of the existing work for estimating QTE has focused on analyzing observational data based on untestable causal assumptions, little work has gone into designing randomized experiments specifically for estimating QTE. In this manuscript, we propose two covariate-adjusted response adaptive design strategies–fully adaptive designs and multi-stage designs–to efficiently estimate the QTE. We demonstrate that the QTE estimator obtained from our designs attains the optimal variance lower bound from a semiparametric theory perspective, which does not impose any parametric assumptions on underlying data distributions. Moreover, we show that using continuous covariates in multi-stage designs can improve the precision of the estimated QTE compared to the classical fully adaptive setting. We illustrate the finite-sample performance of our designs through Monte Carlo experiments and one synthetic case study on charitable giving. Our proposed designs offer a new approach to conducting randomized experiments to estimate QTE, which can have important implications for policy and program evaluation.},
  archive      = {J_JOE},
  author       = {Zhonghua Li and Lan Luo and Jingshen Wang and Long Feng},
  doi          = {10.1016/j.jeconom.2024.105857},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105857},
  shortjournal = {J. Econ.},
  title        = {Efficient quantile covariate adjusted response adaptive experiments},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Refining public policies with machine learning: The case of tax auditing. <em>JOE</em>, <em>249</em>, 105847. (<a href='https://doi.org/10.1016/j.jeconom.2024.105847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study how machine learning techniques can be used to improve tax auditing efficiency using administrative data without the need of randomized audits. Using Italy’s population data on sole proprietorship tax returns and audits, our new approach addresses the challenge that predictions must be trained on human-selected data. There are substantial margins for raising revenue from audits by improving the selection of taxpayers to audit with machine learning. Replacing the 10% least promising audits with an equal number selected by our algorithm raises detected tax evasion by as much as 39%, and evasion that is actually paid back by 29%.},
  archive      = {J_JOE},
  author       = {Marco Battaglini and Luigi Guiso and Chiara Lacava and Douglas L. Miller and Eleonora Patacchini},
  doi          = {10.1016/j.jeconom.2024.105847},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105847},
  shortjournal = {J. Econ.},
  title        = {Refining public policies with machine learning: The case of tax auditing},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian neural networks for macroeconomic analysis. <em>JOE</em>, <em>249</em>, 105843. (<a href='https://doi.org/10.1016/j.jeconom.2024.105843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Macroeconomic data is characterized by a limited number of observations (small T ), many time series (big K ) but also by featuring temporal dependence. Neural networks, by contrast, are designed for datasets with millions of observations and covariates. In this paper, we develop Bayesian neural networks (BNNs) that are well-suited for handling datasets commonly used for macroeconomic analysis in policy institutions. Our approach avoids extensive specification searches through a novel mixture specification for the activation function that appropriately selects the form of nonlinearities. Shrinkage priors are used to prune the network and force irrelevant neurons to zero. To cope with heteroskedasticity, the BNN is augmented with a stochastic volatility model for the error term. We illustrate how the model can be used in a policy institution through simulations and by showing that BNNs produce more accurate point and density forecasts compared to other machine learning methods.},
  archive      = {J_JOE},
  author       = {Niko Hauzenberger and Florian Huber and Karin Klieber and Massimiliano Marcellino},
  doi          = {10.1016/j.jeconom.2024.105843},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105843},
  shortjournal = {J. Econ.},
  title        = {Bayesian neural networks for macroeconomic analysis},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inequality and the zero lower bound. <em>JOE</em>, <em>249</em>, 105819. (<a href='https://doi.org/10.1016/j.jeconom.2024.105819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies how household inequality shapes the effects of the zero lower bound (ZLB) on nominal interest rates on aggregate dynamics. To do so, we consider a heterogeneous agent New Keynesian (HANK) model with an occasionally binding ZLB and solve for its fully nonlinear stochastic equilibrium using a novel neural network algorithm. In this setting, changes in the monetary policy stance influence households’ precautionary savings by altering the frequency of ZLB events. As a result, the model features monetary policy non-neutrality in the long run. The degree of long-run non-neutrality, i.e., by how much monetary policy shifts real rates in the ergodic distribution of the model, can be substantial when we combine low inflation targets and high levels of wealth inequality.},
  archive      = {J_JOE},
  author       = {Jesús Fernández-Villaverde and Joël Marbet and Galo Nuño and Omar Rachedi},
  doi          = {10.1016/j.jeconom.2024.105819},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105819},
  shortjournal = {J. Econ.},
  title        = {Inequality and the zero lower bound},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing heterogeneous treatment effect with quantile regression under covariate-adaptive randomization. <em>JOE</em>, <em>249</em>, 105808. (<a href='https://doi.org/10.1016/j.jeconom.2024.105808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In economic studies and clinical trials, it is prevalent to observe heterogeneous treatment effects that vary depending on the relative locations of units in the distribution of responses. In this study, we propose using quantile regression to estimate and conduct inference for conditional quantile treatment effects (cQTEs) in covariate-adaptive randomized experiments. First, we present sufficient conditions for consistently estimating the cQTEs, concerning the bias due to omitting important covariates in the inference stage. Second, we derive the weak convergence of the quantile regression process and develop a covariate-adaptive randomized bootstrap ( CAR-BS ) for standard error estimation. Our theoretical results indicate that the Wald test adjusted by CAR-BS is valid in terms of the Type I error, for a large class of covariate-adaptive randomization procedures at different quantiles, regardless of the choice of covariates used in inference. We perform extensive numerical and empirical studies to demonstrate advantages of the new method in various settings.},
  archive      = {J_JOE},
  author       = {Yang Liu and Lucy Xia and Feifang Hu},
  doi          = {10.1016/j.jeconom.2024.105808},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105808},
  shortjournal = {J. Econ.},
  title        = {Testing heterogeneous treatment effect with quantile regression under covariate-adaptive randomization},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On superlevel sets of conditional densities and multivariate quantile regression. <em>JOE</em>, <em>249</em>, 105807. (<a href='https://doi.org/10.1016/j.jeconom.2024.105807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some common proposals of multivariate quantiles do not sufficiently control the probability content, while others do not always accurately reflect the concentration of probability mass. We suggest superlevel sets of conditional multivariate densities as an alternative to current multivariate quantile definitions. Hence, the superlevel set is a function of conditioning variables much like in quantile regression. We show that conditional superlevel sets have favorable mathematical and intuitive features, and support a clear probabilistic interpretation. We derive the superlevel sets for a conditional or marginal density of interest from an (overfitted) multivariate Gaussian mixture model. This approach guarantees logically consistent (i.e., non-crossing) conditional superlevel sets and also allows us to obtain more traditional univariate quantiles. We demonstrate recovery of the true conditional univariate quantiles for distributions with correlation, heteroskedasticity, or asymmetry and apply our method in univariate and multivariate settings to a study on household expenditures.},
  archive      = {J_JOE},
  author       = {Annika Camehl and Dennis Fok and Kathrin Gruber},
  doi          = {10.1016/j.jeconom.2024.105807},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105807},
  shortjournal = {J. Econ.},
  title        = {On superlevel sets of conditional densities and multivariate quantile regression},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential quantile regression for stream data by least squares. <em>JOE</em>, <em>249</em>, 105791. (<a href='https://doi.org/10.1016/j.jeconom.2024.105791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive stream data are common in modern economics applications, such as e-commerce and finance. They cannot be permanently stored due to storage limitation, and real-time analysis needs to be updated frequently as new data become available. In this paper, we develop a sequential algorithm, SQR, to support efficient quantile regression (QR) analysis for stream data. Due to the non-smoothness of the check loss, popular gradient-based methods do not directly apply. Our proposed algorithm, partly motivated by the Bayesian QR, converts the non-smooth optimization into a least squares problem and is hence significantly faster than existing algorithms that all require solving a linear programming problem in local processing. We further extend the SQR algorithm to composite quantile regression (CQR), and prove that the SQR estimator is unbiased, asymptotically normal and enjoys a linear convergence rate under mild conditions. We also demonstrate the estimation and inferential performance of SQR through simulation experiments and a real data example on a US used car price data set.},
  archive      = {J_JOE},
  author       = {Ye Fan and Nan Lin},
  doi          = {10.1016/j.jeconom.2024.105791},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105791},
  shortjournal = {J. Econ.},
  title        = {Sequential quantile regression for stream data by least squares},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile control via random forest. <em>JOE</em>, <em>249</em>, 105789. (<a href='https://doi.org/10.1016/j.jeconom.2024.105789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies robust inference procedure for treatment effects in panel data with flexible relationship across units via the random forest method. The key contribution of this paper is twofold. First, we propose a direct construction of prediction intervals for the treatment effect by exploiting the information of the joint distribution of the cross-sectional units using random forest. In particular, we propose a Quantile Control Method (QCM) using the Quantile Random Forest (QRF) to accommodate flexible cross-sectional structure as well as high dimensionality. Second, we establish the asymptotic consistency of QRF under the panel/time series setup with high dimensionality, which is of theoretical interest on its own right. In addition, Monte Carlo simulations are conducted and show that prediction intervals via the QCM have excellent coverage probability for the treatment effects comparing to existing methods in the literature, and are robust to heteroskedasticity, autocorrelation, and various types of model misspecifications. Finally, an empirical application to study the effect of the economic integration between Hong Kong and mainland China on Hong Kong’s economy is conducted to highlight the potential of the proposed method.},
  archive      = {J_JOE},
  author       = {Qiang Chen and Zhijie Xiao and Qingsong Yao},
  doi          = {10.1016/j.jeconom.2024.105789},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105789},
  shortjournal = {J. Econ.},
  title        = {Quantile control via random forest},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monitoring multi-country macroeconomic risk: A quantile factor-augmented vector autoregressive (QFAVAR) approach. <em>JOE</em>, <em>249</em>, 105730. (<a href='https://doi.org/10.1016/j.jeconom.2024.105730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-country quantile factor-augmented vector autoregression is proposed to model heterogeneities both across countries and across characteristics of the distributions of macroeconomic time series. The presence of quantile factors enables a parsimonious summary of these two heterogeneities by accounting for dependencies in the cross-sectional dimension as well as across different quantiles of macroeconomic data. Using monthly euro area data, the strong empirical performance of the new model in gauging the impact of global shocks on country-level macroeconomic risks is demonstrated. The short-term tail forecasts of QFAVAR outperform those of FAVARs with symmetric Gaussian errors as well as univariate and multivariate specifications featuring stochastic volatility. Modeling individual quantiles enables scenario analysis of macroeconomic risks, a unique feature absent in FAVARs with stochastic volatility or flexible error distributions.},
  archive      = {J_JOE},
  author       = {Dimitris Korobilis and Maximilian Schröder},
  doi          = {10.1016/j.jeconom.2024.105730},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105730},
  shortjournal = {J. Econ.},
  title        = {Monitoring multi-country macroeconomic risk: A quantile factor-augmented vector autoregressive (QFAVAR) approach},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unconditional quantile partial effects via conditional quantile regression. <em>JOE</em>, <em>249</em>, 105678. (<a href='https://doi.org/10.1016/j.jeconom.2024.105678'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a semi-parametric procedure for estimation of unconditional quantile partial effects using quantile regression coefficients. The estimator is based on an identification result showing that, for continuous covariates, unconditional quantile effects are a weighted average of conditional ones at particular quantile levels that depend on the covariates. We propose a two-step estimator for the unconditional effects where in the first step one estimates a structural quantile regression model, and in the second step a nonparametric regression is applied to the first step coefficients. We establish the asymptotic properties of the estimator, say consistency and asymptotic normality. Monte Carlo simulations show numerical evidence that the estimator has very good finite sample performance and is robust to the selection of bandwidth and kernel. To illustrate the proposed method, we study the canonical application of the Engel’s curve, i.e. food expenditures as a share of income.},
  archive      = {J_JOE},
  author       = {Javier Alejo and Antonio F. Galvao and Julian Martinez-Iriarte and Gabriel Montes-Rojas},
  doi          = {10.1016/j.jeconom.2024.105678},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105678},
  shortjournal = {J. Econ.},
  title        = {Unconditional quantile partial effects via conditional quantile regression},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributional counterfactual analysis in high-dimensional setup. <em>JOE</em>, <em>249</em>, 105675. (<a href='https://doi.org/10.1016/j.jeconom.2024.105675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of treatment effect estimation, this paper proposes a new methodology to recover the counterfactual distribution when there is a single (or a few) treated unit and possibly a high-dimensional number of potential controls observed in a panel structure. The methodology accommodates, albeit does not require, the number of units to be larger than the number of time periods (high-dimensional setup). As opposed to model only the conditional mean, we propose to model the entire conditional quantile function (CQF) in the absence of intervention and estimate it using the pre-intervention period using a penalized regression. We derive non-asymptotic bounds for the estimated CQF valid uniformly over the quantiles, allowing the practitioner to re-construct the entire contractual distribution. Moreover, we bound the probability coverage of this estimated CQF which can be used to construct valid confidence intervals for the (possibly random) treatment effect for every post-intervention period or simultaneously. We also propose a new hypothesis test for the sharp null of no-effect based on the L p norm of deviation of the estimated CQF to the population one. Interestingly, the null distribution is quasi-pivotal in the sense that it only depends on the estimated CQF, L p norm, and the number of post-intervention periods, but not on the size of the post-intervention period. For that reason, critical values can then be easily simulated. We illustrate the methodology is by revisiting the empirical study in Acemoglu et al. (2016).},
  archive      = {J_JOE},
  author       = {Ricardo Masini},
  doi          = {10.1016/j.jeconom.2024.105675},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105675},
  shortjournal = {J. Econ.},
  title        = {Distributional counterfactual analysis in high-dimensional setup},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast inference for quantile regression with tens of millions of observations. <em>JOE</em>, <em>249</em>, 105673. (<a href='https://doi.org/10.1016/j.jeconom.2024.105673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data analytics has opened new avenues in economic research, but the challenge of analyzing datasets with tens of millions of observations is substantial. Conventional econometric methods based on extreme estimators require large amounts of computing resources and memory, which are often not readily available. In this paper, we focus on linear quantile regression applied to “ultra-large” datasets, such as U.S. decennial censuses. A fast inference framework is presented, utilizing stochastic subgradient descent (S-subGD) updates. The inference procedure handles cross-sectional data sequentially: (i) updating the parameter estimate with each incoming “new observation”, (ii) aggregating it as a Polyak–Ruppert average, and (iii) computing a pivotal statistic for inference using only a solution path. The methodology draws from time-series regression to create an asymptotically pivotal statistic through random scaling. Our proposed test statistic is calculated in a fully online fashion and critical values are calculated without resampling. We conduct extensive numerical studies to showcase the computational merits of our proposed inference. For inference problems as large as ( n , d ) ∼ ( 1 0 7 , 1 0 3 ) , where n is the sample size and d is the number of regressors, our method generates new insights, surpassing current inference methods in computation. Our method specifically reveals trends in the gender gap in the U.S. college wage premium using millions of observations, while controlling over 1 0 3 covariates to mitigate confounding effects.},
  archive      = {J_JOE},
  author       = {Sokbae Lee and Yuan Liao and Myung Hwan Seo and Youngki Shin},
  doi          = {10.1016/j.jeconom.2024.105673},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105673},
  shortjournal = {J. Econ.},
  title        = {Fast inference for quantile regression with tens of millions of observations},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference on quantile processes with a finite number of clusters. <em>JOE</em>, <em>249</em>, 105672. (<a href='https://doi.org/10.1016/j.jeconom.2024.105672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I introduce a generic method for inference on entire quantile and regression quantile processes in the presence of a finite number of large and arbitrarily heterogeneous clusters. The method asymptotically controls size by generating statistics that exhibit enough distributional symmetry such that randomization tests can be applied. The randomization test does not require ex-ante matching of clusters, is free of user-chosen parameters, and performs well at conventional significance levels with as few as five clusters. The method tests standard (non-sharp) hypotheses and can even be asymptotically similar in empirically relevant situations. The main focus of the paper is inference on quantile treatment effects but the method applies more broadly. Numerical and empirical examples are provided.},
  archive      = {J_JOE},
  author       = {Andreas Hagemann},
  doi          = {10.1016/j.jeconom.2024.105672},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105672},
  shortjournal = {J. Econ.},
  title        = {Inference on quantile processes with a finite number of clusters},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous estimation and group identification for network vector autoregressive model with heterogeneous nodes. <em>JOE</em>, <em>249</em>, 105564. (<a href='https://doi.org/10.1016/j.jeconom.2023.105564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals or companies in a large social or financial network often display rather heterogeneous behaviors for various reasons. In this work, we propose a network vector autoregressive model with a latent group structure to model heterogeneous dynamic patterns observed from network nodes, for which group-wise network effects and time-invariant fixed-effects can be naturally incorporated. In our framework, the model parameters and network node memberships can be simultaneously estimated by minimizing a least-squares type objective function. In particular, our theoretical investigation allows the number of latent groups G to be over-specified when achieving the estimation consistency of the model parameters and group memberships, which significantly improves the robustness of the proposed approach. When G is correctly specified, valid statistical inference can be made for model parameters based on the asymptotic normality of the estimators. A data-driven criterion is developed to consistently identify the true group number for practical use. Extensive simulation studies and two real data examples are used to demonstrate the effectiveness of the proposed methodology.},
  archive      = {J_JOE},
  author       = {Xuening Zhu and Ganggang Xu and Jianqing Fan},
  doi          = {10.1016/j.jeconom.2023.105564},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105564},
  shortjournal = {J. Econ.},
  title        = {Simultaneous estimation and group identification for network vector autoregressive model with heterogeneous nodes},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric approach to estimation of marginal mean effects and marginal quantile effects. <em>JOE</em>, <em>249</em>, 105455. (<a href='https://doi.org/10.1016/j.jeconom.2023.05.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a semiparametric generalized linear model and study estimation of both marginal mean effects and marginal quantile effects in this model. We propose an approximate maximum likelihood estimator, and rigorously establish the consistency, the asymptotic normality, and the semiparametric efficiency of our method in both the marginal mean effect and the marginal quantile effect estimation. Simulation studies are conducted to illustrate the finite sample performance, and we apply the new tool to analyze a Swiss non-labor income data and discover a new interesting predictor.},
  archive      = {J_JOE},
  author       = {Seong-ho Lee and Yanyuan Ma and Elvezio Ronchetti},
  doi          = {10.1016/j.jeconom.2023.05.002},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105455},
  shortjournal = {J. Econ.},
  title        = {Semiparametric approach to estimation of marginal mean effects and marginal quantile effects},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-splitting algorithms for ultrahigh dimensional quantile regression. <em>JOE</em>, <em>249</em>, 105426. (<a href='https://doi.org/10.1016/j.jeconom.2023.01.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with computational issues related to penalized quantile regression (PQR) with ultrahigh dimensional predictors. Various algorithms have been developed for PQR, but they become ineffective and/or infeasible in the presence of ultrahigh dimensional predictors due to the storage and scalability limitations. The variable updating schema of the feature-splitting algorithm that directly applies the ordinary alternating direction method of multiplier (ADMM) to ultrahigh dimensional PQR may make the algorithm fail to converge. To tackle this hurdle, we propose an efficient and parallelizable algorithm for ultrahigh dimensional PQR based on the three-block ADMM. The compatibility of the proposed algorithm with parallel computing alleviates the storage and scalability limitations of a single machine in the large-scale data processing. We establish the rate of convergence of the newly proposed algorithm. In addition, Monte Carlo simulations are conducted to compare the finite sample performance of the proposed algorithm with that of other existing algorithms. The numerical comparison implies that the proposed algorithm significantly outperforms the existing ones. We further illustrate the proposed algorithm via an empirical analysis of a real-world data set.},
  archive      = {J_JOE},
  author       = {Jiawei Wen and Songshan Yang and Christina Dan Wang and Yifan Jiang and Runze Li},
  doi          = {10.1016/j.jeconom.2023.01.028},
  journal      = {Journal of Econometrics},
  month        = {5},
  pages        = {105426},
  shortjournal = {J. Econ.},
  title        = {Feature-splitting algorithms for ultrahigh dimensional quantile regression},
  volume       = {249},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reprint of: Finite underidentification. <em>JOE</em>, <em>248</em>, 105947. (<a href='https://doi.org/10.1016/j.jeconom.2025.105947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I adapt the Generalised Method of Moments to deal with nonlinear models in which a finite number of isolated parameter values satisfy the moment conditions. I also study the closely related class of first-order underidentified models, whose expected Jacobian is rank deficient but not necessarily zero. In both cases, my proposed procedures exploit the underidentification structure to yield parameter estimators and underidentification tests within a standard asymptotically normal GMM framework. I study nonlinear models with and without separation of data and parameters. I also illustrate my proposed inference procedures with applications to production function estimation and dynamic panel data models.},
  archive      = {J_JOE},
  author       = {Enrique Sentana},
  doi          = {10.1016/j.jeconom.2025.105947},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105947},
  shortjournal = {J. Econ.},
  title        = {Reprint of: Finite underidentification},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying the volatility risk price through the leverage effect. <em>JOE</em>, <em>248</em>, 105943. (<a href='https://doi.org/10.1016/j.jeconom.2024.105943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In asset pricing models with stochastic volatility, uncertainty about volatility affects risk premia through two channels: aversion to decreasing returns and aversion to increasing volatility. We analyze the identification of and robust inference for structural parameters measuring investors’ aversions to these risks: the return risk price and the volatility risk price. In the presence of a leverage effect (instantaneous causality between the asset return and its volatility), we study the identification of both structural parameters with the price data only, without relying on additional option pricing models or option data. We analyze this identification challenge in a nonparametric discrete-time exponentially affine model, complementing the continuous-time approach of Bandi and Renò (2016). We then specialize to a parametric model and derive the implied minimum distance criterion relating the risk prices to the asset return and volatility’s joint distribution. This criterion is almost flat when the leverage effect is small, and we introduce identification-robust confidence sets for both risk prices regardless of the magnitude of the leverage effect.},
  archive      = {J_JOE},
  author       = {Xu Cheng and Eric Renault and Paul Sangrey},
  doi          = {10.1016/j.jeconom.2024.105943},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105943},
  shortjournal = {J. Econ.},
  title        = {Identifying the volatility risk price through the leverage effect},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification, inference and risk. <em>JOE</em>, <em>248</em>, 105938. (<a href='https://doi.org/10.1016/j.jeconom.2024.105938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Bertille Antoine and Patrick Gagliardini and René Garcia and Enrique Sentana},
  doi          = {10.1016/j.jeconom.2024.105938},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105938},
  shortjournal = {J. Econ.},
  title        = {Identification, inference and risk},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional ecological inference. <em>JOE</em>, <em>248</em>, 105918. (<a href='https://doi.org/10.1016/j.jeconom.2024.105918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of ecological inference when one observes the conditional distributions of Y | W and Z | W from aggregate data and attempts to infer the conditional distribution of Y | Z without observing Y and Z in the same sample. First, we show that this problem can be transformed into a linear equation involving operators for which, under suitable regularity assumptions, least squares solutions are available. We then propose the use of the least squares solution with the minimum Hilbert–Schmidt norm, which, in our context, can be structurally interpreted as the solution with minimum dependence between Y and Z . Interestingly, in the case where the conditioning variable W is discrete and belongs to a finite set, such as the labels of units/groups/cities, the solution of this minimal dependence has a closed form. In the more general case, we use a regularization scheme and show the convergence of our proposed estimator. A numerical evaluation of our procedure is proposed.},
  archive      = {J_JOE},
  author       = {Christian Bontemps and Jean-Pierre Florens and Nour Meddahi},
  doi          = {10.1016/j.jeconom.2024.105918},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105918},
  shortjournal = {J. Econ.},
  title        = {Functional ecological inference},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification-robust and simultaneous inference in multifactor asset pricing models. <em>JOE</em>, <em>248</em>, 105915. (<a href='https://doi.org/10.1016/j.jeconom.2024.105915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes exact identification-robust confidence sets for the zero-beta rate and ex-post factor prices in asset pricing models. Exploiting the information from the cross-sectional intercept allows us to impose or formally test model-consistent restrictions, including those resulting from traded factors in excess of the zero beta-rate or from return spreads. Analytical projection-based solutions for confidence set outcomes are developed. The proposed procedures are extended to the case of missing factors. Empirical and simulation results with traded and non-traded factors show that model-consistent restrictions and elusive factors can materially affect model fit, identification, inference and temporal constancy of pricing influence.},
  archive      = {J_JOE},
  author       = {Marie-Claude Beaulieu and Jean-Marie Dufour and Lynda Khalaf},
  doi          = {10.1016/j.jeconom.2024.105915},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105915},
  shortjournal = {J. Econ.},
  title        = {Identification-robust and simultaneous inference in multifactor asset pricing models},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-run risk in stationary vector autoregressive models. <em>JOE</em>, <em>248</em>, 105905. (<a href='https://doi.org/10.1016/j.jeconom.2024.105905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a local-to-unity/small sigma model for stationary processes with long-range persistence and non-negligible long-run prediction and estimation risks. The model represents a process containing unobserved short and long-run components measured on different time scales. The short-run component is defined in calendar time, while the long-run component evolves in rescaled time with ultra-long units. We develop estimation and long-run prediction methods for time series with multivariate Vector Autoregressive (VAR) short-run components and reveal the impossibility of estimating consistently some of the long-run parameters, which causes significant estimation and prediction risks in the long run. A simulation study and an application to macroeconomic data illustrate the approach.},
  archive      = {J_JOE},
  author       = {Christian Gourieroux and Joann Jasiak},
  doi          = {10.1016/j.jeconom.2024.105905},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105905},
  shortjournal = {J. Econ.},
  title        = {Long-run risk in stationary vector autoregressive models},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering asset market participation from household consumption and income. <em>JOE</em>, <em>248</em>, 105867. (<a href='https://doi.org/10.1016/j.jeconom.2024.105867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an asset pricing model featuring time-varying limited participation in both bond and stock markets and household heterogeneity. Households participate in financial markets with a certain probability that depends on their individual income and on asset market conditions. We use indirect inference to uncover individual asset market participation from individual consumption data and asset prices. Our model very accurately reproduces the proportions of stockholders in the Survey of Consumer Finances over three-year intervals, provides a reasonable estimate of stock market participation costs, and is able to price characteristic-based stock portfolios with the top decile of households identified as stockholders.},
  archive      = {J_JOE},
  author       = {Veronika Czellar and René Garcia and François Le Grand},
  doi          = {10.1016/j.jeconom.2024.105867},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105867},
  shortjournal = {J. Econ.},
  title        = {Uncovering asset market participation from household consumption and income},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weak identification in discrete choice models. <em>JOE</em>, <em>248</em>, 105866. (<a href='https://doi.org/10.1016/j.jeconom.2024.105866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the impact of weak identification in discrete choice models, and provide insights into the determinants of identification strength in these models. Using these insights, we propose a novel test that can consistently detect weak identification in commonly applied discrete choice models, such as probit, logit, and many of their extensions. Furthermore, we demonstrate that when the null hypothesis of weak identification is rejected, Wald-based inference can be carried out using standard formulas and critical values. A Monte Carlo study compares our proposed testing approach against commonly applied weak identification tests. The results simultaneously demonstrate the good performance of our approach and the fundamental failure of using conventional weak identification tests for linear models in the discrete choice model context. Lastly, we apply our approach in two empirical examples: married women labor force participation, and US food aid and civil conflicts.},
  archive      = {J_JOE},
  author       = {David T. Frazier and Eric Renault and Lina Zhang and Xueyan Zhao},
  doi          = {10.1016/j.jeconom.2024.105866},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105866},
  shortjournal = {J. Econ.},
  title        = {Weak identification in discrete choice models},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional spectral methods. <em>JOE</em>, <em>248</em>, 105863. (<a href='https://doi.org/10.1016/j.jeconom.2024.105863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We model predictive scale-specific cycles. By employing suitable matrix representations, we express the forecast errors of covariance-stationary multivariate time series in terms of conditionally orthonormal scale-specific bases. The representations yield conditionally orthogonal decompositions of these forecast errors. They also provide decompositions of their variances and betas in terms of scale-specific variances and betas capturing predictive variability and co-variability over cycles of alternative lengths without spillovers across cycles. Making use of the proposed representations within the classical family of time-varying conditional volatility models, we document the role of time-varying volatility forecasts in generating orthogonal predictive scale-specific cycles in returns. We conclude by providing suggestive evidence that the conditional variances of the predictive return cycles ( i ) may be priced over short-to-medium horizons and ( i i ) may offer economically-relevant trading signals over these same horizons.},
  archive      = {J_JOE},
  author       = {Federico M. Bandi and Yinan Su},
  doi          = {10.1016/j.jeconom.2024.105863},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105863},
  shortjournal = {J. Econ.},
  title        = {Conditional spectral methods},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exogeneity tests and weak identification in IV regressions: Asymptotic theory and point estimation. <em>JOE</em>, <em>248</em>, 105821. (<a href='https://doi.org/10.1016/j.jeconom.2024.105821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides new insights on exogeneity tests in linear IV models and their use for estimation, when identification fails or may not be strong. We make two main contributions. First , we show that Durbin–Wu–Hausman (DWH) and Revankar–Hartley (RH) exogeneity tests have correct level asymptotically, even when the first-stage coefficient matrix (which controls identification) is rank-deficient. We provide necessary and sufficient conditions under which these tests are consistent. In particular, we show that test consistency can hold even when identification fails, provided at least one component of the structural parameter vector is identifiable. Second , we study point estimation after estimator (or model) selection, when the outcome of a DWH/RH test determines whether OLS or an IV method is employed in the second-stage. For this purpose, we use ( non-local ) concepts of asymptotic bias , asymptotic mean squared error (AMSE), and asymptotic relative efficiency (ARE), which remain applicable even when the estimators considered do not have moments (as can happen for 2SLS) or may be inconsistent. We study the asymptotic properties of OLS, 2SLS, and pretest estimators which select OLS or 2SLS based on the outcome of a DWH/RH test. We show that: (i) OLS typically dominates 2SLS estimator asymptotically for MSE across a broad spectrum of cases, including weak identification and moderate endogeneity; (ii) exogeneity-pretest estimators exhibit consistently good performance and asymptotically dominate both OLS and 2SLS. The proposed theoretical findings are documented by Monte Carlo simulations.},
  archive      = {J_JOE},
  author       = {Firmin Doko Tchatoka and Jean-Marie Dufour},
  doi          = {10.1016/j.jeconom.2024.105821},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105821},
  shortjournal = {J. Econ.},
  title        = {Exogeneity tests and weak identification in IV regressions: Asymptotic theory and point estimation},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation-based estimation with many auxiliary statistics applied to long-run dynamic analysis. <em>JOE</em>, <em>248</em>, 105814. (<a href='https://doi.org/10.1016/j.jeconom.2024.105814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing asymptotic theory for estimators obtained by simulated minimum distance does not cover situations in which the number of components of the auxiliary statistics (or number of matched “moments”) is large — typically larger than the sample size. We establish the consistency of the simulated minimum distance estimator in this situation and derive its asymptotic distribution. Our estimator is easy to implement and allows us to exploit all the informational content of a large number of auxiliary statistics without having to, (i) know these functions explicitly, or (ii) choose a priori which functions are the most informative. As a result, we are able to exploit, among other things, long-run information. We illustrate the implementation of the proposed method through Monte-Carlo simulation experiments based on small- and medium-scale New Keynesian models. These examples highlight how to conveniently exploit valuable information from matching a large number of impulse responses including at long-run horizons.},
  archive      = {J_JOE},
  author       = {Bertille Antoine and Wenqian Sun},
  doi          = {10.1016/j.jeconom.2024.105814},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105814},
  shortjournal = {J. Econ.},
  title        = {Simulation-based estimation with many auxiliary statistics applied to long-run dynamic analysis},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The chained difference-in-differences. <em>JOE</em>, <em>248</em>, 105783. (<a href='https://doi.org/10.1016/j.jeconom.2024.105783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the identification, estimation, and inference of long-term (binary) treatment effect parameters when balanced panel data is not available, or consists of only a subset of the available data. We develop a new estimator: the chained difference-in-differences, which leverages the overlapping structure of many unbalanced panel data sets. This approach consists in aggregating a collection of short-term treatment effects estimated on multiple incomplete panels. Our estimator accommodates (1) multiple time periods, (2) variation in treatment timing, (3) treatment effect heterogeneity, (4) general missing data patterns, and (5) sample selection on observables. We establish the asymptotic properties of the proposed estimator and discuss identification and efficiency gains in comparison to existing methods. Finally, we illustrate its relevance through (i) numerical simulations, and (ii) an application about the effects of an innovation policy in France.},
  archive      = {J_JOE},
  author       = {Christophe Bellégo and David Benatia and Vincent Dortet-Bernadet},
  doi          = {10.1016/j.jeconom.2024.105783},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105783},
  shortjournal = {J. Econ.},
  title        = {The chained difference-in-differences},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularizing stock return covariance matrices via multiple testing of correlations. <em>JOE</em>, <em>248</em>, 105753. (<a href='https://doi.org/10.1016/j.jeconom.2024.105753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a large-scale inference approach for the regularization of stock return covariance matrices. The framework allows for the presence of heavy tails and multivariate GARCH-type effects of unknown form among the stock returns. The approach involves simultaneous testing of all pairwise correlations, followed by setting non-statistically significant elements to zero. This adaptive thresholding is achieved through sign-based Monte Carlo resampling within multiple testing procedures, controlling either the traditional familywise error rate, a generalized familywise error rate, or the false discovery proportion. Subsequent shrinkage ensures that the final covariance matrix estimate is positive definite and well-conditioned while preserving the achieved sparsity. Compared to alternative estimators, this new regularization method demonstrates strong performance in simulation experiments and real portfolio optimization.},
  archive      = {J_JOE},
  author       = {Richard Luger},
  doi          = {10.1016/j.jeconom.2024.105753},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105753},
  shortjournal = {J. Econ.},
  title        = {Regularizing stock return covariance matrices via multiple testing of correlations},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spanning latent and observable factors. <em>JOE</em>, <em>248</em>, 105743. (<a href='https://doi.org/10.1016/j.jeconom.2024.105743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor analysis is a widely used tool to summarize high dimensional panel data via a small dimensional set of latent factors. Many applications in finance and macroeconomics, are often focused on observable factors with an economic interpretation. The objective of this paper is to provide a test to answer a question which naturally comes up in discussions regarding latent versus observable factors: do latent and observable factors span the same space? We derive asymptotic properties of a formal test and propose a bootstrap version with improved small sample properties. We find empirical evidence for a small number of factors common between a small number of traditional Fama–French risk factors – or returns on a few stocks (i.e. “magnificent” 5 or 7) – and large panels of US, North American and international portfolio returns.},
  archive      = {J_JOE},
  author       = {E. Andreou and P. Gagliardini and E. Ghysels and M. Rubin},
  doi          = {10.1016/j.jeconom.2024.105743},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105743},
  shortjournal = {J. Econ.},
  title        = {Spanning latent and observable factors},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification robust inference for the risk premium in term structure models. <em>JOE</em>, <em>248</em>, 105728. (<a href='https://doi.org/10.1016/j.jeconom.2024.105728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose identification robust statistics for testing hypotheses on the risk premia in dynamic affine term structure models. We do so using the moment equation specification proposed in Adrian et al. (2013) . Statistical inference based on their three-stage estimator requires knowledge of the risk factors’ quality and can be misleading when the β ’s are weak, which results when sampling errors are of comparable order of magnitude as the risk factor loadings. We extend the subset (factor) Anderson–Rubin test from Guggenberger et al. (2012) to models with multiple dynamic factors and time-varying risk prices. It provides a computationally tractable manner to conduct identification robust tests on a few risk premia when a larger number is present. We use it to analyze potential identification issues arising in the data from Adrian et al. (2013) for which we show that some factors, though potentially weak, may drive the time variation of risk prices, and weak identification issues are more prominent in multi-factor models.},
  archive      = {J_JOE},
  author       = {Frank Kleibergen and Lingwei Kong},
  doi          = {10.1016/j.jeconom.2024.105728},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105728},
  shortjournal = {J. Econ.},
  title        = {Identification robust inference for the risk premium in term structure models},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficiency bounds for moment condition models with mixed identification strength. <em>JOE</em>, <em>248</em>, 105723. (<a href='https://doi.org/10.1016/j.jeconom.2024.105723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moment condition models with mixed identification strength are models that are point identified but with estimating moment functions that are allowed to drift to 0 uniformly over the parameter space. Even though identification fails in the limit, depending on how slow the moment functions vanish, consistent estimation is possible. Existing estimators such as the generalized method of moment (GMM) estimator exhibit a pattern of nonstandard or even heterogeneous rate of convergence that materializes by some parameter directions being estimated at a slower rate than others. This paper derives asymptotic semiparametric efficiency bounds for regular estimators of parameters of these models. We show that GMM estimators are regular and that the so-called two-step GMM estimator – using the inverse of estimating function’s variance as weighting matrix – is semiparametrically efficient as it reaches the minimum variance attainable by regular estimators. This estimator is also asymptotically minimax efficient with respect to a large family of loss functions. Monte Carlo simulations are provided that confirm these results.},
  archive      = {J_JOE},
  author       = {Prosper Dovonon and Yves F. Atchadé and Firmin Doko Tchatoka},
  doi          = {10.1016/j.jeconom.2024.105723},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105723},
  shortjournal = {J. Econ.},
  title        = {Efficiency bounds for moment condition models with mixed identification strength},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Score-type tests for normal mixtures. <em>JOE</em>, <em>248</em>, 105717. (<a href='https://doi.org/10.1016/j.jeconom.2024.105717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing normality against discrete normal mixtures is complex because some parameters turn increasingly underidentified along alternative ways of approaching the null, others are inequality constrained, and several higher-order derivatives become identically 0. These problems make the maximum of the alternative model log-likelihood function numerically unreliable. We propose score-type tests asymptotically equivalent to the likelihood ratio as the largest of two simple intuitive statistics that only require estimation under the null. One novelty of our approach is that we treat symmetrically both ways of writing the null hypothesis without excluding any region of the parameter space. We derive the asymptotic distribution of our tests under the null and sequences of local alternatives. We also show that their asymptotic distribution is the same whether applied to observations or standardized residuals from heteroskedastic regression models. Finally, we study their power in simulations and apply them to the residuals of Mincer earnings functions.},
  archive      = {J_JOE},
  author       = {Dante Amengual and Xinyue Bei and Marine Carrasco and Enrique Sentana},
  doi          = {10.1016/j.jeconom.2024.105717},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105717},
  shortjournal = {J. Econ.},
  title        = {Score-type tests for normal mixtures},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When uncertainty and volatility are disconnected: Implications for asset pricing and portfolio performance. <em>JOE</em>, <em>248</em>, 105654. (<a href='https://doi.org/10.1016/j.jeconom.2023.105654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze an environment where the uncertainty in the equity market return and its volatility are both stochastic and may be potentially disconnected. We solve a representative investor’s optimal asset allocation and derive the resulting conditional equity premium and risk-free rate in equilibrium. Our empirical analysis shows that the equity premium appears to be earned for facing uncertainty, especially high uncertainty that is disconnected from lower volatility, rather than for facing volatility as traditionally assumed. Incorporating the possibility of a disconnect between volatility and uncertainty significantly improves portfolio performance, over and above the performance obtained by conditioning on volatility only.},
  archive      = {J_JOE},
  author       = {Yacine Aït-Sahalia and Felix Matthys and Emilio Osambela and Ronnie Sircar},
  doi          = {10.1016/j.jeconom.2023.105654},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105654},
  shortjournal = {J. Econ.},
  title        = {When uncertainty and volatility are disconnected: Implications for asset pricing and portfolio performance},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The term structure of macroeconomic risks at the effective lower bound. <em>JOE</em>, <em>248</em>, 105383. (<a href='https://doi.org/10.1016/j.jeconom.2023.01.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new macro-finance model that solves the tension between tractability, flexibility in macroeconomic dynamics, and consistency of the term structures of treasury yields with the effective lower bound (ELB). I use the term structures of U.S. nominal and real treasury yields from 1990 to explore the interdependence between inflation expectations, volatility, and monetary policy at the ELB. The estimation reveals that real yields stay elevated during the ELB due to large premia and deflation fears, produced by a persistent shift in inflation dynamics, with low average inflation and heightened inflation volatility.},
  archive      = {J_JOE},
  author       = {Guillaume Roussellet},
  doi          = {10.1016/j.jeconom.2023.01.005},
  journal      = {Journal of Econometrics},
  month        = {3},
  pages        = {105383},
  shortjournal = {J. Econ.},
  title        = {The term structure of macroeconomic risks at the effective lower bound},
  volume       = {248},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation error and numerical instability in estimating random coefficient logit demand models. <em>JOE</em>, <em>247</em>, 105953. (<a href='https://doi.org/10.1016/j.jeconom.2025.105953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nonlinear GMM-IV estimator of Berry, Levinsohn and Pakes (1995) can suffer from numerical instability resulting in a wide range of parameter estimates and economic implications. This has been reported to depend on technical details such as the choice of the optimization algorithm, starting values, and convergence criteria. We show that numerical approximation errors in the estimator’s moment function are the main driver of this instability. With accurate approximation, the estimation approach is well-behaved. We provide a simple method to determine the required number of simulation draws.},
  archive      = {J_JOE},
  author       = {Daniel Brunner and Florian Heiss and André Romahn and Constantin Weiser},
  doi          = {10.1016/j.jeconom.2025.105953},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105953},
  shortjournal = {J. Econ.},
  title        = {Simulation error and numerical instability in estimating random coefficient logit demand models},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The robust F-statistic as a test for weak instruments. <em>JOE</em>, <em>247</em>, 105951. (<a href='https://doi.org/10.1016/j.jeconom.2025.105951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the linear model with a single endogenous variable, (Montiel Olea and Pflueger 2013) proposed the effective F-statistic as a test for weak instruments in terms of the Nagar bias of the two-stage least squares (2SLS) or limited information maximum likelihood (LIML) estimator relative to a benchmark worst-case bias. We show that their methodology for the 2SLS estimator applies to a class of linear generalized method of moments (GMM) estimators with an associated class of generalized effective F-statistics. The standard robust F-statistic is a member of this class. The associated GMMf estimator, with the extension “f” for first-stage, has the weight matrix based on the first-stage residuals. In the grouped-data IV designs of Andrews (2018) with moderate and high levels of endogeneity and where the robust F-statistic is large but the effective F-statistic is small, the GMMf estimator is shown to behave much better in terms of bias than the 2SLS estimator.},
  archive      = {J_JOE},
  author       = {Frank Windmeijer},
  doi          = {10.1016/j.jeconom.2025.105951},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105951},
  shortjournal = {J. Econ.},
  title        = {The robust F-statistic as a test for weak instruments},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative estimation of nonparametric regressions with continuous endogenous variables and discrete instruments. <em>JOE</em>, <em>247</em>, 105950. (<a href='https://doi.org/10.1016/j.jeconom.2025.105950'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a nonparametric regression model with continuous endogenous independent variables when only discrete instruments are available that are independent of the error term. Although this framework is very relevant for applied research, its implementation is challenging, as the regression function becomes the solution to a nonlinear integral equation. We propose a simple iterative procedure to estimate such models and showcase some of its asymptotic properties. In a simulation experiment, we detail its implementation in the case when the instrumental variable is binary. We conclude with an empirical application to returns to education.},
  archive      = {J_JOE},
  author       = {Samuele Centorrino and Frédérique Fève and Jean-Pierre Florens},
  doi          = {10.1016/j.jeconom.2025.105950},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105950},
  shortjournal = {J. Econ.},
  title        = {Iterative estimation of nonparametric regressions with continuous endogenous variables and discrete instruments},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uniform inference for cointegrated vector autoregressive processes. <em>JOE</em>, <em>247</em>, 105944. (<a href='https://doi.org/10.1016/j.jeconom.2024.105944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uniformly valid inference for cointegrated vector autoregressive processes has so far proven difficult due to certain discontinuities arising in the asymptotic distribution of the least squares estimator. We extend asymptotic results from the univariate case to multiple dimensions and show how inference can be based on these results. Furthermore, we show that lag augmentation and a recent instrumental variable procedure can also yield uniformly valid tests and confidence regions. We verify the theoretical findings and investigate finite sample properties in simulation experiments for two specific examples.},
  archive      = {J_JOE},
  author       = {Christian Holberg and Susanne Ditlevsen},
  doi          = {10.1016/j.jeconom.2024.105944},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105944},
  shortjournal = {J. Econ.},
  title        = {Uniform inference for cointegrated vector autoregressive processes},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bond risk premiums at the zero lower bound. <em>JOE</em>, <em>247</em>, 105939. (<a href='https://doi.org/10.1016/j.jeconom.2024.105939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We document that the spread between long- and short-term government bond yields is a stronger predictor of excess bond returns when the U.S. economy is at the zero lower bound (ZLB) than away from this bound. The Gaussian shadow rate model with a linear or quadratic shadow rate is unable to explain this change in return predictability. The same holds for the quadratic term structure model and the autoregressive gamma-zero model that also enforce the ZLB. In contrast, the linear-rational square-root model explains our new empirical finding because the model allows for unspanned stochastic volatility as seen in bond yields.},
  archive      = {J_JOE},
  author       = {Martin M. Andreasen and Kasper Jørgensen and Andrew Meldrum},
  doi          = {10.1016/j.jeconom.2024.105939},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105939},
  shortjournal = {J. Econ.},
  title        = {Bond risk premiums at the zero lower bound},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shrinkage estimators for periodic autoregressions. <em>JOE</em>, <em>247</em>, 105937. (<a href='https://doi.org/10.1016/j.jeconom.2024.105937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A periodic autoregression [PAR] is a seasonal time series model where the autoregressive parameters vary over the seasons. A drawback of PAR models is that the number of parameters increases dramatically when the number of seasons gets large. Hence, one needs many periods with intra-seasonal data to be able to get reliable parameter estimates. Therefore, these models are rarely applied for weekly or daily observations. In this paper we propose shrinkage estimators which shrink the periodic autoregressive parameters to a common value determined by the data. We derive the asymptotic properties of these estimators in case of a quadratic penalty and we illustrate the bias–variance trade-off. Empirical illustrations show that shrinkage improves forecasting with PAR models.},
  archive      = {J_JOE},
  author       = {Richard Paap and Philip Hans Franses},
  doi          = {10.1016/j.jeconom.2024.105937},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105937},
  shortjournal = {J. Econ.},
  title        = {Shrinkage estimators for periodic autoregressions},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference on dynamic systemic risk measures. <em>JOE</em>, <em>247</em>, 105936. (<a href='https://doi.org/10.1016/j.jeconom.2024.105936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systemic risk measures (SRM) quantify the risk of a system induced by the possible distress of any of its components. Applications in economics and finance are numerous. We define a general dynamic framework for the risk factors, allowing us to obtain explicit expressions of the corresponding dynamic SRM. We deduce an easy-to-implement statistical approach which, based on semi-parametric assumptions, reduces to estimating univariate location-scale models and to computing (static) quantiles of the residuals. We derive a sound asymptotic theory (including confidence intervals, tests, validity of a residual bootstrap) for major SRM, namely the Conditional VaR (CoVaR) and Delta-CoVaR. Our theoretical results are illustrated via Monte-Carlo experiments and real financial and macroeconomic time series.},
  archive      = {J_JOE},
  author       = {Christian Francq and Jean-Michel Zakoïan},
  doi          = {10.1016/j.jeconom.2024.105936},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105936},
  shortjournal = {J. Econ.},
  title        = {Inference on dynamic systemic risk measures},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Individual welfare analysis: Random quasilinear utility, independence, and confidence bounds. <em>JOE</em>, <em>247</em>, 105927. (<a href='https://doi.org/10.1016/j.jeconom.2024.105927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel framework for individual-level welfare analysis. It builds on a parametric model for continuous demand with a quasilinear utility function, allowing for heterogeneous coefficients and unobserved individual-good-level preference shocks. We obtain bounds on the individual-level consumer welfare loss at any confidence level due to a hypothetical price increase, solving a scalable optimization problem constrained by a novel confidence set under an independence restriction. This confidence set is computationally simple and robust to weak instruments, nonlinearity, and partial identification. The validity of the confidence set is guaranteed by our new results on the joint limiting distribution of the independence test by Chatterjee (2021). These results together with the confidence set may have applications beyond welfare analysis. Monte Carlo simulations and two empirical applications on gasoline and food demand demonstrate the effectiveness of our method.},
  archive      = {J_JOE},
  author       = {Junlong Feng and Sokbae Lee},
  doi          = {10.1016/j.jeconom.2024.105927},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105927},
  shortjournal = {J. Econ.},
  title        = {Individual welfare analysis: Random quasilinear utility, independence, and confidence bounds},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On testing for spatial or social network dependence in panel data allowing for network variability. <em>JOE</em>, <em>247</em>, 105925. (<a href='https://doi.org/10.1016/j.jeconom.2024.105925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces robust generalized Moran I tests for network-generated cross-sectional dependence in a panel data setting where unit-specific effects can be random or fixed. Network dependence may originate from endogenous variables, exogenous variables, and/or disturbances, and the network dependence is allowed to vary over time. The formulation of the test statistics also aims at accommodating situations where the researcher is unsure about the exact nature of the network. Unit-specific effects are eliminated using the Helmert transformation, which is well known to yield time-orthogonality for linear forms of transformed disturbances. Given the specification of our test statistics, these orthogonality properties also extend to the quadratic forms that underlie our test statistics. This greatly simplifies the expressions for the asymptotic variances of our test statistics and their estimation. Monte Carlo simulations suggest that the generalized Moran I tests introduced in this paper have the proper size and can provide substantial improvement in robustness when the researcher faces uncertainty about the specification of the network topology.},
  archive      = {J_JOE},
  author       = {Xiaodong Liu and Ingmar R. Prucha},
  doi          = {10.1016/j.jeconom.2024.105925},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105925},
  shortjournal = {J. Econ.},
  title        = {On testing for spatial or social network dependence in panel data allowing for network variability},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling large dimensional datasets with markov switching factor models. <em>JOE</em>, <em>247</em>, 105919. (<a href='https://doi.org/10.1016/j.jeconom.2024.105919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a novel large dimensional approximate factor model with regime changes in the loadings driven by a latent first order Markov process. By exploiting the equivalent linear representation of the model, we first recover the latent factors by means of Principal Component Analysis. We then cast the model in state–space form, and we estimate loadings and transition probabilities through an EM algorithm based on a modified version of the Baum–Lindgren–Hamilton–Kim filter and smoother that makes use of the factors previously estimated. Our approach is appealing as it provides closed form expressions for all estimators. More importantly, it does not require knowledge of the true number of factors. We derive the theoretical properties of the proposed estimation procedure, and we show their good finite sample performance through a comprehensive set of Monte Carlo experiments. The empirical usefulness of our approach is illustrated through three applications to large U.S. datasets of stock returns, macroeconomic variables, and inflation indexes.},
  archive      = {J_JOE},
  author       = {Matteo Barigozzi and Daniele Massacci},
  doi          = {10.1016/j.jeconom.2024.105919},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105919},
  shortjournal = {J. Econ.},
  title        = {Modelling large dimensional datasets with markov switching factor models},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bootstrapping out-of-sample predictability tests with real-time data. <em>JOE</em>, <em>247</em>, 105916. (<a href='https://doi.org/10.1016/j.jeconom.2024.105916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we develop a block bootstrap approach to out-of-sample inference when real-time data are used to produce forecasts. In particular, we establish its first-order asymptotic validity for West-type (1996) tests of predictive ability in the presence of regular data revisions. This allows the user to conduct asymptotically valid inference without having to estimate the asymptotic variances derived in Clark and McCracken’s (2009) extension of West (1996) when data are subject to revision. Monte Carlo experiments indicate that the bootstrap can provide satisfactory finite sample size and power even in modest sample sizes. We conclude with an application to inflation forecasting that revisits the results in Ang et al. (2007) in the presence of real-time data.},
  archive      = {J_JOE},
  author       = {Sílvia Gonçalves and Michael W. McCracken and Yongxu Yao},
  doi          = {10.1016/j.jeconom.2024.105916},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105916},
  shortjournal = {J. Econ.},
  title        = {Bootstrapping out-of-sample predictability tests with real-time data},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural disasters as macroeconomic tail risks. <em>JOE</em>, <em>247</em>, 105914. (<a href='https://doi.org/10.1016/j.jeconom.2024.105914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce quantile and moment impulse response functions for structural quantile vector autoregressive models. We use them to study how climate-related natural disasters affect the predictive distribution of output growth and inflation. Disasters strongly shift the forecast distribution particularly in the tails. They result in an initial sharp increase of the downside risk for growth, followed by a temporary rebound. Upside risk to inflation increases markedly for a few months and then subsides. As a result, natural disasters have a persistent impact on the conditional variance and skewness of macroeconomic aggregates which standard linear models estimating conditional mean dynamics fail to match. We perform a scenario analysis to evaluate the hypothetical effects of more frequent large disasters on the macroeconomy due to increased atmospheric carbon concentration. Our results indicate a substantially higher conditional volatility of growth and inflation as well as increased upside risk to inflation particularly in a scenario where only currently pledged climate policies are implemented.},
  archive      = {J_JOE},
  author       = {Sulkhan Chavleishvili and Emanuel Moench},
  doi          = {10.1016/j.jeconom.2024.105914},
  journal      = {Journal of Econometrics},
  month        = {1},
  pages        = {105914},
  shortjournal = {J. Econ.},
  title        = {Natural disasters as macroeconomic tail risks},
  volume       = {247},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
