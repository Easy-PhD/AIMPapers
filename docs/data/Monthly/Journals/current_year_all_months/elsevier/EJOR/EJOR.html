<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EJOR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ejor">EJOR - 474</h2>
<ul>
<li><details>
<summary>
(2025). Optimal lot-sizing and service level weighting in sequential multi-attribute global transportation service procurement. <em>EJOR</em>, <em>327</em>(3), 1052-1072. (<a href='https://doi.org/10.1016/j.ejor.2025.05.037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of on-demand global transportation service procurement (oGTSP) through digital trading platforms has accelerated due to frequent fluctuations in transport capacity. In the oGTSP model, the exporter must consider logistics service quality and transport prices when sourcing global logistics services. To satisfy the continuous transport needs, procurement is conducted sequentially throughout multiple auction cycles. For a single auction, we constructed a service-level weight-scoring function and analysed the trading parties’ behavioural strategies to obtain an auction equilibrium strategy in a specific context. Then, we developed a multi-cycle sequential decision method based on a single-cycle equilibrium decision by forwarders that can dynamically adjust the auction lot size to help the exporter obtain optimal utility. Finally, based on the real case of a large electronic product exporter, the proposed approach was verified. The results demonstrated that exporters should pay more attention to the quality of service when choosing freight forwarders to improve the utility of transportation service procurement. The exporter can attract more forwarders to participate in auctions to obtain more capacity supply by increasing the weighting of service levels. Besides, the proposed auction system could effectively accommodate strategic forwarders with learning abilities. The exporter’s utility will significantly improve if the freight forwarders have learning ability. There is a marginal diminishing effect in that the benefits from additional participation of learning-oriented bidders are initially large but eventually stabilized. The strategic auction participation of learning-oriented freight forwarders smooths the capacity supply trend, reduces extreme fluctuations and makes multi-cycle predictions more accurate.},
  archive      = {J_EJOR},
  author       = {Xiang T.R. Kong and Zhan He and Kaize Yu and Pengyu Yan},
  doi          = {10.1016/j.ejor.2025.05.037},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {1052-1072},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal lot-sizing and service level weighting in sequential multi-attribute global transportation service procurement},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Government’s optimal inter-temporal subsidy and manufacturer’s dynamic pricing in the presence of strategic consumers. <em>EJOR</em>, <em>327</em>(3), 1039-1051. (<a href='https://doi.org/10.1016/j.ejor.2025.05.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Governments in many countries offer fiscal incentives—such as subsidies or tax breaks—to consumers to encourage the purchase of environmentally-friendly products like solar panels and electric vehicles. Early adoption by consumers facilitates manufacturers’ learning-by-doing and reduces production cost over time, although the cost reduction itself is subject to uncertainty. Governments face a challenge: should they commit to a multi-period subsidy path (commitment policy) or adjust the subsidy contingent on the realized production cost reduction (dynamic policy)? What are the implications for manufacturers and consumers? We consider a two-period monopoly setting to study these policies. Given the subsidy policy, the manufacturer sets its prices, whereas consumers strategically decide when to purchase the product, if at all. Naturally, the two policies result in different subsidy paths. We find that products with higher initial unit cost (implying higher prices) do not deserve higher subsidies. Our key result is that governments, who seek to maximize expected social welfare, should adopt the dynamic policy. Insightfully, the four components of social welfare—consumer surplus, manufacturer’s profit, environmental benefit and subsidy expenditure—may all be realized higher under the commitment policy than under the dynamic policy when the realized cost reduction falls short of its expected value. This is because the second-period effective price (price minus subsidy) is more sensitive to cost uncertainty under the dynamic policy. Nevertheless, the dominance of the dynamic policy persists also when considering the realized social welfare. We study several extensions demonstrating the robustness of our results, while highlighting certain exceptions.},
  archive      = {J_EJOR},
  author       = {Weichun Chen and Benny Mantin and Bo Li},
  doi          = {10.1016/j.ejor.2025.05.027},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {1039-1051},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Government’s optimal inter-temporal subsidy and manufacturer’s dynamic pricing in the presence of strategic consumers},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated model for predictive maintenance and inventory management under a reliability chance constraint. <em>EJOR</em>, <em>327</em>(3), 1023-1038. (<a href='https://doi.org/10.1016/j.ejor.2025.05.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new model that integrates opportunistic maintenance and routine maintenance to enhance the effectiveness of predictive maintenance and inventory management in complex manufacturing systems subject to a reliability chance constraint. It considers both hard and soft failure modes and their mutual dependence. When a machine experiences a hard failure, an opportunistic maintenance policy is utilized on the machine’s components. When the soft failure degradation level of a machine component surpasses a threshold, imperfect preventive maintenance or replacement maintenance is carried out. The choice of component supplier, including OEM and aftermarket suppliers, significantly impacts the joint decision model. To improve the model’s realism and applicability, a random variable representing supplier availability intervals is introduced, reflecting a more nuanced understanding of supply chain dynamics. We develop a simulation optimization method to determine the degradation thresholds for opportunistic and regular maintenance, the component inventory policy, and supplier selection. The objective is to minimize the total maintenance and inventory cost, while ensuring a high level of system reliability. The proposed algorithm effectively addresses the system reliability chance constraint by formulating a surrogate model of the quantile of system downtime. A numerical study is conducted to verify the efficacy of the proposed model and to demonstrate the efficiency of the solution method in finding the optimal feasible solution. Furthermore, the influence of critical factors in the model on the optimal policy is analyzed to derive useful managerial insights.},
  archive      = {J_EJOR},
  author       = {Kuo-Hao Chang and Xin-Pei Wu and Robert Cuckler},
  doi          = {10.1016/j.ejor.2025.05.018},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {1023-1038},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An integrated model for predictive maintenance and inventory management under a reliability chance constraint},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ordinal regression meets online learning: Interactive preference learning for multiple criteria choice and ranking with provable guarantees. <em>EJOR</em>, <em>327</em>(3), 1003-1022. (<a href='https://doi.org/10.1016/j.ejor.2025.05.045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a theoretical and practical bridge between ordinal regression for multiple criteria choice and ranking problems and the framework of sequential prediction, also known as online learning. By reframing the ordinal regression as a sequential prediction task, we study a general class of algorithms that assign probabilities to a sequence of the preferences expressed by the Decision Maker (DM). This approach allows us to evaluate various statistical algorithms on a common basis, providing theoretical guarantees on their regret. To model the likelihood, we employ an additive value function that scores pairwise comparisons given by the DM. We explore two likelihood models: (1) a linear model, which we demonstrate is analogous to sequential investment, and (2) the Bradley–Terry model, widely used in statistics and preference learning. For both models, we establish theoretical bounds for the Bayesian method and the Regularized Maximum Likelihood algorithm (also known as Follow the Regularized Leader). We design Monte Carlo Markov Chain methods based on Metropolis–Hastings and Nested Sampling for efficient approximation of the posterior in Bayesian methods. Extensive empirical testing on synthetic and real-world data shows that our methods outperform the best existing approaches in the literature.},
  archive      = {J_EJOR},
  author       = {Marco Grillo and Wojciech Kotłowski and Miłosz Kadziński},
  doi          = {10.1016/j.ejor.2025.05.045},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {1003-1022},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Ordinal regression meets online learning: Interactive preference learning for multiple criteria choice and ranking with provable guarantees},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alternative ranking in trust network group decision-making: A distributionally robust optimization method. <em>EJOR</em>, <em>327</em>(3), 986-1002. (<a href='https://doi.org/10.1016/j.ejor.2025.05.052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In group decision making problems, preference information can be conveniently and productively used to express the decision-makers’ evaluations over the given set of alternatives. However, the inherent imprecision of preference information may lead to fragile priority weights and unreliable alternative ranking. In this study, we propose a distributionally robust ranking model based on social networks to derive stable priorities, which takes into account the influence of uncertain preference information and the strength of relationships among decision-makers. Specifically, to capture the true data-generating distribution of uncertain parameters, we first develop a distributionally robust ranking model with a moment-based ambiguity set that contains all possible probability distributions over a support set. Then, we verify that the solutions exhibit strong finite-sample performance guarantees. Additionally, the developed model can be reformulated into an equivalent semidefinite programming model. To account for the strength of relationships among decision-makers, we employ propagation efficiency based on Shannon’s theorem, and develop the trust propagation and aggregation operators to obtain decision-makers’ weights. Finally, a numerical experiment is provided, in which the justification and robustness of the distributionally robust ranking model outperform several benchmark models by comparative discussions and robustness analyses.},
  archive      = {J_EJOR},
  author       = {Longlong Shao and Jinpei Liu and Chenyi Fu and Ning Zhu and Huayou Chen},
  doi          = {10.1016/j.ejor.2025.05.052},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {986-1002},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Alternative ranking in trust network group decision-making: A distributionally robust optimization method},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When and should streamers choose high-quality products? effects of streamer types. <em>EJOR</em>, <em>327</em>(3), 971-985. (<a href='https://doi.org/10.1016/j.ejor.2025.05.057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of live-streaming commerce, research has largely focused on manufacturers, leaving streamer decision-making underexplored. This study uses game theory to analyze streamers’ product selection strategies, while also examining how streamer types influence these decisions. The findings reveal that: (a) Streamers do not always prioritize high-quality products. Their choices are shaped by various factors, including product pricing, quality gaps, commission ratios, fan-shoppers’ trust, and sales abilities. High-quality manufacturers are advised to collaborate with knowledge-based streamers, while low-quality manufacturers should partner with entertainment-based streamers. Moderate commission ratios can optimize profits for all parties. (b) For well-known products, knowledge-based streamers with strong sales abilities are more likely to select high-quality items, as they can leverage fan-shoppers' willingness to pay. In contrast, entertainment-based streamers do not exhibit this preference. For unknown products, entertainment-based streamers with strong sales abilities may promote low-quality items, even resorting to deception. Interestingly, entertainment-based streamers with weaker sales abilities may promote high-quality products, while knowledge-based streamers may opt for lower-quality options. (c) When product quality is endogenous, streamers with lower sales abilities should focus on entertainment-based content to attract attention. As their sales abilities improve and fan-shoppers’ trust grows, they should transition to knowledge-based content.},
  archive      = {J_EJOR},
  author       = {Shengyan Cheng and Qiang Guo and Chris K Anderson},
  doi          = {10.1016/j.ejor.2025.05.057},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {971-985},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {When and should streamers choose high-quality products? effects of streamer types},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting two-dimensional projection-efficient units in data envelopment analysis under big data scenarios. <em>EJOR</em>, <em>327</em>(3), 957-970. (<a href='https://doi.org/10.1016/j.ejor.2025.05.053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of big data, traditional estimation methods may struggle to process large datasets efficiently. Ali (1993) laid the foundation for improving efficiency assessment using Data Envelopment Analysis (DEA). Building on this work, we demonstrate how to detect two-dimensional projection-efficient units. This is achieved by projecting the multidimensional DEA production frontier onto two-dimensional subspaces and utilizing slope analysis to identify key efficient units. These units are then linked to their full-dimensional counterparts to define projection-efficient units. We propose using these key efficient units as a preliminary step to speed up the identification of full-dimensional efficient units or to estimate the relative density of datasets. Simulations show that our method reduces computation time for the two fastest approaches by an average of 54.2 % across different datasets.},
  archive      = {J_EJOR},
  author       = {Shuqi Xu and Qingyuan Zhu and Zhiyang Shen and Michael Vardanyan and Yinghao Pan},
  doi          = {10.1016/j.ejor.2025.05.053},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {957-970},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Detecting two-dimensional projection-efficient units in data envelopment analysis under big data scenarios},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deck of cards method for hierarchical, robust and stochastic ordinal regression. <em>EJOR</em>, <em>327</em>(3), 937-956. (<a href='https://doi.org/10.1016/j.ejor.2025.05.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the recently introduced application of the Deck of Cards Method (DCM) to ordinal regression proposing two extensions related to two main research trends in Multiple Criteria Decision Aiding, namely scaling and ordinal regression generalizations. On the one hand, procedures, different from DCM (e.g. AHP, BWM, MACBETH) to collect and elaborate Decision Maker’s (DM’s) preference information are considered to define an overall evaluation of reference alternatives. On the other hand, Robust Ordinal Regression and Stochastic Multicriteria Acceptability Analysis are used to offer the DM more detailed and realistic decision-support outcomes. More specifically, we consider preference imprecision and indetermination through a set of admissible comprehensive evaluations of alternatives provided by the whole set of value functions compatible with DM’s preference information rather than relying on a single definitive evaluation based on one value function. In addition, we also consider alternatives evaluated on a set of criteria hierarchically structured. The methodology we propose allows the DM to provide precise or imprecise information at different levels of the hierarchy of criteria. Like scaling procedures, the compatible value function we consider can be of a different nature, such as weighted sum, linear or general monotone value function, or Choquet integral. Consequently, the approach we propose is versatile and well-equipped to be adapted to DM’s characteristics and requirements. The applicability of the proposed methodology is shown by a didactic example based on a large ongoing research project in which Italian regions are evaluated on criteria representing Circular Economy, Innovation-Driven Development and Smart Specialization Strategies.},
  archive      = {J_EJOR},
  author       = {Salvatore Corrente and Salvatore Greco and Silvano Zappalà},
  doi          = {10.1016/j.ejor.2025.05.025},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {937-956},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Deck of cards method for hierarchical, robust and stochastic ordinal regression},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum likelihood probability measures over sets: Existence, computation, and convergence. <em>EJOR</em>, <em>327</em>(3), 922-936. (<a href='https://doi.org/10.1016/j.ejor.2025.07.054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider maximum likelihood estimation of a distribution over a general measurable space where realizations of the uncertainty are not directly observable but instead are known to lie within observable sets. We show that maximum likelihood estimates concentrate on a collection of maximal intersections (CMI) and can be found by solving a convex optimization problem whose size is linear in the size of the CMI. We provide an enumerative algorithm to compute the estimates and show that there are estimates that assign positive weight only to T + 1 elements of the CMI ( T being the number of observed sets). Motivated by this, we provide a column generation algorithm to compute the estimates that avoids enumerating the CMI. Under the assumption that either the observed sets are mixed-integer representable, or that the range of the underlying distribution is finite and known, we provide formulations of the algorithms that can be solved with commercial solvers. We study convergence properties of the maximum likelihood estimate both in terms of traditional notions of converge, as well as in terms of Wasserstein distances. Our results show that convergence to the underlying distribution cannot be guaranteed in general, but we identify sufficient conditions for convergence. We also perform numerical experiments that show that the estimates can be computed within minutes, that column generation can significantly reduce computational times, and that there is convergence even in cases where no theoretical guarantees are known.},
  archive      = {J_EJOR},
  author       = {Juan S. Borrero and Denis Sauré},
  doi          = {10.1016/j.ejor.2025.07.054},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {922-936},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Maximum likelihood probability measures over sets: Existence, computation, and convergence},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Worst-case values of target semi-variances with applications to robust portfolio selection. <em>EJOR</em>, <em>327</em>(3), 905-921. (<a href='https://doi.org/10.1016/j.ejor.2025.07.057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expected regret and target semi-variance are two of the most important risk measures for downside risk. When the distribution of a loss is uncertain, and only partial information of the loss is known, their worst-case values play important roles in robust risk management for finance, insurance, and many other fields. Jagannathan (1977) derived the worst-case expected regrets when only the mean and variance of a loss are known and the loss is arbitrary, symmetric, or non-negative. While Chen et al. (2011) obtained the worst-case target semi-variances under similar conditions but focusing on arbitrary losses. In this paper, we first complement the study of Chen et al. (2011) on the worst-case target semi-variances and derive the closed-form expressions for the worst-case target semi-variance when only the mean and variance of a loss are known and the loss is symmetric or non-negative. Then, we investigate worst-case target semi-variances over uncertainty sets that represent undesirable scenarios faced by an investor. Our methods for deriving these worst-case values are different from those used in Jagannathan (1977) and Chen et al. (2011). As applications of the results derived in this paper, we propose robust portfolio selection methods that minimize the worst-case target semi-variance of a portfolio loss over different uncertainty sets. To explore the insights of our robust portfolio selection methods, we conduct numerical experiments with real financial data and compare our portfolio selection methods with several portfolio selection models related to the models proposed in this paper.},
  archive      = {J_EJOR},
  author       = {Jun Cai and Zhanyi Jiao and Tiantian Mao},
  doi          = {10.1016/j.ejor.2025.07.057},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {905-921},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Worst-case values of target semi-variances with applications to robust portfolio selection},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint model for longitudinal and spatio-temporal survival data. <em>EJOR</em>, <em>327</em>(3), 892-904. (<a href='https://doi.org/10.1016/j.ejor.2025.07.060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In credit risk analysis, survival models with fixed and time-varying covariates are commonly used to predict a borrower’s time-to-event. When time-varying covariates are endogenous, jointly modeling their evolution with the event time — known as the joint model for longitudinal and time-to-event data — provides a principled approach. In addition to temporal dynamics, incorporating borrowers’ geographical information can enhance predictive accuracy by capturing spatial clustering and its variation over time. We propose the Spatio-Temporal Joint Model (STJM), a Bayesian hierarchical model that accounts for spatial and temporal effects and their interaction. The STJM captures the impact of unobserved heterogeneity across regions, affecting borrowers residing in the same area at a given time. To ensure scalability to large datasets, we implement the model using the Integrated Nested Laplace Approximation (INLA) framework. We apply the STJM to predict the time to full prepayment on a large dataset of 57,258 US mortgage borrowers with more than 2.5 million observations. Empirical results indicate that including spatial effects consistently improves the performance of the joint model. However, the gains are less definitive when we additionally include spatio-temporal interactions.},
  archive      = {J_EJOR},
  author       = {Victor Medina-Olivares and Finn Lindgren and Raffaella Calabrese and Jonathan Crook},
  doi          = {10.1016/j.ejor.2025.07.060},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {892-904},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Joint model for longitudinal and spatio-temporal survival data},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Staggered routing in autonomous mobility-on-demand systems. <em>EJOR</em>, <em>327</em>(3), 875-891. (<a href='https://doi.org/10.1016/j.ejor.2025.06.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In autonomous mobility-on-demand systems, effectively managing vehicle flows to mitigate induced congestion and ensure efficient operations is imperative for system performance and positive customer experience. Against this background, we study the potential of staggered routing, i.e., purposely delaying trip departures from a system perspective, in order to reduce congestion and ensure efficient operations while still meeting customer time windows. We formalize the underlying planning problem and show how to efficiently model it as a mixed integer linear program. Moreover, we present a matheuristic that allows us to efficiently solve large-scale real-world instances both in an offline full-information setting and its online rolling horizon counterpart. We conduct a numerical study for Manhattan, New York City, focusing on low- and highly-congested scenarios. Our results show that in low-congestion scenarios, staggering trip departures allows mitigating, on average, 98 % of the induced congestion in a full information setting. In a rolling horizon setting, our algorithm allows us to reduce 82 % of the induced congestion. In high-congestion scenarios, we observe an average reduction of 60 % as the full information bound and an average reduction of 30 % in our online setting. Surprisingly, we show that these reductions can be reached by shifting trip departures by a maximum of six minutes in both the low and high-congestion scenarios.},
  archive      = {J_EJOR},
  author       = {Antonio Coppola and Gerhard Hiermann and Dario Paccagnan and Maximilian Schiffer},
  doi          = {10.1016/j.ejor.2025.06.008},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {875-891},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Staggered routing in autonomous mobility-on-demand systems},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The storage location assignment and picker routing problem: A generic branch-cut-and-price algorithm. <em>EJOR</em>, <em>327</em>(3), 857-874. (<a href='https://doi.org/10.1016/j.ejor.2025.05.041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Storage Location Assignment Problem (SLAP) and the Picker Routing Problem (PRP) have received significant attention in the literature due to their pivotal role in the performance of the Order Picking (OP) activity, the most resource-intensive process of warehousing logistics. The two problems are traditionally considered at different decision-making levels: tactical for the SLAP, and operational for the PRP. However, this paradigm has been challenged by the emergence of modern practices in e-commerce warehouses, where decisions are more dynamic. This shift makes the integrated problem, called the Storage Location Assignment and Picker Routing Problem (SLAPRP), pertinent to consider. Scholars have investigated several variants of the SLAPRP, including different warehouse layouts and routing policies. Nevertheless, the available computational results suggest that each variant requires an ad-hoc formulation. Moreover, achieving a complete integration of the two problems, where the routing is solved optimally, remains out of reach for commercial solvers, even on trivial instances. In this paper, we propose an exact solution framework that addresses a broad class of variants of the SLAPRP, including all the previously existing ones. This paper proposes a Branch-Cut-and-Price framework based on a novel formulation with an exponential number of variables, which is strengthened with a novel family of non-robust valid inequalities. We have developed an ad-hoc branching scheme to break symmetries and maintain the size of the enumeration tree manageable. Computational experiments show that our framework can effectively solve medium-sized instances of several SLAPRP variants and outperforms the state-of-the-art methods from the literature.},
  archive      = {J_EJOR},
  author       = {Thibault Prunet and Nabil Absi and Diego Cattaruzza},
  doi          = {10.1016/j.ejor.2025.05.041},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {857-874},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The storage location assignment and picker routing problem: A generic branch-cut-and-price algorithm},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust parallel machine selection and scheduling with uncertain release times. <em>EJOR</em>, <em>327</em>(3), 838-856. (<a href='https://doi.org/10.1016/j.ejor.2025.05.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a parallel machine selection and scheduling (PMSS) problem with uncertain release times. To handle uncertain release times, we propose a two-stage robust PMSS model where the release time deviation (RTD) is characterized by a budget uncertainty set. In the first stage, machine selection and job assignment decisions are made to minimize startup costs before the uncertainties are revealed. In the second stage, once release times are known, job sequences are optimized to minimize the makespan on each machine. Robust constraints are introduced to ensure that the worst-case minimum makespan on each machine does not exceed a pre-specified due date. The proposed model is a tri-level min–max–min optimization problem with mixed-integer recourse decisions, which cannot be solved efficiently by existing algorithms. To this end, we propose a novel logic-based Benders decomposition (LBBD) algorithm with strengthened Benders cuts and speedup techniques. Specifically, we first provide an equivalent mixed-integer linear programming reformulation for the max–min subproblem by analyzing an optimality condition of the worst-case RTD. Second, we design novel combinatorial and analytical Benders cuts, which dominate cuts found in the literature, and we further strengthen them by lifting procedures. Third, we design a relaxation-and-correction procedure and a warm-start procedure to speed up the LBBD algorithm. Numerical experiments show the proposed robust model greatly reduces job tardiness compared with the deterministic model. The proposed cuts efficiently reduce the runtime, and the LBBD algorithm is at least three orders of magnitude faster than the state-of-the-art column-and-constraint-generation algorithm.},
  archive      = {J_EJOR},
  author       = {Linyuan Hu and Yuli Zhang and Muyang Wen and Roel Leus and Ningwei Zhang},
  doi          = {10.1016/j.ejor.2025.05.032},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {838-856},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust parallel machine selection and scheduling with uncertain release times},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast optimization approach for a complex real-life 3D multiple bin size bin packing problem. <em>EJOR</em>, <em>327</em>(3), 820-837. (<a href='https://doi.org/10.1016/j.ejor.2025.05.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a real-life air cargo loading problem which is a variant of the three-dimensional Variable Size Bin Packing Problem with special bin forms of cuboid and non-cuboid unit load devices (ULDs). Packing is constrained by additional practical restrictions, such as load stability, (non-)stackable items, and weight distribution constraints. To solve the problem, we present an insertion heuristic embedded into a Randomized Greedy Search. The solution space is limited by only considering certain candidate points (so-called extreme points), which are promising positions to load an item. We extend the concept of extreme points proposed in the literature and allow moving extreme points for non-cuboid ULDs. A special sorting of the items, which combines a layered structure and free packing, is suggested. Moreover, we propose dividing the space of each ULD into smaller cells to accelerate the collision, non-floating, and stackability check while loading items. In a computational study, we analyze individual algorithm components and show the effectiveness of our method on adapted real-life instances from the literature.},
  archive      = {J_EJOR},
  author       = {Katrin Heßler and Timo Hintsch and Lukas Wienkamp},
  doi          = {10.1016/j.ejor.2025.05.016},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {820-837},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A fast optimization approach for a complex real-life 3D multiple bin size bin packing problem},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning assisted differential evolution for the dynamic resource constrained multi-project scheduling problem with static project schedules. <em>EJOR</em>, <em>327</em>(3), 808-819. (<a href='https://doi.org/10.1016/j.ejor.2025.05.059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large modular construction projects, such as shipbuilding, multiple similar projects arrive stochastically. At project arrival, a schedule has to be created, in which future modifications are difficult and/or undesirable. Since all projects use the same set of shared resources, current scheduling decisions influence future scheduling possibilities. To model this problem, we introduce the Dynamic Resource Constrained Multi-project Scheduling Problem with Static project Schedules. To find schedules, both a greedy approach and simulation-based approach with varying scenarios are introduced. Although the simulation-based approach schedules projects proactively, the computing times are long, even for small instances. Therefore, a method is introduced that learns from schedules obtained in the simulation-based method and uses a neural network to estimate the objective function value. It is shown that this method achieves a significant improvement in objective function value over the greedy algorithm, while only requiring a fraction of the computation time of the simulation-based method.},
  archive      = {J_EJOR},
  author       = {T. van der Beek and J.T. van Essen and J. Pruyn and K. Aardal},
  doi          = {10.1016/j.ejor.2025.05.059},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {808-819},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Machine learning assisted differential evolution for the dynamic resource constrained multi-project scheduling problem with static project schedules},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A flexible mathematical model for home health care problems. <em>EJOR</em>, <em>327</em>(3), 791-807. (<a href='https://doi.org/10.1016/j.ejor.2025.05.055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the health and social care sectors it is common for some specialized teams to travel to patients homes to provide care. These teams are typically made up of by a number of staff members with varying skills, starting locations and working hours. Patients require different types of care, during specific time windows, and may have special requirements, such as needing two staff members, or multiple visits with some sort of temporal dependency between them. Since teams need to decide which staff member will visit each patient, as well as the routes they will take to do so, this kind of planning problem is known in the literature as the Home Health Care Routing and Scheduling Problem (HHCRSP). We introduce a new mixed integer linear programming formulation for the HHCRSP that extends previous models. Our formulation can readily be adapted to address more specific variants in the scientific literature, proving a larger number of optimal solutions and stronger lower bounds on benchmark instances using the same computational framework. We further propose an instance generator for producing scenarios that closely resemble those of the National Health Service in the United Kingdom.},
  archive      = {J_EJOR},
  author       = {Miguel Reula and Consuelo Parreño-Torres and Carlos Lamas-Fernandez and Antonio Martinez-Sykora},
  doi          = {10.1016/j.ejor.2025.05.055},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {791-807},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A flexible mathematical model for home health care problems},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The dial-a-ride problem with limited pickups per trip. <em>EJOR</em>, <em>327</em>(3), 776-790. (<a href='https://doi.org/10.1016/j.ejor.2025.05.051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dial-a-Ride Problem (DARP) is an optimization problem that involves determining optimal routes and schedules for several vehicles to pick up and deliver items at minimum cost. Motivated by real-world carpooling and crowdshipping scenarios, we introduce an additional constraint imposing a maximum number on the number of pickups per trip. This results in the Dial-a-Ride Problem with Limited Pickups per Trip (DARP-LPT). We apply a fragment-based method for DARP-LPT, where a fragment is a partial path. Specifically, we extend two formulations from Rist and Forbes (2021): the Fragment Flow Formulation (FFF) and the Pickup-Space Fragment Formulation (PSFF). Furthermore, our results show that PSFF outperforms FFF, which in turn surpasses traditional arc-based formulations in both solution quality and computational efficiency. Additionally, we compare several existing fragment sets that differ in the length of their partial paths and find that the sets with shorter partial paths yield the best solution times when used with PSFF. In addition, we propose a new mixed fragment set, which is useful when the sets with longer partial paths become too large. In such cases, it yields the lowest CPU time.},
  archive      = {J_EJOR},
  author       = {Boshuai Zhao and Kai Wang and Wenchao Wei and Roel Leus},
  doi          = {10.1016/j.ejor.2025.05.051},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {776-790},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The dial-a-ride problem with limited pickups per trip},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new class of lower bounds for scheduling a batch processing machine to minimize makespan. <em>EJOR</em>, <em>327</em>(3), 754-775. (<a href='https://doi.org/10.1016/j.ejor.2025.05.047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of minimizing makespan on a batch-processing machine with limited capacity. Each job has a size and processing time, and multiple jobs can be processed simultaneously in a batch, provided the machine’s capacity is not exceeded. The batch processing time is determined by the longest processing time in batch. We show that the existing lower bound method has a worst-case performance ratio of 1/2, and propose a class of lower bound procedures ( LB m ) and its improved variant ( ILB m ). The new procedures take integer m , used to partition jobs depending on whether their sizes are greater than B / m or not, and provide tighter bounds as m increases. We prove that the worst-case performance ratio of LB m and ILB m is no worse than 4/7. Additionally, we show that they can be computed efficiently for m ≤3. Based on the structure of the proposed lower bound procedures, we introduce different valid inequalities ( VI ) and embed them into an existing MILP model to achieve a formulation with a tighter LP bound. To gain understanding on the quality of the bounds, we employ them in a branch and bound ( B & B ) algorithm. Results indicate that the B&B with new lower bound methods increases the number of optimally solved problem instances by 44% and 35% compared to the existing B&B and branch and price algorithms, respectively. Furthermore, the lower bound-driven VI s help increase the number of solved problems by more than 30%, achieving an optimality rate exceeding 96% across a wide range of problem instances.},
  archive      = {J_EJOR},
  author       = {Ali Husseinzadeh Kashan and Onur Ozturk},
  doi          = {10.1016/j.ejor.2025.05.047},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {754-775},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A new class of lower bounds for scheduling a batch processing machine to minimize makespan},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EATKG: An open-source efficient exact algorithm for the two-dimensional knapsack problem with guillotine constraints. <em>EJOR</em>, <em>327</em>(3), 735-753. (<a href='https://doi.org/10.1016/j.ejor.2025.05.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the Two-Dimensional Knapsack Problem with Guillotine Constraints, which is a famous NP -hard problem and is commonly encountered in industries where rectangular raw materials are cut into smaller pieces using guillotine cuts. We propose an efficient exact algorithm (EATKG) to solve this problem, which incorporates advanced techniques and novel elements, including an adapted preprocessing procedure, two enhanced upper bounds, an improved bidirectional tree search approach, and an iterative combination enumeration process. These components effectively balance the computation of upper and lower bounds and handle the issue of memory overflow. We extensively evaluate EATKG on eight classic benchmark sets, comprising 1,277 instances. Our algorithm solves 87% of the instances with an average computing time of 7 seconds, and 93% with an average computing time of 49 seconds. Moreover, EATKG efficiently solves nearly all small- and medium-sized instances, providing better solutions for 46 instances and tighter upper bounds for 109 instances. These results demonstrate the superior performance of our algorithm compared to leading algorithms. To support future research, we have made the source code for the proposed algorithm, along with the corresponding instance data, aggregated results, and detailed solutions, publicly available. This will facilitate further investigations and comparisons of solution methods.},
  archive      = {J_EJOR},
  author       = {Sunkanghong Wang and Roberto Baldacci and Qiang Liu and Lijun Wei},
  doi          = {10.1016/j.ejor.2025.05.033},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {735-753},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {EATKG: An open-source efficient exact algorithm for the two-dimensional knapsack problem with guillotine constraints},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Greedy randomized adaptive search procedures with path relinking. an analytical review of designs and implementations. <em>EJOR</em>, <em>327</em>(3), 717-734. (<a href='https://doi.org/10.1016/j.ejor.2025.02.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is a comprehensive review of the Greedy Randomized Adaptive Search Procedure (GRASP) metaheuristic and its hybridization with Path Relinking (PR). GRASP with PR has become a widely adopted approach for solving hard optimization problems since its proposal in 1999. The paper covers the historical development of GRASP with PR and its theoretical foundations, as well as recent advances in its implementation and application. The review includes a careful analysis of PR variants, paying special attention to memory-based and randomized designs, with a total of ten different implementations. It identifies the design questions that are still open in the scientific literature. The experimental section applies advanced PR implementations on two well-known combinatorial optimization problems, linear ordering and max-cut, in an effort to answer these open questions. The paper also explores the hybridization of PR and other metaheuristics, such as tabu search, scatter search, and random-keys genetic algorithms. Overall, this review provides valuable insights for researchers and practitioners seeking to implement GRASP with PR for solving optimization problems.},
  archive      = {J_EJOR},
  author       = {Manuel Laguna and Rafael Martí and Anna Martínez-Gavara and Sergio Pérez-Peló and Mauricio G.C. Resende},
  doi          = {10.1016/j.ejor.2025.02.022},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {717-734},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Greedy randomized adaptive search procedures with path relinking. an analytical review of designs and implementations},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The devil in the details: Dynamic prediction of loan portfolio profitability with macroeconomic drivers through multi-state modelling. <em>EJOR</em>, <em>327</em>(2), 703-715. (<a href='https://doi.org/10.1016/j.ejor.2025.07.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In typical loan portfolios such as mortgages and credit cards, many accounts often experience different stages of delinquency before eventually recovering, fully repaying their balance, or defaulting. From the lender perspective, these events, coupled with the state of the economy, can affect cash-flow and profitability significantly. This paper presents a novel framework for dynamic monitoring future expected profit margins and cash flows of loan accounts, taking into account (i) individual risk profiles, (ii) macroeconomic trends, and (iii) transitions between different stages of delinquency. We make three contributions. First, we show a method to predict future cash flows and profit margins over the life of a loan where the predicted probabilities of an account jumping between delinquency states are obligor specific, time varying, adjusted to be competing risks and dependent on predictions from a macroeconomic model. This model is much more comprehensive model than those in the literature. Second, we investigate different methods to compute optimised cut-offs to be used with the transition probabilities to predict jumps an account is expected to make between different states of delinquency. Third, we illustrate the method using a large sample of 30-year term mortgages and show the expected profit margins for segments of the portfolio. The method will be particularly useful to lenders, who must comply with IFRS9 or CECL.},
  archive      = {J_EJOR},
  author       = {Viani B. Djeundje and Jonathan Crook and Galina Andreeva},
  doi          = {10.1016/j.ejor.2025.07.008},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {703-715},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The devil in the details: Dynamic prediction of loan portfolio profitability with macroeconomic drivers through multi-state modelling},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bankruptcy prediction with fractional polynomial transformation of financial ratios. <em>EJOR</em>, <em>327</em>(2), 690-702. (<a href='https://doi.org/10.1016/j.ejor.2025.07.036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that simple nonlinear transformations of financial ratios, within a multivariate fractional polynomial approach, yield substantial improvements in bankruptcy prediction. The approach selects optimal power functions balancing parsimony and complexity. Focusing on a dataset comprising of non-financial firms, we develop a parsimonious nonlinear logit model with minimal parameter specification and clear interpretability, outperforming linear logit models. The model improves the in-sample fit, while out-of-sample it significantly reduces costly misclassification errors and improves discriminatory power. Similar insights are obtained when applying fractional polynomials on a secondary dataset consisting of banking firms. Interestingly, the fractional polynomial model compares favourably with other nonlinear models. By simulating a competitive loan market, we demonstrate that the bank using the fractional polynomial model builds a higher-quality loan portfolio, resulting in superior risk-adjusted profitability compared to banks employing alternative models.},
  archive      = {J_EJOR},
  author       = {Zenon Taoushianis},
  doi          = {10.1016/j.ejor.2025.07.036},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {690-702},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Bankruptcy prediction with fractional polynomial transformation of financial ratios},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entrepreneurs’ optimal decisions in equity crowdfunding campaigns. <em>EJOR</em>, <em>327</em>(2), 673-689. (<a href='https://doi.org/10.1016/j.ejor.2025.07.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Equity crowdfunding is a method of financing an initiative whereby an entrepreneur sells shares in her firm to a group of people (the crowd) on a dedicated platform. Understanding the forces that shape the behavior of both buyers in the crowd and entrepreneurs in equity crowdfunding platforms can help design more efficient platforms and increase the welfare of all participants. We therefore develop a common value sequential crowdfunding game-theoretic model, where the entrepreneur sells a percentage of her firm in order to raise money for its establishment and then shares the future value of the firm with the crowd. Buyers on the platform who visit the campaign decide whether or not to invest in it. Each buyer’s decision depends on the amount that has already been invested before him and on his own knowledge about the firm and the market in which it operates (which we model as a noisy signal that he obtains regarding the true value of the firm). By offering a different percentage in the firm, the entrepreneur leads the crowd to a different equilibrium. We characterize these equilibria and then analyze the entrepreneur’s decision. We show that the entrepreneur’s optimal percentage she offers for sale is non monotonic in the ex-ante probability of success. This is in-line with recent empirical findings. We further show that when buyers’ signals are very noisy, the entrepreneur may prefer buyers that have a less accurate signal regarding the true value of the firm over buyers with a more accurate signal.},
  archive      = {J_EJOR},
  author       = {Hana Tzur and Ella Segev},
  doi          = {10.1016/j.ejor.2025.07.004},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {673-689},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Entrepreneurs’ optimal decisions in equity crowdfunding campaigns},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A decision-making framework for supporting an equitable global vaccine distribution under humanitarian perspectives. <em>EJOR</em>, <em>327</em>(2), 655-672. (<a href='https://doi.org/10.1016/j.ejor.2025.05.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is motivated by the occurrence of vaccine nationalism in the setting of pandemics. Certain high-income countries (HICs) aggressively accumulated vaccinations while showing little concern for the vaccination challenges faced by low- and middle- income countries. This disparity fosters the proliferation and mutation of viruses, thus risking the global population’s health and welfare. Hence, we create a data-driven framework to tackle this humanitarian problem by facilitating the provision of vaccines. The framework comprises of two models: a network model named multi-strain Susceptible–Vaccinated–Infected–Removed–Susceptible and a vaccine distribution model with equitable constraints. The latter also encompasses the diverse uncertainty associated with vaccination hesitancy in different countries, in order to avoid potential wastage of resources. The vaccine distribution from our framework is based on greedy thought, thus enabling decision-makers to actively engage in the real-time vaccine allocation process. When the suggested framework is applied to the scenario of the COVID-19 pandemic, the simulation results indicate that fair distributions could accelerate the end of the pandemic. Additional scenarios, such as equitable levels and traveling intensity, are also examined in the sensitivity analysis. The progression of the epidemic under vaccine nationalism is moreover simulated to highlight its harmfulness and validate the efficacy of our framework. We demonstrate that the inequitable advantage experienced by HICs is temporary, as HICs are bound to suffer from virus variants in due course when vaccinations become less efficacious against them.},
  archive      = {J_EJOR},
  author       = {Jian Zhou and Junyang Cai and Athanasios A. Pantelous and Zhen Li and Musen Kingsley Li},
  doi          = {10.1016/j.ejor.2025.05.007},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {655-672},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A decision-making framework for supporting an equitable global vaccine distribution under humanitarian perspectives},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of geopolitical strain on global pharmaceutical supply chain design and drug shortages. <em>EJOR</em>, <em>327</em>(2), 641-654. (<a href='https://doi.org/10.1016/j.ejor.2025.05.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging geopolitical risks have begun to threaten global supply chains, including those that produce life-saving drugs. Export bans may prevent a company from shipping products internationally, and it is unclear how these new dynamics may affect company plans and persistent, worldwide drug shortages. To address these questions, we present a global pharmaceutical supply chain design model that considers the risk of export bans that are induced by supplier capacity disruptions and corresponding price increases. The model takes the company’s perspective as a decision-maker looking to locate plants and distribute drugs globally. It is a two-stage stochastic program that includes uncertainty in capacity, ability-to-export, and demand. The model is solved by integrating the Sample Average Approximation and L-shaped methods. We present conditions related to when demand will be met and a case study of a generic oncology drug. We find that preparing for geopolitical strain may increase resilience and profits as well as reduce shortages in the short term. At baseline, expected global shortages are high (17.2%) with disparities across country income levels (0.3%, 0.8%, 87.2%, and 87.6% for high, upper-middle, lower-middle, and low income countries, respectively). Pricing policies may improve drug access overall, back-shoring may slightly improve access for the country where it is implemented, and bilateral alliances may not be effective at improving access.},
  archive      = {J_EJOR},
  author       = {Martha L. Sabogal De La Pava and Emily L. Tucker},
  doi          = {10.1016/j.ejor.2025.05.002},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {641-654},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Effects of geopolitical strain on global pharmaceutical supply chain design and drug shortages},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling consumer stickiness in online platform pricing. <em>EJOR</em>, <em>327</em>(2), 623-640. (<a href='https://doi.org/10.1016/j.ejor.2025.04.041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the operational practice of JD.com, China’s largest online retailer, our study delves into the phenomenon of consumer stickiness. It measures the probability that consumers will remain loyal to a specific product, refraining from purchasing alternatives, even in the temporary absence of the focal product. Based on real data from JD.com, we show that consumer stickiness has a significantly positive impact on online sales, which is used to justify our formulation of the demand function in theoretical analysis. Specifically, we adopt a game-theoretical approach to analyze the impact of consumer stickiness on two-period pricing strategies in monopolistic and competitive markets. Findings reveal that incorporating consumer stickiness leads to differentiated pricing strategies, with low-quality products reducing prices in the second period and high-quality products increasing them. Stickiness enhances total sales in monopolistic markets with high-quality products and in competitive markets with high market potential. Furthermore, stickiness contributes to increased revenue and improvements in consumer surplus and social welfare under large or small market conditions, underscoring its strategic importance for pricing and welfare outcomes. These findings contribute valuable insights into the dynamics of online platform competition and highlight the strategic implications of consumer stickiness in influencing pricing and platform revenue.},
  archive      = {J_EJOR},
  author       = {Nina Yan and Tingting Tong and Gangshu (George) Cai},
  doi          = {10.1016/j.ejor.2025.04.041},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {623-640},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Modeling consumer stickiness in online platform pricing},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for the real-time inventory rack storage assignment and replenishment problem. <em>EJOR</em>, <em>327</em>(2), 606-622. (<a href='https://doi.org/10.1016/j.ejor.2025.05.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The e-commerce industry is quickly transforming towards more automation and technological advancements. With the growing intricacy of warehouse operations, there is a need for control systems that can efficiently handle this complexity. This study considers a Robotic Mobile Fulfillment System (RMFS), a semi-automated warehousing system. This system employs autonomous mobile robots (AMRs) to retrieve inventory racks from the storage area; this way, human activity is eliminated within the storage area itself. The fleet of robots both store and retrieve the inventory racks to either workstations, where human pickers are stationed that pick items from the racks, or replenishment stations, where depleted inventory racks can be restocked with items. An attractive characteristic of the RMFS is that it dynamically changes the positioning of the inventory racks based on the frequency of inventory rack requests and the state of their stock levels. The optimization objective considered in this study for the dynamic positioning problem of the racks within the storage area is to minimize the average cycle time of the mobile robots to perform retrieval and replenishment activities. We propose a deep reinforcement learning approach to train a decision-making agent to learn a policy for the storage assignment and replenishment of inventory racks. The learned policy is compared to the commonly used decision rules in the academic literature on this problem. The experimental results show the potential benefits of training an agent to learn a storage and replenishment policy. Cycle time improvements up to 5.4 % can be achieved over the best-performing decision rules. This research contributes to advancing the understanding of intelligent storage assignment and replenishment strategies for the real-time decision-making process within an RMFS.},
  archive      = {J_EJOR},
  author       = {Sander Teck and Tú San Phạm and Louis-Martin Rousseau and Pieter Vansteenwegen},
  doi          = {10.1016/j.ejor.2025.05.008},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {606-622},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Deep reinforcement learning for the real-time inventory rack storage assignment and replenishment problem},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying hidden critical elements in interconnected systems: An influence dynamics analysis approach considering structural constraints. <em>EJOR</em>, <em>327</em>(2), 592-605. (<a href='https://doi.org/10.1016/j.ejor.2025.05.038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The systems analysis field has traditionally focused on identifying the essential elements within an interconnected system and analyzing the cause-and-effect relationships among them. However, most decision-making methods in system analysis have been one-sided. They primarily rely on the interactions between elements to make decisions, neglecting to account for the non-uniform influence attenuation among elements and unequal importance diffusion. Furthermore, these two factors are closely tied to the topological structure of systems, which has been an often-overlooked aspect in previous research, leading to inaccurate identification and omission of hidden key elements. In response to these challenges, this paper introduces a novel method called SIDA ( S tructural-constrained I nfluence D ynamic A nalysis). We utilize structural constraint coefficients derived from structural hole theory to describe the non-uniform attenuation. Furthermore, we integrate an influence and distance-weighted PageRank algorithm to manage the unequal importance diffusion taking into account both the influence and the distance between elements within systems. We validated our proposed method through a comprehensive analysis of a pharmaceutical industry ecosystem, comparing its performance with previous approaches to validate its effectiveness and practicality. The case study results demonstrate that SIDA produces more reasonable element analysis and ranking outcomes.},
  archive      = {J_EJOR},
  author       = {Caibo Zhou and Wenyan Song and Huiwen Wang and Lihong Wang},
  doi          = {10.1016/j.ejor.2025.05.038},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {592-605},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Identifying hidden critical elements in interconnected systems: An influence dynamics analysis approach considering structural constraints},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust binary and multinomial logit models for classification with data uncertainties. <em>EJOR</em>, <em>327</em>(2), 577-591. (<a href='https://doi.org/10.1016/j.ejor.2025.05.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary logit (BNL) and multinomial logit (MNL) models are the two most widely used discrete choice models for travel behavior modeling and prediction. However, in many scenarios, the collected data for those models are subject to measurement errors. Previous studies on measurement errors mostly focus on “better estimating model parameters” with training data. In this study, we focus on using BNL and MNL for classification problems, that is, to “better predict the behavior of new samples” when measurement errors occur in testing data. To this end, we propose a robust BNL and MNL framework that is able to account for data uncertainties in both features and labels. The models are based on robust optimization theory that minimizes the worst-case loss over a set of uncertainty data scenarios. Specifically, for feature uncertainties, we assume that the ℓ p -norm of the measurement errors in features is smaller than a pre-established threshold. We model label uncertainties by limiting the number of mislabeled choices to at most Γ . Based on these assumptions, we derive a tractable robust counterpart. The derived robust-feature BNL and the robust-label MNL models are exact. However, the formulation for the robust-feature MNL model is an approximation of the exact robust optimization problem. An upper bound of the approximation gap is provided. We prove that the robust estimators are inconsistent but with a higher trace of the Fisher information matrix. They are preferred when out-of-sample data has errors due to the shrunk scale of the estimated parameters. The proposed models are validated in a binary choice data set and a multinomial choice data set, respectively. Results show that the robust models (both features and labels) can outperform the conventional BNL and MNL models in prediction accuracy and log-likelihood. We show that the robustness works like “regularization” and thus has better generalizability.},
  archive      = {J_EJOR},
  author       = {Baichuan Mo and Yunhan Zheng and Xiaotong Guo and Ruoyun Ma and Jinhua Zhao},
  doi          = {10.1016/j.ejor.2025.05.013},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {577-591},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust binary and multinomial logit models for classification with data uncertainties},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalarisation-based risk concepts for robust multi-objective optimisation. <em>EJOR</em>, <em>327</em>(2), 559-576. (<a href='https://doi.org/10.1016/j.ejor.2025.04.054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust optimisation is a well-established framework for optimising functions in the presence of uncertainty. The inherent goal of this problem is to identify a collection of inputs whose outputs are both desirable for the decision maker, whilst also being robust to the underlying uncertainties in the problem. In this work, we study the multi-objective case of this problem. We identify that the majority of all robust multi-objective algorithms rely on two key operations: robustification and scalarisation. Robustification refers to the strategy that is used to account for the uncertainty in the problem. Scalarisation refers to the procedure that is used to encode the relative importance of each objective to a scalar-valued reward. As these operations are not necessarily commutative, the order that they are performed in has an impact on the resulting solutions that are identified and the final decisions that are made. The purpose of this work is to give a thorough exposition on the effects of these different orderings and in particular highlight when one should opt for one ordering over the other. As part of our analysis, we showcase how many existing risk concepts can be integrated into the specification and solution of a robust multi-objective optimisation problem. Besides this, we also demonstrate how one can principally define the notion of a robust Pareto front and a robust performance metric based on our “robustify and scalarise” methodology. To illustrate the efficacy of these new ideas, we present two insightful case studies which are based on real-world data sets.},
  archive      = {J_EJOR},
  author       = {Ben Tu and Nikolas Kantas and Robert M. Lee and Behrang Shafei},
  doi          = {10.1016/j.ejor.2025.04.054},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {559-576},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Scalarisation-based risk concepts for robust multi-objective optimisation},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A structured framework for supporting the participatory development of consensual scenario narratives. <em>EJOR</em>, <em>327</em>(2), 540-558. (<a href='https://doi.org/10.1016/j.ejor.2025.04.048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High levels of uncertainty faced by decision makers can be alleviated by characterizing multiple possible ways in which the future might unfold with scenario narratives. Aiming at describing alternative plausible chains of outcomes of key uncertainty factors, scenario narratives are often associated with graphical networks describing the relationships between the outcomes of the factors. We present a participatory framework for bottom-up development of such networks, the PACNAP (PArticipatory development of Consensual narratives through Network Aggregation and Pruning) framework. In this framework, relationships of influence between factor outcomes are judged by a group of scenario process participants. We develop an optimization model for pruning an aggregated graph based on these judgments. The model selects those edges of the aggregate graph that the participants most agree upon and can be tailored to identify compact graphs of varying degrees of cyclicity. As a result, a variety of graphical representations of varying structural richness can be explored to arrive at a succinct representation of a consensus view on the structure of a joint narrative. To this end, the main formal results are the representation of the participants’ agreement lexicographically in a linear objective function of a 0-1 program, and the translation of the requisites of the compactness and cyclicity of the resulting pruned graphs into a set of network flow constraints. The problem of identifying a consensus graphical representation is a general one and our graph pruning method has application potential outside the specific domain of narrative development as well.},
  archive      = {J_EJOR},
  author       = {Teemu Seeve and Eeva Vilkkumaa and Alec Morton},
  doi          = {10.1016/j.ejor.2025.04.048},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {540-558},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A structured framework for supporting the participatory development of consensual scenario narratives},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Index policies for campaign promotion strategies in reward-based crowdfunding. <em>EJOR</em>, <em>327</em>(2), 515-539. (<a href='https://doi.org/10.1016/j.ejor.2025.07.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reward-based crowdfunding plays a crucial role in fundraising for start-up entrepreneurs. Recent studies, however, have shown that the actual success rate of fundraising projects is surprisingly low across multiple crowdfunding platforms. This paper considers crowdfunding platforms’ decision-making of selecting projects to highlight on their homepage to boost the chance of success for projects, and investigates promotion strategies aiming at maximizing platforms’ revenue over a fixed period. We characterize backers’ investment decisions by a discrete choice model with a time-varying coefficient of herding effect, and formulate the problem as a stochastic dynamic program, which is however computationally intractable. To address this issue, we follow the Whittle’s Restless Bandit approach to decompose the problem into a collection of single-project problems and prove indexability for each project under some mild conditions. We show that the index values of the proposed index policy can be directly derived from the value-to-go of each project under the non-promotion policy, which is calculated recursively offline with a linear-time complexity. Moreover, to further enhance the scalability we develop a closed-form approximation to calculate the index values online. To the best of our knowledge, this work is the first in the literature to develop index policies for campaign promotions in reward-based crowdfunding. It is also the first attempt to provide indexability analysis of bi-dimensional restless bandits coupled by not only resource but also demand. Extensive numerical experiments show that the proposed index policies outperform the other benchmark heuristics in most of the scenarios considered.},
  archive      = {J_EJOR},
  author       = {Chenguang Wang and Dong Li and Baibing Li},
  doi          = {10.1016/j.ejor.2025.07.020},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {515-539},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Index policies for campaign promotion strategies in reward-based crowdfunding},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Singular control in a cash management model with ambiguity. <em>EJOR</em>, <em>327</em>(2), 500-514. (<a href='https://doi.org/10.1016/j.ejor.2025.07.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a singular control model of cash reserve management, driven by a diffusion under ambiguity. The manager is assumed to have maxmin preferences over a set of priors characterized by κ -ignorance. A verification theorem is established to determine the firm’s cost function and the optimal cash policy; the latter taking the form of a control barrier policy. In a model driven by arithmetic Brownian motion, we use Dynkin games to show that an increase in ambiguity leads to higher expected costs under the worst-case prior and a narrower inaction region. The latter effect can be used to provide an ambiguity-driven explanation for observed cash management behavior. Our findings can be applied to broader applications of singular control in managing inventories under ambiguity.},
  archive      = {J_EJOR},
  author       = {Arnon Archankul and Giorgio Ferrari and Tobias Hellmann and Jacco J.J. Thijssen},
  doi          = {10.1016/j.ejor.2025.07.023},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {500-514},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Singular control in a cash management model with ambiguity},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair and profitable even when serving different customer classes: How pricing and lead-time quotation can help. <em>EJOR</em>, <em>327</em>(2), 491-499. (<a href='https://doi.org/10.1016/j.ejor.2025.05.034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we model a production/inventory system serving two classes of customers with a single type of product. Demand from each class depends on the price and the lead-time quoted. One class comprises customers sensitive to delays who are willing to pay higher prices for shorter lead times. Customers of the other class are price sensitive who can tolerate waiting longer for product delivery if they are charged lower prices. By modeling the system as an M n / M / 1 type make-to-stock queue, we propose four fair policies. These fair policies assure that customers are charged lower prices when they are quoted longer lead times and a high proportion of the deliveries is made during the quoted lead times. Two FCFS (first-come, first-served) policies ignore class differences. Two multilevel rationing (MR) policies prioritize the delay-sensitive class over the other. While determining the price and the lead-time, the refined FCFS and MR policies additionally consider the order in which a customer enters the queue. With a numerical study, we explore when the MR policies taking customer differences into consideration are more profitable than the FCFS policies.},
  archive      = {J_EJOR},
  author       = {Sinan Dede and Barış Balcıog̃lu},
  doi          = {10.1016/j.ejor.2025.05.034},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {491-499},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fair and profitable even when serving different customer classes: How pricing and lead-time quotation can help},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nested branch-and-price for multi-mode nanosatellite task scheduling with interior-point regularization and GPU acceleration. <em>EJOR</em>, <em>327</em>(2), 469-490. (<a href='https://doi.org/10.1016/j.ejor.2025.05.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capabilities of nanosatellites are constrained by their limited power availability and size, which poses challenges for mission planning and operation. This study addresses the Offline Nanosatellite Task Scheduling (ONTS) problem by introducing multi-mode capability into the scheduling process, enhancing its relevance for more realistic, adaptable, and robust mission planning. We propose a Mixed-Integer Linear Programming (MILP) model for this problem that accommodates conventional resource and temporal constraints across multiple operational modes. The MILP model is improved with valid inequalities incorporating auxiliary variables alongside multi-mode cover cuts enhanced with lifting procedures. Furthermore, we introduce a Nested Branch-and-Price (NB&P) algorithm that enhances the standard branch-and-price approach by incorporating a dual-level optimization structure for handling hierarchical scheduling. This dual framework simultaneously facilitates job allocation and mode selection, employing dynamic column generation influenced by dual prices to progressively refine task schedules towards optimality. Additionally, enhanced interior-point methods have been effectively adapted for tackling large-scale instances, aligned with a GPU-accelerated dynamic programming solution utilizing CUDA technology. Empirical evaluations show that the modified MILP approach, combined with the NB&P algorithm, significantly improves computational efficiency, demonstrating up to a 629-fold reduction in computation time and consistently achieving zero-gap solutions.},
  archive      = {J_EJOR},
  author       = {Laio Oriel Seman and Cezar Antônio Rigo and Eduardo Camponogara and Pedro Munari},
  doi          = {10.1016/j.ejor.2025.05.020},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {469-490},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Nested branch-and-price for multi-mode nanosatellite task scheduling with interior-point regularization and GPU acceleration},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An exact algorithm for fleet co-deployment and slot co-chartering in a sustainable shipping alliance under emissions trading system. <em>EJOR</em>, <em>327</em>(2), 450-468. (<a href='https://doi.org/10.1016/j.ejor.2025.05.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shipping alliances have emerged as a cooperation platform between independent shipping companies, aiming to enhance customer satisfaction and exploit the economies of scale through capacity and information sharing. A sustainable shipping alliance should operate in a profitable, fair and environmentally friendly way under emerging Emissions Trading System (ETS). A non-convex mixed-integer nonlinear programming model is suggested to jointly optimize the fleet co-deployment in the network, sailing speed in each shipping leg, schedule design for each shipping service, and the slot allocation and co-chartering for each alliance member. These decisions ultimately determine each company’s carbon emissions. Under the ETS, companies are charged for emissions that exceed their allowances, while any surplus allowances can be traded for revenue in carbon markets. In addition to maximizing the alliance’s total profit, this study minimizes profit margin variation among members in proportion to their investment, promoting fairness in a novel way. A tailored spatial branch-and-bound (SB&B) algorithm is developed to deliver the global optimal solution for the problem. Novel problem relaxation and branching strategies are suggested based on the structure of the programming model. The SB&B algorithm significantly outperforms an existing non-convex nonlinear solver. Compared to case which do not consider slot co-chartering and fairness, our study improves total profit by 3.13 %, meets 0.52 % more freight demand, and ensures a fairer profit distribution on average. Under the ETS, carbon emissions can be reduced by up to 54.3 %, with smaller ships being used and average sailing speeds decreasing as the emission trading price rises from $0/tonne to $300/tonne.},
  archive      = {J_EJOR},
  author       = {Yadong Wang and Shenghui Zhu and Çağatay Iris},
  doi          = {10.1016/j.ejor.2025.05.021},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {450-468},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An exact algorithm for fleet co-deployment and slot co-chartering in a sustainable shipping alliance under emissions trading system},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lagrangian relaxation and branch-and-price algorithm for resource assignment problem in divisional seru systems. <em>EJOR</em>, <em>327</em>(2), 432-449. (<a href='https://doi.org/10.1016/j.ejor.2025.02.038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses seru formation problem in divisional seru production system (SPS), which focuses on job-seru assignment, worker-seru assignment and operation-worker assignment in each seru. The problem is formulated as a mixed-integer nonlinear programming (MINLP) model with the objective of minimizing training and processing costs of workers. Once the job-seru assignment is determined, we employ a mixed-integer linear programming (MILP) model to describe worker-seru and operation-worker assignment in each seru. To tackle this challenge, we propose a two-phase approach to deal with this problem. In the first phase, we propose a Lagrangian relaxation algorithm to determine job-seru assignment, this approach can quickly compute the lower bound of the MILP by enumerating all possible job-seru assignments and eliminate unpromising ones. Subsequently, in the second phase, for each remaining job-seru assignment, we develop a branch-and-price algorithm to solve the MILP exactly. It is in the branch-and-bound framework, each node is solved by column generation (CG) algorithm. In CG, we apply a Dantzig Wolfe decompose to divide the original problem into a master problem and the pricing problems. A novel label-setting algorithm is employed based on the characteristics of the pricing problem. Additionally, we introduce effective acceleration strategies such as dominance rules and heuristic pricing. It facilitates the selection of the optimal job-seru assignment and obtains the optimal solution for the entire problem. Finally, extensive experiments validate the effectiveness and superiority of our proposed algorithm. We also discuss the impact of selected parameters on the cost and offer managerial insights.},
  archive      = {J_EJOR},
  author       = {Shiming Chen and Chengkuan Zeng and Yu Zhang and Jiafu Tang and Chongjun Yan},
  doi          = {10.1016/j.ejor.2025.02.038},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {432-449},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Lagrangian relaxation and branch-and-price algorithm for resource assignment problem in divisional seru systems},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The generalized assignment problem with fixed processing times and uniform processing costs to minimize total cost. <em>EJOR</em>, <em>327</em>(2), 420-431. (<a href='https://doi.org/10.1016/j.ejor.2025.05.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized assignment problem (GAP) is a foundational problem in operations research, but its progress is quite limited. In this paper we study an important special case of the GAP with fixed processing times and uniform processing costs, where the upper bound of the makespan is given, and the objective to minimize the total processing cost. We prove a critical technical lemma, which enables us to develop an approximation algorithm with an improved performance ratio of 1 + ( γ − 1 ) ϵ , where ϵ ∈ ( 0 , 1 3 ] can be any small constant and γ is the maximum to the minimum processing cost per unit time on a machine, improving on the existing performance ratio of 2 + γ 3 in the literature. For the general problem when γ is arbitrarily, we show that it is NP-hard to approximate within a constant performance ratio. For the special case when γ is a constant, we present an efficient PTAS (polynomial time approximation scheme) by applying the technical lemma. Our techniques and results bring new insights into the GAP research.},
  archive      = {J_EJOR},
  author       = {Weidong Li and Jinwen Ou},
  doi          = {10.1016/j.ejor.2025.05.031},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {420-431},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The generalized assignment problem with fixed processing times and uniform processing costs to minimize total cost},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using helical polyhedron for online irregular strip packing problem with free rotations. <em>EJOR</em>, <em>327</em>(2), 407-419. (<a href='https://doi.org/10.1016/j.ejor.2025.05.019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The packing of irregular pieces is widely applied across various industries including metalworking, woodworking, clothing manufacturing, and leather goods production. Allowing rotation during packing, particularly in scenarios where materials are homogeneous, can yield superior outcomes by reducing material wastage, thus contributing to cost-saving and environmental preservation. This study investigates the online irregular strip packing problem allowing free rotation, inspired by a leather handicraft workshop, where orders arrive infrequently and vary widely in content. The objective is to minimize the sheet length utilized. Most existing literature models irregular strip packing problem with rotation as a nonlinear programming problem, making it challenging to obtain the optimal position and orientation of every single input piece despite advancements in optimization solvers. In this paper, a novel approach is proposed to solve online irregular strip packing problem with rotation. We rotate the input polygon while simultaneously translating it along the z -axis, forming a helix. Thus, the problem of selecting the rotation angle is transformed into determining the z -coordinate of the helix’s cross-section. Subsequently, meshing the helix into a polyhedron allows us to propose a mixed integer linear formulation based on its Minkowski sum with other polygons. To ensure guaranteed optimality, we introduce a branch-and-bound algorithm tailored to the problem. Extensive numerical experiments indicate the effectiveness and competitiveness of our algorithm over state-of-the-art nonlinear formulations for irregular strip packing problem with rotation.},
  archive      = {J_EJOR},
  author       = {Yulin Liu and Li Zheng},
  doi          = {10.1016/j.ejor.2025.05.019},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {407-419},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Using helical polyhedron for online irregular strip packing problem with free rotations},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Presolving and cutting planes for the generalized maximal covering location problem. <em>EJOR</em>, <em>327</em>(2), 394-406. (<a href='https://doi.org/10.1016/j.ejor.2025.05.017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the generalized maximal covering location problem (GMCLP) which establishes a fixed number of facilities to maximize the weighted sum of the covered customers, allowing customer weights to be positive or negative. Due to the huge number of linear constraints to model the covering relations between the candidate facility locations and customers, and particularly the poor linear programming (LP) relaxation, the GMCLP is extremely difficult to solve by state-of-the-art mixed integer programming (MIP) solvers. To improve the computational performance of MIP-based approaches for solving GMCLPs, we propose customized presolving and cutting plane techniques, which are isomorphic aggregation, dominance reduction, and two-customer inequalities. The isomorphic aggregation and dominance reduction can not only reduce the problem size but also strengthen the LP relaxation of the MIP formulation of the GMCLP. The two-customer inequalities can be embedded into a branch-and-cut framework to further strengthen the LP relaxation of the MIP formulation on the fly. By extensive computational experiments, we show that all three proposed techniques can substantially improve the capability of MIP solvers in solving GMCLPs. In particular, for a testbed of 40 instances with identical numbers of customers and candidate facility locations in the literature, the proposed techniques enable us to provide optimal solutions for 13 previously unsolved benchmark instances; for a testbed of 336 instances where the number of customers is much larger than the number of candidate facility locations, the proposed techniques can turn most of them from intractable to easily solvable.},
  archive      = {J_EJOR},
  author       = {Wei Lv and Cheng-Yang Yu and Jie Liang and Wei-Kun Chen and Yu-Hong Dai},
  doi          = {10.1016/j.ejor.2025.05.017},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {394-406},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Presolving and cutting planes for the generalized maximal covering location problem},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years of research in scheduling — Theory and applications. <em>EJOR</em>, <em>327</em>(2), 367-393. (<a href='https://doi.org/10.1016/j.ejor.2025.01.034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an overview of scheduling research done over the last half century. The main focus is on what is typically referred to as machine scheduling. The first section describes the general framework for machine scheduling models and introduces the notation. The second section discusses the basic deterministic machine scheduling models, including single machine, parallel machines, flow shops, job shops, and open shops. The third section describes more elaborate models, including multi-objective and multi-agent scheduling models, scheduling with controllable processing times, scheduling with rejection, just-in-time scheduling, scheduling with due date assignments, time-dependent scheduling, and scheduling with batching and setups. The two subsequent sections consider scheduling under uncertainty; section four goes into online and robust scheduling and section five covers stochastic scheduling models. The next section describes a variety of important scheduling applications, including applications in manufacturing, in services, and in information processing. The last section presents the main conclusions and discusses future research directions.},
  archive      = {J_EJOR},
  author       = {Alessandro Agnetis and Jean-Charles Billaut and Michael Pinedo and Dvir Shabtay},
  doi          = {10.1016/j.ejor.2025.01.034},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {2},
  pages        = {367-393},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of research in scheduling — Theory and applications},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reverse stackelberg model for demand response in local energy markets. <em>EJOR</em>, <em>327</em>(1), 352-366. (<a href='https://doi.org/10.1016/j.ejor.2025.06.017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era where renewable energy resources are increasingly integrated into our power systems, and consumer-centric approaches gain traction, local energy markets emerge as a pivotal mechanism for empowering prosumers. This paper presents a novel bilevel optimization model that uniquely blends the dynamics of peer-to-peer energy markets with the physical realities of power distribution networks. The innovation steems from introducing a tariff design approach based on affine functions to shape prosumer behavior towards operationally efficient and secure energy exchanges. This is critical as previous market designs often overlooked the physical constraints of power flows, leading to potential risks in voltage regulation and economic efficiency. The lower level of the model encapsulates the interactions among prosumers in a generalized Nash equilibrium problem (GNEP), modeling active and reactive power injections of prosumers. The upper level, representing the role of the distribution system operator, strategically computes tariffs to steer the market to an operationally efficient equilibrium. The paper relies on the classical Nikaido–Isoda (NI) reformulation to characterize the GNEP, a key aspect in leveraging a proof of strong stability of the lower-level solution. Computational experiments on various IEEE test feeder instances reveal the model’s capacity to efficiently align prosumer behavior with operational objectives, utilizing only the tariff information, thereby simplifying the decision-making process in complex distribution systems.},
  archive      = {J_EJOR},
  author       = {Juan Sepúlveda and Luce Brotcorne and Hélène Le Cadre},
  doi          = {10.1016/j.ejor.2025.06.017},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {352-366},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A reverse stackelberg model for demand response in local energy markets},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic capacity investment with common ownership. <em>EJOR</em>, <em>327</em>(1), 340-351. (<a href='https://doi.org/10.1016/j.ejor.2025.05.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study how common ownership affects the magnitude and dynamics of investments in a duopoly. Followers exhibit less aggressive timing and quantity reactions because they internalize their effects on leaders. Leaders are therefore more likely to opt for a deterrence strategy, but their own internalization of followers softens their decisions. If firm roles are exogenous, high common ownership links lead to a relatively efficient staged investment outcome. Conversely, if firm roles are endogenous, high common ownership drives the winner of the preemption race to concede a “follower monopoly.” Our numerical analysis finds that common ownership is generally detrimental to consumer surplus and welfare.},
  archive      = {J_EJOR},
  author       = {Domenico De Giovanni and Richard Ruble and Dimitrios Zormpas},
  doi          = {10.1016/j.ejor.2025.05.026},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {340-351},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Strategic capacity investment with common ownership},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physician scheduling in case managers style emergency departments: Machine learning-aided solution approaches. <em>EJOR</em>, <em>327</em>(1), 326-339. (<a href='https://doi.org/10.1016/j.ejor.2025.05.035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency department (ED) crowding has become a common phenomenon worldwide. A number of interventions have been proposed to improve operations in EDs, such as scheduling physicians to manage varying patient demands. Motivated by a collaboration with a large ED, we study physician scheduling in the ED. The ED is modeled as a time-varying case managers system where the number of patients simultaneously assigned to a single physician is limited by maximum caseloads. To match real-life scenarios, we consider time-varying patient arrivals, temporary ED overloading, and patient-physician assignments. We first analyze patient flow and service procedures using real data to capture the features of the ED. Next, a mathematical model of physician scheduling is constructed. To effectively solve this complex problem, two machine learning-based solution approaches are designed. The first approach integrates an extreme gradient boosting model with Gurobi. The second involves a variable neighborhood search algorithm, in which a long short-term memory network is incorporated to evaluate the solution to the problem. Numerical experiments indicate that the proposed approaches can yield high-quality solutions within reasonable time frames. The physician schedules generated by the second approach outperform those generated by the first approach and are also superior to the actual schedules used by our partner ED. For the data from the stable period, our solutions reduce the average patient waiting time and total physician working time by 10.32 % and 14.79 %, respectively, compared to the actual ED schedules. During the COVID-19 outbreak, these two metrics are respectively reduced by 8.06 % and 12.9 %.},
  archive      = {J_EJOR},
  author       = {Ran Liu and Bo Zhou and Shiming Wang and Huiyin Ouyang},
  doi          = {10.1016/j.ejor.2025.05.035},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {326-339},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Physician scheduling in case managers style emergency departments: Machine learning-aided solution approaches},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for solving the stochastic e-waste collection problem. <em>EJOR</em>, <em>327</em>(1), 309-325. (<a href='https://doi.org/10.1016/j.ejor.2025.04.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing influence of the internet and information technology, Electrical and Electronic Equipment (EEE) has become a gateway to technological innovations. However, discarded devices, also called e-waste, pose a significant threat to the environment and human health if not properly treated, disposed of, or recycled. In this study, we extend a novel model for the e-waste collection in an urban context: the Heterogeneous VRP with Multiple Time Windows and Stochastic Travel Times (HVRP-MTWSTT). We propose a solution method that employs deep reinforcement learning to guide local search heuristics (DRL-LSH). The contributions of this paper are as follows: (1) HVRP-MTWSTT represents the first stochastic VRP in the context of the e-waste collection problem, incorporating complex constraints such as multiple time windows across a multi-period horizon with a heterogeneous vehicle fleet, (2) The DRL-LSH model uses deep reinforcement learning to provide an online adaptive operator selection layer, selecting the appropriate heuristic based on the search state. The computational experiments demonstrate that DRL-LSH outperforms the state-of-the-art hyperheuristic method by 24.26% on large-scale benchmark instances, with the performance gap increasing as the problem size grows. Additionally, to demonstrate the capability of DRL-LSH in addressing real-world problems, we tested and compared it with reference metaheuristic and hyperheuristic algorithms using a real-world e-waste collection case study in Singapore. The results showed that DRL-LSH significantly outperformed the reference algorithms on a real-world instance in terms of operating profit.},
  archive      = {J_EJOR},
  author       = {Dang Viet Anh Nguyen and Aldy Gunawan and Mustafa Misir and Lim Kwan Hui and Pieter Vansteenwegen},
  doi          = {10.1016/j.ejor.2025.04.033},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {309-325},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Deep reinforcement learning for solving the stochastic e-waste collection problem},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing the finnish colorectal cancer population screening program with decision programming. <em>EJOR</em>, <em>327</em>(1), 295-308. (<a href='https://doi.org/10.1016/j.ejor.2025.04.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Finland, colorectal cancer (CRC) incidence rates have steadily increased over the last decades and as of 2020, CRC is the second most common cancer in both males and females. CRC is a crucial concern for the public health of Finland, highlighted by the recent implementation of a national population screening program. In this paper, we optimize the screening test positivity cut-off levels and the use of potential incentives for stratified populations to minimize cancer prevalence. The optimization results, computed with the novel Decision Programming approach for discrete multi-stage decision problems under uncertainty, show the optimal cut-off levels and uses of incentives for Finnish target groups subject to different constraints on colonoscopy capacity. The outcomes of these optimal strategies are estimated to determine the expected corresponding prevalences of CRC and required colonoscopies, and expected third-party costs. Finally, measures describing different equality perspectives are presented.},
  archive      = {J_EJOR},
  author       = {Lauri Neuvonen and Mary Dillon and Eeva Vilkkumaa and Ahti Salo and Maija Jäntti and Sirpa Heinävaara},
  doi          = {10.1016/j.ejor.2025.04.022},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {295-308},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing the finnish colorectal cancer population screening program with decision programming},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rescue network design considering uncertainty and deprivation cost in urban waterlogging disaster relief. <em>EJOR</em>, <em>327</em>(1), 280-294. (<a href='https://doi.org/10.1016/j.ejor.2025.04.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a rescue network design problem involving uncertainty and deprivation cost, in which decisions on pumping station setup and drainage truck location and allocation are considered simultaneously. We formulate the problem as a two-stage nonlinear stochastic programming model that is difficult to solve directly because the objective function contains a nonlinear convex deprivation cost function. To address the nonlinearity in the model, quadratic outer approximation and second-order cone programming approaches are employed. Furthermore, utilizing the characteristic that affected time can take finite discrete values, an exact linearization approach is developed to reformulate the deprivation cost function, which leads to a mixed-integer linear programing reformulation. To solve large-scale reformulation problems, a scenario grouping-based progressive hedging algorithm is proposed. A method of constructing must-link constraints is used with K-means++ to efficiently group scenarios. Moreover, extensive numerical experiments and a real-world case study (of a waterlogging risk zone in Zhengzhou, China) are presented to test the applicability and efficiency of the proposed model and solution approaches. Computational results show that the exact linearization approach is competitive in dealing with the deprivation cost function. The proposed algorithm demonstrates the best computational performance in solving large-scale problems.},
  archive      = {J_EJOR},
  author       = {Shaolong Hu and Qing-Mi Hu and Zhaoyang Lu and Lingxiao Wu},
  doi          = {10.1016/j.ejor.2025.04.025},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {280-294},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Rescue network design considering uncertainty and deprivation cost in urban waterlogging disaster relief},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Electric vehicle fleet charging management: An approximate dynamic programming policy. <em>EJOR</em>, <em>327</em>(1), 263-279. (<a href='https://doi.org/10.1016/j.ejor.2025.04.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing prevalence of electric vehicles (EVs) requires efficient charging management strategies to tackle the challenges associated with their integration into the power grid. This requirement is particularly true for Charging-as-a-Service (CaaS) providers, who manage charging services for fleet operators in exchange for a fixed service fee. Incorporating uncertainty into optimization models for this dynamic environment further complicates the associated optimization problem, which falls into the NP-hard class. This research introduces an innovative approximate dynamic programming (ADP) policy for managing the charging of EV fleets at a charging depot equipped with diverse multi-connector chargers. A feature mapping analysis identifies critical system features that shape the future costs of a decision. A comparative analysis illustrates the effectiveness of the proposed policy in terms of cost reduction and service level. Moreover, we observe significant reductions in computation time when updating charging decisions compared to a two-stage rule-based model developed as a benchmark. In addition to benefits for EV fleet operators and CaaS providers, the proposed policy contributes to power grid sustainability by reducing charge load during peak hours, thereby enhancing overall grid stability and efficiency.},
  archive      = {J_EJOR},
  author       = {Ehsan Mahyari and Nickolas Freeman},
  doi          = {10.1016/j.ejor.2025.04.031},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {263-279},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Electric vehicle fleet charging management: An approximate dynamic programming policy},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel centralized cross-efficiency evaluation via explainable artificial intelligence in the context of big data. <em>EJOR</em>, <em>327</em>(1), 247-262. (<a href='https://doi.org/10.1016/j.ejor.2025.05.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-efficiency evaluation in data envelopment analysis (DEA) assumes that decision making units (DMUs) have full flexibility in choosing weights according to their individual preferences. However, this total autonomy may be inapplicable in some centralized organizational scenarios. To address this problem, this paper introduces a novel centralized cross-efficiency evaluation which considers both individual and organizational preferences with assistance of explainable artificial intelligence (XAI) in the context of big data. Specifically, XAI is first applied to approach the organizational efficiency function and then calculate the marginal contribution of each variable as the variable importance, which represents the organizational preference. Furthermore, we propose a centralized secondary goal model to select the unique optimal weight profile from the candidate weights that remain self-efficiency as Pareto-optimal, such that the deviation between individual and organizational preferences is minimized. In addition, a centralization factor is introduced to ensure that the model's centralization degree corresponds to the actual centralization level in organizational management. Finally, the proposed method is applied to evaluate the efficiency of DMUs within three different centralized organizations sequentially. The results verify that the proposed method yields more discriminative and robust efficiency scores within organizations compared to several previous cross-efficiency evaluation methods.},
  archive      = {J_EJOR},
  author       = {Min Yang and Zixuan Wang and Liang Liang},
  doi          = {10.1016/j.ejor.2025.05.012},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {247-262},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A novel centralized cross-efficiency evaluation via explainable artificial intelligence in the context of big data},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal insurance design with lambda-value-at-risk. <em>EJOR</em>, <em>327</em>(1), 232-246. (<a href='https://doi.org/10.1016/j.ejor.2025.04.038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores optimal insurance solutions based on the Lambda-Value-at-Risk ( Λ VaR ). Using the expected value premium principle, we first analyze a stop-loss indemnity and provide a closed-form expression for the deductible parameter. A necessary and sufficient condition for the existence of a positive and finite deductible is also established. We then generalize the stop-loss indemnity and show that, akin to the VaR model, a limited stop-loss indemnity remains optimal within the Λ VaR framework. Further, we examine the use of Λ ′ VaR as a premium principle and show that full or no insurance is optimal. We also identify that a limited loss indemnity is optimal when Λ ′ VaR is solely used to determine the risk-loading in the premium principle. Additionally, we investigate the impact of model uncertainty, particularly in scenarios where the loss distribution is unknown but lies within a specified uncertainty set. Our findings suggest that a limited stop-loss indemnity is optimal when the uncertainty set is defined using a likelihood ratio. Meanwhile, when only the first two moments of the loss distribution are available, we provide a closed-form expression for the optimal deductible in a stop-loss indemnity.},
  archive      = {J_EJOR},
  author       = {Tim J. Boonen and Yuyu Chen and Xia Han and Qiuqi Wang},
  doi          = {10.1016/j.ejor.2025.04.038},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {232-246},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal insurance design with lambda-value-at-risk},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Condition-based production: Maximizing manufacturing revenue considering failure risk and reject rates. <em>EJOR</em>, <em>327</em>(1), 218-231. (<a href='https://doi.org/10.1016/j.ejor.2025.04.051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing productivity in manufacturing is crucial for increasing output and reducing costs; however, it can also negatively impact product quality and accelerate system degradation. This study is the first to propose a method for dynamically adjusting productivity while considering both system degradation and product quality. We construct a dynamic programming model using optimal control theory to address both fixed maintenance cycles and the joint optimization of production and maintenance strategies. Our approach identifies optimal production strategies for various scenarios, showing that integrating product quality considerations with productivity and degradation significantly enhances overall outcomes. Extensive numerical studies validate our results, demonstrating that this comprehensive optimization scheme not only increases production system revenue but also reduces maintenance costs as well as product defects. By accounting for the dual impact of productivity on system degradation and product quality, this research provides a more holistic and practical strategy for maximizing manufacturing revenue and product reliability. The findings offer significant theoretical and practical value, guiding enterprises toward achieving a balance between high productivity, system longevity, and product quality.},
  archive      = {J_EJOR},
  author       = {Xiaolei Lv and Liangxing Shi and Yingdong He and Zhen He},
  doi          = {10.1016/j.ejor.2025.04.051},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {218-231},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Condition-based production: Maximizing manufacturing revenue considering failure risk and reject rates},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moral hazard in data envelopment analysis benchmarking. <em>EJOR</em>, <em>327</em>(1), 203-217. (<a href='https://doi.org/10.1016/j.ejor.2025.05.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper delves into the concept of moral hazard in data envelopment analysis (DEA) benchmarking. The moral hazard issue emerges when decision-making units (DMUs) conceal their actions in the application of best practices, driven by the costs involved and the possibility of incomplete reimbursement. This issue remains unexplored in DEA benchmarking because previous studies assume that applying best practices is straightforward once these practices have been identified. Therefore, we postulate the presence of information asymmetry pertaining to the optimal production possibilities of DMUs, and regard applying best practices in benchmarking as a moral hazard issue. To address this issue, we formulate an incentive game and propose efficient contracts, where DEA Russell-like measures are first employed to describe DMUs’ effort levels. We prove applying best practices is the dominate strategy equilibrium of the incentive game with the implementation of efficient contracts. By exploring moral hazard in DEA benchmarking, this paper recommends the managers to incorporate considerations of information asymmetry when embarking on benchmarking activities.},
  archive      = {J_EJOR},
  author       = {Xiangyang Tao and Qiaoyu Peng},
  doi          = {10.1016/j.ejor.2025.05.001},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {203-217},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Moral hazard in data envelopment analysis benchmarking},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Indiscriminate disruption of conditional inference on multivariate gaussians. <em>EJOR</em>, <em>327</em>(1), 191-202. (<a href='https://doi.org/10.1016/j.ejor.2025.06.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multivariate Gaussian distribution underpins myriad operations-research, decision-analytic, and machine-learning models (e.g., Bayesian optimization, Gaussian influence diagrams, and variational autoencoders). However, despite recent advances in adversarial machine learning (AML), inference for Gaussian models in the presence of an adversary is notably understudied. Therefore, we consider a self-interested attacker who wishes to disrupt a decisionmaker’s conditional inference and subsequent actions by corrupting a set of evidentiary variables. To avoid detection, the attacker also desires the attack to appear plausible wherein plausibility is determined by the density of the corrupted evidence. We consider white- and grey-box settings such that the attacker has complete and incomplete knowledge about the decisionmaker’s underlying multivariate Gaussian distribution, respectively. Select instances are shown to reduce to quadratic and stochastic quadratic programs, and structural properties are derived to inform solution methods. We assess the impact and efficacy of these attacks in three examples, including, a real estate evaluation application, an interest rate prediction task, and the use of linear Gaussian state space models. Each example leverages an alternative underlying model, thereby highlighting the attacks’ broad applicability. Through these applications, we also juxtapose the behavior of the white- and grey-box attacks to understand how uncertainty and structure affect attacker behavior.},
  archive      = {J_EJOR},
  author       = {William N. Caballero and Matthew LaRosa and Alexander A. Fisher and Vahid Tarokh},
  doi          = {10.1016/j.ejor.2025.06.011},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {191-202},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Indiscriminate disruption of conditional inference on multivariate gaussians},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-averse contextual predictive maintenance and operations scheduling with flexible generation under wind energy uncertainty. <em>EJOR</em>, <em>327</em>(1), 174-190. (<a href='https://doi.org/10.1016/j.ejor.2025.06.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring resiliency and sustainability of power systems operations under the uncertainty of the intermittent nature of renewables is becoming a critical concern while considering the integration of flexible generation resources that provide additional adjustability during planning. To address this emerging issue, this study proposes a risk-averse contextual predictive generator maintenance and operations scheduling problem with traditional and flexible generation resources under wind energy uncertainty. We formulate this problem as a two-stage risk-averse stochastic mixed-integer program, where the first-stage determines the maintenance and unit commitment related decisions of the traditional generation units, whereas the second-stage determines the corresponding decisions for flexible generators along with the production related plans of all generators. To integrate contextual information and the uncertainty around the wind power, we propose a Gaussian Process Regression approach for predicting wind power generation, which is then leveraged into this stochastic program. Since this problem is computationally challenging to solve with a mixed-integer recourse due to the second-stage decisions involving flexible generation resources, we provide two versions of a progressive hedging based solution algorithm by first utilizing the classical progressive hedging approach and then leveraging the Frank–Wolfe algorithm for improving the solution quality. In both versions, we extend these algorithms to the risk-averse setting and present various computational enhancements. Our results on the IEEE 118-bus instances demonstrate the impact of adopting a risk-averse approach compared to risk-neutral and deterministic alternatives with a better worst-case performance, and highlight the value of integrating flexible generation and contextual information with resilient maintenance and operations schedules leading to cost-effective plans with less component failures. Furthermore, our solution algorithms provide good quality solutions in significantly less time compared to the off-the-shelf solver, where the Frank–Wolfe version of the algorithm is capable of finding optimal solutions in majority of the test instances.},
  archive      = {J_EJOR},
  author       = {Natalie Randall and Beste Basciftci},
  doi          = {10.1016/j.ejor.2025.06.005},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {174-190},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Risk-averse contextual predictive maintenance and operations scheduling with flexible generation under wind energy uncertainty},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guaranteed bounds for optimal stopping problems using kernel-based non-asymptotic uniform confidence bands. <em>EJOR</em>, <em>327</em>(1), 162-173. (<a href='https://doi.org/10.1016/j.ejor.2025.05.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce an approach for obtaining probabilistically guaranteed upper and lower bounds on the true optimal value of stopping problems. Bounds of existing simulation-and-regression approaches, such as those based on least squares Monte Carlo and information relaxation, are stochastic in nature and therefore do not come with a finite sample guarantee. Our data-driven approach is fundamentally different as it allows replacing the sampling error with a pre-specified confidence level. The key to this approach is to use high- and low-biased estimates that are guaranteed to over- and underestimate, respectively, the conditional expected continuation value that appears in the stopping problem’s dynamic programming formulation with a pre-specified confidence level. By incorporating these guaranteed over- and underestimates into a backward recursive procedure, we obtain probabilistically guaranteed bounds on the problem’s true optimal value. As a byproduct we present novel kernel-based non-asymptotic uniform confidence bands for regression functions from a reproducing kernel Hilbert space. We derive closed-form formulas for the cases where the data-generating distribution is either known or unknown, which makes our data-driven approach readily applicable in a range of practical situations including simulation. We illustrate the applicability of the proposed bounding procedure by valuing a Bermudan put option.},
  archive      = {J_EJOR},
  author       = {Martin Glanzer and Sebastian Maier and Georg Ch. Pflug},
  doi          = {10.1016/j.ejor.2025.05.028},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {162-173},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Guaranteed bounds for optimal stopping problems using kernel-based non-asymptotic uniform confidence bands},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How live-streaming commerce interacts with two-period pricing when selling to strategic customers. <em>EJOR</em>, <em>327</em>(1), 148-161. (<a href='https://doi.org/10.1016/j.ejor.2025.05.042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live-streaming commerce serves as a novel marketing tool in the digital era, facilitating real-time interactions between streamers and customers to enhance product sales. However, the transition back to solely traditional online channels following live-stream events presents decision-making challenges for firms regarding dynamic pricing, price commitment, and price matching schemes, particularly when dealing with strategic customers. We develop a two-period theoretical model to explore the selection of pricing schemes by a firm who may launch a live-streaming sales. Our analysis reveals that while live-streaming cannot necessarily enhance profitability under price commitment scheme, it significantly boosts profits under dynamic pricing and price matching schemes due to amplified pricing flexibility. The increased customer patience in strategic waiting diminishes the benefits of live-streaming under dynamic pricing, whereas it enhances them under price matching with reimbursement mechanisms. Moreover, dynamic pricing and price matching schemes, traditionally perceived as less favorable, emerge as potentially more effective due to the real-time value enhancement effect of live-streaming commerce, which can mitigate customers’ strategic delay in purchasing decisions. The firm’s selection of an optimal pricing scheme under live-streaming commerce can also maximize consumer surplus, thereby enhancing social welfare and promoting a positive image of corporate social responsibility advocated by governmental bodies. This “win-win-win” scenario for the firm, customers, and government can be achieved through dynamic pricing or price commitment schemes. In this context, a conducive social environment can be fostered, enabling all participants to collaborate in expanding opportunities and benefiting everyone.},
  archive      = {J_EJOR},
  author       = {Pingping Chen and Lei Xie and Lu Dai and Zhuzhu Song},
  doi          = {10.1016/j.ejor.2025.05.042},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {148-161},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {How live-streaming commerce interacts with two-period pricing when selling to strategic customers},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sourcing and supplying strategies under supply risk of critical components. <em>EJOR</em>, <em>327</em>(1), 136-147. (<a href='https://doi.org/10.1016/j.ejor.2025.05.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trade wars and geopolitical conflicts have intensified the supply risks associated with high-quality critical components in cross-border high-tech supply chains. In response, companies have restructured their supply networks by adopting multiple sourcing strategies and shifting from global to regional models. Consequently, domestic manufacturers reliant on overseas procurement may turn to low-quality domestic suppliers, while foreign suppliers may increase their provision of reliable medium-quality components to maintain market share. This paper examines the strategic supply decisions of a foreign supplier and the procurement strategies of a domestic manufacturer in a competitive market. Our findings reveal that these strategies are shaped by the interplay between diversification and competition effects. When the diversification effect outweighs the competition effect, the foreign supplier finds it profitable to increase the supply of medium-quality components. Conversely, if the competition effect is stronger, the foreign supplier refrains from doing so. It is beneficial for the domestic manufacturer to source low-end critical components, with the two effects becoming more pronounced when both low- and medium-quality critical components are available. However, when the foreign supplier expands its supply of medium-quality components, the foreign manufacturer faces adverse effects. Offering medium-quality products can erode the market share of high-quality products, whereas providing low-quality products may not have the same effect. Furthermore, we extend the basic model to incorporate demand uncertainty and demonstrate the robustness of our main findings.},
  archive      = {J_EJOR},
  author       = {Jie Cui and Jingming Pan and Zhiyi Song},
  doi          = {10.1016/j.ejor.2025.05.010},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {136-147},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Sourcing and supplying strategies under supply risk of critical components},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust optimization of a procurement and routing strategy for multiperiod multimodal transport in an uncertain environment. <em>EJOR</em>, <em>327</em>(1), 115-135. (<a href='https://doi.org/10.1016/j.ejor.2025.05.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a collaborative optimization strategy for multiperiod procurement and multimodal transportation that considers cost factors such as procurement, transportation, transshipment, and storage costs incurred for early arrival. A mixed-integer planning model is established to minimize the overall operating costs of cross-border e-commerce enterprises by arranging procurement, transportation, and storage strategies. Considering the fluctuation of procurement costs with the market environment, this study constructs robust optimization models and develops linear robust equivalence models through mathematical transformation to improve the efficiency of problem solving. A hybrid heuristic algorithm, KIGALNS, is proposed to solve this problem. Finally, a series of numerical experiments are conducted to show that our robust model can better address multimodal transportation path optimization problems such as procurement cost uncertainty. In addition, the correctness of the proposed model and the effectiveness of the algorithm and collaborative optimization strategy were verified. Finally, the case analysis shows that the early procurement strategy helps reduce total operating costs, and the robust model can effectively handle multimodal transportation path optimization problems such as uncertain procurement costs. While promoting cost reduction and efficiency improvement in transportation, the proposed approach comprehensively considers the impact of procurement plans and uncertain factors, providing theoretical guidance and scientific solutions for joint decision-making in enterprise procurement transportation.},
  archive      = {J_EJOR},
  author       = {Fang Guo and Jingfu Liang and Runliu Niu and Zhihong Huang and Qixuan Liu},
  doi          = {10.1016/j.ejor.2025.05.004},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {115-135},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust optimization of a procurement and routing strategy for multiperiod multimodal transport in an uncertain environment},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-stage approach for root sequence index allocation. <em>EJOR</em>, <em>327</em>(1), 95-114. (<a href='https://doi.org/10.1016/j.ejor.2025.05.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Root Sequence Index (RSI) is a parameter used in mobile wireless networks to allocate uplink channels between user equipment and base stations. Inadequate RSI assignment to neighbor radios may lead to failure in service establishment and performance degradation. Wireless networks are also dynamic, with uncertain modifications in time due to, for instance, seasonal foliage. In this paper, we model RSI allocation with seasonal changes in the network as a multistage robust problem, being the first proactive, look-ahead method to consider uncertainty in RSI assignments. We develop methods to solve this stochastic problem, aiming to minimize the possible interference and network changes in time. A mixed-integer programming model, a classic Biased Random-Key Genetic Algorithm (BRKGA), and a novel BRKGA hybridized with Dijkstra’s algorithm are explored and compared. We also introduce a Monte Carlo-based simulation methodology to obtain scenarios. The hybrid BRKGA-based approach is shown to obtain more robust solutions in shorter computational times.},
  archive      = {J_EJOR},
  author       = {Mariana A. Londe and Carlos E. Andrade and Luciana S. Pessoa},
  doi          = {10.1016/j.ejor.2025.05.015},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {95-114},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A multi-stage approach for root sequence index allocation},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterated greedy for the yard crane scheduling problem with input/output assignment. <em>EJOR</em>, <em>327</em>(1), 84-94. (<a href='https://doi.org/10.1016/j.ejor.2025.04.052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The yard crane scheduling problem (YCSP) consists of optimizing container loading for storage and retrieval requests from yard cranes at port terminals. This paper studies a realistic generalization of the YCSP that incorporates the assignments of input/output (I/O) points during the optimization stage. I/O points serve as buffers between the different transportation modes in the port terminal. These are limited in number and unproductive idle times might result if a container schedule exhausts I/O point availability. The resulting problem entails not only scheduling container storage and retrieval requests, but also the assignment of the I/O points. We introduce a series of simple, yet powerful, Iterated Greedy (IG) methods. These include variations of the destruction and reconstruction operators, coordination with novel local search procedures and problem-specific knowledge speed-ups. The proposed IG methods are carefully calibrated and evaluated using comprehensive computational experiments. The results indicate that small changes in the features of the algorithm have a profound impact on performance. Comparisons against the state-of-the-art approaches for this particular problem result in a strong, and statistically significant performance advantage for the proposed IG procedures.},
  archive      = {J_EJOR},
  author       = {Hongtao Wang and Rubén Ruiz and Fulgencia Villa and Eva Vallada},
  doi          = {10.1016/j.ejor.2025.04.052},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {84-94},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Iterated greedy for the yard crane scheduling problem with input/output assignment},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solution-hashing search based on layout-graph transformation for unequal circle packing. <em>EJOR</em>, <em>327</em>(1), 58-83. (<a href='https://doi.org/10.1016/j.ejor.2025.05.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of packing unequal circles into a circular container is a classic and challenging optimization problem in the field of computational geometry. This study introduces a suite of innovative and efficient methods to tackle this problem. Firstly, we present a novel layout-graph transformation method that represents configurations as graphs, together with an inexact hash method facilitating fast comparison of configurations on isomorphism or similarity. Leveraging these advancements, we propose an iterative solution-hashing search algorithm adept at circumventing redundant exploration through efficient configuration recording. Additionally, we introduce several enhancements to refine the optimization and search processes, including an adaptive adjacency maintenance method, an efficient vacancy detection technique, and a Voronoi-based locating method. Our algorithm demonstrates excellent performance over existing state-of-the-art methods through comprehensive computational experiments across various benchmark instances, showcasing quality applicability and versatility. Notably, our algorithm improves the best-known results for 116 out of 239 benchmark instances while achieving parity with the remaining instances.},
  archive      = {J_EJOR},
  author       = {Jianrong Zhou and Jiyao He and Kun He},
  doi          = {10.1016/j.ejor.2025.05.003},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {58-83},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Solution-hashing search based on layout-graph transformation for unequal circle packing},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formulations and branch-and-cut algorithms for cycle covers with up to p cycles. <em>EJOR</em>, <em>327</em>(1), 42-57. (<a href='https://doi.org/10.1016/j.ejor.2025.04.047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a positive integer p and a weighted undirected graph G = ( V , E ) , we study a problem in which the objective is to find a minimum weight set of up to p elementary cycles partitioning the vertices of G . We study several exponentially sized formulations including (i) edge variables only; (ii) edge and depot variables only; (iii) edge, depot and node-depot assignment (NDA) variables only; (iv) edge, depot, NDA and edge-depot assignment (EDA) variables. New flow formulations are also introduced, and relations between all the formulations are established. Branch-and-cut algorithms based on many of these formulations are proposed, and computational experiments are conducted to compare the performance of the different algorithms. The computational testing reveals that some of the formulations including edge, depot and NDA or EDA variables produce the best initial lower bounds and that the best computational times are obtained with the algorithms based on formulations including edge and depot variables only. The best performing algorithm (in terms of computational times) is capable of solving several instances with up to 442 nodes for different values of p .},
  archive      = {J_EJOR},
  author       = {Francisco Canas and Luís Gouveia},
  doi          = {10.1016/j.ejor.2025.04.047},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {42-57},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Formulations and branch-and-cut algorithms for cycle covers with up to p cycles},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective evolutionary algorithm for packing rectangles into a fixed size circular container. <em>EJOR</em>, <em>327</em>(1), 22-41. (<a href='https://doi.org/10.1016/j.ejor.2025.04.044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the general problem of orthogonally packing rectangles in a fixed size circular container. This is a computationally challenging combinatorial optimization problem with important real-world applications and has recently received much attention from the operations research community. We propose an effective evolutionary algorithm for four variants of the problem, which integrates an improved decoding procedure and several dedicated search operators for population initialization and new solution generation. Computational results on 108 popular benchmark instances show that the proposed algorithm advances the state of the art in practically solving these four variants of the problem by finding 53 new best solutions (26 for the variants of maximizing the area of the packed items and 27 for the variants of maximizing the number of the packed items). We perform experiments to verify the design of key algorithmic components.},
  archive      = {J_EJOR},
  author       = {Xiangjing Lai and Lei Wang and Jin-Kao Hao and Qinghua Wu},
  doi          = {10.1016/j.ejor.2025.04.044},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {22-41},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An effective evolutionary algorithm for packing rectangles into a fixed size circular container},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years at the interface between financial modeling and operations research. <em>EJOR</em>, <em>327</em>(1), 1-21. (<a href='https://doi.org/10.1016/j.ejor.2025.01.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last fifty years, there has been an increasing intersection of methodologies, applications, and contributions at the frontier of finance and operations research. This invited paper selectively reviews this literature, aiming to provide a building block for future research at the intersection between the two fields. Our review revolves around four main themes: option pricing, interest rate and credit risk modeling, investment strategies, and financial econometrics. The review explores possible avenues for future research, particularly related to machine learning, high-dimensional statistics and a renewed behavioral approach.},
  archive      = {J_EJOR},
  author       = {Frank J. Fabozzi and Maria Cristina Recchioni and Roberto Renò},
  doi          = {10.1016/j.ejor.2025.01.001},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years at the interface between financial modeling and operations research},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The cost of uninformed market timing. <em>EJOR</em>, <em>326</em>(3), 724-731. (<a href='https://doi.org/10.1016/j.ejor.2025.05.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investment board meetings typically include a macroeconomic review, and a discussion of the implications for asset allocation. Investors who are able to time the market can no-doubt obtain abnormal returns, but what is the cost for investors who attempt to time the market but have no genuine timing ability? We prove that for virtually any uninformed timing strategy there is a constant-allocation strategy that dominates it by First-degree Stochastic Dominance. Thus, constant allocation is superior not only for risk-averters, but for all investors with non-decreasing preferences, including Prospect Theory investors and investors with various aspiration levels. The cost of uninformed market timing is shown to be almost double than previous estimates, at about 2 % per year.},
  archive      = {J_EJOR},
  author       = {Moshe Levy},
  doi          = {10.1016/j.ejor.2025.05.014},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {724-731},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The cost of uninformed market timing},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated framework to improve the resiliency of electricity distribution systems exposed to wildfires. <em>EJOR</em>, <em>326</em>(3), 707-723. (<a href='https://doi.org/10.1016/j.ejor.2025.04.035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate modeling of the complex and unique interaction between the electricity distribution systems and wildfires is crucial for mitigating their devastating consequences. In this study, we develop an optimization framework for designing strategic wildfire prevention policies that involve preemptive practices, such as electricity infrastructure hardening and public safety power shutoffs. Unlike existing studies that consider the pre- and post-wildfire event decisions separately, our approach captures in a unified framework the interaction between the preemptive strategic actions, the wildfire propagation, and the post-event operational decisions, such as the microgrid formation and the electricity distribution policies. To identify resilient strategies, we propose a worst-case-analysis approach that leverages a tri-level interdiction model focused on mitigating the worst possible disruption an uncontrolled wildfire can cause. To demonstrate the benefits of our framework, we conduct a case study based on the IEEE 14 bus and IEEE 30 bus systems, testing their performance under the proposed prevention policies for different initial settings and risk scenarios. We observe that the hardening strategies are fundamental for minimizing the unserved demand and the detrimental effect on the electricity infrastructure inflicted by wildfires. Furthermore, our results provide evidence that public safety power shutoffs are particularly beneficial in scenarios where the hardening budget is low. Similarly, we note that microgrids formed around distributed generators significantly improve the resiliency of electricity distribution systems in post-disaster scenarios.},
  archive      = {J_EJOR},
  author       = {Prasangsha Ganguly and Sayanti Mukherjee and Jose L. Walteros and Luis Herrera},
  doi          = {10.1016/j.ejor.2025.04.035},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {707-723},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An integrated framework to improve the resiliency of electricity distribution systems exposed to wildfires},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleashing the power of text for credit default prediction: Comparing human-written and generative AI-refined texts. <em>EJOR</em>, <em>326</em>(3), 691-706. (<a href='https://doi.org/10.1016/j.ejor.2025.04.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the integration of a representative large language model, ChatGPT, into lending decision-making with a focus on credit default prediction. Specifically, we use ChatGPT to analyse and interpret loan assessments written by loan officers and generate refined versions of these texts. Our comparative analysis reveals significant differences between generative artificial intelligence (AI)-refined and human-written texts in terms of text length, semantic similarity, and linguistic representations. Using deep learning techniques, we show that incorporating unstructured text data, particularly ChatGPT-refined texts, alongside conventional structured data significantly enhances credit default predictions. Furthermore, we demonstrate how the contents of both human-written and ChatGPT-refined assessments contribute to the models’ prediction and show that the effect of essential words is highly context-dependent. Moreover, we find that ChatGPT’s analysis of borrower delinquency contributes the most to improving predictive accuracy. We also evaluate the business impact of the models based on human-written and ChatGPT-refined texts, and find that, in most cases, the latter yields higher profitability than the former. This study provides valuable insights into the transformative potential of generative AI in financial services.},
  archive      = {J_EJOR},
  author       = {Zongxiao Wu and Yizhe Dong and Yaoyiran Li and Baofeng Shi},
  doi          = {10.1016/j.ejor.2025.04.032},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {691-706},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Unleashing the power of text for credit default prediction: Comparing human-written and generative AI-refined texts},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evacuation network design under road capacity improvement and uncertainty: Second-order cone programming reformulations and benders decomposition. <em>EJOR</em>, <em>326</em>(3), 674-690. (<a href='https://doi.org/10.1016/j.ejor.2025.04.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work first presents a stochastic shelter location and evacuation planning problem with considering road capacity improvement strategies, in which the fixed setup cost of shelters and the improvement cost of road capacity are subject to a budget limit. To explicitly capture the impact of traffic volumes and road capacity improvement decisions on evacuation time, the Bureau of Public Roads function is employed. The problem is formulated as a non-convex mixed-integer nonlinear program (MINLP) model that is difficult to solve directly since the objective function is a multivariable non-convex nonlinear function. To tackle the non-convex MINLP, second-order cone programming (SOCP) reformulations that can be directly solved by using the state-of-the-art solvers are developed. Furthermore, a Benders decomposition (BD) approach that utilizes duality results of SOCP and employs acceleration strategies associated with valid inequalities, multi-cut, strengthened Benders cuts, knapsack inequalities, and callback routine, is proposed to solve large-scale problems. Moreover, extensive numerical experiments and a real-world case study (a potential hurricane risk zone in Texas, U.S.) are conducted to verify the applicability and effectiveness of the proposed model and solution approaches. Computational results show that the derived reformulations are competitive in dealing with small- and medium-scale problems, whereas BD approach demonstrates the best computational performance in solving large-scale problems. The devised acceleration strategies are effective in improving the computational efficiency of the BD approach. In addition, exerting investment for those shelters and arcs that are close to evacuation regions is useful to reduce the expected total evacuation time.},
  archive      = {J_EJOR},
  author       = {Qing-Mi Hu and Shaolong Hu and Zhijie Sasha Dong and Yongjia Song},
  doi          = {10.1016/j.ejor.2025.04.030},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {674-690},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Evacuation network design under road capacity improvement and uncertainty: Second-order cone programming reformulations and benders decomposition},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal capital structure with earnings above a floor. <em>EJOR</em>, <em>326</em>(3), 656-673. (<a href='https://doi.org/10.1016/j.ejor.2025.04.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper derives the optimal capital structure of a firm whose earnings follow a geometric Brownian motion with a lower reflecting barrier. The barrier can be interpreted as a market intervention threshold (e.g., a price floor) by the government or an exit threshold of weak competitors in the market. Unlike in the standard model with no barrier, the firm is able to issue riskless debt to a certain capacity determined by the barrier. The higher the barrier, the larger the riskless debt capacity, and the firm prefers riskless capital structure rather than risky capital structure. Notably, with intermediate barrier levels, the firm can choose riskless capital structure with lower leverage than the level with no barrier. This mechanism can help explain debt conservatism observed in practice. The paper also entails several implications of public intervention by examining the lowest barrier (i.e., the weakest intervention) to achieve riskless capital structure.},
  archive      = {J_EJOR},
  author       = {Michi Nishihara and Takashi Shibata},
  doi          = {10.1016/j.ejor.2025.04.023},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {656-673},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal capital structure with earnings above a floor},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal sequential stochastic shortest path interdiction. <em>EJOR</em>, <em>326</em>(3), 641-655. (<a href='https://doi.org/10.1016/j.ejor.2025.04.009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the periodic interaction between a leader and a follower in the context of network interdiction where, in each period, the leader first blocks (momentarily) passage through a subset of arcs in a network, and then the follower traverses the shortest path in the interdicted network. We assume that arc costs are stochastic and that while their underlying distribution is known to the follower, it is not known by the leader. We cast the problem of the leader, who aims at maximizing the cumulative cost incurred by the evader, using the multi-armed bandit framework. Such a setting differs from the traditional bandit in that the feedback elicited by playing an arm is the reaction of an adversarial agent. After developing a fundamental limit in the achievable performance by any admissible policy, we adapt traditional policies developed for linear bandits to our setting. We show that a critical step in such an adaptation is to ensure that the cost vectors imputed by these algorithms lie within a polyhedron characterizing information that can be collected without noise and in finite time. Within such a polyhedron, the problem can be mapped into a linear bandit. The polyhedron has exponentially many constraints in the worst case, which are indirectly tackled by solving several mathematical programs. We test the proposed policies and relevant benchmarks through a set of numerical experiments. Our results show that the adapted policies can significantly outperform the performance of the base policies at the price of increasing their computational complexity.},
  archive      = {J_EJOR},
  author       = {Juan S. Borrero and Denis Sauré and Natalia Trigo},
  doi          = {10.1016/j.ejor.2025.04.009},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {641-655},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal sequential stochastic shortest path interdiction},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the stability of model explanations in instance-dependent cost-sensitive credit scoring. <em>EJOR</em>, <em>326</em>(3), 630-640. (<a href='https://doi.org/10.1016/j.ejor.2025.05.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance-dependent cost-sensitive (IDCS) classifiers offer a promising approach to improving cost-efficiency in credit scoring by tailoring loss functions to instance-specific costs. However, the impact of such loss functions on the stability of model explanations remains unexplored in literature, despite increasing regulatory demands for transparency. This study addresses this gap by evaluating the stability of Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) when applied to IDCS models. Using four publicly available credit scoring datasets, we first assess the discriminatory power and cost-efficiency of IDCS classifiers, introducing a novel metric to enhance cross-dataset comparability. We then investigate the stability of SHAP and LIME feature importance rankings under varying degrees of class imbalance through controlled resampling. Our results reveal that while IDCS classifiers improve cost-efficiency, they produce significantly less stable explanations compared to traditional models, particularly as class imbalance increases, highlighting a critical trade-off between cost optimization and interpretability in credit scoring. Amid increasing regulatory scrutiny on explainability, this research underscores the pressing need to address stability issues in IDCS classifiers to ensure that their cost advantages are not undermined by unstable or untrustworthy explanations.},
  archive      = {J_EJOR},
  author       = {Matteo Ballegeer and Matthias Bogaert and Dries F. Benoit},
  doi          = {10.1016/j.ejor.2025.05.039},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {630-640},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Evaluating the stability of model explanations in instance-dependent cost-sensitive credit scoring},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning method for optimal investment under relative performance criteria among heterogeneous agents. <em>EJOR</em>, <em>326</em>(3), 615-629. (<a href='https://doi.org/10.1016/j.ejor.2025.04.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphon games have been introduced to study games with many players who interact through a weighted graph of interaction. By passing to the limit, a game with a continuum of players is obtained, in which the interactions are through a graphon. In this paper, we focus on a graphon game for optimal investment under relative performance criteria, and we propose a deep learning method. The method builds upon two key ingredients: first, a characterization of Nash equilibria by forward–backward stochastic differential equations and, second, recent advances of machine learning algorithms for stochastic differential games. We provide numerical experiments on two different financial models. In each model, we compare the effect of several graphons, which correspond to different structures of interactions.},
  archive      = {J_EJOR},
  author       = {Mathieu Laurière and Ludovic Tangpi and Xuchen Zhou},
  doi          = {10.1016/j.ejor.2025.04.018},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {615-629},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A deep learning method for optimal investment under relative performance criteria among heterogeneous agents},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel structures and subscription strategies for AI-driven logistics data products. <em>EJOR</em>, <em>326</em>(3), 597-614. (<a href='https://doi.org/10.1016/j.ejor.2025.04.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the application of large models in artificial intelligence (AI), this paper proposes a new business model for AI-driven data product transactions in the freight market. We develop a game-theoretic model for the logistics data supply chain comprising a logistics data provider and a logistics data integrator. Observing the opportunity for the logistics data provider to directly sell AI-driven data products to consumers and supply data sets to the logistics data integrator, we explore two channel structures: a single-channel structure and a dual-channel structure. Furthermore, the logistics data provider can choose whether or not to subscribe to the value-added services provided by Cyber–Physical Internet (CPI), which enhance data product quality but also incur additional costs. This study presents the following results. First, our findings debunk the prevailing belief about product quality strategy that improving data product quality instead impairs the profit when targeting a high licensing rate and a large number of affluent consumers. Second, a dual-channel structure is only viable if the licensing rate is sufficiently high or the market is dominated by budget-conscious consumers, otherwise a single-channel structure is a superior choice. Third, subscribing to the value-added services provided by CPI, even when free, may not benefit the logistics data provider due to the spillover effect in a dual-channel structure. Managerial implications enable logistics data providers to achieve greater economic efficiency under various market conditions by adopting suitable channel structures and leveraging value-added services and pricing tools, thereby promoting AI-driven data product transactions.},
  archive      = {J_EJOR},
  author       = {Shulin He and Mengdi Zhang and Shuaian Wang and George Q. Huang},
  doi          = {10.1016/j.ejor.2025.04.003},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {597-614},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Channel structures and subscription strategies for AI-driven logistics data products},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Infeasibility conditions and resolution strategies for super-efficiency models under weak disposability and null-jointness: A directional distance function approach with endogenous directions. <em>EJOR</em>, <em>326</em>(3), 585-596. (<a href='https://doi.org/10.1016/j.ejor.2025.04.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies have not focused on the infeasibility of super-efficiency models under the weak disposability and null-jointness (WDJ) assumptions, despite the wide adoption of these two conditions in fields where undesirable outputs exist, like the evaluation of energy and environmental efficiency. This paper employs a directional distance function (DDF) approach to investigate super-efficiency feasibility under these assumptions. We note that DDF-based super-efficiency models using frequently-used exogenous directions may encounter infeasibility under the WDJ assumptions, even when constant returns to scale (CRS) are assumed. We present the specific conditions that lead to this infeasibility. By utilizing endogenous directions, we construct a feasible DDF-based CRS super-efficiency model under the WDJ assumptions, ensuring that the DDF super-efficiency scores remain within a maximum of 1. We also find that the DDF-based super-efficiency model under the WDJ assumptions is infeasible in certain cases when variable returns to scale (VRS) are assumed, regardless of whether directions are endogenous or exogenous. To address this issue, we propose a modified DDF-based VRS super-efficiency model that aims to maintain the WDJ assumptions as much as possible. This VRS model ensures feasibility and generates DDF super-efficiency scores below 1. Some properties of the models and their relationships are discussed. Finally, several examples and a real case from the literature validate the feasibility and practical applicability of the proposed models.},
  archive      = {J_EJOR},
  author       = {Ruiyue Lin and Zongxin Li},
  doi          = {10.1016/j.ejor.2025.04.039},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {585-596},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Infeasibility conditions and resolution strategies for super-efficiency models under weak disposability and null-jointness: A directional distance function approach with endogenous directions},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Planning methods using data envelopment analysis and markov systems. <em>EJOR</em>, <em>326</em>(3), 569-584. (<a href='https://doi.org/10.1016/j.ejor.2025.04.050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the extension of a modelling framework that integrates data envelopment analysis (DEA) and markov systems, into a two-stage setting. In a recent paper in EJOR, a single-stage DEA-markov hybrid model was introduced, establishing a research direction blending these seemingly distinct approaches to address the attainability problem in workforce planning. Markov systems are widely used in scenarios where a population system (e.g., staff profiles, patients with chronic conditions) begins the planning horizon in a specific state and aims to transition to a new state by the end of the horizon. Although it is common for this horizon to encompass multiple steps, this hybrid model considered attainability within a single-step horizon. In the current study, we investigate problems in two phases and integrate a network DEA approach with markovian population systems under various assumptions, resulting into new variations of the relevant models. The decision maker (DM) can specify potential future outcomes (e.g., personnel flows) in consecutive steps in time, and use DEA to identify feasible courses of action through convexity (or even use the second stage in a normative manner to identify optimal flows). The two-stage DEA model captures the DM’s relative preferences for future states and provides measures of efficacy of potential flows relative to the ultimate desired state. Consequently, the organization can plan interventions to enhance the probability of achieving some anticipated goal. The paper includes illustrations using data from workforce planning and concludes with a discussion on relevant issues in healthcare, circular economy and social radicalization.},
  archive      = {J_EJOR},
  author       = {Andreas C. Georgiou and Georgios Tsaples and Emmanuel Thanassoulis},
  doi          = {10.1016/j.ejor.2025.04.050},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {569-584},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Planning methods using data envelopment analysis and markov systems},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust portfolio optimization meets arbitrage pricing theory. <em>EJOR</em>, <em>326</em>(3), 558-568. (<a href='https://doi.org/10.1016/j.ejor.2025.04.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust portfolio optimization models are crucial for mitigating the impact of significant forecasting errors on expected asset returns. However, despite their significance, existing approaches often overlook a fundamental characteristic of financial markets: the absence of arbitrage opportunities. This paper presents a novel portfolio optimization model that integrates the classical mean–variance approach, the Fama and French Factor Model, and the Arbitrage Pricing Theory within a robust optimization framework. The proposed model utilizes return statistics to shape the uncertainty set boundaries but further enhances its representation by explicitly incorporating the no-arbitrage condition. The resulting formulation is non-convex and can be viewed as a trilevel optimization problem. To address these challenges, a cutting-plane algorithm is presented. Numerical experiments on multiple datasets and under various transaction cost levels confirm consistent outperformance over benchmark models in terms of cumulative returns and risk-adjusted metrics.},
  archive      = {J_EJOR},
  author       = {Mateus Waga and Davi Valladão and Alexandre Street},
  doi          = {10.1016/j.ejor.2025.04.004},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {558-568},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust portfolio optimization meets arbitrage pricing theory},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eliminating conflicts in group decision-making: Exploring potential information cocoon effects across varied levels of psychological resilience. <em>EJOR</em>, <em>326</em>(3), 544-557. (<a href='https://doi.org/10.1016/j.ejor.2025.04.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In group decision-making (GDM), conflicts often arise, requiring decision-makers (DMs) to adjust their opinions. Variations in DMs’ backgrounds, expertise, and dynamic environmental interactions shape their psychological states, consequently affecting their information-processing strategies and potentially contributing to information cocoon effects. This study aims to develop a conflict-elimination framework that (1) evaluates DMs’ psychological states, (2) identifies the dual nature of information cocoon effects (ICEs) shaping their behaviors, and (3) formulates targeted conflict resolution strategies based on these insights. First, we employ a resilience model to quantify psychological resilience as an indicator of DMs’ psychological states. For highly resilient DMs — less susceptible to ICEs — a tailored conflict elimination strategy using a bi-level optimization model is introduced. For less resilient DMs — more prone to cocoon influences — we examine the conditions under which ICEs can obstruct or facilitate conflict resolution. We then design corresponding optimization models to harness these effects constructively. A numerical demonstration and sensitivity analysis confirm the proposed framework’s effectiveness. Our approach enhances decision-making efficiency and improves conflict resolution outcomes by aligning resolution strategies with DMs’ psychological states and the nature of their ICEs.},
  archive      = {J_EJOR},
  author       = {Siqi Zhang and Jianjun Zhu},
  doi          = {10.1016/j.ejor.2025.04.028},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {544-557},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Eliminating conflicts in group decision-making: Exploring potential information cocoon effects across varied levels of psychological resilience},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Valuation of power purchase agreements for corporate renewable energy procurement. <em>EJOR</em>, <em>326</em>(3), 530-543. (<a href='https://doi.org/10.1016/j.ejor.2025.05.054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corporate renewable power purchase agreements (PPAs) are long-term contracts that enable companies to source renewable energy without having to develop and operate their own capacities. Typically, producers and consumers agree on a fixed per-unit price at which power is purchased. The value of the PPA to the buyer depends on the so called capture price defined as the difference between this fixed price and the market value of the produced volume during the duration of the contract. To model the capture price, practitioners often use either fundamental or statistical approaches to model future market prices, which both have their inherent limitations. We propose a new approach that blends the logic of fundamental electricity market models with statistical learning techniques. In particular, we use regularized inverse optimization in a quadratic fundamental bottom-up model of the power market to estimate the marginal costs of different technologies as a parametric function of exogenous factors. We compare the out-of-sample performance in forecasting the capture price using market data from three European countries and demonstrate that our approach outperforms established statistical learning benchmarks. We then discuss the case of a photovoltaic plant in Spain to illustrate how to use the model to value a PPA from the buyer’s perspective.},
  archive      = {J_EJOR},
  author       = {Roozbeh Qorbanian and Nils Löhndorf and David Wozabal},
  doi          = {10.1016/j.ejor.2025.05.054},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {530-543},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Valuation of power purchase agreements for corporate renewable energy procurement},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive sequential selection procedures for optimal quantile with control variates. <em>EJOR</em>, <em>326</em>(3), 515-529. (<a href='https://doi.org/10.1016/j.ejor.2025.05.049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces adaptive sequential selection procedures leveraging control variate quantile estimators for efficient quantile-based ranking and selection in simulation studies. Two variations are proposed: one simplifies estimation using binary control variates, and the other employs a discrete approximation to derive a post-stratified control variate quantile estimator. Theoretical analysis establishes the asymptotic validity and efficiency of these methods, including a novel central limit theorem for the post-stratified estimator. Numerical experiments on normal distributions and a basic queueing problem demonstrate the superior performance and adaptability of the proposed procedures. This work advances the integration of variance reduction techniques into quantile-based ranking-and-selection procedures, providing a robust framework for practical applications.},
  archive      = {J_EJOR},
  author       = {Shing Chih Tsai and Guangxin Jiang},
  doi          = {10.1016/j.ejor.2025.05.049},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {515-529},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Adaptive sequential selection procedures for optimal quantile with control variates},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic appointment rescheduling with patient preferences. <em>EJOR</em>, <em>326</em>(3), 498-514. (<a href='https://doi.org/10.1016/j.ejor.2025.05.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines patient-initiated appointment rescheduling with consideration of patient preferences. Online rescheduling policies are investigated for the selection and sequential offering of new appointments upon the arrival of a rescheduling request via a telephone call. Appointments are offered until the patient accepts one or the maximum number of offers is reached. The aim is to reschedule appointments using a weighted function to maximise the patients’ satisfaction, optimise the operational performance, and minimise the number of patients deferred to a future time horizon. Different patient types are taken into account characterised by their uncertainties in rescheduling, cancellation, no-show, and service duration. The rescheduling process is formulated as a stochastic dynamic scheduling problem and approximated using a Markov Decision Process (MDP). Two heuristic policies are proposed, referred to as the myopic stochastic and the MDP-based algorithms. Both policies apply a simulation-optimisation approach that considers patient preferences and expected operational performance. To determine the set of offered appointments, the MDP-based algorithm additionally accounts for expected future rescheduling requests. Computational experiments are performed on real-life instances. The results demonstrate that the two proposed policies yield solutions of high quality. The myopic stochastic policy outperforms the MDP-based policy when it is challenging to offer suitable slots due to high capacity utilisation or a lack of clear patient preferences. Conversely, the MDP-based algorithm delivers better results when capacity utilisation is lower and there is some variation in preferences across days and patients.},
  archive      = {J_EJOR},
  author       = {Tine Meersman and Broos Maenhout and Dieter Fiems},
  doi          = {10.1016/j.ejor.2025.05.005},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {498-514},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Dynamic appointment rescheduling with patient preferences},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does carrier collaboration require combinatorial auctions?. <em>EJOR</em>, <em>326</em>(3), 481-497. (<a href='https://doi.org/10.1016/j.ejor.2025.04.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carrier collaboration has emerged as an important way to increase the efficiency of the logistics sector and has attracted significant interest as a research topic in the past decades. Most frameworks for carrier collaboration rely on combinatorial auctions to allocate bundles of transportation requests to carriers. In this paper, we analyze whether combinatorial auctions are indeed needed for this purpose, or additive pricing schemes, in which requests are priced and allocated separately, could also lead to efficient allocations. Using additive pricing would considerably simplify the entire process. Starting from concepts of economic equilibria, we argue that theory does not rule out this possibility even for complex valuations. We then analyze how often additive pricing schemes can support an efficient allocation of requests by simulating capacity-constrained Traveling Salesperson Problems. Our results indicate that, depending on the geographical configuration of requests, this is indeed possible in a large fraction of problems. Thus we argue that the current focus of carrier collaboration research on bundle generation and combinatorial auctions needs to be reconsidered.},
  archive      = {J_EJOR},
  author       = {Rudolf Vetschera and Dmitriy Knyazev},
  doi          = {10.1016/j.ejor.2025.04.021},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {481-497},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Does carrier collaboration require combinatorial auctions?},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Truth, trust, and trade-offs: When blockchain in supply chains backfires. <em>EJOR</em>, <em>326</em>(3), 467-480. (<a href='https://doi.org/10.1016/j.ejor.2025.05.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the role of blockchain in achieving information transparency and generating trust in a dyadic supply chain with one retailer sourcing from a single supplier. The retailer has superior information regarding the demand distribution, whereas the supplier sets the capacity in preparation for the selling season. Two sources of risk are identified: information risk, which captures the incentives of the retailer to portray a favorable market condition to the supplier to encourage the supplier to secure an ample capacity, and demand risk, which captures the potential for lost sales or excess capacity investment, even when knowing the correct demand distribution. We demonstrate how blockchain eliminates information risk for the supplier. As an alternative to blockchain, we analyze a commitment contract where the retailer can order in advance a certain number of units; this commitment can serve as a signaling tool to convey market information to the supplier. We argue that the commitment contract can eliminate the information risk for the supplier (as can blockchain), but it can also reduce the supplier’s demand risk. We conclude that in many instances, the supplier and the supply chain can become worse off when blockchain is used, while the retailer favors this technology.},
  archive      = {J_EJOR},
  author       = {Tal Avinadav and Noam Shamir},
  doi          = {10.1016/j.ejor.2025.05.011},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {467-480},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Truth, trust, and trade-offs: When blockchain in supply chains backfires},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Circular economy application in pharmaceutical supply chains in the UK: A holistic evolutionary game approach. <em>EJOR</em>, <em>326</em>(3), 451-466. (<a href='https://doi.org/10.1016/j.ejor.2025.05.009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The environmental hazards of improperly managed waste have gained universal recognition among scholars and stakeholders. These hazards are especially critical in the pharmaceutical sector since leftover medications contain active chemicals that threaten the environment and human health. Nonetheless, implementation of adequate measures to ensure proper collection and treatment of pharmaceutical leftovers remains insufficient, and tons of unwanted medications are discarded in landfills and wastewater annually. Such outcomes are due to lack of coordination between the parties involved and poor incentive systems in place. To address this issue, we study coordination in pharmaceutical reverse supply chains and government incentive strategies. We employ the evolutionary game methodology to evaluate strategic behaviour of pharmacies and a waste recycler under different incentive plans. We are focusing on both reward- and awareness-driven customer segments to boost the return volume of unwanted medications. Moreover, supply chain coordination is investigated as a tool to enhance the economic viability of the system. We compare the incentive plans based on return volume, participation rate, budget spend, and implementation time, to recommend the most effective plan. An extensive numerical study provides insights into the performance of the incentive plans in different conditions. The results reveal that a plan that provides proper incentives to pharmacies for targeting both, reward- and awareness-driven customers, coupled with contract-based coordination, outperforms other plans, and does not necessarily require a budget allocation. Our study is motivated by the UK’s National Health System but it is generalisable to pharmaceutical reverse supply chains in other countries as well.},
  archive      = {J_EJOR},
  author       = {Nazanin Nami and Grigory Pishchulov and Joao Quariguasi Frota Neto},
  doi          = {10.1016/j.ejor.2025.05.009},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {451-466},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Circular economy application in pharmaceutical supply chains in the UK: A holistic evolutionary game approach},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scatter search with path relinking for linear bilevel problems. <em>EJOR</em>, <em>326</em>(3), 439-450. (<a href='https://doi.org/10.1016/j.ejor.2025.04.043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The literature includes very few instances of scatter search applications to bilevel optimization. These implementations have been proposed for problems in the field of logistics involving integer variables and are based on a structure where scatter search sets the values of the decisions at the upper level followed by the solution of the lower level problem. In this work, we develop a scatter search for solving linear bilevel problems. Our proposal employs a tailored path relinking procedure that generates solutions that are boundary feasible extreme points located in the trajectory between infeasible and feasible bilevel solutions. We perform scientific experimentation to determine the most effective configuration of our scatter search with path relinking. We also perform competitive experiments to determine where the proposed solution method stands when compared to the state of the art for tackling linear bilevel problems.},
  archive      = {J_EJOR},
  author       = {Herminia I. Calvete and Carmen Galé and José A. Iranzo and Manuel Laguna},
  doi          = {10.1016/j.ejor.2025.04.043},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {439-450},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Scatter search with path relinking for linear bilevel problems},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Branch-and-cut-and-price for agile earth observation satellite scheduling. <em>EJOR</em>, <em>326</em>(3), 427-438. (<a href='https://doi.org/10.1016/j.ejor.2025.04.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Agile Earth Observation Satellite scheduling selects and sequences satellite observations of possible targets on the Earth’s surface, each with a specific profit and multiple time windows. The objective is to maximize the collected profit of all observations completed under some operational constraints. The problem can be modeled as a variant of the Team Orienteering Problem with Time Windows (TOPTW). The key differences with the regular TOPTW are twofold: first, a time-dependent transition time is required for each pair of consecutive observations to adjust the camera’s look angles. Second, the time windows of each target vary during different observation cycles, called “orbits”. Some targets are invisible during certain orbits. We call this variant the Time-dependent Team Orienteering Problem with Variable Time Windows. In this paper, we present an efficient branch-and-cut-and-price (BCP) algorithm that exploits the problem’s characteristics to solve it to optimality. Some algorithmic enhancements have been implemented, such as a Lagrangian bound, an ng-path relaxation, a primal heuristic, and subset-row inequalities. Extensive experiments on different configurations of benchmark instances demonstrate the superior performance of the proposed BCP algorithm and its algorithmic enhancements. Moreover, the primal heuristic yields a high-quality lower bound and outperforms state-of-the-art heuristics. Finally, we adopt our framework to solve the well-known TOPTW, and our algorithm is much faster than state-of-the-art exact algorithms.},
  archive      = {J_EJOR},
  author       = {Guansheng Peng and Jianjiang Wang and Guopeng Song and Aldy Gunawan and Lining Xing and Pieter Vansteenwegen},
  doi          = {10.1016/j.ejor.2025.04.014},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {427-438},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Branch-and-cut-and-price for agile earth observation satellite scheduling},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). First-improvement or best-improvement? an in-depth local search computational study to elucidate a dominance claim. <em>EJOR</em>, <em>326</em>(3), 413-426. (<a href='https://doi.org/10.1016/j.ejor.2025.04.019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local search methods start from a feasible solution and improve it by successive minor modifications until a solution that cannot be further improved is encountered. They are a common component of most metaheuristics. Two fundamental local search strategies exist: first-improvement and best-improvement. In this work, we perform an in-depth computational study using consistent performance metrics and rigorous statistical tests on several classes of test problems considering different initialization strategies and neighborhood structures to evaluate whether one strategy is dominant over the other. The numerical results show that computational experiments previously reported in the literature claiming the dominance of one strategy over the other for the TSP given an initialization method (random or greedy) cannot be extrapolated to other problems. Still, our results highlight the need for thorough experimentation and stress the importance of examining instance feature spaces and optimization landscapes to choose the best strategy for each problem and context, as no rule of thumb seems to exist for identifying the best local search strategy in the general case.},
  archive      = {J_EJOR},
  author       = {Daniel Aloise and Robin Moine and Celso C. Ribeiro and Jonathan Jalbert},
  doi          = {10.1016/j.ejor.2025.04.019},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {413-426},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {First-improvement or best-improvement? an in-depth local search computational study to elucidate a dominance claim},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years of data envelopment analysis. <em>EJOR</em>, <em>326</em>(3), 389-412. (<a href='https://doi.org/10.1016/j.ejor.2024.12.049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Envelopment Analysis (DEA) has emerged as a powerful analytical tool, revolutionising the field of Operational Research (OR) and contributing to advancements in performance evaluation methodologies. This practical literature review delves into the extensive body of research surrounding DEA, focusing particularly on its evolution within the last 50 years. Drawing upon a comprehensive analysis of publications in top-tier OR journals this literature review paper offers analysis of DEA's development, highlighting key milestones, methodological advancements, and emerging trends. Central to this exploration is the introduction of the COOPER-framework—a structured approach derived from influential papers in the field—that provides a state-of-the-art synthesis of DEA methodologies. Emphasizing non-parametric models and addressing the challenges posed by complex decision-making environments, the COOPER-framework serves as a valuable resource for both experienced scholars and newcomers to the field. By incorporating feedback loops to navigate interconnected decisions, the framework ensures the reliability and robustness of DEA analyses. Through this literature review, we aim to not only refine existing methodologies but also provide a practical tool for researchers, fostering collaboration and driving further innovation in the field of DEA.},
  archive      = {J_EJOR},
  author       = {Anna Mergoni and Ali Emrouznejad and Kristof De Witte},
  doi          = {10.1016/j.ejor.2024.12.049},
  journal      = {European Journal of Operational Research},
  month        = {11},
  number       = {3},
  pages        = {389-412},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of data envelopment analysis},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A prescriptive tree-based model for energy-efficient room scheduling: Considering uncertainty in energy generation and consumption. <em>EJOR</em>, <em>326</em>(2), 374-388. (<a href='https://doi.org/10.1016/j.ejor.2025.02.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the energy-efficient room scheduling (ERS) problem by considering uncertainties in energy consumption and renewable energy generation in buildings. Rather than the conventional ‘predict, then optimise’ approach, we propose an improved prescriptive tree-based (IPTB) model that directly ‘prescribes’ scheduling solutions. Our model utilises contextual information on energy consumption (e.g., temperature and humidity) and renewable energies (e.g., wind speeds and sunlight) to generate direct ERS solutions. It is trained using a novel optimisation loss function that aligns historical ERS solutions with current conditions, ensuring robustness and tractability by exploiting problem-specific properties. To evaluate the proposed model’s performance, experiments on randomly generated ERS instances demonstrate that the IPTB model is trained efficiently across various problem sizes and consistently outperforms advanced data-driven optimisation methods in prescriptive accuracy. Moreover, the IPTB model achieves more balanced energy consumption, particularly under practical scenarios emphasising on energy demand charges. A case study using real-world datasets from six buildings at Monash University, Australia, validates the model’s effectiveness in addressing complex practical constraints inherent in ERS problems.},
  archive      = {J_EJOR},
  author       = {Siping Chen and Raymond Chiong and Debiao Li},
  doi          = {10.1016/j.ejor.2025.02.023},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {374-388},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A prescriptive tree-based model for energy-efficient room scheduling: Considering uncertainty in energy generation and consumption},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A secure cross-silo collaborative method for imbalanced credit scoring. <em>EJOR</em>, <em>326</em>(2), 357-373. (<a href='https://doi.org/10.1016/j.ejor.2025.04.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of information technology, there is an increasing amount of available data that can reflect a borrower’s creditworthiness, providing new avenues for credit scoring innovations. However, such data is commonly distributed across companies in various industries, and how to take advantage of multi-party collaboration while protecting customer data privacy is a major challenge. In this study, we propose an interpretable vertical logistic regression method with adaptive cost sensitivity (IVLR-ACS) for imbalanced credit scoring. Specifically, we construct a collaborative credit scoring method based on vertical logistic regression to preserve the privacy and security of multi-party information. First, to address the imbalanced class distribution problem, we develop an adaptive cost-sensitive (ACS) loss function to enhance the default risk identification of the proposed method. Then, to overcome the potential problem that the proposed method may suffer from malicious attackers adopting other technical means to steal participants’ private information, we design a differential privacy algorithm with adaptive gradient clipping and noise perturbation decay (ADDP) to train the proposed method. Finally, to improve the interpretability of collaborative multi-party credit decision-making, we introduce a feature importance interpretation method inherent to the logistic regression model to analyze the prediction results. We test the performance of the proposed method on eight credit scoring datasets and analyze its interpretability, privacy, complexity, and communication cost. Extensive experimental results demonstrate the competitiveness of the proposed method to utilize multi-party information securely and effectively.},
  archive      = {J_EJOR},
  author       = {Zhongyi Wang and Yuhang Tian and Sihan Li and Jin Xiao},
  doi          = {10.1016/j.ejor.2025.04.020},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {357-373},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A secure cross-silo collaborative method for imbalanced credit scoring},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating shift planning and pick-up and delivery problems under limited courier availability. <em>EJOR</em>, <em>326</em>(2), 343-356. (<a href='https://doi.org/10.1016/j.ejor.2025.03.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delivery couriers increasingly demand working in a flexible arrangement. Flexible contracts can be cost-effective from the perspective of a delivery company, but may also cripple its ability to serve all customers in time as couriers are not always available. To alleviate the complexities caused by courier-related constraints, delivery companies usually operate with a work schedule consisting of multiple shifts—allowing couriers to choose the shifts they prefer working in. Inspired by this trend and a real-world case, we propose a variant of pick-up and delivery problem with time windows considering multiple shifts and courier availability. We present a mixed-integer linear programming model and a set partitioning model that maximize the total profit by deciding on the shift allocation and the couriers’ delivery routes, taking courier shift preferences and vehicle availability into account. The model is solved exactly using a branch-price-and-cut algorithm. We generate a set of practically relevant instances to conduct computational experiments. Our results highlight the benefits of bundling customer requests and the challenges posed by limited courier availability in terms of the number of requests served. Bundling mitigates the negative impact of limited courier availability, while having more couriers can serve as an alternative.},
  archive      = {J_EJOR},
  author       = {Pinar Ozyavas and Evrim Ursavas and Paul Buijs and Ruud Teunter},
  doi          = {10.1016/j.ejor.2025.03.031},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {343-356},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integrating shift planning and pick-up and delivery problems under limited courier availability},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building sustainability composite indicators using a multi-criteria approach. <em>EJOR</em>, <em>326</em>(2), 326-342. (<a href='https://doi.org/10.1016/j.ejor.2025.04.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building sustainability composite indicators is a complex process that has been addressed according to different strategies. One interesting approach is based on the compromise between the maximum aggregate solution and the most balanced solution, by considering the most displaced indicator regarding the ideal. However, some shortcomings were identified in this approach. First, several decision-making units may present an equal composite indicator, and hence the same position in the ranking, while corresponding to different sustainability situations. Second, the use of only the maximum deviation to define the most balanced solution requires a more integrated approach. Thus, this paper proposes a novel aggregation methodology for building sustainability composite indicators, where a new normalized entropy indicator for the most balanced solution is proposed and integrated with the sustainability criteria of the maximum aggregate solution and maximum deviation. The method proposed was applied to two illustrative examples from the literature and provided promising and robust results.},
  archive      = {J_EJOR},
  author       = {António Xavier and Rui Fragoso and Maria de Belém Costa Freitas},
  doi          = {10.1016/j.ejor.2025.04.024},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {326-342},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Building sustainability composite indicators using a multi-criteria approach},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust elicitable functionals. <em>EJOR</em>, <em>326</em>(2), 311-325. (<a href='https://doi.org/10.1016/j.ejor.2025.04.017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elicitable functionals and (strictly) consistent scoring functions are of interest due to their utility of determining (uniquely) optimal forecasts, and thus the ability to effectively backtest predictions. However, in practice, assuming that a distribution is correctly specified is too strong a belief to reliably hold. To remediate this, we incorporate a notion of statistical robustness into the framework of elicitable functionals, meaning that our robust functional accounts for “small” misspecifications of a baseline distribution. Specifically, we propose a robustified version of elicitable functionals by using the Kullback–Leibler divergence to quantify potential misspecifications from a baseline distribution. We show that the robust elicitable functionals admit unique solutions lying at the boundary of the uncertainty region, and provide conditions for existence and uniqueness. Since every elicitable functional possesses infinitely many scoring functions, we propose the class of b-homogeneous strictly consistent scoring functions, for which the robust functionals maintain desirable statistical properties. We show the applicability of the robust elicitable functional in several examples: in a reinsurance setting and in robust regression problems.},
  archive      = {J_EJOR},
  author       = {Kathleen E. Miao and Silvana M. Pesenti},
  doi          = {10.1016/j.ejor.2025.04.017},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {311-325},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust elicitable functionals},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple sustainability criteria mapping of gas station incident consequences and subsequent decision optimisation. <em>EJOR</em>, <em>326</em>(2), 299-310. (<a href='https://doi.org/10.1016/j.ejor.2025.04.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents work towards a systematic mapping of the consequences of an incident at a public-use gas station, informing a subsequent decision regarding which gas stations to close in a given geographical zone in times of heightened risk. A criteria hierarchy of economic, environmental and social sustainability impacts is proposed. A scoring method over the set of sustainability criteria is developed. A two-level lexicographic vulnerability score for a gas filling station that considers potential loss of life and combined sustainability is then elicited. A goal programming methodology with loss of life, sustainability impact and population inconvenience goals is formulated. A case study based on the city of Portsmouth, UK is developed, with the vulnerability scores and closure recommendations of a set of gas stations calculated. Sensitivity analysis is undertaken with respect to target levels and weighting factors. The results are discussed and wider conclusions drawn.},
  archive      = {J_EJOR},
  author       = {DF Jones and O Ivanov and O Arsirii and P Crook and L Kanada and A Labib and RM Teeuw and S Smyk},
  doi          = {10.1016/j.ejor.2025.04.026},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {299-310},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multiple sustainability criteria mapping of gas station incident consequences and subsequent decision optimisation},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technology choice under the cap-and-trade policy: The impact of emission cap and technology efficiency. <em>EJOR</em>, <em>326</em>(2), 286-298. (<a href='https://doi.org/10.1016/j.ejor.2025.04.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies how competing firms make technology choices and production decisions under the cap-and-trade policy when they engage in both product and emission trading markets. Using a two-stage game theoretical model, we analyze firms’ responses to stricter emission caps and efficiency improvements of clean technology. Interestingly, we identify a “reverse trading” phenomenon where the firm with traditional technology and hence higher emission intensity sells emission allowances to the firm with clean technology because the latter operates with a higher profit margin and can afford a higher premium for allowances. Furthermore, stricter regulations incentivize firms to adopt clean technology only if the technology efficiency exceeds a certain level. Otherwise, no matter how low the emission cap is, neither firm will adopt it due to higher production costs. Additionally, cleaner technology does not necessarily provide firms greater incentives to adopt it. The efficiency of clean technology has a non-monotonic effect on firms’ adoption incentives because it not only affects the adopting firm but also has a positive spillover effect on the firm using traditional technology through emission trading. From a regulatory perspective, we propose setting a moderate emission cap to maximize social welfare, and as clean technology becomes more efficient, the optimal cap should be further tightened when the technology is already highly efficient. These findings provide practical insights for policymakers in designing the cap-and-trade policy tailored to different levels of technology improvements.},
  archive      = {J_EJOR},
  author       = {Shuhui Dong and Xiaole Wu},
  doi          = {10.1016/j.ejor.2025.04.029},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {286-298},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Technology choice under the cap-and-trade policy: The impact of emission cap and technology efficiency},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally robust scheduling for the two-stage hybrid flowshop with uncertain processing time. <em>EJOR</em>, <em>326</em>(2), 270-285. (<a href='https://doi.org/10.1016/j.ejor.2025.04.037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present paper, we investigate the two-stage hybrid flowshop with uncertain processing time. The true probability distribution of the processing time is unknown, but the statistical features can be extracted from historical data, such as the mean, lower and upper bounds. To obtain the exact scheduling result, a distributionally robust optimization (DRO) model is built to minimize the worst-case expected makespan. Then the inner problem is further reformulated as a minimization problem with a fixed sequence based on duality theory and the totally unimodular property. In addition, valid lower and upper bounds are introduced to transform the DRO model into an equivalent mixed-integer linear programming (MILP) problem with McCormick inequalities, which can be handled directly with the off-the-shelf commercial solvers. The numerical analysis demonstrates the higher computational efficiency of the DRO-based model compared with its stochastic programming (SP) counterpart. In particular, the DRO model consistently outperforms the SP model in terms of worst-case indicators. And in most cases, the DRO model triumphs the SP model in terms of average, up-quartile and up-decile indicators. Moreover, the optimal schedule obtained by the DRO model demonstrates stronger stability compared with the deterministic model. These features shed light on the principles behind reliable schedules for the two-stage hybrid flowshop scheduling model, thereby enhancing the robustness of the manufacturing system in the face of process uncertainty.},
  archive      = {J_EJOR},
  author       = {Zhi Pei and Rong Dou and Jiayan Huang and Haimin Lu},
  doi          = {10.1016/j.ejor.2025.04.037},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {270-285},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Distributionally robust scheduling for the two-stage hybrid flowshop with uncertain processing time},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Capacitated hub location routing problem with time windows and stochastic demands for the design of intra-city express systems. <em>EJOR</em>, <em>326</em>(2), 255-269. (<a href='https://doi.org/10.1016/j.ejor.2025.05.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on planning an intra-city express system in a practical environment. Various operation characteristics, such as vehicle capacity, hub capacity, time windows, and stochastic demands, have been considered. Therefore, we introduce a capacitated hub location routing problem with time windows and stochastic demand and formulate it using a multi-stage recourse model. In this model, long-term decisions (hub location and client-to-hub allocation) are made first, and short-term decisions (vehicle routing) are determined after revealing stochastic variables. To solve the problem, we propose a hybrid stochastic variable neighbourhood search (HSVNS) algorithm, which integrates an adaptive large neighbourhood search (ALNS) algorithm within a stochastic variable neighbourhood search (SVNS) framework. Numerical experiments and case studies indicate that the HSVNS algorithm can provide high-quality solutions within a reasonable computation time for instances with up to 70 clients and that considering stochastic factors can efficiently reduce operation costs, especially for instances with tight vehicle capacity and loose time windows.},
  archive      = {J_EJOR},
  author       = {Yuehui Wu and Hui Fang and Ali Gul Qureshi and Tadashi Yamada},
  doi          = {10.1016/j.ejor.2025.05.006},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {255-269},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Capacitated hub location routing problem with time windows and stochastic demands for the design of intra-city express systems},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable pathfinding problems for a correlated network: A linear programming problem in a hypergraph. <em>EJOR</em>, <em>326</em>(2), 234-254. (<a href='https://doi.org/10.1016/j.ejor.2025.04.046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the NP-hard reliable path problem, which seeks the path with minimum travel cost in correlated road networks, formulated as mean-variance (m-v) and mean-standard deviation (m-s) shortest path problems. This study proposes a novel approach that transforms these nonlinear binary integer programming models into standard linear programming (LP) problems using structure-preserving linearization and graph transformation techniques. The resulting LP formulations guarantee global optimality, overcoming the computational challenges of real-world networks. Numerical experiments on real-world networks demonstrate that the proposed method efficiently identifies the globally optimal path, matching the performance of exact methods like branch-and-bound while offering greater model flexibility. These findings provide a scalable and robust framework for reliable path selection in complex transportation networks.},
  archive      = {J_EJOR},
  author       = {Kenetsu Uchida and Yifan Wang and Ryuichi Tani},
  doi          = {10.1016/j.ejor.2025.04.046},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {234-254},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Reliable pathfinding problems for a correlated network: A linear programming problem in a hypergraph},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the integration of reinforcement learning and simulated annealing for the parallel batch scheduling problem with setups. <em>EJOR</em>, <em>326</em>(2), 220-233. (<a href='https://doi.org/10.1016/j.ejor.2025.04.042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by semiconductor applications, where wafer lots are grouped into families and processed on batch machines, this paper addresses a generalized unrelated parallel-batch scheduling problem. The goal is to minimize total completion time (flow time) while considering family- and machine-dependent setup times. We propose a mixed-integer programming formulation, establish a necessary condition for optimal schedules, and develop a polynomial-time heuristic for batching and sequencing. We also evaluate Q-Learning, a model-free reinforcement learning algorithm, for neighborhood selection within two Simulated Annealing-based metaheuristics: Stochastic Local Search (SLS) and Adaptive Large Neighborhood Search (ALNS). Results show that SLS and ALNS achieve better solutions and faster convergence compared to existing approaches. Finally, we conclude that while Q-Learning has the potential to improve solution quality in certain cases, it also increases the complexity of the algorithms, making them harder to configure and scale.},
  archive      = {J_EJOR},
  author       = {Gustavo Alencar Rolim and Caio Paziani Tomazella and Marcelo Seido Nagano},
  doi          = {10.1016/j.ejor.2025.04.042},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {220-233},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {On the integration of reinforcement learning and simulated annealing for the parallel batch scheduling problem with setups},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-dimensional cutting stock problem with flexible length and usable leftovers in the steel industry. <em>EJOR</em>, <em>326</em>(2), 207-219. (<a href='https://doi.org/10.1016/j.ejor.2025.04.036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce a two-dimensional cutting stock problem with flexible length and usable leftovers, in which multiple objectives, including minimizing the waste area of material, the exceeding area of orders and the number of slitter adjustments, are considered simultaneously. This problem is inspired by a real-world made-to-order manufacturer of special steel plates. We propose a non-linear mathematical programming model for this problem. This model is then linearized and reinforced by symmetry-breaking inequalities and other valid inequalities. To solve this model, we propose an iterated local search algorithm, which is able to tackle large instances of the problem. Numerical results demonstrate the validity of the proposed model and the effectiveness of the iterated local search algorithm. Besides, sensitive experiments are conducted to assess the impact of different parameter settings on the objective function.},
  archive      = {J_EJOR},
  author       = {Yunfeng Ma and Jiayi Zhang and Xijie Yang and Jihao Li and Xiaoxin Su and Haoxun Chen},
  doi          = {10.1016/j.ejor.2025.04.036},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {207-219},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Two-dimensional cutting stock problem with flexible length and usable leftovers in the steel industry},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years of operational research applied to healthcare. <em>EJOR</em>, <em>326</em>(2), 189-206. (<a href='https://doi.org/10.1016/j.ejor.2024.12.040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper gives an overview of five decades of operational research applied to healthcare, structured along nine key application domains: personnel scheduling, blood supply chain management, cancer diagnosis and treatment, emergency medical response and disaster relief, infectious diseases, long-term conditions, diagnostic imaging, public health, and operating room scheduling. Each section summarises the main contributions, developments, recent trends and future research. The review focuses on the European context, which is dominated by public healthcare systems with its specific strengths and challenges.},
  archive      = {J_EJOR},
  author       = {Jeroen Beliën and Sally Brailsford and Erik Demeulemeester and Derya Demirtas and Erwin W. Hans and Paul Harper},
  doi          = {10.1016/j.ejor.2024.12.040},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {2},
  pages        = {189-206},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of operational research applied to healthcare},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pareto front for two-stage distributionally robust optimization problems. <em>EJOR</em>, <em>326</em>(1), 174-188. (<a href='https://doi.org/10.1016/j.ejor.2025.04.053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-stage distributionally robust optimization is a recent optimization technique to handle uncertainty that is less conservative than robust optimization and more flexible than stochastic programming. The probability distribution of the uncertain parameters is not known but is assumed to belong to an ambiguity set. The size of certain types of ambiguity sets - such as several discrepancy-based ambiguity sets - is defined by a single parameter that makes it possible to control the degree of conservatism of the underlying optimization problem. Finding the values to assign to this parameter is a very relevant research topic. Hence, in this paper, we propose an exact and several heuristic methods for determining the control parameter values leading to all the relevant first-stage solutions. Our algorithmic approach resembles the ϵ − constrained method used to generate the Pareto front of a bi-objective problem. To demonstrate the applicability and efficacy of the proposed approaches, we conduct experiments on three different problems: scheduling, berth allocation, and facility location. The results obtained indicate that the proposed approaches provide sets of first-stage solutions very close to the optimal in a reasonable time.},
  archive      = {J_EJOR},
  author       = {Agostinho Agra and Filipe Rodrigues},
  doi          = {10.1016/j.ejor.2025.04.053},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {174-188},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Pareto front for two-stage distributionally robust optimization problems},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic entering time of a commerce platform. <em>EJOR</em>, <em>326</em>(1), 157-173. (<a href='https://doi.org/10.1016/j.ejor.2025.04.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a commerce platform that consists of two queues: one for buyers of an item and the other for sellers of the item. The platform is operated under the first-join-first-trade discipline. Upon a trade, the buyer and the seller gain respective profits but incur the cost of waiting in the platform. To maximize their expected payoffs from trading, both buyers and sellers can choose their arrival times. We characterize the Nash equilibrium in terms of a system of integro-differential equations for arrival time distributions. We show that a unique Nash equilibrium exists by proving that the system of integro-differential equations has a unique solution. Additionally, we compute the price of anarchy and investigate other disciplines that can improve it. Specifically, we show that charging platform usage fees improves the price of anarchy.},
  archive      = {J_EJOR},
  author       = {Chia-Li Wang and Bara Kim and Jeongsim Kim},
  doi          = {10.1016/j.ejor.2025.04.016},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {157-173},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Strategic entering time of a commerce platform},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing the steady-state probabilities of the number of customers in the system of a tandem queueing system, a machine learning approach. <em>EJOR</em>, <em>326</em>(1), 141-156. (<a href='https://doi.org/10.1016/j.ejor.2025.04.040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tandem queueing networks are widely used to model systems where services are provided in sequential stages. In this study, we assume that each station in the tandem system operates under a general renewal process. Additionally, we assume that the arrival process for the first station is governed by a general renewal process, which implies that arrivals at subsequent stations will likely deviate from a renewal pattern. This study leverages neural networks to approximate the marginal steady-state distribution of the number of customers based on the external inter-arrival and service time distributions. Our approach involves decomposing each station and estimating the departure process by characterizing its first five moments and auto-correlation values without limiting the analysis to linear or first-lag auto-correlation. We demonstrate that this method outperforms existing models, establishing it as state-of-the-art. Furthermore, we present a detailed analysis of the impact of the i th moments of inter-arrival and service times on steady-state probabilities of the number of customers in the system, showing that the first five moments are nearly sufficient to determine these probabilities. Similarly, we analyze the influence of inter-arrival auto-correlation, revealing that the first two lags of the first- and second-degree polynomial auto-correlation values almost wholly determine the steady-state probabilities of the number of customers in the system of a G/GI/1 queue.},
  archive      = {J_EJOR},
  author       = {Eliran Sherzer},
  doi          = {10.1016/j.ejor.2025.04.040},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {141-156},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Computing the steady-state probabilities of the number of customers in the system of a tandem queueing system, a machine learning approach},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-echelon vehicle routing problem with mobile satellites and multiple commodities. <em>EJOR</em>, <em>326</em>(1), 124-140. (<a href='https://doi.org/10.1016/j.ejor.2025.04.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper extends the two-echelon vehicle routing problem (2E-VRP) by considering multiple commodities, multiple depots, and mobile satellites (i.e., the so-called 3M-2E-VRP). This problem also accommodates flexible last-mile delivery strategies by allowing direct deliveries via first-echelon vehicles (mobile satellites) and indirect deliveries through goods exchanges at meeting points, such as parking lots or customer locations. We first model the problem as a mixed-integer linear programming (MILP); and then develop an innovative metaheuristic algorithm to solve medium and large problem instances. The proposed metaheuristic (the so-called AS-LNS) combines an innovative Approximate Scheduling (AS) approach with Large Neighborhood Search (LNS). Computational experiments validate the 3M-2E-VRP formulation and demonstrate the effectiveness of the proposed AS-LNS algorithm. Key managerial insights are further presented through a comprehensive sensitivity analysis, wherein the impact of key parameters, such as fuel consumption and wage costs, and comparison of different problem variants, is investigated on last-mile delivery strategies.},
  archive      = {J_EJOR},
  author       = {Aria Dahimi and Virginie Lurkin and Mehrdad Mohammadi and Tom Van Woensel},
  doi          = {10.1016/j.ejor.2025.04.027},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {124-140},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A two-echelon vehicle routing problem with mobile satellites and multiple commodities},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Newsvendor stockouts and option discriminability. <em>EJOR</em>, <em>326</em>(1), 111-123. (<a href='https://doi.org/10.1016/j.ejor.2025.04.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision making is the process of resolving conflict between different options that vary in discriminability. We study how conflict between the goals of profit maximisation and customer satisfaction determines decision making in the newsvendor problem. The paper consists of three studies which explore conflict from different positions. In Study 1 we show that stockouts cause newsvendor subjects to increase their stocking levels, whereas this effect is absent in a neutrally framed version of the same problem. Conflict is reflected in longer response times under low profit margin, where there is an increased likelihood of ex-post conflict, than under high profit margin. We also find that some subjects are more concerned than others of nonpecuniary factors, and this affects their decision making. In Study 2 we show that an endogenous conflict manipulation affects newsvendor behaviour. We theorise that broad bracketing (reappraisal of the choice situation) should decrease conflict, for which we find some evidence, but we also find that subjects decide less optimally when they use broad bracketing than when they use narrow bracketing. In Study 3 we use a binary newsvendor problem and model the decision process of the newsvendor using the diffusion decision model. The results show that, as in Study 1, the profit margin environment affects how newsvendors respond to conflict. Furthermore, the relative decision evidence towards the optimal choice accumulates at a slower rate after there has been a stockout. Our findings highlight that understanding decision biases in operations should include non-monetary goals, such as avoiding stockouts.},
  archive      = {J_EJOR},
  author       = {Ilkka Leppänen},
  doi          = {10.1016/j.ejor.2025.04.002},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {111-123},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Newsvendor stockouts and option discriminability},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic reconfigurations of matrix assembly layouts. <em>EJOR</em>, <em>326</em>(1), 96-110. (<a href='https://doi.org/10.1016/j.ejor.2025.03.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional assembly lines have become less efficient due to increasing customization and changing demand (e.g., the trend in e-vehicles). Matrix assembly systems, in which automated guided vehicles move products between the workstations laid out on a grid, are gaining popularity. One advantage of such systems is that they are easier to reconfigure compared to traditional assembly lines. In this work, we develop a methodology for the configuration and reconfiguration of matrix assembly layouts under changing demand. The decisions consist of selecting active stations, task assignments, and product flows for each period of a multi-period planning horizon. The three objective functions minimize the number of active stations, the number of reconfigurations, and the total flow distance. We formulate a lexicographic multi-objective mixed-integer linear programming model for this problem. We develop an exact solution approach using period-based, layout-based, and Benders decompositions. For our numerical tests, we adapt standard instances from the literature. In terms of computational performance, our approach is, on average, 53.3% faster than the original MIP solved with a commercial solver for practice-size instances. Our insights reveal that matrix layouts with dynamic reconfigurations enhance the active number of stations by 31.3% and reduce flow distances by 12.4% on average, compared to static layouts.},
  archive      = {J_EJOR},
  author       = {O. Baturhan Bayraktar and Martin Grunow and Rainer Kolisch},
  doi          = {10.1016/j.ejor.2025.03.023},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {96-110},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Dynamic reconfigurations of matrix assembly layouts},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-period hub network design from a dual perspective: An integrated approach considering congestion, demand uncertainty, and service quality optimization. <em>EJOR</em>, <em>326</em>(1), 78-95. (<a href='https://doi.org/10.1016/j.ejor.2025.04.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a hub network design problem that considers three key factors: congestion, demand uncertainty, and multi-periodicity. Unlike classical models, which tend to address these factors separately, our model considers them simultaneously, providing a more realistic representation of hub network design challenges. Our model also incorporates service level considerations of network users, extending beyond the focus on transportation costs. Service quality is evaluated using two measures: travel time and the number of hubs visited during travel. Moreover, our model allows for adjustments in capacity levels and network structure throughout the planning horizon, adding a dynamic and realistic aspect to the problem setting. The inherent nonlinear nonconvex integer programming problem is reformulated into a mixed-integer second-order cone programming (SOCP) problem. To manage the model’s complexity, we propose an exact solution algorithm based on Benders decomposition, where the sub-problems are solved using a column generation technique. The efficacy of the solution approach is demonstrated through extensive computational experiments. Additionally, we discuss the benefits of each considered feature in terms of transportation costs and their impact on network structure, providing insights for the field.},
  archive      = {J_EJOR},
  author       = {Vedat Bayram and Çiya Aydoğan and Kamyar Kargar},
  doi          = {10.1016/j.ejor.2025.04.011},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {78-95},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multi-period hub network design from a dual perspective: An integrated approach considering congestion, demand uncertainty, and service quality optimization},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reactive scheduling of uncertain jobs with maximum time lags. <em>EJOR</em>, <em>326</em>(1), 69-77. (<a href='https://doi.org/10.1016/j.ejor.2025.04.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a scheduling problem characterized by uncertain task durations and maximum time lags, a combination that has received little attention in the literature. The problem involves a set of jobs, each comprising a sequence of tasks where the penultimate task has uncertain duration, known only within a given range, and the final tasks are identical across all jobs. There must be no idle time between consecutive tasks of the same job, except for a bounded delay allowed between the penultimate and final tasks, which is consistent across all jobs. Each task requires a subset of renewable resources, and jobs have specific release dates and arrive in real time. The challenge is to determine the starting times of the tasks such that no resource or temporal constraints are violated, regardless of the realized task durations, with the goal of minimizing the total waiting time of the jobs. This problem is particularly relevant to bio-manufacturing applications. We propose a method to iteratively schedule the jobs in polynomial time, ensuring the optimal insertion of each job relative to the already scheduled ones. We also present a comprehensive computational evaluation of the proposed method.},
  archive      = {J_EJOR},
  author       = {Péter Györgyi and Tamás Kis and Evelin Szögi},
  doi          = {10.1016/j.ejor.2025.04.013},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {69-77},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Reactive scheduling of uncertain jobs with maximum time lags},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow-shop and job-shop robust scheduling problems with budgeted uncertainty. <em>EJOR</em>, <em>326</em>(1), 54-68. (<a href='https://doi.org/10.1016/j.ejor.2025.04.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study different solution methods for two two-stage robust, multi-machine scheduling problems, namely permutation flow-shop and job-shop scheduling problems under uncertainty budget. Compact formulations of the problems are proposed and two decomposition approaches are presented: a Benders decomposition approach and a column and constraint generation approach. Computational experiments show that for small-sized instances, a compact formulation of the problem quickly yields optimal solutions. However, for larger instances, decomposition methods, particularly the column and constraint generation method with a master problem solved using constraint programming, provide better quality solutions. An acceleration method for the column and constraint generation algorithm is proposed. This method is generic and can be applied to any two-stage robust optimisation problem.},
  archive      = {J_EJOR},
  author       = {Carla Juvin and Laurent Houssin and Pierre Lopez},
  doi          = {10.1016/j.ejor.2025.04.012},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {54-68},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Flow-shop and job-shop robust scheduling problems with budgeted uncertainty},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A proximal splitting algorithm for generalized DC programming with applications in signal recovery. <em>EJOR</em>, <em>326</em>(1), 42-53. (<a href='https://doi.org/10.1016/j.ejor.2025.04.034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difference-of-convex (DC) program is an important model in nonconvex optimization due to its structure, which encompasses a wide range of practical applications. In this paper, we aim to tackle a generalized class of DC programs, where the objective function is formed by summing a possibly nonsmooth nonconvex function and a differentiable nonconvex function with Lipschitz continuous gradient, and then subtracting a nonsmooth continuous convex function. We develop a proximal splitting algorithm that utilizes proximal evaluation for the concave part and Douglas–Rachford splitting for the remaining components. The algorithm guarantees subsequential convergence to a critical point of the problem model. Under the widely used Kurdyka–Łojasiewicz property, we establish global convergence of the full sequence of iterates and derive convergence rates for both the iterates and the objective function values, without assuming the concave part is differentiable. The performance of the proposed algorithm is tested on signal recovery problems with a nonconvex regularization term and exhibits competitive results compared to notable algorithms in the literature on both synthetic data and real-world data.},
  archive      = {J_EJOR},
  author       = {Tan Nhat Pham and Minh N. Dao and Nima Amjady and Rakibuzzaman Shah},
  doi          = {10.1016/j.ejor.2025.04.034},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {42-53},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A proximal splitting algorithm for generalized DC programming with applications in signal recovery},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A branch and bound algorithm for continuous multiobjective optimization problems using general ordering cones. <em>EJOR</em>, <em>326</em>(1), 28-41. (<a href='https://doi.org/10.1016/j.ejor.2025.04.045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing branch and bound algorithms for multiobjective optimization problems require a significant computational cost to approximate the entire Pareto optimal solution set. In this paper, we propose a new branch and bound algorithm that approximates a part of the Pareto optimal solution set by introducing the additional preference information in the form of ordering cones. The basic idea is to replace the Pareto dominance induced by the nonnegative orthant with the cone dominance induced by a larger ordering cone in the discarding test. In particular, we consider both polyhedral and non-polyhedral cones, and propose the corresponding cone dominance-based discarding tests, respectively. In this way, the subboxes that do not contain efficient solutions with respect to the ordering cone will be removed, even though they may contain Pareto optimal solutions. We prove the global convergence of the proposed algorithm. Finally, the proposed algorithm is applied to a number of test instances as well as to 2- to 5-objective real-world constrained problems.},
  archive      = {J_EJOR},
  author       = {Weitian Wu and Xinmin Yang},
  doi          = {10.1016/j.ejor.2025.04.045},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {28-41},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A branch and bound algorithm for continuous multiobjective optimization problems using general ordering cones},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speeding up grover’s algorithm. <em>EJOR</em>, <em>326</em>(1), 13-27. (<a href='https://doi.org/10.1016/j.ejor.2025.02.034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To find one of the M target entries in an unstructured database that has N entries, Grover’s algorithm performs π 4 N M iterations. In each iteration, a function is called that evaluates whether an entry is a target entry. Compared to a classical procedure, that requires N M evaluations, Grover’s algorithm achieves a quadratic speedup. This quadratic speedup, combined with its general applicability as a search algorithm, have made Grover’s algorithms one of the most important (quantum) algorithms available today. A study to improve its runtime is more than justified. For this purpose, we investigate two speedup strategies: (1) reducing the number of iterations that are performed in each run of Grover’s algorithm and (2) partitioning the database. For each of these strategies, we not only execute Grover’s algorithm in parallel, but also explore the possibility to execute Grover’s algorithm in series on a single Quantum Processing Unit (QPU) as well as on multiple QPUs. For each combination of execution mode (serial, parallel, and serial/parallel) and speedup strategy, we show how to obtain optimal policies using closed-form expressions and a gradient-descent procedure. For a single QPU we obtain a speedup factor of 1.1382 when compared to a textbook implementation of Grover’s algorithm. If multiple QPUs are at our disposal, we obtain a speedup factor of at most 1 . 1382 Q , where Q denotes the number of QPUs. In addition, we show that the dominant policies that minimize the expected number of Grover iterations also minimize the expected number of iterations that are performed per qubit.},
  archive      = {J_EJOR},
  author       = {Stefan Creemers},
  doi          = {10.1016/j.ejor.2025.02.034},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {13-27},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Speeding up grover’s algorithm},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The quadratic knapsack problem. <em>EJOR</em>, <em>326</em>(1), 1-12. (<a href='https://doi.org/10.1016/j.ejor.2024.12.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quadratic knapsack problem is a relevant N P -hard combinatorial optimization problem, inspired, since the Seventies, by a number of real-world applications. After its formal definition in 1980, it was subject to intensive research, especially in the last two decades. No recent review on this problem appeared in the literature after a well-known survey, published in 2007 but updated to 2003. The purpose of this work is to provide a thorough overview of classical and recent results on the quadratic knapsack problem. We examine mathematical models, linearizations and reformulations. We review upper bounds, exact algorithms, heuristic and metaheuristic approaches, and provide a comparison of their computational performance.},
  archive      = {J_EJOR},
  author       = {Laura Galli and Silvano Martello and Paolo Toth},
  doi          = {10.1016/j.ejor.2024.12.032},
  journal      = {European Journal of Operational Research},
  month        = {10},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The quadratic knapsack problem},
  volume       = {326},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two axiomatizations of the pairwise netting proportional rule in financial networks. <em>EJOR</em>, <em>325</em>(3), 553-567. (<a href='https://doi.org/10.1016/j.ejor.2025.04.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider financial networks where agents are linked to each other via mutual liabilities. In case of bankruptcy, one needs to distribute the assets of bankrupt agents over the other agents. One common approach is to first apply pairwise netting of mutual liabilities and next use the proportional rule to determine the payments based on the net liabilities. We refer to this as the pairwise netting proportional rule. The pairwise netting proportional rule satisfies the basic requirements of claims boundedness, limited liability, priority of creditors, and continuity. It also satisfies the desirable properties of net impartiality, an agent that has two creditors with the same net claim pays the same amount to both creditors on top of pairwise netting, and invariance to mitosis, an agent that splits into a number of identical agents is not affecting the payments of the other agents. We first demonstrate that if net impartiality and invariance to mitosis, together with the basic requirements, are regarded as imperative properties, then payments should be determined by the pairwise netting proportional rule. We also obtain a second axiomatization by dropping the continuity requirement and replacing invariance to mitosis by the axiom of invariance to proportional splitting, a proportional assignment of the assets and liabilities of an agent to a newly created agent, should not affect the payments of the other agents.},
  archive      = {J_EJOR},
  author       = {Péter Csóka and P. Jean-Jacques Herings},
  doi          = {10.1016/j.ejor.2025.04.008},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {553-567},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Two axiomatizations of the pairwise netting proportional rule in financial networks},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended warranty pricing in a competitive aftermarket under logit demand. <em>EJOR</em>, <em>325</em>(3), 541-552. (<a href='https://doi.org/10.1016/j.ejor.2025.04.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is common for multiple firms—such as manufacturers, retailers, and third-party insurers—to coexist and compete in the aftermarket for durable products. In this paper, we study price competition in a partially concentrated aftermarket where one firm offers multiple extended warranty (EW) contracts while the others offer a single one. The demand for EWs is described by the multinomial logit model. We show that, at equilibrium, such an aftermarket behaves like a combination of monopoly and oligopoly. Building upon this base model, we further investigate sequential pricing games for a durable product and its EWs to accommodate the ancillary nature of after-sales services. We consider two scenarios: one where the manufacturer (as the market leader) sets product and EW prices simultaneously , and another where these decisions are made sequentially . Our analysis demonstrates that offering EWs incentivizes the manufacturer to lower the product price, thereby expanding the market potential for EWs. Simultaneous product–EW pricing leads to a price concession on EWs compared to sequential pricing, effectively reducing the intensity of competition in the aftermarket. Overall, the competitiveness of an EW hinges on its ability to deliver high value to consumers at low marginal cost to its provider. While our focus is on EWs, the proposed game-theoretical pricing models apply broadly to other ancillary after-sales services.},
  archive      = {J_EJOR},
  author       = {Xiao-Lin Wang and Shizhe Peng and Xiaoge Zhang},
  doi          = {10.1016/j.ejor.2025.04.001},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {541-552},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Extended warranty pricing in a competitive aftermarket under logit demand},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage robust optimization approach for enhanced community resilience under tornado hazards. <em>EJOR</em>, <em>325</em>(3), 525-540. (<a href='https://doi.org/10.1016/j.ejor.2025.03.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Catastrophic tornadoes cause severe damage and are a threat to human wellbeing, making it critical to determine mitigation strategies to reduce their impact. One such strategy, following recent research, is to retrofit existing structures. To this end, in this article we propose a model that considers a decision-maker (a government agency or a public–private consortium) who seeks to allocate resources to retrofit and recover wood-frame residential structures, to minimize the population dislocation due to an uncertain tornado. In the first stage the decision-maker selects the retrofitting strategies, and in the second stage the recovery decisions are made after observing the tornado. As tornado paths cannot be forecasted reliably, we take a worst-case approach to uncertainty where paths are modeled as arbitrary line segments on the plane. Under the assumption that an area is affected if it is sufficiently close to the tornado path, the problem is framed as a two-stage robust optimization problem with a mixed-integer non-linear uncertainty set. We solve this problem by using a decomposition column-and-constraint generation algorithm that solves a two-level integer problem at each iteration. This problem, in turn, is solved by a decomposition branch-and-cut method that exploits the geometry of the uncertainty set. To illustrate the model’s applicability, we present a case study based on Joplin, Missouri. Our results show that there can be up to 20% reductions in worst-case population dislocation by investing $15 million in retrofitting and recovery and that our approach outperforms other retrofitting policies.},
  archive      = {J_EJOR},
  author       = {Mehdi Ansari and Juan S. Borrero and Andrés D. González},
  doi          = {10.1016/j.ejor.2025.03.001},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {525-540},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Two-stage robust optimization approach for enhanced community resilience under tornado hazards},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean-variance optimization in finite horizon markov decision processes and its application to revenue management. <em>EJOR</em>, <em>325</em>(3), 516-524. (<a href='https://doi.org/10.1016/j.ejor.2025.03.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, risk-averse decision-making is crucial. In this context, the mean–variance (MV) criterion is widely accepted and often used to find the right balance between maximizing expected rewards and avoiding poor performances. In dynamic settings, however, it is challenging to efficiently compute policies under the MV objective and hence, surrogates like the exponential utility model are often used. In this paper, we consider MV optimization for discrete time Markov decision processes (MDP) with finite horizon. Our approach is based on a system of tractable subproblems with distorted variance that allows to identify mean–variance combinations that cannot be attained. The number of subproblems to solve can be chosen such that a predetermined ex-ante optimality gap is obtained. We illustrate the effectiveness and the applicability of our approach for different revenue management examples. We find that competitive ex-ante and ex-post optimality gaps lower than 0.0001% can be reliably obtained with acceptable computational effort.},
  archive      = {J_EJOR},
  author       = {Rainer Schlosser and Jochen Gönsch},
  doi          = {10.1016/j.ejor.2025.03.030},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {516-524},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Mean-variance optimization in finite horizon markov decision processes and its application to revenue management},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioral dynamic portfolio selection with S-shaped utility and epsilon-contaminations. <em>EJOR</em>, <em>325</em>(3), 500-515. (<a href='https://doi.org/10.1016/j.ejor.2025.03.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the classical cumulative prospect theory (CPT), we propose a CPT-like functional characterized by the modeling of uncertainty on gains and losses through two epsilon-contaminations of a reference probability measure. Such functional is used to perform a dynamic portfolio selection in a finite horizon binomial market model, reducing it to an iterative search problem over the set of optimal solutions of a family of pairs of non-linear optimization problems on the final wealth. Despite the computational hardness of the resulting pairs of problems, epsilon-contaminations allow to represent each solution in terms of the partition generated by the stock price random variable at maturity, obtaining a sensible reduction of variables and constraints. In turn, the optimization task can be reduced to the maximization of a real-valued function of one real variable, revealing the possible ill-posedness of the problem. The resulting model is discussed by means of some paradigmatic examples on market data and a sensitivity analysis.},
  archive      = {J_EJOR},
  author       = {Andrea Cinfrignini and Davide Petturiti and Barbara Vantaggi},
  doi          = {10.1016/j.ejor.2025.03.029},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {500-515},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Behavioral dynamic portfolio selection with S-shaped utility and epsilon-contaminations},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consumer preference estimation based on intertemporal choice data: A chance constrained data envelopment analysis method. <em>EJOR</em>, <em>325</em>(3), 487-499. (<a href='https://doi.org/10.1016/j.ejor.2025.03.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choice behavior reflects consumer preferences. Consumers often purchase products online nowadays, which can be viewed as a choice process. If a consumer makes multiple transactions over a period of time, then we can say the consumer make multiple intertemporal choices. This study focuses on the problem of learning consumer preferences from intertemporal choice data. The main challenges in this research include the ratio relationship between some attributes, the variability of choice set and the uncertainty of attribute values. To address these challenges, we propose a consumer preference model based on the chance-constrained data envelopment analysis (DEA) framework. In the model, we assume consumer choice has maximum utility value, and define a performance cost utility function to capture the ratio relationship between some attributes. We then develop two scenarios for the consumer preference model, depending on whether the uncertain variables are correlated. The estimated consumer preferences can be used to predict each consumer's choice and item ranking. To validate our model, we conduct two numerical experiments, and analyze the impact of some parameters on the preference and evaluation results. The results show that the estimated preference values are accurate when the values of risk indicator and correlation coefficients are small, and our model performs well on the predictions of choice and item ranking.},
  archive      = {J_EJOR},
  author       = {Ping Wang and Qingxian An and Liang Liang},
  doi          = {10.1016/j.ejor.2025.03.021},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {487-499},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Consumer preference estimation based on intertemporal choice data: A chance constrained data envelopment analysis method},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust facility location considering protection and backup under facility disruptions with decision-dependent uncertainty. <em>EJOR</em>, <em>325</em>(3), 474-486. (<a href='https://doi.org/10.1016/j.ejor.2025.03.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing a resilient supply chain network is crucial to mitigate the impact of disruptions on supply facilities. This paper merges the facility fortification problem into the robust facility location problem by consideration of proactive and response strategy to against supply facility disruptions and introduces a novel two-stage robust optimization (TRO) model with decision-dependent uncertainty (DDU). Within this TRO framework, facility location, protection, and backup decisions are treated as here-and-now decisions, anticipating the worst-case scenario of facility failures under disruption. Subsequent enabling backup and allocation decisions are wait-and-see decisions. Some theoretic properties on the TRO-DDU model are proposed and a decomposition method based on the nested column-and-constraint generation (NC&CG) algorithm is presented to solve the model exactly. Extensive numerical experiments are conducted to demonstrate the effectiveness of considering both protection and backup decisions and to examine their importance from the perspectives of the impact of protection budget, backup budget, total budget, disruption risk level, minimum number of facilities to open and backup cost coefficient. Also, we propose optimal budget plans for the manager based on maximizing average marginal efficiency (AME) to avoid meaningless additional budget investment. We highlight the advantages of the proposed model in random disruption scenarios compared to the one-stage robust model and one-stage model with random disruption risk. The results show how our model strategically mitigates facility disruption risks and enhances system performance.},
  archive      = {J_EJOR},
  author       = {Haitao HU and Jiafu TANG},
  doi          = {10.1016/j.ejor.2025.03.027},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {474-486},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust facility location considering protection and backup under facility disruptions with decision-dependent uncertainty},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Managing supply chains facing extreme weather: Supplier’s nature and investment. <em>EJOR</em>, <em>325</em>(3), 457-473. (<a href='https://doi.org/10.1016/j.ejor.2025.02.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, extreme weather conditions have been playing havoc on supply chains. Companies are investing in climate-specific assets to mitigate weather-related risks and ensure continuous supply chain activities. In this paper, we analytically investigate a supplier’s decision regarding climate-specific investment to understand how the probability of weather disruption, its effort during extreme weather, and its reciprocal behavior influence the buyer’s optimal order decision. We develop a dynamic game model to capture and examine the nature of the supplier–buyer interaction. We also investigate the impact of the supplier’s nature on this interaction during an extreme weather event. Our key findings are as follows: (i) With a higher probability of extreme weather disruption, the supplier prefers generic investment over climate-specific investment. (ii) With a decrease in the probability of extreme weather events, the non-proactive supplier prefers generic investment, whereas the proactive supplier prefers climate-specific investment. This situation allows proactive suppliers to reveal themselves and buyers to recognize the nature of suppliers by observing their investment type. The buyer chooses to source from the proactive supplier when it anticipates extreme weather scenarios. (iii) A downstream buyer orders less from the non-proactive supplier and orders the required quantity from the proactive supplier. There exists a pooling equilibrium for the supplier–buyer interaction where all types of suppliers choose a climate-specific investment, and the buyer orders its required order quantity. Subsequently, through extended analysis, we demonstrate that the proactive supplier charges a higher per-unit price to the buyer in the case of a higher possibility of extreme weather events. The proactive supplier charges a reduced per-unit price only if it exhibits reciprocal behavior. From our analysis, we also find that, with an increase in the cost of climate-specific investment, a non-proactive supplier tends to choose a generic investment and a proactive supplier tends to choose a climate-specific investment.},
  archive      = {J_EJOR},
  author       = {Indranil Biswas and Dewang Pagare and Sunil Tiwari and Tsan-Ming Choi},
  doi          = {10.1016/j.ejor.2025.02.026},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {457-473},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Managing supply chains facing extreme weather: Supplier’s nature and investment},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic financing options in a supply chain facing guarantee shortages and capital constraints under demand uncertainty. <em>EJOR</em>, <em>325</em>(3), 444-456. (<a href='https://doi.org/10.1016/j.ejor.2025.02.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When distributors, as small and medium-sized enterprises (SMEs), encounter financial constraints and market uncertainty, manufacturers can offer guarantees and product buyback (credit-buyback-financing strategy, CBF) to mitigate risk and enhance output. Recently, the rise in popularity of third-party guarantee institutions introduces additional options for supply chain members, including coguarantee (credit-coguarantee-financing strategy, CCF) or a combination of coguarantee and buyback (credit-coguarantee-buyback-financing strategy, CCBF). To determine the optimal financing strategy, this paper scrutinizes the efficiencies and profitability associated with these three formats within an analytical framework. We find that the downstream distributor always favors CCBF, while the manufacturer’s inclination shifts from CBF to CCF/CCBF as the buyback price decreases (he prefers CCF when the coguarantee share is high; prefers CCBF otherwise). The guarantee institution mirrors the manufacturer’s choices, expressing a preference for CCF when the buyback price is high and the coguarantee share is moderate. Especially, a Pareto improvement is achievable for three partners by employing CCBF under certain conditions. In this case, CCBF induces an appropriate guarantee fee rate, promoting order quantities without excessive default risk, thereby benefiting all parties involved. These results provide valuable insights for managers in identifying a financing strategy that facilitates a triple-win situation.},
  archive      = {J_EJOR},
  author       = {Xiaoliang Zhu and Yingchen Yan and Guoqing Yang},
  doi          = {10.1016/j.ejor.2025.02.039},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {444-456},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Strategic financing options in a supply chain facing guarantee shortages and capital constraints under demand uncertainty},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compact formulations and valid inequalities for parallel machine scheduling with conflicts. <em>EJOR</em>, <em>325</em>(3), 433-443. (<a href='https://doi.org/10.1016/j.ejor.2025.04.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of scheduling conflicting jobs on parallel machines consists in assigning a set of jobs to a set of machines so that no two conflicting jobs are allocated to the same machine, and the maximum processing time among all machines is minimized. We propose a new compact mixed integer linear formulation based on the representatives model for the vertex coloring problem, which overcomes a number of issues inherent in the natural assignment model. We present a polyhedral study of the associated polytope, and describe classes of valid inequalities inherited from the stable set polytope. We describe branch-and-cut algorithms for the problem, and report on computational experiments with benchmark instances. Our computational results on the hardest instances of the benchmark set show that the proposed algorithms are superior (either in running time or quality of the solutions) to the current state-of-the-art methods. We find that our new method performs better than the existing ones especially when the gap between the optimal value and the trivial lower bound (i.e., the sum of all processing times divided by the number of machines) increases.},
  archive      = {J_EJOR},
  author       = {Phablo F.S. Moura and Roel Leus and Hande Yaman},
  doi          = {10.1016/j.ejor.2025.04.006},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {433-443},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Compact formulations and valid inequalities for parallel machine scheduling with conflicts},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Including mechanical requirements in a bi-objective nesting and scheduling model for additive manufacturing. <em>EJOR</em>, <em>325</em>(3), 416-432. (<a href='https://doi.org/10.1016/j.ejor.2025.03.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the increasing relevance of Additive Manufacturing (AM) as Manufacturing-as-a-service (Maas), the AM scheduling (and related nesting) problem has been increasingly investigated. Due to their business nature, Maas companies are interested in minimizing both the makespan and the total tardiness; however, most of the literature focuses only on one of them. This work fills this gap proposing a mixed-integer linear programming (MILP) model that minimizes both makespan and total tardiness. In doing so, for the first time in the literature, considerations on parts’ strength are included. During nesting procedures, indeed, parts can be oriented in different ways, with this choice affecting not only the total processing time (as considered by the literature) but also the strength achievable: if this is lower than what planned, parts might fail unexpectedly with detrimental consequences. Thus, this work ensures that parts are produced with the required strength. In doing so, we focus on a parallel unrelated AM batch scheduling problem for metallic parts. Considering the multi-objective and NP-hard nature of the problem, an ε-constraint algorithm and a non-dominated sorting genetic algorithm-II (NSGA-II) are developed to solve the problem. Four different problem-specific decoding mechanisms are integrated into the NSGA-II to improve its search capability and solution-building performance. Their performances are evaluated through computational experiments, showing that the integrated mechanisms improve the performance of the NSGA-II. Finally, through numerical instances and analysis of the super Pareto front, we derive managerial insights on the impact of strength requirements and machines’ number and features on the objectives.},
  archive      = {J_EJOR},
  author       = {Ibrahim Kucukkoc and Serena Finco and Mirco Peron and Gulsen Aydin Keskin},
  doi          = {10.1016/j.ejor.2025.03.022},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {416-432},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Including mechanical requirements in a bi-objective nesting and scheduling model for additive manufacturing},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequencing with learning, forgetting and task similarity. <em>EJOR</em>, <em>325</em>(3), 400-415. (<a href='https://doi.org/10.1016/j.ejor.2025.03.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human–machine workplaces, employee skills change over time due to learning and forgetting effects, which greatly affects efficiency. We model these effects within a simple production system. When a job is processed, due to learning the next job of the same type requires less processing time. Meanwhile, jobs of other types are forgotten, hence their future processing times increase. In our work, the amount of learning and forgetting depends on the full matrix similarity of the two job types and on the size of the jobs processed. We describe a dynamic programming algorithm that minimizes the makespan optimally. More generally, the learning level of a job type has an upper limit above which further learning is lost, and a lower limit below which further forgetting is saved. For this more general problem, we adapt our dynamic programming algorithm to provide tight lower bounds that validate the performance of simple heuristic approaches and genetic algorithms. A computational study demonstrates that these procedures routinely deliver optimal, or very close to optimal, solutions for up to eight job types and 80 jobs. Our work provides what are apparently the first effective procedures for optimization of large-scale production schedules with learning and forgetting effects defined by full matrix similarity. This identifies a critical opportunity for human–machine collaboration to improve productivity and support operational excellence.},
  archive      = {J_EJOR},
  author       = {Shuling Xu and Fulong Xie and Nicholas G. Hall},
  doi          = {10.1016/j.ejor.2025.03.002},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {400-415},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Sequencing with learning, forgetting and task similarity},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formulating opinion dynamics from belief formation, diffusion and updating in social network group decision-making: Towards developing a holistic framework. <em>EJOR</em>, <em>325</em>(3), 381-399. (<a href='https://doi.org/10.1016/j.ejor.2024.12.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactions in social networks have become an integral part of people’s daily lives. In various decision-making situations, individuals usually hold diverse prior beliefs and engage in communication with their social connections to make informed decisions. However, most existing research focuses on isolated steps of this process, overlooking the overall complexity of decision-making in social networks. To bridge this important research gap, our paper aims to explore the key steps involved in the process and develop a holistic framework for analyzing how individuals form, exchange and update beliefs, ultimately leading to opinion dynamics and group decision behaviors in a social network. Specifically, relevant literature that focuses on different steps will be reviewed and drawn together to characterize the decision-making process in a comprehensive and systematic manner: individuals form initial beliefs following the principle of multiple criteria decision-making intuitively, information propagates in the social network and affects individuals’ beliefs differently in a form of social influence, beliefs evolve through dynamic interactions with others, and eventually individuals make their decisions, leading to group decision behaviors in the social network. Applications will be briefly discussed to illustrate the practical implications of this research. Finally, conclusions and future research outlook will be discussed in detail. It is expected that the holistic framework developed on the basis of the comprehensive literature review can provide in-depth insights into decision analysis in social networks and shed light on future research and applications towards effective integration of decision science, operational research, and social network analysis.},
  archive      = {J_EJOR},
  author       = {Tao Wen and Rui Zheng and Ting Wu and Zeyi Liu and Mi Zhou and Tahir Abbas Syed and Darminder Ghataoura and Yu-wang Chen},
  doi          = {10.1016/j.ejor.2024.12.015},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {3},
  pages        = {381-399},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Formulating opinion dynamics from belief formation, diffusion and updating in social network group decision-making: Towards developing a holistic framework},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From data to diagnosis: A logical learning method to enhance interpretability in bipolar and major depressive disorder identification. <em>EJOR</em>, <em>325</em>(2), 362-380. (<a href='https://doi.org/10.1016/j.ejor.2025.03.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significance of intelligent diagnosis technology in enhancing early detection efficiency is paramount. However, the complexity of machine learning algorithms often hampers result interpretability. This paper proposes an interpretable diagnostic method named logical learning, which combines multi-attribute value theory, machine learning, and optimization techniques. It simulates physicians’ diagnostic rules/logic using an interactive value function, considering the marginal values and importance of features, along with their interactions. A variant of a gradient descent optimization algorithm and cross-validation are utilized to estimate a comprehensive decision model from historical diagnosis data. The logical learning method is applied to distinguish bipolar disorder (BD) and major depressive disorder (MDD) using the electronic medical records of 6157 patients from a large hospital in western China. It provides the degree of contribution of each feature to the diagnosis and explicitly indicates which symptoms’ presence, abnormally high or low biomarkers have significant contributions to the diagnosis of BD or MDD. With an AUC (area under the curve) of 0.851 and an accuracy of 0.803, the proposed method demonstrates superior performance than traditional machine learning.},
  archive      = {J_EJOR},
  author       = {Xingli Wu and Ting Zhu},
  doi          = {10.1016/j.ejor.2025.03.016},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {362-380},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {From data to diagnosis: A logical learning method to enhance interpretability in bipolar and major depressive disorder identification},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality management and feedback operation for user-generated content considering dynamic value belief. <em>EJOR</em>, <em>325</em>(2), 344-361. (<a href='https://doi.org/10.1016/j.ejor.2025.03.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decisions concerning the configuration of user-generated content (UGC) present challenges for quality management and platform monetization. UGC quality, including historical accumulated quality, impacts users’ consumption behaviour and advertising revenues. This study defines the value of UGC consumption based on content quality and price. We model the dynamic accumulation of historical values as value belief using differential equations. A differential game framework is employed to investigate the dynamic interplay between quality and advertising strategies for contributors and the platform. To address concerns about UGC quality, we introduce a user feedback mechanism that reflects post-engagement experiences and influences platform strategies. We investigate dynamic quality and advertising strategies within a decentralized UGC operational framework, both with and without the user feedback mechanism. The findings reveal that user feedback mitigates the negative effects of low-quality UGC and improves platform operational flexibility. Continuous advertising strategies may not always be beneficial, particularly when value belief is low. Furthermore, the user feedback mechanism exhibits different effects under varying subsidy and market potential scenarios. It helps sustain content quality in low-subsidy or unfavourable market conditions while amplifying profitability in high-subsidy or expansive markets. The platform can also determine content type strategies based on UGC period length. More continuous UGC generates short-term profits, whereas more decentralized UGC fosters long-term growth potential. These insights offer strategic guidance for platforms in determining operational models and premium content incentive schemes.},
  archive      = {J_EJOR},
  author       = {Bei Bian and Haiyan Wang},
  doi          = {10.1016/j.ejor.2025.03.026},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {344-361},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Quality management and feedback operation for user-generated content considering dynamic value belief},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Duration forecasting in resource constrained projects: A hybrid risk model combining complexity indicators with sensitivity measures. <em>EJOR</em>, <em>325</em>(2), 329-343. (<a href='https://doi.org/10.1016/j.ejor.2025.03.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study combines complexity measures from the project scheduling literature and sensitivity measures from the risk analysis literature to improve project duration forecasts in resource constrained projects. A hybrid risk model is proposed incorporating project network measures, resource-related indicators, and risk sensitivity metrics. The hybrid risk model is then used for forecasting the duration of unseen projects. The study contributes to the existing literature by integrating newly proposed activity sensitivity metrics and network and resource related indicators in project forecasting. Additionally, it conducts a large-scale experiment to compare the accuracy of the hybrid risk model against benchmark methods, including Monte Carlo simulations and relevant machine learning algorithms. The results show that inclusion of resource-related variables within the hybrid risk model significantly improves the accuracy, validating recently proposed metrics. The hybrid risk model outperforms most of the benchmark methods in high-uncertainty projects, emphasizing the importance of accurately estimating the flexibility in activity start times. Furthermore, the hybrid risk model of this paper is particularly effective for parallel projects, demonstrating a better performance under various uncertainty and flexibility conditions. Finally, the results are validated using empirical project data.},
  archive      = {J_EJOR},
  author       = {Izel Ünsal Altuncan and Mario Vanhoucke},
  doi          = {10.1016/j.ejor.2025.03.012},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {329-343},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Duration forecasting in resource constrained projects: A hybrid risk model combining complexity indicators with sensitivity measures},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data envelopment analysis fixed cost allocation based on dynamic bargaining game and the nash equilibrium. <em>EJOR</em>, <em>325</em>(2), 317-328. (<a href='https://doi.org/10.1016/j.ejor.2025.03.009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fixed cost allocation (FCA) poses a significant challenge for decision-making units (DMUs) contributing to a shared cost. Each DMU aims to allocate the minimum possible cost to itself. As a result, different DMUs hold varying and often conflicting allocation proposals and preferences, which prevents them from reaching a consensus on the FCA outcome. This paper develops a dynamic bargaining game-based fixed cost allocation (DBG-FCA) approach within the data envelopment analysis (DEA) framework to address this issue. The DBG-FCA approach employs an iterative process where each DMU proposes its preferred allocation during each iteration. All DMUs gradually converge on a consensus FCA outcome through dynamic negotiations and gradual compromise. Notably, this consensus upholds the individual rationality of each DMU, allowing them to align their proposals with their specific interests. Furthermore, the analysis establishes that the resulting FCA solution constitutes a Nash equilibrium, guaranteeing stability and universal acceptance among the DMUs. The effectiveness of the proposed approach is further illustrated through a numerical example and a case study involving FCA across 14 bank branches, along with a comparison to existing FCA methods.},
  archive      = {J_EJOR},
  author       = {Junfei Chu and Yanhua Dong and Weijiao Wang and Yuting Rui and Zhe Yuan},
  doi          = {10.1016/j.ejor.2025.03.009},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {317-328},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Data envelopment analysis fixed cost allocation based on dynamic bargaining game and the nash equilibrium},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Profit-based uncertainty estimation with application to credit scoring. <em>EJOR</em>, <em>325</em>(2), 303-316. (<a href='https://doi.org/10.1016/j.ejor.2025.03.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit scoring is pivotal in financial risk management and has attracted significant research interest. While existing studies primarily concentrate on enhancing model predictive power and economic value, they often overlook the crucial aspect of predictive uncertainty, especially in the context of deep neural networks applied to credit scoring. This study addresses uncertainty estimation in credit scoring and evaluates three widely used uncertainty methods across various credit datasets. Additionally, guided by the maximum profit criterion, we propose two profit-based uncertainty metrics to assess profit uncertainties stemming from predictive uncertainty, specifically targeting class-dependent and instance-dependent cost scenarios. Subsequently, we develop a classification system with a rejection mechanism based on these metrics. Our approach aims to improve model profitability and reduce predictive uncertainty, specifically regarding model profit. Empirical results across several benchmark credit datasets indicate that our proposed framework outperforms existing methods in terms of increasing model profit in different credit-scoring scenarios. Furthermore, sensitivity analyses of varying cost parameter settings highlight the robustness of our framework.},
  archive      = {J_EJOR},
  author       = {Yong Xu and Gang Kou and Daji Ergu},
  doi          = {10.1016/j.ejor.2025.03.007},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {303-316},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Profit-based uncertainty estimation with application to credit scoring},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service composition and optimal selection in cloud manufacturing under event-dependent distributional uncertainty of manufacturing capabilities. <em>EJOR</em>, <em>325</em>(2), 281-302. (<a href='https://doi.org/10.1016/j.ejor.2025.03.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service composition and optimal selection in cloud manufacturing involves the allocation of available manufacturing cloud services (MCSs) derived from a diverse array of manufacturing resources to satisfy personalized demand of customers. Existing studies generally neglect the uncertainty of manufacturing capabilities for providing MCSs. To this end, we use an event-dependent hybrid ambiguity set consisting of the box support set, Wasserstein metric, mean, and expected cross-deviation, where the support is conditional on each event, to capture the uncertainty of manufacturing capabilities, and cast the problem as a two-stage distributionally robust optimization model. We provide model bound analysis with theoretical gap guarantees, including the lower and upper bounds derived from the solution of the linear relaxation of the resulting reformulation, and sensitivity bounds for varying some ambiguity-set parameters. To exactly solve the reformulation, we design a customized constraint generation algorithm incorporating some improvement strategies, a variant of classical Benders decomposition, which decomposes the reformulation into a relaxed master problem and an adversarial separation subproblem which identifies valid constraints to tighten the relaxed master problem. Importantly, we transform the bilinear separation subproblem into a 0-1 mixed-integer linear program, observing the property that the linear-relaxed solution is integer, which makes the separation subproblem more easy to solve. Ultimately, we conduct numerical studies on the case study of a group enterprise producing large cement equipment in Tianjin, China, to evaluate the effectiveness of the solution algorithm, quantify the benefits of accounting for event-dependent distributional ambiguity over its single-event counterpart and stochastic and deterministic counterparts, and verify the value of considering the event-dependent hybrid ambiguity set over the Wasserstein and moment counterparts, and measure the quality of the upper and lower bounds and sensitivity bounds.},
  archive      = {J_EJOR},
  author       = {Zunhao Luo and Dujuan Wang and Yunqiang Yin and Joshua Ignatius and T.C.E. Cheng},
  doi          = {10.1016/j.ejor.2025.03.005},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {281-302},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Service composition and optimal selection in cloud manufacturing under event-dependent distributional uncertainty of manufacturing capabilities},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated investment, retrofit and abandonment energy system planning with multi-timescale uncertainty using stabilised adaptive benders decomposition. <em>EJOR</em>, <em>325</em>(2), 261-280. (<a href='https://doi.org/10.1016/j.ejor.2025.04.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the REORIENT (REnewable resOuRce Investment for the ENergy Transition) model for energy systems planning with the following novelties: (1) integrating capacity expansion, retrofit and abandonment planning, and (2) using multi-horizon stochastic mixed-integer linear programming with multi-timescale uncertainty. We apply the model to the European energy system considering: (a) investment in new hydrogen infrastructures, (b) capacity expansion of the European power system, (c) retrofitting oil and gas infrastructures in the North Sea region for hydrogen production and distribution, and abandoning existing infrastructures, and (d) long-term uncertainty in oil and gas prices and short-term uncertainty in time series parameters. We utilise the structure of multi-horizon stochastic programming and propose a stabilised adaptive Benders decomposition to solve the model efficiently. We first conduct a sensitivity analysis on retrofitting costs of oil and gas infrastructures. We then compare the REORIENT model with a conventional investment planning model regarding costs and investment decisions. Finally, the computational performance of the algorithm is presented. The results show that: (1) when the retrofitting cost is below 20% of the cost of building new ones, retrofitting is economical for most of the existing pipelines, (2) platform clusters keep producing oil due to the massive profit, and the clusters are abandoned in the last investment stage, (3) compared with a traditional investment planning model, the REORIENT model yields 24% lower investment cost in the North Sea region, and (4) the enhanced Benders algorithm is up to 6.8 times faster than the level method stabilised adaptive Benders.},
  archive      = {J_EJOR},
  author       = {Hongyu Zhang and Ignacio E. Grossmann and Ken McKinnon and Brage Rugstad Knudsen and Rodrigo Garcia Nava and Asgeir Tomasgard},
  doi          = {10.1016/j.ejor.2025.04.005},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {261-280},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integrated investment, retrofit and abandonment energy system planning with multi-timescale uncertainty using stabilised adaptive benders decomposition},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A budget-adaptive allocation rule for optimal computing budget allocation. <em>EJOR</em>, <em>325</em>(2), 247-260. (<a href='https://doi.org/10.1016/j.ejor.2025.04.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation-based ranking and selection (R&S) is a popular technique for optimizing discrete-event systems (DESs). It evaluates the mean performance of system designs by simulation outputs and aims to identify the best system design from a set of alternatives by intelligently allocating a limited simulation budget. In R&S, the optimal computing budget allocation (OCBA) is an efficient budget allocation rule that asymptotically maximizes the probability of correct selection (PCS). In this paper, we first show the asymptotic OCBA rule can be recovered by considering a large-scale problem with a specific large budget. Considering a sufficiently large budget can greatly simplify computations, but it also causes the asymptotic OCBA rule ignoring the impact of budget. To address this, we then derive a budget-adaptive rule under the setting where budget is not large enough to simplify computations. The proposed budget-adaptive rule determines the ratio of total budget allocated to designs based on the budget size, and its budget-adaptive property highlights the significant impact of budget on allocation strategy. Based on the proposed budget-adaptive rule, two heuristic algorithms are developed. In the numerical experiments, the superior efficiency of our proposed allocation rule is shown.},
  archive      = {J_EJOR},
  author       = {Zirui Cao and Haowei Wang and Ek Peng Chew and Haobin Li and Kok Choon Tan},
  doi          = {10.1016/j.ejor.2025.04.015},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {247-260},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A budget-adaptive allocation rule for optimal computing budget allocation},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical process control for queue length trajectories using fourier analysis. <em>EJOR</em>, <em>325</em>(2), 233-246. (<a href='https://doi.org/10.1016/j.ejor.2025.03.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new statistical process control method for monitoring the number of waiting entities for queues. It is based on dynamic characterization of the number-in-system (NIS) data trajectory via Fourier coefficient magnitudes. Since monitoring periods are necessarily short, we investigate windowing methods for dampening the impact of the Gibbs phenomenon, which can contaminate the Fourier characterization. Secondly, we use this knowledge to present a short-window modified version of the waFm statistic, a weighted average of Fourier magnitudes, within a Cumulative sum (CUSUM) control chart. The waFm CUSUM chart works well even when only periodic NIS reports are available. The proposed method is frequently superior to the best existing methods in controlled experiments considering both non-contiguous and contiguous windows of data illustrating its use for the monitoring of both stationary and non-stationary systems. It is superior to, or competitive with, existing methods even when the nature of departure from control is known. We illustrate performance in simple queues and a more realistic scenario based on a job shop model.},
  archive      = {J_EJOR},
  author       = {Lucy E. Morgan and Russell R. Barton},
  doi          = {10.1016/j.ejor.2025.03.013},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {233-246},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Statistical process control for queue length trajectories using fourier analysis},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scheduling electric vehicle regular charging tasks: A review of deterministic models. <em>EJOR</em>, <em>325</em>(2), 221-232. (<a href='https://doi.org/10.1016/j.ejor.2024.11.044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formulate a fairly general deterministic problem of scheduling electric vehicle (EV) regular charging tasks on parallel chargers over time. Charging task is called regular if it is performed at the same time and in the same place. The charging time and place can be fixed or selectable The scheduling decision time range can be an interval or a circle. Charging tasks may or may not be preemptive. Each charging preemption may imply a setup time and a cost. Each task requires a given amount of energy to be received from the chargers. This energy determines the charging time requirement. The chargers consume electric power which can be limited from above. The objective is to minimize the cost of the chargers and their locations, received energy, maximum power and setups, or a function of task completion times. The problem is typical for urban electric buses with fixed timetables and charging points at depots or along the route. It can also be a part of a more general EV routing and charging scheduling problem, which is often decomposed into the routing and charging scheduling parts in order to reduce computational complexity. Various special cases of this problem have been studied in the literature, in the theoretical and practical contexts. We review and analyze these special cases using traditional scheduling terminology, thereby creating a bridge between theoretical machine scheduling and practical charging scheduling research.},
  archive      = {J_EJOR},
  author       = {Alexandre Dolgui and Sergey Kovalev and Mikhail Y. Kovalyov},
  doi          = {10.1016/j.ejor.2024.11.044},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {221-232},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Scheduling electric vehicle regular charging tasks: A review of deterministic models},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2025 editors’ awards for excellence in reviewing. <em>EJOR</em>, <em>325</em>(2), 219-220. (<a href='https://doi.org/10.1016/j.ejor.2025.03.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EJOR},
  author       = {Roman Słowiński},
  doi          = {10.1016/j.ejor.2025.03.025},
  journal      = {European Journal of Operational Research},
  month        = {9},
  number       = {2},
  pages        = {219-220},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {2025 editors’ awards for excellence in reviewing},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simplicial homology approach for assessing and rectifying coverage of sensor networks for improved crop management. <em>EJOR</em>, <em>325</em>(1), 204-218. (<a href='https://doi.org/10.1016/j.ejor.2025.03.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a mathematical framework and solution approach aimed at enhancing wireless sensor network coverage, specifically focusing on agricultural applications. Sensor networks in precision agriculture can efficiently monitor environmental parameters and control factors affecting crop yield and quality. However, challenges such as sensor failures and communication disruptions due to vegetation interference can hinder achieving complete coverage, leading to reduced productivity. It is therefore necessary to effectively identify, locate, and rectify sensor coverage holes, i.e., areas lacking sensor coverage. To address this, we utilize principles from graph theory, algebraic topology and optimization. Specifically, sensor networks are modeled as Rips complexes, while concepts from simplicial homology and linear programming are used to verify the presence and identify the locations of coverage holes, respectively. By utilizing constructs from abstract simplicial complexes, we then introduce a hole removal heuristic that identifies a minimal number of sensors, along with their locations, that need to be added to the network to achieve complete coverage. It is also shown that the presented framework is adaptable to hybrid sensor networks, where autonomous agents can serve as mobile sensors to remove coverage holes. The approach is validated using extensive numerical simulations for a small farm of 62 acres with 400 sensors and shown that complete sensor coverage can be obtained for network topologies with a varying number and sizes of coverage holes. Key observations pertaining to the performance of the proposed method are drawn from the simulation results.},
  archive      = {J_EJOR},
  author       = {Maciej Rysz and Panos M. Pardalos and Siddhartha S. Mehta},
  doi          = {10.1016/j.ejor.2025.03.010},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {204-218},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A simplicial homology approach for assessing and rectifying coverage of sensor networks for improved crop management},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asset allocation with factor-based covariance matrices. <em>EJOR</em>, <em>325</em>(1), 189-203. (<a href='https://doi.org/10.1016/j.ejor.2025.03.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine whether a factor-based framework to construct the covariance matrix can enhance the performance of minimum-variance portfolios. We conduct a comprehensive comparative analysis of a wide range of factor models, which can differ based on the machine learning dimensionality reduction approach used to construct the latent factors and whether the covariance matrix is static or dynamic. The results indicate that factor models exhibit superior predictive accuracy compared to several covariance benchmarks, which can be attributed to the reduced degree of over predictions. Factor-based portfolios generate statistically significant outperformance with respect to standard deviation and Sharpe ratio relative to multiple portfolio benchmarks. After accounting for transaction costs strategies based on static covariance matrices outperform dynamic specifications in terms of risk-adjusted returns.},
  archive      = {J_EJOR},
  author       = {Thomas Conlon and John Cotter and Iason Kynigakis},
  doi          = {10.1016/j.ejor.2025.03.015},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {189-203},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Asset allocation with factor-based covariance matrices},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing Asia–Europe container network: The suez canal and cape of good hope routes in a changing world. <em>EJOR</em>, <em>325</em>(1), 167-188. (<a href='https://doi.org/10.1016/j.ejor.2025.03.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to develop an approach for evaluating various maritime transportation routes in light of ongoing disruptions and evolving global factors, including changes in demand, fluctuations in fuel prices, geopolitical shifts, and environmental considerations. For each alternative route, a profit maximization liner shipping problem with speed optimization is solved. Both edge-based speed optimization and maximum transit time of commodities are considered. We have proposed a mathematical programming formulation followed by an efficient hybrid approach for the optimization problem. The hybrid approach utilizes a population-based heuristic to optimize the route and an exact algorithm to optimize commodities and speed. A thorough analysis is made on the costs associated with the different alternative routes. Although the approach can be applied to different regions of the world and under the variation of several factors, we focus on the Asia–Europe trade route and on the evolution of the market demand. The comparison is made between a route going through Suez Canal and the alternative route of going through the Cape of Good Hope. Besides the high efficiency of the solution procedure, we have found that the Cape of Good Hope route can be economically interesting especially if there is demand in some African ports and if vessels of less than 20,000 TEU capacity are used.},
  archive      = {J_EJOR},
  author       = {Sadeque Hamdan and Dominique Feillet and Ali Cheaitou and Pierre Cariou and Nadjib Brahimi},
  doi          = {10.1016/j.ejor.2025.03.008},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {167-188},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing Asia–Europe container network: The suez canal and cape of good hope routes in a changing world},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Super conflict resolution approach based on minimum loss considering altruistic behavior and fairness concern. <em>EJOR</em>, <em>325</em>(1), 147-166. (<a href='https://doi.org/10.1016/j.ejor.2025.03.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to difference in strength, power, or status among decision makers (DMs), traditional strategic conflict may be evolved into a super conflict problem where a powerful DM can force others to approach his strategy although other DMs suffer loss. For instance, the government can mobilize relevant enterprises to engage in ecological and environmental governance. Also, DMs may conduct irrational behaviors during negotiations, such as altruistic behavior and fairness concern. To guarantee interests of DMs and sustainable development of decision system, exploring the equilibrium strategy and state that resolves such conflict problem becomes necessary. Therefore, based on theories of conflict analysis, group consensus, Nash bargaining, Berge equilibrium and fairness theory, this paper constructs a super conflict resolution model. Firstly, this paper defines a super conflict information system by introducing the concept of ‘strategy’ and DMs’ information vectors containing DMs’ evaluation values regarding conflict states. Then from the perspective of group negotiation and consensus, we divide the super conflict resolution into three stages and construct a super conflict resolution model based on minimum loss. Considering coalitions’ altruistic behavior and fairness concern, we optimize the model by defining Berge equilibrium and fairness utility function. Finally, a case study of river basin pollution treatment verifies the validity and rationality of the proposed model.},
  archive      = {J_EJOR},
  author       = {Qin Jiang and Yong Liu and Jia-qi An},
  doi          = {10.1016/j.ejor.2025.03.018},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {147-166},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Super conflict resolution approach based on minimum loss considering altruistic behavior and fairness concern},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A nonparametric online control chart for monitoring crowd density using relative density-ratio estimation. <em>EJOR</em>, <em>325</em>(1), 132-146. (<a href='https://doi.org/10.1016/j.ejor.2025.03.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing incidence of fatal crowd stampede disasters at public events due to rapid urbanization and escalating population density, there is an urgent need for real-time monitoring of crowd flow to prevent such tragic incidents. This paper proposes a novel nonparametric online control chart for crowd density monitoring, called the RDR-CUSUM chart. This chart utilizes a new statistic based on the principles of cumulative sum (CUSUM) and relative density ratio (RDR). We present an RDR estimation method for deriving this statistic, which is enhanced by an S-M Invertor algorithm to ensure the efficiency required for real-time application. Numerical analyses demonstrate that the proposed chart can quickly respond to the distribution’s mean, variance, pattern, and different distribution parameter shifts. Moreover, the effectiveness of the method has been validated through two application examples, which illustrate its proficiency in detecting changes in crowd density and providing early warnings of potential crowd stampede disasters.},
  archive      = {J_EJOR},
  author       = {Wenhui Zhou and Yibin Xie and Zhibin Zheng},
  doi          = {10.1016/j.ejor.2025.03.006},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {132-146},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A nonparametric online control chart for monitoring crowd density using relative density-ratio estimation},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heteroscedasticity-aware stratified sampling to improve uplift modeling. <em>EJOR</em>, <em>325</em>(1), 118-131. (<a href='https://doi.org/10.1016/j.ejor.2025.02.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized controlled trials (RCTs) are conducted in many business applications including online marketing or customer churn prevention to investigate the effect of specific treatments (coupons, retention offers, mailings, etc.). RCTs allow for the estimation of average treatment effects and the training of (uplift) models for the heterogeneity of treatment effects across individuals. The problem with RCTs is that they are costly, and this cost increases with the number of individuals included. These costs have inspired research on how to conduct experiments with a small number of individuals while still obtaining precise treatment effect estimates. We contribute to this literature a heteroskedasticity-aware stratified sampling (HS) scheme. We leverage the fact that different individuals have different noise levels in their outcome and that precise treatment effect estimation requires more observations from the “high-noise” individuals than from the “low-noise” individuals. We show theoretically and empirically that HS sampling yields significantly more precise estimates of the ATE, improves uplift models, and makes their evaluation more reliable compared to RCT data sampled completely randomly. Due to these benefits and the simplicity of our approach, we expect HS sampling to be valuable in many real-world applications in business and beyond.},
  archive      = {J_EJOR},
  author       = {Björn Bokelmann and Stefan Lessmann},
  doi          = {10.1016/j.ejor.2025.02.030},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {118-131},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Heteroscedasticity-aware stratified sampling to improve uplift modeling},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A logic-based benders decomposition approach for a fuel delivery problem with time windows, unsplit compartments, and split deliveries. <em>EJOR</em>, <em>325</em>(1), 100-117. (<a href='https://doi.org/10.1016/j.ejor.2025.03.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a single-period fuel delivery problem in which a distribution company has to transport multiple types of fuel from a depot to a set of service stations using a heterogeneous set of multi-compartment vehicles. Among other characteristics, the problem includes time windows, a limit on the duration of each route, unsplit compartments, and split deliveries. We propose two mixed integer programming (MIP) formulations and a logic-based Benders decomposition approach. The first formulation is an arc-based model while the second is based on the possible trips. The logic-based Benders decomposition follows a trip-based principle and breaks down the problem into a generalized assignment master problem and a subproblem responsible for identifying violated feasibility cuts implicated by the time-related constraints. It takes advantage of the problem-specific characteristics that allow efficiently solving the resulting subproblems. The logic-based Benders decomposition also serves as a heuristic, which works by limiting the number of trips generated throughout the process. Symmetry breaking constraints and preprocessing procedures are also proposed to help solving the formulations. The computational experiments using synthetic instances show that the logic-based Benders decomposition outperforms the other formulations and is very effective in solving the considered benchmark instances. It solved to optimality instances with up to 20 customers. The MIP heuristic obtained solutions within 7.2% of the optimal cost for all but one of the tested instances with up to 25 customers. Furthermore, it proved to be a viable approach for medium-sized instances where the exact logic-based Benders decomposition encountered difficulties.},
  archive      = {J_EJOR},
  author       = {Rafael A. Melo and Celso C. Ribeiro and Sebastián Urrutia and Pieter Vansteenwegen},
  doi          = {10.1016/j.ejor.2025.03.003},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {100-117},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A logic-based benders decomposition approach for a fuel delivery problem with time windows, unsplit compartments, and split deliveries},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cyclic stochastic two-echelon inventory routing for an application in medical supply. <em>EJOR</em>, <em>325</em>(1), 81-99. (<a href='https://doi.org/10.1016/j.ejor.2025.02.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug availability in clinics is essential for patient services, whose demand for medication is uncertain. Thus, clinics must have a variety of drugs available, leading to high inventory holding costs. In Germany, it is common for a larger central clinic to take over the procurement of drugs and distribute them to smaller surrounding clinics, which results in a two-echelon network structure. The clinics, however, operate according to their inventory policy as they plan independently. Additionally, the inventory policies include instant replenishment orders to avoid shortages, which can be executed by various vehicles, such as vans or aerial drones, because the orders only involve a few medications. We present a two-stage stochastic program for a multi-product two-echelon inventory routing problem with stochastic demands. We decide on the cost-optimal cyclic delivery patterns and reorder points for the clinics with instant replenishment orders as recourse decision. Further, we introduce an adaptive large neighborhood search with problem-specific operators that modify the routing, delivery periods, and reorder points. We present a case study at a large German clinic that supplies multiple surrounding clinics and plans to integrate drone instead of van deliveries for emergency resupply. Our integrated approach leads to cost savings of 57% for the surrounding clinics and 18% for the central clinic. Using drone delivery compared to van delivery, the average stock of medication at surrounding clinics can be reduced, resulting in a total cost decrease of 29% while maintaining medication availability.},
  archive      = {J_EJOR},
  author       = {Alexander Rave and Pirmin Fontaine and Heinrich Kuhn},
  doi          = {10.1016/j.ejor.2025.02.032},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {81-99},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Cyclic stochastic two-echelon inventory routing for an application in medical supply},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The robust bike sharing rebalancing problem: Formulations and a branch-and-cut algorithm. <em>EJOR</em>, <em>325</em>(1), 67-80. (<a href='https://doi.org/10.1016/j.ejor.2025.02.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bike Sharing Systems (BSSs) offer a sustainable and efficient urban transportation solution, bringing flexible and eco-friendly alternatives to city logistics. During their operation, BSSs may suffer from unbalanced bike distribution among stations, requiring rebalancing operations throughout the system. The inherent uncertain demand at the stations further complicates these rebalancing operations, even when performed during downtime. This paper addresses this challenge by introducing the Robust Bike Sharing Rebalancing Problem (RBRP), which relies on Robust Optimization techniques to promote better decisions in rebalancing operations in BSSs. Very few studies have considered uncertainty in this context, despite it being a common characteristic with a significant impact on the performance of the system. We present two new formulations and a tailored branch-and-cut algorithm for the RBRP. The first formulation is compact and based on the linearization of recursive equations, while the second is based on robust rounded capacity inequalities and feasibility cuts. Computational results based on benchmark instances indicate the effectiveness of our approaches to face uncertain demand in rebalancing operations and highlight the benefits of using robust solutions to support decision-making in this context.},
  archive      = {J_EJOR},
  author       = {Bruno P. Bruck and Walton P. Coutinho and Pedro Munari},
  doi          = {10.1016/j.ejor.2025.02.029},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {67-80},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The robust bike sharing rebalancing problem: Formulations and a branch-and-cut algorithm},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bi-objective unrelated parallel machine scheduling problem with additional resources and soft precedence constraints. <em>EJOR</em>, <em>325</em>(1), 53-66. (<a href='https://doi.org/10.1016/j.ejor.2025.03.019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates an unrelated parallel machine scheduling problem with practical production constraints, where the upper limit of additional resources varies across distinct time intervals, and the precedence relationships between jobs can be violated by paying specified penalty costs. We formulate a novel mixed integer linear programming model to minimize the makespan and total penalty cost simultaneously. To tackle this problem, we utilize the framework of the multi-objective evolutionary algorithm based on decomposition, which decomposes the original problem into multiple scalar subproblems. In solving each subproblem, we propose three decoding algorithms to explore different solution spaces in the Pareto front and design a cyclic decoding mechanism inspired by the greedy idea. Furthermore, a 2-swap local search strategy is applied in the evolutionary process to enhance the proposed algorithm. Computational experiments on extensive numerical instances indicate that the cyclic decoding mechanism performs better than the single decoding algorithm. The results of small-scale instances show that the evolutionary algorithms, regardless of using the 2-swap local search, outperform the MILP model in terms of computational efficiency when achieving the optimal Pareto front. For large-scale instances, applying the 2-swap local search strategy significantly enhances the quality of the Pareto front, albeit at the cost of a substantial increase in computational time. The results also demonstrate the superior effectiveness and efficiency of the proposed algorithm compared to NSGA-II.},
  archive      = {J_EJOR},
  author       = {Mengxing Gao and ChenGuang Liu and Xi Chen},
  doi          = {10.1016/j.ejor.2025.03.019},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {53-66},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A bi-objective unrelated parallel machine scheduling problem with additional resources and soft precedence constraints},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptation, comparison and practical implementation of fairness schemes in kidney exchange programs. <em>EJOR</em>, <em>325</em>(1), 38-52. (<a href='https://doi.org/10.1016/j.ejor.2025.02.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kidney Exchange Programs (KEPs) typically maximize overall patient benefit through donor exchanges. This aggregation of benefits (utilitarian objective) calls into question potential individual patient disparities in terms of access to transplantation in KEPs. Moreover, current KEP policies are all-or-nothing, meaning that only one exchange plan is determined — each patient is either selected or not as part of that unique solution. In this work, we extend the space of policies by seeking a lottery over the set of exchange plans that contemplates the (ex-ante) probability of patients being in a solution. To guide the determination of our policy, we adapt popular fairness schemes to KEPs to balance the usual approach of maximizing the utilitarian objective. Different combinations of fairness and utilitarian objectives are modeled as conic programs with an exponential number of variables. We propose a column generation approach to solve them effectively in practice. Finally, we make an extensive comparison of the different schemes in terms of the balance of utility and fairness score, and validate the scalability of our methodology for benchmark instances from the literature.},
  archive      = {J_EJOR},
  author       = {William St-Arnaud and Margarida Carvalho and Golnoosh Farnadi},
  doi          = {10.1016/j.ejor.2025.02.014},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {38-52},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Adaptation, comparison and practical implementation of fairness schemes in kidney exchange programs},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Patient-to-room assignment with single-rooms entitlements: Combinatorial insights and integer programming formulations. <em>EJOR</em>, <em>325</em>(1), 20-37. (<a href='https://doi.org/10.1016/j.ejor.2025.02.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient-to-room assignment (PRA) is a scheduling problem in decision support for hospitals. It consists of assigning patients to rooms during their stay at a hospital according to certain conditions and objectives, e.g., ensuring gender separated rooms, avoiding transfers and respecting single-room requests. This work presents combinatorial insights about the feasibility of PRA and about how (many) single-room requests can be respected. We further compare different integer programming (IP) formulations for PRA as well as the influence of different objectives on the runtime using real-world data. Based on these results, we develop a fast IP-based solution approach, which obtains high quality solutions. In contrast to previous IP-formulations, the results of our computational study indicate that large, real-world instances can be solved to a high degree of optimality within (fractions of) seconds. We support this result by a computational study using a large set of realistic but randomly generated instances with 50% to 95% capacity utilisation.},
  archive      = {J_EJOR},
  author       = {Tabea Brandt and Christina Büsing and Felix Engelhardt},
  doi          = {10.1016/j.ejor.2025.02.018},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {20-37},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Patient-to-room assignment with single-rooms entitlements: Combinatorial insights and integer programming formulations},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transportation and delivery in flow-shop scheduling problems: A systematic review. <em>EJOR</em>, <em>325</em>(1), 1-19. (<a href='https://doi.org/10.1016/j.ejor.2024.11.034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a literature review of flow-shop scheduling problems with transportation or delivery of jobs. Flow-shop scheduling problems are one of the most widely studied optimisation problems in the literature on Operations Research. Although these have traditionally been studied assuming negligible or constant transport times, this does not correspond to real manufacturing scenarios in the industry. In fact, the extensive automation and synchronisation demanded by Industry 4.0 may well be a driving factor in the growing interest in the literature on flow-shop scheduling problems with transport constraints. Despite this interest, the literature is disjointed, and many terms have been used interchangeably. This review aims to organise the literature on the topic and propose a new notation for these problems. This contribution is expected to help structure advancements in the field, classifying them by problem type. Furthermore, a detailed study is carried out on the complexity and relationship between different variants. This provides a representation of the advances discovered in the literature while also demonstrating new theoretical results, before finally identifying the most promising research directions.},
  archive      = {J_EJOR},
  author       = {Victor Fernandez-Viagas},
  doi          = {10.1016/j.ejor.2024.11.034},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Transportation and delivery in flow-shop scheduling problems: A systematic review},
  volume       = {325},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Platform acquisition in a triple-channel supply chain. <em>EJOR</em>, <em>324</em>(3), 1035-1046. (<a href='https://doi.org/10.1016/j.ejor.2025.02.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the observation that some online retail platforms begin to enter offline channels by acquiring traditional retailers, this paper studies the impact of platform acquisition in a context where a manufacturer sells a product via an online agency selling and two offline reselling channels. Considering three changes brought by platform acquisition in practice, namely, showrooming, “pay online but pickup in store (POPS)” and pricing power transfer effects, we investigate how platform acquisition impacts the firms’ pricing behaviors and profits, customer surpluses, and social welfare. Our analysis shows that, although acquisition may hurt the upstream manufacturer and the non-acquired traditional retailer, it always makes the platform and the acquired traditional retailer better off as a whole. Further, it may benefit the total supply chain and improve consumer surpluses and social welfare. Interestingly, platform acquisition leads to a demand siphon force (shifting demand from the non-acquired channel to the two channels related to acquisition) when platform acquisition brings strong showrooming and POPS effects; otherwise, it leads to the opposite demand spillover force. Finally, regarding the decision on the acquisition fee, there exists a moderate range that enables the platform and offline retailer to achieve a win-win situation. These findings collectively provide some valuable insights for decision-makers to understand the impacts of platform acquisition in a competitive triple-channel supply chain.},
  archive      = {J_EJOR},
  author       = {Xiaoran Liu and Lusheng Shao and Xuwei Qin},
  doi          = {10.1016/j.ejor.2025.02.033},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {1035-1046},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Platform acquisition in a triple-channel supply chain},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Should blockchain be used to eliminate greenwashing for green and live-streaming platform operations under carbon trading systems?. <em>EJOR</em>, <em>324</em>(3), 1017-1034. (<a href='https://doi.org/10.1016/j.ejor.2025.02.017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines a supply chain comprising a live-streaming platform and a manufacturer that sells its products via the platform in the agency or resale mode. Under the carbon trading system, the manufacturer adopts green technology and exhibits the greenwashing behaviour, and the platform chooses whether to use blockchain to eliminate this behaviour. We find that the existence of greenwashing generates a higher profit for the platform in the agency mode. In the resale mode, the existence of greenwashing generates a higher (lower) profit for the platform if the additional profit brought by blockchain is low (high) at a low or moderate cap. However, when the cap is high, the existence of greenwashing hurts the platform’s profit. Furthermore, considering the government’s decision, we find that the existence of greenwashing hurts (improves) the social welfare at a high (low) correlation coefficient between the carbon trading price and potential market demand.},
  archive      = {J_EJOR},
  author       = {Xiaoping Xu and Xinru Chen and T.C.E. Cheng and Tsan-Ming Choi and Yuanyuan Yang},
  doi          = {10.1016/j.ejor.2025.02.017},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {1017-1034},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Should blockchain be used to eliminate greenwashing for green and live-streaming platform operations under carbon trading systems?},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stow & pick: Optimizing combined stowing and picking tours in scattered storage warehouses. <em>EJOR</em>, <em>324</em>(3), 1002-1016. (<a href='https://doi.org/10.1016/j.ejor.2025.02.037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To streamline order fulfillment, many e-commerce retailers apply scattered storage in their picker-to-parts warehouses. Instead of putting homogeneous unit loads into racks, they break up the pallets and store individual products within mixed shelves. This increases the probability that (somewhere in the vast warehouses) products, ending up jointly on hardly predictable pick lists, can be picked from neighboring shelves. Scattered storage promises a reduction of picker travel but comes at the price of additional stowing effort. Instead of applying separate workforces, stowing and picking can also be combined. This paper provides an exact routing algorithm for a worker with a picking cart who departs from the depot with multiple bins filled with products to stow, switches to picking underway, and returns bins full of picked products to the depot. We identify nine different combined stowing and picking policies. We provide an exact solution procedure based on the branch&bound paradigm for all these policies, based on the parallel-aisle structure of warehouses. If a constant limits the number of relevant regions (i.e., picking aisle subsections between adjacent cross aisles with a storage position to be visited), this algorithm guarantees polynomial runtime. We apply our algorithm to benchmark the gains of the nine combined stowing and picking policies compared to the traditional approach, where stowing and picking are executed by separate workforces. Depending on the selected policy, average gains between 11.8 and 42.0 % arise.},
  archive      = {J_EJOR},
  author       = {Stefan Bock and Nils Boysen},
  doi          = {10.1016/j.ejor.2025.02.037},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {1002-1016},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Stow & pick: Optimizing combined stowing and picking tours in scattered storage warehouses},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving a multi-resolution model of the train platforming problem using lagrangian relaxation with dynamic multiplier aggregation. <em>EJOR</em>, <em>324</em>(3), 981-1001. (<a href='https://doi.org/10.1016/j.ejor.2025.03.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-speed railway stations are crucial junctions in high-speed railway networks. Compared to operations on the tracks between stations, trains have more routing possibilities within stations. As a result, track allocation at a station is relatively complicated. In this study, we aim to solve the train platforming problem for a busy high-speed railway station by considering comprehensive track resources and interlocking configurations. A multi-resolution space–time network is constructed to capture infrastructure information from a macroscopic and a microscopic perspective. Additionally, we propose a nonlinear programming model that minimizes a weighted sum of total travel time and total deviation time for trains at the station. We apply Lagrangian Relaxation combined with dynamic multiplier aggregation to a linearized version of the model and demonstrate how this induces a decomposable, macroscopic train-specific path choice problem that is guided by aggregated Lagrange multipliers, which are dynamically generated based on microscopic resource capacity violations. As case studies, the proposed model and solution approach are applied to a small virtual railway station and two high-speed railway hub stations located on two of the busiest high-speed railway lines in China. Through a comparison of other approaches that include Logic-based Benders Decomposition, we highlight the superiority of the proposed method; on realistic instances, the proposed method finds solution that are, on average, approximately 2% from optimality for one station and less than 5% from optimality for the other.},
  archive      = {J_EJOR},
  author       = {Qin Zhang and Richard Martin Lusby and Pan Shang and Chang Liu and Wenqian Liu},
  doi          = {10.1016/j.ejor.2025.03.004},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {981-1001},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Solving a multi-resolution model of the train platforming problem using lagrangian relaxation with dynamic multiplier aggregation},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). It’s all in the mix: Technology choice between driverless and human-driven vehicles in sharing systems. <em>EJOR</em>, <em>324</em>(3), 969-980. (<a href='https://doi.org/10.1016/j.ejor.2025.02.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operators of vehicle-sharing systems such as carsharing or ride-hailing can benefit from integrating driverless vehicles into their fleet. In this context, we study the impact of optimal fleet size and composition on an operator’s profitability, which entails a non-trivial tradeoff between operational benefits and higher upfront investment for driverless vehicles. We analyze a strategic fleet sizing and composition problem, integrating a rebalancing problem, which we formalize as a Markov decision process. We incorporate the rebalancing problem with a time-dependent fluid approximation to devise a scalable linear programming solution approach, which we improve by state-dependent emergency rebalancing. We present a numerical study on artificial and real-world instances that reveals significant profit improvement potential of driverless and mixed fleets compared to human-driven fleets. For real-world instances, the profit improvement amounts up to 20.4% over exclusively human-driven fleets. If both vehicle types incur equal operational costs, operators optimally mix a small number of driverless vehicles with a large number of human-driven vehicles. Mixed fleets are particularly beneficial if demand varies over time, and operators consequently shift rebalancing to lower-demand periods.},
  archive      = {J_EJOR},
  author       = {Layla Martin and Stefan Minner and Marco Pavone and Maximilian Schiffer},
  doi          = {10.1016/j.ejor.2025.02.004},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {969-980},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {It’s all in the mix: Technology choice between driverless and human-driven vehicles in sharing systems},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preference learning for efficient bundle selection in horizontal transport collaborations. <em>EJOR</em>, <em>324</em>(3), 953-968. (<a href='https://doi.org/10.1016/j.ejor.2025.02.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve routing efficiency, transport service providers can enter a horizontal transport collaboration that uses a combinatorial auction to reallocate delivery orders. To find the optimal allocation, the carriers have to report bids for all possible combinations of available delivery orders. As this number grows exponentially with the number of orders to be reallocated, they are faced with an enormous computational challenge. To lift this burden, the auctioneer may offer only a limited set of order combinations. However, selecting this limited set is itself a stochastic combinatorial optimization problem known as the Bundle Selection Problem. In contrast to previous one-shot approaches to solve this problem, in this paper, a partial preference learning scheme is applied that iteratively queries carriers’ valuations, uses their responses to train preference models and then uses these fitted models to estimate valuations for new combinations of orders. This work investigates different ways to realize such a concept and analyzes their respective improvement in collaboration gains. The results indicate that the suggested algorithm can yield travel time savings of up to 20% higher than those achieved by a random benchmark and up to 10% higher than those of a literature benchmark if at least 40 query-response pairs are considered.},
  archive      = {J_EJOR},
  author       = {Steffen Elting and Jan Fabian Ehmke and Margaretha Gansterer},
  doi          = {10.1016/j.ejor.2025.02.002},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {953-968},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Preference learning for efficient bundle selection in horizontal transport collaborations},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized strategyproof mechanisms with best of both worlds fairness and efficiency. <em>EJOR</em>, <em>324</em>(3), 941-952. (<a href='https://doi.org/10.1016/j.ejor.2025.02.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of mechanism design for allocating a set of indivisible items among agents with private preferences on items. We aim to design a mechanism that is strategyproof (in which agents find it optimal to report their true preferences) and ensures a certain level of fairness and efficiency. We first establish that no deterministic mechanism can simultaneously be strategyproof, fair, and efficient for the allocation of indivisible chores. We then introduce randomness to address this impossibility. For allocating indivisible chores, we propose randomized mechanisms that are strategyproof in expectation as well as ex-ante and ex-post (best of both worlds) fair and efficient. For allocating mixed items—where an item may be a good (positive utility) for one agent and a chore (negative utility) for another, we propose randomized mechanisms that are strategyproof in expectation while ensuring fairness and efficiency for two-agent scenarios.},
  archive      = {J_EJOR},
  author       = {Ankang Sun and Bo Chen},
  doi          = {10.1016/j.ejor.2025.02.027},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {941-952},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Randomized strategyproof mechanisms with best of both worlds fairness and efficiency},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Streamlining emergency response: A K-adaptable model and a column-and-constraint-generation algorithm. <em>EJOR</em>, <em>324</em>(3), 925-940. (<a href='https://doi.org/10.1016/j.ejor.2025.02.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency response refers to the systematic response to an unexpected, disruptive occurrence such as a natural disaster. The response aims to mitigate the consequences of the occurrence by providing the affected region with the necessary supplies. A critical factor for a successful response is its timely execution, but the unpredictable nature of disasters often prevents quick reactionary measures. Preallocating the supplies before the disaster takes place allows for a faster response, but requires more overall resources because the time and place of the disaster are not yet known. This gives rise to a trade-off between how quickly a response plan is executed and how precisely it targets the affected areas. Aiming to capture the dynamics of this trade-off, we develop a K -adjustable robust model, which allows a maximum of K second-stage decisions, i.e., response plans. This mitigates tractability issues and allows the decision-maker to seamlessly navigate the gap between the readiness of a proactive yet rigid response and the accuracy of a reactive yet highly adjustable one. The approaches we consider to solve the K -adaptable model are twofold: Via a branch-and-bound method as well as a static robust reformulation in combination with a column-and-constraint generation algorithm. In a computational study, we compare and contrast the different solution approaches and assess their potential.},
  archive      = {J_EJOR},
  author       = {Paula Weller and Fabricio Oliveira},
  doi          = {10.1016/j.ejor.2025.02.016},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {925-940},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Streamlining emergency response: A K-adaptable model and a column-and-constraint-generation algorithm},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can blockchain implementation combat food fraud: Considering consumers’ delayed quality perceptions. <em>EJOR</em>, <em>324</em>(3), 908-924. (<a href='https://doi.org/10.1016/j.ejor.2025.02.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food fraud is driven by unethical enterprises’ economic incentives and endures due to consumers’ delayed quality perceptions, while present solutions make it impossible for ethical firms to verify food quality in a timely and convincing manner. To that end, this paper focuses on a duopoly competition between an ethical firm (H) and an unethical firm (L) incorporating consumers’ delayed quality perceptions. Considering the industry credibility crisis that food fraud may trigger, we analytically explore the conditions and effects of firm H to combat food fraud with the aid of blockchain technology (BCT). Counterintuitively, this paper finds that quality improvements beyond industry standards are not always beneficial to firms. Indeed, firms H and L will adopt quality improvements only when marginal returns are higher or delayed perceived time of quality (DPTQ) is longer, respectively. When deciding whether to adopt BCT, H should consider not only the implementation cost and DPTQ, but also the non-monotonic impact of DPTQ on the cost threshold. In addition, it is found that the application of BCT will always motivate H to make quality improvements and further enhance the goodwill, demand, and profits, but that the effect of combating food fraud is stage-specific. Within DPTQ, BCT will only serve as a marketing tool to enhance the competitiveness of H. It is outside of DPTQ that BCT can be effective in combating food fraud, not only by accelerating L's exit from the market, but also by increasing H's market share at the same time.},
  archive      = {J_EJOR},
  author       = {Deqing Ma and Xueping Wu and Kaifu Li and Jinsong Hu},
  doi          = {10.1016/j.ejor.2025.02.028},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {908-924},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Can blockchain implementation combat food fraud: Considering consumers’ delayed quality perceptions},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Product design and pricing decisions in platform-based co-creation. <em>EJOR</em>, <em>324</em>(3), 893-907. (<a href='https://doi.org/10.1016/j.ejor.2025.02.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-creation, a new business model that requires platform enterprises, manufacturers, and even consumers to participate in product research and development, has become increasingly popular in recent years. Simultaneously, technological advances in platforms have provided a convenient channel for consumers to contribute creative ideas in co-creation activities, providing an opportunity to rebuild business models. In this study, we consider a setting where a platform enterprise, a manufacturer, and a group of consumers jointly design and produce a co-created product. We focus on how a platform impacts co-creation. In the co-creation process, the platform integrates the consumers’ ideas and chooses a product innovation design (i.e., either an aesthetic-oriented or a functionality-oriented product design), after which the manufacturer sells the co-created product in a heterogeneous market. We demonstrate that each type of product innovation design has its own scope of application, and neither is strictly dominant. We find that when the product value exerted by consumers is higher than a certain threshold, a lose-lose situation may occur for the profits of the platform and manufacturer. Furthermore, we endogenize the consumers’ effort decisions in co-creation and find that each type of product innovation design can still be the equilibrium strategy.},
  archive      = {J_EJOR},
  author       = {Siyuan Zhu and Tengfei Nie and Jianghua Zhang and Shaofu Du},
  doi          = {10.1016/j.ejor.2025.02.015},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {893-907},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Product design and pricing decisions in platform-based co-creation},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated assessment of a robust choquet integral preference model for efficient multicriteria decision support. <em>EJOR</em>, <em>324</em>(3), 871-892. (<a href='https://doi.org/10.1016/j.ejor.2025.02.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision problems are often characterized by complex criteria dependencies, which can hamper the development of an efficient and theoretically accurate multicriteria decision aid model. These criteria interactions have the form of either a redundancy or synergistic effect and require arduous and demanding preference statements for their quantification. This paper investigates interactions between pairs of criteria in decision models and addresses them with the proposition of an MCDA framework, coupling the elicitation protocol of the method of cards and the 2-additive Choquet integral preference model. An interactive robustness control algorithm ensures the concurrent acquisition of a stable decision model and satisfactory evaluation results. Robustness is assessed with a portfolio of robustness indicators, spanning from the variability of the preference parameters to the reduction of the model's feasible space and rank acceptability indices. At the core of the algorithm, a heuristic module generates pairwise elicitation questions and selects those delivering the highest expected information gain. The whole framework is stress-tested with a small-scale decision problem, where three versions of the heuristics are automatically applied, with the machine randomly answering the questions. Subsequently, the same problem is approached with the involvement of a real decision maker, with a view to appraising the required cognitive effort and receiving valuable feedback.},
  archive      = {J_EJOR},
  author       = {Eleftherios Siskos and Antoine Desbordes and Peter Burgherr and Russell McKenna},
  doi          = {10.1016/j.ejor.2025.02.011},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {871-892},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integrated assessment of a robust choquet integral preference model for efficient multicriteria decision support},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural feedback and behavioral decision making in queuing systems: A hybrid simulation framework. <em>EJOR</em>, <em>324</em>(3), 855-870. (<a href='https://doi.org/10.1016/j.ejor.2025.02.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional queuing models mostly leave human judgment and decision making outside the scope of the system, ignoring their role as determinants of system performance. However, empirical evidence has shown that human behavior can substantially alter the system’s output. In this paper, we develop a hybrid approach that improves our understanding of the interplay between individual heterogeneous human agents and aggregate system behavior. We formulate human behavioral responses as feedback control processes, explicitly capturing the agent’s objectives and available information about the system’s state, accounting for delays and possible distortions. Our modeling approach taps into a behavioral modeling tradition that values realism and representativeness, making the formulations flexible and easily adaptable to specific situations. We illustrate our approach by considering a queuing system with delay announcement, commonly found in service and manufacturing settings. We find that the system continuously cycles between periods of low and high utilization, creating a suboptimal mode with predictable periods of high and low congestion and fewer customers served overall. By structuring the effect of behavioral responses as feedback loops, we formally analyze the observed system behavior and map it to behavioral decisions. The proposed modeling and analysis framework can guide system design and improve performance in scenarios where key dynamics are driven by both feedback structure and stochasticity. It provides generalizable structural explanations of the impact of human behavior in queuing systems.},
  archive      = {J_EJOR},
  author       = {Sergey Naumov and Rogelio Oliva},
  doi          = {10.1016/j.ejor.2025.02.010},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {855-870},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Structural feedback and behavioral decision making in queuing systems: A hybrid simulation framework},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective cooperative co-evolution algorithm with hypervolume-based Q-learning for hybrid seru system. <em>EJOR</em>, <em>324</em>(3), 839-854. (<a href='https://doi.org/10.1016/j.ejor.2025.02.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hybrid seru system (HSS), which is an innovative production pattern that emerges from real-world production situations, is practical because it includes both serus and a flow line, allowing temporary workers who are unable to complete all tasks to be assigned to the flow line . We focus on the HSS by minimising both makespan and total labour time. The HSS includes two complicated coupled NP-hard subproblems: hybrid seru formation and hybrid seru scheduling. Thus, we developed a multi-objective cooperative co-evolution algorithm with hypervolume-based Q-learning (MOCC HVQL) involving hybrid seru formation and scheduling subpopulations, evolved using a genetic algorithm. To achieve balance between exploration and exploitation, a hypervolume-based Q-learning mechanism is proposed to adaptively adjust the number of non-dominated hybrid seru formations/scheduling in co-evolution. To reduce computational time and enhance population diversity, a population partitioning mechanism is proposed. Extensive comparative results demonstrate that the MOCC HVQL outperforms state-of-the-art algorithms in terms of solution convergence and diversity, with the hypervolume metric increasing by 22 % and inverse generational distance metric decreasing by 76 %. Compared with a pure seru system (PSS), the HSS can significantly reduce training tasks, thereby conserving the training budget. In scenarios with fewer workers and more batches, a positive phenomenon, where the HSS significantly decreases the training tasks relative to PSS while only slightly increasing the makespan, was observed. In specific instances, the HSS reduced the number of training tasks by 50 %, while only increasing the makespan by 10.5 %.},
  archive      = {J_EJOR},
  author       = {Zhecong Zhang and Yang Yu and Xuqiang Qi and Yangguang Lu and Xiaolong Li and Ikou Kaku},
  doi          = {10.1016/j.ejor.2025.02.025},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {839-854},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multi-objective cooperative co-evolution algorithm with hypervolume-based Q-learning for hybrid seru system},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving soft and hard-clustered vehicle routing problems: A bi-population collaborative memetic search approach. <em>EJOR</em>, <em>324</em>(3), 825-838. (<a href='https://doi.org/10.1016/j.ejor.2025.02.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The soft-clustered vehicle routing problem is a natural generalization of the classic capacitated vehicle routing problem, where the routing decision must respect the already taken clustering decisions. It is a relevant routing problem with numerous practical applications, such as packages or parcels delivery. Population-based evolutionary algorithms have already been adapted to solve this problem. However, they usually evolve a single population and suffer from early convergence especially for large instances, resulting in sub-optimal solutions. To maintain a high diversity so as to avoid premature convergence, this work proposes a bi-population collaborative memetic search method that adopts a bi-population structure to balance between exploration and exploitation, where two populations are evolved in a cooperative way. Starting from an initial population generated by a data-driven and knowledge-guided population initialization, two heterogeneous memetic searches are then performed by employing a pair of complementary crossovers (i.e., a multi-route edge assembly crossover and a group matching-based crossover) to generate offspring solutions, and a bilevel variable neighborhood search to explore the solution space at both cluster and customer levels. Once the two evolved new populations are obtained, a cooperative evolution mechanism is applied to obtain a new population. Extensive experiments on 404 benchmark instances show that the proposed algorithm significantly outperforms the current state-of-the-art algorithms. In particular, the proposed algorithm discovers new upper bounds for 16 out of the 26 large-sized benchmark instances, while matching the best-known solutions for the remaining 9 large-sized instances. Ablation experiments are conducted to verify the effectiveness of each key algorithmic module. Finally, the inherent generality of the proposed method is verified by applying it to the well-known (hard) clustered vehicle routing problem.},
  archive      = {J_EJOR},
  author       = {Yangming Zhou and Lingheng Liu and Una Benlic and Zhi-Chun Li and Qinghua Wu},
  doi          = {10.1016/j.ejor.2025.02.021},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {825-838},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Solving soft and hard-clustered vehicle routing problems: A bi-population collaborative memetic search approach},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A weighted distribution-free model for parallel machine scheduling with uncertain job processing times. <em>EJOR</em>, <em>324</em>(3), 814-824. (<a href='https://doi.org/10.1016/j.ejor.2024.12.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a parallel machine scheduling and due date assignment problem with uncertain job processing times (JPTs) under the earliness and tardiness criteria. To mitigate the risk of over-conservative scheduling, we propose a weighted distribution-free model that considers both the worst-case and best-case expected costs when only the mean and variance of the JPTs are available. We derive the optimal weight such that the maximal regret value is minimized, and show that the optimal job assignment and sequence decisions do not rely on the weight. For identical parallel machine scheduling, we establish the optimality of an extended smallest-variance-first rule. For non-identical parallel machine scheduling, we provide an equivalent mixed 0-1 second-order conic programming reformulation and develop a two-phase approximation algorithm that integrates a greedy algorithm with a supermodularity-based random search (SRS). We show that the approximation ratio of the greedy algorithm is m , where m is the number of machines. We also provide performance guarantees for the SRS. Numerical experiments validate the effectiveness of the proposed model and demonstrate that the proposed algorithms are capable of producing near-optimal schedules.},
  archive      = {J_EJOR},
  author       = {Yuli Zhang and Zihan Cheng and Ningwei Zhang and Raymond Chiong},
  doi          = {10.1016/j.ejor.2024.12.027},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {814-824},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A weighted distribution-free model for parallel machine scheduling with uncertain job processing times},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing omnichannel assortments and inventory provisions under the multichannel attraction model. <em>EJOR</em>, <em>324</em>(3), 799-813. (<a href='https://doi.org/10.1016/j.ejor.2025.01.035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assortment optimization presents a complex challenge for retailers, as it depends on numerous decision factors. Changes in assortment can result in demand redistribution with multi-layered consequences. This complexity is even more pronounced for omnichannel retailers, which have to manage assortments across multiple sales channels. Choice modeling has emerged as an effective method in assortment optimization, capturing customer shopping behavior and shifts in demand as assortments change. In this paper, we utilize the multichannel attraction model – a discrete choice model specifically designed for omnichannel environments – and generalize it for the case of a retailer managing both an online store and a network of physical stores. We integrate assortment decisions with optimal inventory decisions, assuming stochastic demand. Our model shows that overlooking the demand variability can result in suboptimal assortment decisions due to the demand pooling effect. We derive complexity results for the assortment optimization problem, which we formulate as a mixed-integer second-order cone program. We then develop two heuristic algorithms based on different relaxations of the formulated optimization problem. Furthermore, we conduct an extensive numerical analysis to provide managerial insights. We find that an increasing coefficient of variation of demand has a dual effect on optimal assortment sizes, initially causing a decrease in online assortment size due to rising costs, followed by an increase in online assortment size because of the demand pooling effect. Finally, we evaluate the potential benefits of omnichannel assortment optimization compared to assortment optimization in siloed channels.},
  archive      = {J_EJOR},
  author       = {Andrey Vasilyev and Sebastian Maier and Ralf W. Seifert},
  doi          = {10.1016/j.ejor.2025.01.035},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {799-813},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing omnichannel assortments and inventory provisions under the multichannel attraction model},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-drone rescue search in a large network. <em>EJOR</em>, <em>324</em>(3), 787-798. (<a href='https://doi.org/10.1016/j.ejor.2025.02.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural disasters are recurring emergencies that can result in numerous deaths and injuries. When a natural disaster occurs, rescue teams can be sent to help affected survivors, but deploying them efficiently is a challenge. Rescuers not knowing where affected survivors are located poses a significant challenge in delivering aid. With the development of new technologies, there are new possibilities to reduce this uncertainty, alleviating this challenge. One can first send out automated drones to locate affected survivors and then send rescue teams to their locations. We develop a model for the search process and construct mathematical methods to construct efficient search routes. We utilize a divide and conquer technique to determine the routes that are most likely to yield an efficient search. We combine this with our mathematical methods to construct efficient search routes in real-time and a method to update these routes in real-time as drones gather information.},
  archive      = {J_EJOR},
  author       = {Victor Gonzalez and Patrick Jaillet},
  doi          = {10.1016/j.ejor.2025.02.003},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {787-798},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multi-drone rescue search in a large network},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mathematical models and heuristics for double-load crane scheduling in slab yards. <em>EJOR</em>, <em>324</em>(3), 773-786. (<a href='https://doi.org/10.1016/j.ejor.2025.02.036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a novel crane scheduling problem with unconstrained double-load operations (CSP-UDL) in slab yards. It aims to optimize the sequence of crane operations to minimize makespan. Unlike conventional crane operations, which handle one or two slabs per trip, the unconstrained double-load operation enables the transport of more than two slabs in a single trip, thereby improving logistic efficiency and reducing the makespan. To tackle this problem, we propose two mixed integer linear programming (MILP) models for solving small- and medium-sized instances. We develop two heuristics for large-sized instances: a hybrid heuristic and a matheuristic. The hybrid heuristic integrates tabu search within an adaptive large neighborhood search (ALNS) framework, while the matheuristic integrates this hybrid heuristic with an MILP model, leveraging the strengths of both exact and heuristic methods. Extensive computational experiments demonstrate that while the proposed MILP models can exactly solve instances with up to 50 tasks, the hybrid heuristic and the matheuristic demonstrate robust performance in solving large-sized instances.},
  archive      = {J_EJOR},
  author       = {Zixiong Dong and Ada Che and Jianguang Feng},
  doi          = {10.1016/j.ejor.2025.02.036},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {773-786},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Mathematical models and heuristics for double-load crane scheduling in slab yards},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An iterated local search algorithm for the traveling purchaser problem. <em>EJOR</em>, <em>324</em>(3), 759-772. (<a href='https://doi.org/10.1016/j.ejor.2025.02.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Traveling Purchaser Problem (TPP) is a generalization of the Traveling Salesman Problem (TSP) in which a list of items must be acquired by visiting a subset of markets. The objective is to minimize the total cost sustained along the route, including purchasing and traveling costs. Due to the NP-hard nature of the problem, solving the TPP in an exact manner is computationally challenging, implying the need for heuristic approaches to obtain quality solutions efficiently. This study proposes an algorithm based on the metaheuristic Iterated Local Search (ILS), complemented by a route configuration procedure that adjusts the subset of markets in the solution. The ILS is tested in benchmark instances, providing a performance comparison with other methods. The computational experiment for the asymmetric instances reveals the effectiveness and efficiency of the ILS, outperforming previously published results with statistical significance. Additional experiments are presented for the symmetric instances, pointing to the competitiveness and versatility of the ILS in relation to other heuristic approaches used in the literature.},
  archive      = {J_EJOR},
  author       = {Tomás Kapancioglu and Raquel Bernardino},
  doi          = {10.1016/j.ejor.2025.02.024},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {759-772},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An iterated local search algorithm for the traveling purchaser problem},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integer linear programming formulations for the maximum flow blocker problem. <em>EJOR</em>, <em>324</em>(3), 742-758. (<a href='https://doi.org/10.1016/j.ejor.2025.02.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a network with capacities and blocker costs associated with its arcs, we study the maximum flow blocker problem (FB). This problem seeks to identify a minimum-cost subset of arcs to be removed from the network, ensuring that the maximum flow value from the source to the destination in the remaining network does not exceed a specified threshold. The FB finds applications in telecommunication networks and monitoring of civil infrastructures, among other domains. We undertake a comprehensive study of several new integer linear programming (ILP) formulations designed for the FB. The first type of model, featuring an exponential number of constraints, is solved through tailored Branch-and-Cut algorithms. In contrast, the second type of ILP model, with a polynomial number of variables and constraints, is solved using a state-of-the-art ILP solver. The latter formulation establishes a structural connection between the FB and the maximum flow interdiction problem (FI), introducing a novel approach to obtaining solutions for each problem from the other. The ILP formulations proposed for solving the FB are evaluated thanks to a theoretical analysis assessing the strength of their LP relaxations. Additionally, the exact methods presented in this paper undergo a thorough comparison through an extensive computational campaign involving a set of real-world and synthetic instances. Our tests aim to evaluate the performance of the exact algorithms and identify the features of instances that can be solved with proven optimality.},
  archive      = {J_EJOR},
  author       = {Isma Bentoumi and Fabio Furini and A. Ridha Mahjoub and Sébastien Martin},
  doi          = {10.1016/j.ejor.2025.02.013},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {742-758},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integer linear programming formulations for the maximum flow blocker problem},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum-expectation matching under recourse. <em>EJOR</em>, <em>324</em>(3), 732-741. (<a href='https://doi.org/10.1016/j.ejor.2025.02.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of maximizing the expected size of a matching in the case of unreliable vertices and/or edges. The assumption is that the solution is built in several steps. In a given step, edges with successfully matched vertices are made permanent; but upon edge or vertex failures, the remaining vertices become eligible for reassignment. This process may be repeated a given number of times, and the objective is to end with the overall maximum number of matched vertices. An application of this problem is found in kidney exchange programs, going on in several countries, where a vertex is an incompatible patient–donor pair and an edge indicates cross-compatibility between two pairs; the objective is to match these pairs so as to maximize the number of served patients. A new scheme is proposed for matching rearrangement in case of failure, along with a prototype algorithm for computing the optimal expectation for the number of matched edges (or vertices), considering a possibly limited number of rearrangements. Computational experiments reveal the relevance and limitations of the algorithm, in general terms and for the kidney exchange application.},
  archive      = {J_EJOR},
  author       = {João Pedro Pedroso and Shiro Ikeda},
  doi          = {10.1016/j.ejor.2025.02.012},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {732-741},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Maximum-expectation matching under recourse},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Last fifty years of integer linear programming: A focus on recent practical advances. <em>EJOR</em>, <em>324</em>(3), 707-731. (<a href='https://doi.org/10.1016/j.ejor.2024.11.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed-integer linear programming (MILP) has become a cornerstone of operations research. This is driven by the enhanced efficiency of modern solvers, which can today find globally optimal solutions within seconds for problems that were out of reach a decade ago. The versatility of these solvers allowed successful applications in many areas, such as transportation, logistics, supply chain management, revenue management, finance, telecommunications, and manufacturing. Despite the impressive success already obtained, many challenges remain, and MILP is still a very active field. This article provides an overview of the most significant results achieved in advancing the MILP solution methods. Given the immense literature on this topic, we made deliberate choices to focus on computational aspects and recent practical performance improvements, emphasizing research that reports computational experiments. We organize our survey into three main parts, dedicated to branch-and-cut methods, Dantzig–Wolfe decomposition, and Benders decomposition. The paper concludes by highlighting ongoing challenges and future opportunities in MILP research.},
  archive      = {J_EJOR},
  author       = {François Clautiaux and Ivana Ljubić},
  doi          = {10.1016/j.ejor.2024.11.018},
  journal      = {European Journal of Operational Research},
  month        = {8},
  number       = {3},
  pages        = {707-731},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Last fifty years of integer linear programming: A focus on recent practical advances},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pareto-optimal insurance under robust distortion risk measures. <em>EJOR</em>, <em>324</em>(2), 690-705. (<a href='https://doi.org/10.1016/j.ejor.2025.03.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper delves into the optimal insurance contracting problem from the perspective of Pareto optimality. The potential policyholder (PH) and finitely many insurers all apply distortion risk measures for insurance negotiation and are assumed to be ambiguous about the underlying loss distribution. Ambiguity is modeled via sets of probability measures for each agent, and those sets are generated through Wasserstein balls around possibly different benchmark distributions. We derive the analytical forms of the optimal indemnity functions and the worst-case survival functions from all the parties’ perspectives. We illustrate more implications through numerical examples.},
  archive      = {J_EJOR},
  author       = {Tim J. Boonen and Wenjun Jiang},
  doi          = {10.1016/j.ejor.2025.03.020},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {690-705},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Pareto-optimal insurance under robust distortion risk measures},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian nash equilibrium in price competition under multinomial logit demand. <em>EJOR</em>, <em>324</em>(2), 669-689. (<a href='https://doi.org/10.1016/j.ejor.2025.02.019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Bayesian Nash equilibrium (BNE) model for analyzing price competition under multinomial logit demand where firm’s marginal cost is private information: each firm may predict the range of the marginal cost of its rival but does not know the true marginal cost. Differing from the existing Nash equilibrium models (Aksoy-Pierson et al., 2013; Pang et al., 2015) where the market equilibrium is described as a tuple of prices at which no firm can be better off by unilaterally changing its position, the BNE is a tuple of firm’s optimal price functions each of which depends on their respective marginal costs and the equilibrium is a situation at which no firm can be better off by unilaterally changing its own optimal price function. This kind of equilibrium model may help individual firms set optimal prices strategically for their future products. We derive sufficient conditions for existence and uniqueness of a continuous BNE. Moreover, we propose a computational scheme to calculate an approximate BNE. Specifically, we develop step-like approximation of firm’s optimal price function and then convert the BNE problem into a finite-dimensional stochastic variational inequality problem (SVIP). We demonstrate how the specific structure of the SVIP may be decomposed into scenario-based VIP and solve the latter by the well-known progressive hedging method. Preliminary numerical tests show that the computational scheme works well.},
  archive      = {J_EJOR},
  author       = {Jian Liu and Hailin Sun and Huifu Xu},
  doi          = {10.1016/j.ejor.2025.02.019},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {669-689},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Bayesian nash equilibrium in price competition under multinomial logit demand},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust pricing for demand response under bounded rationality in residential electricity distribution. <em>EJOR</em>, <em>324</em>(2), 654-668. (<a href='https://doi.org/10.1016/j.ejor.2025.01.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies residential users’ electricity consumption in response to dynamic pricing in smart grid, from the behavioral economics angle. Particularly, this research employs a novel approach, i.e ., the boundedly rational user decision (BRUD) modeling framework, which assumes users will accept an electricity consumption schedule whose total utility is within an “acceptable bound” of his/her maximum utility. Subsequently, we study the optimistic and pessimistic/robust pricing models via bi-level optimization, where the utility company minimizes the total system cost while considering the best and worst case under BRUD, respectively. In order to solve the pricing models efficiently, we present two methods, i.e ., the penalty method and the Lagrangian-dual method, both with the help of cutting planes. Computational results show that the proposed dynamic pricing schemes are effective in decreasing the total system cost. Furthermore, the Lagrangian-dual-based method outperforms the penalty method due to the exploitation of the special convexity structure of the problem. Finally, when compared to solutions without considering the irrationality of user behavior, solutions from the BRUD models are more efficient in reducing total system cost. Furthermore, the additional benefits gained from considering irrationality tend to grow when user’s monetary value of convenience increases.},
  archive      = {J_EJOR},
  author       = {Guanxiang Yun and Qipeng P. Zheng and Lihui Bai and Eduardo L. Pasiliao},
  doi          = {10.1016/j.ejor.2025.01.023},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {654-668},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust pricing for demand response under bounded rationality in residential electricity distribution},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Goodness-of-fit in production models: A bayesian perspective. <em>EJOR</em>, <em>324</em>(2), 644-653. (<a href='https://doi.org/10.1016/j.ejor.2025.01.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a general approach for modeling production technologies, allowing for modeling both inefficiency and noise that are specific for each input and each output. The approach is based on amalgamating ideas from nonparametric activity analysis models for production and consumption theory with stochastic frontier models. We do this by effectively re-interpreting the activity analysis models as simultaneous equations models in Bayesian compression and artificial neural networks framework. We make minimal assumptions about noise in the data and we allow for flexible approximations to input- and output-specific slacks. We use compression to solve the problem of an exceeding number of parameters in general production technologies and also incorporate environmental variables in the estimation. We also present Monte Carlo simulation results and an empirical illustration of this approach for US banking data.},
  archive      = {J_EJOR},
  author       = {Mike Tsionas and Valentin Zelenyuk and Xibin Zhang},
  doi          = {10.1016/j.ejor.2025.01.030},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {644-653},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Goodness-of-fit in production models: A bayesian perspective},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The π-transportation problem: On the value of split transports for the physical internet concept. <em>EJOR</em>, <em>324</em>(2), 629-643. (<a href='https://doi.org/10.1016/j.ejor.2025.01.038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Physical Internet (PI or π ) is a design metaphor that applies the digital internet as an archetype to rethink freight logistics and transportation in a more sustainable, interoperable, and cooperative way. Analogously to the protocols of the digital internet, freight should be encapsulated into standardized π -containers and transported through an open network of cooperating π -hubs. Despite the inspiring analogy, parallels are not absolute. Digital data packages can be duplicated without cost, can be (re-)sent anywhere in short time, generate no return flows, and their routing decisions have to be taken in microseconds. In this paper, we focus on a characteristic of the digital internet that can be emulated by the PI but has received less attention yet: split transports. Analogously to the internet protocol, which forwards each data packet individually according to dynamically adapted routing tables, larger shipments can be split into multiple smaller π -containers, so that different containers with the same destination may be routed via different paths. To evaluate whether these split transports significantly promote the aims of the PI, we derive a basic scheduling problem, called the π -transportation problem, where a given set of (either split or unsplit) shipments aim to travel along a linear transport corridor with given π -hubs. Based on this problem, we derive analytical and computational results to quantify the impact of split transports on the success of the PI concept. Our results suggest that split transports are not among the important features when redesigning the transportation sector.},
  archive      = {J_EJOR},
  author       = {Nils Boysen and Dirk Briskorn and Benoit Montreuil and Lennart Zey},
  doi          = {10.1016/j.ejor.2025.01.038},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {629-643},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The π-transportation problem: On the value of split transports for the physical internet concept},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fighting sampling bias: A framework for training and evaluating credit scoring models. <em>EJOR</em>, <em>324</em>(2), 616-628. (<a href='https://doi.org/10.1016/j.ejor.2025.01.040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scoring models support decision-making in financial institutions. Their estimation and evaluation rely on labeled data from previously accepted clients. Ignoring rejected applicants with unknown repayment behavior introduces sampling bias, as the available labeled data only partially represents the population of potential borrowers. This paper examines the impact of sampling bias and introduces new methods to mitigate its adverse effect. First, we develop a bias-aware self-labeling algorithm for scorecard training, which debiases the training data by adding selected rejects with an inferred label. Second, we propose a Bayesian framework to address sampling bias in scorecard evaluation. To provide reliable projections of future scorecard performance, we include rejected clients with random pseudo-labels in the test set and use Monte Carlo sampling to estimate the scorecard’s expected performance across label realizations. We conduct extensive experiments using both synthetic and observational data. The observational data includes an unbiased sample of applicants accepted without scoring, representing the true borrower population and facilitating a realistic assessment of reject inference techniques. The results show that our methods outperform established benchmarks in predictive accuracy and profitability. Additional sensitivity analysis clarifies the conditions under which they are most effective. Comparing the relative effectiveness of addressing sampling bias during scorecard training versus evaluation, we find the latter much more promising. For example, we estimate the expected return per dollar issued to increase by up to 2.07 and up to 5.76 percentage points when using bias-aware self-labeling and Bayesian evaluation, respectively.},
  archive      = {J_EJOR},
  author       = {Nikita Kozodoi and Stefan Lessmann and Morteza Alamgir and Luis Moreira-Matias and Konstantinos Papakonstantinou},
  doi          = {10.1016/j.ejor.2025.01.040},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {616-628},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fighting sampling bias: A framework for training and evaluating credit scoring models},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monitoring bank risk around the world using unsupervised learning. <em>EJOR</em>, <em>324</em>(2), 590-615. (<a href='https://doi.org/10.1016/j.ejor.2025.01.036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a transparent and dynamic decision support tool that ranks clusters of listed banks worldwide by riskiness. It is designed to be flexible in updating and editing the values and quantities of banks, indicators, and clusters. For constructing this tool, a large set of stand-alone and systemic risk indicators are computed and reduced to fewer representative factors. These factors are set as features for an adjusted version of a nested k-means algorithm that handles missing data. This algorithm gathers banks per clusters of riskiness and ranks them. The results of the individual banks' multidimensional clustering are also aggregable per country and region, enabling the identification of areas of fragility. Empirically, we rank five clusters of 256 listed banks and compute 72 indicators, which are reduced to 12 components based on 10 main factors, over the 2004–2024 period. The findings emphasize the importance of giving special consideration to the ambiguous impact of banks' size on systemic risk measures.},
  archive      = {J_EJOR},
  author       = {Mathieu Mercadier and Amine Tarazi and Paul Armand and Jean-Pierre Lardy},
  doi          = {10.1016/j.ejor.2025.01.036},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {590-615},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Monitoring bank risk around the world using unsupervised learning},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-class support vector machine based on minimization of reciprocal-geometric-margin norms. <em>EJOR</em>, <em>324</em>(2), 580-589. (<a href='https://doi.org/10.1016/j.ejor.2025.03.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Support Vector Machine (SVM) method for multi-class classification. It follows multi-objective multi-class SVM (MMSVM), which maximizes class-pair margins on a multi-class linear classifier. The proposed method, called reciprocal-geometric-margin-norm SVM (RGMNSVM) is derived by applying the ℓ p -norm scalarization and convex approximation to MMSVM. Additionally, we develop the margin theory for multi-class linear classification, in order to justify minimization of reciprocal class-pair geometric margins. Experimental results on synthetic datasets explain situations where the proposed RGMNSVM successfully works, while conventional multi-class SVMs fail to fit underlying distributions. Results of classification performance evaluation using benchmark data sets show that RGMNSVM is generally comparable with conventional multi-class SVMs. However, we observe that the proposed approach to geometric margin maximization actually performs better classification accuracy for certain real-world data sets.},
  archive      = {J_EJOR},
  author       = {Yoshifumi Kusunoki and Keiji Tatsumi},
  doi          = {10.1016/j.ejor.2025.03.028},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {580-589},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multi-class support vector machine based on minimization of reciprocal-geometric-margin norms},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). K-tree: Crossing sharp boundaries in regression trees to find neighbors. <em>EJOR</em>, <em>324</em>(2), 567-579. (<a href='https://doi.org/10.1016/j.ejor.2025.02.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional classification and regression trees (CARTs) utilize a top-down, greedy approach to split the feature space into sharply defined, axis-aligned sub-regions (leaves). Each leaf treats all of the samples therein uniformly during the prediction process, leading to a constant predictor. Although this approach is well known for its interpretability and efficiency, it overlooks the complex local distributions within and across leaves. As the number of features increases, this limitation becomes more pronounced, often resulting in a concentration of samples near the boundaries of the leaves. Such clustering suggests that there is potential in identifying closer neighbors in adjacent leaves, a phenomenon that is unexplored in the literature. Our study addresses this gap by introducing the k -Tree methodology, a novel method that extends the search for nearest neighbors beyond a single leaf to include adjacent leaves. This approach has two key innovations: (1) establishing an adjacency relationship between leaves across the tree space and (2) designing novel intra-leaf and inter-leaf distance metrics through an optimization lens, which are tailored to local data distributions within the tree. We explore three implementations of the k -Tree methodology: (1) the Post-hoc k -Tree (P k -Tree), which integrates the k -Tree methodology into constructed decision trees, (2) the Advanced k -Tree, which seamlessly incorporates the k -Tree methodology during the tree construction process, and (3) the P k -random forest, which integrates the P k -Tree principles with the random forest framework. The results of empirical evaluations conducted on a variety of real-world and synthetic datasets demonstrate that the k -Tree methods have greater prediction accuracy over the traditional models. These results highlight the potential of the k -Tree methodology in enhancing predictive analytics by providing a deeper insight into the relationships between samples within the tree space.},
  archive      = {J_EJOR},
  author       = {Xuecheng Tian and Shuaian Wang and Lu Zhen and Zuo-Jun (Max) Shen},
  doi          = {10.1016/j.ejor.2025.02.031},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {567-579},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {K-tree: Crossing sharp boundaries in regression trees to find neighbors},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A predict-and-optimize approach to profit-driven churn prevention. <em>EJOR</em>, <em>324</em>(2), 555-566. (<a href='https://doi.org/10.1016/j.ejor.2025.02.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel, profit-driven classification approach for churn prevention by framing the task of targeting customers for a retention campaign as a regret minimization problem within a predict-and-optimize framework. This is the first churn prevention model to utilize this approach. Our main objective is to leverage individual customer lifetime values (CLVs) to ensure that only the most valuable customers are targeted. In contrast, many profit-driven strategies focus on churn probabilities while considering average CLVs, often resulting in significant information loss due to data aggregation. Our proposed model aligns with the principles of the predict-and-optimize framework and can be efficiently solved using stochastic gradient descent methods. Results from 13 churn prediction datasets, sourced from an investment company, underscore the effectiveness of our approach, which achieves the highest average performance in terms of profit compared to other well-established strategies.},
  archive      = {J_EJOR},
  author       = {Nuria Gómez-Vargas and Sebastián Maldonado and Carla Vairetti},
  doi          = {10.1016/j.ejor.2025.02.008},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {555-566},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A predict-and-optimize approach to profit-driven churn prevention},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential product launches with post-sale updates. <em>EJOR</em>, <em>324</em>(2), 538-554. (<a href='https://doi.org/10.1016/j.ejor.2025.01.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As technology evolves, a seller may offer sequential releases of its product over time, with new versions offering superior performance. Beyond offering new product releases over time, sellers now increasingly have the option of offering post-sale software updates, thereby potentially extending product longevity. The potential to change product life cycles via software updates is of strategic importance, particularly since past and future product releases are interrelated. Considering stochastic technology evolution and strategic consumers, we study the strategy of augmenting paid product releases with free product updates. By contrasting this strategy against two benchmark product-launch policies without free updates, we show that offering free updates can generate nontrivial profit gains and optimally lengthen the seller’s new-product introduction cycles. We explore the sensitivity of the seller’s optimal release decisions and profit to key drivers, including costs and technology uncertainty. We also investigate the impact of consumer heterogeneity, distinct time-discount rates between the seller and consumers, and allowing multiple updates per product release. Our results show that while free updates ostensibly appear to favor consumers and do deliver utility to customers, they also are an effective mechanism for a seller to increase profits.},
  archive      = {J_EJOR},
  author       = {Monire Jalili and Michael S. Pangburn and Euthemia Stavrulaki and Shubin Xu},
  doi          = {10.1016/j.ejor.2025.01.018},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {538-554},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Sequential product launches with post-sale updates},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A branch-and-price algorithm for fast and equitable last-mile relief aid distribution. <em>EJOR</em>, <em>324</em>(2), 522-537. (<a href='https://doi.org/10.1016/j.ejor.2025.01.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the ϵ -constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul’s Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers.Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.},
  archive      = {J_EJOR},
  author       = {Mahdi Mostajabdaveh and F. Sibel Salman and Walter J. Gutjahr},
  doi          = {10.1016/j.ejor.2025.01.032},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {522-537},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A branch-and-price algorithm for fast and equitable last-mile relief aid distribution},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The interplay between charitable donation strategies and sales mode selection in the platform. <em>EJOR</em>, <em>324</em>(2), 506-521. (<a href='https://doi.org/10.1016/j.ejor.2025.01.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the emergence of offline and online donations, this paper explores the interplay between charitable donations and strategic choice of sales mode in a philanthropic supply chain consisting of a manufacturer and a platform. We consider two donation strategies, offline donations and both offline and online donations that are traceable by blockchain technology, and two business models, i.e., reselling sales mode and agency sales mode. Donations by the manufacturer are used to boost its charitable image, which in turn affects positively the demand. As such image can only be built over time, we adopt a differential game formalism that captures both the strategic interactions between the two players and the dynamic nature of the problem. We characterize and compare the equilibrium strategies and outcomes for different choices of selling mode and donation option. Our findings can be summarized as follows. First, we obtain that only under some conditions that online donations enhance the charitable image, members’ profits, consumer surplus, and social welfare. Second, regardless of the sales mode, the conditions for the platform to adopt online donations are the most stringent, and the conditions for the enhancement of the charitable image are the most lenient. Third, the implementation of online donations does not have much impact on the Pareto regions of the agency mode but has a much greater impact on the Pareto regions of the reselling mode, especially for medium and large online donation amounts. These changes hinge on the trade-offs for members between online and offline donations.},
  archive      = {J_EJOR},
  author       = {Chen Zhu and Georges Zaccour},
  doi          = {10.1016/j.ejor.2025.01.031},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {506-521},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The interplay between charitable donation strategies and sales mode selection in the platform},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic model for physician staffing and scheduling in emergency departments with multiple treatment stages. <em>EJOR</em>, <em>324</em>(2), 492-505. (<a href='https://doi.org/10.1016/j.ejor.2025.01.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new solution for the Emergency Department (ED) staffing and scheduling problem, considering uncertainty in patient arrival patterns, multiple treatment stages, and resource capacity. A two-stage stochastic mathematical programming model was developed. We employed a Sample Average Approximation (SAA) method to generate scenarios and a discrete event simulation to evaluate the results. The model was applied in a large hospital, with 72,988 medical encounters and 85 physicians in a ten-month period. Compared to the hospital’s actual scheduling, we obtained an overall average waiting time reduction from 54.6 (54.0–55.1) to 16.8 (16.7–17.0) minutes and an average Length of Stay reduction from 102.1 (101.7–102.4) to 64.3 (64.2–64.5) minutes. Therefore, this study offers a stochastic model that effectively addresses uncertainties in EDs, aligning physician schedules with patient arrivals and potentially improving the quality of service by reducing waiting times.},
  archive      = {J_EJOR},
  author       = {Janaina F. Marchesi and Silvio Hamacher and Igor Tona Peres},
  doi          = {10.1016/j.ejor.2025.01.027},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {492-505},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Stochastic model for physician staffing and scheduling in emergency departments with multiple treatment stages},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Last mile delivery routing problem with some-day option. <em>EJOR</em>, <em>324</em>(2), 477-491. (<a href='https://doi.org/10.1016/j.ejor.2025.02.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {E-commerce retailers are challenged to maintain cost-efficiency and customer satisfaction while pursuing sustainability, especially in the last mile. In response, retailers are offering a range of delivery speeds, including same-day and instant options. Faster deliveries, while trending, often increase costs and emissions due to limited planning time and reduced consolidation opportunities in the last mile. In contrast, this paper proposes the inclusion of a slower delivery option, termed some-day. Slowing down the delivery process allows for greater shipment consolidation, achieving cost savings and environmental goals simultaneously. We introduce the dynamic and stochastic some-day delivery problem, which accounts for a latest delivery day, customer time windows, and capacity limitations within a multi-period planning framework. Our solution approach is based on addressing auxiliary prize-collecting vehicle routing problems with time windows (PCVRPTW) on a daily basis, where the prize reflects the benefit of promptly serving the customer. We develop a hybrid adaptive large neighborhood search with granular insertion operators, outperforming existing metaheuristics for PCVRPTWs. Our numerical study shows significant cost savings with only small increases in delivery times compared to an earliest policy.},
  archive      = {J_EJOR},
  author       = {Stefan Voigt and Markus Frank and Heinrich Kuhn},
  doi          = {10.1016/j.ejor.2025.02.001},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {477-491},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Last mile delivery routing problem with some-day option},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-crossing vs. independent lead times in a lost-sales inventory system with compound poisson demand. <em>EJOR</em>, <em>324</em>(2), 466-476. (<a href='https://doi.org/10.1016/j.ejor.2025.01.042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we provide an analysis of an inventory system with compound Poisson demand and a sequential supply system, specifically focusing on non-crossing lead times. The purpose is to facilitate a comparison with order crossing caused by independent lead times. We apply a new approach for modeling exogenous, non-crossing Erlang distributed lead times. It is assumed that the inventory is controlled by continuous review and a base-stock level. Any part of a customer demand which cannot be satisfied immediately from inventory is lost. Set-up costs are negligible and the relevant cost parameters for choosing the best base stock consist of the holding cost rate and a shortage cost per unit lost. We provide a proof that the long-run average total relevant cost is a convex function of the base stock. Hence, the base stock is easy to compute and it is optimal in the case of geometrically distributed customer demand sizes. For demand sizes which are not geometric, we suggest an approximation to specify the base stock. An approximation is also suggested for the case when the lead time is only specified by its mean and standard deviation (SD). Our numerical study shows that the average cost is very sensitive to the SD. This in sharp contrast to the complete insensitivity of SD in case of geometric demand sizes and independent lead times.},
  archive      = {J_EJOR},
  author       = {Søren Glud Johansen and Anders Thorstenson},
  doi          = {10.1016/j.ejor.2025.01.042},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {466-476},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Non-crossing vs. independent lead times in a lost-sales inventory system with compound poisson demand},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The use of IoT sensor data to dynamically assess maintenance risk in service contracts. <em>EJOR</em>, <em>324</em>(2), 454-465. (<a href='https://doi.org/10.1016/j.ejor.2025.01.041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the value of using operational sensor data to improve the risk assessment of service contracts that cover all maintenance-related costs during a fixed period. An initial estimate of the contract risk is determined by predicting the maintenance costs via a gradient-boosting machine based on the machine’s and contract’s characteristics observable at the onset of the contract period. We then periodically update this risk assessment based on operational sensor data observed throughout the contract period. These sensor data reveal operational machine usage that drives the maintenance risk. We validate our approach on a portfolio of about 4,000 full-service contracts of industrial equipment and show how dynamic sensor data improves risk differentiation.},
  archive      = {J_EJOR},
  author       = {Stijn Loeys and Robert N. Boute and Katrien Antonio},
  doi          = {10.1016/j.ejor.2025.01.041},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {454-465},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The use of IoT sensor data to dynamically assess maintenance risk in service contracts},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimising makespan and total tardiness for the flowshop group scheduling problem with sequence dependent setup times. <em>EJOR</em>, <em>324</em>(2), 436-453. (<a href='https://doi.org/10.1016/j.ejor.2025.02.009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of optimizing multiple objectives while considering job groups and partial due dates is prevalent in the flowshop group scheduling problem (FGSP). Despite its significance, the multi-objective FGSP with partial due dates (MFGSP) remains largely unaddressed in existing FGSP literature. In this paper, we bridge this gap by introducing a mixed integer linear programming model and an iterated greedy algorithm tailored for MFGSP with sequence-dependent group setup times, aimed at minimizing both makespan and total tardiness concurrently. Our proposed approach delves into the specific characteristics of times, acknowledging the inherent conflicts between objectives and the unique nature of each objective. We propose two novel local search operators: one inspired by the asymmetric traveling salesman problem and the other based on a domination criterion. These operators are seamlessly integrated into the iterated greedy algorithm framework, augmented with a cone-weighted scalar method as a fitness function and adaptive perturbation parameters. Extensive experimental evaluations demonstrate the efficacy and efficiency of our proposed algorithm, showcasing its capability to solve the MFGSP effectively. Through this research, we contribute a practical and versatile solution to a largely unexplored area in group scheduling optimization.},
  archive      = {J_EJOR},
  author       = {Xuan He and Quan-Ke Pan and Liang Gao and Janis S. Neufeld and Jatinder N.D. Gupta},
  doi          = {10.1016/j.ejor.2025.02.009},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {436-453},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Minimising makespan and total tardiness for the flowshop group scheduling problem with sequence dependent setup times},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybridizing carousel greedy and kernel search: A new approach for the maximum flow problem with conflict constraints. <em>EJOR</em>, <em>324</em>(2), 414-435. (<a href='https://doi.org/10.1016/j.ejor.2025.02.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses a variant of the maximum flow problem where specific pairs of arcs are not allowed to carry positive flow simultaneously. Such restrictions are known in the literature as negative disjunctive constraints or conflict constraints . The problem is known to be strongly NP-hard and several exact approaches have been proposed in the literature. In this paper, we present a heuristic algorithm for the problem, based on two different approaches: Carousel Greedy and Kernel Search. These two approaches are merged to obtain a fast and effective matheuristic, named Kernousel. In particular, the computational results reveal that exploiting the information gathered by the Carousel Greedy to build the set of most promising variables (the kernel set ), makes the Kernel Search more effective. To validate the performance of the new hybrid method, we compare it with the two components running individually. Results are also evaluated against the best-known solutions available in the literature for the problem. The new hybrid method provides 15 new best-known values on benchmark instances.},
  archive      = {J_EJOR},
  author       = {F. Carrabs and R. Cerulli and R. Mansini and D. Serra and C. Sorgente},
  doi          = {10.1016/j.ejor.2025.02.006},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {414-435},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Hybridizing carousel greedy and kernel search: A new approach for the maximum flow problem with conflict constraints},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient branch-and-bound algorithm for the one-to-many shortest path problem with additional disjunctive conflict constraints. <em>EJOR</em>, <em>324</em>(2), 398-413. (<a href='https://doi.org/10.1016/j.ejor.2025.01.044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we study an extension of the ordinary one-to-many shortest path problem that also considers additional disjunctive conflict relations between the arcs: an optimal shortest path tree is not allowed to include any conflicting arc pair. As is the case with many polynomially solvable combinatorial optimization problems, the addition of conflict relations makes the problem NP -hard. We propose a novel branch-and-bound algorithm, which benefits from the solution of the one-to-many shortest path relaxations, an efficient primal–dual reoptimization scheme and a fast infeasibility detection procedure for pruning the branch-and-bound tree. According to the extensive computational tests it is possible to say that the novel algorithm is very efficient.},
  archive      = {J_EJOR},
  author       = {Bahadır Pamuk and Temel Öncan and İ. Kuban Altınel},
  doi          = {10.1016/j.ejor.2025.01.044},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {398-413},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An efficient branch-and-bound algorithm for the one-to-many shortest path problem with additional disjunctive conflict constraints},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A column generation-based matheuristic for an inventory-routing problem with driver-route consistency. <em>EJOR</em>, <em>324</em>(2), 382-397. (<a href='https://doi.org/10.1016/j.ejor.2025.02.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a variant of an inventory-routing problem (IRP) that enforces two conditions on the structure of the solution: time-invariant routes, and a fixed, injective (i.e., one-to-one) assignment of routes to vehicles. The practical benefits of concurrent route invariance and driver assignments are numerous. Fixed routes reduce the solution space of the problem and improve its tractability; they simplify operations; and they increase the viability of newer delivery technologies like drones and autonomous vehicles. Consistency between driver and customer is linked to improved service, driver job satisfaction and delivery efficiency, and is also an important consideration in certain contexts like home healthcare. After formulating the problem as a mixed integer-linear program, we recast it as a set partitioning problem whose linear relaxation is solved via column generation. Due to the prohibitively expensive nature of the pricing problem that generates new columns, we present a novel column generation-based heuristic for it that relies on decoupling routing and inventory management decisions. We demonstrate the effectiveness of the proposed method via a numerical study.},
  archive      = {J_EJOR},
  author       = {Waleed Najy and Claudia Archetti and Ali Diabat},
  doi          = {10.1016/j.ejor.2025.02.007},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {382-397},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A column generation-based matheuristic for an inventory-routing problem with driver-route consistency},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years of reliability in operations research. <em>EJOR</em>, <em>324</em>(2), 361-381. (<a href='https://doi.org/10.1016/j.ejor.2024.09.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides an overview of the historical evolution of reliability in the scientific area of operations research. Historical views and future perspectives are specifically offered with regards to reliability modeling and inference, treatment of uncertainty in reliability modelling and analysis, definition of importance measures for identifying those elements of a system which are critical for its reliability, optimization of the design, operation and maintenance of a system with respect to its reliability, adversarial issues, and the growing focus on machine learning for reliability modeling and optimization. The overview and perspectives given are rich but by no means they cover all the great developments and advancements done, nor they point at all still-open issues and coming challenges.},
  archive      = {J_EJOR},
  author       = {Terje Aven and David Rios Insua and Refik Soyer and Xiaoyan Zhu and Enrico Zio},
  doi          = {10.1016/j.ejor.2024.09.010},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {2},
  pages        = {361-381},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of reliability in operations research},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The three-way decision model and multi-attribute decision-making: Methodological traps and challenges. <em>EJOR</em>, <em>324</em>(1), 351-360. (<a href='https://doi.org/10.1016/j.ejor.2025.02.035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three-way decision (3WD) model has lately gained a lot of attention in data analysis and decision-making, and has even become a methodological framework that brings new insights into old problems. Our research is inspired by Bisht and Pal (2024) who introduced an innovative 3WD model which frames multi-attribute decision-making (MADM) problems. Considering both the loss and utility associated with various alternatives, they deduced the threshold values and directly integrated them to determine the final thresholds of 3WD. However, this approach does not fully address the precondition of the minimum expected cost or the maximum expected return inherent in the Bayesian decision procedure. In this paper, we would like to fill this research gap and discuss methodological aspects of establishing the decision thresholds of the 3WD model. Specifically, the experimental analysis demonstrates that the decision results generated by the decision model of Bisht and Pal (2024) may be inconsistent with the optimal decision results derived from the maximum expected return, thus potentially leading to conflicting outcomes. In the present paper, we address this problem and deduce more optimal expressions for the thresholds on the basis of the Bayesian decision procedure. In addition, through comparative experimental analysis, the rationality and effectiveness of the proposed threshold calculation method are verified. Finally, in the concrete derivation process, we also reveal the criteria for the existence of two-way decision, which makes our model applicable to more general situations.},
  archive      = {J_EJOR},
  author       = {Decui Liang and Chenglong Yang},
  doi          = {10.1016/j.ejor.2025.02.035},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {351-360},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The three-way decision model and multi-attribute decision-making: Methodological traps and challenges},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The first mile is the hardest: A deep learning-assisted matheuristic for container assignment in first-mile logistics. <em>EJOR</em>, <em>324</em>(1), 335-350. (<a href='https://doi.org/10.1016/j.ejor.2025.01.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban logistics has been recognized as one of the most complex and expensive part of e-commerce supply chains. An increasing share of this complexity comes from the first mile, where shipments are initially picked up to be fed into the transportation network. First-mile pickup volumes have become fragmented due to the enormous growth of e-commerce marketplaces, which allow even small-size vendors access to the global market. These local vendors usually cannot palletize their own shipments but instead rely on containers provided by a logistics provider. From the logistics provider’s perspective, this situation poses the following novel problem: from a given pool of containers, how many containers of what size should each vendor receive when? It is neither desirable to supply too little container capacity because undersupply leads to shipments being loose-loaded, i.e., loaded individually without consolidation in a container; nor should the assigned containers be too large because oversupply wastes precious space. We demonstrate NP-hardness of the problem and develop a matheuristic, which uses a mathematical solver to assemble partial container assignments into complete solutions. The partial assignments are generated with the help of a deep neural network (DNN), trained on realistic data from a European e-commerce logistics provider. The deep learning-assisted matheuristic allows serving the same number of vendors with about 6% fewer routes than the rule of thumb used in practice due to better vehicle utilization. We also investigate the trade-off between loose-loaded shipments and space utilization and the effect on the routes of the collection vehicles.},
  archive      = {J_EJOR},
  author       = {Simon Emde and Ana Alina Tudoran},
  doi          = {10.1016/j.ejor.2025.01.024},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {335-350},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The first mile is the hardest: A deep learning-assisted matheuristic for container assignment in first-mile logistics},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven condition-based maintenance optimization given limited data. <em>EJOR</em>, <em>324</em>(1), 324-334. (<a href='https://doi.org/10.1016/j.ejor.2025.01.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unexpected failures of operating systems can result in severe consequences and huge economic losses. To prevent them, preventive maintenance based on condition data can be performed. Existing studies either rely on the assumption of a known deterioration process or an abundance of data. However, in practice, it is unlikely that the deterioration process is known, and data is often limited (to a few runs-to-failure), especially for new systems. This paper presents a fully data-driven approach for condition-based maintenance (CBM) optimization that is especially useful in situations with limited data. The approach uses penalized logistic regression to estimate the failure probability as a function of the deterioration level and allows any deterioration level to be selected as the preventive maintenance threshold, also those that have not been observed in the past. Numerical results indicate that the preventive maintenance thresholds resulting from our proposed approach closely approach the optimal values.},
  archive      = {J_EJOR},
  author       = {Yue Cai and Bram de Jonge and Ruud H. Teunter},
  doi          = {10.1016/j.ejor.2025.01.010},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {324-334},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Data-driven condition-based maintenance optimization given limited data},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic pharmaceutical product portfolio management with flexible resource profiles. <em>EJOR</em>, <em>324</em>(1), 308-323. (<a href='https://doi.org/10.1016/j.ejor.2025.01.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pharmaceutical industry faces growing pressure to develop innovative, affordable products faster. Completing clinical trials on time is crucial, as revenue strongly depends on the finite patent protection. In this paper, we consider dynamic resource allocation for pharmaceutical product portfolio management and clinical trial scheduling, proposing a modelling framework, where resource profiles for ongoing clinical trials are flexible, with the possibility to add additional resources, thereby accelerating the completion of a clinical trial and enhancing pipeline profitability. Specifically, we treat both resource profiles and clinical trial scheduling as decision variables in the management of multiple pharmaceutical products to maximise the expected discounted profit, accounting for uncertainty in clinical trial outcomes. We formulate this problem as a Markov decision process and design a Monte Carlo tree search approach that can identify the best decision for each state by utilising a base policy to estimate value functions. We significantly improve the algorithm efficiency by proposing a statistical racing procedure using correlated sampling (common random numbers) and Bernstein’s inequality. We demonstrate the effectiveness of the proposed approach on a pharmaceutical drug development pipeline problem, finding that the proposed modelling framework with flexible resource profiles improves the resource efficiency and profitability, and the proposed Monte Carlo tree search algorithm outperforms existing approaches in terms of efficiency and solution quality.},
  archive      = {J_EJOR},
  author       = {Xin Fei and Jürgen Branke and Nalân Gülpınar},
  doi          = {10.1016/j.ejor.2025.01.011},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {308-323},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Dynamic pharmaceutical product portfolio management with flexible resource profiles},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulative assessment of patrol car allocation and response time. <em>EJOR</em>, <em>324</em>(1), 290-307. (<a href='https://doi.org/10.1016/j.ejor.2024.12.035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capacity planning of police resources is crucial to operating an effective and robust police service. However, due to the high operational heterogeneity and variability among different calls for service, key performance estimates that link resource allocation and utilization to emergency response times are a challenging task in and of itself. In the literature, two main instruments are proposed to provide appropriate estimates: queueing models, which yield closed-form expressions for key performance characteristics under limiting assumptions, and simulation models, which seek to capture more of the real-world structure of police operations at the cost of increased computational effort. Utilizing an extensive dataset comprising over two million calls for service, we have created a discrete-event simulation tailored to capture police operations within a major metropolitan area in Germany. Our analysis involves comparing this simulation against an implementation of the widely cited multiple car dispatch queueing model by Green and Kolesar (1989) found in the literature. Our findings underscore that our simulation model yields significantly improved estimates for key performance indicators reflective of real-world scenarios. Notably, we demonstrate the consequential impact on resource allocation resulting from these enhanced estimates. The superior accuracy of our model facilitates the development of capacity plans that align more effectively with actual workloads, consequently fostering heightened security measures and cost efficiencies for society. Additionally, our study involves rectifying discrepancies in the presentation of the queueing model and highlighting three specific areas for future research.},
  archive      = {J_EJOR},
  author       = {Tobias Cors and Malte Fliedner and Knut Haase and Tobias Vlćek},
  doi          = {10.1016/j.ejor.2024.12.035},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {290-307},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Simulative assessment of patrol car allocation and response time},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel adaptive parameter fractional-order gradient descent learning for stock selection decision support systems. <em>EJOR</em>, <em>324</em>(1), 276-289. (<a href='https://doi.org/10.1016/j.ejor.2025.01.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient descent methods are widely used as optimization algorithms for updating neural network weights. With advancements in fractional-order calculus, fractional-order gradient descent algorithms have demonstrated superior optimization performance. Nevertheless, existing fractional-order gradient descent algorithms have shortcomings in terms of structural design and theoretical derivation. Specifically, the convergence of fractional-order algorithms in the existing literature relies on the assumed boundedness of network weights. This assumption leads to uncertainty in the optimization results. To address this issue, this paper proposes several adaptive parameter fractional-order gradient descent learning (AP-FOGDL) algorithms based on the Caputo and Riemann–Liouville derivatives. To fully leverage the convergence theorem, an adaptive learning rate is designed by introducing computable upper bounds. The convergence property is then theoretically proven for both derivatives, with and without the adaptive learning rate. Moreover, to enhance prediction accuracy, an amplification factor is employed to increase the adaptive learning rate. Finally, practical applications on a stock selection dataset and a bankruptcy dataset substantiate the feasibility, high accuracy, and strong generalization performance of the proposed algorithms. A comparative study between the proposed methods and other relevant gradient descent methods demonstrates the superiority of the AP-FOGDL algorithms.},
  archive      = {J_EJOR},
  author       = {Mingjie Ma and Siyuan Chen and Lunan Zheng},
  doi          = {10.1016/j.ejor.2025.01.013},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {276-289},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Novel adaptive parameter fractional-order gradient descent learning for stock selection decision support systems},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biform game consensus analysis of group decision making with unconnected social network. <em>EJOR</em>, <em>324</em>(1), 259-275. (<a href='https://doi.org/10.1016/j.ejor.2025.01.019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today's network era, people's decisions are susceptibly influenced by others, especially the ones they trust. This study confines to studying social network group decision making (SNGDM). Due to the mutual influence of consensus level and consensus adjustment among decision makers (DMs), this study utilizes biform game theory to propose an innovative consensus mechanism for facilitating group decision making with unconnected social networks. Specifically, in the context of a DM social network with multiple trust relationship-based connected components, we construct a multi-objective programming model to determine the consensus adjustment. Within each connected component, we employ the digraph game theory to study DMs' consensus adjustments, leveraging the directional and asymmetrical characteristics of trust-relationships. We then analyze the consensus adjustments of feasible DM coalitions using built optimization models and define the di-Myerson value. Additionally, we construct several axiomatic systems to show the rationality of consensus allocation results. We identify partial trust-relationships that increase the consensus adjustments of DMs as irrational, and design an algorithm to address them, thereby reducing the cost of consensus. Finally, we present a case study that showcases the real-world application of our new theoretical results. This is the first bi-form game consensus mechanism based on trust relationship for SNGDM.},
  archive      = {J_EJOR},
  author       = {Jie Tang and Zi-Jun Li and Fan-Yong Meng and Zai-Wu Gong and Witold Pedrycz},
  doi          = {10.1016/j.ejor.2025.01.019},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {259-275},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Biform game consensus analysis of group decision making with unconnected social network},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formulating human risk response in epidemic models: Exogenous vs endogenous approaches. <em>EJOR</em>, <em>324</em>(1), 246-258. (<a href='https://doi.org/10.1016/j.ejor.2025.01.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent pandemic emphasized the need to consider the role of human behavior in shaping epidemic dynamics. In particular, it is necessary to extend beyond the classical epidemiological structures to fully capture the interplay between the spread of disease and how people respond. Here, we focus on the challenge of incorporating change in human behavior in the form of “risk response” into compartmental epidemiological models, where humans adapt their actions in response to their perceived risk of becoming infected. The review examines 37 papers containing over 40 compartmental models, categorizing them into two fundamentally distinct classes: exogenous and endogenous approaches to modeling risk response. While in exogenous approaches, human behavior is often included using different fixed parameter values for certain time periods, endogenous approaches seek for a mechanism internal to the model to explain changes in human behavior as a function of the state of disease. We further discuss two different formulations within endogenous models as implicit versus explicit representation of information diffusion. This analysis provides insights for modelers in selecting an appropriate framework for epidemic modeling.},
  archive      = {J_EJOR},
  author       = {Leah LeJeune and Navid Ghaffarzadegan and Lauren M. Childs and Omar Saucedo},
  doi          = {10.1016/j.ejor.2025.01.004},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {246-258},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Formulating human risk response in epidemic models: Exogenous vs endogenous approaches},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of loyal and new customer segments on product upgrades: The role of quality differentiation through online reviews. <em>EJOR</em>, <em>324</em>(1), 231-245. (<a href='https://doi.org/10.1016/j.ejor.2024.12.045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firms often strive to expand their market share beyond their established customer base by launching quality upgrades in their products. They recognize that customers often gauge product quality through online reviews. We develop an analytical model to examine the quality upgrade strategies of two competing firms, revealing two potential market equilibria. In the unilateral upgrading equilibrium where only one firm upgrades, the upgrading firm sees an initial increase in loyal demand, leading to higher prices. This price adjustment, however, may deter potential new customers who turn to the more affordable non-upgrading competitor, referred to as the substitution effect. Despite attracting more loyal customers, the upgrading firm may experience a net loss in broader market share due to the substitution effect. In the bilateral upgrading equilibrium where both firms upgrade and engage in quality competition, the situation becomes akin to a prisoner’s dilemma if loyal customers show indifference to quality improvements. The gains from loyal customers are outweighed by fierce competition for new customers, ultimately disadvantaging both firms. Furthermore, our findings indicate that review-revealed quality difference between the two products leads to a higher degree of quality improvement effort by the high-quality firm, while reducing that of the low-quality firm. Intriguingly, in the unilateral equilibrium, the high-quality firm may not benefit from its review-revealed superior quality, while the low-quality firm may not be disadvantaged, depending on the substitution effect relatively.},
  archive      = {J_EJOR},
  author       = {Qiang Huang and Joshua Ignatius and Huaming Song and Junsong Bian and Canran Gong},
  doi          = {10.1016/j.ejor.2024.12.045},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {231-245},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Impact of loyal and new customer segments on product upgrades: The role of quality differentiation through online reviews},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal resource allocation: Convex quantile regression approach. <em>EJOR</em>, <em>324</em>(1), 221-230. (<a href='https://doi.org/10.1016/j.ejor.2025.01.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal allocation of resources across sub-units in the context of centralized decision-making systems such as bank branches or supermarket chains is a classical application of operations research and management science. In this paper, we develop quantile allocation models to examine how much the output and productivity could potentially increase if the resources were efficiently allocated between units. We increase robustness to random noise and heteroscedasticity by utilizing the local estimation of multiple production functions using convex quantile regression. The quantile allocation models then rely on the estimated shadow prices instead of detailed data of units and allow the entry and exit of units. Our empirical results on Finland’s business sector show that the marginal products of labor and capital largely depart from their respective marginal costs and also reveal that the current allocation of resources is far from optimal. A large potential for productivity gains could be achieved through better allocation, especially for the reallocation of capital, keeping the current technology and resources fixed.},
  archive      = {J_EJOR},
  author       = {Sheng Dai and Natalia Kuosmanen and Timo Kuosmanen and Juuso Liesiö},
  doi          = {10.1016/j.ejor.2025.01.003},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {221-230},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal resource allocation: Convex quantile regression approach},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Value-driven multidimensional welfare analysis: A dominance approach with application to comparisons of european populations. <em>EJOR</em>, <em>324</em>(1), 200-220. (<a href='https://doi.org/10.1016/j.ejor.2024.11.043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of comparing multidimensional probability distributions and its use in comparing the social welfare of different populations. We introduce theoretical results on two multidimensional stochastic orders, termed multidimensional first- and second-order dominance, that characterise the dominance relations and permit the practical comparison of discrete multidimensional probability distributions. Our results form the basis for a new framework for social welfare evaluation, which accommodates multiple dimensions of individual welfare, permits incorporating value judgements and enables robust social welfare comparisons. Our framework utilises non-decreasing and potentially concave multi-attribute functions to model individual welfare. We describe how this enables capturing a variety of trade-offs between welfare attributes as well as incorporating concerns about inequality in social welfare evaluation. Our framework also incorporates a welfare measurement scale. This facilitates a richer form of analysis, compared to other dominance-based methods, from which we can gauge the overall level of social welfare in different populations relative to some meaningful benchmarks, as opposed to deriving only partial rankings. We illustrate the application of our framework with a case study investigating social welfare across 31 European countries based on the EU-SILC dataset.},
  archive      = {J_EJOR},
  author       = {Nikolaos Argyris and Lars Peter Østerdal and M. Azhar Hussain},
  doi          = {10.1016/j.ejor.2024.11.043},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {200-220},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Value-driven multidimensional welfare analysis: A dominance approach with application to comparisons of european populations},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact and heuristic algorithms for cardinality-constrained assortment optimization problem under the cross-nested logit model. <em>EJOR</em>, <em>324</em>(1), 183-199. (<a href='https://doi.org/10.1016/j.ejor.2024.12.037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of assortment optimization problems where customers choose products according to the cross-nested logit (CNL) model and the number of products offered in the assortment cannot exceed a fixed number. Currently, no exact method exists for this NP-hard problem that can efficiently solve even small instances (e.g., 50 products with a cardinality limit of 10). In this paper, we propose an exact solution method that addresses this problem by finding the fixed point of a function through binary search. The parameterized problem at each iteration corresponds to a nonlinear binary integer programming problem, which we solve using a tailored Branch-and-Bound algorithm incorporating a novel variable-fixing mechanism, branching rule and upper bound generation strategy. Given that the computation time of the exact method can grow exponentially, we also introduce two polynomial-time heuristic algorithms with different solution strategies to handle larger instances. Numerical results demonstrate that our exact algorithm can optimally solve all test instances with up to 150 products and more than 90% of instances with up to 300 products within a one-hour time limit. Using the exact method as a benchmark, we find that the best-performing heuristic achieves optimal solutions for the majority of test instances, with an average optimality gap of 0.2%.},
  archive      = {J_EJOR},
  author       = {Le Zhang and Shadi Sharif Azadeh and Hai Jiang},
  doi          = {10.1016/j.ejor.2024.12.037},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {183-199},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Exact and heuristic algorithms for cardinality-constrained assortment optimization problem under the cross-nested logit model},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The multi-armed bandit problem under the mean-variance setting. <em>EJOR</em>, <em>324</em>(1), 168-182. (<a href='https://doi.org/10.1016/j.ejor.2025.03.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical multi-armed bandit problem involves a learner and a collection of arms with unknown reward distributions. At each round, the learner selects an arm and receives new information. The learner faces a tradeoff between exploiting the current information and exploring all arms. The objective is to maximize the expected cumulative reward over all rounds. Such an objective does not involve a risk-reward tradeoff, which is fundamental in many areas of application. In this paper, we build upon Sani et al. (2012)’s extension of the classical problem to a mean–variance setting. We relax their assumptions of independent arms and bounded rewards, and we consider sub-Gaussian arms. We introduce the Risk-Aware Lower Confidence Bound algorithm to solve the problem, and study some of its properties. We perform numerical simulations to demonstrate that, in both independent and dependent scenarios, our approach outperforms the algorithm suggested by Sani et al. (2012).},
  archive      = {J_EJOR},
  author       = {Hongda Hu and Arthur Charpentier and Mario Ghossoub and Alexander Schied},
  doi          = {10.1016/j.ejor.2025.03.011},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {168-182},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The multi-armed bandit problem under the mean-variance setting},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving markov decision processes via state space decomposition and time aggregation. <em>EJOR</em>, <em>324</em>(1), 155-167. (<a href='https://doi.org/10.1016/j.ejor.2025.01.037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although there are techniques to address large scale Markov decision processes (MDP), a computationally adequate solution of the so-called curse of dimensionality still eludes, in many aspects, a satisfactory treatment. In this paper, we advance in this issue by introducing a novel multi-subset partitioning scheme to allow for a distributed evaluation of the MDP, aiming to accelerate convergence and enable distributed policy improvement across the state space, whereby the value function and the policy improvement step can be performed independently, one subset at a time. The scheme’s innovation hinges on a design that induces communication properties that allow us to evaluate time aggregated trajectories via absorption analysis, thereby limiting the computational effort. The paper introduces and proves the convergence of a class of distributed time aggregation algorithms that combine the partitioning scheme with two-phase time aggregation to distribute the computations and accelerate convergence. In addition, we make use of Foster’s sufficient conditions for stochastic stability to develop a new theoretical result which underpins a partition design that guarantees that large regions of the state space are rarely visited and have a marginal effect on the system’s performance. This enables the design of approximate algorithms to find near-optimal solutions to large scale systems by focusing on the most visited regions of the state space. We validate the approach in a series of experiments featuring production and inventory and queuing applications. The results highlight the potential of the proposed algorithms to rapidly approach the optimal solution under different problem settings.},
  archive      = {J_EJOR},
  author       = {Rodrigo e Alvim Alexandre and Marcelo D. Fragoso and Virgílio J.M. Ferreira Filho and Edilson F. Arruda},
  doi          = {10.1016/j.ejor.2025.01.037},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {155-167},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Solving markov decision processes via state space decomposition and time aggregation},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure identification for partially linear partially concave models. <em>EJOR</em>, <em>324</em>(1), 142-154. (<a href='https://doi.org/10.1016/j.ejor.2025.01.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially linear partially concave models are semiparametric regression models that can capture linear and concavity-constrained nonlinear effects within one framework. A fundamental problem of this kind of model is deciding which covariates have linear effects and which covariates have strictly concave effects. Assuming that the true regression function is partially linear partially concave and sparse, we develop two structure selection procedures for classifying the covariates into linear, strictly concave, and irrelevant subsets. We show that the procedures based on penalized concavity-constrained additive regressions can correctly identify structures even if the underlying true functions are nonadditive; namely, the proposed procedures are additively faithful in a general setting. We prove that consistent structure selection is achievable when the total number of covariates and the number of concave covariates grow at polynomial rates with sample size. We introduce algorithms to implement the proposed procedures and demonstrate their performance by simulation analysis.},
  archive      = {J_EJOR},
  author       = {Jianhui Xie and Zhewen Pan},
  doi          = {10.1016/j.ejor.2025.01.014},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {142-154},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Structure identification for partially linear partially concave models},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supplier encroachment with decision biases. <em>EJOR</em>, <em>324</em>(1), 129-141. (<a href='https://doi.org/10.1016/j.ejor.2025.01.043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The retail market is being increasingly invaded by suppliers who are establishing their own direct selling channels, thanks to the rise of e-commerce and internet technology. The cost of direct sales has been identified as a crucial factor in the strategic interaction between suppliers and retailers. However, retailers often struggle to accurately assess this cost due to their decision biases. To address this issue, we propose a strategic mental model to examine how these biases impact supplier encroachment and firms’ performance outcomes. Our analysis reveals that a retailer can benefit from having an underestimation bias. Additionally, the bias of one firm can benefit the other, depending on whether it is an underestimation or overestimation bias. Interestingly, both the supplier and the retailer can earn more when they are biased compared to when neither of them is biased, resulting in a win-win scenario. However, if the supplier fails to recognize the retailer’s bias, the possibility of mutual gains is eliminated. Furthermore, when biases are present, the option of encroachment may backfire for the supplier compared to the scenario without encroachment, and the retailer may be worse off in the sequential encroachment setting compared to the simultaneous setting. Finally, we extend our model by considering imperfect substitutability between the retailer’s and supplier’s products, and the main conclusions remain robust. Our findings suggest that considering these biases alters the nature of strategic dynamics and provides new insights into supplier encroachment and information management.},
  archive      = {J_EJOR},
  author       = {Xiaolong Guo and Zenghui Su and Fangkezi Zhou},
  doi          = {10.1016/j.ejor.2025.01.043},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {129-141},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Supplier encroachment with decision biases},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordination of master planning in supply chains. <em>EJOR</em>, <em>324</em>(1), 118-128. (<a href='https://doi.org/10.1016/j.ejor.2025.01.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new mechanism for coordinating master planning in a buyer–supplier supply chain. Parties take different roles in the mechanism: there is an informed party (IP) and a reporting party (RP). The mechanism consists of three steps. First, the RP defines a lump sum payment, which he will receive if a supply proposal that deviates from the default is implemented. Second, parties generate new supply proposals based on mathematical programming models. The RP communicates his profit changes for the new proposals to the IP. Third, the IP decides whether to implement one of the new proposals and to pay to the RP the lump sum minus the RP’s local profit change. We show that the mechanism is strategyproof, budget-balanced and individually rational. If the RP has uniformly distributed prior knowledge about the actual surplus from coordination, at least 3/4 of the surplus will be realized on the average. Our computational tests suggest that the coordination mechanism is effective and versatile. It identifies near optimal solutions for various master planning models and outperforms an existing approach from literature.},
  archive      = {J_EJOR},
  author       = {Martin Albrecht},
  doi          = {10.1016/j.ejor.2025.01.033},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {118-128},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Coordination of master planning in supply chains},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep controlled learning for inventory control. <em>EJOR</em>, <em>324</em>(1), 104-117. (<a href='https://doi.org/10.1016/j.ejor.2025.01.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of Deep Reinforcement Learning (DRL) to inventory management is an emerging field. However, traditional DRL algorithms, originally developed for diverse domains such as game-playing and robotics, may not be well-suited for the specific challenges posed by inventory management. Consequently, these algorithms often fail to outperform established heuristics; for instance, no existing DRL approach consistently surpasses the capped base-stock policy in lost sales inventory control. This highlights a critical gap in the practical application of DRL to inventory management: the highly stochastic nature of inventory problems requires tailored solutions. In response, we propose Deep Controlled Learning (DCL), a new DRL algorithm designed for highly stochastic problems. DCL is based on approximate policy iteration and incorporates an efficient simulation mechanism, combining Sequential Halving with Common Random Numbers. Our numerical studies demonstrate that DCL consistently outperforms state-of-the-art heuristics and DRL algorithms across various inventory settings, including lost sales, perishable inventory systems, and inventory systems with random lead times. DCL achieves lower average costs in all test cases while maintaining an optimality gap of no more than 0.2%. Remarkably, this performance is achieved using the same hyperparameter set across all experiments, underscoring the robustness and generalizability of our approach. These findings contribute to the ongoing exploration of tailored DRL algorithms for inventory management, providing a foundation for further research and practical application in this area.},
  archive      = {J_EJOR},
  author       = {Tarkan Temizöz and Christina Imdahl and Remco Dijkman and Douniel Lamghari-Idrissi and Willem van Jaarsveld},
  doi          = {10.1016/j.ejor.2025.01.026},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {104-117},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Deep controlled learning for inventory control},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal outbound shipment policy for an inventory system with advance demand information. <em>EJOR</em>, <em>324</em>(1), 92-103. (<a href='https://doi.org/10.1016/j.ejor.2025.01.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines a single-echelon inventory system that fulfills stochastic orders from a production facility using a time-based shipment consolidation strategy. In this system, the production facility provides advance demand information to the warehouse, ensuring that all orders are placed with a positive demand lead time. Using value iteration, we identify the optimal outbound shipment quantities while accounting for costs related to early deliveries, late deliveries, and shipments. Additionally, this research highlights the impact of advance demand information on transportation capacity planning and the optimization of load factors The results from value iteration enable us to observe the general structure of the optimal dispatch policy, and we determine that it is a multidimensional threshold policy. Based on this observation, we introduce an approximated three-level threshold policy with acceptable performance. Furthermore, the decision itself is easier to interpret and to explain. To analyze large-scale instances, we compare several heuristic policies. First, we develop a deep reinforcement learning algorithm that approximates the value of the post-decision state instead of the pre-decision state. We compare our approach to value iteration and find that our method works very well; the average optimality gap is 0.08%. Additionally, three simple heuristic policies are proposed that might be justifiable in specific situations. Finally, we find that the value of advance demand information does not follow a linear pattern but decreases as the demand lead time increases. Furthermore, the transportation capacity should be planned in the range of the mean demand between two shipments.},
  archive      = {J_EJOR},
  author       = {Jana Ralfs and Dai T. Pham and Gudrun P. Kiesmüller},
  doi          = {10.1016/j.ejor.2025.01.020},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {92-103},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal outbound shipment policy for an inventory system with advance demand information},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating public transport in sustainable last-mile delivery: Column generation approaches. <em>EJOR</em>, <em>324</em>(1), 75-91. (<a href='https://doi.org/10.1016/j.ejor.2024.12.047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the problem of coordinating a three-echelon last-mile delivery system. In the first echelon, trucks transport parcels from distribution centres outside the city to public transport stops. In the second echelon, the parcels move on public transport and reach the city centre. In the third echelon, zero-emission vehicles pick up the parcels at public transport stops and deliver them to customers. We introduce two extended formulations for this problem. The first has two exponential sets of variables, while the second has one. We propose column generation algorithms and compare several methods to solve the pricing problems on specially constructed graphs. We also devise dual bounds, which we can compute even when the graphs are so large that not a single pricing round completes within the time limit. Compared to previous formulations, our models find 16 new best known solutions out of an existing dataset of 24 instances from the literature.},
  archive      = {J_EJOR},
  author       = {Diego Delle Donne and Alberto Santini and Claudia Archetti},
  doi          = {10.1016/j.ejor.2024.12.047},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {75-91},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integrating public transport in sustainable last-mile delivery: Column generation approaches},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic worker allocation in seru production systems with actor–critic and pointer networks. <em>EJOR</em>, <em>324</em>(1), 62-74. (<a href='https://doi.org/10.1016/j.ejor.2025.01.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the rapid evolution of manufacturing industries, customer demands may change dramatically, which challenges the conventional production systems. Seru production system (SPS) is a key to deal with uncertain varieties and fluctuating volumes. In dynamic scenarios, orders with uncertain demands arrive over time. For each arriving order, appropriate workers should be allocated to assemble it. This study investigates the dynamic worker allocation problem with the objective of maximizing the revenue obtained by the SPS. To tackle this problem, a novel algorithm that integrates actor–critic and pointer networks is proposed. The global-and-local attention mechanism and twin focus encoders are particularly designed to address the dynamic and uncertain properties of the problem. The algorithm is compared to three approaches, including the standard actor–critic algorithm, proximal policy optimization algorithm, and the approximation algorithm with the best approximation ratio, in different scenarios, i.e., small, medium, and large factories. The proposed algorithm outperforms the standard actor–critic approach and proximal policy optimization algorithm, showing performance gaps ranging from 7.23% to 37.44%. It also outperforms the approximation algorithm, with gaps between 56.73% and 96.94%. Numerical results of the three scenarios show that the proposed algorithm is more efficient and effective in handling uncertainty and dynamics, making it a promising solution for real-world manufacturing production systems.},
  archive      = {J_EJOR},
  author       = {Dongni Li and Hongbo Jin and Yaoxin Zhang},
  doi          = {10.1016/j.ejor.2025.01.012},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {62-74},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Dynamic worker allocation in seru production systems with actor–critic and pointer networks},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simulation–optimization approach for capacitated lot-sizing in a multi-level pharmaceutical tablets manufacturing process. <em>EJOR</em>, <em>324</em>(1), 49-61. (<a href='https://doi.org/10.1016/j.ejor.2025.01.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses an iterative simulation–optimization approach to estimate high-quality solutions for the multi-level capacitated lot-sizing problem with linked lot sizes and backorders (MLCLSP-L-B) based on probabilistic demand. It presents the application of the Generalized Uncertainty Framework (GUF) to the MLCLSP-L-B. The research provides an exact mathematical problem formulation and a variable neighborhood search (VNS) algorithm for the GUF. The evaluation procedure uses anonymized real-world data of multi-level pharmaceutical tablets manufacturing processes. It compares the GUF against a two-stage stochastic programming (SP) approach from the literature regarding manufacturing costs and customer service levels. Finally, planning rules and managerial insights are given for the tablets manufacturing processes.},
  archive      = {J_EJOR},
  author       = {Michael Simonis and Stefan Nickel},
  doi          = {10.1016/j.ejor.2025.01.028},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {49-61},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A simulation–optimization approach for capacitated lot-sizing in a multi-level pharmaceutical tablets manufacturing process},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-machine scheduling with fixed energy recharging times to minimize the number of late jobs and the number of just-in-time jobs: A parameterized complexity analysis. <em>EJOR</em>, <em>324</em>(1), 40-48. (<a href='https://doi.org/10.1016/j.ejor.2025.01.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study single-machine scheduling problems where processing each job requires both processing time and rechargeable energy. Subject to a predefined energy capacity, energy can be recharged after each job during a fixed recharging period. Our focus is on two due date-related scheduling criteria: minimizing the number of late jobs and maximizing the weighted number of jobs completed exactly at their due dates. This study aims to analyze the parameterized tractability of the two problems and develop fixed-parameter algorithms with respect to three natural parameters: the number of different due dates v d , the number of different processing times v p , and the number of different energy consumptions v e . Following the proofs of NP -hardness across several contexts, we demonstrate that both problems remain intractable when parameterized by v d and v p . To complement our results, we show that both problems become fixed-parameter tractable (FPT) when parameterized by v e and v d , and are solvable in polynomial time when both v e and v p are constant.},
  archive      = {J_EJOR},
  author       = {Renjie Yu and Daniel Oron},
  doi          = {10.1016/j.ejor.2025.01.007},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {40-48},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Single-machine scheduling with fixed energy recharging times to minimize the number of late jobs and the number of just-in-time jobs: A parameterized complexity analysis},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The dynamic team orienteering problem. <em>EJOR</em>, <em>324</em>(1), 22-39. (<a href='https://doi.org/10.1016/j.ejor.2025.01.009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a new dynamic routing problem, namely the Dynamic Team Orienteering Problem (DTOP), which is a dynamic variant of the Team Orienteering Problem (TOP). In the DTOP, some customer locations are known a priori, while others are dynamic, with each location associated with a profit value. The goal is to maximize the sum of collected profits by visiting a set of customer locations within a time limit. This problem arises in several practical applications such as disaster relief, technician, tourist, and school bus routing problems. We adopt a Multiple Plan Approach (MPA) to solve the proposed problem, utilizing both a consensus function method and a demand-served method to select the distinguished plan—the most promising solution from a pool of alternative routing plans. To assess the effectiveness of these methods, we employ a sophisticated greedy algorithm tailored to address the unique challenges posed by the DTOP. In addition, we employ a reference offline algorithm designed for solving the static variant of the problem. To facilitate our evaluation, we introduce a comprehensive set of 1161 new benchmark instances for the DTOP, adapted from well-established TOP benchmark instances. Our comparative analysis centers on the average percentage deviation of algorithmic solutions from the reference offline solutions.},
  archive      = {J_EJOR},
  author       = {Emre Kirac and Ashlea Bennett Milburn and Ridvan Gedik},
  doi          = {10.1016/j.ejor.2025.01.009},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {22-39},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The dynamic team orienteering problem},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tournament design: A review from an operational research perspective. <em>EJOR</em>, <em>324</em>(1), 1-21. (<a href='https://doi.org/10.1016/j.ejor.2024.10.044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every sport needs rules. Tournament design refers to the rules that determine how a tournament, a series of games between a number of competitors, is organized. This study aims to provide an overview of the tournament design literature from the perspective of operational research. Three important design criteria are discussed: efficacy, fairness, and attractiveness. Our survey classifies the papers discussing these properties according to the main components of tournament design: format, seeding, draw, scheduling, and ranking. We also outline several open questions and promising directions for future research.},
  archive      = {J_EJOR},
  author       = {Karel Devriesere and László Csató and Dries Goossens},
  doi          = {10.1016/j.ejor.2024.10.044},
  journal      = {European Journal of Operational Research},
  month        = {7},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Tournament design: A review from an operational research perspective},
  volume       = {324},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating the research and development performance of chinese industry: A two-stage prospect data envelopment analysis approach. <em>EJOR</em>, <em>323</em>(3), 1040-1059. (<a href='https://doi.org/10.1016/j.ejor.2025.01.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With growing investments inindustry research and development (R&D) innovation in China, evaluating whether R&D resources assigned to industries areeffectively used is essential. However, limited research has been conducted on the assessment of R&D effectiveness in Chinese industries that encompasses both the internal process of R&D production and the psychological risks encountered by decision-makers (DMs).Hence, this study puts forward a two-stage prospect data envelopment analysis approach that can characterise the risk attitude of DM in evaluation. By employing this approach, we assess the R&D activities of 28 industries in China from an overall perspective and explore the actual influence of DMs’ risk psychology on the evaluation results through sensitivity and comparative analyses. Furthermore, we categorise the R&D performance of 28 Chinese industries into four quadrants for analysis and focus on the R&D performance of key industries such as extraction of petroleum and natural gas, mining of ferrous metal ores and manufacture of tobacco. Based on the findings, we provide a range of policy recommendations regarding the R&D activities of Chinese industries.},
  archive      = {J_EJOR},
  author       = {Hui-hui Liu and Guo-liang Yang and Jian-wei Gao and Ya-ping Wang and Guo-hua Ni},
  doi          = {10.1016/j.ejor.2025.01.002},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {1040-1059},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Investigating the research and development performance of chinese industry: A two-stage prospect data envelopment analysis approach},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Business cycle and realized losses in the consumer credit industry. <em>EJOR</em>, <em>323</em>(3), 1024-1039. (<a href='https://doi.org/10.1016/j.ejor.2024.12.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the determinants of losses given default (LGD) in consumer credit. Utilizing a unique dataset encompassing over 6 million observations of Italian consumer credit over a long time span, we find that macroeconomic and social (MS) variables significantly enhance the forecasting performance at both individual and portfolio levels, improving R 2 by up to 10 percentage points. Our findings are robust across various model specifications. Non-linear forecast combination schemes employing neural networks consistently rank among the top performers in terms of mean absolute error, RMSE, R 2 , and model confidence sets in every tested scenario. Notably, every model that belongs to the superior set systematically includes MS variables. The relationship between expected LGD and macro predictors, as revealed by accumulated local effects plots and Shapley values, supports the intuition that lower real activity, a rising cost-of-debt to GDP ratio, and heightened economic uncertainty are associated with higher LGD for consumer credit. Our results on the influence of MS variables complement and slightly differ from those of related papers. These discrepancies can be attributed to the comprehensive nature of our database – spanning broader dimensions in space, time, sectors, and types of consumer credit – the variety of models utilized, and the analyses conducted.},
  archive      = {J_EJOR},
  author       = {Walter Distaso and Francesco Roccazzella and Frédéric Vrins},
  doi          = {10.1016/j.ejor.2024.12.026},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {1024-1039},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Business cycle and realized losses in the consumer credit industry},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expected information of noisy attribute forecasts for probabilistic forecasts. <em>EJOR</em>, <em>323</em>(3), 1013-1023. (<a href='https://doi.org/10.1016/j.ejor.2024.12.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper extends the maximum entropy (ME) model to include uncertainty about noisy moment forecasts. In this framework the noise propagates to the ME model through the constrained optimization’s Lagrange multipliers. The mutual information and expected Fisher information are included for assessing effects of the noisy moment forecasts on the ME model and its parameters. A new mean–variance decomposition of the mutual information is derived for the normal distribution when the mean and variance are both noisy. A simulation estimator is used to estimate the expected information for noisy ME models on finite support. A family of ensemble of individual level noisy ME forecast models is introduced which includes individual level versions of the conditional logit and multiplicative competitive interaction models as specific cases. To illustrate the implementation and merits of the proposed noisy ME framework, the classic loaded dice problem and discrete choice analysis are examined.},
  archive      = {J_EJOR},
  author       = {Omid M. Ardakani and Robert F. Bordley and Ehsan S. Soofi},
  doi          = {10.1016/j.ejor.2024.12.024},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {1013-1023},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Expected information of noisy attribute forecasts for probabilistic forecasts},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does the quantity discount mechanism offer a loophole for retailer collusion? impacts and responses. <em>EJOR</em>, <em>323</em>(3), 999-1012. (<a href='https://doi.org/10.1016/j.ejor.2024.12.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantity discount mechanism is an effective and widely used tool by manufacturers to encourage downstream retailers to increase their order volumes. As wholesale prices decrease with larger order quantities, retailers have an incentive to collude and achieve joint procurement. Two joint procurement modes—group buying (GB) and agency procurement (AP)—are considered to characterize the phenomenon of retailer collusion. In GB mode, retailers purchase as a group and enjoy the same per-unit wholesale price. In contrast, in the AP mode, a leading retailer assumes responsibility for aggregating orders and submitting the total order to the manufacturer while having the authority to set the resale price. A dual-channel model is developed to investigate joint procurement among competing retailers, aiming to identify its underlying driving forces and impacts. Our findings indicate that, compared to individual purchasing (IP), GB is always attainable for retailers, whereas AP is only attainable under intense competition when retailers are symmetric. We reveal that retailers engaging in joint procurement do not always aim to achieve lower wholesale prices. In some cases, the objective may be to mitigate price competition. This finding suggests that joint procurement by retailers results in a reduction in total order quantity, which significantly diminishes the manufacturer’s profit. In response to the challenges of retailer collusion, we explore the feasibility and potential value of offering a coordinated quantity discount mechanism, wherein the manufacturer gives up the pursuit of maximizing its own profit in favor of optimizing the profits of the entire supply chain, making concessions to the retailers. We identify the scenarios in which a coordinated quantity discount contract can eliminate the loophole for retailer collusion and highlight both the value and necessity of achieving contract coordination.},
  archive      = {J_EJOR},
  author       = {Shaofu Du and Xiahui Sun and Li Hu and Tsan-Ming Choi},
  doi          = {10.1016/j.ejor.2024.12.007},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {999-1012},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Does the quantity discount mechanism offer a loophole for retailer collusion? impacts and responses},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-activity shift scheduling under uncertainty: The value of shift flexibility. <em>EJOR</em>, <em>323</em>(3), 988-998. (<a href='https://doi.org/10.1016/j.ejor.2024.12.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a multi-activity shift scheduling problem under demand uncertainty, exploring various levels of flexibility in adapting aspects of the shift schedule (e.g., activity assignment, break assignment, selection of shift type and shift end time) to late-arriving demand information. To address the resulting complex two-stage stochastic combinatorial optimisation problems, we propose a novel two-stage stochastic mixed-integer programming formulation leveraging state-expanded networks and a clustering-based sequential sampling approach for efficiently solving large-scale problem instances. In computational experiments on stochastic problems derived from well-known multi-activity shift scheduling instances, we show that this method effectively solves instances with up to 10 activities and 100 demand scenarios, approaching near-optimality within an average time of less than one hour. From a managerial standpoint, our study provides insights into the structure of good first-stage scheduling decisions as well as into the impact of different flexibility levels on expected costs of the solutions, thereby offering valuable support for decisions such as adjusting employees’ salaries in exchange for increased shift flexibility.},
  archive      = {J_EJOR},
  author       = {Felix Hagemann and Till Frederik Porrmann and Michael Römer},
  doi          = {10.1016/j.ejor.2024.12.028},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {988-998},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multi-activity shift scheduling under uncertainty: The value of shift flexibility},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring technical efficiency under variable returns to scale using debreu's loss function. <em>EJOR</em>, <em>323</em>(3), 975-987. (<a href='https://doi.org/10.1016/j.ejor.2024.12.050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a model that makes two contributions to the measurement of technical efficiency under a technology with variable returns to scale. First, the criteria for identifying an optimal benchmark are not limited to technical dominance and Pareto efficiency, but also include maximum average productivity, defined as the ratio between a weighted linear aggregate of outputs and inputs. Second, the paper contributes a conceptual basis for correcting the shadow prices of inputs and outputs to reflect the influence of returns to scale. Debreu's loss function is used to value inefficiency as the difference between the virtual input and output using the shadow prices of the supporting hyperplane at the optimal reference. The efficiency score is a virtual profitability index with endogenous shadow prices that reflect the valuation of inputs and outputs with a microeconomic rationale, i.e., it is not a distance measure based on aggregation with exogenous weights of the difference between observed and optimal quantities. Two further results follow from these contributions. First, the radial input-output orientation to maximise productivity is endogenous. It is conditioned by the nature of the returns to scale. Second, the efficiency measure based on the loss function exhibits the desirable properties in a radial context, including the indication property, because the efficiency score incorporates non-radial slack.},
  archive      = {J_EJOR},
  author       = {Juan José Díaz-Hernández and David-José Cova-Alonso and Eduardo Martínez-Budría},
  doi          = {10.1016/j.ejor.2024.12.050},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {975-987},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Measuring technical efficiency under variable returns to scale using debreu's loss function},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The copeland ratio ranking method for abstract decision problems. <em>EJOR</em>, <em>323</em>(3), 966-974. (<a href='https://doi.org/10.1016/j.ejor.2024.12.042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problem of ranking a finite number of alternatives on the basis of a dominance relation. We firstly investigate some disadvantages of the Copeland ranking method, of the degree ratio ranking method and of the modified degree ratio ranking method which were characterized by using clone properties and classical axiomatic properties. Then, we introduce some alternative axiomatic properties and propose a new ranking method which is defined by the Copeland ratio of alternatives (i.e., the Copeland score of an alternative divided by its total degree). We show that this proposed ranking method coincides with the Copeland ranking method, the degree ratio ranking method and the modified degree ratio ranking method for abstract decision problems with complete and asymmetric dominance relations. Subsequently, we prove that this new ranking method is able to overcome the mentioned disadvantages of these ranking methods. After that, we provide a characterization for the Copeland ratio ranking method using the introduced axiomatic properties.},
  archive      = {J_EJOR},
  author       = {Weibin Han and Adrian Van Deemen},
  doi          = {10.1016/j.ejor.2024.12.042},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {966-974},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The copeland ratio ranking method for abstract decision problems},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective evolutionary algorithm with mutual-information-guided improvement phase for feature selection in complex manufacturing processes. <em>EJOR</em>, <em>323</em>(3), 952-965. (<a href='https://doi.org/10.1016/j.ejor.2024.12.036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex manufacturing processes (CMP) involve numerous features that impact product quality. Therefore, selecting key process features (KPF) is crucial for effective quality prediction and control in CMPs. This paper proposes a KPF (feature) selection method for the high-dimensional CMP data. The KPF selection problem is formulated as a bi-objective combinatorial optimization task of maximizing the geometric mean measure and minimizing the number of selected features. To solve this challenging high-dimensional KPF selection problem, we propose a novel multi-objective evolutionary algorithm (MOEA) called NSGAII-MIIP. NSGAII-MIIP applies an improvement phase (called MIIP) to purify the non-dominated solutions obtained by genetic operators during the iteration process to improve the FS performance. The improvement phase is guided by a mutual-information-based feature importance measure considering both a feature’s relevance degree to class (product quality level) and its redundancy degree to selected features. This allows MIIP to efficiently update non-dominated solutions by selecting relevant features and eliminating redundant features. Moreover, MIIP is seamlessly integrated into the solution ranking process of NSGAII-MIIP so that solutions from the improvement phase can be ranked together with original solutions in the population efficiently. Experiments on eight datasets show that NSGAII-MIIP has better KPF selection performance than eight state-of-the-art multi-objective FS methods. Moreover, NSGAII-MIIP exhibits superior search performance compared to eight typical multi-objective optimization algorithms.},
  archive      = {J_EJOR},
  author       = {An-Da Li and Zhen He and Qing Wang and Yang Zhang and Yanhui Ma},
  doi          = {10.1016/j.ejor.2024.12.036},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {952-965},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A multi-objective evolutionary algorithm with mutual-information-guided improvement phase for feature selection in complex manufacturing processes},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opinion convergence and management: Opinion dynamics in interactive group decision-making. <em>EJOR</em>, <em>323</em>(3), 938-951. (<a href='https://doi.org/10.1016/j.ejor.2024.12.046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making processes are significantly influenced by internal social network interactions and external information inputs. While previous research has highlighted the role of social networks in opinion evolution, the dynamics of information dissemination and its interaction with these networks are less understood. To bridge this gap, we introduce the Social-Information-Opinion Dynamic Supernetwork (SIO-DS) model, which integrates critical factors such as the impact of external information and opinion propagation, alongside the influence of internal social network structures and individual willingness to adjust opinions. This model takes into account the varied levels of confidence and individualized dynamic influence among decision makers, recognizing both their asymmetry and diversity. It performs opinion dynamics using bounded confidence models and parameters that govern information dissemination. We found that scale-free networks, which feature influential leaders, are more effective at reaching consensus compared to small-world networks, which are hindered by limited inter-group connections. The speed of information dissemination is critical; moderate speeds help in maintaining a stable consensus by balancing social influence, while very fast or slow speeds risk exacerbating polarization based on how social influence is managed. The SIO-DS model has broad implications for enhancing decision-making in corporate management by optimizing network structures, in public policy by managing public opinion, and in crisis management by developing effective communication strategies. Ultimately, this model not only deepens our understanding of opinion dynamics but also provides practical tools for improving decision-making quality and efficiency in various contexts.},
  archive      = {J_EJOR},
  author       = {Yuan Xu and Shifeng Liu and T.C.E. Cheng and Xue Feng and Jun Wang and Xiaopu Shang},
  doi          = {10.1016/j.ejor.2024.12.046},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {938-951},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Opinion convergence and management: Opinion dynamics in interactive group decision-making},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven preference learning methods for sorting problems with multiple temporal criteria. <em>EJOR</em>, <em>323</em>(3), 918-937. (<a href='https://doi.org/10.1016/j.ejor.2024.12.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present novel preference learning approaches for sorting problems with multiple temporal criteria. They leverage an additive value function as the basic preference model, adapted for accommodating time series data. Given assignment examples concerning reference alternatives, we learn such a model using convex quadratic programming. It is characterized by fixed-time discount factors and operates within a regularization framework. This approach enables the consideration of temporal interdependencies between timestamps while mitigating the risk of overfitting. To enhance scalability and accommodate learnable time discount factors, we introduce a novel monotonic Recurrent Neural Network (mRNN). It captures the evolving dynamics of preferences over time, while upholding critical properties inherent to multiple criteria sorting problems. These include criteria monotonicity, preference independence, and the natural ordering of classes. The proposed mRNN can describe the preference dynamics by depicting piecewise linear marginal value functions and personalized time discount factors along with time. Thus, it effectively combines the interpretability of traditional sorting methods with the predictive potential offered by deep preference learning models. We comprehensively assess the proposed models on synthetic data scenarios and a real-case study centered on classifying valuable users within a mobile gaming app based on their historical in-game behavioral sequences. Empirical findings underscore the notable performance improvements achieved by the proposed models when compared to a spectrum of baseline methods, spanning machine learning, deep learning, and conventional multiple criteria sorting approaches.},
  archive      = {J_EJOR},
  author       = {Yijun Li and Mengzhuo Guo and Miłosz Kadziński and Qingpeng Zhang and Chenxi Xu},
  doi          = {10.1016/j.ejor.2024.12.020},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {918-937},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Data-driven preference learning methods for sorting problems with multiple temporal criteria},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conical free disposal hull estimators of directional distances and luenberger productivity indices for general technologies. <em>EJOR</em>, <em>323</em>(3), 907-917. (<a href='https://doi.org/10.1016/j.ejor.2024.12.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directional distances are a popular tool in productivity and efficiency analysis due to their versatility in evaluating the distance of Decision Making Units (DMU) to the efficient frontier of the production set. The theoretical and statistical properties of these measures are well-established in various contexts. However, the measurement of directional distances to the cone spanned by the attainable set has not yet been explored. This cone is necessary to define the Luenberger indices for general technologies. This paper aims to fill this gap by presenting a method for defining and estimating directional distances to this cone, applicable to general technologies without imposing convexity. We also discuss the statistical properties of these measures, enabling us to measure distances to non-convex attainable sets under Constant Returns to Scale (CRS), as well as measure and estimate Luenberger productivity indices and their decompositions for general technologies. In addition, we provide a detailed description of how to make inferences on these indices. Finally, we offer simulated data and a practical example of inference on Luenberger productivity indices and their decompositions using a well-known real data set.},
  archive      = {J_EJOR},
  author       = {Cinzia Daraio and Simone Di Leo and Léopold Simar},
  doi          = {10.1016/j.ejor.2024.12.025},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {907-917},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Conical free disposal hull estimators of directional distances and luenberger productivity indices for general technologies},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The collaborative berth allocation problem with row-generation algorithms for stable cost allocations. <em>EJOR</em>, <em>323</em>(3), 888-906. (<a href='https://doi.org/10.1016/j.ejor.2024.12.048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent supply chain disruptions and crisis response policies (e.g., the COVID-19 pandemic and the Red Sea crisis) have highlighted the role of container terminals as crucial and scarce resources in the global economy. To tackle these challenges, the industry increasingly aims for advanced operational collaboration among multiple stakeholders, as demonstrated by the ambitions of the recently founded Gemini alliance. Nonetheless, collaborative planning models often disregard the requirements and incentives of stakeholders or simply solve idealized small instances. Motivated by the above, we design novel and effective collaboration mechanisms among terminal operators that share the resources (berths and quay cranes). We first define the collaborative berth allocation problem and propose a mixed integer linear programming (MILP) model to minimize the total cost of all terminals, referred to as the coalitional costs. We adopt the core and the nucleolus concepts from cooperative game theory to allocate the coalitional costs such that stakeholders have stable incentives to collaborate. To obtain solutions for realistic instance sizes, we propose two exact row-generation-based core and nucleolus algorithms that are versatile and can be used for various combinatorial optimization problems. To the best of our knowledge, the proposed row-generation approach for the nucleolus is the first of its kind for combinatorial optimization problems. Extensive experiments demonstrate that the collaborative berth allocation approach achieves up to 28.44% of cost savings, increasing the solution space in disruptive situations, while the proposed core and nucleolus solutions guarantee the collaboration incentives for individual terminals.},
  archive      = {J_EJOR},
  author       = {Xiaohuan Lyu and Eduardo Lalla-Ruiz and Frederik Schulte},
  doi          = {10.1016/j.ejor.2024.12.048},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {888-906},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The collaborative berth allocation problem with row-generation algorithms for stable cost allocations},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic decentralization of self-branded and contract manufacturing businesses. <em>EJOR</em>, <em>323</em>(3), 868-887. (<a href='https://doi.org/10.1016/j.ejor.2025.01.017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the incentive of a competitive contract manufacturer (CCM) to adopt a decentralized structure by segregating contract manufacturing from its self-branded business. We consider an original equipment manufacturer (OEM) with the option to outsource production either to a CCM producing its self-branded product, or to a non-competitive contract manufacturer (NCM) also serving another OEM. The CCM has the option to centralize or decentralize its two businesses and competes in quantity with both OEMs in the end-user market. Our analysis of the strategic interactions between the OEM's outsourcing decision and the CCM's organizational structure choice shows that the likelihood of the OEM outsourcing to the CCM increases when the CCM adopts a decentralized structure compared to a centralized one. Under decentralization, a sufficiently low wholesale price offered by the contract manufacturing division provides the OEM with a competitive advantage. Consequently, the CCM is motivated to strategically deploy a decentralized structure to attract contract manufacturing business from the OEM, even though decentralization yields a lower profit than centralization. However, the CCM must be cautious when implementing a decentralized structure to secure orders from the OEM. The resulting intensified market competition undermines its profit from self-branded business and potentially makes it worse off from producing for the OEM. In such case, the CCM should maintain a centralized structure and uphold a purely competitive relationship with the OEM. Moreover, we demonstrate how the profitability of another OEM supplied by the NCM is influenced by the interplay between the CCM and the OEM.},
  archive      = {J_EJOR},
  author       = {Wei Li and Yanglei Li and Jing Chen and Bintong Chen},
  doi          = {10.1016/j.ejor.2025.01.017},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {868-887},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Strategic decentralization of self-branded and contract manufacturing businesses},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Managing social responsibility efforts with the consideration of violation probability. <em>EJOR</em>, <em>323</em>(3), 852-867. (<a href='https://doi.org/10.1016/j.ejor.2025.01.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corporate social responsibility (CSR) has a strong impact on the external image of the enterprise. The violation of CSR not only harms the enterprise but also negatively affects other firms in the supply chain. This paper establishes a game-theoretical model to study the management of social responsibility efforts with considerations of violation probability. The upstream manufacturer and downstream retailer can reduce the violation probability by exerting CSR efforts. Specifically, we study the following four models, including both participants exerting efforts, only the manufacturer exerting effort, only the retailer exerting effort, and neither participant exerting effort. Our analysis shows that as the effort cost of the manufacturer increases, the retailer may increase or decrease his effort level under both participants exerting efforts, due to the complementary and substitution effects between the efforts of the manufacturer and retailer. We also find that compared with both participants exerting efforts, the retailer may increase or decrease his effort level under only the retailer exerting effort, and the effort level of the manufacturer may grow or shrink under only the manufacturer exerting effort. In addition, we study the decision matrix for the manufacturer and retailer, and find that in equilibrium the manufacturer always has incentives to exert CSR effort, while the retailer may prefer a free ride and sometimes chooses not to exert effort. Interestingly, we find that the total supply chain profit may not be the highest under both participants exerting efforts, but it is the lowest under neither participant exerting effort.},
  archive      = {J_EJOR},
  author       = {Jiayan Xu and Housheng Duan and Sijing Deng},
  doi          = {10.1016/j.ejor.2025.01.016},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {852-867},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Managing social responsibility efforts with the consideration of violation probability},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact algorithms for routing electric autonomous mobile robots in intralogistics. <em>EJOR</em>, <em>323</em>(3), 830-851. (<a href='https://doi.org/10.1016/j.ejor.2024.12.041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In intralogistics and manufacturing, autonomous mobile robots (AMRs) are usually electrically powered and recharged by battery swapping or induction. We investigate AMR route planning in these settings by studying different variants of the electric vehicle routing problem with due dates (EVRPD). We consider three common recharging strategies: battery swapping, inductive recharging with full recharges, and inductive recharging with partial recharges. Moreover, we consider two different objective functions: the standard objective of minimizing the total distance traveled and the minimization of the total completion times of transport jobs. The latter is of particular interest in intralogistics, where time aspects are of crucial importance and the earliest possible completion of jobs often has priority. In this context, recharging decisions also play an essential role. For solving the EVRPD variants, we propose exact branch-price-and-cut algorithms that rely on ad-hoc labeling algorithms tailored to the respective variants. We perform an extensive computational study to generate managerial insights on the AMR route planning problem and to assess the performance of our solution approach. The experiments are based on newly introduced instances featuring typical characteristics of AMR applications in intralogistics and manufacturing and on standard benchmark instances from the literature. The detailed analysis of our results reveals that inductive recharging with partial recharges is competitive with battery swapping, while using a full-recharges strategy has considerable drawbacks in an AMR setup.},
  archive      = {J_EJOR},
  author       = {Anne Meyer and Timo Gschwind and Boris Amberg and Dominik Colling},
  doi          = {10.1016/j.ejor.2024.12.041},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {830-851},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Exact algorithms for routing electric autonomous mobile robots in intralogistics},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexibility-based price discrimination in a competitive context considering consumers’ socioeconomic status. <em>EJOR</em>, <em>323</em>(3), 810-829. (<a href='https://doi.org/10.1016/j.ejor.2025.01.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the impact of flexibility-based price discrimination (FBPD) on the pricing and quality strategy of the adopting firm and its competitor, as well as the impact on the welfare of consumers. We assume that the inflexible consumers being targeted for price discrimination can be either high-income consumers or low-income consumers, and the high-income consumers are more sensitive to product quality. We show that depending on who the targeted inflexible consumers are, the impact of FBPD on all firms and consumers can be either negative or positive. If an FBPD is to exploit the inflexibility of low-income consumers, it will not only make the vulnerable group even more disadvantaged but also lower the firms’ incentive to produce high-quality products. On the contrary, if an FBPD is to exploit the inflexibility of high-income consumers, it will increase the firms’ incentive to produce high-quality products, and the targeted consumers will be compensated by having higher quality products. However, the firms might engage in excessive quality enhancement, leading to a situation where the competition between the firms falls into a prisoner’s dilemma. Our research results suggest that the application of FBPD could necessitate a comprehensive regulatory framework to ensure ethical implementation while safeguarding consumer welfare, particularly that of vulnerable groups.},
  archive      = {J_EJOR},
  author       = {Jian Zhang and Emily B. Laidlaw and Raymond A. Patterson},
  doi          = {10.1016/j.ejor.2025.01.005},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {810-829},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Flexibility-based price discrimination in a competitive context considering consumers’ socioeconomic status},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinate or collaborate? reducing food waste in perishable-product supply chains. <em>EJOR</em>, <em>323</em>(3), 795-809. (<a href='https://doi.org/10.1016/j.ejor.2024.12.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing food waste in supply chains (SCs) with multiple decision-makers is challenging. A common approach grocery retailers use to reduce waste is requiring manufacturers to only send products with a long remaining shelf life (“minimum life on receipt”-MLOR). However, its impact on manufacturers remains unclear. To evaluate the effectiveness of MLOR agreements on food waste, we investigate two strategies: (1) collaborating on setting the MLOR level and (2) coordinating the SC via contract. Through collaboration, we analytically show that if the MLOR agreement does not demand solely fresh products, it raises manufacturer profits, enabling potential wholesale price reduction. This might incentivize retailers to collaborate to reduce the MLOR level. We demonstrate that the coordinating strategy can reduce waste in the SC and is most beneficial when the wholesale price is high, and the issuing policy is FIFO. We introduce possible coordination contracts and show that in coordinated SCs, manufacturers always provide the highest MLOR level without requiring any restrictive MLOR agreements. Governments mainly focus on reducing retail waste and promoting retailers to request higher MLOR. However, these efforts can backfire by creating more waste for manufacturers. Reducing the MLOR allows retailers to negotiate lower wholesale prices, increasing profitability while reducing waste. Although SC coordination is known for reducing inefficiency, it may not be the best strategy for reducing waste, especially when the issuing policy is more LIFO than FIFO. Specifically, while coordination might be a better strategy for online retailers, collaboration can be a better strategy for brick-and-mortar retailers.},
  archive      = {J_EJOR},
  author       = {Navid Mohamadi and Sandra Transchel and Jan C. Fransoo},
  doi          = {10.1016/j.ejor.2024.12.039},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {795-809},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Coordinate or collaborate? reducing food waste in perishable-product supply chains},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Judgmental selection of parameters for simple forecasting models. <em>EJOR</em>, <em>323</em>(3), 780-794. (<a href='https://doi.org/10.1016/j.ejor.2024.12.034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era dominated by big data and machine and deep learning solutions, judgment has still an important role to play in decision making. Behavioural operations are on the rise as judgment complements automated algorithms in many practical settings. Over the years, new and exciting uses of judgment have emerged, with some providing fresh and innovative insights on algorithmic approaches. The forecasting field, in particular, has seen judgment infiltrating in several stages of the forecasting process, such as the production of purely judgmental forecasts, judgmental revisions of formal (statistical) forecasts, and as an alternative to statistical selection between forecasting models. In this paper, we take the first steps towards exploring a neglected use of judgment in forecasting: the manual selection of the parameters for forecasting models. We focus on a simple but widely-used forecasting model, the Simple Exponential Smoothing, and, through a behavioural experiment, we investigate the performance of individuals versus algorithms in selecting optimal modelling parameters under different conditions. Our results suggest that the use of judgment on the task of parameter selection could improve forecasting accuracy. However, individuals also suffer from anchoring biases.},
  archive      = {J_EJOR},
  author       = {Fotios Petropoulos and Evangelos Spiliotis},
  doi          = {10.1016/j.ejor.2024.12.034},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {780-794},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Judgmental selection of parameters for simple forecasting models},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trade-off between utility and fairness in two-agent single-machine scheduling. <em>EJOR</em>, <em>323</em>(3), 767-779. (<a href='https://doi.org/10.1016/j.ejor.2025.01.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem arising when two agents, each owning a set of jobs, compete to schedule their jobs on a common processing resource. Each schedule implies a certain utility for each agent and an overall system utility. We are interested in solutions that incorporate some criterion of fairness for the agents and, at the same time, are satisfactory from the viewpoint of system utility. More precisely, we investigate the trade-off between fairness and system utility when both agents want to minimize the total completion time of their respective jobs. We analyze the structure of the set of such trade-off solutions, and propose an exact algorithm for their computation, based on the Lagrangian relaxation of a MILP formulation of the problem. A large set of computational experiments has been carried out to show the viability of the approach. Moreover, the results show that in most cases a solution having a high degree of fairness can be obtained by sacrificing a very limited amount of system utility.},
  archive      = {J_EJOR},
  author       = {Alessandro Agnetis and Mario Benini and Gaia Nicosia and Andrea Pacifici},
  doi          = {10.1016/j.ejor.2025.01.025},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {767-779},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Trade-off between utility and fairness in two-agent single-machine scheduling},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new effective heuristic for the prisoner transportation problem. <em>EJOR</em>, <em>323</em>(3), 753-766. (<a href='https://doi.org/10.1016/j.ejor.2025.01.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Prisoner Transportation Problem is an NP-hard combinatorial problem and a complex variant of the Dial-a-Ride Problem. Given a set of requests for pick-up and delivery and a homogeneous fleet, it consists of assigning requests to vehicles to serve all requests, respecting the problem constraints such as route duration, capacity, ride time, time windows, multi-compartment assignment of conflicting prisoners and simultaneous services in order to optimize a given objective function. In this paper, we present a new solution framework to address this problem that leads to an efficient heuristic. A comparison with computational results from previous papers shows that the heuristic is very competitive for some classes of benchmark instances from the literature and clearly superior in the remaining cases. Finally, suggestions for future studies are presented.},
  archive      = {J_EJOR},
  author       = {Luciano Ferreira and Marcos Vinicius Milan Maciel and José Valério de Carvalho and Elsa Silva and Filipe Pereira Alvelos},
  doi          = {10.1016/j.ejor.2025.01.029},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {753-766},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A new effective heuristic for the prisoner transportation problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formulations and branch-and-cut algorithms for the period travelling salesman problem. <em>EJOR</em>, <em>323</em>(3), 739-752. (<a href='https://doi.org/10.1016/j.ejor.2025.01.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address two variants of the Period Travelling Salesman Problem: one where some nodes cannot be visited consecutively over the time horizon, and another one where this restriction is not imposed. A new flow-based formulation that uses specific information about the visit patterns of nodes is studied and empirical tests show that it is able to solve test instances where a flow-based formulation based on the Single Commodity Flow formulation for the Travelling Salesman Problem reached the time limit. Non-compact formulations are studied in this work as well. We propose two new sets of exponentially-sized valid inequalities that have not been studied yet in the literature. A formulation which is based on connectivity cuts per period enhanced with these sets of valid inequalities proved to be the most efficient and it was able to solve several instances.},
  archive      = {J_EJOR},
  author       = {Sofia Henriques and Ana Paias},
  doi          = {10.1016/j.ejor.2025.01.015},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {739-752},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Formulations and branch-and-cut algorithms for the period travelling salesman problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness in repetitive scheduling. <em>EJOR</em>, <em>323</em>(3), 724-738. (<a href='https://doi.org/10.1016/j.ejor.2024.12.052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research found that fairness plays a key role in customer satisfaction. Therefore, many manufacturing and services industries have become aware of the need to treat customers fairly. Still, there is a huge lack of models that enable industries to make operational decisions fairly, such as a fair scheduling of the customers’ jobs. Our main aim in this research is to provide a unified framework to enable schedulers to make fair decisions in repetitive scheduling environments. For doing so, we consider a set of repetitive scheduling problems involving a set of n clients. In each out of q consecutive operational periods ( e.g. days), each one of the customers submits a job for processing by an operational system. The scheduler’s aim is to provide a schedule for each of the q periods such that the quality of service (QoS) received by each of the clients will meet a certain predefined threshold. The QoS of a client may take several different forms, e.g. , the number of days that the customer receives its job later than a given due date, the number of times the customer receives his preferred time slot for service, or the sum of waiting times for service. We analyze the single machine variant of the problem for several different definitions of QoS, and classify the complexity of the corresponding problems using the theories of classical and parameterized complexity. We also study the price of fairness, i.e., the loss in the system’s efficiency that results from the need to provide fair solutions.},
  archive      = {J_EJOR},
  author       = {Danny Hermelin and Hendrik Molter and Rolf Niedermeier and Michael Pinedo and Dvir Shabtay},
  doi          = {10.1016/j.ejor.2024.12.052},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {724-738},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fairness in repetitive scheduling},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving the parallel processor scheduling and bin packing problems with contiguity constraints: Mathematical models and computational studies. <em>EJOR</em>, <em>323</em>(3), 701-723. (<a href='https://doi.org/10.1016/j.ejor.2024.09.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parallel processor scheduling and bin packing problems with contiguity constraints are important in the field of combinatorial optimization because both problems can be used as components of effective exact decomposition approaches for several two-dimensional packing problems. In this study, we provide an extensive review of existing mathematical formulations for the two problems, together with some model enhancements and lower bounding techniques, and we empirically evaluate the computational behavior of each of these elements using a state-of-the-art solver on a large set of literature instances. We also assess whether recent developments such as meet-in-the middle patterns and the reflect formulation can be used to solve the two problems more effectively. Our experiments demonstrate that some features, such as the mathematical model used, have a major impact on whether an approach is able to solve an instance, whereas other features, such as the use of symmetry-breaking constraints, do not bring any empirical advantage despite being useful in theory. Overall, our goal is to help the research community design more effective yet simpler algorithms to solve the parallel processor scheduling and bin packing problems with contiguity constraints and closely related extensions so that, eventually, those can be integrated into a larger number of exact methods for two-dimensional packing problems.},
  archive      = {J_EJOR},
  author       = {Fatih Burak Akçay and Maxence Delorme},
  doi          = {10.1016/j.ejor.2024.09.013},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {701-723},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Solving the parallel processor scheduling and bin packing problems with contiguity constraints: Mathematical models and computational studies},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Milk adulteration testing and impact of farmers efficiency heterogeneity: A strategic analysis. <em>EJOR</em>, <em>323</em>(2), 686-700. (<a href='https://doi.org/10.1016/j.ejor.2024.12.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by economic motives, dairy farmers adulterate milk to increase its perceived quality, posing a serious risk to consumer health. We analyse a milk supply chain in which smallholder dairy farmers can adulterate milk and explore the feasibility of selling it to end consumers through an aggregator. Using a non-cooperative sequential game between the aggregator and farmers, we examine the impact of two testing strategies offered by the aggregator to curb adulteration - (i) individual (testing milk procured from each farmer individually) and (ii) composite (testing the milk after aggregating the portions procured from all the farmers). Our analyses reveal that the aggregator can control milk adulteration by judiciously using testing and penalty mechanisms. We find that a higher market price (aggregation effect) , fetched by the aggregator because of its bargaining power owing to the consolidation of milk supplies, is essential for its operation. It leads to higher revenue for the aggregator and expands the zone in which it is profitable for the aggregator to operate. However, our results show that the efficiency heterogeneity among farmers, which leads to the less efficient farmers free-riding on the more efficient ones, has a detrimental effect on the aggregator operation. We also explore the impact of external uncertainties on the supply chain and observe that the composite testing strategy becomes more profitable for the aggregator when external uncertainties increase. Our results provide important policy recommendations for aggregators adopting optimal testing strategies.},
  archive      = {J_EJOR},
  author       = {Samir Biswas and Preetam Basu and Balram Avittathur},
  doi          = {10.1016/j.ejor.2024.12.001},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {686-700},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Milk adulteration testing and impact of farmers efficiency heterogeneity: A strategic analysis},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from the aggregated optimum: Managing port wine inventory in the face of climate risks. <em>EJOR</em>, <em>323</em>(2), 671-685. (<a href='https://doi.org/10.1016/j.ejor.2024.11.046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Port wine stocks ameliorate during storage, facilitating product differentiation according to age. This induces a trade-off between immediate revenues and further maturation. Varying climate conditions in the limited supply region lead to stochastic purchase prices for wine grapes. Decision makers must integrate recurring purchasing, production, and issuance decisions. Because stocks from different age classes can be blended to create final products, the solution space increases exponentially in the number of age classes. We model the problem of managing port wine inventory as a Markov decision process, considering decay as an additional source of uncertainty. For small problems, we derive general management strategies from the long-run behavior of the optimal policy. Our solution approach for otherwise intractable large problems, therefore, first aggregates age classes to create a tractable problem representation. We then use machine learning to train tree-based decision rules that reproduce the optimal aggregated policy and the enclosed management strategies. The derived rules are scaled back to solve the original problem. Learning from the aggregated optimum outperforms benchmark rules by 21.4% in annual profits (while leaving a 2.8%-gap to an upper bound). For an industry case, we obtain a 17.4%-improvement over current practices. Our research provides distinct strategies for how producers can mitigate climate risks. The purchasing policy dynamically adapts to climate-dependent price fluctuations. Uncertainties are met with lower production of younger products, whereas strategic surpluses of older stocks ensure high production of older products. Moreover, a wide spread in the age classes used for blending reduces decay risk exposure.},
  archive      = {J_EJOR},
  author       = {Alexander Pahr and Martin Grunow and Pedro Amorim},
  doi          = {10.1016/j.ejor.2024.11.046},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {671-685},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Learning from the aggregated optimum: Managing port wine inventory in the face of climate risks},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible enhanced indexation models through stochastic dominance and ordered weighted average optimization. <em>EJOR</em>, <em>323</em>(2), 657-670. (<a href='https://doi.org/10.1016/j.ejor.2024.11.050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss portfolio selection strategies for Enhanced Indexation (EI), which are based on stochastic dominance relations. The goal is to select portfolios that stochastically dominate a given benchmark but that, at the same time, must generate some excess return with respect to a benchmark index. To achieve this goal, we propose a new methodology that selects portfolios using the ordered weighted average (OWA) operator, which generalizes previous approaches based on minimax selection rules and still leads to solving linear programming models. We also introduce a new type of approximate stochastic dominance rule and show that it implies the almost Second-order Stochastic Dominance (SSD) criterion proposed by Lizyayev and Ruszczyński (2012). We prove that our EI model based on OWA selects portfolios that dominate a given benchmark through this new form of stochastic dominance criterion. We test the performance of the obtained portfolios in an extensive empirical analysis based on real-world datasets. The computational results show that our proposed approach outperforms several SSD-based strategies widely used in the literature, as well as the global minimum variance portfolio.},
  archive      = {J_EJOR},
  author       = {Francesco Cesarone and Justo Puerto},
  doi          = {10.1016/j.ejor.2024.11.050},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {657-670},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Flexible enhanced indexation models through stochastic dominance and ordered weighted average optimization},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal fulfillment and replenishment for omnichannel retailers with standard shipping contracts. <em>EJOR</em>, <em>323</em>(2), 642-656. (<a href='https://doi.org/10.1016/j.ejor.2024.11.051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {E-commerce sales rise exponentially and represent an increasing proportion of global retail. To benefit from this, traditional brick-and-mortar stores enter the e-commerce market and become omnichannel retailers. However, the profitability of omnichannel retailers remains questionable due to high shipment and fulfillment costs. This paper addresses this challenge, focusing on using standard shipping contracts as a potential solution. Such contracts promise delivery within a given number of periods. Once a customer orders, the retailer should set a delivery period. In this way, retailers are flexible in setting exact delivery days, providing an opportunity for jointly optimizing product replenishment and customer fulfillment. We provide a generic model for the use of standard shipping contracts and formulate it as a Markov decision process. We provide optimal solutions using a modified policy iteration algorithm. Our results show that using standard shipping contracts creates a win-win situation: It increases profits and customer service. The observed profit increase is directly linked to maintaining less on-hand inventory. This effect is more pronounced for higher valued products and longer replenishment lead times. Additionally, we propose a heuristic policy that performs within 4% of the optimal policy.},
  archive      = {J_EJOR},
  author       = {Bartu Arslan and Albert H. Schrotenboer and Zümbül Atan},
  doi          = {10.1016/j.ejor.2024.11.051},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {642-656},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal fulfillment and replenishment for omnichannel retailers with standard shipping contracts},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank matrix estimation via nonconvex spectral regularized methods in errors-in-variables matrix regression. <em>EJOR</em>, <em>323</em>(2), 626-641. (<a href='https://doi.org/10.1016/j.ejor.2025.02.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional matrix regression has been studied in various aspects, such as statistical properties, computational efficiency and application to specific instances including multivariate regression, system identification and matrix compressed sensing. Current studies mainly consider the idealized case that the covariate matrix is obtained without noise, while the more realistic scenario that the covariates may always be corrupted with noise or missing data has received little attention. We consider the general errors-in-variables matrix regression model and proposed a unified framework for low-rank estimation based on nonconvex spectral regularization. Then from the statistical aspect, recovery bounds for any stationary points are provided to achieve statistical consistency. From the computational aspect, the proximal gradient method is applied to solve the nonconvex optimization problem and is proved to converge to a small neighborhood of the global solution in polynomial time. Consequences for concrete models such as matrix compressed sensing models with additive noise and missing data are obtained via verifying corresponding regularity conditions. Finally, the performance of the proposed nonconvex estimation method is illustrated by numerical experiments on both synthetic and real neuroimaging data.},
  archive      = {J_EJOR},
  author       = {Xin Li and Dongya Wu},
  doi          = {10.1016/j.ejor.2025.02.005},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {626-641},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Low-rank matrix estimation via nonconvex spectral regularized methods in errors-in-variables matrix regression},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From collaborative filtering to deep learning: Advancing recommender systems with longitudinal data in the financial services industry. <em>EJOR</em>, <em>323</em>(2), 609-625. (<a href='https://doi.org/10.1016/j.ejor.2025.01.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RS) are highly relevant for multiple domains, allowing to construct personalized suggestions for consumers. Previous studies have strongly focused on collaborative filtering approaches, but the inclusion of longitudinal data (LD) has received limited attention. To address this gap, we investigate the impact of incorporating LD for recommendations, comparing traditional collaborative filtering approaches, multi-label classifier (MLC) algorithms, and a deep learning model (DL) in the form of gated recurrent units (GRU). Additional analysis for the best performing model is provided through SHapley Additive exPlanations (SHAP), to uncover relations between the different recommended products and features. Thus, this article contributes to operational research literature by (1) comparing several MLC techniques and RS, including state-of-the-art DL models in a real-life scenario, (2) the comparison of various featurization techniques to assess the impact of incorporating LD on MLC performance, (3) the evaluation of LD as sequential input through the use of DL models, (4) offering interpretable model insights to improve the understanding of RS with LD. The results uncover that DL models are capable of extracting information from longitudinal features for overall higher and statistically significant performance. Further, SHAP values reveal that LD has the higher impact on model output and managerial relevant temporal patterns emerge across product categories.},
  archive      = {J_EJOR},
  author       = {Stephanie Beyer Díaz and Kristof Coussement and Arno De Caigny},
  doi          = {10.1016/j.ejor.2025.01.022},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {609-625},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {From collaborative filtering to deep learning: Advancing recommender systems with longitudinal data in the financial services industry},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On enhancing the explainability and fairness of tree ensembles. <em>EJOR</em>, <em>323</em>(2), 599-608. (<a href='https://doi.org/10.1016/j.ejor.2025.01.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree ensembles are one of the most powerful methodologies in Machine Learning. In this paper, we investigate how to make tree ensembles more flexible to incorporate explainability and fairness in the training process, possibly at the expense of a decrease in accuracy. While explainability helps the user understand the key features that play a role in the classification task, with fairness we ensure that the ensemble does not discriminate against a group of observations that share a sensitive attribute. We propose a Mixed Integer Linear Optimization formulation to train an ensemble of trees that, apart from minimizing the misclassification cost, controls for sparsity as well as the accuracy in the sensitive group. Our formulation is scalable in the number of observations since its number of binary decision variables is independent of the number of observations. In our numerical results, we show that for standard datasets used in the fairness literature, we can dramatically enhance the fairness of the benchmark, namely the popular Random Forest, while using only a few features, all without damaging the misclassification cost.},
  archive      = {J_EJOR},
  author       = {Emilio Carrizosa and Kseniia Kurishchenko and Dolores Romero Morales},
  doi          = {10.1016/j.ejor.2025.01.008},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {599-608},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {On enhancing the explainability and fairness of tree ensembles},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The manufacturer’s resale strategy for trade-ins. <em>EJOR</em>, <em>323</em>(2), 583-598. (<a href='https://doi.org/10.1016/j.ejor.2024.12.017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To cope with the ever-increasing number of used cars, many automobile manufacturers now offer trade-in programs whereby they resell used cars to generate revenue. Consumers have the alternative of selling their used cars via an online peer-to-peer (P2P) resale platform, which charges a commission on each transaction. This paper studies a manufacturer’s traded-in resale strategy and assess how the manufacturer’s resale strategy and profits are affected by the presence of online P2P platforms. We find that in the absence of P2P platforms, the manufacturer may opt against implementing a resale program, whereas it will always do so in the presence of P2P platforms. This suggests a notable shift in manufacturers’ optimal choice of trade-in resale strategies due to the emergence of P2P platforms. Furthermore, the study reveals that the introduction of P2P platforms may diminishes the profits of manufacturers who have implemented a resale program. Importantly, the study underscores that manufacturers are not necessarily obliged to adopt a planned obsolescence strategy. When P2P platforms are absent, implementing a resale program allows manufacturers to increase profits by producing products that are either less or more durable. However, in the face of competition from P2P platforms, profitability can only be enhanced by making products more durable. This suggests that a platform’s emergence can alter how the depreciation rate affects a manufacturer’s profit and hence its optimal product design strategies. Understanding these dynamics is crucial for effectively navigating the growing used car market.},
  archive      = {J_EJOR},
  author       = {Shu Hu and Stuart X. Zhu and Ke Fu},
  doi          = {10.1016/j.ejor.2024.12.017},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {583-598},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The manufacturer’s resale strategy for trade-ins},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connections between multiple-objective programming and weight restricted data envelopment analysis: The role of the ordering cone. <em>EJOR</em>, <em>323</em>(2), 571-582. (<a href='https://doi.org/10.1016/j.ejor.2024.12.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores some new, important and interesting connections between Multiple-Objective Programming (MOP) and Data Envelopment Analysis (DEA). We show that imposing weight restrictions in DEA corresponds to changing the ordering cone in MOP in a specific way. The new ordering cone is constructed and its properties are proved, providing useful insights about the connections between MOP and DEA. After providing several theoretical results, we illustrate them on a real-world data set. In addition to their theoretical appeal, our results hold significant practical importance for several reasons which are addressed in the paper.},
  archive      = {J_EJOR},
  author       = {Pekka Korhonen and Majid Soleimani-damaneh and Jyrki Wallenius},
  doi          = {10.1016/j.ejor.2024.12.002},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {571-582},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Connections between multiple-objective programming and weight restricted data envelopment analysis: The role of the ordering cone},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An incremental preference elicitation-based approach to learning potentially non-monotonic preferences in multi-criteria sorting. <em>EJOR</em>, <em>323</em>(2), 553-570. (<a href='https://doi.org/10.1016/j.ejor.2024.11.047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging assignment example preference information, to determine the shape of marginal utility functions and category thresholds of the threshold-based multi-criteria sorting (MCS) model, has emerged as a focal point of current research within the realm of MCS. Most studies assume decision makers can provide all assignment example preference information in batch and that their preferences over criteria are monotonic, which may not align with practical MCS problems. This paper introduces a novel incremental preference elicitation-based approach to learning potentially non-monotonic preferences in MCS problems, enabling decision makers to progressively provide assignment example preference information. Specifically, we first construct a max-margin optimization-based model to model potentially non-monotonic preferences and inconsistent assignment example preference information in each iteration of the incremental preference elicitation process. Using the optimal objective function value of the max-margin optimization-based model, we devise information amount measurement methods and question selection strategies to pinpoint the most informative alternative in each iteration within the framework of uncertainty sampling in active learning. Once the termination criterion is satisfied, the sorting result for non-reference alternatives can be determined through the use of two optimization models, i.e., the max-margin optimization-based model and the complexity controlling optimization model. Subsequently, two incremental preference elicitation-based algorithms are developed to learn potentially non-monotonic preferences, considering different termination criteria. Ultimately, we apply the proposed approach to a firm financial state rating problem to elucidate the detailed implementation steps, and perform computational experiments on both artificial and real-world data sets to compare the proposed question selection strategies with several benchmark strategies.},
  archive      = {J_EJOR},
  author       = {Zhuolin Li and Zhen Zhang and Witold Pedrycz},
  doi          = {10.1016/j.ejor.2024.11.047},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {553-570},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An incremental preference elicitation-based approach to learning potentially non-monotonic preferences in multi-criteria sorting},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coevolutionary algorithm for exploiting a large fuzzy outranking relation. <em>EJOR</em>, <em>323</em>(2), 540-552. (<a href='https://doi.org/10.1016/j.ejor.2024.12.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outranking approach in Multiple Criteria Decision Analysis (MCDA) uses ranking procedures to exploit a fuzzy outranking relation, which captures the decision maker's notion of a ranking. However, as decision problems become more complex and computer performance improves, new ranking procedures are needed to rank complex data sets that decision-makers may not interpret. This paper discusses recent efforts and potential directions for developing ranking procedures that use multiobjective evolutionary algorithms (MOEAs) to exploit a fuzzy outranking relation. After that, based on the cooperative coevolutionary algorithms (CCEA) approach, we suggest some fundamental modifications to extend the RP 2 -NSGA-II+H algorithm that improve the scalability of this MOEA to exploit large-sized fuzzy outranking relations. Empirical results indicate that adjustments improve the RP 2 -NSGA-II+H algorithm for the addressed problem. The proposed ranking procedure outperforms RP 2 -NSGA-II+H in terms of ranking error rates based on the experiments conducted. Our experimental results also demonstrate that the proposed approach can be scaled for instances of the ranking problem of up to one thousand alternatives.},
  archive      = {J_EJOR},
  author       = {Jesús Jaime Solano Noriega and Juan Carlos Leyva López and Carlos Andrés Oñate Ochoa and José Rui Figueira},
  doi          = {10.1016/j.ejor.2024.12.012},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {540-552},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A coevolutionary algorithm for exploiting a large fuzzy outranking relation},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain adoption and coordination strategies for green supply chains considering consumer privacy concern. <em>EJOR</em>, <em>323</em>(2), 525-539. (<a href='https://doi.org/10.1016/j.ejor.2024.12.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consumers’ uncertainty about the value of green products will reduce their willingness to pay, thereby obstructing green product promotion. Blockchain can eliminate this uncertainty but bring privacy concerns. We develop a game theoretical model to study a green supply chain composed of one manufacturer and one retailer, aiming to explore the implications of partial or full blockchain adoption on green product manufacturing. Subsequently, we consider the use of revenue-sharing and cost-sharing contracts as mechanisms to coordinate the supply chain that adopts blockchain technologies. We show that adopting blockchain for some products benefits the manufacturer and the retailer, and consumers’ privacy concerns make it impossible for blockchain to be adopted for all products. Interestingly, partial or full blockchain adoption does not affect the green investment level. Furthermore, we find that revenue-sharing and cost-sharing contracts are always beneficial for the manufacturer. However, it can be beneficial for the retailer only when the revenue-sharing or cost-sharing ratio is small. Surprisingly, the effectiveness of the coordinating contract is not affected by consumers’ privacy concerns. Finally, when comparing the wholesale price contract with two coordination mechanisms, we find that the manufacturer and the retailer can agree on adopting a cost-sharing contract when both revenue- and cost-sharing ratios are low. When the revenue-sharing ratio is moderate and the cost-sharing ratio is low, a revenue-sharing contract is adopted. In all other cases, trading is conducted according to the wholesale price contract. These insights can contribute to optimize the application of blockchain in green supply chains.},
  archive      = {J_EJOR},
  author       = {Changhua Liao and Qihui Lu and Salar Ghamat and Helen Huifen Cai},
  doi          = {10.1016/j.ejor.2024.12.022},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {525-539},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Blockchain adoption and coordination strategies for green supply chains considering consumer privacy concern},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An operation-agnostic stochastic user equilibrium model for mobility-on-demand networks with congestible capacities. <em>EJOR</em>, <em>323</em>(2), 504-524. (<a href='https://doi.org/10.1016/j.ejor.2024.12.038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the impact of privately-owned Mobility-on-Demand (MoD) services is important from a regulatory perspective. There is a need to model multimodal equilibria with MoD to support policymaking. While there exists a large body of literature on MoD services focusing on service design under equilibrium modeling, these studies commonly adopt assumptions of MoD operational policies. However, such policies might not be shared with regulatory agencies due to commercial privacy concerns of private operators. We model multimodal equilibrium with MoD systems in an operation-agnostic manner based on empirical observations of flow and capacity. This is done with a Flow-Capacity Interaction (FC) matrix that captures systematic effect of congestible capacities, a phenomenon in MoD systems where capacities are affected by flows. The FC matrix encapsulates the operation and demand patterns by capturing the empirical equilibrium relationship between flows and capacities. An operation-agnostic logit-based stochastic user equilibrium (SUE) formulation is proposed and proof of equivalence of the SUE formulation is derived. The proof shows that, unlike static capacities, path delays are not just the sum of the Lagrange multipliers of the links on the paths, but dependent on the whole network. We name this phenomenon as “non-separable link delays”. A solution algorithm that finds SUE with a bounded path set is proposed, with a custom Frank-Wolfe algorithm to solve the non-linear SUE formulation. Since the FC matrix cannot be directly observed, an inverse optimization problem is introduced to estimate it with observed flow and capacity data. Two numerical examples are provided with sensitivity tests. An empirical example with yellow taxi data of downtown Manhattan, NY is provided to demonstrate effectiveness of estimating the FC matrix from real data, and for determining the equilibrium that captures the underlying flow-capacity dynamics.},
  archive      = {J_EJOR},
  author       = {Bingqing Liu and David Watling and Joseph Y.J. Chow},
  doi          = {10.1016/j.ejor.2024.12.038},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {504-524},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An operation-agnostic stochastic user equilibrium model for mobility-on-demand networks with congestible capacities},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Product line extensions and distribution channels in pharmaceutical supply chain. <em>EJOR</em>, <em>323</em>(2), 490-503. (<a href='https://doi.org/10.1016/j.ejor.2024.12.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to aggressive generic competition after the original drug’s patent expires, various original firms extend their product lines by introducing an authorized generic drug with both lower quality and cost, either via internal distribution or third-party distribution. In this paper, we develop a game-theoretic model to investigate the product line extension and distribution channel decisions for an original firm that has already sold an original drug and considers introducing an authorized generic drug to compete against the generic firm. We show that product line extension enables the original firm to leverage the value of drug differentiation by price discriminating the patients with heterogeneous preferences for quality, but it also leads to original drug’s profit loss caused by the internal cannibalization. Given an internal distribution channel, when the cost gap is not small for the internal cannibalization to be less aggressive, the original firm will extend the product line, which could surprisingly benefit the generic firm but harm the patients. In contrast, under a third-party distribution channel, the original firm always prefers to extend the product line by setting a low wholesale price, which always reduces the generic firm’s profit but increases the patient surplus. Finally, contrary to the conventional wisdom that a decentralized channel always harms the original firm compared with a centralized one due to the double marginalization, our results suggest that when the original drug has a small cost gap or a large quality gap relative to the generic drug, the original firm is better off with using the third-party distribution to introduce the authorized generic drug than the internal distribution, as it permits higher original drug’s profit due to alleviated internal cannibalization, although at the expense of lower authorized generic drug’s profit.},
  archive      = {J_EJOR},
  author       = {Ran Tao and Yanfei Lan and Ruiqing Zhao and Rong Gao},
  doi          = {10.1016/j.ejor.2024.12.013},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {490-503},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Product line extensions and distribution channels in pharmaceutical supply chain},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated differentiated time slot pricing and order dispatching with uncertain customer demand in on-demand food delivery. <em>EJOR</em>, <em>323</em>(2), 471-489. (<a href='https://doi.org/10.1016/j.ejor.2024.12.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiated time slot pricing (DTSP) is a promising approach to enhance the efficiency and cost-effectiveness of food delivery platforms by influencing customers’ choices regarding delivery time slots. In this paper, we investigate the integrated problem of DTSP at the tactical level and order dispatching at the operational level, formulating it as a two-stage stochastic programming model. The first-stage model determines the delivery price for each time slot to maximize the system’s expected profit. The second-stage model generates the optimal order dispatching plan to minimize the generalized system cost under each stochastic scenario. To efficiently estimate the order dispatching cost for each scenario, we develop an order consolidation dispatching algorithm (OCDA) to solve the second-stage order dispatching subproblem under each demand scenario. Building on OCDA, we propose a hybrid adaptive large neighborhood search (HALNS) heuristic to solve the integrated problem. Extensive case studies based on real-world data verify the effectiveness of the proposed approach and demonstrate the benefits of DTSP strategy. Our numerical analysis provides important managerial insights for operating food delivery platforms.},
  archive      = {J_EJOR},
  author       = {Bo Zhang and Elkafi Hassini and Yun Zhou and Meng Zhao and Xiangpei Hu},
  doi          = {10.1016/j.ejor.2024.12.011},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {471-489},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integrated differentiated time slot pricing and order dispatching with uncertain customer demand in on-demand food delivery},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal forecast reconciliation with time series selection. <em>EJOR</em>, <em>323</em>(2), 455-470. (<a href='https://doi.org/10.1016/j.ejor.2024.12.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecast reconciliation ensures forecasts of time series in a hierarchy adhere to aggregation constraints, enabling aligned decision making. While forecast reconciliation can enhance overall accuracy in a hierarchical or grouped structure, it can lead to worse forecasts for certain series, with the greatest gains typically seen in series that originally have poorly performing base forecasts. In practical applications, some series in a structure often produce poor base forecasts due to model misspecification or low forecastability. To mitigate their negative impact, we propose two categories of forecast reconciliation methods that incorporate automatic time series selection based on out-of-sample and in-sample information, respectively. These methods keep “poor” base forecasts unused in forming reconciled forecasts, while adjusting the weights assigned to the remaining series accordingly when generating bottom-level reconciled forecasts. Additionally, our methods ameliorate disparities stemming from varied estimators of the base forecast error covariance matrix, alleviating challenges associated with estimator selection. Empirical evaluations through two simulation studies and applications using Australian labour force and domestic tourism data demonstrate the potential of the proposed methods to exclude series with high scaled forecast errors and show promising results.},
  archive      = {J_EJOR},
  author       = {Xiaoqian Wang and Rob J. Hyndman and Shanika L. Wickramasuriya},
  doi          = {10.1016/j.ejor.2024.12.004},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {455-470},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal forecast reconciliation with time series selection},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the discrete and continuous edge improvement problems: Models and algorithms. <em>EJOR</em>, <em>323</em>(2), 441-454. (<a href='https://doi.org/10.1016/j.ejor.2024.12.051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the edge improvement problem where the fixed edge traversal time assumption of the traditional network flow problems is relaxed. We consider two variants of the problem: one where improvement decisions are restricted to a discrete set (discrete edge improvement problem), and the other where they can take any value within a specified range (continuous edge improvement problem). We first analyze both problem variants on a tree-shaped network and discuss their computational complexities. For the general case, where the underlying network has no special structure, we provide mixed-integer programming (MIP) formulations for both versions of the problem. To the best of our knowledge, this study is the first to propose and compare different formulations for the discrete edge improvement problem and to present a formulation for the continuous edge improvement problem. Since the developed models do not perform well for medium and large problem instances, we introduce a Benders decomposition algorithm to solve the discrete edge improvement problem. Additionally, we employ it heuristically to find high-quality solution for the continuous edge improvement problem within reasonable times. We also devise an MIP formulation to find lower bounds for the continuous edge improvement problem, leveraging the McCormick envelopes and optimal solution properties. Our experiments demonstrate that the Benders decomposition algorithm outperforms the other formulations for the discrete edge improvement problem, while the heuristic method proposed for the continuous edge improvement problem provides quite well results even for large problem instances.},
  archive      = {J_EJOR},
  author       = {Esra Koca and A. Burak Paç},
  doi          = {10.1016/j.ejor.2024.12.051},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {441-454},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Exploring the discrete and continuous edge improvement problems: Models and algorithms},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast and effective breakpoints heuristic algorithm for the quadratic knapsack problem. <em>EJOR</em>, <em>323</em>(2), 425-440. (<a href='https://doi.org/10.1016/j.ejor.2024.12.019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Quadratic Knapsack Problem (QKP) involves selecting a subset of elements that maximizes the sum of pairwise and singleton utilities without exceeding a given budget. The pairwise utilities are nonnegative, the singleton utilities may be positive, negative, or zero, and the node costs are nonnegative. We introduce a Breakpoints Algorithm for QKP, named QKBP, which is based on a technique proposed in Hochbaum (2009) for efficiently generating the concave envelope of the solutions to the relaxation of the problem for all values of the budget. Our approach utilizes the fact that breakpoints in the concave envelopes are optimal solutions for their respective budgets. For budgets between breakpoints, a fast greedy heuristic derives high-quality solutions from the optimal solutions of adjacent breakpoints. The QKBP algorithm is a heuristic which is highly scalable due to an efficient parametric cut procedure used to generate the concave envelope. This efficiency is further improved by a newly developed compact problem formulation. Our extensive computational study on both existing and new benchmark instances, with up to 10,000 elements, shows that while some leading algorithms perform well on a few instances, QKBP consistently delivers high-quality solutions regardless of instance size, density, or budget. Moreover, QKBP achieves these results in significantly faster running times than all leading algorithms. The source code of the QKBP algorithm, the benchmark instances, and the detailed results are publicly available on GitHub.},
  archive      = {J_EJOR},
  author       = {D.S. Hochbaum and P. Baumann and O. Goldschmidt and Y. Zhang},
  doi          = {10.1016/j.ejor.2024.12.019},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {425-440},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A fast and effective breakpoints heuristic algorithm for the quadratic knapsack problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving the multiobjective quasi-clique problem. <em>EJOR</em>, <em>323</em>(2), 409-424. (<a href='https://doi.org/10.1016/j.ejor.2024.12.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a simple undirected graph G , a quasi-clique is a subgraph of G whose density is at least γ ( 0 < γ ≤ 1 ) . Finding a maximum quasi-clique has been addressed from two different perspectives: ( i ) maximizing vertex cardinality for a given edge density; and ( i i ) maximizing edge density for a given vertex cardinality. However, when no a priori preference information about cardinality and density is available, a more natural approach is to consider the problem from a multiobjective perspective. We introduce the Multiobjective Quasi-clique (MOQC) problem, which aims to find a quasi-clique by simultaneously maximizing both vertex cardinality and edge density. To efficiently address this problem, we explore the relationship among MOQC, its single-objective counterpart problems, and a bi-objective optimization problem, along with several properties of the MOQC problem and quasi-cliques. We propose a baseline approach using ɛ -constraint scalarization and introduce a Two-phase strategy, which applies a dichotomic search based on weighted sum scalarization in the first phase and an ɛ -constraint methodology in the second phase. Additionally, we present a Three-phase strategy that combines the dichotomic search used in Two-phase with a vertex-degree-based local search employing novel sufficient conditions to assess quasi-clique efficiency, followed by an ɛ -constraint in a final stage. Experimental results on synthetic and real-world sparse graphs indicate that the integrated use of dichotomic search and local search, together with mechanisms to assess quasi-clique efficiency, makes the Three-phase strategy an effective approach for solving the MOQC problem in sparse graphs in terms of running time and ability to produce new efficient quasi-cliques.},
  archive      = {J_EJOR},
  author       = {Daniela Scherer dos Santos and Kathrin Klamroth and Pedro Martins and Luís Paquete},
  doi          = {10.1016/j.ejor.2024.12.018},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {409-424},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Solving the multiobjective quasi-clique problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discrete optimization: A quantum revolution?. <em>EJOR</em>, <em>323</em>(2), 378-408. (<a href='https://doi.org/10.1016/j.ejor.2024.12.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop several quantum procedures and investigate their potential to solve discrete optimization problems. First, we introduce a binary search procedure and illustrate how it can be used to effectively solve the binary knapsack problem. Next, we introduce two other procedures: a hybrid branch-and-bound procedure that allows to exploit the structure of the problem and a random-ascent procedure that can be used to solve problems that have no clear structure and/or are difficult to solve using traditional methods. We explain how to assess the performance of these procedures and perform an elaborate computational experiment. Our results show that we can match the worst-case performance of the best classical algorithms when solving the binary knapsack problem. After improving and generalizing our procedures, we show that they can be used to solve any discrete optimization problem. To illustrate, we show how to solve the quadratic binary knapsack problem. For this problem, our procedures outperform the best classical algorithms. In addition, we demonstrate that our procedures can be used as heuristics to find (near-) optimal solutions in limited time Not only does our work provide the tools required to explore a myriad of future research directions, it also shows that quantum computing has the potential to revolutionize the field of discrete optimization.},
  archive      = {J_EJOR},
  author       = {Stefan Creemers and Luis Fernando Pérez Armas},
  doi          = {10.1016/j.ejor.2024.12.016},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {378-408},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Discrete optimization: A quantum revolution?},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years of multiple criteria decision analysis: From classical methods to robust ordinal regression. <em>EJOR</em>, <em>323</em>(2), 351-377. (<a href='https://doi.org/10.1016/j.ejor.2024.07.038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Criteria Decision Analysis (MCDA) is a subfield of Operational Research that aims to support Decision-Makers (DMs) in the decision-making process through mathematical models and computational procedures. In this perspective, MCDA employs structured and traceable protocols to identify potential actions and the criteria for evaluating them. MCDA procedures aim to define recommendations consistent with the preferences of DMs for the specific decision problem at hand. These problems are generally formulated in terms of either choosing the best action, classifying actions into pre-defined and ordered decision classes, or ranking actions from best to worst. As the evaluation criteria are generally conflicting, the main challenge is to aggregate them into a mathematical preference model representing the DM value system. We review the development of MCDA over the past fifty years and describe its evolution with examples of distinctive methods. They are distinguished by the type of preference information elicited by DMs, the type of the preference model (criteria aggregation), and the way of converting the preference relation induced by the preference model in the set of potential actions into a decision recommendation. We focus on MCDA methods with a finite set of actions. References to specific application areas will be given. In the conclusion section, some prospective avenues of research will be outlined.},
  archive      = {J_EJOR},
  author       = {Salvatore Greco and Roman Słowiński and Jyrki Wallenius},
  doi          = {10.1016/j.ejor.2024.07.038},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {351-377},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of multiple criteria decision analysis: From classical methods to robust ordinal regression},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversification for infinite-mean pareto models without risk aversion. <em>EJOR</em>, <em>323</em>(1), 341-350. (<a href='https://doi.org/10.1016/j.ejor.2025.01.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study stochastic dominance between portfolios of independent and identically distributed (iid) extremely heavy-tailed (i.e., infinite-mean) Pareto random variables. With the notion of majorization order, we show that a more diversified portfolio of iid extremely heavy-tailed Pareto random variables is larger in the sense of first-order stochastic dominance. This result is further generalized for Pareto random variables caused by triggering events, random variables with tails being Pareto, bounded Pareto random variables, and positively dependent Pareto random variables. These results provide an important implication in investment: Diversification of extremely heavy-tailed Pareto profits uniformly increases investors’ profitability, leading to a diversification benefit. Remarkably, different from the finite-mean setting, such a diversification benefit does not depend on the decision maker’s risk aversion.},
  archive      = {J_EJOR},
  author       = {Yuyu Chen and Taizhong Hu and Ruodu Wang and Zhenfeng Zou},
  doi          = {10.1016/j.ejor.2025.01.039},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {341-350},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Diversification for infinite-mean pareto models without risk aversion},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sustainable optimal stock portfolios: What relationship between sustainability and performance?. <em>EJOR</em>, <em>323</em>(1), 323-340. (<a href='https://doi.org/10.1016/j.ejor.2025.01.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to compare different strategies to combine sustainability and optimality in stock portfolios to assess whether there is an association between their average ESG (Environmental, Social, Governance) score and their financial performance and, if so, whether it depends on the specific strategy used. To this end, we confront the risk-adjusted performance of three ESG-compliant optimal portfolios resulting from: (i) optimizing on an ESG-screened sample, (ii) including a portfolio ESG-score constraint in the optimization on an unscreened sample, (iii) our original proposal of optimizing with an ESG-score constraint (so as to reach a target) over a slightly screened sample (so as to exclude companies with lowest sustainability). The optimization is implemented with Bloomberg ESG scores over a sample from the EURO STOXX Index in the period January 2007–August 2022 by minimizing portfolio residual risk. Two are the main conclusions from our results. First, we never find a significant negative association between portfolios’ average ESG score and performance independently of the strategy used. Second, we find a positive association when the first and the third strategy are implemented with a high screening level. To be noted that the relationship between the ESG score and the risk-return ratio in the initial investment set plays a relevant role. If, as in our dataset, this relationship is essentially convex, with an appropriate level of screening portfolios are composed only by stocks whereby a higher ESG score is associated with a higher risk-return profile.},
  archive      = {J_EJOR},
  author       = {Beatrice Bertelli and Costanza Torricelli},
  doi          = {10.1016/j.ejor.2025.01.021},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {323-340},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Sustainable optimal stock portfolios: What relationship between sustainability and performance?},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive distributions and the market return: The role of market illiquidity. <em>EJOR</em>, <em>323</em>(1), 309-322. (<a href='https://doi.org/10.1016/j.ejor.2025.01.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper evaluates the role of volatility-free stock market illiquidity proxies in forecasting monthly stock market returns. We adopt a probabilistic approach to multivariate time-series modelling using Bayesian nonparametric vector autoregressions. These models flexibly capture complex joint dynamics among financial variables through data-driven regime switching. Out-of-sample forecasts maintain accuracy as the horizon increases. Adding illiquidity generates statistical improvements in out-of-sample predictive accuracy. We highlight the operational importance of market illiquidity after selecting the most appropriate forecasting model that delivers profitable strategies that outperform a range of multivariate models; as well as the historical mean.},
  archive      = {J_EJOR},
  author       = {Michael Ellington and Maria Kalli},
  doi          = {10.1016/j.ejor.2025.01.006},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {309-322},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Predictive distributions and the market return: The role of market illiquidity},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning approach for solution space reduction in aircraft disruption recovery. <em>EJOR</em>, <em>323</em>(1), 297-308. (<a href='https://doi.org/10.1016/j.ejor.2024.11.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aircraft recovery, a critical step in airline operations recovery, aims to minimize the cost of disrupted aircraft schedules. The exact methods for aircraft recovery are computationally expensive and operationally infeasible in practice. Heuristics and hybrid approaches offer faster solutions but have inconsistent solution quality, often leading to large losses. We propose a supervised machine learning approach to accelerate aircraft recovery by pruning the solution space of the optimization problem. It leverages similarities with previously solved problem instances through an offline model-training phase, identifies components of the optimal solutions for new problem instances in the online phase, and links them to the optimization model to rapidly generate high-quality solutions. Computational results, from multiple historical disruption instances for a large US airline, demonstrate that this approach significantly outperforms exact methods on computational runtime while producing similarly high-quality solutions. It also outperforms existing heuristics due to its ability to prune solution spaces in a more principled manner, leading to higher quality solutions in similarly short runtimes. For a runtime budget of two minutes, our approach provides a solution within 1.5% of the true optimal cost, resulting in an average daily saving of over $390,000 compared to all existing approaches. The main drivers of these improvements are explainable in terms of key airline operational metrics and are validated through extensive sensitivity and robustness tests.},
  archive      = {J_EJOR},
  author       = {Navid Rashedi and Nolan Sankey and Vikrant Vaze and Keji Wei},
  doi          = {10.1016/j.ejor.2024.11.025},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {297-308},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A machine learning approach for solution space reduction in aircraft disruption recovery},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A rolling horizon heuristic approach for a multi-stage stochastic waste collection problem. <em>EJOR</em>, <em>323</em>(1), 276-296. (<a href='https://doi.org/10.1016/j.ejor.2024.11.041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a multi-stage stochastic optimization model to solve an inventory routing problem for the collection of recyclable municipal waste. The objective is the maximization of the total expected profit of the waste collection company. The decisions are related to the selection of the bins to be visited and the corresponding routing plan in a predefined time horizon. Stochasticity in waste accumulation is modeled through scenario trees generated via conditional density estimation and dynamic stochastic approximation techniques. The proposed formulation is solved through a rolling horizon approach, providing a rigorous worst-case analysis on its performance. Extensive computational experiments are carried out on small- and large-sized instances based on real data provided by a large Portuguese waste collection company. The impact of stochasticity on waste generation is examined through stochastic measures, showing the importance of adopting a stochastic model over a deterministic formulation when addressing a waste collection problem. The performance of the rolling horizon approach is evaluated, demonstrating that this heuristic provides cost-effective solutions in short computational time. Managerial insights related to different geographical configurations of the instances and varying levels of uncertainty are finally discussed.},
  archive      = {J_EJOR},
  author       = {Andrea Spinelli and Francesca Maggioni and Tânia Rodrigues Pereira Ramos and Ana Paula Barbosa-Póvoa and Daniele Vigo},
  doi          = {10.1016/j.ejor.2024.11.041},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {276-296},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A rolling horizon heuristic approach for a multi-stage stochastic waste collection problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed solution of the day-ahead pump and valve scheduling problem for dynamically adaptive water distribution networks with storage. <em>EJOR</em>, <em>323</em>(1), 267-275. (<a href='https://doi.org/10.1016/j.ejor.2024.11.035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the computation of daily schedules of pumps and boundary valves for the minimization of energy costs in water distribution networks (WDN) with dynamically adaptive configurations. The considered problem combines integer (“on”/“off”) pump control variables, non-convex energy conservation constraints and time-coupling mass conservation constraints. For operational WDNs, the resulting non-convex mixed-integer non-linear program (MINLP) is too large to be solved using available methods. We propose a tailored heuristic solution method based on the Alternating Direction Method of Multipliers which distributes and coordinates the solution of smaller problems corresponding to individual time steps of the original MINLP. The proposed method is applied to a large-scale WDN from the UK. The daily schedule of pumps and boundary valves obtained for the dynamically adaptive network configuration, computed in 12 min, is shown to be at most 6% suboptimal and nearly 5% cheaper than the globally optimal schedule corresponding to the traditional (sectorized) network configuration. The proposed algorithm outperforms alternative off-the-shelf and tailored approaches, providing a scalable method to compute good solutions to the complex day-ahead pump and valve scheduling problem in operational dynamically adaptive WDNs.},
  archive      = {J_EJOR},
  author       = {Aly-Joy Ulusoy and Ivan Stoianov},
  doi          = {10.1016/j.ejor.2024.11.035},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {267-275},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Distributed solution of the day-ahead pump and valve scheduling problem for dynamically adaptive water distribution networks with storage},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adoption model of cryptocurrencies. <em>EJOR</em>, <em>323</em>(1), 253-266. (<a href='https://doi.org/10.1016/j.ejor.2024.11.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network effect, measured by users’ adoption, is considered an important driver of cryptocurrency market dynamics. This study examines the role of adoption timing in cryptocurrency markets by decomposing total adoption into two components: innovators (early adopters) and imitators (late adopters). We find that the innovators’ component is the primary driver of the association between user adoption and cryptocurrency returns, both in-sample and out-of-sample. Next, we show that innovators’ adoption improves price efficiency, while imitators’ adoption contributes to noisier prices. Furthermore, we demonstrate that the adoption model captures significant cryptocurrency market phenomena, such as herding behaviour, more effectively, making it better suited for forecasting models in cryptocurrency pricing. These results suggest that our methodology for linking early and late adopters to market dynamics can be applied to various domains, offering a framework for future research at the intersection of operational research and financial markets.},
  archive      = {J_EJOR},
  author       = {Khaladdin Rzayev and Athanasios Sakkas and Andrew Urquhart},
  doi          = {10.1016/j.ejor.2024.11.024},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {253-266},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An adoption model of cryptocurrencies},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal co-development contracts for companion diagnostics. <em>EJOR</em>, <em>323</em>(1), 241-252. (<a href='https://doi.org/10.1016/j.ejor.2024.11.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The market for companion diagnostics is expected to be a US$10.07 billion by 2026. Companion diagnostics have the potential to make expensive drugs cost-effective by identifying patients who would benefit from them. We consider the contract design problem between a pharmaceutical company which owns a drug that is effective for a particular subset of the patient population and a biotech company which owns some technology that could facilitate the development of a companion diagnostic. We obtain theoretical and practical results. We determine when both parties enter such a contract and fully characterize the optimal solutions in closed-form. We find sufficient conditions under which the optimal contract exhibits a particular structure. We show that the first-best can be achieved in some cases and identify sufficient conditions under which the biotech company would not work alone but participates in the project with the pharmaceutical company’s subsidy. We find that heuristics based on practical preferences could be costly to the pharmaceutical company and hence the principal should use the second-best solution; and contract type depends heavily on the biotech company’s workforce level, unit cost of workforce and information level.},
  archive      = {J_EJOR},
  author       = {Sakine Batun and Mehmet A. Begen and Gregory S. Zaric},
  doi          = {10.1016/j.ejor.2024.11.031},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {241-252},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal co-development contracts for companion diagnostics},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating non-overfitted convex production technologies: A stochastic machine learning approach. <em>EJOR</em>, <em>323</em>(1), 224-240. (<a href='https://doi.org/10.1016/j.ejor.2024.11.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overfitting is a classical statistical issue that occurs when a model fits a particular observed data sample too closely, potentially limiting its generalizability. While Data Envelopment Analysis (DEA) is a powerful non-parametric method for assessing the relative efficiency of decision-making units (DMUs), its reliance on the minimal extrapolation principle can lead to concerns about overfitting, particularly when the goal extends beyond evaluating the specific DMUs in the sample to making broader inferences. In this paper, we propose an adaptation of Stochastic Gradient Boosting to estimate production possibility sets that mitigate overfitting while satisfying shape constraints such as convexity and free disposability. Our approach is not intended to replace DEA but to complement it, offering an additional tool for scenarios where generalization is important. Through simulation experiments, we demonstrate that the proposed method performs well compared to DEA, especially in high-dimensional settings. Furthermore, the new machine learning-based technique is compared to the Corrected Concave Non-parametric Least Squares (C 2 NLS), showing competitive performance. We also illustrate how the usual efficiency measures in DEA can be implemented under our approach. Finally, we provide an empirical example based on data from the Program for International Student Assessment (PISA) to demonstrate the applicability of the new method.},
  archive      = {J_EJOR},
  author       = {Maria D. Guillen and Vincent Charles and Juan Aparicio},
  doi          = {10.1016/j.ejor.2024.11.030},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {224-240},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Estimating non-overfitted convex production technologies: A stochastic machine learning approach},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond leagues: A single incomplete round robin tournament for multi-league sports timetabling. <em>EJOR</em>, <em>323</em>(1), 208-223. (<a href='https://doi.org/10.1016/j.ejor.2024.11.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most sports associations regularly face the problem of determining and scheduling games for dozens if not hundreds of non-professional (youth) teams. For practical reasons and player convenience, it is key that the schedule respects venue capacities and minimizes travel distance. A classic approach is to split up teams over leagues, and then have each league play a round robin tournament. In a round robin tournament, each team competes against every other team in the tournament an equal number of times. This paper proposes an alternative approach, organizing a single yet incomplete round robin tournament involving all teams. In this format, which can be seen as a static Swiss system tournament, each team plays the same number of games, but teams are not required to face the same opponents. We exploit this flexibility to reduce the total travel distance and venue capacity conflicts. We provide theoretical results on the computational complexity of finding an incomplete round robin tournament, as well as sufficient conditions on its existence. Besides a Benders’ decomposition for the classic round robin approach, we develop a relax-and-fix and an iterative two-phase decomposition metaheuristic for the incomplete round robin approach. The metaheuristic first determines the home-away status of teams based on their club’s venue capacity, and thereafter selects suitable opponents while minimizing travel distances. Extensive experiments using real-life benchmark instances from the literature confirm the advantage of an incomplete round robin tournament compared to the classic multi-league round robin approach and validate the effectiveness of the proposed heuristics.},
  archive      = {J_EJOR},
  author       = {Miao Li and David Van Bulck and Dries Goossens},
  doi          = {10.1016/j.ejor.2024.11.007},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {208-223},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Beyond leagues: A single incomplete round robin tournament for multi-league sports timetabling},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consensus modeling for maximum expert with quadratic cost under various uncertain contexts: A data-driven robust approach. <em>EJOR</em>, <em>323</em>(1), 192-207. (<a href='https://doi.org/10.1016/j.ejor.2024.10.034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus optimization models are valuable tools for addressing negotiated group decision-making challenges, particularly those involving critical decision-related data such as costs and preferences. However, the idealized approach to information fusion in consensus decision-making presents challenges in adapting to practical conditions, leading to less credible consensus solutions. To simulate a more realistic decision-making scenario, this study integrates unit adjustment costs in a quadratic form into a maximum expert consensus model. This quadratic cost formulation captures the complex resistance to cost changes encountered by experts when adjusting solutions, promoting a deliberate approach to solution updates and facilitating improved decision-making. Moreover, economic insights elucidate the effect of quadratic costs on decision-making behavior. Additionally, the feasibility of reaching a consensus may be impeded by high uncertainty in real-world decision-making scenarios. This study separately tackles decision environments characterized by unit adjustment costs and individual preference uncertainty. It employs a robust optimization approach to incorporate uncertain costs and preferences into the optimization model. Data-driven robust maximum expert consensus models are then developed to objectively manage available historical data. An enhanced genetic algorithm is introduced as a solution method to address the proposed models. The proposed models are ultimately applied to evaluate policy options for the development of new energy vehicles in Changsha. Comparative and sensitivity analyses are conducted, showing the superior performance of the proposed models.},
  archive      = {J_EJOR},
  author       = {Jinpeng Wei and Xuanhua Xu and Shaojian Qu and Qiuhan Wang},
  doi          = {10.1016/j.ejor.2024.10.034},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {192-207},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Consensus modeling for maximum expert with quadratic cost under various uncertain contexts: A data-driven robust approach},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-attribute utility preference robust optimization: A continuous piecewise linear approximation approach. <em>EJOR</em>, <em>323</em>(1), 170-191. (<a href='https://doi.org/10.1016/j.ejor.2024.11.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a bi-attribute decision making problem where the decision maker’s (DM’s) objective is to maximize the expected utility of outcomes with two attributes but where the true utility function which captures the DM’s risk preference is ambiguous. To tackle this ambiguity, we propose a maximin bi-attribute utility preference robust optimization (BUPRO) model where the optimal decision is based on the worst-case utility function in an ambiguity set of plausible utility functions constructed using partially available information such as the DM’s specific preference for certain lotteries. Specifically, we consider a BUPRO model with two attributes, where the DM’s risk attitude is bivariate risk-averse and the ambiguity set is defined by a linear system of inequalities represented by the Lebesgue–Stieltjes integrals of the DM’s utility functions. To solve the inner infinite-dimensional minimization problem, we propose a continuous piecewise linear approximation approach to approximate the DM’s unknown true utility. Unlike the univariate case, we partition the domain of the utility function into a set of small non-overlapping rectangles and then divide each rectangle into two triangles by either the main diagonal (Type-1) or the counter diagonal (Type-2). The inner minimization problem based on the piecewise linear utility function can be reformulated as a mixed-integer linear program and the outer maximization problem can be solved efficiently by the derivative-free method. In the case that all the small triangles are partitioned either in Type-1 or in Type-2, the inner minimization can be formulated as a finite dimensional linear program and the overall maximin as a single mixed-integer program. To quantify the approximation errors, we derive, under some mild conditions, the error bound for the difference between the BUPRO model and the approximate BUPRO model in terms of the ambiguity set, the optimal value and the optimal solutions. Finally, we carry out some numerical tests to examine the performance of the proposed models and computational schemes. The results demonstrate the efficiency of the computational schemes and highlight the stability of the BUPRO model against data perturbations.},
  archive      = {J_EJOR},
  author       = {Qiong Wu and Wei Wang and Sainan Zhang and Huifu Xu},
  doi          = {10.1016/j.ejor.2024.11.001},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {170-191},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Bi-attribute utility preference robust optimization: A continuous piecewise linear approximation approach},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tactical workforce sizing and scheduling decisions for last-mile delivery. <em>EJOR</em>, <em>323</em>(1), 153-169. (<a href='https://doi.org/10.1016/j.ejor.2024.12.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the problems of workforce sizing and shift scheduling of a logistic operator delivering parcels in the last-mile segment of the supply chain. Our working hypothesis is that the relevant decisions are affected by two main trade-offs: workforce size and shift stability. A large workforce can deal with demand fluctuations but incurs higher fixed costs; by contrast, a small workforce might require excessive outsourcing to third-party logistic providers. Stable shifts, i.e., with predictable start times and lengths, improve worker satisfaction and reduce turnover; at the same time, they might be less able to adapt to an unsteady demand. We test these assumptions through an extensive computational campaign based on a novel mathematical formulation. We find that extreme shift stability is, indeed, unsuitable for last-mile operations. At the same time, introducing a very limited amount of flexibility achieves similar effects as moving to a completely flexible system while ensuring a better work-life balance for the workers. Several recent studies in the social sciences have warned about the consequences of precarious working conditions for couriers and retail workers and have recommended — among other things — stable work schedules. Our work shows that it is possible to offer better working conditions in terms of shift stability without sacrificing the company’s bottom line. Thus, companies prioritising profitability (as is often the case) can improve workers’ well-being and increase retention with a negligible cost impact.},
  archive      = {J_EJOR},
  author       = {Minakshi Punam Mandal and Alberto Santini and Claudia Archetti},
  doi          = {10.1016/j.ejor.2024.12.006},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {153-169},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Tactical workforce sizing and scheduling decisions for last-mile delivery},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic scheduling and routing decisions in online meal delivery platforms with mixed force. <em>EJOR</em>, <em>323</em>(1), 139-152. (<a href='https://doi.org/10.1016/j.ejor.2024.11.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates stochastic scheduling and routing problems in the online meal delivery (OMD) service. The huge increase in meal delivery demand requires the service providers to construct a highly efficient logistics network to deal with a large-volume of time-sensitive and fluctuating fulfillment, often using inhouse and crowdsourced drivers to secure the ambitious service quality. We aim to address the problem of developing an effective scheduling and routing policy that can handle real-life situations. To this end, we first model the dynamic problem as a Markov Decision Process (MDP) and analyze the structural properties of the optimal policy. Then we propose four integrated approaches to solve the operational level scheduling and routing problem. In addition, we provide a continuous approximation formula to estimate the bounds of required fleet size for the inhouse drivers. Numerical experiments based on a real dataset show the effectiveness of the proposed solution approaches. We also obtain several managerial insights that can help decision makers in solving similar resource allocation problems in real-time.},
  archive      = {J_EJOR},
  author       = {Yanlu Zhao and Laurent Alfandari and Claudia Archetti},
  doi          = {10.1016/j.ejor.2024.11.028},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {139-152},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Stochastic scheduling and routing decisions in online meal delivery platforms with mixed force},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information sharing across competing platforms with varying information capabilities. <em>EJOR</em>, <em>323</em>(1), 125-138. (<a href='https://doi.org/10.1016/j.ejor.2024.11.048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competing online retail platforms frequently function as both agency and reselling channels. This paper explores a manufacturer’s channel selection strategy in the context of downstream platform competition and information sharing, taking into account the platforms’ varying levels of information capability. Our research indicates that the manufacturer opts for a hybrid channel approach. Competing platforms aim to be selected as agency channels by offering information sharing and reduced commission fees. Interestingly, the manufacturer chooses the platform with lesser information capability as her agency channel to gain access to shared demand data, while opting for the platform with greater capability as reselling channel without accessing his demand data. The platform with inferior information capability is more inclined to establish a revenue-sharing partnership with the manufacturer to mitigate risks, leading him to decrease his commission rate to attract the manufacturer to select him as the agency channel. We demonstrate that, under conditions of demand uncertainty, a significant distinction between agency and reselling channels lies in the distribution of risk, i.e., whether the platform assumes the risk alone or shares it with the manufacturer. Furthermore, we highlight the free-ride effect , wherein an agency platform can benefit from his rival’s superior information capability. As a result, a complex relationship, characterized by both cooperation and rivalry, may develop between the two platforms.},
  archive      = {J_EJOR},
  author       = {Haoruo Zhu and Yaodong Ni and Yongbo Xiao},
  doi          = {10.1016/j.ejor.2024.11.048},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {125-138},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Information sharing across competing platforms with varying information capabilities},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing bus bridging services with mode choice in response to urban rail transit emergencies. <em>EJOR</em>, <em>323</em>(1), 108-124. (<a href='https://doi.org/10.1016/j.ejor.2024.11.042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During urban rail transit (URT) emergencies, stranded passengers may choose to seek alternative modes of transportation instead of waiting in the URT system for the bus bridging service to commence. To tackle this challenge, we present an optimization-based approach focused on identifying promising bus bridging lines and devising efficient services. Specifically, we introduce a candidate line generation (CLG) model designed to identify potential bus bridging lines. This model is akin to solving a k-all pair elementary shortest path problem with resource constraints ( k -APESPPRC). We develop an exact algorithm based on the label setting algorithm and Lawler's algorithm to solve this model effectively. Subsequently, our approach allocates limited bus resources to determine line selection, frequency determination, bus deployment, and passenger assignment on the integrated network (i.e., partial URT network and bus network) with the consideration of mode choice. Given the inherent complexity of this problem, we introduce an optimization-based tabu search method ( opt -tabu) designed to efficiently solve real-size instances. To demonstrate the effectiveness of our approach, we present a real case study conducted in Hong Kong, showcasing its efficiency and practicality. In summary, this study makes a valuable contribution to the transportation industry by providing a practical and efficient approach to managing URT emergencies, emphasizing the significance of considering mode choice in the context of bus bridging services.},
  archive      = {J_EJOR},
  author       = {Yun Wang and Yu Zhou and Hai Yang and Bin Yu and Xiaobing Liu},
  doi          = {10.1016/j.ejor.2024.11.042},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {108-124},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing bus bridging services with mode choice in response to urban rail transit emergencies},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The location routing problem with time windows and load-dependent travel times for cargo bikes. <em>EJOR</em>, <em>323</em>(1), 97-107. (<a href='https://doi.org/10.1016/j.ejor.2024.11.040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Last-mile delivery with traditional delivery trucks is ecologically unfriendly and leads to high road utilization. Thus, cities seek for different delivery options to solve these problems. One promising option is the use of cargo bikes in last-mile delivery. These bikes are typically released at micro hubs, which are small containers or facilities located at advantageous places in the city center. Since the bike’s travel speed depends on its remaining load and the street gradient, placing the hubs at valleys might cause additional work for rides. Therefore, the following question arises: How high is the impact of load-dependent travel times on micro hubs’ cost-optimal placements? To answer this question, we introduce the location routing problem with time windows and load-dependent travel times. We formulate the problem as a mixed-integer linear program and introduce an adaptive large neighborhood search with a problem-specific procedure for micro hub placements and problem-specific operators to solve larger instances. In a numerical study, we find that load-dependent travel times significantly influence the location of hubs, following that hubs with a higher elevation are preferably used. Moreover, customers are served from hubs with a similar elevation. This would not be the case if load-dependent travel times are ignored, resulting in an increase in costs by up to 2.7 % or, instead, to up to 26 % infeasible solutions as time windows are not adhered to.},
  archive      = {J_EJOR},
  author       = {Alexander Rave and Pirmin Fontaine},
  doi          = {10.1016/j.ejor.2024.11.040},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {97-107},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The location routing problem with time windows and load-dependent travel times for cargo bikes},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven ordering policies for target oriented newsvendor with censored demand. <em>EJOR</em>, <em>323</em>(1), 86-96. (<a href='https://doi.org/10.1016/j.ejor.2024.10.045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s fiercely competitive business environment, meeting and surpassing earnings expectations is paramount for public companies. This study focuses on how companies selling newsvendor-type products determine the order quantity to maximize the probability of achieving a target profit (known as profitability). Decision-makers often face challenges in real-life situations where the true demand distributions are unknown, and they have to rely on historical demand data. In some cases, they may only have access to sales data, which is referred to as censored demand. We propose data-driven ordering policies that aim to maximize profitability based solely on historical demand data and sales data respectively. Specifically, we first develop a data-driven nonparametric model using historical demand data, and then present a mixed-integer programming to solve the model. In the case of censored demand, we further propose an enhanced data-driven nonparametric model that leverages the Kaplan–Meier estimator to correct sales data. We prove that the proposed data-driven ordering policies are asymptotically optimal and consistent, regardless of whether the demand is censored or not. To avoid overestimation of true profitability due to sampling error, we propose nonparametric bootstrap methods to estimate the lower confidence bound of profitability, providing a conservative estimate. We also demonstrate the consistency of the lower confidence bound of profitability obtained through the bootstrap-based numerical methods. Finally, we conduct numerical experiments using synthetic data to showcase the effectiveness of the proposed methods.},
  archive      = {J_EJOR},
  author       = {Wanpeng Wang and Shiming Deng and Yuying Zhang},
  doi          = {10.1016/j.ejor.2024.10.045},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {86-96},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Data-driven ordering policies for target oriented newsvendor with censored demand},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One benders cut to rule all schedules in the neighbourhood. <em>EJOR</em>, <em>323</em>(1), 62-85. (<a href='https://doi.org/10.1016/j.ejor.2024.12.009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logic-Based Benders Decomposition (LBBD) and its Branch-and-Cut variant, namely Branch-and-Check, enjoy an extensive applicability on a broad variety of problems, including scheduling. As the application of LBBD to resource-constrained scheduling remains less explored, we propose a position-based Mixed-Integer Linear Programming (MILP) formulation for scheduling on unrelated parallel machines. To improve upon it, we notice that certain k − OPT neighbourhoods could be explored by regular local search operators, thus allowing us to integrate Local Branching into Branch-and-Check. After enumerating such neighbourhoods and obtaining their local optima – hence, proving that they are suboptimal – a local branching cut (applied as a Benders cut) eliminates all their solutions at once, thus avoiding an overload of the master problem with Benders cuts. However, to guarantee convergence to optimality, the constructed neighbourhood should be exhaustively explored, hence this time-consuming procedure must be accelerated by domination rules or selectively implemented on nodes which are more likely to reduce the optimality gap. In this study, we apply this idea on the ‘internal (job) swaps’ to construct formulation-specific 4-OPT neighbourhoods. We experiment extensively with the minimisation of total completion times and total tardiness on unrelated machines with sequence-dependent and resource-constrained setups. Our results show that our proposed use of local branching reduces optimality gaps considerably compared to standard Branch-and-Check or a monolithic Constraint Programming implementation. The simplicity of our approach allows its transferability to other neighbourhoods of the same or analogous formulations.},
  archive      = {J_EJOR},
  author       = {Ioannis Avgerinos and Ioannis Mourtos and Stavros Vatikiotis and Georgios Zois},
  doi          = {10.1016/j.ejor.2024.12.009},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {62-85},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {One benders cut to rule all schedules in the neighbourhood},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-phase algorithm for the three-dimensional loading vehicle routing problem with split pickups and time windows. <em>EJOR</em>, <em>323</em>(1), 45-61. (<a href='https://doi.org/10.1016/j.ejor.2024.12.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a survey of Belgian logistics service providers, the efficiency of first-mile pickup operations was identified as a key area for improvement, given the increasing number of returns in e-commerce, which has a significant impact on traffic congestion, carbon emissions, energy consumption and operational costs. However, the complexity of first-mile pickup operations, resulting from the small number of parcels to be collected at each pickup location, customer time windows, and the need to efficiently accommodate the highly heterogeneous cargo inside the vans, has hindered the development of real-world solution approaches. This article tackles this operational problem as a vehicle routing problem with time windows, time-dependent travel durations, and split pickups and integrates practical 3D container loading constraints such as vertical and horizontal stability as well as a more realistic reachability constraint to replace the classical “Last In First Out” (LIFO) constraint. To solve it, we propose a three-phase heuristic based on a savings constructive heuristic, an extreme point concept for the loading aspect and a General Variable Neighborhood Search as an improvement phase for both routing and packing. Numerical experiments are conducted to assess the performance of the algorithm on benchmark instances and new instances are tested to validate the positive managerial impacts on cost when allowing split pickups and on driver working duration when extending customer time windows. In addition, we show the impacts of considering the reachability constraint on cost and of the variation of speed during peak hours on schedule feasibility.},
  archive      = {J_EJOR},
  author       = {Emeline Leloup and Célia Paquay and Thierry Pironet and José Fernando Oliveira},
  doi          = {10.1016/j.ejor.2024.12.005},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {45-61},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A three-phase algorithm for the three-dimensional loading vehicle routing problem with split pickups and time windows},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-index rule for managing temporary congestion. <em>EJOR</em>, <em>323</em>(1), 34-44. (<a href='https://doi.org/10.1016/j.ejor.2024.11.045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work in healthcare operations provide empirical evidence for the deterioration of service quality due to congestion. Motivated by these findings, we formulate a novel scheduling problem to study how a service provider should prioritize jobs in order to mitigate the impact of temporary congestion-related issues. We analyze the model and show that the optimal policy can be interpreted as a dynamic priority rule that operates in two phases. When the system is overloaded, it is optimal to process jobs according to an index that generalizes Smith’s rule by incorporating the congestion cost. Once the system is no longer overloaded, Smith’s rule becomes optimal. However, the decision about which job to process earlier versus later appears to be challenging (we establish a polynomial time reduction from the partition problem). Our work shows that to respond to congestion, the decision maker should deviate from default scheduling practices and adjust jobs’ urgency at times of congestion to account for potential congestion-related costs. This increases the priority that should be given to shorter jobs (which reduces the time the system is congested), while still taking into account other job characteristics.},
  archive      = {J_EJOR},
  author       = {Yaron Shaposhnik},
  doi          = {10.1016/j.ejor.2024.11.045},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {34-44},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A dual-index rule for managing temporary congestion},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiskilled workforce staffing and scheduling: A logic-based benders’ decomposition approach. <em>EJOR</em>, <em>323</em>(1), 20-33. (<a href='https://doi.org/10.1016/j.ejor.2024.11.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the staffing and scheduling problem of a multiskilled workforce with uncertain demand. We formulate the problem as a two-stage stochastic integer program. The first stage considers strategic decisions, including recruiting permanent staff from an available pool and training them with additional skills, and the second stage focuses on operational decisions, involving the allocation of the multiskilled workforce and the hiring of temporary staff to accommodate uncertain demand. To effectively solve problems of practical sizes, we develop a novel solution algorithm based on the logic-based Benders’ decomposition (LBBD) approach, incorporating a customized analytical cut. We validate our approach through a case study using the data from a prefabrication company, demonstrating the significant cost savings achieved through workforce multiskilling. Our experimental results show that the proposed method is substantially more efficient than the latest Gurobi solver, up to 133 times faster and on average 29 times faster than directly solving the monolithic deterministic equivalent problem (MDEP).},
  archive      = {J_EJOR},
  author       = {Araz Nasirian and Lele Zhang and Alysson M. Costa and Babak Abbasi},
  doi          = {10.1016/j.ejor.2024.11.033},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {20-33},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multiskilled workforce staffing and scheduling: A logic-based benders’ decomposition approach},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost efficiency in water supply systems: An applied review on optimization models for the pump scheduling problem. <em>EJOR</em>, <em>323</em>(1), 1-19. (<a href='https://doi.org/10.1016/j.ejor.2024.07.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for efficient pump operation in water supply systems (WSS) has become increasingly important over time, driven by the growing energy consumption and the associated energy costs. Forecasts for 2050 anticipate a global increase in water demand by 55%, indicating an increasing surge in WSS energy consumption. Control of pumping stations, which consume 70% of the energy in WSS, is the most critical area for optimization. This optimization challenge is commonly referred as the pump scheduling problem (PSP), and can be addressed using a variety of mathematical formulations. While numerous formulations exist to solve this optimization problem, the large majority of the studies are focus on the optimization techniques, sidelining the problem formulation. Due to the unique physical characteristics of each WSS, individual mathematical formulations may exhibit different levels of performance. In addition to general pumps’ operation optimization, the employment of variable speed pumps (VSP) can lead to significant energy savings compared to fixed speed pumps (FSP). However, despite their apparent benefits, many established optimization models for the PSP have not yet incorporated VSP decision variables into their formulations. Therefore, this work aims to review the main mathematical formulations for the pump scheduling problem for WSS with VSP and to present a quantitative comparative study of three mathematical formulations applied to a case study in the literature. The comparative analysis here presented revealed that the optimization model based on duty cycles is more cost-efficient when compared to alternative approaches discussed in the literature.},
  archive      = {J_EJOR},
  author       = {Marlene Brás and Ana Moura and António Andrade-Campos},
  doi          = {10.1016/j.ejor.2024.07.039},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Cost efficiency in water supply systems: An applied review on optimization models for the pump scheduling problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study of asset and liability management applied to brazilian pension funds. <em>EJOR</em>, <em>322</em>(3), 1059-1076. (<a href='https://doi.org/10.1016/j.ejor.2024.11.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asset and Liability Management (ALM) is a critical framework for pension funds, ensuring they have sufficient assets to meet future liabilities (pension payments) while managing investment risks effectively. This paper utilizes Brazilian data to develop an ALM model specifically for pension funds in the country. The model employs an optimization strategy that minimizes expected contributions made by individuals throughout their working lives. This optimization adheres to cash flow limitations and regulatory restrictions. The objective function leverages a min–max robust optimization approach based on a three-scenario planning scheme inspired by Brazil’s Interbank Rate. We incorporate a machine learning approach based on CMARS to predict confidence intervals for the key stochastic model parameters, particularly those related to the real returns of Brazilian investment classes. The findings empower pension fund managers to formulate well-informed investment strategies. We highlight allocation strategies that can reduce contribution rates without jeopardizing fund solvency, even for managers with a more aggressive risk profile favoring higher stock market allocations. Additionally, the study is enriched by an empirical analysis using data from a Brazilian pension fund, demonstrating the model’s practical application. In short, this model offers valuable insights that can benefit a wide range of pension funds in the Brazilian market, and it could also be applied to similar situations globally.},
  archive      = {J_EJOR},
  author       = {Wilton Bernardino and Rodrigo Falcão and João Jr. and Raydonal Ospina and Filipe Costa de Souza and José Jonas Alves Correia},
  doi          = {10.1016/j.ejor.2024.11.016},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {1059-1076},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A study of asset and liability management applied to brazilian pension funds},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integration of support vector machines and mean-variance optimization for capital allocation. <em>EJOR</em>, <em>322</em>(3), 1045-1058. (<a href='https://doi.org/10.1016/j.ejor.2024.11.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel methodology for portfolio optimization that is the first to integrate support vector machines (SVMs) with cardinality-constrained mean–variance optimization. We propose augmenting cardinality-constrained mean–variance optimization with a preference for portfolios with the property that a low-dimensional hyperplane can separate assets eligible for investment from those ineligible. We present convex mixed-integer quadratic programming models that jointly select a portfolio and a separating hyperplane. This joint selection optimizes a tradeoff between risk-adjusted returns, hyperplane margin, and classification errors made by the hyperplane. The models are amenable to standard commercial branch-and-bound solvers, requiring no custom implementation. We discuss the properties of the proposed optimization models and draw connections between existing portfolio optimization and SVM approaches. We develop a parameter selection strategy to address the selection of big- M s and provide a financial interpretation of the proposed approach’s parameters. The parameter strategy yields valid big- M values, ensures the risk of the resulting portfolio is within a factor of the lowest possible risk, and produces informative hyperplanes for practitioners. The mathematical programming models and the associated parameter selection strategy are amenable to financial backtesting. The models are evaluated in-sample and out-of-sample on two distinct datasets in a rolling horizon backtesting framework. The portfolios resulting from the proposed approach display improved out-of-sample risk-adjusted returns compared to cardinality-constrained mean–variance optimization.},
  archive      = {J_EJOR},
  author       = {David Islip and Roy H. Kwon and Seongmoon Kim},
  doi          = {10.1016/j.ejor.2024.11.022},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {1045-1058},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integration of support vector machines and mean-variance optimization for capital allocation},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The yin and yang of banking: Modeling desirable and undesirable outputs. <em>EJOR</em>, <em>322</em>(3), 1025-1044. (<a href='https://doi.org/10.1016/j.ejor.2024.11.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel by-production approach to modeling desirable and undesirable output production processes in the US banking sector. We utilize the structural proxy variable framework in which desirable outputs (different types of loans and other income-generating activities) are exogenous, which is a common practice in the banking literature. The undesirable output is non-performing loans (NPLs). To address the endogeneity of variable inputs (purchased funds and core deposits) in the production of desirable outputs, we employ an input distance function and rely on the bank’s cost-minimizing behavioral assumption. We specify the undesirable output technology as a function of desirable outputs as well as other factors such as total non-transaction accounts, undivided profits, and capital reserves. Using US commercial bank data from 2001 to 2020, we find that bank productivity exhibits steady growth in desirable outputs. Banks prioritize reducing the overall productivity impact of NPLs post-crisis, shifting focus from pre-crisis service provision.},
  archive      = {J_EJOR},
  author       = {Yulu Wang and Subal C. Kumbhakar and Man Jin},
  doi          = {10.1016/j.ejor.2024.11.004},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {1025-1044},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The yin and yang of banking: Modeling desirable and undesirable outputs},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the valuation of legacy power production in liberalized markets via option-pricing. <em>EJOR</em>, <em>322</em>(3), 1005-1024. (<a href='https://doi.org/10.1016/j.ejor.2024.10.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legacy assets can constitute entry barriers in liberalized power markets. Regulations pertaining to such assets have many objectives, the most important of which are to transfer the benefits of an economical production technology to consumers and foster competition. To that end, countries have adopted various regulations but there is no consensus today on identifying the first best solution. Inspired by the French regulation of historical nuclear production and considering the market risk that now prevails in the sector, we propose an option-based approach to regulating legacy assets that reflects production costs and encompasses optionality at the same time. To achieve that aim, we study a competitive, but financially incomplete market where the incumbent and several competitors exchange legacy production via a regulated call option. Agents do not face the same risk exposure and their attitudes toward risk, which we model by coherent risk measures, might differ. The result is a stochastic equilibrium model of regulated option-pricing in incomplete markets that we calibrate numerically and solve for the French market. We quantify the option value and assess its impact on the system for various regimes of the spot market, including the one of very high and volatile prices of the recent energy crisis. We also analyze the impacts of risk aversion and the option’s maturity. Based on our analysis, we provide recommendations for enhancing the current French regulation of historical nuclear production.},
  archive      = {J_EJOR},
  author       = {Ibrahim Abada and Mustapha Belkhouja and Andreas Ehrenmann},
  doi          = {10.1016/j.ejor.2024.10.033},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {1005-1024},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {On the valuation of legacy power production in liberalized markets via option-pricing},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-averse algorithmic support and inventory management. <em>EJOR</em>, <em>322</em>(3), 993-1004. (<a href='https://doi.org/10.1016/j.ejor.2024.11.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study how managers allocate resources in response to algorithmic recommendations that are programmed with specific levels of risk aversion. Using the anchoring and adjustment heuristic, we derive our predictions and test them in a series of multi-item newsvendor experiments. We find that highly risk-averse algorithmic recommendations have a strong and persistent influence on order decisions, even after the recommendations are no longer available. Furthermore, we show that these effects are similar regardless of factors such as source of advice (i.e., human vs. algorithm) and decision autonomy (i.e., whether the algorithm is externally assigned or chosen by the subjects themselves). Finally, we disentangle the effect of risk attitude from that of anchor distance and find that subjects selectively adjust their order decisions by relying more on algorithmic advice that contrasts with their inherent risk preferences. Our findings suggest that organizations can strategically utilize risk-averse algorithmic tools to improve inventory decisions while preserving managerial autonomy.},
  archive      = {J_EJOR},
  author       = {Pranadharthiharan Narayanan and Jeeva Somasundaram and Matthias Seifert},
  doi          = {10.1016/j.ejor.2024.11.013},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {993-1004},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Risk-averse algorithmic support and inventory management},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implementing no free disposability in data envelopment analysis. <em>EJOR</em>, <em>322</em>(3), 978-992. (<a href='https://doi.org/10.1016/j.ejor.2024.11.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data envelopment analysis (DEA) relies on two main postulates of convexity and inefficiency (free disposability). No free disposability postulate is suggested to address undesirable measures. In this study, we demonstrate how no-disposability assumption can be correctly integrated into the DEA framework. We propose the appropriate constraints that should be used in the absence of the free disposability postulate in a DEA model. The additional constraints bound the previously unbounded feasible region (production technology) rather than altering the strongly efficient frontier. We also discuss that treating an undesirable output (input) as a desirable input (output) does not affect the corresponding efficient frontier of a dataset, but misrepresents its corresponding production technology in the presence of free disposability postulate. We provide numerical examples to clarify the concerns in treating an undesirable measure as a desirable measure. A real-life example of United States’ electric power plants is also discussed.},
  archive      = {J_EJOR},
  author       = {Dariush Khezrimotlagh and Joe Zhu},
  doi          = {10.1016/j.ejor.2024.11.029},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {978-992},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Implementing no free disposability in data envelopment analysis},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective route planning of an unmanned air vehicle in continuous terrain: An exact and an approximation algorithm. <em>EJOR</em>, <em>322</em>(3), 960-977. (<a href='https://doi.org/10.1016/j.ejor.2024.11.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) are widely used for military and civilian purposes. Effective route planning is an important component of their successful missions. In this study, we address the route planning problem of a UAV tasked with collecting information from various target locations in a protected terrain. We consider multiple targets, three objectives, and time-dependent information availability. Modeling the movement of UAVs in a continuous terrain in the presence of multiple objectives is complex. Conflicting objectives typically lead to a continuum of efficient trajectory options between two targets. We formulate the routing problem as a mixed-integer programming (MIP) model that captures the movement in the continuous terrain. We demonstrate the superiority of the continuous terrain formulation over the simplified discretized terrain formulation. We also develop an approximation algorithm that reduces the computational requirements of the MIP model substantially while ensuring a desired level of precision.},
  archive      = {J_EJOR},
  author       = {Erdi Dasdemir and Murat Köksalan and Diclehan Tezcaner Öztürk},
  doi          = {10.1016/j.ejor.2024.11.015},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {960-977},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multi-objective route planning of an unmanned air vehicle in continuous terrain: An exact and an approximation algorithm},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effectiveness of social distancing under partial compliance of individuals. <em>EJOR</em>, <em>322</em>(3), 949-959. (<a href='https://doi.org/10.1016/j.ejor.2024.11.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social distancing reduces infectious disease transmission by limiting contact frequency and proximity within a community. However, compliance varies due to its impact on daily life. This paper explores the effects of compliance on social distancing effectiveness through a “social distancing game”, where community members make decisions based on personal utility. We conducted numerical experiments to evaluate how different policy settings for social distancing affect disease transmission. Our findings suggest several key points for developing effective social distancing policies. Firstly, while generally effective, overly strict policies may lead to noncompliance and reduced effectiveness. Secondly, the public health benefits of social distancing need to be balanced against social costs, emphasizing policy efficiency. Lastly, for diseases with low reinfection risk, a segmented policy exempting immune individuals could lessen both infections and socioeconomic costs.},
  archive      = {J_EJOR},
  author       = {Hyelim Shin and Taesik Lee},
  doi          = {10.1016/j.ejor.2024.11.006},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {949-959},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Effectiveness of social distancing under partial compliance of individuals},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Process improvement under the reference price effect. <em>EJOR</em>, <em>322</em>(3), 937-948. (<a href='https://doi.org/10.1016/j.ejor.2024.10.037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-reducing process improvement, leading to price reductions and consequently sales growth, is becoming increasingly relevant in today’s uncertain economic environment. However, existing process improvement studies involving sales growth typically assume that consumers only consider the current price of a product when making a purchase. In reality, sales growth also often stems from the reference price effect, where consumers factor in both the current and past prices. By incorporating the reference price effect, we examine the process improvement investment decisions and pricing strategies in a decentralized supply chain. We develop a two-period game-theoretic model, where the supplier invests in process improvement to reduce production costs, and the supplier and the retailer set their prices. This approach differs from existing reference price effect literature, where prices are predetermined exogenously in a decentralized supply chain. We find that the reference price effect stimulates process improvement investment, making both firms more profitable. However, a more prominent reference price effect may significantly decrease supply chain efficiency in the presence of process improvement, resulting in lower profits that move away from what an integrated firm would achieve. When firms set their own prices, the reference price effect intensifies competition for profits and worsens misalignment caused by process improvement. This outcome contrasts with existing studies, which usually argue that the reference price effect increases efficiency. Therefore, managers should consider consumer responses to price changes when making process improvement investment decisions and analyze their impacts on both supply chain profitability and efficiency.},
  archive      = {J_EJOR},
  author       = {Zeming Wang and Jasper Veldman and Ruud Teunter},
  doi          = {10.1016/j.ejor.2024.10.037},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {937-948},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Process improvement under the reference price effect},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust multilinear target-based decision analysis considering high-dimensional interactions. <em>EJOR</em>, <em>322</em>(3), 920-936. (<a href='https://doi.org/10.1016/j.ejor.2024.10.036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multilinear Target-based Preference Functions (MTPFs) support multi-attribute decision problems characterized by attribute interactions and targets. However, existing research falls short in flexibly modeling high-dimensional interactions and lacks robustness in decision-making recommendations when faced with uncertain parameters and targets. The paper proposes a robust multilinear target-based decision analysis framework considering high-dimensional interactions, along with uncertainties in parameters and targets. First, the necessity of high-dimensional interactions and the limitations of available MTPFs in modeling high-dimensional interactions are demonstrated. Second, the MTPFs based on the 2-interactive fuzzy measure and the Nonmodularity index are proposed to model the high-dimensional interactions and simultaneously reduce the computational challenges of parameter identification. Third, new descriptive measures are proposed based on the Stochastic Multicriteria Acceptability Analysis to evaluate the robustness of decision recommendations subject to uncertain targets and parameters. The validation and advantages of the framework are illustrated with simulation studies and an application in customer competitive evaluation of smart thermometer patches.},
  archive      = {J_EJOR},
  author       = {Qiong Feng and Shurong Tong and Salvatore Corrente and Xinwei Zhang},
  doi          = {10.1016/j.ejor.2024.10.036},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {920-936},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust multilinear target-based decision analysis considering high-dimensional interactions},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Queues with service resetting. <em>EJOR</em>, <em>322</em>(3), 908-919. (<a href='https://doi.org/10.1016/j.ejor.2024.12.044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service time fluctuations heavily affect the performance of queueing systems, causing long waiting times and backlogs. Recently, it was shown that when service times are solely determined by the server, service resetting can mitigate the deleterious effects of service time fluctuations and drastically improve queue performance (Bonomo et al., 2022). Yet, in many queueing systems, service times have two independent sources: the intrinsic server slowdown ( S ) and the jobs’ inherent size ( X ). In these, so-called S & X queues (Gardner et al., 2017), service resetting results in a newly drawn server slowdown while the inherent job size remains unchanged. Remarkably, resetting can be useful even then. To show this, we develop a comprehensive theory of S & X queues with service resetting. We consider cases where the total service time is either a product or a sum of the service slowdown and the jobs’ inherent size. For both cases, we derive expressions for the total service time distribution and its mean under a generic service resetting policy. Two prevalent resetting policies are discussed in more detail. We first analyze the constant-rate (Poissonian) resetting policy and derive explicit conditions under which resetting reduces the mean service time and improves queue performance. Next, we consider the sharp (deterministic) resetting policy. While results hold regardless of the arrival process, we dedicate special attention to the S & X -M/G/1 queue with service resetting, and obtain the distribution of the number of jobs in the system and their sojourn time. Our analysis highlights situations where service resetting can be used as an effective tool to improve the performance of S & X queueing systems. Several examples are given to illustrate our analytical results, which are corroborated using numerical simulations.},
  archive      = {J_EJOR},
  author       = {Ofek Lauber Bonomo and Uri Yechiali and Shlomi Reuveni},
  doi          = {10.1016/j.ejor.2024.12.044},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {908-919},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Queues with service resetting},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximating G(t)/GI/1 queues with deep learning. <em>EJOR</em>, <em>322</em>(3), 889-907. (<a href='https://doi.org/10.1016/j.ejor.2024.12.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world queueing systems exhibit a time-dependent arrival process and can be modeled as a G ( t ) / G I / 1 queue. Despite its wide applicability, little can be derived analytically about this system, particularly its transient behavior. Yet, many services operate on a schedule where the system is empty at the beginning and end of each day; thus, such systems are unlikely to enter a steady state. In this paper, we apply a supervised machine learning approach to solve a fundamental problem in queueing theory: estimating the transient distribution of the number in the system for a G ( t ) / G I / 1 . We develop a neural network mechanism that provides a fast and accurate predictor of these distributions for moderate horizon lengths and practical settings. It is based on using a Recurrent Neural Network (RNN) architecture based on the first several moments of the time-dependent inter-arrival and the stationary service time distributions; we call it the Moment-Based Recurrent Neural Network (RNN) method ( MBRNN ). Our empirical study suggests MBRNN requires only the first four inter-arrival and service time moments. We use simulation to generate a substantial training dataset and present a thorough performance evaluation to examine the accuracy of our method using two different test sets. We perform sensitivity analysis over different ranges of Squared Coefficient of Variation (SCV) of the inter-arrival and service time distribution and average utilization level. We show that even under the configuration with the worst performance errors, the mean number of customers over the entire timeline has an error of less than 3%. We further show that our method outperforms fluid and diffusion approximations. While simulation modeling can achieve high accuracy (in fact, we use it as the ground truth), the advantage of the MBRNN over simulation is runtime. While the runtime of an accurate simulation of a G ( t ) / G I / 1 queue can be measured in hours, the MBRNN analyzes hundreds of systems within a fraction of a second. We demonstrate the benefit of this runtime speed when our model is used as a building block in optimizing the service capacity for a given time-dependent arrival process. This paper focuses on a G ( t ) / G I / 1 , however the MBRNN approach demonstrated here can be extended to other queueing systems, as the training data labeling is based on simulations (which can be applied to more complex systems) and the training is based on deep learning, which can capture very complex time sequence tasks. In summary, the MBRNN has the potential to revolutionize our ability for transient analysis of queueing systems.},
  archive      = {J_EJOR},
  author       = {Eliran Sherzer and Opher Baron and Dmitry Krass and Yehezkel Resheff},
  doi          = {10.1016/j.ejor.2024.12.030},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {889-907},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Approximating G(t)/GI/1 queues with deep learning},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-sharing in energy communities. <em>EJOR</em>, <em>322</em>(3), 870-888. (<a href='https://doi.org/10.1016/j.ejor.2024.12.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy communities are considered one of the pillars of the energy transition, owing to the rapid development of digital smart appliances and metering. They benefit from strong political support to accommodate their penetration in Europe. Nevertheless, the pace at which they have developed has been very slow compared with what was expected a decade ago. Many articles have revealed some of the underlying reasons, among which are social heterogeneity among participants, unfavorable local regulations, and inadequate governance. Most recently, a nascent body of research has highlighted the need to find adequate sharing rules for the benefits of community projects. Because of the complexity of these rules, the appointment of a community manager or coordinator may be necessary. This paper follows suit by providing guidance to policy makers or community managers about optimal risk-sharing schemes among members of an energy community. By modeling and simulating energy communities that invest in a rooftop photo-voltaic project and face some degree of production and remuneration risk, we find that a high level of risk aversion makes it impossible to allocate the risk in a stable way. Furthermore, we show that some communities whose members’ risk aversion is too heterogeneous cannot form successfully. Besides, even when risk can be allocated in a stable manner, we show that fair allocations are so complex that they require the intervention of a coordinator or a community manager. Finally, we analyze the advantages of developing judicious risk-sharing instruments between communities and a central entity for providing stability.},
  archive      = {J_EJOR},
  author       = {Ibrahim Abada and Andreas Ehrenmann and Xavier Lambin},
  doi          = {10.1016/j.ejor.2024.12.029},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {870-888},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Risk-sharing in energy communities},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Truck–drone routing problem with stochastic demand. <em>EJOR</em>, <em>322</em>(3), 854-869. (<a href='https://doi.org/10.1016/j.ejor.2024.11.036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Truck–drone combination involves launch/retrieval of rotary-wing drones on trucks, which can address the issues of limited endurance and capacity of rotary-wing drones in delivery systems. Truck–drone combination technologies provide a compelling alternative to traditional emergency logistics systems that rely on on-ground transportation networks. Thus far, little research has been conducted on the truck–drone routing variant with stochastic demand, which is closely related to emergency logistics systems. Herein, we formally define the truck–drone routing problem with stochastic demand (TDRP-SD), which involves drones responding quickly to stochastic demands and restocking the supply. In particular, a new restocking policy, termed the truck–drone synchronized (TDS) restocking policy, is introduced to complement the traditional restocking operations that rely on ground vehicles. We analyze the characteristics of the introduced restocking policy and develop several propositions to address the computational burden caused by the dynamic programming computation of the expected cost. We propose a hybrid heuristic that combines the state-of-the-art Slack Induction by String Removals (SISRs) and greedy insertion utilizing blink rules. Several mechanisms, such as short-route deep search, lower-bound and upper-bound guiding, and simulated annealing, are adopted to ensure the algorithm performance. In computational experiments, the hybrid heuristic solves two types of benchmark instances and achieves new solutions. In addition, a collection of converted instances with up to 302 customers is effectively solved. The sensitivity analysis demonstrates the performance of the TDS restocking policy.},
  archive      = {J_EJOR},
  author       = {Feilong Wang and Hongqi Li and Hanxi Xiong},
  doi          = {10.1016/j.ejor.2024.11.036},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {854-869},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Truck–drone routing problem with stochastic demand},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of vendor preferences on commission policy of E-commerce platform. <em>EJOR</em>, <em>322</em>(3), 841-853. (<a href='https://doi.org/10.1016/j.ejor.2024.11.037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the prevalent online marketplaces, vendors manage daily operations while e-commerce platforms (EPs) that provide auxiliary services and charge commission fees. Two commission policies are examined in this article: Fixed Commission Policy (FCP), involving a fixed usage fee, and Ordinary Commission Policy (OCP), incorporating an additional fee proportional to sales revenue alongside the fixed usage fee, with this proportion referred to as the commission rate. We develop a two-stage game-theoretical model of an e-commerce supply chain, wherein the EP sets the commission policy and a risk-sensitive vendor determines its stock level. The vendor's risk attitude is characterized by three key preferences: reference preference, utility weight preference, and loss aversion preference. Under reasonable assumptions, we establish the existence and uniqueness of the game equilibrium, yielding several key insights: (i) Vendor preferences significantly influence the commission policy, with reference preference being central in shaping the optimal commission rate. Specifically, while the FCP is optimal for risk-neutral vendors, it may not be suitable when vendors are risk-sensitive. (ii) The ratio of unit cost to retail price is the primary driver of variations in optimal commission rate. Moreover, the optimal commission rate tends to decrease as this ratio increases. (iii) In the presence of risk sensitivity, a commission policy maximizing the EP's profit can lead to Pareto improvement compared to one aimed at centralized profit maximization. Our analysis offers valuable insights into the design of commission policies for EPs, providing credible explanations for various economic phenomena associated with these policies in e-commerce practices.},
  archive      = {J_EJOR},
  author       = {Jiansheng Dai and Xinyu Zhang},
  doi          = {10.1016/j.ejor.2024.11.037},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {841-853},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Impact of vendor preferences on commission policy of E-commerce platform},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resilient transportation network design with disruption uncertainty and lead times. <em>EJOR</em>, <em>322</em>(3), 827-840. (<a href='https://doi.org/10.1016/j.ejor.2024.11.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-efficient and reliable transports are needed to supply products competitively. Thus, particularly in increasingly complex and global supply chains, identifying the optimal transportation mode is a critical decision. Transportation modes, however, are prone to disruptions, such as hurricanes, low water levels, or port shutdowns, resulting in transportation stops and cost increases. To counteract these disruptions, different resilience strategies are studied to increase the capability of a network to withstand, adapt, and recover from disruptions. For a cost-optimal use, it is necessary to determine the optimal mix of strategic, tactical, and operational strategies. We provide a decision-support model that decides on the optimal mix of resilience strategies, such as multi-sourcing, inventory, or operational re-routing, for a supply chain with transportation disruption uncertainty to minimize total expected costs. The problem is formulated as a two-stage stochastic mixed-integer linear program that explicitly considers lead times. To handle large instances, we propose a Benders decomposition approach enhanced through lower-bound lifting and valid inequalities, branch-and-benders-cut, and a warm-start heuristic. Computational experiments show that large instances can be solved to near-optimality, whereas a commercial solver does not find feasible solutions. We present a case study for a company’s inbound supply chain design with recurring transportation cost uncertainty. Considering disruption and lead time effects, a mix of resilience strategies from strategic to operational level leads to cost improvements of up to 50%. Furthermore, we show that the ability to predict disruptions can further reduce resilience-related costs by 10% if sufficient operational re-routing capacities are available.},
  archive      = {J_EJOR},
  author       = {Daniel Müllerklein and Pirmin Fontaine},
  doi          = {10.1016/j.ejor.2024.11.021},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {827-840},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Resilient transportation network design with disruption uncertainty and lead times},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of channel role on the outsourcing of after-sales service with asymmetric retailer competition. <em>EJOR</em>, <em>322</em>(3), 812-826. (<a href='https://doi.org/10.1016/j.ejor.2024.11.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After-sales service is support provided to a customer after purchase, which potentially leads to higher customer satisfaction and is demand-enhancing. Using a game-theoretic model in which a manufacturer determines its after-sales service and distribution channel strategies in the presence of two asymmetric retailers, we identify channel position as an important criterion in determining the outsourcing of after-sales service. Specifically, outsourcing to a third-party provider, due to its lack of channel interaction, is never an optimal choice for the manufacturer unless the third-party has a significant cost advantage in providing after-sales service. However, because of the channel role of the retailers, the manufacturer outsources to the large retailer rather than undertaking the after-sales service in-house, when the competing small retailer is less competitive and the cost of service provision is high. The trade-off between the manufacturer outsourcing the service and undertaking that in-house involves whether the manufacturer accommodates the small retailer in the market. When service provision is outsourced, the large retailer enjoys a lower wholesale price if the small retailer is present, and therefore the large retailer subsidizes the manufacturer to induce the manufacturer to accommodate the small retailer. However, the manufacturer, when undertaking the service by itself, forgoes the small retailer. Finally, we show that when the manufacturer adopts a multi-retailer distribution channel, the large retailer benefits because improved after-sales service increases demand and consumer valuation of the product. We also demonstrate the robustness of our key results in multiple extensions.},
  archive      = {J_EJOR},
  author       = {Shuguang Zhang and Wei Shi Lim and Ziqiu Ye},
  doi          = {10.1016/j.ejor.2024.11.020},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {812-826},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The impact of channel role on the outsourcing of after-sales service with asymmetric retailer competition},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retailer involvement in eco-conscious consumer-oriented carbon footprint reduction. <em>EJOR</em>, <em>322</em>(3), 795-811. (<a href='https://doi.org/10.1016/j.ejor.2024.10.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retailer involvement and consumer eco-consciousness are two critical considerations for firms when designing comprehensive carbon footprint reduction (CFR) plans. This paper constructs a supply chain (SC) where one manufacturer and one retailer make CFR and pricing strategies in response to eco-conscious consumers. Eco-conscious consumers form a reference carbon footprint to assess the product's green level. To achieve SC coordination, a cost-sharing contract is introduced. Our results suggest that retailer involvement in CFR always benefits the environment, SC performance, consumer surplus, and social welfare. During CFR cooperation, the manufacturer strategically affects the retailer's CFR decision by adjusting the CFR level, impacting environmental and economic outcomes. Although higher CFR efficiency by the manufacturer can benefit the environment, SC performance, consumer surplus, and social welfare, a similar emphasis by the retailer may have adverse effects. Surprisingly, a lower reference carbon footprint for eco-conscious consumers may be worse for the environment, depending on the retailer's CFR efficiency. Furthermore, implementing a cost-sharing contract under the retailer's different CFR efficiency yields two distinct impacts: a multi-win situation or an incentive conflict. Extended studies are further examined to validate the robustness of these main results.},
  archive      = {J_EJOR},
  author       = {Feiying Jiang and Weilai Huang and Jun Yang and Hongchen Duan},
  doi          = {10.1016/j.ejor.2024.10.030},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {795-811},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Retailer involvement in eco-conscious consumer-oriented carbon footprint reduction},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A speed-up procedure and new heuristics for the classical job shop scheduling problem: A computational evaluation. <em>EJOR</em>, <em>322</em>(3), 783-794. (<a href='https://doi.org/10.1016/j.ejor.2024.11.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The speed-up procedure proposed for the permutation flowshop scheduling problem with makespan minimisation (commonly denoted as Taillard’s acceleration) remains, after 30 years, one of the most important and relevant studies in the scheduling literature. Since its proposal, this procedure has been included in countless approximate optimisation algorithms, and its use is mandatory for several scheduling problems. Unfortunately, despite the importance of such a procedure in solving scheduling problems, we are not aware of any related speed-up procedure proposed for the classical job-shop scheduling problem. First, this study aims to fill this gap by proposing a novel speed-up procedure for the job-shop scheduling problem with makespan minimisation, capable of reducing the complexity of insertion-based procedures n times. Second, to test its performance, the procedure is embedded in a critical-path-based local search method. Furthermore, we thirdly propose five constructive and composite heuristics to obtain high-quality solutions in short time intervals. The composite heuristics apply the previous procedure to reduce their computational efforts. Finally, to complete the study, we conduct an extensive computational evaluation on 243 test instances from eight distinct benchmarks. In this evaluation, 30 heuristics are re-implemented and compared under the same computer conditions. The results indicate the superiority of the proposed approaches compared to the competitive algorithms for the problem under study.},
  archive      = {J_EJOR},
  author       = {Victor Fernandez-Viagas and Carla Talens and Bruno de Athayde Prata},
  doi          = {10.1016/j.ejor.2024.11.026},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {783-794},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A speed-up procedure and new heuristics for the classical job shop scheduling problem: A computational evaluation},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-dimensional bin packing with pattern-dependent processing time. <em>EJOR</em>, <em>322</em>(3), 770-782. (<a href='https://doi.org/10.1016/j.ejor.2024.11.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper the classical one-dimensional bin packing problem is integrated with scheduling elements: a due date is assigned to each item and the time required to process each bin depends on the pattern being used. The objective is to minimize a convex combination of the material waste and the delay costs, both significant in many real-world contexts. We present a novel pattern-based mixed integer linear formulation suitable for different classical scheduling objective functions, and focus on the specific case where the delay cost corresponds to the maximum tardiness. The formulation is tackled by a branch-and-price algorithm where the pricing of the column generation scheme is a quadratic problem solved by dynamic programming. A sequential value correction heuristic (SVC) is used to feed with warm starting solutions the column generation which, in turn, feeds the SVC with optimal prices so as to compute refined feasible solutions during the enumeration. Computational tests show that both column generation and branch-and-price substantially outperform standard methods in computing dual bounds and exact solutions. Additional tests are presented to analyze the sensitivity to parameters’ changes.},
  archive      = {J_EJOR},
  author       = {Fabrizio Marinelli and Andrea Pizzuti and Wei Wu and Mutsunori Yagiura},
  doi          = {10.1016/j.ejor.2024.11.023},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {770-782},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {One-dimensional bin packing with pattern-dependent processing time},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ε-constraint procedures for pareto front optimization of large size discrete time/cost trade-off problem. <em>EJOR</em>, <em>322</em>(3), 753-769. (<a href='https://doi.org/10.1016/j.ejor.2024.11.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrete time/cost trade-off problem (DTCTP) optimizes the project duration and/or cost while considering the trade-off between activity durations and their direct costs. The complete and non-dominated time-cost profile over the set of feasible project durations is achieved within the framework of Pareto front problem. Despite the importance of Pareto front optimization in project and portfolio management, exact procedures have achieved very limited success in solving the problem for large size instances. This study develops exact procedures based on combinations of mixed-integer linear programming (MILP), ε -constraint method, network and problem reduction techniques, and present new bounding strategies to solve the Pareto problem for large size instances. This study also provides new large size benchmark problem instances aiming to represent the size of real-life projects for the DTCTP. The new instances, therefore, are generated to include up to 990 activities and nine execution modes. Computational experiments reveal that the procedures presented herein can remarkably outperform the state-of-the-art exact methods. The new exact procedures enabled obtaining the optimal Pareto front for instances with serial networks that include more than 200 activities for the first time.},
  archive      = {J_EJOR},
  author       = {Saman Aminbakhsh and Rifat Sönmez and Tankut Atan},
  doi          = {10.1016/j.ejor.2024.11.032},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {753-769},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {ε-constraint procedures for pareto front optimization of large size discrete time/cost trade-off problem},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overcoming poor data quality: Optimizing validation of precedence relation data. <em>EJOR</em>, <em>322</em>(3), 740-752. (<a href='https://doi.org/10.1016/j.ejor.2024.11.009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insufficient data quality prevents data usage by decision support systems (DSS) in many areas of business. This is the case for data on precedence relations between tasks, which is relevant, for instance, in project scheduling and assembly line balancing. Inaccurate data on unnecessary precedence relations cannot be used, otherwise the recommendations of DSS may turn infeasible. So, unnecessary relations must be satisfied, diminishing the baseline problem’s solution space and the business result. Experts can validate the data, but their time is limited. We apply an optimization lens and formulate the data validation problem (DVP). Restricted by the available time budget, an expert dynamically receives queries about specific data entries and corrects or validates them. The DVP searches for an interview policy that states queries to the expert, each using up some of the time budget, in a way that maximizes the (weighted) number of removed precedence relations. We model the DVP as a dynamic program, derive optimal policies for several important special cases and design a heuristic interview policy LSTD. In a case study of an automobile manufacturer, this policy substantially reduces the stations’ idle time after selectively addressing about 8% of the data entries. We prove theoretically and numerically that data validation by experts can lead to significant savings. The number of queries required to validate the data exhaustively is much less than naive estimates. Additionally, the probability to remove an unnecessary precedence relation per query in a series of queries is high, even for simple interview policies.},
  archive      = {J_EJOR},
  author       = {Benedikt Finnah and Jochen Gönsch and Alena Otto},
  doi          = {10.1016/j.ejor.2024.11.009},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {740-752},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Overcoming poor data quality: Optimizing validation of precedence relation data},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years of maintenance optimization: Reflections and perspectives. <em>EJOR</em>, <em>322</em>(3), 725-739. (<a href='https://doi.org/10.1016/j.ejor.2024.07.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On the occasion of the 50th anniversary of the Association of European Operational Research Societies (EURO), we share our perspectives and reflections on maintenance research. We review the main methods and techniques for optimizing when and what to maintain, providing concrete examples as illustrations. We also discuss the optimization of the logistics support system surrounding the act of maintenance. In doing so, we highlight the multidisciplinary nature of maintenance research and its interface with other domains, such as spare parts inventory management, production scheduling, and transportation planning. We support our reflections with basic text-mining analyses of the archive of the European Journal of Operational Research , the journal published in collaboration with EURO. With this paper, we introduce interested researchers to maintenance optimization and share opportunities to close the gaps between the current state of research and real-world needs.},
  archive      = {J_EJOR},
  author       = {Joachim Arts and Robert N. Boute and Stijn Loeys and Heletjé E. van Staden},
  doi          = {10.1016/j.ejor.2024.07.002},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {725-739},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of maintenance optimization: Reflections and perspectives},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do stable outcomes survive in marriage problems with myopic and farsighted players?. <em>EJOR</em>, <em>322</em>(2), 713-724. (<a href='https://doi.org/10.1016/j.ejor.2024.12.043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider marriage problems where myopic and farsighted players interact and analyze these problems by means of the myopic-farsighted stable set. We require that coalition members are only willing to deviate if they all strictly benefit from doing so. Our first main result establishes the equivalence of myopic-farsighted stable sets based on arbitrary coalitional deviations and those based on pairwise deviations. We are interested in the question whether the core is still the relevant solution concept when myopic and farsighted agents interact and whether more farsighted agents are able to secure more preferred core elements. For marriage problems where all players are myopic as well as those where all players are farsighted, myopic-farsighted stable sets lead to the same prediction as the core. The same result holds for α -reducible marriage problems, without any assumptions on the set of farsighted agents. These results change when one side of the market is more farsighted than the other. For general marriage problems where all women are farsighted, only one core element can be part of a myopic-farsighted stable set, the woman-optimal stable matching. If the woman-optimal stable matching is dominated from the woman point of view by an individually rational matching, then no core element can be part of a myopic-farsighted stable set.},
  archive      = {J_EJOR},
  author       = {P. Jean-Jacques Herings and Ana Mauleon and Vincent Vannetelbosch},
  doi          = {10.1016/j.ejor.2024.12.043},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {713-724},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Do stable outcomes survive in marriage problems with myopic and farsighted players?},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust optimal investment and consumption strategies with portfolio constraints and stochastic environment. <em>EJOR</em>, <em>322</em>(2), 693-712. (<a href='https://doi.org/10.1016/j.ejor.2024.12.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a continuous-time investment–consumption problem with model uncertainty in a general diffusion-based market with random model coefficients. We assume that a power utility investor is ambiguity-averse, with the preference to robustness captured by the homothetic multiplier robust specification, and the investor’s investment and consumption strategies are constrained to closed convex sets. To solve this constrained robust control problem, we employ the stochastic Hamilton–Jacobi–Bellman–Isaacs equations, backward stochastic differential equations, and bounded mean oscillation martingale theory. Furthermore, we show the investor incurs (non-negative) utility loss, i.e. the loss in welfare, if model uncertainty is ignored. When the model coefficients are deterministic, we establish formally the relationship between the investor’s robustness preference and the robust optimal investment–consumption strategy and the value function, and the impact of investment and consumption constraints on the investor’s robust optimal investment–consumption strategy and value function. Extensive numerical experiments highlight the significant impact of ambiguity aversion, consumption and investment constraints, on the investor’s robust optimal investment–consumption strategy, utility loss, and value function. Key findings include: (1) short-selling restriction always reduces the investor’s utility loss when model uncertainty is ignored; (2) the effect of consumption constraints on utility loss is more delicate and relies on the investor’s risk aversion level.},
  archive      = {J_EJOR},
  author       = {Len Patrick Dominic M. Garces and Yang Shen},
  doi          = {10.1016/j.ejor.2024.12.010},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {693-712},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust optimal investment and consumption strategies with portfolio constraints and stochastic environment},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constraint learning approaches to improve the approximation of the capacity consumption function in lot-sizing models. <em>EJOR</em>, <em>322</em>(2), 679-692. (<a href='https://doi.org/10.1016/j.ejor.2024.11.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical capacitated lot-sizing models include capacity constraints relying on a rough estimation of capacity consumption. The plans resulting from these models are often not executable on the shop floor. This paper investigates the use of constraint learning approaches to replace the capacity constraints in lot-sizing models with machine learning models. Integrating machine learning models into optimization models is not straightforward since the optimizer tends to exploit constraint approximation errors to minimize the costs. To overcome this issue, we introduce a training procedure that guarantees overestimation in the training sample. In addition, we propose an iterative training example generation approach. We perform numerical experiments with standard lot-sizing instances, where we assume the shop floor is a flexible job-shop. Our results show that the proposed approach provides 100% feasible plans and yields lower costs compared to classical lot-sizing models. Our methodology is competitive with integrated lot-sizing and scheduling models on small instances, and it scales well to realistic size instances when compared to the integrated approach.},
  archive      = {J_EJOR},
  author       = {David Tremblet and Simon Thevenin and Alexandre Dolgui},
  doi          = {10.1016/j.ejor.2024.11.039},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {679-692},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Constraint learning approaches to improve the approximation of the capacity consumption function in lot-sizing models},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Where to plan shared streets: Development and application of a multicriteria spatial decision support tool. <em>EJOR</em>, <em>322</em>(2), 665-678. (<a href='https://doi.org/10.1016/j.ejor.2024.11.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the growing recognition of the vital role played by streets as public spaces in enhancing the vibrancy of urban life, various concepts aiming at creating greener and more inclusive streets have gained popularity in recent years, especially in North America. Shared streets are one example of such concepts that have attracted the attention of citizens and of urban and transportation planning professionals alike. This was the case in the city of Sherbrooke (Quebec, Canada) where, in response to numerous citizens’ requests, a need was identified to develop decision aid tools to help evaluate and rank street segments based on their potential to become shared streets. To achieve this, an action-research project was initiated in which we conducted a socio-technical process based on MACBETH, a multicriteria evaluation method. The project led to the development of a spatial decision support tool, operationally used today by the city professionals. This tool ensures a more informed and transparent decision-making process and supports shared streets planning policy. The methods developed are generalizable and can be adapted to other cities facing similar planning problems.},
  archive      = {J_EJOR},
  author       = {Alexandre Cailhier and Irène Abi-Zeid and Roxane Lavoie and Francis Marleau-Donais and Jérôme Cerutti},
  doi          = {10.1016/j.ejor.2024.11.012},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {665-678},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Where to plan shared streets: Development and application of a multicriteria spatial decision support tool},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inherently interpretable machine learning for credit scoring: Optimal classification tree with hyperplane splits. <em>EJOR</em>, <em>322</em>(2), 647-664. (<a href='https://doi.org/10.1016/j.ejor.2024.10.046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate and interpretable credit scoring model plays a crucial role in helping financial institutions reduce losses by promptly detecting, containing, and preventing defaulters. However, existing models often face a trade-off between interpretability and predictive accuracy. Traditional models like Logistic Regression (LR) offer high interpretability but may have limited predictive performance, while more complex models may improve accuracy at the expense of interpretability. In this paper, we tackle the credit scoring problem with imbalanced data by proposing two new classification models based on the optimal classification tree with hyperplane splits (OCT-H). OCT-H provides transparency and easy interpretation with ‘if-then’ decision tree rules. The first model, the cost-sensitive optimal classification tree with hyperplane splits (CSOCT-H). The second model, the optimal classification tree with hyperplane splits based on maximizing F1-Score (OCT-H-F1), aims to directly maximize the F1-score. To enhance model scalability, we introduce a data sample reduction method using data binning and feature selection. We then propose two solution methods: a heuristic approach and a method utilizing warm-start techniques to accelerate the solving process. We evaluated the proposed models on four public datasets. The results show that OCT-H significantly outperforms traditional interpretable models, such as Decision Trees (DT) and Logistic Regression (LR), in both predictive performance and interpretability. On certain datasets, OCT-H performs as well as or better than advanced ensemble tree models, effectively narrowing the gap between interpretable models and black-box models.},
  archive      = {J_EJOR},
  author       = {Jiancheng Tu and Zhibin Wu},
  doi          = {10.1016/j.ejor.2024.10.046},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {647-664},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Inherently interpretable machine learning for credit scoring: Optimal classification tree with hyperplane splits},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hedging political risk in international portfolios. <em>EJOR</em>, <em>322</em>(2), 629-646. (<a href='https://doi.org/10.1016/j.ejor.2024.10.017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that internationally diversified portfolios carry sizeable political risk premia and expose investors to tail risk. We obtain political efficient frontiers with and without hedging political risk using a portfolio selection model for skewed distributions and develop a new asymptotic inference test to compare portfolio performance. Politically hedged portfolios outperform a broad market index and the equally weighted portfolio for US, Eurozone, and Japanese investors. Political risk hedging is not subsumed by currency hedging, and the diversification gains of politically hedged portfolios persist under currency hedging and transaction cost frictions. Hedging political risk induces equity home bias but does not fully explain the puzzle.},
  archive      = {J_EJOR},
  author       = {Somayyeh Lotfi and Giovanni Pagliardi and Efstathios Paparoditis and Stavros A. Zenios},
  doi          = {10.1016/j.ejor.2024.10.017},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {629-646},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Hedging political risk in international portfolios},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal ranking model of fuzzy preference relations with self-confidence for addressing self-confidence failure. <em>EJOR</em>, <em>322</em>(2), 615-628. (<a href='https://doi.org/10.1016/j.ejor.2024.11.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy preference relation with self-confidence (FPR-SC) uses semantic self-confidence to illustrate the hesitation of experts in affirming given preference values. However, extant ranking derivation methods of FPRs-SC suffer from self-confidence failure problem. Specifically, when the logical operations of self-confidence levels are replaced by algebraic operations on semantic subscripts, the derived rankings may be unstable or independent of the self-confidence. To address this issue, an optimal ranking model of FPR-SC with chance-constrained is proposed and extended to group decision-making. Based on the concept of ‘reliability level’ in parameter estimation, the evaluation information expressed by ‘preference values + self-confidence levels’ is first explained using probability distributions to achieve dimensional unity between qualitative self-confidence and quantitative preference. A multiplicative consistency-driven optimal model is then designed to assess the individuality of self-confidence. Guided by the ‘3 σ ’ principle, FPR-SC is further replaced by random variables following asymmetric bilateral truncated normal distributions. This transformation captures the inner cognition of individuals during subjective judgment, and ensures effective constraints on the numerical range through the asymmetric design. Finally, motivated by the minimization of information deviation, an FPR-SC optimal ranking model with chance-constrained is constructed, and its effectiveness is verified.},
  archive      = {J_EJOR},
  author       = {Chonghui Zhang and Dandan Luo and Weihua Su and Lev Benjamin},
  doi          = {10.1016/j.ejor.2024.11.011},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {615-628},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal ranking model of fuzzy preference relations with self-confidence for addressing self-confidence failure},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-objective ranking and selection using stochastic kriging. <em>EJOR</em>, <em>322</em>(2), 599-614. (<a href='https://doi.org/10.1016/j.ejor.2024.11.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider bi-objective ranking and selection problems, where the goal is to correctly identify the Pareto-optimal solutions among a finite set of candidates for which the objective function values have to be estimated from noisy evaluations. When identifying these solutions, the noise perturbing the observed performance may lead to two types of errors: solutions that are truly Pareto-optimal may appear to be dominated, and solutions that are truly dominated may appear to be Pareto-optimal. We propose a novel Bayesian bi-objective ranking and selection method that sequentially allocates extra samples to competitive solutions, in view of reducing the misclassification errors when identifying the solutions with the best expected performance. The approach uses stochastic kriging to build reliable predictive distributions of the objectives, and exploits this information to decide how to resample. The experiments are designed to evaluate the algorithm on several artificial and practical test problems. The proposed approach is observed to consistently outperform its competitors (a well-known state-of-the-art algorithm and the standard equal allocation method), which may also benefit from the use of stochastic kriging information.},
  archive      = {J_EJOR},
  author       = {Sebastian Rojas Gonzalez and Juergen Branke and Inneke Van Nieuwenhuyse},
  doi          = {10.1016/j.ejor.2024.11.008},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {599-614},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Bi-objective ranking and selection using stochastic kriging},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel sigma-mu multiple criteria decision aiding approach for mutual funds portfolio selection. <em>EJOR</em>, <em>322</em>(2), 589-598. (<a href='https://doi.org/10.1016/j.ejor.2024.11.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Sigma-Mu approach is proposed for mutual funds portfolio selection. The mean and variance of the overall performance of each asset are considered, according to an additive aggregation model, subject to weights’ preferences provided by the decision maker. These preferences concern two independent sets of weights, i.e., those pertaining to the investment indicators and those pertaining to the time periods associated with the estimation of the indicators. For the first time in the Sigma-Mu framework, a weighting matrix is exploited, assisting on the development of a method to appraise the sources of variance, due to the weighting scheme of either the indicators or the periods. The Mu's, Sigma's and covariances estimated according to the Sigma-Mu approach, enter as inputs to mixed-integer quadratic programming (MIQP) mean-variance portfolio optimization models, in order to implement an empirical testing procedure, for a period of 8 years. The underlying MIQP models are equipped to consider non-convex investment policy constraints, such as the number of securities to be included in the portfolio, specific binary buy-in thresholds, the desired exposure of the portfolio to each investment advisor etc. The dataset that has been chosen for the empirical testing includes European mutual funds, that offer a broad exposure to the whole span of investment strategies and styles. The results document that the suggested approach may effectively be utilized in mutual funds investment management, since the portfolios constructed by the suggested methodology are associated with superior absolute and risk-adjusted performance against benchmarks.},
  archive      = {J_EJOR},
  author       = {Luís C. Dias and Panos Xidonas and Aristeidis Samitas},
  doi          = {10.1016/j.ejor.2024.11.003},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {589-598},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A novel sigma-mu multiple criteria decision aiding approach for mutual funds portfolio selection},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Capacity planning of renewable energy systems using stochastic dual dynamic programming. <em>EJOR</em>, <em>322</em>(2), 573-588. (<a href='https://doi.org/10.1016/j.ejor.2024.12.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a capacity expansion model for deciding the new electricity generation and transmission capacity to complement an existing hydroelectric reservoir system. The objective is to meet a forecast demand at least expected cost, namely the capital cost of the investment plus the expected discounted operating cost of the system. The optimal operating policy for any level of capacity investment can be computed using stochastic dual dynamic programming. We show how to combine a multistage stochastic operational model of the hydro system with a capacity expansion model to create a single model that can be solved by existing open-source solvers for multistage stochastic programs without the need for customized decomposition algorithms. We illustrate our method by applying it to a model of the New Zealand electricity system and comparing the solutions obtained with those found in a previous study.},
  archive      = {J_EJOR},
  author       = {J. Hole and A.B. Philpott and O. Dowson},
  doi          = {10.1016/j.ejor.2024.12.031},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {573-588},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Capacity planning of renewable energy systems using stochastic dual dynamic programming},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential quantile-based sensitivity in discontinuous models. <em>EJOR</em>, <em>322</em>(2), 554-572. (<a href='https://doi.org/10.1016/j.ejor.2024.12.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential sensitivity measures provide valuable tools for interpreting complex computational models, as used in applications ranging from simulation to algorithmic prediction. Taking the derivative of the model output in direction of a model parameter can reveal input–output relations and the relative importance of model parameters and input variables. Nonetheless, it is unclear how such derivatives should be taken when the model function has discontinuities and/or input variables are discrete. We present a general framework for addressing such problems, considering derivatives of quantile-based output risk measures, with respect to distortions to random input variables (risk factors), which impact the model output through step-functions. We prove that, subject to weak technical conditions, the derivatives are well-defined and we derive the corresponding formulas. We apply our results to the sensitivity analysis of compound risk models and to a numerical study of reinsurance credit risk in a multi-line insurance portfolio.},
  archive      = {J_EJOR},
  author       = {Silvana M. Pesenti and Pietro Millossovich and Andreas Tsanakas},
  doi          = {10.1016/j.ejor.2024.12.008},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {554-572},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Differential quantile-based sensitivity in discontinuous models},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic approach for price optimization problems with decision-dependent uncertainty. <em>EJOR</em>, <em>322</em>(2), 541-553. (<a href='https://doi.org/10.1016/j.ejor.2024.12.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Price determination is a central research topic of revenue management in marketing. The important aspect in pricing is controlling the stochastic behavior of demand, and the previous studies have tackled price optimization problems with uncertainties. However, many of those studies assumed that uncertainties are independent of decision variables (i.e., prices) and did not consider situations where demand uncertainty depends on price. Although some price optimization studies have dealt with decision-dependent uncertainty, they make application-specific assumptions in order to obtain optimal solutions. To handle a wider range of applications with decision-dependent uncertainty, we propose a general non-convex stochastic optimization formulation. This approach aims to maximize the expectation of a revenue function with respect to a random variable representing demand under a decision-dependent distribution. We derived an unbiased stochastic gradient estimator by using a well-tuned variance reduction parameter and used it for a projected stochastic gradient descent method to find a stationary point of our problem. We conducted synthetic experiments and simulation experiments with real data on a retail service application. The results show that the proposed method outputs solutions with higher total revenues than baselines.},
  archive      = {J_EJOR},
  author       = {Yuya Hikima and Akiko Takeda},
  doi          = {10.1016/j.ejor.2024.12.023},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {541-553},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Stochastic approach for price optimization problems with decision-dependent uncertainty},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of frequency and magnitude of natural disasters on inventory prepositioning. <em>EJOR</em>, <em>322</em>(2), 511-540. (<a href='https://doi.org/10.1016/j.ejor.2024.10.038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural disasters have been adversely affecting human societies for many centuries. One effective strategy in preparation for a timely response to such disasters is inventory prepositioning. Holding the right amount of inventory is critical in minimizing the social and economic impact and cost. Scholars usually model such problems assuming common Probability Density Functions (PDFs) like Normal, Poisson, and Exponential to simplify calculations. Following Gumbel’s studies regarding “the nature of nature” and exploring the nature of extreme events, in this research, we address the following research questions: first, do the magnitude and timing of natural disasters follow specific PDFs? Second, how can unrealistic assumptions affect such disasters’ economic and social costs of such disasters? Third, how should researchers and practitioners correct their assumptions when modeling inventory prepositioning? To answer these questions, we design a semi-Markovian model for an (S,s) inventory system that considers the magnitude and the timing of disasters for general PDFs. The model is an inventory system that considers the magnitude and the timing of disasters for general PDFs. The model is analytically solved and tested with real data from 1996 to 2019 regarding typhoons in Florida and earthquakes in California. Our findings show that correct assumptions about the time between disasters are far more critical than the disaster’s magnitude regarding the resulting social and economic costs. In this respect, we can summarize our findings as follows: (1) if the maximum inventory level (S) depends only on the average demand, the impact of assumed PDFs is insignificant; (2) if S depends on both the average and standard deviation (STD) of demand, the impact of the employed PDFs is significant; and (3) the STD is the main factor influenced by the type of the PDF.},
  archive      = {J_EJOR},
  author       = {Keyvan Fardi and Fatemeh Ghasemzadeh and Reza Zanjirani Farahani and Nasrin Asgari and Benjamin Laker and Rubén Ruiz},
  doi          = {10.1016/j.ejor.2024.10.038},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {511-540},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The impact of frequency and magnitude of natural disasters on inventory prepositioning},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cap-and-trade under a dual-channel setting in the presence of information asymmetry. <em>EJOR</em>, <em>322</em>(2), 500-510. (<a href='https://doi.org/10.1016/j.ejor.2024.11.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cap-and-trade, a widely used carbon regulation policy, encourages firms to adopt carbon abatement technologies to reduce emissions. Traditional supply-chain literature on this policy assumes symmetrical information, overlooking the fact that carbon abatement efforts and costs are often private and vary significantly across geographies, industries, and pollutants. In this paper we explore a dual-channel setting involving a manufacturer and a retailer, where the manufacturer, subject to cap-and-trade regulations, has undisclosed information about its carbon abatement costs. Our findings reveal that high abatement costs can paradoxically benefit the manufacturer, the environment, consumers, and overall social welfare. Our result also cautions that a higher carbon trading price (e.g., due to more ambitious emission reduction targets) can disincentivize the manufacturer from investing in carbon abatement. Moreover, a higher production cost, while resulting in lower market output, can increase pollution generation. We contribute the following to the practitioner debate about the impact of carbon policies: for an industry with a large market size, our findings lend support to governments to implement a cap-and-trade policy, because the manufacturer, customers and social welfare can be better off under a cap-and-trade policy than under a tax policy or no carbon policy. Additionally, we suggest that in such industries, governments need not enforce information transparency within the supply chain.},
  archive      = {J_EJOR},
  author       = {Hubert Pun and Salar Ghamat},
  doi          = {10.1016/j.ejor.2024.11.014},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {500-510},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Cap-and-trade under a dual-channel setting in the presence of information asymmetry},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-echelon multi-trip vehicle routing problem with synchronization for an integrated water- and land-based transportation system. <em>EJOR</em>, <em>322</em>(2), 480-499. (<a href='https://doi.org/10.1016/j.ejor.2024.10.047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on two-echelon synchronized logistics problems in the context of integrated water- and land-based transportation (IWLT) systems. The aim is to meet the increasing demand in city logistics as a result of the growth in transport activities, including parcel delivery, food delivery, and waste collection. We propose two models, a novel mixed integer linear joint model, and a logic-based Benders’ decomposition (LBBD) model, for a two-echelon problem under realistic settings such as multi-trips, time windows, and synchronization at the satellites with no storage and limited resource capacities. The objective is to optimize transfers and satellite assignments, thereby reducing overall logistics costs for street vehicles and vessels. Computational experiments demonstrate that the LBBD model is more robust in terms of solution quality and solution time on average while the added value of the LBBD is more evident when solving large-scale instances with 100 customers, reducing the overall costs by 10.6% on average and significantly reducing the fleet costs on both networks. Furthermore, we assess the effect of changing cost parameters and satellite locations in the proposed IWLT system–analyzing system behavior and suggesting potential improvements–and evaluate several system alternatives in city logistics–consisting of different transportation network designs (single- and two-echelon), vehicle types, and operational constraints. On average, the proposed two-echelon IWLT system reduces the number of kilometers traveled by vehicles at street level by ranging from 20% to 30% compared to a typical single-echelon service design that relies solely on trucks.},
  archive      = {J_EJOR},
  author       = {Cigdem Karademir and Breno A. Beirigo and Bilge Atasoy},
  doi          = {10.1016/j.ejor.2024.10.047},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {480-499},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A two-echelon multi-trip vehicle routing problem with synchronization for an integrated water- and land-based transportation system},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-machine preemptive scheduling with assignable due dates or assignable weights to minimize total weighted late work. <em>EJOR</em>, <em>322</em>(2), 467-479. (<a href='https://doi.org/10.1016/j.ejor.2024.11.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study single-machine preemptive scheduling to minimize the total weighted late work with assignable due dates or assignable weights. For the problem with assignable due dates, we show that it is binary N P -hard, solvable in pseudo-polynomial time, and solvable in polynomial time when all the jobs have agreeable processing times and weights. For the problem with assignable weights, we show that it is solvable in polynomial time. For the problem with assignable due dates and assignable weights, we show that it is binary N P -hard, solvable in pseudo-polynomial time, and solvable in polynomial time when all the jobs have the same processing times.},
  archive      = {J_EJOR},
  author       = {Rubing Chen and Xinyu Dong and Jinjiang Yuan and C.T. Ng and T.C.E. Cheng},
  doi          = {10.1016/j.ejor.2024.11.010},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {467-479},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Single-machine preemptive scheduling with assignable due dates or assignable weights to minimize total weighted late work},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dedicated branch-price-and-cut algorithm for advance patient planning and surgeon scheduling. <em>EJOR</em>, <em>322</em>(2), 448-466. (<a href='https://doi.org/10.1016/j.ejor.2024.10.042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the patient planning and surgeon scheduling in the operating room theatre. The problem considers the simultaneous planning of patients and the assignment of time blocks to surgeons so that they can perform the surgery of their patients. The timing and length of the allotted time blocks depend on the patient characteristics on the surgeons’ waiting lists. Solving this problem in an exact manner is challenging due to the large number of rooms, surgeons, and patients involved. To overcome this challenge, we propose an efficient branch-price-and-cut algorithm to find an optimal solution in an acceptable time span. For that purpose, we include different dedicated mechanisms to accelerate the solution-finding process. In this regard, the branch-price-and-cut tree is set up using an intelligent branching scheme, the nodes are searched in order of the lowest number of fractional variables, and improved bounds are computed to prune nodes earlier. To tighten the convex hull of the linear programming relaxation in each node, the algorithm relies on a row generation mechanism for adding valid inequalities. We conducted various computational experiments to demonstrate the performance of our algorithm and validate for each component the contribution of the implemented optimisation principles. Additionally, we show the superior performance of the proposed algorithm to alternative optimisation procedures.},
  archive      = {J_EJOR},
  author       = {Babak Akbarzadeh and Broos Maenhout},
  doi          = {10.1016/j.ejor.2024.10.042},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {448-466},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A dedicated branch-price-and-cut algorithm for advance patient planning and surgeon scheduling},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new branch-and-cut approach for integrated planning in additive manufacturing. <em>EJOR</em>, <em>322</em>(2), 427-447. (<a href='https://doi.org/10.1016/j.ejor.2024.10.040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been considerable interest in the transformative potential of additive manufacturing (AM) since it allows for producing highly customizable and complex components while reducing lead times and costs. The rise of AM for traditional and new business models enforces the need for efficient planning procedures for AM facilities. In this area, the assignment and sequencing of components to be built by an AM machine, also called a 3D printer, is a complex challenge combining two combinatorial problems: The first decision involves the grouping of parts into production batches, akin to the well-known bin packing problem. Subsequently, the second problem pertains to the scheduling of these batches onto the available machines, which corresponds to a parallel machine scheduling problem. For minimizing makespan, this paper proposes a new branch-and-cut algorithm for integrated planning for unrelated parallel machines. The algorithm is based on combinatorial Benders decomposition: The scheduling problem is considered in the master problem, while the feasibility of an obtained solution with respect to the packing problem is checked in the sub-problem. Current state-of-the-art techniques are extended to solve the orthogonal packing with rotation and used to speed up the solution of the sub-problem. Extensive computational tests on existing and new benchmark instances show the algorithm’s superior performance, improving the makespan by 18.7% on average, with improvements reaching up to 97.6% for large problems compared to an existing integrated mixed-integer programming model.},
  archive      = {J_EJOR},
  author       = {Benedikt Zipfel and Felix Tamke and Leopold Kuttner},
  doi          = {10.1016/j.ejor.2024.10.040},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {427-447},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A new branch-and-cut approach for integrated planning in additive manufacturing},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecast accuracy and inventory performance: Insights on their relationship from the m5 competition data. <em>EJOR</em>, <em>322</em>(2), 414-426. (<a href='https://doi.org/10.1016/j.ejor.2024.12.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although it is generally accepted that more accurate forecasts contribute towards better inventory performance, this relationship may often be weak, also depending on the structural characteristics of the products being forecast, the inventory policy considered, and the underlying expenses, among others. To empirically explore the connection between forecast accuracy and key costs associated with inventory control, namely holding, ordering, and lost sales costs, we consider the data set of the M5 competition and conduct detailed simulations using popular methods to generate quantile forecasts. Our results are analyzed for various setups of the order-up-to policy and for series of different demand patterns. We find that forecast accuracy is more relevant when holding cost is similar or larger than that associated with lost sales. Therefore, in applications where the latter cost exceeds the former, the preferable forecasting method may not be the most accurate one, especially for relatively short review periods and lead times, as well as products characterized by intermittency. Based on our results we discuss some practical concerns for decision making.},
  archive      = {J_EJOR},
  author       = {Evangelos Theodorou and Evangelos Spiliotis and Vassilios Assimakopoulos},
  doi          = {10.1016/j.ejor.2024.12.033},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {414-426},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Forecast accuracy and inventory performance: Insights on their relationship from the m5 competition data},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Degree reduction techniques for polynomial optimization problems. <em>EJOR</em>, <em>322</em>(2), 401-413. (<a href='https://doi.org/10.1016/j.ejor.2024.12.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new approach to quadrify a polynomial programming problem, i.e. reduce the polynomial program to a quadratic program, before solving it. The proposed approach, QUAD-RLT, exploits the Reformulation-Linearization Technique (RLT) structure to obtain smaller relaxations that can be solved faster and still provide high quality bounds. QUAD-RLT is compared to other quadrification techniques that have been previously discussed in the literature. The paper presents theoretical as well as computational results showing the advantage of QUAD-RLT compared to other quadrification techniques. Furthermore, rather than quadrifying a polynomial program, QUAD-RLT is generalized to reduce the degree of the polynomial to any degree. Computational results show that reducing the degree of the polynomial to a degree that is higher than two provides computational advantages in certain cases compared to fully quadrifying the problem. Finally, QUAD-RLT along with other quadrification/degree reduction schemes are implemented and made available in the freely available software RAPOSa.},
  archive      = {J_EJOR},
  author       = {Brais González-Rodríguez and Joe Naoum-Sawaya},
  doi          = {10.1016/j.ejor.2024.12.021},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {401-413},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Degree reduction techniques for polynomial optimization problems},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benders decomposition for bi-objective linear programs. <em>EJOR</em>, <em>322</em>(2), 376-400. (<a href='https://doi.org/10.1016/j.ejor.2024.09.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a new decomposition technique for solving bi-objective linear programming problems. The proposed methodology combines the bi-objective simplex algorithm with Benders decomposition and can be used to obtain a complete set of efficient extreme solutions, and the corresponding set of extreme non-dominated points, for a bi-objective linear programme. Using a Benders-like reformulation, the decomposition approach decouples the problem into a bi-objective master problem and a bi-objective subproblem, each of which is solved using the bi-objective parametric simplex algorithm. The master problem provides candidate efficient solutions that the subproblem assesses for feasibility and optimality. As in standard Benders decomposition, optimality and feasibility cuts are generated by the subproblem and guide the master problem solve. This paper discusses bi-objective Benders decomposition from a theoretical perspective, proves the correctness of the proposed reformulation and addresses the need for so-called weighted optimality cuts. Furthermore, we present an algorithm to solve the reformulation and discuss its performance for three types of bi-objective optimisation problems.},
  archive      = {J_EJOR},
  author       = {Andrea Raith and Richard Lusby and Ali Akbar Sohrabi Yousefkhan},
  doi          = {10.1016/j.ejor.2024.09.004},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {376-400},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Benders decomposition for bi-objective linear programs},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review and ranking of operators in adaptive large neighborhood search for vehicle routing problems. <em>EJOR</em>, <em>322</em>(2), 357-375. (<a href='https://doi.org/10.1016/j.ejor.2024.05.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article systematically reviews the literature on adaptive large neighborhood search (ALNS) to gain insights into the operators used for vehicle routing problems (VRPs) and their effectiveness. The ALNS has been successfully applied to a variety of optimization problems, particularly variants of the VRP. The ALNS gradually improves an initial solution by modifying it using removal and insertion operators. However, relying solely on adaptive operator selection is not advisable. Instead, authors often conduct experiments to identify operators that improve the solution quality or remove detrimental ones. This process is mostly cumbersome due to the wide variety of operators, further complicated by inconsistent nomenclature. The objectives of this review are threefold: First, to classify ALNS operators using a unified terminology; second, to analyze their performance; and third, to present guidelines for the development and analysis of ALNS algorithms in the future based on the outcomes of the performance evaluation. In this review, we conduct a network meta-analysis of 211 articles published between 2006 and 2023 that have applied ALNS algorithms in the context of VRPs. We employ incomplete pairwise comparison matrices, similar to rankings used in sports, to rank the operators. We identify 57 distinct removal and 42 insertion operators, and the analysis ranks them based on their effectiveness. Sequence-based removal operators, which remove sequences of customers in the current solution, are found to be the most effective. The best-performing insertion operators are those that exhibit foresight, such as regret insertion operators. Finally, guidelines and possible future research directions are discussed.},
  archive      = {J_EJOR},
  author       = {Stefan Voigt},
  doi          = {10.1016/j.ejor.2024.05.033},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {2},
  pages        = {357-375},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A review and ranking of operators in adaptive large neighborhood search for vehicle routing problems},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrigendum to “Two-stage no-wait proportionate flow shop scheduling with minimal service time variation and optional job rejection” [European journal of operational research, 305, 608-616]. <em>EJOR</em>, <em>322</em>(1), 355-356. (<a href='https://doi.org/10.1016/j.ejor.2024.12.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EJOR},
  author       = {Christos Koulamas and George J. Kyparisis and S.S. Panwalkar},
  doi          = {10.1016/j.ejor.2024.12.003},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {355-356},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Corrigendum to “Two-stage no-wait proportionate flow shop scheduling with minimal service time variation and optional job rejection” [European journal of operational research, 305, 608-616]},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A revisit of the optimal excess-of-loss contract. <em>EJOR</em>, <em>322</em>(1), 341-354. (<a href='https://doi.org/10.1016/j.ejor.2024.11.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well-known that Excess-of-Loss reinsurance has more marketability than Stop-Loss reinsurance, though Stop-Loss reinsurance is the most prominent setting discussed in the optimal (re)insurance design literature. We point out that optimal reinsurance policy under Stop-Loss leads to a zero insolvency probability, which motivates our paper. We remedy this peculiar property of the optimal Stop-Loss reinsurance contract by investigating the optimal Excess-of-Loss reinsurance contract instead. We also provide estimators for the optimal Excess-of-Loss and Stop-Loss contracts and investigate their statistical properties under many premium principle assumptions and various risk preferences, which, according to our knowledge, have never been investigated in the literature. Simulated data and real-life data are used to illustrate our main theoretical findings.},
  archive      = {J_EJOR},
  author       = {Ernest Aboagye and Vali Asimit and Tsz Chai Fung and Liang Peng and Qiuqi Wang},
  doi          = {10.1016/j.ejor.2024.11.027},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {341-354},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A revisit of the optimal excess-of-loss contract},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic growth-optimal portfolio choice under risk control. <em>EJOR</em>, <em>322</em>(1), 325-340. (<a href='https://doi.org/10.1016/j.ejor.2024.10.043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a mean-risk portfolio choice problem for log-returns in a continuous-time, complete market. It is a growth-optimal portfolio choice problem under risk control. The risk of log-returns is measured by weighted Value-at-Risk (WVaR), which is a generalization of Value-at-Risk (VaR) and Expected Shortfall (ES). We characterize the optimal terminal wealth and obtain analytical expressions when risk is measured by VaR or ES. We demonstrate that using VaR increases losses while ES reduces losses during market downturns. Moreover, the efficient frontier is a concave curve that connects the minimum-risk portfolio with the growth optimal portfolio, as opposed to the vertical line when WVaR is used on terminal wealth, and thus allows for a meaningful characterization of the risk-return trade-off and aids investors in setting reasonable investment targets. We also apply our model to benchmarking and illustrate how investors with benchmarking may overperform/underperform the market depending on economic conditions.},
  archive      = {J_EJOR},
  author       = {Pengyu Wei and Zuo Quan Xu},
  doi          = {10.1016/j.ejor.2024.10.043},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {325-340},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Dynamic growth-optimal portfolio choice under risk control},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general valuation framework for rough stochastic local volatility models and applications. <em>EJOR</em>, <em>322</em>(1), 307-324. (<a href='https://doi.org/10.1016/j.ejor.2024.11.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rough volatility models are a new class of stochastic volatility models that have been shown to provide a consistently good fit to implied volatility smiles of SPX options. They are continuous-time stochastic volatility models, whose volatility process is driven by a fractional Brownian motion with the corresponding Hurst parameter less than a half. Albeit the empirical success, the valuation of derivative securities under rough volatility models is challenging. The reason is that it is neither a semi-martingale nor a Markov process. This paper proposes a novel valuation framework for rough stochastic local volatility (RSLV) models. In particular, we introduce the perturbed stochastic local volatility (PSLV) model as the semi-martingale approximation for the RSLV model and establish its existence, uniqueness, Markovian representation and convergence. Then we propose a fast continuous-time Markov chain (CTMC) approximation algorithm to the PSLV model and establish its convergence. Numerical experiments demonstrate the convergence of our approximation method to the true prices, and also the remarkable accuracy and efficiency of the method in pricing European, barrier and American options. Comparing with existing literature, a significant reduction in the CPU time to arrive at the same level of accuracy is observed.},
  archive      = {J_EJOR},
  author       = {Wensheng Yang and Jingtang Ma and Zhenyu Cui},
  doi          = {10.1016/j.ejor.2024.11.002},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {307-324},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A general valuation framework for rough stochastic local volatility models and applications},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An analysis of information disclosure in build–operate–transfer road projects. <em>EJOR</em>, <em>322</em>(1), 292-306. (<a href='https://doi.org/10.1016/j.ejor.2024.10.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the Build–Operate–Transfer (BOT) approach, private firms are involved in delivering public road projects. Before launching a BOT road project, the government typically collects demand information by commissioning a demand forecast report, which serves as its private signal. This paper studies how the government should disclose this private information to the firm. We first show that the firm’s optimal road capacity and toll price generate a convex social welfare function with respect to the firm’s belief over demand distribution. As a result, the government either adopts a full-disclosure policy or a partial-disclosure policy, under the former of which the government sends signals that are perfectly correlated with true demand states, while under the latter, the government adopts a mixed strategy in formulating its signals. We show that the government is inclined towards full disclosure if it faces a sufficiently high demand level and towards partial disclosure otherwise. Considering the importance of the lump-sum subsidy in attracting firm participation under demand uncertainty, we have provided conditions under which it is effective in affecting the government’s information disclosure strategies. Furthermore, we have studied the value of information disclosure and found that it is generally first increasing and then decreasing with the firm’s prior belief. Moreover, it is found that government subsidies can improve the value of information disclosure if and only if the subsidy cost is sufficiently small.},
  archive      = {J_EJOR},
  author       = {Zhuo Feng and Ying Gao and Jinbo Song and Qiaochu He},
  doi          = {10.1016/j.ejor.2024.10.032},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {292-306},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An analysis of information disclosure in build–operate–transfer road projects},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The truck traveling salesman problem with drone and boat for humanitarian relief distribution in flood disaster: Mathematical model and solution methods. <em>EJOR</em>, <em>322</em>(1), 270-291. (<a href='https://doi.org/10.1016/j.ejor.2024.10.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an optimization model to distribute logistical items from a warehouse to shelters in the case of humanitarian flood disaster relief. The model utilizes three transportation modes, namely, a truck, a drone, and an inflatable boat. We refer to this problem as the Traveling Salesman Problem with Drone and Boat (TSP-DB). The truck acts as a mothership vehicle, carrying a drone and a boat. Shelters in the dry area can be served by truck and drone, while those in the flooded area can be accessed by boat and drone. The drone and boat are deployed from the truck to deliver items to shelters. Due to the limited capacity and the high relative demand at a shelter, the drone can only make one visit at a time before returning to the truck, while the boat can perform multiple visits in a single trip. The objective is to minimize the completion time. The proposed problem is first modeled using mixed integer linear programming. As the problem is hard to solve exactly, especially for relatively larger instances, an effective matheuristic that combines an exact method and the metaheuristic record-to-record travel algorithm is then proposed. The performance of the proposed approach is assessed using generated and benchmark instances. The results reveal that our method is robust and competitive when compared against existing state-of-the-art methods on related traveling salesman problems with drones. The proposed method is also applied to a real case study in Jakarta, Indonesia, where interesting and valuable managerial insights are discussed and analyzed.},
  archive      = {J_EJOR},
  author       = {Fadillah Ramadhan and Chandra Ade Irawan and Said Salhi and Zhao Cai},
  doi          = {10.1016/j.ejor.2024.10.022},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {270-291},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The truck traveling salesman problem with drone and boat for humanitarian relief distribution in flood disaster: Mathematical model and solution methods},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven inventory control for large product portfolios: A practical application of prescriptive analytics. <em>EJOR</em>, <em>322</em>(1), 254-269. (<a href='https://doi.org/10.1016/j.ejor.2024.10.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the real-world inventory management problem of a large network of pharmacies, this paper proposes and studies a practically relevant Prescriptive Analytics approach for data-driven dynamic inventory control of large portfolios of interrelated products. We extend existing research on weighted Sample Average Approximation by integrating a ‘global learning’ model that effectively exploits cross-learning opportunities within the product portfolio. The results of an extensive numerical evaluation on real-world data suggest that our approach outperforms relevant benchmarks—in particular, models that rely on ‘local learning’ strategies where weight functions are trained separately for each product. The numerical results also allow us to derive important practical and structural insights regarding the value of contextual information in our global learning framework.},
  archive      = {J_EJOR},
  author       = {Felix G. Schmidt and Richard Pibernik},
  doi          = {10.1016/j.ejor.2024.10.012},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {254-269},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Data-driven inventory control for large product portfolios: A practical application of prescriptive analytics},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel robust optimization model for nonlinear support vector machine. <em>EJOR</em>, <em>322</em>(1), 237-253. (<a href='https://doi.org/10.1016/j.ejor.2024.12.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present new optimization models for Support Vector Machine (SVM), with the aim of separating data points in two or more classes. The classification task is handled by means of nonlinear classifiers induced by kernel functions and consists in two consecutive phases: first, a classical SVM model is solved, followed by a linear search procedure, aimed at minimizing the total number of misclassified data points. To address the problem of data perturbations and protect the model against uncertainty, we construct bounded-by-norm uncertainty sets around each training data and apply robust optimization techniques. We rigorously derive the robust counterpart extension of the deterministic SVM approach, providing computationally tractable reformulations. Closed-form expressions for the bounds of the uncertainty sets in the feature space have been formulated for typically used kernel functions. Finally, extensive numerical results on real-world datasets show the benefits of the proposed robust approach in comparison with various SVM alternatives in the machine learning literature.},
  archive      = {J_EJOR},
  author       = {Francesca Maggioni and Andrea Spinelli},
  doi          = {10.1016/j.ejor.2024.12.014},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {237-253},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A novel robust optimization model for nonlinear support vector machine},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label feature selection considering label importance-weighted relevance and label-dependency redundancy. <em>EJOR</em>, <em>322</em>(1), 215-236. (<a href='https://doi.org/10.1016/j.ejor.2024.11.038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information theory has emerged as a prominent approach for analyzing feature relevance and redundancy in multi-label feature selection. However, traditional information theory-based methods encounter two primary issues. Firstly, when evaluating feature relevance, they fail to consider the differing importance of each label within the entire label set. Secondly, when assessing feature redundancy, they overlook the varying dependencies of the selected features on the labels. To address these issues, this paper proposes a novel multi-label feature selection method that considers label importance-weighted relevance and label-dependency redundancy. Specifically, we introduce the concept of label importance weight (LIW) to measure the significance of each label within the entire label set. Based on this LIW, we define a feature relevance term called label importance-weighted relevance (LIWR). Subsequently, we leverage the uncertainty coefficient to quantify the dependence of the selected features on the labels, treating it as a weight. Building upon this weight, we establish a feature redundancy term known as label-dependency redundancy (LDR). Finally, we formulate a feature evaluation criterion called LIWR-LDR by maximizing LIWR and minimizing LDR, accompanied by the presentation of a corresponding feature selection algorithm. Extensive experiments conducted on 25 multi-label datasets demonstrate the effectiveness of LIWR-LDR.},
  archive      = {J_EJOR},
  author       = {Xi-Ao Ma and Haibo Liu and Yi Liu and Justin Zuopeng Zhang},
  doi          = {10.1016/j.ejor.2024.11.038},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {215-236},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multi-label feature selection considering label importance-weighted relevance and label-dependency redundancy},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fused large language model for predicting startup success. <em>EJOR</em>, <em>322</em>(1), 198-214. (<a href='https://doi.org/10.1016/j.ejor.2024.09.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investors are continuously seeking profitable investment opportunities in startups and, hence, for effective decision-making, need to predict a startup’s probability of success. Nowadays, investors can use not only various fundamental information about a startup (e.g., the age of the startup, the number of founders, and the business sector) but also textual description of a startup’s innovation and business model, which is widely available through online venture capital (VC) platforms such as Crunchbase. To support the decision-making of investors, we develop a machine learning approach with the aim of locating successful startups on VC platforms. Specifically, we develop, train, and evaluate a tailored, fused large language model to predict startup success. Thereby, we assess to what extent self-descriptions on VC platforms are predictive of startup success. Using 20,172 online profiles from Crunchbase, we find that our fused large language model can predict startup success, with textual self-descriptions being responsible for a significant part of the predictive power. Our work provides a decision support tool for investors to find profitable investment opportunities.},
  archive      = {J_EJOR},
  author       = {Abdurahman Maarouf and Stefan Feuerriegel and Nicolas Pröllochs},
  doi          = {10.1016/j.ejor.2024.09.011},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {198-214},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A fused large language model for predicting startup success},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of many conflicting objectives on decision-makers’ cognitive burden and decision consistency. <em>EJOR</em>, <em>322</em>(1), 182-197. (<a href='https://doi.org/10.1016/j.ejor.2024.10.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical planning and decision-making problems are often better and more accurately formulated with multiple conflicting objectives rather than a single objective. This study investigates a situation relevant for Multiple Criteria Decision Making (MCDM) as well as Evolutionary Multi-objective Optimization (EMO), where the decision-maker needs to make a series of choices between nondominated options characterized by multiple objectives. The cognitive capacity of humans is limited, which leads to cognitive burden that influences human decision-makers’ decisions. We measure how the varying number of objectives influences cognitive burden in a laboratory study, and the impacts that this burden has on the decision-makers’ behavior and the consistency of their decisions. We use psychophysiological, behavioral, and self-report methods. Our results suggest that a higher number of objectives (i) increases cognitive burden significantly, (ii) leads to adopting strategies in which only a limited number of objectives is considered, and (iii) decreases decision consistency.},
  archive      = {J_EJOR},
  author       = {J. Matias Kivikangas and Eeva Vilkkumaa and Julian Blank and Ville Harjunen and Pekka Malo and Kalyanmoy Deb and Niklas J. Ravaja and Jyrki Wallenius},
  doi          = {10.1016/j.ejor.2024.10.039},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {182-197},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Effects of many conflicting objectives on decision-makers’ cognitive burden and decision consistency},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust min-max (regret) optimization using ordered weighted averaging. <em>EJOR</em>, <em>322</em>(1), 171-181. (<a href='https://doi.org/10.1016/j.ejor.2024.10.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In decision-making under uncertainty, several criteria have been studied to aggregate the performance of a solution over multiple possible scenarios. This paper introduces a novel variant of ordered weighted averaging (OWA) for optimization problems. It generalizes the classic OWA approach, which includes the robust min–max optimization as a special case, as well as the min–max regret optimization. We derive new complexity results for this setting, including insights into the inapproximability and approximability of this problem. In particular, we provide stronger positive approximation results that asymptotically improve the previously best-known bounds for the classic OWA approach.},
  archive      = {J_EJOR},
  author       = {Werner Baak and Marc Goerigk and Adam Kasperski and Paweł Zieliński},
  doi          = {10.1016/j.ejor.2024.10.028},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {171-181},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust min-max (regret) optimization using ordered weighted averaging},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficiency decomposition and frontier projection of two-stage network DEA under variable returns to scale. <em>EJOR</em>, <em>322</em>(1), 157-170. (<a href='https://doi.org/10.1016/j.ejor.2024.10.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency decomposition and frontier projection of traditional two-stage network data envelopment analysis (DEA) model under variable returns to scale (VRS) are often not equivalent; which not only contradicts DEA theory, but also reduces the scientificity of the model. The main reason for this inequivalence is that there is a synergistic effect of variable scale return in two different stages. Therefore, this paper describes the production frontier of two-stage DEA under VRS for analyzing this synergistic effect, and then the efficiency evaluation pitfalls of two-stage DEA under VRS are identified. From the input orientation, output orientation, non-orientation perspectives, different two-stage network DEA models under VRS are respectively constructed to solve these evaluation pitfalls, and the equivalence relationships of their multiplier model and envelopment model are proved; and then the efficiency decomposition and frontier projection with equivalence relationship can be obtained to meet the different needs of decision-makers. Furthermore, variable intermediate element is discussed in the non-orientation model for achieving the Pareto optimality of two stages during the process of efficiency decomposition and frontier projection. By these models, the theoretical foundation of two-stage network DEA under VRS has been further improved. Finally, two examples are provided to illustrate the effectiveness of the new models.},
  archive      = {J_EJOR},
  author       = {Lei Chen and Ying-Ming Wang},
  doi          = {10.1016/j.ejor.2024.10.011},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {157-170},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Efficiency decomposition and frontier projection of two-stage network DEA under variable returns to scale},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal computation budget allocation with gaussian process regression. <em>EJOR</em>, <em>322</em>(1), 147-156. (<a href='https://doi.org/10.1016/j.ejor.2024.11.049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Ranking and Selection (R&S) in the presence of spatial correlation among designs. The performance of each design can only be evaluated through stochastic simulation with heterogeneous noise. Our primary objective is to maximize the probability of correct selection (PCS) by optimally allocating the simulation budget considering the spatial correlation among designs. We propose using Gaussian process regression (GPR) to model the spatial correlation and develop a GPR-based optimal computing budget allocation (GPOCBA) framework to derive an asymptotically optimal allocation policy. Additionally, we analyze the impact of spatial correlation on allocation policy and quantify its benefits under specific cases. We also introduce a sequential implementation of GPOCBA and establish convergence results. Numerical experiments show that the proposed GPOCBA method significantly outperforms the widely used OCBA, demonstrating improved computational efficiency by considering spatial correlation in R&S problems.},
  archive      = {J_EJOR},
  author       = {Mingjie Hu and Jie Xu and Chun-Hung Chen and Jian-Qiang Hu},
  doi          = {10.1016/j.ejor.2024.11.049},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {147-156},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal computation budget allocation with gaussian process regression},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal capacity planning for cloud service providers with periodic, time-varying demand. <em>EJOR</em>, <em>322</em>(1), 133-146. (<a href='https://doi.org/10.1016/j.ejor.2024.11.017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Allocating capacity to private cloud computing services is challenging because demand is time-varying, there are often no buffers, and customers can re-submit jobs a finite number of times. We model this setting using a multi-station queueing network where servers represent CPU cores and jobs not immediately processed retry several times. Assuming retrial rates are stationary and that there is a maximum number of retrial attempts, we determine an optimal service capacity and retrial interval under an admission control policy employed by our partner institution — the server informs customers when they should next attempt service without enforcement. We introduce a recursive representation of the offered load which approximates the fluid dynamics of the system. We then use this representation to develop a solution technique that minimizes the total variation in the constructed offered load. We prove this approach is linked to maximizing system throughput and that in certain settings, the optimal stationary and time-varying retrial intervals are equivalent. Utilizing a data set of cloud computing requests spanning a 24-hour period, our analysis indicates that the optimal policy prescribes a 10% reduction in capacity. We also investigate the fidelity of the fluid model and the sensitivity of our recommendations to the behavior of retrial jobs. We find that retrial-time announcements allow a provider to satisfy service level agreements while encouraging retrial jobs to be processed during off-peak periods. Further, the policy is suitably robust to a customer’s willingness to comply with the suggested retrial times.},
  archive      = {J_EJOR},
  author       = {Eugene Furman and Adam Diamant},
  doi          = {10.1016/j.ejor.2024.11.017},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {133-146},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal capacity planning for cloud service providers with periodic, time-varying demand},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ranking and selection with two-stage decision. <em>EJOR</em>, <em>322</em>(1), 121-132. (<a href='https://doi.org/10.1016/j.ejor.2024.11.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranking & selection (R&S) is concerned with the selection of the best decision from a finite set of alternative decisions when the outcome of the decision has to be estimated using stochastic simulation. In this paper, we extend the R&S problem to a two-stage setting where after a first-stage decision has been made, some information may be observed and a second-stage decision then needs to be made based on the observed information to achieve the best outcome. We then extend two popular single-stage R&S algorithms, expected value of information (EVI) and optimal computing budget allocation (OCBA), to efficiently solve the new two-stage R&S problem. We prove the consistency of the new two-stage EVI (2S-EVI) and OCBA (2S-OCBA) algorithms. Experiment results on benchmark test problems and a two-stage multi-product assortment problem show that both algorithms outperform applying single-stage EVI and OCBA in the two-stage setting. Between 2S-EVI and 2S-OCBA, numerical results suggest that 2S-EVI tends to perform better with smaller number of decisions at first and second stage while 2S-OCBA has better performance for larger problems.},
  archive      = {J_EJOR},
  author       = {Tianxiang Wang and Jie Xu and Juergen Branke and Jian-Qiang Hu and Chun-Hung Chen},
  doi          = {10.1016/j.ejor.2024.11.005},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {121-132},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Ranking and selection with two-stage decision},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-phase matheuristic for assignment and truck loading problems. <em>EJOR</em>, <em>322</em>(1), 105-120. (<a href='https://doi.org/10.1016/j.ejor.2024.10.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel two-phase matheuristic for assignment and truck loading. The scope of our approach involves scenarios with an extensive amount of items requiring assignment to a heterogeneous fleet of trucks and subsequent transportation. A distinguishing feature of our matheuristic lies in the fact that the complex underlying problem is divided into subproblems which later are merged into a global solution. The proposed matheuristic tackles the assignment problem in its first phase, followed by a complex truck loading algorithm to validate the assignment solution in the second phase. This enables the identification of optimal solutions and adaptability to diverse use cases. This approach is motivated by the ROADEF/EURO challenge 2022 and is awarded with the scientific prize of the challenge. We demonstrate superior performance on selected instances, showcasing the potential for significant cost reduction in Renault’s supply chains.},
  archive      = {J_EJOR},
  author       = {Jakob Schulte and Daniel Wetzel},
  doi          = {10.1016/j.ejor.2024.10.020},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {105-120},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Two-phase matheuristic for assignment and truck loading problems},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retailer-manufacturer partnerships in E-commerce: Dual product strategy and market share dynamics. <em>EJOR</em>, <em>322</em>(1), 85-104. (<a href='https://doi.org/10.1016/j.ejor.2024.10.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new practice among online retail platforms, e.g., Amazon and Wayfair, is to offer their own private label product and a substitutable exclusive manufacturer product. We employ a game theoretic approach to examine conditions under which a retailer and a manufacturer find it optimal to enter into such a partnership. Our analysis reveals that a retailer finds it profitable to partner with a manufacturer with one of two profiles. The first is a manufacturer with low unit production and holding costs and large capacity, and the retailer gives the manufacturer most of the market share. In this case, the retailer uses a low private product price and marketing effort to pressure the manufacturer to set a low exclusive product price, which increases the manufacturer's revenue and the retailer's fees. The second is a manufacturer with a large consumer base, high unit production and holding costs, and small capacity. For this profile, the retailer takes most of the market share by offering a low private product price, and the manufacturer is unable to counteract the retailer's low private product price and marketing effort. The partnership results in a lower price when the marketing effort is costly and the retailer relies on private product price, intensifying price competition. Under some conditions, the retailer's profit may decrease in his share of the manufacturer's revenue. Also, the retailer may increase the marketing effort and decrease the private product price, not to take market share, but to pressure the manufacturer to decrease price.},
  archive      = {J_EJOR},
  author       = {Raziyeh Reza-Gharehbagh and Moutaz Khouja and Ramzi Hammami},
  doi          = {10.1016/j.ejor.2024.10.031},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {85-104},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Retailer-manufacturer partnerships in E-commerce: Dual product strategy and market share dynamics},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic pickup-and-delivery for collaborative platforms with time-dependent travel and crowdshipping. <em>EJOR</em>, <em>322</em>(1), 70-84. (<a href='https://doi.org/10.1016/j.ejor.2024.09.048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a pickup-and-delivery problem that arises when customers randomly submit requests over the course of a day from a choice of vendors on a collaborative e-commerce portal. Based on the attributes of a customer request, a dispatcher dynamically schedules the delivery service on either a dedicated vehicle or a crowdshipper, both of whom experience time-dependent travel times. While dedicated vehicles are available throughout the day, the availability of crowdshippers is unknown a priori and they appear randomly for only portions of the day. With an objective of minimizing the sum of routing costs, piece-rate crowdshipper payments, and lateness charges, we model the uncertainty in request arrivals and crowdshipper appearances as a Markov decision process. To determine an action at each decision epoch, we employ a heuristic that partially destroys the existing routes and repairs them under the guidance of a parameterized cost function approximation that accounts for the remaining temporal capacity of delivery vehicles. We benchmark our real-time heuristic with an adaptive large neighborhood search and demonstrate the effectiveness of our method with several performance metrics. In addition, we conduct computational experiments to demonstrate the impact of inserting wait time in the route scheduling and the benefit of explicitly modeling time-dependent travel times. Through our computational testing, we also investigate the potential of demand management mechanisms that facilitate many-to-one request bundles or one-to-many request bundles to reduce the cost to service requests.},
  archive      = {J_EJOR},
  author       = {Sara Stoia and Demetrio Laganà and Jeffrey W. Ohlmann},
  doi          = {10.1016/j.ejor.2024.09.048},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {70-84},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Dynamic pickup-and-delivery for collaborative platforms with time-dependent travel and crowdshipping},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact solution methods for the resource constrained project scheduling problem with a flexible project structure. <em>EJOR</em>, <em>322</em>(1), 56-69. (<a href='https://doi.org/10.1016/j.ejor.2024.10.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Resource Constrained Project Scheduling Problem with a flexible Project Structure (RCPSP-PS) is a generalization of the Resource Constrained Project Scheduling Problem (RCPSP). In the RCPSP, the goal is to determine a minimal makespan schedule subject to precedence and resource constraints. The generalization introduced in the RCPSP-PS is that, instead of executing all activities, only a subset of all activities has to be executed. We present a model that is based on two graphs: one representing precedence relations and one representing the activity selection structure. The latter defines which subset of activities has to be executed. Additionally, we present theoretical properties of this model and give an exact solution method that makes use of these properties by generating cutting planes and setting bounds on variables. Furthermore, three problem properties are introduced to classify problems in the literature. We compare our model to a model from literature on instances that possess a subset of these three problem properties and find a reduction in computing time. Furthermore, by comparing results on instances that possess all problem properties, it is shown that the computing times are decreased and better lower bounds are found by the cutting planes and variable bounds presented in this paper.},
  archive      = {J_EJOR},
  author       = {T. van der Beek and J.T. van Essen and J. Pruyn and K. Aardal},
  doi          = {10.1016/j.ejor.2024.10.029},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {56-69},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Exact solution methods for the resource constrained project scheduling problem with a flexible project structure},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The distributed flow shop scheduling problem with inter-factory transportation. <em>EJOR</em>, <em>322</em>(1), 39-55. (<a href='https://doi.org/10.1016/j.ejor.2024.10.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large manufacturing companies often manage a network of multiple factories, creating a distributed flow shop scheduling problem for flowline manufacturing processes. This problem involves assigning jobs to one of several distributed factories, each equipped with identical flow shops, and completing the jobs within their designated factory. We expand upon the traditional distributed flow shop scheduling problem by incorporating the transportation of intermediate goods between factories. This transportation can occur after each machine, with shipping times dependent on the distance between the origin and destination factories, potentially delaying further processing. Our objective is to minimize the makespan across all factories. We introduce an iterated greedy search procedure specifically designed for distributed flow shop scheduling with inter-factory transportation. Based on a graph representation, a speed-up procedure is developed to improve the algorithms search ability. Through computational studies, we demonstrate the effectiveness and efficiency of our proposed algorithm, and provide insights into the benefits of transportation. Our findings indicate that incorporating transportation enhances machine utilization across factories by alleviating bottlenecks, ultimately improving overall efficiency and reducing makespan. Besides, we are able to quantify the trade-off between transport times and the benefits of inter-factory transportation.},
  archive      = {J_EJOR},
  author       = {Tristan Becker and Janis Neufeld and Udo Buscher},
  doi          = {10.1016/j.ejor.2024.10.026},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {39-55},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The distributed flow shop scheduling problem with inter-factory transportation},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic selection of the best performing control point approach for project control with resource constraints. <em>EJOR</em>, <em>322</em>(1), 15-38. (<a href='https://doi.org/10.1016/j.ejor.2024.10.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During project execution, the actual project progress shows deviations from the baseline schedule due to uncertainty. To complete the project timely, project monitoring is performed at discrete control points to identify project opportunities/problems and take possible corrective actions. These control points affect the quality of project monitoring and corrective actions, but little guidance is available on identifying situations where the control points pay off the most in terms of project duration. This paper proposes new control point approaches considering the risk, the complexity of the network, and subnetwork information to determine the timing of project monitoring and action taking. Moreover, new parameters are proposed to model more realistic project characteristics. Subsequently, a classification model is developed to select the best performing control point approach given project characteristics. An extensive computational experiment is conducted on a set of 3,810 artificial projects with diverse project characteristics to evaluate the performance of the classification model and further validate it on empirical project data. The computational results indicate that the classification model outperforms the average performance of any proposed control point approaches. The results also show that the resource variability that indicates the resource usage deviations between project activities is the primary driver for detecting the best control point approach for projects with resource constraints.},
  archive      = {J_EJOR},
  author       = {Jie Song and Jinbo Song and Mario Vanhoucke},
  doi          = {10.1016/j.ejor.2024.10.025},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {15-38},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Automatic selection of the best performing control point approach for project control with resource constraints},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on the traveling salesman problem and its variants in a warehousing context. <em>EJOR</em>, <em>322</em>(1), 1-14. (<a href='https://doi.org/10.1016/j.ejor.2024.04.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of e-commerce and its fast-delivery expectations, efficiently routing pickers in warehouses and distribution centers has received renewed interest. The processes and the resulting routing problems in this environment are diverse. For instance, not only human pickers have to be routed but also autonomous picking robots or mobile robots that accompany human pickers. Traditional picker routing, in which a single picker has to visit a given set of picking positions in a picker-to-parts process, can be modeled as the classical Traveling Salesman Problem (TSP). The more involved processes of e-commerce fulfillment, however, require solving more complex TSP variants, such as the clustered, generalized, or prize-collecting TSP. In this context, our paper provides two main contributions: We systematically survey the large number of TSP variants that are known in the routing literature and check whether meaningful applications in warehouses exist that correspond to the respective TSP variant. If they do, we survey the existing research and investigate the computational complexity of the TSP variant in the warehousing context. Previous research has shown that the classical TSP is efficiently solvable in the parallel-aisle structure of warehouses. Consequently, some TSP variants also turn out to be efficiently solvable in the warehousing context, whereas others remain NP -hard. We survey existing complexity results, provide new ones, and identify future research needs.},
  archive      = {J_EJOR},
  author       = {Stefan Bock and Stefan Bomsdorf and Nils Boysen and Michael Schneider},
  doi          = {10.1016/j.ejor.2024.04.014},
  journal      = {European Journal of Operational Research},
  month        = {4},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A survey on the traveling salesman problem and its variants in a warehousing context},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On subsidization of investments in R&D and production capacity. <em>EJOR</em>, <em>321</em>(3), 1036-1054. (<a href='https://doi.org/10.1016/j.ejor.2024.10.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers a sequential investment project which starts with a product innovation phase, and subsequently, once R&D is completed, a production phase. The investment decisions are the timing and size of the R&D investment, and the size of the production capacity. We show that from a social welfare perspective the firm starts the R&D project too late and installs a too low production capacity. We find that subsidization of the R&D investment is more effective in reducing this welfare loss than subsidization of the production capacity. In fact, we reach the same conclusion under both isoelastic and linear demand.},
  archive      = {J_EJOR},
  author       = {Martijn W. Ketelaars and Peter M. Kort},
  doi          = {10.1016/j.ejor.2024.10.021},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {1036-1054},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {On subsidization of investments in R&D and production capacity},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new lattice approach for risk-minimization hedging under generalized autoregressive conditional heteroskedasticity models. <em>EJOR</em>, <em>321</em>(3), 1021-1035. (<a href='https://doi.org/10.1016/j.ejor.2024.10.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the calculation of risk-minimization hedging strategies, specifically local and global risk minimization strategies for contingent claims under affine and non-affine GARCH models with the known closed forms of its first four moments across times under the physical measure. A unified and efficient willow tree method is introduced for various GARCH models. Unlike methods that provide option values and hedging ratios solely at the inception time, the proposed willow tree method generates a comprehensive table of option values and hedging ratios at each discrete time step across possible asset prices. Additionally, the method showcases robust performance in hedging at lower frequencies than the underlying asset’s modeling frequency (e.g., weekly or monthly hedging using a daily GARCH model). Lastly, the willow tree method outperforms the Monte Carlo method, offering greater efficiency, accuracy, and flexibility in solving risk-minimization hedging problems.},
  archive      = {J_EJOR},
  author       = {Junmei Ma and Chen Wang and Wei Xu},
  doi          = {10.1016/j.ejor.2024.10.002},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {1021-1035},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A new lattice approach for risk-minimization hedging under generalized autoregressive conditional heteroskedasticity models},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate additive subordination with applications in finance. <em>EJOR</em>, <em>321</em>(3), 1004-1020. (<a href='https://doi.org/10.1016/j.ejor.2024.10.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a tractable multivariate pure jump process in which the trading time is described by an additive subordinator. The multivariate process retains the additivity property, and therefore is time inhomogeneous, i.e., its increments are independent but non stationary. We provide the theoretical framework of our process, perform a sensitivity analysis with respect to the time inhomogeneity parameters, and design a Monte Carlo scheme to simulate the trajectories of the process. We then employ the model in the context of option pricing in the FX market. We take advantage of the specific features of currency triangles to extract the joint dynamics of FX log-rates. Extensive tests based on observed market data show that our model outperforms well established pure jump benchmarks.},
  archive      = {J_EJOR},
  author       = {Giovanni Amici and Laura Ballotta and Patrizia Semeraro},
  doi          = {10.1016/j.ejor.2024.10.010},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {1004-1020},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multivariate additive subordination with applications in finance},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forest management with fire simulation. <em>EJOR</em>, <em>321</em>(3), 991-1003. (<a href='https://doi.org/10.1016/j.ejor.2024.10.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address a forest management problem for timber production with fire concerns, employing a novel simulation-based optimization approach wherein forest management is iteratively guided by the feedback from fire spread simulations. The forest management problem involves selecting an alternative prescription for each stand, subject to various restrictions (e.g. bounds on ecosystems services), to maximize the net present value. For each stand, prescriptions involve projecting forest conditions and outcomes using species-specific growth and yield models, combined with different fuel treatment scenarios. In each iteration, the optimization problem is solved. Fire travel times between adjacent points in a grid representing the forest are calculated, based on fuel models associated with the selected prescriptions and other conditions as wind and slopes. Fire spread is simulated for all potential ignitions. Fire paths with a rate of spread greater than a given threshold are identified and constraints are added to the forestry problem to exclude their associated prescriptions to be jointly selected. This problem is re-optimized and the process is repeated until there are no such paths. We describe computational experiments in a Portuguese forest showing how trade-offs between the net present value and the maximum fire rate of spread can be obtained. When too restrictive conditions are imposed on fire, the approach suggests a set of stands to become fire breaks. We also conducted experiments to demonstrate how the impact of the forest surroundings, as well as bounds on ecosystem services, can be evaluated with respect to these trade-offs.},
  archive      = {J_EJOR},
  author       = {Filipe Alvelos and Isabel Martins and Susete Marques},
  doi          = {10.1016/j.ejor.2024.10.013},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {991-1003},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Forest management with fire simulation},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiple asset-type, collaborative vehicle routing problem with proximal servicing of demands. <em>EJOR</em>, <em>321</em>(3), 974-990. (<a href='https://doi.org/10.1016/j.ejor.2024.10.009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research examines the problem of routing multiple assets of different types over a network to service demands in a collaborative manner. The servicing is collaborative in that, when servicing a demand, the different types of assets must do so nearly simultaneously. Moreover, whereas some asset types must service demands by visiting them, other asset types may provide service proximally. This study sets forth a mixed-integer linear program to model this variant of a vehicle routing problem. In addition to directly solving problem instances via a commercial solver, this research proposes two permutations of a model decomposition heuristic, as well as two preprocessing techniques to impose instance-specific bounds on selected decision variables. Comparative testing on mesh networks evaluates nine combinations of solution methods and preprocessing options to solve a set of 216 instances that vary significant parameters. Results manifest trade-offs between the likelihood of finding a feasible solution with bounded computational effort and the relative quality of solutions identified. For larger networks, the preprocessing technique leveraging a nearest neighbor heuristic in combination with any solution method most frequently identified feasible solutions for the set of test instances (i.e., ∼ 90% of instances), with lesser solution quality (i.e., within 15% of the best solutions identified, on average). Worst performing for larger networks was a model decomposition technique that first routes assets providing service proximally, and omitting either preprocessing technique; although this combination yielded the best solutions when it identified a feasible solution, it only did so for ∼ 55% of instances. Other solution method performances exhibit noteworthy nuance, as detailed herein. Further testing of the solution procedures on 216 additional instances for a scenario motivated by a disaster relief using a city road network yielded relatively consistent results; the superlative method leveraged model decomposition and a nearest neighbor preprocessing heuristic, albeit when routing proximally-serving assets first.},
  archive      = {J_EJOR},
  author       = {Stephen D. Donnel and Brian J. Lunday and Nicholas T. Boardman},
  doi          = {10.1016/j.ejor.2024.10.009},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {974-990},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A multiple asset-type, collaborative vehicle routing problem with proximal servicing of demands},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An approximate dynamic programming approach for solving aircraft fleet engine maintenance problem: Methodology and a case study. <em>EJOR</em>, <em>321</em>(3), 958-973. (<a href='https://doi.org/10.1016/j.ejor.2024.10.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a long-term engine maintenance planning problem for an aircraft fleet. The objective is to guarantee sufficient on-wing engines to reach service levels while effectively organizing shop visits for engines. However, complexity arises from intricate maintenance policies and uncertainty in engine deterioration. To address this problem, we propose a graph-based approach representing high-dimensional engine statuses and transitions. We then formulate the problem as a multi-stage stochastic integer program with endogenous uncertainty. We develop an approximate dynamic programming algorithm enhanced by dynamic graph generation and policy-sifting techniques so as to reduce the computational overhead in large problems. We demonstrate the efficacy of our method, compared with other popular methods, in terms of running time and solution quality. In the case study, we present an implementation in a real-world decision system in China Southern Airlines, in which the proposed method works seamlessly with other supporting modules and significantly improves the efficiency of engine maintenance management.},
  archive      = {J_EJOR},
  author       = {Miao Zhang and Jingyuan Yang and Chuwen Zhang and Simai He and Huikang Liu and Jinshen Wang and Zizhuo Wang},
  doi          = {10.1016/j.ejor.2024.10.008},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {958-973},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An approximate dynamic programming approach for solving aircraft fleet engine maintenance problem: Methodology and a case study},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contagion network, portfolio credit risk, and financial crisis. <em>EJOR</em>, <em>321</em>(3), 942-957. (<a href='https://doi.org/10.1016/j.ejor.2024.09.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a network-based nonlinear dynamical system model to study credit exposure risk at the portfolio level. This model captures the complex characteristics of contagion arising from the microstructural interdependencies among firms, especially looping effects. We find that when contagion features are not adequately modeled, portfolio credit risk is generally underestimated, but becomes overestimated during a crisis. This can partly explain the outbreak and subsequent intensification of a crisis such as the 2008 financial crisis. We also derive an expression for the portfolio loss of a regular network, which has implications for measurement and pricing of portfolio credit risk.},
  archive      = {J_EJOR},
  author       = {Michael C. Fu and Bingqing Li and Fei Li and Rongwen Wu},
  doi          = {10.1016/j.ejor.2024.09.026},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {942-957},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Contagion network, portfolio credit risk, and financial crisis},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multistage stochastic programming with a random number of stages: Applications in hurricane disaster relief logistics planning. <em>EJOR</em>, <em>321</em>(3), 925-941. (<a href='https://doi.org/10.1016/j.ejor.2024.10.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a logistics planning problem of prepositioning relief commodities in preparation for an impending hurricane landfall. We model the problem as a multi-period network flow problem where the objective is to minimize the total expected logistics cost of operating the network to meet the demand for relief commodities. We assume that the hurricane’s attributes evolve over time according to a Markov chain model, and the demand quantity at each demand point is calculated based on the hurricane’s attributes (intensity and location) at the terminal stage, which corresponds to the hurricane’s landfall. We introduce a fully adaptive multi-stage stochastic programming (MSP) model that allows the decision-maker to adapt their logistics decisions over time according to the evolution of the hurricane’s attributes. In addition, we develop a novel extension of the standard MSP model to address the challenge of having a random number of stages in the planning horizon due to the uncertain landfall time of the hurricane. We benchmark the performance of the adaptive decision policy given by the MSP models with alternative decision policies, including a static policy, a rolling-horizon policy, a wait-and-see policy, and a decision-tree-based policy, all based on two-stage stochastic programming models. Our numerical results and sensitivity analyses provide key insights into the value of MSP in the hurricane disaster relief logistics planning problem.},
  archive      = {J_EJOR},
  author       = {Murwan Siddig and Yongjia Song},
  doi          = {10.1016/j.ejor.2024.10.004},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {925-941},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multistage stochastic programming with a random number of stages: Applications in hurricane disaster relief logistics planning},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic behavior in multi-criteria sorting with trust relationships-based consensus mechanism: Application in supply chain risk management. <em>EJOR</em>, <em>321</em>(3), 907-924. (<a href='https://doi.org/10.1016/j.ejor.2024.10.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strategic behavior is common in multi-criteria sorting, where manipulated alternatives are allocated to the target category to achieve the intended goal. Engaging in such strategic behavior in sorting consensus often comes at a cost, which is closely tied to the degree of preference adjustments and the level of trust relationships among decision makers. This study investigates strategic behavior using piecewise cost-based consensus mechanism in social trust network. Firstly, the trust relationship dependent piecewise cost is formally defined and formulated by considering the impact of trust relationship on the preference adjustments willingness to achieve sorting consensus. Then, based on the trust relationship dependent piecewise cost, a preference strategic manipulation model is proposed to achieve strategic behavior in sorting consensus, which is implemented by minimizing the cost of strategically guiding decision makers in adjusting their preferences. Considering that social trust network affects strategic manipulation, a preference-network collaborative strategic manipulation model is further proposed, which is designed to strategically guide decision makers in adjusting both their preferences and social trust networks. Moreover, the practicality of proposed strategic manipulation models is exemplified through a case study on supply chain risk management and their performance is validated via simulation analysis.},
  archive      = {J_EJOR},
  author       = {Fang Wang and Hengjie Zhang and Jigan Wang},
  doi          = {10.1016/j.ejor.2024.10.027},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {907-924},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Strategic behavior in multi-criteria sorting with trust relationships-based consensus mechanism: Application in supply chain risk management},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A compromise solution approach for efficiency measurement with shared input: The case of tourist hotels in taiwan. <em>EJOR</em>, <em>321</em>(3), 895-906. (<a href='https://doi.org/10.1016/j.ejor.2024.10.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared input occurs often when a production system performs two or more functions and some of the functions share the same input. To determine the proportion of the shared input devoted to each function, the conventional data envelopment analysis (DEA) models that allow each decision making unit (DMU) to select the most favorable value to attain the highest efficiency score is usually used. The result is that many DMUs assign proportions of different bound values to the same function, which is not reasonable. This paper proposes a compromise solution approach that aggregates the viewpoints of all DMUs to obtain a consensus regarding the proportion of the shared input used by each function. The approach is actually a parametric DEA approach for constructing the production function. To increase the explanation power, transcendental functions are considered. The tourist hotel, where the accommodation and catering functions share the input of management employees is used for illustration. The results show that a transcendental function has stronger explanation power, and reasonable compromised values for the proportions of the effort of the management employees devoted to the two functions are obtained.},
  archive      = {J_EJOR},
  author       = {Chiang Kao and Shiang-Tai Liu},
  doi          = {10.1016/j.ejor.2024.10.024},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {895-906},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A compromise solution approach for efficiency measurement with shared input: The case of tourist hotels in taiwan},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interrelationships of non-cooperative, classical and pareto coalitional stability definitions. <em>EJOR</em>, <em>321</em>(3), 884-894. (<a href='https://doi.org/10.1016/j.ejor.2024.10.035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theorems are established on the interrelationships among non-cooperative, classical and Pareto coalitional stability definitions within the framework of the graph model for conflict resolution. The classical coalition stability concepts are first redefined and then, based on the concept of Pareto coalition improvement, new definitions are proposed for Pareto coalition Nash, Pareto coalition general metarational, Pareto coalition symmetric metarational, and Pareto coalition sequential stability. Relations among Pareto coalition stability definitions, earlier coalition stability definitions, and non-cooperative analogues are explained. The practical applicability of these stability definitions is illustrated through their application to a real-world conflict, a 2014 offshore oil exploration dispute in the South China Sea. The equilibrium results confirm the theoretical validity of the new definitions, and show that valuable insights that can be garnered using the Pareto coalition stability approach.},
  archive      = {J_EJOR},
  author       = {Ziming Zhu and D. Marc Kilgour and Keith W. Hipel and Jing Yu},
  doi          = {10.1016/j.ejor.2024.10.035},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {884-894},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Interrelationships of non-cooperative, classical and pareto coalitional stability definitions},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consensus methods with nash and Kalai–Smorodinsky bargaining game for large-scale group decision-making. <em>EJOR</em>, <em>321</em>(3), 865-883. (<a href='https://doi.org/10.1016/j.ejor.2024.10.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the significant advancements in communication technology, group decision-making (GDM) can now be implemented online, allowing a large number of decision-makers (DMs) to participate concurrently. However, current methods for large-scale group decision-making (LSGDM) are primarily suitable for 20 to 50 DMs, and their effectiveness in scenarios involving thousands or even tens of thousands of participants has yet to be fully validated. Furthermore, as the number of participants increases, the evaluation information becomes increasingly diverse and complex. At the same time, the social networks associated with the DMs typically become sparse, making information sharing and consensus building more challenging. In light of these challenges, we develop two new methods based on cooperative games to effectively address the challenges in super LSGDM. First, we propose a two-stage semi-supervised fuzzy C-means (FCM) clustering method with trust constraints, which aims to address the issue of sparsity in relationships within large-scale social networks. This method utilizes trust relationships as reliable resources and prior knowledge to guide and supervise the clustering process. On this basis, we discuss three scenarios from the perspective of cooperative games: (i) subgroup optimal consensus adjustments in non-cooperative situations, (ii) group optimal consensus adjustments in cooperative situations, and (iii) subgroup optimal consensus adjustments in cooperative situations. Subsequently, we view the consensus adjustment allocation as a cost cooperative game problem and propose two new LSGDM consensus methods based on Nash Bargaining (NB) and Kalai–Smorodinsky Bargaining (KSB). Finally, experiments on real datasets demonstrate the superiority and reliability of our proposed LSGDM methods.},
  archive      = {J_EJOR},
  author       = {Yufeng Shen and Xueling Ma and Gang Kou and Rosa M. Rodríguez and Jianming Zhan},
  doi          = {10.1016/j.ejor.2024.10.016},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {865-883},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Consensus methods with nash and Kalai–Smorodinsky bargaining game for large-scale group decision-making},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simulation optimization approach for weight valuation in analytic hierarchy process. <em>EJOR</em>, <em>321</em>(3), 851-864. (<a href='https://doi.org/10.1016/j.ejor.2024.10.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analytic hierarchy process (AHP) is a structured technique used to analyze complex decision-making situations such as resource allocation, benchmarking, and quality management. In the weight valuation step of using AHP to select the best design, pairwise comparison matrices are used to calculate the local priorities for designs that have contentious and unresolved criticisms. In this study, we propose a Bayesian approach using a Dirichlet-multinomial model to estimate local priorities during weight valuation. Experts are only asked to select the best design with respect to predetermined criterion. Subsequently, local priorities are estimated without pairwise comparison matrices. To improve the efficiency of the AHP, we propose two expert allocation policies (AHP-KG and AHP-AKG) based on the ranking and selection procedures. Our numerical results show that the proposed AHP-KG and AHP-AKG policies outperform pure exploration and proportional allocation policies.},
  archive      = {J_EJOR},
  author       = {Hui Xiao and Sha Zeng and Yi Peng and Gang Kou},
  doi          = {10.1016/j.ejor.2024.10.018},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {851-864},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A simulation optimization approach for weight valuation in analytic hierarchy process},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A risk-averse latency location-routing problem with stochastic travel times. <em>EJOR</em>, <em>321</em>(3), 837-850. (<a href='https://doi.org/10.1016/j.ejor.2024.10.041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a latency location-routing problem with stochastic travel times is investigated. The problem is cast as a two-stage stochastic program. The ex-ante decision comprises the location of the depots. The ex-post decision regards the routing, which adapts to the observed travel times. A risk-averse decision-maker is assumed, which is conveyed by adopting the latency CVaR α as the objective function. The problem is formulated mathematically. An efficient multi-start variable neighborhood search algorithm is proposed for tackling the problem when uncertainty is captured by a finite set of scenarios. This procedure is then embedded into a sampling mechanism so that realistic instances of the problem can be tackled, namely when the travel times are represented by random vectors with an infinite support. An extensive computational analysis is conducted to assess the methodological developments proposed and the relevance of capturing uncertainty in the problem. Additional insights include the impact of the risk level in the solutions.},
  archive      = {J_EJOR},
  author       = {Alan Osorio-Mora and Francisco Saldanha-da-Gama and Paolo Toth},
  doi          = {10.1016/j.ejor.2024.10.041},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {837-850},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A risk-averse latency location-routing problem with stochastic travel times},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic dual dynamic programming for optimal power flow problems under uncertainty. <em>EJOR</em>, <em>321</em>(3), 814-836. (<a href='https://doi.org/10.1016/j.ejor.2024.09.045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning in the power sector has to take into account the physical laws of alternating current (AC) power flows as well as uncertainty in the data of the problems, both of which greatly complicate optimal decision making. We propose a computationally tractable framework to solve multi-stage stochastic optimal power flow (OPF) problems in AC power systems. Our approach uses recent results on dual convex semi-definite programming (SDP) relaxations of OPF problems in order to adapt the stochastic dual dynamic programming (SDDP) algorithm for problems with a Markovian structure. We show that the usual SDDP lower bound remains valid and that the algorithm converges to a globally optimal policy of the stochastic AC-OPF problem as long as the SDP relaxations are tight. To test the practical viability of our approach, we set up a case study of a storage siting, sizing, and operations problem. We show that the convex SDP relaxation of the stochastic problem is usually tight and discuss ways to obtain near-optimal physically feasible solutions when this is not the case. The algorithm finds a physically feasible policy with an optimality gap of 3% and yields a significant added value of 27% over a rolling deterministic policy, which leads to overly optimistic policies and underinvestment in flexibility. This suggests that the common industry practice of assuming direct current and deterministic problems should be reevaluated by considering models that incorporate realistic AC flows and stochastic elements in the data as potentially more realistic alternatives.},
  archive      = {J_EJOR},
  author       = {Adriana Kiszka and David Wozabal},
  doi          = {10.1016/j.ejor.2024.09.045},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {814-836},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Stochastic dual dynamic programming for optimal power flow problems under uncertainty},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust concave utility maximization over chance constraints. <em>EJOR</em>, <em>321</em>(3), 800-813. (<a href='https://doi.org/10.1016/j.ejor.2024.10.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper first studies an expected utility problem with chance constraints and incomplete information on a decision maker’s utility function. The model maximizes the worst-case expected utility of random outcome over a set of concave functions within a novel ambiguity set, while the underlying probability distribution is known with the assumption of a discretization of possible realizations for the uncertainty parameter. To obtain computationally tractable formulations, we employ a discretization approach to derive a max–min chance-constrained approximation of this problem. This approximation is further reformulated as a mixed-integer program. We show that the discrete approximation converges to the true counterpart under mild assumptions. We also present a row generation algorithm for optimizing the max–min program. A computational study for a bin-packing problem and a multi-item newsvendor problem is conducted to demonstrate the benefit of the proposed framework and the computational efficiency of our algorithm. We find that the row generation algorithm can significantly reduce the computational time, and our robust policy could achieve a better out-of-sample performance when compared with the non-robust policy and the one without the chance constraints.},
  archive      = {J_EJOR},
  author       = {Shanshan Wang and Sanjay Mehrotra and Chun Peng},
  doi          = {10.1016/j.ejor.2024.10.007},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {800-813},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust concave utility maximization over chance constraints},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Signaling or not? the pricing strategy under fairness concerns and cost information asymmetry. <em>EJOR</em>, <em>321</em>(3), 789-799. (<a href='https://doi.org/10.1016/j.ejor.2024.10.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supply chain fairness issues have become crucial and prevalent recently, whereas the operational decisions in the fair chain are more and more challenging when involving information asymmetry. Considering the fact that the upstream supplier of a chain generally has private production cost information, this paper investigates how the supplier strategically makes pricing decisions under the own private information and the downstream fairness concerns. Based on inequity aversion theory, we set up a signaling game between the supplier and the retailer with two kinds of inequity aversion, disadvantageous inequity aversion and advantageous inequity aversion, respectively. Compared with the scenario of symmetric information, under asymmetric information, we find that: the high-cost supplier distorts the wholesale price upward to avoid the low-cost supplier's price mimicry when the retailer has disadvantageous inequity aversion and the cost difference between the two types of suppliers is small; the supply chain's expected profit and expected utility may increase due to the reason that the low-cost supplier distorts his wholesale price downward when the retailer has advantageous inequity aversion, which mitigates the double marginalization. Moreover, we extend the base model to the scenario of endogenous prior probability on supplier cost type and we show that when the retailer has disadvantageous inequity aversion, cost information asymmetry may improve the willingness of the high-cost supplier to become the low-cost supplier. Our results can provide some practical insights on managing cost information asymmetry from the perspectives of the supply chain and the industry.},
  archive      = {J_EJOR},
  author       = {He Huang and Dandan Wu and Hongyan Xu},
  doi          = {10.1016/j.ejor.2024.10.006},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {789-799},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Signaling or not? the pricing strategy under fairness concerns and cost information asymmetry},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Due date-oriented picker routing, an efficient exact solution algorithm, and its application to pick-from-store omnichannel retailing. <em>EJOR</em>, <em>321</em>(3), 775-788. (<a href='https://doi.org/10.1016/j.ejor.2024.10.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of e-commerce and omnichannel retailing has sparked renewed interest in picker routing in warehouses. This paper presents two significant methodological advances in this well-established field. First, it is a well-known fact that the parallel-aisle layout of warehouses, as opposed to general graphs, allows for polynomial-time solutions of the Traveling Salesman Problem. We show that the parallel-aisle structure can also be exploited when pickers are tasked with visiting storage positions associated with specific due dates. We establish that picker routing in warehouses, subject to soft due date constraints, is a binary NP -hard problem. We also present an exact branch-and-bound algorithm with pseudo-polynomial time complexity. This algorithm effectively solves instances with up to 60 picking positions and five cross aisles within a few seconds while guaranteeing optimality. Second, for even larger pick lists, we demonstrate the successful integration of our algorithm into a real-time framework. This approach allows us to avoid extended solution times that would otherwise delay the picker’s departure, without compromising the quality of the solution. To illustrate the practical relevance of these two methodological innovations, we apply our routing algorithm to the context of pick-from-store omnichannel retailing. By assigning due dates to critical products, we significantly reduce stockout occurrences for online customers. These stockouts occur when the stock level, initially deemed sufficient to confirm an online order, is depleted by walk-in customers before the picker reaches the relevant shelf.},
  archive      = {J_EJOR},
  author       = {Stefan Bock and Nils Boysen},
  doi          = {10.1016/j.ejor.2024.10.015},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {775-788},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Due date-oriented picker routing, an efficient exact solution algorithm, and its application to pick-from-store omnichannel retailing},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain enabled traceability — An analysis of pricing and traceability effort decisions in supply chains. <em>EJOR</em>, <em>321</em>(3), 760-774. (<a href='https://doi.org/10.1016/j.ejor.2024.10.019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite numerous use cases, enterprise-wide implementations of blockchains have seen limited success. This raises the question of when do firms adopt blockchains and do blockchains benefit supply chains. To answer this, we examine a dyadic supply chain consisting of a buyer and a supplier and analyze their traceability effort and pricing decisions. Our results show that the demand-side, supply-side and reputational factors influencing blockchain adoption are primarily complementary and in the absence of one of them, firms can still adopt blockchain. Furthermore, even in the absence of individual benefits for a supply chain partner, there exist conditions under which blockchain adoption benefits the supply chain that can incentivize players to join blockchain. Overall, we contribute by offering a framework that supply chain players can use to assess the likelihood of blockchain implementation success or failure and address the challenges pertaining to incentives and cost imbalances in blockchain implementation.},
  archive      = {J_EJOR},
  author       = {Prakash Awasthy and Tanushree Haldar and Debabrata Ghosh},
  doi          = {10.1016/j.ejor.2024.10.019},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {760-774},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Blockchain enabled traceability — An analysis of pricing and traceability effort decisions in supply chains},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing the price of fairness in scheduling problems with two agents. <em>EJOR</em>, <em>321</em>(3), 750-759. (<a href='https://doi.org/10.1016/j.ejor.2024.10.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the price of fairness in several scheduling problems with two agents, each with a set of nonpreemptive jobs, competing to execute their respective jobs on a single machine. Each agent expects to minimize its objective function, which depends on the completion times of its own jobs. Several objective functions are considered, including makespan, total (weighted) completion time and maximum tardiness. We focus on problems in which both agents pursue the same objective function. For each problem, we analyze the price of fairness and the complexity to find the fairness schedules among the Pareto optimal schedules. When the objective functions of both agents are total completion time, we design an algorithm to generate a near-fair solution and analyze its price of fairness.},
  archive      = {J_EJOR},
  author       = {Jin Yu and Peihai Liu and Xiwen Lu and Manzhan Gu},
  doi          = {10.1016/j.ejor.2024.10.023},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {750-759},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Analyzing the price of fairness in scheduling problems with two agents},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimizing the number of late jobs and total late work with step-learning. <em>EJOR</em>, <em>321</em>(3), 734-749. (<a href='https://doi.org/10.1016/j.ejor.2024.09.042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study single-machine scheduling problems with step-learning, where an improvement in processing time is experienced if a job is started at, or after, a job-dependent learning-date. We consider minimizing two functions: the number of late jobs and the total late work, and we show that when at least a common due-date or common learning-date is assumed, the problem is NP -hard in the ordinary sense; however, when both are arbitrary, the problem becomes strongly NP -hard. For each of the problems where at least one of the dates is assumed to be common, we analyze the structure of an optimal job schedule with and without idle time and propose pseudo-polynomial time dynamic programming algorithms. We also show that the problem of minimizing the weighted number of late jobs with step-learning can be solved with a minor change to the algorithms for the unweighted case. In addition to this, we show that when a common due-date is assumed and no idle time is allowed, the problem of minimizing the total late work is equivalent to that of minimizing the makespan. Furthermore, we provide a more efficient algorithm to solve the problem of minimizing makespan under the assumption of a common learning-date than the one in the existing literature. Lastly, we show that our analysis can also be applied to the case of step-deterioration, where instead, the processing times of jobs increase at a given date.},
  archive      = {J_EJOR},
  author       = {Johnson Phosavanh and Daniel Oron},
  doi          = {10.1016/j.ejor.2024.09.042},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {734-749},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Minimizing the number of late jobs and total late work with step-learning},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scheduling multi-skill technicians and reassignable tasks in a cloud computing company. <em>EJOR</em>, <em>321</em>(3), 717-733. (<a href='https://doi.org/10.1016/j.ejor.2024.09.050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a multi-skill technician and reassignable task scheduling problem in a cloud computing company. In the problem, multi-skill technicians are assigned to process a large number of tasks from customer requests in a certain scheduling horizon. The tasks are allowed to be reassigned to another technician multiple times, and one technician can process multiple tasks in parallel. The company not only focuses on processing efficiency, but also expects to improve customers’ experience and technicians’ satisfaction. We characterize the feasible solutions and introduce a weighted objective with three metrics: processing efficiency, response delay, and workload balance. An effective two-stage hierarchical optimization method embedded in a greedy randomized adaptive search procedure framework is proposed. In the first stage, initial solutions are generated by a greedy randomized construction procedure, and then improved by local search with an ejection chain operator to optimize processing efficiency. In the second stage, two local search procedures with five operators for improving response delay or workload balance are designed. Computational experiments are conducted to evaluate the effectiveness of our algorithm. The results show that the proposed algorithm is competent in fast computing a schedule of high quality. It also reveals that reassignments are helpful in reducing response delay and balancing workloads in the scheduling.},
  archive      = {J_EJOR},
  author       = {Shuang Jin and Jiaming Tao and Minghui Lai and Qian Hu},
  doi          = {10.1016/j.ejor.2024.09.050},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {717-733},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Scheduling multi-skill technicians and reassignable tasks in a cloud computing company},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skill development in the field of scheduling: A structured literature review. <em>EJOR</em>, <em>321</em>(3), 697-716. (<a href='https://doi.org/10.1016/j.ejor.2024.04.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employee skills are seen as a main driver of competitive advantages of enterprises. This article provides a state-of-the-art overview of research related to skill management in the field of operational research. For this purpose, ‘skill management’ is used as an umbrella term to integrate the different quantitative approaches found in this field. The structured literature review is based on six keywords and includes articles published between 1998 and 2022 in nine major operational research and operational management journals (European Journal of Operational Research, International Journal of Production Research, International Journal of Production Economics, Management Science, Operations Research, Omega, Journal of Operations Management, Production and Operations Management, and Manufacturing & Service Operations Management). Further relevant literature considering skill-related topics and machine scheduling problems is also discussed. The publications included in this review are analyzed in depth with regard to theoretical results on employee skill development. Moreover, a unified notation is introduced that covers different models, machine environments, and objectives. To the best of our knowledge, this is the first review where learning, forgetting, and training aspects are jointly considered. Moreover, the review also highlights substantial research gaps and avenues for future research.},
  archive      = {J_EJOR},
  author       = {Patricia Heuser and Peter Letmathe and Thomas Vossen},
  doi          = {10.1016/j.ejor.2024.04.005},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {3},
  pages        = {697-716},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Skill development in the field of scheduling: A structured literature review},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-consistent asset allocation for risk measures in a lévy market. <em>EJOR</em>, <em>321</em>(2), 676-695. (<a href='https://doi.org/10.1016/j.ejor.2024.09.049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Focusing on gains & losses relative to a risk-free benchmark instead of terminal wealth, we consider an asset allocation problem to maximize time-consistently a mean-risk reward function with a general risk measure which is (i) law-invariant, (ii) cash- or shift-invariant, and (iii) positively homogeneous, and possibly plugged into a general function. Examples include (relative) Value at Risk, coherent risk measures, variance, and generalized deviation risk measures. We model the market via a generalized version of the multi-dimensional Black–Scholes model using α -stable Lévy processes and give supplementary results for the classical Black–Scholes model. The optimal solution to this problem is a Nash subgame equilibrium given by the solution of an extended Hamilton–Jacobi–Bellman equation. Moreover, we show that the optimal solution is deterministic under appropriate assumptions.},
  archive      = {J_EJOR},
  author       = {Felix Fießinger and Mitja Stadje},
  doi          = {10.1016/j.ejor.2024.09.049},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {676-695},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Time-consistent asset allocation for risk measures in a lévy market},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic clearing and contagion in financial networks. <em>EJOR</em>, <em>321</em>(2), 664-675. (<a href='https://doi.org/10.1016/j.ejor.2024.09.046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce a generalized extension of the Eisenberg–Noe model of financial contagion to allow for time dynamics of the interbank liabilities, including a dynamic examination of default risk. This framework separates the cash account and long-term capital account to more accurately model the health of a financial institution. In doing so, such a system allows us to distinguish between delinquency and default as well as between defaults resulting from either insolvency or illiquidity.},
  archive      = {J_EJOR},
  author       = {Tathagata Banerjee and Alex Bernstein and Zachary Feinstein},
  doi          = {10.1016/j.ejor.2024.09.046},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {664-675},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Dynamic clearing and contagion in financial networks},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Likelihood-ratio test for technological differences in two-stage data envelopment analysis for panel data. <em>EJOR</em>, <em>321</em>(2), 644-663. (<a href='https://doi.org/10.1016/j.ejor.2024.09.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the question of adapting a likelihood-ratio test in the two-stage data envelopment analysis (DEA) framework, where DEA estimates are regressed against external factors. We focus on the hypotheses of testing the technological difference across time periods (or groups) and propose two bootstrapping procedures. Our Monte Carlo (MC) simulation shows that the proposed test has a substantially better-estimated size for the case of smoothing the ‘spurious ones’, rather than removing them. MC simulation also confirms the curse of dimensionality when the input and output variables increase in the production model. Finally, the proposed test is demonstrated in an empirical illustration.},
  archive      = {J_EJOR},
  author       = {Kai Du and Valentin Zelenyuk},
  doi          = {10.1016/j.ejor.2024.09.039},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {644-663},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Likelihood-ratio test for technological differences in two-stage data envelopment analysis for panel data},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation-based emergency department staffing and scheduling optimization considering part-time work shifts. <em>EJOR</em>, <em>321</em>(2), 631-643. (<a href='https://doi.org/10.1016/j.ejor.2024.09.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency department (ED) physicians have long been in short supply, which causes serious overcrowding and the excessive waiting of patients in many countries. ED managers are under great pressure to make staffing and scheduling decisions. To address this problem, this paper introduces part-time work shifts with fixed starting times and durations in peak hours to increase the flexibility and reduce patients’ waiting times. Then, a two-stage framework is proposed to optimize the weekly staffing and scheduling decisions with both fixed and part-time work shifts. Stage I is a weekly staffing problem to determine the number of physicians for both types of work shifts. The weekly staffing problem is decomposed and then solved using a simulation optimization method. Specifically, an adaptive partitioning empirical stochastic branch and bound (ESB&B-AP) algorithm is proposed by employing adaptive cut-generation methods in the process of feasible region partitioning, which significantly improves the searching efficiency of the algorithm. Stage II is a scheduling problem to determine the work schedules of physicians based on a predetermined set of work patterns. Numerical experiments validate the advantages of part-time work shifts, as well as the effectiveness of the two-stage optimization framework and the ESB&B-AP algorithm.},
  archive      = {J_EJOR},
  author       = {Xiuxian Wang and Andrea Matta and Na Geng and Liping Zhou and Zhibin Jiang},
  doi          = {10.1016/j.ejor.2024.09.020},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {631-643},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Simulation-based emergency department staffing and scheduling optimization considering part-time work shifts},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bank financial sustainability evaluation: Data envelopment analysis with random forest and shapley additive explanations. <em>EJOR</em>, <em>321</em>(2), 614-630. (<a href='https://doi.org/10.1016/j.ejor.2024.09.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring financial sustainability is imperative for a financial institution's overall stability. To mitigate the risk of bank failure amid financial crises, effective management of financial sustainability performance becomes paramount. This study introduces a comprehensive framework for the accurate and efficient quantification, indexing, and evaluation of financial sustainability within the American banking industry. Our approach begins by conceptualizing financial sustainability as a multi-stage, multifactor structure. We construct a composite index through a three-stage network data envelopment analysis (DEA) and subsequently develop a random forest classification model to predict financial sustainability outcomes. The classification model attains an average testing recall rate of 84.34 %. Additionally, we employ SHapley Additive exPlanations (SHAP) to scrutinize the impacts of contextual variables on financial sustainability performance across various substages and the overall banking process, as well as to improve the interpretability and transparency of the classification results. SHAP results reveal the significance and effects of contextual variables, and noteworthy differences in contextual impacts emerge among different banking substages. Specifically, loans and leases, interest income, total liabilities, total assets, and market capitalization positively contribute to the deposit stage; revenue to assets positively influences the loan stage; and revenue per share positively affects the profitability stage. This study serves the managerial objective of assisting banks in capturing financial sustainability and identifying potential sources of unsustainability. By unveiling the “black box” of financial sustainability and deciphering its internal dynamics and interactions, banks can enhance their ability to monitor and control financial sustainability performance more effectively.},
  archive      = {J_EJOR},
  author       = {Yu Shi and Vincent Charles and Joe Zhu},
  doi          = {10.1016/j.ejor.2024.09.030},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {614-630},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Bank financial sustainability evaluation: Data envelopment analysis with random forest and shapley additive explanations},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing extended warranty options with preventive maintenance service under multinomial logit model. <em>EJOR</em>, <em>321</em>(2), 600-613. (<a href='https://doi.org/10.1016/j.ejor.2024.09.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extended warranty (EW) services have become the primary source of profit for manufacturers. How to design the appropriate EW service to balance customer user experience with the manufacturer’s profit issues has received great attention. Implementing preventive maintenance (PM) actions can not only reduce the customer’s lifecycle cost but also enhance the manufacturer’s brand image. Therefore, EW bundled with PM service has emerged in the after-sales market today. To enrich customers’ choice, we investigate the design of EW menus bundled with PM service options, and then we jointly determine EW prices and the frequency of PM actions for the provided menus. In the proposed model, we combine the multinomial logit model with prospect theory to characterize customer choice behavior when presented with different EW menu options. Based on that, with the aim of maximizing the manufacturer’s profit, we can derive sets of offers, the corresponding EW service prices, and the number of included PM actions. We also introduce two additional models that account for different customer purchase scenarios. Numerical results reveal that when the manufacturer chooses to offer a unified EW menu to all customers, it is consistently more profitable to present the menu at the point of product sales. Another interesting finding is that customized EW menus designed for various customer segments do not always outperform unified ones. Overall, this study will provide a foundation for manufacturers to make informed decisions regarding the selection and design of EW menus in various scenarios.},
  archive      = {J_EJOR},
  author       = {Anshu Dai and Xi Yang and Duo Yang and Ting Li and Xin Wang and Shuguang He},
  doi          = {10.1016/j.ejor.2024.09.024},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {600-613},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing extended warranty options with preventive maintenance service under multinomial logit model},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based dynamic multilayer graph neural networks for loan default prediction. <em>EJOR</em>, <em>321</em>(2), 586-599. (<a href='https://doi.org/10.1016/j.ejor.2024.09.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, a model with GAT, LSTM, and the attention mechanism provides the best results. Empirical results demonstrate that, when it comes to predicting probability of default for the borrowers, our proposed model brings both better results and novel insights for the analysis of the importance of connections and timestamps, compared to traditional methods.},
  archive      = {J_EJOR},
  author       = {Sahab Zandi and Kamesh Korangi and María Óskarsdóttir and Christophe Mues and Cristián Bravo},
  doi          = {10.1016/j.ejor.2024.09.025},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {586-599},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Attention-based dynamic multilayer graph neural networks for loan default prediction},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Condition-based switching, loading, and age-based maintenance policies for standby systems. <em>EJOR</em>, <em>321</em>(2), 565-585. (<a href='https://doi.org/10.1016/j.ejor.2024.09.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standby techniques are widely incorporated in structural design to enhance the inherent reliability of systems. To further leverage the system performance during operation, decision-makers can adopt operational policies to manage system degradation. Specifically, at the system level, unit switching that dynamically determines the online unit contributes to avoiding unexpected shutdowns. At the unit level, adjusting load levels to manage the trade-off between condition degradation and revenue accumulation is crucial for maximizing profit. Additionally, adopting age-based maintenance as a tactical decision, which effectively facilitates the integration of maintenance resources, can be implemented to restore a degraded system. For instance, maintenance is typically scheduled at fixed moments for multi-generator power systems located in remote areas. In between maintenance moments, the proactive switching of generators can ensure uninterrupted output, and the adjustment of load levels for online generators helps to maximize output. Motivated by such engineering practices, this paper investigates condition-based switching, loading, and age-based maintenance policies for standby systems to maximize the expected profit rate in the long-run horizon. The problem is formulated as a Markov decision process. The structural properties of the control-limit switching and monotone loading policies are analyzed for easy policy implementation and efficient problem solutions. For comparative purposes, several heuristic policies are proposed and evaluated. Finally, numerical examples are presented to validate theoretical results and illustrate the superiority of the proposed risk control policy.},
  archive      = {J_EJOR},
  author       = {Xian Zhao and Rong Li and He Han and Qingan Qiu},
  doi          = {10.1016/j.ejor.2024.09.014},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {565-585},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Condition-based switching, loading, and age-based maintenance policies for standby systems},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the minimum cost conflict mediation path to a desired resolution within the inverse graph model framework. <em>EJOR</em>, <em>321</em>(2), 543-564. (<a href='https://doi.org/10.1016/j.ejor.2024.10.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing inverse graph model for conflict resolution (GMCR) research primarily concentrates on identifying the required preferences of decisions makers (DMs) such that a desired state is an equilibrium. However, the process of transitioning from the current state to the desired equilibrium is not explored. In this paper, we propose a minimum adjustment cost model taking account of preference adjustment costs to identify the required preferences for a desired state to be an equilibrium. Subsequently, we introduce the concept of transition costs for the first time to quantify expenses involved in guiding a DM transition from one state to another and develop a minimum cost conflict mediation path model. This model aims to identify the most efficient path that minimizes the cost of transitioning from the current state to the desired equilibrium. Moreover, to accommodate the consideration of multiple desired equilibria, we extend the minimum cost conflict mediation path model to analyze and determine the optimal path for transitioning from the current state to one of the identified desired equilibria with the overall minimum cost. Furthermore, to address uncertainty surrounding transition costs, we formulate a probability maximizing conflict mediation path model that considers a limited budget available for the mediation process. Finally, a real-world dispute, the fracking conflict in the province of New Brunswick, Canada, is utilized to demonstrate the application of the proposed models.},
  archive      = {J_EJOR},
  author       = {Yan Zhu and Yucheng Dong and Hengjie Zhang and Liping Fang},
  doi          = {10.1016/j.ejor.2024.10.014},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {543-564},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Exploring the minimum cost conflict mediation path to a desired resolution within the inverse graph model framework},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring environmental inefficiency through machine learning: An approach based on efficiency analysis trees and by-production technology. <em>EJOR</em>, <em>321</em>(2), 529-542. (<a href='https://doi.org/10.1016/j.ejor.2024.10.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of this study is to introduce machine learning-type extensions for the measurement of environmental inefficiency based on regression trees under shape constraints. The new methods developed are implemented using a by-production approach that distinguishes two technologies, one related to the generation of pollution and the other to the production of good outputs. In particular, we define two alternative approaches to measuring environmental inefficiency: by-production Efficiency Analysis Trees (by-production EAT) and by-production Convexified Efficiency Analysis Trees (by-production CEAT). The main advantage of the methods developed is that they do not suffer from the typical statistical problem of overfitting connected to Free Disposal Hull (FDH) and Data Envelopment Analysis (DEA). The performance of the new models is evaluated through a simulation study which shows that the new approaches outperform FDH and DEA in terms of mean squared error and bias. We also illustrate the practical usefulness of the new techniques through empirical application to 43 developing and developed countries over a fifteen-year period - from 2000 to 2014. Our empirical findings using real data clearly indicate the higher discriminating power of the by-production EAT and CEAT models as compared respectively to FDH and DEA.},
  archive      = {J_EJOR},
  author       = {Maria D. Guillen and Juan Aparicio and Magdalena Kapelko and Miriam Esteve},
  doi          = {10.1016/j.ejor.2024.10.003},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {529-542},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Measuring environmental inefficiency through machine learning: An approach based on efficiency analysis trees and by-production technology},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the integration of multiple criteria decision aiding and forecasting: Does it create value in portfolio selection?. <em>EJOR</em>, <em>321</em>(2), 516-528. (<a href='https://doi.org/10.1016/j.ejor.2024.09.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A systemic integration of multiple criteria decision aiding (MCDA) and forecasting is presented for enhancing the quality of investment decisions. First, we provide a new methodological approach for combining forward-looking information, as an input to further MCDA analysis and we illustrate that the informational content of the MCDA scores is distinct from that of the original data used as input to the MCDA. Moreover, we provide very compelling evidence that the produced MCDA scores and rankings have incremental information, over and above that being contained in either the original data or the forecasts themselves. Therefore, it can be used successfully in providing methods which will have a high, a priori, chance of resulting in financial performance enhancements; to this end, we have illustrated this finding with a number of different approaches, including asset rotation and portfolio construction . Next, we show that the MCDA ranked variables can be used for both in-sample explaining and out-of-sample forecasting of critical economic variables, such as the real GDP growth or the unemployment rate, so that MCDA might be able to assist in identifying leading indicator properties for economic forecasting. The evaluation of the suggested approach entails extensive empirical testing on the various US economy market sectors , as expressed by the corresponding S&P 500 Exchange Traded Funds (ETFs), for a period of 22 years, documenting specific economic stylized facts, reported for first time in the relevant literature.},
  archive      = {J_EJOR},
  author       = {Panos Xidonas and Dimitris Thomakos and Aristeidis Samitas},
  doi          = {10.1016/j.ejor.2024.09.031},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {516-528},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {On the integration of multiple criteria decision aiding and forecasting: Does it create value in portfolio selection?},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal platform pricing with multi-sided users: A direct and indirect network approach. <em>EJOR</em>, <em>321</em>(2), 503-515. (<a href='https://doi.org/10.1016/j.ejor.2024.09.038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We challenge the dichotomy of network effects and highlight that they are not an exogenous characteristic of networks, but endogenous to the decisions of network users. When users choose which activities to perform in a network, multi-activity users transform indirect into direct network effects and a network effectively becomes one-sided if merely multi-activity users frequent it. Our work contributes to theory by determining the underlying primitives that produce what the literature calls a two-sided market and by highlighting how the standard two-sided pricing results are indeed optimal only under very specific conditions. Our work also reveals that platform design choices that impact multi-activity, potentially to over come the chicken-and-egg problem, will also impact optimal pricing.},
  archive      = {J_EJOR},
  author       = {Mohammed Mardan and Mark J. Tremblay},
  doi          = {10.1016/j.ejor.2024.09.038},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {503-515},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal platform pricing with multi-sided users: A direct and indirect network approach},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized likelihood ratio method for stochastic models with uniform random numbers as inputs. <em>EJOR</em>, <em>321</em>(2), 493-502. (<a href='https://doi.org/10.1016/j.ejor.2024.10.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new unbiased stochastic gradient estimator for a family of stochastic models driven by uniform random numbers as inputs. Dropping the requirement that the tails of the density of the input random variables decay smoothly, the estimator extends the applicability of the generalized likelihood ratio (GLR) method. We demonstrate the new estimator for several general classes of input random variates, including independent inverse transform random variates and dependent input random variables governed by an Archimedean copula. We show how the new estimator works in settings such as density estimation, and we illustrate applications to credit risk derivatives. Numerical experiments substantiate broad applicability and flexibility in dealing with discontinuities in the sample performance.},
  archive      = {J_EJOR},
  author       = {Yijie Peng and Michael C. Fu and Jiaqiao Hu and Pierre L’Ecuyer and Bruno Tuffin},
  doi          = {10.1016/j.ejor.2024.10.001},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {493-502},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Generalized likelihood ratio method for stochastic models with uniform random numbers as inputs},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Worst-case distortion riskmetrics and weighted entropy with partial information. <em>EJOR</em>, <em>321</em>(2), 476-492. (<a href='https://doi.org/10.1016/j.ejor.2024.09.047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss the worst-case distortion riskmetrics for general distributions when only partial information (mean and variance) is known. This result is applicable to a general class of distortion risk measures and variability measures. Furthermore, we also consider the worst-case weighted entropy for general distributions when only partial information is available. Specifically, we provide some applications for entropies, weighted entropies and risk measures. The commonly used entropies include Gini functional, cumulative residual entropy, tail-Gini functional, cumulative Tsallis past entropy, extended Gini coefficient, among others. The risk measures contain some premium principles and shortfalls based on entropy. The shortfalls include the Gini shortfall, extended Gini shortfall, shortfall of cumulative residual entropy and shortfall of cumulative residual Tsallis entropy with order α .},
  archive      = {J_EJOR},
  author       = {Baishuai Zuo and Chuancun Yin},
  doi          = {10.1016/j.ejor.2024.09.047},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {476-492},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Worst-case distortion riskmetrics and weighted entropy with partial information},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically optimal routing of a many-server parallel queueing system with long-run average criterion. <em>EJOR</em>, <em>321</em>(2), 462-475. (<a href='https://doi.org/10.1016/j.ejor.2024.09.044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a parallel queueing system with multiple stations, each of which contains many statistically identical servers and has a dedicated queue. Upon each customer arrival, the system manager must decide to which station the customer should be routed, with the objective of minimizing the system’s long-run average delay cost. One feature of this paper is that a customer’s delay cost depends not only on his/her delay, but also on the routed station. Considering this heterogeneity across stations, we propose a routing policy, which can be regarded as an extension of the MED–FSF policy. Under this policy, any arriving customer will be routed to: (i) the station with the minimum value, which depends on the station’s expected delay and the station index when servers in all stations are fully occupied; or otherwise (ii) the station with a fastest idle server. Using asymptotic analysis, we derive diffusion limits of queue-length processes and their stationary distributions under the proposed policy in the Halfin–Whitt regime. Combined with an asymptotic lower bound result for the long-run average delay cost, we show that the proposed routing policy is asymptotically optimal under the considered objective. Finally, we provide numerical experiments to validate the accuracy of our diffusion approximation, and we compare the performance metrics under the proposed policy with those under other commonly used routing policies.},
  archive      = {J_EJOR},
  author       = {Ping Cao and Zhiheng Zhong},
  doi          = {10.1016/j.ejor.2024.09.044},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {462-475},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Asymptotically optimal routing of a many-server parallel queueing system with long-run average criterion},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New and tractable formulations for the eco-driving and the eco-routing-and-driving problems. <em>EJOR</em>, <em>321</em>(2), 445-461. (<a href='https://doi.org/10.1016/j.ejor.2024.10.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eco-driving and eco-routing problems are both concerned with minimizing the fuel consumption of a single vehicle; the former does so by optimizing the vehicle’s speed profile on a given road segment, and the latter selects the best path for a vehicle from a given origin to a given destination. This paper studies a problem that combines the two, namely an eco-routing-and-driving problem, in which the path and the speed profile on each link of the chosen path are optimized simultaneously. Using the comprehensive modal emissions model as the fuel consumption model, this paper describes new and tractable formulations for both the eco-driving and the eco-routing-and-driving problems through discretization, convexification and approximation, leading to a linear program for the former and a mixed-integer linear program for the latter. Computational results indicate that the new formulation of the eco-driving problem yields significant reductions in the solution time as compared to alternative formulations and algorithms, and that the new formulation of the eco-routing-and-driving problem affords further reductions in fuel consumption when compared with other methods.},
  archive      = {J_EJOR},
  author       = {Fuliang Wu and Hongbo Ye and Tolga Bektaş and Ming Dong},
  doi          = {10.1016/j.ejor.2024.10.005},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {445-461},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {New and tractable formulations for the eco-driving and the eco-routing-and-driving problems},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic reordering and inspection for the multi-item inventory record inaccuracy problem. <em>EJOR</em>, <em>321</em>(2), 428-444. (<a href='https://doi.org/10.1016/j.ejor.2024.09.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inventory Record Inaccuracy (IRI) is a significant challenge in inventory management, caused by discrepancies between actual stock and inventory records due to factors such as spoilage, theft, and obsolescence. Despite extensive academic focus on robust ordering and inventory inspection planning, current literature either considers these in isolation, or focuses on stylized single-item scenarios, not addressing the full dynamics of reordering and inspection in warehouses with many SKUs. We consider such a large warehouse with several heterogeneous items that are subject to shrinkage and limited inspection opportunities. Our approach jointly optimizes reordering and inspection decisions in a dynamic decision framework, incorporating both inspection and travel times. We introduce an algorithmic pipeline for dynamic reordering and inspection that combines a neural network to decide on replenishments and a mixed-integer program to determine an optimal inspection subset. Our results show that ignoring shrinkage can increase costs by up to 95.3% and inspection already becomes viable at a relatively low level of IRI. Our proposed policy saves 8.3% in total costs compared to a static reordering and inspection benchmark, and 21.8% compared to a no-inspection policy.},
  archive      = {J_EJOR},
  author       = {Fabian Akkerman and Dennis Prak and Martijn Mes},
  doi          = {10.1016/j.ejor.2024.09.033},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {428-444},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Dynamic reordering and inspection for the multi-item inventory record inaccuracy problem},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicle routing in precooling logistics with dynamic temperature-dependent product quality decay. <em>EJOR</em>, <em>321</em>(2), 407-427. (<a href='https://doi.org/10.1016/j.ejor.2024.09.041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on precooling operations for post-harvest fruits and vegetables in smallholder countries, aiming to provide an effective way to reduce product losses in the early stage of food supply chains. In this study, we consider the traditional centralized precooling and emerging mobile precooling to fulfill a series of small-scale and scattered precooling requests, with the goal of minimizing the total operating cost while guaranteeing the product quality of farmers. The resulting problem is a variant of the classic heterogeneous fleet vehicle routing problems with time windows, with the additional consideration of heterogeneous service and temperature-dependent product quality decay. The vehicle routing model is formulated by integrating product quality as a constraint in which a dedicated function is developed to capture the quality dynamics under changing temperatures. We design an improved adaptive large neighborhood search method to solve this problem by identifying a strategy that is able to deal with the interactions among decisions. Experiment results quantify the advantages of managing product quality in the early stage of supply chains for perishable products and provide management insights on conducting quality-based precooling services. Numerical experiments based on small-scale instances of the studied problem as well as large-scale benchmark instances verify the effectiveness and efficiency of the proposed algorithm by comparing it with CPLEX and three state-of-the-art algorithms.},
  archive      = {J_EJOR},
  author       = {Na Lin and Argyris Kanellopoulos and Renzo Akkerman and Jianghua Zhang and Junhu Ruan},
  doi          = {10.1016/j.ejor.2024.09.041},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {407-427},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Vehicle routing in precooling logistics with dynamic temperature-dependent product quality decay},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimizing total completion time and makespan for a multi-scenario bi-criteria parallel machine scheduling problem. <em>EJOR</em>, <em>321</em>(2), 397-406. (<a href='https://doi.org/10.1016/j.ejor.2024.09.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-criteria scheduling problems under uncertainty remain a relatively unexplored topic in theoretical computer science despite substantial practical interests. This work studies a bi-objective identical parallel machine scheduling problem under uncertainty, in which the first objective is to minimize the total completion time, and the second is to minimize the makespan. Especially a job’s processing time is assumed to be represented by a polynomial function with respect to scenario u ∈ U , where U ⊂ R + is an interval containing an infinite number of scenarios. In this work, we are looking for a compact and complete description of the set of possibly optimal solutions, along with their objective function values, over the set of scenarios or an approximation with a performance guarantee. First, to better understand the characteristics of the studied problem, we consider two single-objective problems: parallel machine scheduling problem under uncertainty with the total completion time and makespan criterion, respectively. For the problem with the total completion time criterion, we demonstrate that the set of possibly optimal schedules can be found in polynomial time. In contrast, for the problem with the makespan criterion, we provide a ( 1 + ϵ ) -approximation algorithm. For the bi-objective problem, we provide a 2-approximation algorithm for any number of parallel machines and a ( 1 + ϵ ) -approximation algorithm where a fixed number of parallel machines is considered.},
  archive      = {J_EJOR},
  author       = {Xiechen Zhang and Eric Angel and Feng Chu and Damien Regnault},
  doi          = {10.1016/j.ejor.2024.09.032},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {397-406},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Minimizing total completion time and makespan for a multi-scenario bi-criteria parallel machine scheduling problem},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Divide-and-conquer initialization and mutation operators for the large-scale mixed capacitated arc routing problem. <em>EJOR</em>, <em>321</em>(2), 383-396. (<a href='https://doi.org/10.1016/j.ejor.2024.09.043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As cities continue to grow, thus will the size of the routing problems necessary to the functioning of all cities. Applications such as waste collection, road maintenance, or winter gritting span over an entire city, therefore algorithms must be designed to solve large-scale problems. The Capacitated Arc Routing Problem (CARP) is an important combinatorial optimization problem that is typically used to model these applications. Classical algorithms for CARP struggle to find quality solutions for large-scale instances with thousands of services within a reasonable computational budget. To address the issue of scalability, several divide-and-conquer heuristics have recently been proposed. In this paper, we propose to integrate divide-and-conquer heuristics into a memetic algorithm by adapting these as an initialization method and as a mutation operator. The resulting algorithm, which we call Memetic Algorithm with Divide-and-Conquer Mutation (MADCoM), outperforms state-of-the-art algorithms on large-scale instances and new best solutions are found for 17 instances of MCARP, 2 of which are optimal solutions, and for 23 large-scale CARP instances. These results demonstrate the potential of the integration of divide-and-conquer heuristics into metaheuristics as a strategy to efficiently solve large-scale problems.},
  archive      = {J_EJOR},
  author       = {Diogo F. Oliveira and Miguel S.E. Martins and João M.C. Sousa and Susana M. Vieira and José Rui Figueira},
  doi          = {10.1016/j.ejor.2024.09.043},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {383-396},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Divide-and-conquer initialization and mutation operators for the large-scale mixed capacitated arc routing problem},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A tight formulation for the dial-a-ride problem. <em>EJOR</em>, <em>321</em>(2), 363-382. (<a href='https://doi.org/10.1016/j.ejor.2024.09.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridepooling services play an increasingly important role in modern transportation systems. With soaring demand and growing fleet sizes, the underlying route planning problems become increasingly challenging. In this context, we consider the dial-a-ride problem (DARP): Given a set of transportation requests with pick-up and delivery locations, passenger numbers, time windows, and maximum ride times, an optimal routing for a fleet of vehicles, including an optimized passenger assignment, needs to be determined. We present tight mixed-integer linear programming (MILP) formulations for the DARP by combining two state-of-the-art models into novel location-augmented-event-based formulations. Strong valid inequalities and lower and upper bounding techniques are derived to further improve the formulations. We then demonstrate the theoretical and computational superiority of the new models: First, the linear programming relaxations of the new formulations are stronger than existing location-based approaches. Second, extensive numerical experiments on benchmark instances show that computational times are on average reduced by 53.9% compared to state-of-the-art event-based approaches.},
  archive      = {J_EJOR},
  author       = {Daniela Gaul and Kathrin Klamroth and Christian Pfeiffer and Michael Stiglmayr and Arne Schulz},
  doi          = {10.1016/j.ejor.2024.09.028},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {363-382},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A tight formulation for the dial-a-ride problem},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years of metaheuristics. <em>EJOR</em>, <em>321</em>(2), 345-362. (<a href='https://doi.org/10.1016/j.ejor.2024.04.004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we review the milestones in the development of heuristic methods for optimization over the last 50 years. We propose a critical analysis of the main findings and contributions, mainly from a European perspective. Starting with the roots of the area that can be traced back to the classical philosophers, we follow the historical path of heuristics and metaheuristics in the field of operations research and list the main milestones, up to the latest proposals to hybridize metaheuristics with machine learning. We pay special attention to the theories that changed our way of thinking about problem solving, and to the role played by the European Journal of Operational Research in the development of these theories. Our approach emphasizes methodologies and their connections with related areas, which permits to identify potential lines of future research.},
  archive      = {J_EJOR},
  author       = {Rafael Martí and Marc Sevaux and Kenneth Sörensen},
  doi          = {10.1016/j.ejor.2024.04.004},
  journal      = {European Journal of Operational Research},
  month        = {3},
  number       = {2},
  pages        = {345-362},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of metaheuristics},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The demand for hedging of oil producers: A tale of risk and regret. <em>EJOR</em>, <em>321</em>(1), 330-343. (<a href='https://doi.org/10.1016/j.ejor.2024.09.036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rationalizing the relatively low levels of hedging observed in the oil market, compared to those predicted by pure risk minimization, has proven difficult. This article examines whether the objectives of oil producers can explain this discrepancy. From a theoretical perspective, it appears that the observed level of hedging is well explained by risk averse producers who also exhibit regret aversion towards potential losses in the derivatives market. When applying our models to the data, we find that regret effectively rationalizes producers' under-hedging and its persistence. Our results suggest that neither ambiguity surrounding basis risk, nor prospect theory can account for this behavior. Lastly, our findings indicate that relaxing the assumption of market completeness and considering quantity risk also fail to match the observed hedging activity of oil producers.},
  archive      = {J_EJOR},
  author       = {Samuel Ouzan and Pierre Six},
  doi          = {10.1016/j.ejor.2024.09.036},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {330-343},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The demand for hedging of oil producers: A tale of risk and regret},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stabilizing financial networks via mergers and acquisitions. <em>EJOR</em>, <em>321</em>(1), 314-329. (<a href='https://doi.org/10.1016/j.ejor.2024.09.035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A bi-level model is proposed to explore efficient policies for supporting negotiations on financial crisis resolution. In a principal–agent framework, this model minimizes the welfare loss function of a central authority (social planner, SP) through the simultaneous choice of subsidy levels and potential pairs of banks to merge. The SP’s choice of mergers needs to be incentive-compatible with the autonomous choices of banks, and the evaluation of the financial network must adhere to standard accounting principles. Incentive compatibility is enforced by two options for conditions, based on stable matching or competitive bidding. For the evaluation of the financial network, we employ an extended Eisenberg–Noe clearing payment equilibrium by considering bankruptcy costs and the seniority levels of liabilities. Additionally, liabilities are not cleared among solvent banks, and corporate bonds may be used for clearing payments. The bi-level model specifies conditions for the clearing equilibrium. For demonstration, we use major European banks, and a scenario linked to the adverse economic conditions used in the 2016 EU-wide stress testing.},
  archive      = {J_EJOR},
  author       = {Markku Kallio and Aein Khabazian},
  doi          = {10.1016/j.ejor.2024.09.035},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {314-329},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Stabilizing financial networks via mergers and acquisitions},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Existence of equilibrium in a dynamic supply chain game with vertical coordination, horizontal competition, and complementary goods. <em>EJOR</em>, <em>321</em>(1), 302-313. (<a href='https://doi.org/10.1016/j.ejor.2024.09.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider supply chain competition and vertical coordination in a linear–quadratic differential game setting. In this setting, supply chains produce complementary goods and each of them includes a single manufacturer and a single retailer who coordinate their decisions through a revenue-sharing contract with a wholesale price and a fixed sales revenue share. We study a multiple leader-follower Stackelberg game where the manufacturers are the leaders and the retailers are the followers. Competition occurs at both levels of the supply chains. Retailers play Nash and compete in price; manufacturers also play Nash but they compete in choosing their production capacities by exploiting the equilibrium price decisions made by the retailers. We show that open-loop Nash equilibria exist when the manufacturers only receive a wholesale price (there are no longer exploiting the equilibrium price decision made by the retailers, however). When the manufacturers receive both a wholesale-price and a share of the retailers’ sales revenues, equilibria generally no longer exist. The non-existence of an equilibrium stems from the fact that the manufacturers’ instant profits are discontinuous functions of their production capacities. This discontinuity leads to a major technical difficulty in that one cannot apply standard optimal control approaches to study the equilibria of the dynamic game. Our results illustrate the possibility that competition between supply chains might not be sustainable when they sell complementary products and rely on a revenue-sharing agreement.},
  archive      = {J_EJOR},
  author       = {Bertrand Crettez and Naila Hayek and Guiomar Martín-Herrán},
  doi          = {10.1016/j.ejor.2024.09.027},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {302-313},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Existence of equilibrium in a dynamic supply chain game with vertical coordination, horizontal competition, and complementary goods},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable profit-driven hotel booking cancellation prediction based on heterogeneous stacking-based ensemble classification. <em>EJOR</em>, <em>321</em>(1), 284-301. (<a href='https://doi.org/10.1016/j.ejor.2024.08.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of hotel booking cancellation prediction in the hospitality industry is to identify potential cancellations from a large customer base and improve the efficiency of customer retention and capacity management efforts. Whilst prior research has shown that the predictive performance of hotel booking cancellation prediction can be further enhanced by integrating multiple classifiers, the explainability of such models is limited due to low interpretability and limited alignment with company goals. To address this limitation, we propose a novel heterogeneous linear stacking ensemble classifier for profit-driven hotel booking cancellation prediction. It enhances classifier explainability by (1) making models more accountable by axing model training towards profitability and (2) complementing models by global post-hoc model interpretation strategies. Through experiments based on real-world datasets, our proposed classification framework is demonstrated to lead to greater profits than other profit-oriented predictive models. Moreover, an in-depth interpretability analysis demonstrates the framework's ability to identify critical factors significantly impacting hotel cancellations, providing valuable insights for retention campaigns.},
  archive      = {J_EJOR},
  author       = {Zhenkun Liu and Koen W. De Bock and Lifang Zhang},
  doi          = {10.1016/j.ejor.2024.08.026},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {284-301},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Explainable profit-driven hotel booking cancellation prediction based on heterogeneous stacking-based ensemble classification},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measures of stochastic non-dominance in portfolio optimization. <em>EJOR</em>, <em>321</em>(1), 269-283. (<a href='https://doi.org/10.1016/j.ejor.2024.08.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic dominance rules are well-characterized and widely used. This work aims to describe and better understand the situations when they do not hold by developing measures of stochastic non-dominance. They quantify the error caused by assuming that one random variable dominates another one when it does not. To calculate them, we search for a hypothetical random variable that satisfies the stochastic dominance relationship and is as close to the original one as possible. The Wasserstein distance between the optimal hypothetical random variable and the original one is considered as the measure of stochastic non-dominance. Depending on the conditions imposed on the probability distribution of the hypothetical random variable, we distinguish between general and specific measures of stochastic non-dominance. We derive their exact values for random variables with uniform, normal, and exponential distributions. We present relations to almost first-order stochastic dominance and to tractable almost stochastic dominance. Using monthly returns of twelve assets captured by the German stock index DAX, we solve portfolio optimization problems with the first-order and second-order stochastic dominance constraints. The measures of stochastic non-dominance allow us to compare the optimal portfolios with respect to different orders of stochastic dominance from a new angle. We also defined the closest dominating and closest approximately dominating portfolios. They brought a better understanding of the relationship between the two types of optimal portfolios. Using moving window analysis, the relationship of the in-sample measure of stochastic non-dominance to out-of-sample performance was studied, too.},
  archive      = {J_EJOR},
  author       = {Jana Junová and Miloš Kopa},
  doi          = {10.1016/j.ejor.2024.08.029},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {269-283},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Measures of stochastic non-dominance in portfolio optimization},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Industry return prediction via interpretable deep learning. <em>EJOR</em>, <em>321</em>(1), 257-268. (<a href='https://doi.org/10.1016/j.ejor.2024.08.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We apply an interpretable machine learning model, the LassoNet, to forecast and trade U.S. industry portfolio returns. The model combines a regularization mechanism with a neural network architecture. A cooperative game-theoretic algorithm is also applied to interpret our findings. The latter hierarchizes the covariates based on their contribution to the overall model performance. Our findings reveal that the LassoNet outperforms various linear and nonlinear benchmarks concerning out-of-sample forecasting accuracy and provides economically meaningful and profitable predictions. Valuation ratios are the most crucial covariates, followed by individual and cross-industry lagged returns. The constructed industry ETF portfolios attain positive Sharpe ratios and positive and statistically significant alphas, surviving even transaction costs.},
  archive      = {J_EJOR},
  author       = {Lazaros Zografopoulos and Maria Chiara Iannino and Ioannis Psaradellis and Georgios Sermpinis},
  doi          = {10.1016/j.ejor.2024.08.032},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {257-268},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Industry return prediction via interpretable deep learning},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integration of prediction and optimization for smart stock portfolio selection. <em>EJOR</em>, <em>321</em>(1), 243-256. (<a href='https://doi.org/10.1016/j.ejor.2024.08.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) algorithms pose significant challenges in predicting unknown parameters for optimization models in decision-making scenarios. Conventionally, prediction models are optimized independently in decision-making processes, whereas ML algorithms primarily focus on minimizing prediction errors, neglecting the role of decision-making in downstream optimization tasks. The pursuit of high prediction accuracy may not always align with the goal of reducing decision errors. The idea of reducing decision errors has been proposed to address this limitation. This paper introduces an optimization process that integrates predictive regression models within a mean–variance optimization setting. This innovative technique introduces a general loss function to capture decision errors. Consequently, the predictive model not only focuses on forecasting unknown optimization parameters but also emphasizes the predicted values that minimize decision errors. This approach prioritizes decision accuracy over the potential accuracy of unknown parameter prediction. In contrast to traditional ML approaches that minimize standard loss functions such as mean squared error, our proposed model seeks to minimize the objective value derived directly from the decision-making problem. Furthermore, this strategy is validated by developing an optimization-based regression tree model for predicting stock returns and reducing decision errors. Empirical evaluations of our framework reveal its superiority over conventional regression tree methods, demonstrating enhanced decision quality. The computational experiments are conducted on a stock market dataset to compare the effectiveness of the proposed framework with the conventional regression tree-based approach. Remarkably, the results confirm the strengths inherent in this holistic approach.},
  archive      = {J_EJOR},
  author       = {Puja Sarkar and Vivekanand B. Khanapuri and Manoj Kumar Tiwari},
  doi          = {10.1016/j.ejor.2024.08.027},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {243-256},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integration of prediction and optimization for smart stock portfolio selection},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal reinsurance with multivariate risks and dependence uncertainty. <em>EJOR</em>, <em>321</em>(1), 231-242. (<a href='https://doi.org/10.1016/j.ejor.2024.09.037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the optimal reinsurance design from the perspective of an insurer with multiple lines of business, where the reinsurance is purchased by the insurer for each line of business, respectively. For the risk vector generated by the multiple lines of business, we suppose that the marginal distributions are fixed, but the dependence structure between these risks is unknown. Due to the unknown dependence structure, the optimal strategy is investigated for the worst-case scenario. We consider two types of risk measures: Value-at-Risk ( VaR ) and Range-Value-at-Risk (RVaR) including Expected Shortfall ( ES ) as a special case, and general premium principles satisfying certain conditions. To be more practical, the minimization of the total risk is conducted under some budget constraint. For the VaR -based model with only two risks, it turns out that the limited stop-loss reinsurance treaty is optimal for each line of business. For the model with more than two risks, we obtain two types of optimal reinsurance strategies if the marginals have convex or concave distributions on their tail parts by constraining the ceded loss functions to be convex or concave. Moreover, as a special case, the optimal quota-share reinsurance with dependence uncertainty has been studied. Finally, after applying our findings to two risks, some studies have been implemented to obtain both the analytical and numerical optimal reinsurance policies.},
  archive      = {J_EJOR},
  author       = {Tolulope Fadina and Junlei Hu and Peng Liu and Yi Xia},
  doi          = {10.1016/j.ejor.2024.09.037},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {231-242},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal reinsurance with multivariate risks and dependence uncertainty},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive robust online portfolio selection. <em>EJOR</em>, <em>321</em>(1), 214-230. (<a href='https://doi.org/10.1016/j.ejor.2024.09.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The online portfolio selection (OLPS) problem differs from classical portfolio model problems, as it involves making sequential investment decisions. Many OLPS strategies described in the literature capture market movement based on various beliefs and are shown to be profitable. In this paper, we propose a robust optimization (RO)-based strategy that takes transaction costs into account. Moreover, unlike existing studies that calibrate model parameters from benchmark data sets, we develop a novel adaptive scheme that decides the parameters sequentially. With a wide range of parameters as input, our scheme captures market uptrend and protects against market downtrend while controlling trading frequency to avoid excessive transaction costs. We numerically demonstrate the advantages of our adaptive scheme against several benchmarks under various settings. Our adaptive scheme may also be useful in general sequential decision-making problems. Finally, we compare the performance of our strategy with that of existing OLPS strategies using both benchmark and newly collected data sets. Our strategy outperforms these existing OLPS strategies in terms of cumulative returns and competitive Sharpe ratios across diversified data sets, demonstrating its adaptability-driven superiority.},
  archive      = {J_EJOR},
  author       = {Man Yiu Tsang and Tony Sit and Hoi Ying Wong},
  doi          = {10.1016/j.ejor.2024.09.002},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {214-230},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Adaptive robust online portfolio selection},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An indifference result for social choice rules in large societies. <em>EJOR</em>, <em>321</em>(1), 208-213. (<a href='https://doi.org/10.1016/j.ejor.2024.09.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social choice rules can be defined or derived by minimizing distance-based objective functions. One problem with this approach is that any social choice rule can be derived by selecting an appropriate distance function. Another problem comes from the computational difficulty of determining the solution of some social choice rules. We provide a general positive indifference result when looking at expected average distances by showing that on ‘average’ each social choice rule performs equally well with respect to a very large class of distance functions if the number of voters is large. Our result applies also to the frequently employed Kendall τ , Spearman rank correlation and Spearman footrule ‘distance functions’.},
  archive      = {J_EJOR},
  author       = {Dezső Bednay and Balázs Fleiner and Attila Tasnádi},
  doi          = {10.1016/j.ejor.2024.09.018},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {208-213},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An indifference result for social choice rules in large societies},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially adaptive multistage stochastic programming. <em>EJOR</em>, <em>321</em>(1), 192-207. (<a href='https://doi.org/10.1016/j.ejor.2024.09.034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistage stochastic programming is a powerful tool allowing decision-makers to revise their decisions at each stage based on the realized uncertainty. However, organizations are not able to be fully flexible, as decisions cannot be revised too frequently in practice. Consequently, decision commitment becomes crucial to ensure that initially made decisions remain unchanged for a certain period of time. This paper introduces partially adaptive multistage stochastic programming, a new optimization paradigm that strikes an optimal balance between decision flexibility and commitment by determining the best stages to revise decisions depending on the allowed level of flexibility. We introduce a novel mathematical formulation and theoretical properties eliminating certain constraint sets. Furthermore, we develop a decomposition method that effectively handles mixed-integer partially adaptive multistage programs by adapting the integer L-shaped method and Benders decomposition. Computational experiments on stochastic lot-sizing and generation expansion planning problems show substantial advantages attained through optimal selections of revision times when flexibility is limited, while demonstrating computational efficiency attained by employing the proposed properties and solution methodology. By adhering to these optimal revision times, organizations can achieve performance levels comparable to fully flexible settings.},
  archive      = {J_EJOR},
  author       = {Sezen Ece Kayacık and Beste Basciftci and Albert H. Schrotenboer and Evrim Ursavas},
  doi          = {10.1016/j.ejor.2024.09.034},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {192-207},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Partially adaptive multistage stochastic programming},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven dynamic police patrolling: An efficient monte carlo tree search. <em>EJOR</em>, <em>321</em>(1), 177-191. (<a href='https://doi.org/10.1016/j.ejor.2024.09.019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crime is responsible for major financial losses and serious harm to the well-being of individuals, and, hence, a crucial task of police operations is effective patrolling. Yet, in existing decision models aimed at police operations, microscopic routing decisions from patrolling are not considered, and, furthermore, the objective is limited to surrogate metrics (e. g., response time) instead of crime prevention. In this paper, we thus formalize the decision problem of dynamic police patrolling as a Markov decision process that models microscopic routing decisions, so that the expected number of prevented crimes are maximized. We experimentally show that standard solution approaches for our decision problem are not scalable to real-world settings. As a remedy, we present a tailored and highly efficient Monte Carlo tree search algorithm. We then demonstrate our algorithm numerically using real-world crime data from Chicago and show that the decision-making by our algorithm offers significant improvements for crime prevention over patrolling tactics from current practice. Informed by our results, we finally discuss implications for improving the patrolling tactics in police operations.},
  archive      = {J_EJOR},
  author       = {Daniel Tschernutter and Stefan Feuerriegel},
  doi          = {10.1016/j.ejor.2024.09.019},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {177-191},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Data-driven dynamic police patrolling: An efficient monte carlo tree search},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing sequential decision-making under risk: Strategic allocation with switching penalties. <em>EJOR</em>, <em>321</em>(1), 160-176. (<a href='https://doi.org/10.1016/j.ejor.2024.09.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the multiarmed bandit (MAB) problem augmented with a critical real-world consideration: the cost implications of switching decisions. Our work distinguishes itself by addressing the largely unexplored domain of risk-averse MAB problems compounded by switching penalties. Such scenarios are not just theoretical constructs but are reflective of numerous practical applications. Our contribution is threefold: firstly, we explore how switching costs and risk aversion influence decision-making in MAB problems. Secondly, we present novel theoretical results, including the development of the Risk-Averse Switching Index (RASI), which addresses the dual challenges of risk aversion and switching costs, demonstrating its near-optimal efficacy. This heuristic solution method is grounded in dynamic coherent risk measures, enabling a time-consistent evaluation of risk and reward. Lastly, through rigorous numerical experiments, we validate our algorithm’s effectiveness and practical applicability, providing decision-makers with valuable insights and tools for navigating the multifaceted landscape of risk-averse environments with inherent switching costs.},
  archive      = {J_EJOR},
  author       = {Milad Malekipirbazari},
  doi          = {10.1016/j.ejor.2024.09.023},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {160-176},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing sequential decision-making under risk: Strategic allocation with switching penalties},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic branch and bound considering stochastic constraints. <em>EJOR</em>, <em>321</em>(1), 147-159. (<a href='https://doi.org/10.1016/j.ejor.2024.09.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate a simulation optimization problem that poses challenges due to (i) the inability to evaluate the objective and multiple constraint functions analytically; instead, we rely on stochastic simulation to estimate them, and (ii) a discrete and potentially vast solution space. Rather than providing a single optimal solution, our aim is to identify a set of near-optimal solutions within a specific quantile, such as the top 10%. Our investigation covers two different problem settings or frameworks. The first framework is focused solely on a stochastic objective function, disregarding any stochastic constraints. In this context, we propose employing a probabilistic branch-and-bound algorithm to discover a level set of solutions. Alternatively, the second framework involves stochastic constraints. To address such stochastically constrained problems, we utilize a penalty function methodology in conjunction with a probabilistic branch-and-bound algorithm. Furthermore, we establish a convergence analysis of both algorithms to demonstrate their asymptotic validity and highlight their theoretical properties and behavior. Our experimental results provide evidence of the efficiency of our proposed algorithms, showing that they outperform existing approaches in the field of simulation optimization.},
  archive      = {J_EJOR},
  author       = {Hao Huang and Shing Chih Tsai and Chuljin Park},
  doi          = {10.1016/j.ejor.2024.09.016},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {147-159},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Probabilistic branch and bound considering stochastic constraints},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An exact method for the two-echelon split-delivery vehicle routing problem for liquefied natural gas delivery with the boil-off phenomenon. <em>EJOR</em>, <em>321</em>(1), 123-146. (<a href='https://doi.org/10.1016/j.ejor.2024.09.040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we investigate a two-echelon vehicle routing problem for liquefied natural gas (LNG) delivery to determine how to transport LNG from an overseas production terminal to a set of import terminals by vessels, and transport the LNG from the import terminals to a set of filling stations either by tanker trucks or bunker barges. Some important features of this problem are that part of LNG will be evaporated during delivery and split deliveries are allowed at both import terminals and filling stations, which render the problem more intractable than those considered in most of the existing two-echelon vehicle routing studies. The objective is to find the optimal first-echelon and second-echelon delivery schemes to minimize the sum of the routing cost and boil-off cost. To solve the problem, we develop a customized branch-and-price-and-cut (BPC) algorithm incorporating a specialized labeling algorithm tailored to address the issues of LNG evaporation and split deliveries in solving the challenging pricing subproblems. To speed up the solution algorithm, we introduce some heuristic pricing strategies to quickly solve the pricing subproblems, and explore the (strong) k -path inequalities and subset-row inequalities to tighten the lower bound obtained by column generation. We conduct extensive numerical studies on simulation instances and a case study of LNG delivery in region along the Yangtze river, China to verify the effectiveness of the model and proposed algorithm. The numerical results demonstrate that our algorithm significantly outperforms CPLEX and the existing BPC algorithm on related topic, confirm the superiority of our integrated two-echelon solution method over its sequential solution counterpart, and illustrate that the locations of the production terminal and import terminals are highly related to the solution performance.},
  archive      = {J_EJOR},
  author       = {Xiaoyun Xiong and Jialin Han and Yunqiang Yin and T.C.E. Cheng},
  doi          = {10.1016/j.ejor.2024.09.040},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {123-146},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An exact method for the two-echelon split-delivery vehicle routing problem for liquefied natural gas delivery with the boil-off phenomenon},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evaluation of common modeling choices for the vehicle routing problem with stochastic demands. <em>EJOR</em>, <em>321</em>(1), 107-122. (<a href='https://doi.org/10.1016/j.ejor.2024.09.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate three common modeling choices for the vehicle routing problem with stochastic demands: (i) the total expected demand of customers on a route may not exceed the capacity of the vehicle, (ii) the number of routes is fixed, and (iii) demand is distributed with a support that contains negative-valued realizations. We prove that modeling choices (i) and (ii) result in an arbitrarily large increase of the optimal objective value in the worst case. Additionally, we provide lower and upper bounds on the change of the optimal objective value following from (iii) in case the actual distribution of demand is censored, truncated or folded. We also evaluate the consequences of these choices numerically, by employing a state-of-the-art integer L -shaped method to solve the vehicle routing problem with stochastic demands to optimality, which we modify to deal with the alternative choices. We find that restricting the expected demand of a route to the vehicle’s capacity has a limited effect on the optimal objective value for most, but not all, benchmark instances from the literature, while drastically reducing the computation times of the integer L -shaped method. When restricting the number of routes, a similar effect occurs when the total expected demand on a route is not restricted. Otherwise, the computation time decreases only slightly, and even increases for some benchmark instances. For instances from the literature, despite admitting negative realizations, the normal distributions used to model demand are an adequate approximation for censored, truncated and folded normal distributions that have nonnegative supports.},
  archive      = {J_EJOR},
  author       = {Y.N. Hoogendoorn and R. Spliet},
  doi          = {10.1016/j.ejor.2024.09.007},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {107-122},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An evaluation of common modeling choices for the vehicle routing problem with stochastic demands},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fleet repositioning in the tramp ship routing and scheduling problem with bunker optimization: A matheuristic solution approach. <em>EJOR</em>, <em>321</em>(1), 88-106. (<a href='https://doi.org/10.1016/j.ejor.2024.09.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates an important planning problem faced by dry bulk shipping operators, referred to as the Tramp Ship Routing and Scheduling Problem with Bunker Optimization (TSRSPBO). The problem is to maximize the overall profit of a fleet of vessels by selecting cargoes and determining ship routes and schedules. We consider this problem under a set of practically relevant features such as flexibility in cargo quantities, as well as bunkering decisions on where to procure fuel and how much. As a particularly novel feature, we address the regional allocation of vessels at the end of the planning period to be well prepared for meeting (uncertain) future demand. To incorporate this, we consider the TSRSPBO as a two-stage stochastic programming problem, where cargo selection, routing, and bunkering decisions are solved in the first-stage problem, and the recourse cost of fleet repositioning is considered in the second stage. We present arc flow and path flow formulations, where the latter employs a priori generation of feasible routes as input. For solving realistically sized instances, we propose a matheuristic based on an Adaptive Large Neighborhood Search (ALNS) framework that iteratively generates columns and solves the path flow model. Computational experiments based on real data show that this matheuristic finds high-quality solutions for large test instances with 120 cargoes, 30 vessels, and ten bunker ports in less than one hour. Also, considering the TSRSPBO as a two-stage stochastic problem achieves the highest profits and is solved almost as quickly as the deterministic problem variant.},
  archive      = {J_EJOR},
  author       = {Simen Omholt-Jensen and Kjetil Fagerholt and Frank Meisel},
  doi          = {10.1016/j.ejor.2024.09.029},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {88-106},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fleet repositioning in the tramp ship routing and scheduling problem with bunker optimization: A matheuristic solution approach},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The set team orienteering problem. <em>EJOR</em>, <em>321</em>(1), 75-87. (<a href='https://doi.org/10.1016/j.ejor.2024.09.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the Set Team Orienteering Problem (STOP), a generalised variant of the Set Orienteering Problem (SOP), in which customer locations are split into multiple clusters (or groups). Each cluster is associated with a profit that can be gained only if at least one customer from the cluster is visited. There is a fleet of homogeneous vehicles at a depot, and each vehicle has a limited travel time. The goal of the STOP is to find a set of feasible vehicle routes to collect the maximum profit. We first formulate the problem as a Mixed Integer Linear Programming (MILP) to mathematically describe it. A branch-and-price (B&P) algorithm is then developed to solve the problem to optimality. To deal with large instances, we propose a Large Neighbourhood Search (LNS), which relies on problem-tailored solution representation, removal, and insertion operators. Multiple experiments on newly generated instances confirm the performance of our approaches. Remarkably, when tested on the SOP using benchmarks available in the literature, our B&P method achieves optimality in 61.9% of these instances. This is the first time such a large number of SOP instances are solved to optimality. Our LNS outperforms existing algorithms proposed to solve the SOP in terms of solution quality. Out of 612 considered instances, it improves 40 best-known solutions.},
  archive      = {J_EJOR},
  author       = {Tat Dat Nguyen and Rafael Martinelli and Quang Anh Pham and Minh Hoàng Hà},
  doi          = {10.1016/j.ejor.2024.09.021},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {75-87},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The set team orienteering problem},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mathematical models based on decision hypergraphs for designing a storage cabinet. <em>EJOR</em>, <em>321</em>(1), 57-74. (<a href='https://doi.org/10.1016/j.ejor.2024.09.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of designing a cabinet made up of a set of shelves that contain compartments whose contents slide forward on opening. Considering a set of items candidate to be stored in the cabinet over a given time horizon, the problem is to design a set of shelves and a set of compartments on each shelf, and select the items to insert into the compartments. The objective is to maximize the sum of the profits of the selected items. We call our problem the Storage Cabinet Physical Design (SCPD) problem. The SCPD problem combines a two-staged two-dimensional knapsack problem for designing the shelves and compartments with a set of temporal knapsack problems for selecting and assigning items to compartments. We formalize the SCPD problem and formulate it as a maximum cost flow problem in a decision hypergraph with additional linear constraints. To reduce the size of this model, we break symmetries, generalize graph compression techniques and exploit dominance rules for precomputing subproblem solutions. We also present a set of valid inequalities to improve the linear relaxation of the model. We empirically show that solving the arc-flow model with our enhancements outperforms solving a compact mixed integer linear programming formulation of the SCPD problem.},
  archive      = {J_EJOR},
  author       = {Luis Marques and François Clautiaux and Aurélien Froger},
  doi          = {10.1016/j.ejor.2024.09.022},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {57-74},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Mathematical models based on decision hypergraphs for designing a storage cabinet},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The circular balancing problem. <em>EJOR</em>, <em>321</em>(1), 41-56. (<a href='https://doi.org/10.1016/j.ejor.2024.08.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a balancing problem with a minmax objective in a circular setting. This balancing problem involves the arrangement of an even number of items with different weights on a circle while minimizing the maximum total weight of items arranged on any half circle. Due to its generic structure, it may have applications in fair resource allocation schemes. We show the NP-hardness of the problem and develop polynomial-time algorithms when the number of distinct weights is a fixed constant. We propose for the general case a tight 7 / 6 -approximation algorithm and show that it performs better than two existing algorithms designed for an equivalent problem in the literature. The worst-case performance ratio is derived through a linear combination of valid inequalities that are obtained from the problem definition, the properties of the proposed algorithm, and the optimal circular permutation structure. Furthermore, we formulate a more general problem of minimizing the maximum total weight of items on equally divided circular sectors and present its computational complexity and a tight approximation algorithm.},
  archive      = {J_EJOR},
  author       = {Myungho Lee and Kangbok Lee and Michael Pinedo},
  doi          = {10.1016/j.ejor.2024.08.020},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {41-56},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The circular balancing problem},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surgery scheduling in flexible operating rooms by using a convex surrogate model of second-stage costs. <em>EJOR</em>, <em>321</em>(1), 23-40. (<a href='https://doi.org/10.1016/j.ejor.2024.07.036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the elective surgery planning problem in a hospital with operating rooms shared by elective and emergency patients. This problem is split in two distinct phases. First, a subset of patients to be operated in the next planning period is selected and the selected patients are assigned to a block and a tentative starting time. Then, in the online phase of the problem, a policy decides how to insert the emergency patients in the schedule and may cancel planned surgeries. The overall goal is to minimize the expectation of a cost function representing the assignment of patient to blocks, case cancellations, overtime, waiting time and idle time. We model the offline problem by a two-stage stochastic program, and show that the optimal second-stage costs can be approximated by a convex piecewise linear surrogate model that can be computed in a preprocessing step. This results in a mixed integer program which can be solved very fast, even for large instances of the problem. We also describe a greedy policy for the online phase of the problem, and analyze the performance of our approach by comparing it to both heuristic methods or approaches relying on sampling average approximation (SAA) on a large set of benchmarking instances. Our simulations indicate that our approach can reduce the expected costs by as much as 30% compared to heuristic methods and it can solve problems with 1000 patients in about one minute, while SAA-approaches fail to obtain good solutions within 30 min on small instances.},
  archive      = {J_EJOR},
  author       = {Mohammed Majthoub Almoghrabi and Guillaume Sagnol},
  doi          = {10.1016/j.ejor.2024.07.036},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {23-40},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Surgery scheduling in flexible operating rooms by using a convex surrogate model of second-stage costs},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biased random-key genetic algorithms: A review. <em>EJOR</em>, <em>321</em>(1), 1-22. (<a href='https://doi.org/10.1016/j.ejor.2024.03.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is a comprehensive literature review of Biased Random-Key Genetic Algorithms (BRKGA). BRKGA is a metaheuristic that employs random-key-based chromosomes with biased, uniform, and elitist mating strategies in a genetic algorithm framework. The review encompasses over 150 papers with a wide range of applications, including classical combinatorial optimization problems, real-world industrial use cases, and non-orthodox applications such as neural network hyperparameter tuning in machine learning. Scheduling is by far the most prevalent application area in this review, followed by network design and location problems. The most frequent hybridization method employed is local search, and new features aim to increase population diversity. We also detail challenges and future directions for this method. Overall, this survey provides a comprehensive overview of the BRKGA metaheuristic and its applications and highlights important areas for future research.},
  archive      = {J_EJOR},
  author       = {Mariana A. Londe and Luciana S. Pessoa and Carlos E. Andrade and Mauricio G.C. Resende},
  doi          = {10.1016/j.ejor.2024.03.030},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Biased random-key genetic algorithms: A review},
  volume       = {321},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing the multiplicity of optimal solutions to the clonal deconvolution and evolution problem. <em>EJOR</em>, <em>320</em>(3), 777-788. (<a href='https://doi.org/10.1016/j.ejor.2024.09.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Clonal Deconvolution and Evolution Problem consists on unraveling the clonal structure and phylogeny of a tumor using estimated mutation frequency values obtained from multiple biopsies containing mixtures of tumor clones. In this article, we tackle the problem from an optimization perspective and we explore the number of optimal solutions for a given instance. Even in ideal scenarios without noise, we demonstrate that the Clonal Deconvolution and Evolution Problem is highly under-determined, leading to multiple solutions. Through a comprehensive analysis, we examine the factors contributing to the multiplicity of solutions. We find that as the number of samples increases, the number of optimal solutions decreases. Additionally, we explore how this phenomenon operates across various tumor topology scenarios. To address the issue of the existence of multiple solutions, we present sufficient conditions under which the problem can have a unique solution, and we propose a linear programming-based algorithm that leverages mutation orderings to generate instances with a single solution for a given topology. This algorithm encounters numerical challenges when applied to large instance sizes so, to overcome this, we propose a heuristic adaptation that enables the algorithm’s use for instances of any size.},
  archive      = {J_EJOR},
  author       = {Maitena Tellaetxe-Abete and Charles Lawrie and Borja Calvo},
  doi          = {10.1016/j.ejor.2024.09.006},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {777-788},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Addressing the multiplicity of optimal solutions to the clonal deconvolution and evolution problem},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Portfolio default losses driven by idiosyncratic risks. <em>EJOR</em>, <em>320</em>(3), 765-776. (<a href='https://doi.org/10.1016/j.ejor.2024.08.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a portfolio of general defaultable assets with low individual default risk and study the probability of the portfolio default loss exceeding an arbitrary threshold. The latent variables driving defaults are modeled by a mixture structure that combines common shock, systematic risk, and idiosyncratic risk factors. While common shocks and systematic risk have been found by many studies to contribute significantly to portfolio losses, the role of idiosyncratic risks is often found to be negligible. Such conclusions are usually established under the assumption that the portfolio size tends to infinity and idiosyncratic risk factors are not dominant. We study under-investigated scenarios where the portfolio size is fixed and the idiosyncratic risk factors are heavy-tailed, exploring two distinct scenarios: an independence scenario and an asymptotic dependence scenario. The former is standard in the literature, while the latter is motivated by recent studies that have found the inadequacy of relying solely on common factors to capture default clustering. This consideration also reflects the possibility that idiosyncratic reasons can trigger contagion among firms with liabilities to each other. In the independence scenario, even with heavy-tailed idiosyncratic risk factors, the probability of a substantial portfolio loss remains low unless a single asset carries a disproportionately large weight. Conversely, in the asymptotic dependence scenario, the primary drivers of increased exceedance probability are the dependent idiosyncratic risk factors.},
  archive      = {J_EJOR},
  author       = {Shaoying Chen and Zhiwei Tong and Yang Yang},
  doi          = {10.1016/j.ejor.2024.08.015},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {765-776},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Portfolio default losses driven by idiosyncratic risks},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal payoffs under smooth ambiguity. <em>EJOR</em>, <em>320</em>(3), 754-764. (<a href='https://doi.org/10.1016/j.ejor.2024.08.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study optimal payoff choice for an investor in a one-period model under smooth ambiguity preferences, also called KMM preferences as proposed by Klibanoff et al. (2005). In contrast to the existing literature on optimal asset allocation for a KMM investor in a one-period model, we also allow payoffs that are non-linear in the market asset. Our contribution is fourfold. First, we characterize and derive the optimal payoff under KMM preferences. Second, we demonstrate that a KMM investor solves an equivalent problem to an investor under classical subjective expected utility (CSEU) with adjusted second-order probabilities. Third, we show that a KMM investor with exponential ambiguity attitude implicitly maximizes CSEU utility under the ‘worst-case’ second-order probabilities determined by his ambiguity aversion. Fourth, we reveal that optimal payoffs under ambiguity are not necessarily monotonically increasing in the market asset, which we illustrate using a log-normal market asset under drift and volatility uncertainty.},
  archive      = {J_EJOR},
  author       = {An Chen and Steven Vanduffel and Morten Wilke},
  doi          = {10.1016/j.ejor.2024.08.008},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {754-764},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal payoffs under smooth ambiguity},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end, decision-based, cardinality-constrained portfolio optimization. <em>EJOR</em>, <em>320</em>(3), 739-753. (<a href='https://doi.org/10.1016/j.ejor.2024.08.030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolios employing a (factor) risk model are usually constructed using a two step process: first, the risk model parameters are estimated, then the portfolio is constructed. Recent works have shown that this decoupled approach may be improved using an integrated framework that takes the downstream portfolio optimization into account during parameter estimation. In this work we implement an integrated, end-to-end, predict-&-optimize framework to the cardinality-constrained portfolio optimization problem. To the best of our knowledge, we are the first to implement the framework to a nonlinear mixed integer programming problem. Since the feasible region of the problem is discontinuous, we are unable to directly differentiate through it. Thus, we compare three different continuous relaxations of increasing tightness to the problem which are placed as an implicit layers in a neural network. The parameters of the factor model governing the problem’s covariance matrix structure are learned using a loss function that directly corresponds to the decision quality made based on the factor model’s predictions. Using real world financial data, our proposed end-to-end, decision based model is compared to two decoupled alternatives. Results show significant improvements over the traditional decoupled approaches across all cardinality sizes and model variations while highlighting the need of additional research into the interplay between experimental design, problem size and structure, and relaxation tightness in a combinatorial setting.},
  archive      = {J_EJOR},
  author       = {Hassan T. Anis and Roy H. Kwon},
  doi          = {10.1016/j.ejor.2024.08.030},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {739-753},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {End-to-end, decision-based, cardinality-constrained portfolio optimization},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple financial analyst opinions aggregation based on uncertainty-aware quality evaluation. <em>EJOR</em>, <em>320</em>(3), 720-738. (<a href='https://doi.org/10.1016/j.ejor.2024.08.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial analysts’ opinions are pivotal in investment decision-making, as they provide valuable expert knowledge. Aggregating these opinions offers a promising way to unlock their collective wisdom. However, existing opinion aggregation methods are hindered by their inability to effectively assess differences in opinion quality, resulting in suboptimal outcomes. This Study introduces a novel model called SmartMOA, which addresses this limitation by automatically evaluating the quality of each opinion and integrating this evaluation into the aggregation process. Our model begins with a novel Bayesian neural network that leverages the implicit knowledge embedded in the interactions between analysts and stock characteristics. This methodology produces an assessment of individual opinions that accounts for uncertainties. We then formulate a bi-objective combinatorial optimization problem to determine optimal weights for combining multiple analysts’ opinions, simultaneously minimizing the error and uncertainty of the aggregated outcome. Therefore, SmartMOA systematically highlights high-quality opinions during the aggregation process. Using a real dataset spanning eight years, we present comprehensive empirical evidence that demonstrates the superior performance of SmartMOA in heterogeneous analyst opinion aggregation.},
  archive      = {J_EJOR},
  author       = {Shuai Jiang and Wenjun Zhou and Yanhong Guo and Hui Xiong},
  doi          = {10.1016/j.ejor.2024.08.024},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {720-738},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multiple financial analyst opinions aggregation based on uncertainty-aware quality evaluation},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision-focused neural adaptive search and diving for optimizing mining complexes. <em>EJOR</em>, <em>320</em>(3), 699-719. (<a href='https://doi.org/10.1016/j.ejor.2024.07.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing industrial mining complexes, from extraction to end-product delivery, presents a significant challenge due to non-linear aspects and uncertainties inherent in mining operations. The two-stage stochastic integer program for optimizing mining complexes under joint supply and demand uncertainties leads to a formulation with tens of millions of variables and non-linear constraints, thereby challenging the computational limits of state-of-the-art solvers. To address this complexity, a novel solution methodology is proposed, integrating context-aware machine learning and optimization for decision-making under uncertainty. This methodology comprises three components: (i) a hyper-heuristic that optimizes the dynamics of mining complexes, modeled as a graph structure, (ii) a neural diving policy that efficiently performs dives into the primal heuristic selection tree, and (iii) a neural adaptive search policy that learns a block sampling function to guide low-level heuristics and restrict the search space. The proposed neural adaptive search policy introduces the first soft (heuristic) branching strategy in mining literature, adapting the learning-to-branch framework to an industrial context. Deployed in an online fashion, the proposed hybrid methodology is shown to optimize some of the most complex case studies, accounting for varying degrees of uncertainty modeling complexity. Theoretical analyses and computational experiments validate the components’ efficacy, adaptability, and robustness, showing substantial reductions in primal suboptimality and decreased execution times, with improved and more robust solutions that yield higher net present values of up to 40%. While primarily grounded in mining, the methodology shows potential for enabling smart, robust decision-making under uncertainty.},
  archive      = {J_EJOR},
  author       = {Yassine Yaakoubi and Roussos Dimitrakopoulos},
  doi          = {10.1016/j.ejor.2024.07.024},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {699-719},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Decision-focused neural adaptive search and diving for optimizing mining complexes},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiobjective ϵ-constraint based approach for the robust master surgical schedule under multiple uncertainties. <em>EJOR</em>, <em>320</em>(3), 682-698. (<a href='https://doi.org/10.1016/j.ejor.2024.08.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient scheduling of elective surgeries in hospitals is critical for ensuring patient satisfaction, cost-effectiveness, and overall operational efficiency. However, operating theater (OT) managers face complex and competing scheduling problems due to numerous sources of uncertainty and the impact of the proposed schedule on downstream recovery units, such as the intensive care unit (ICU). To address these challenges, this study develops a multiobjective robust planning model for the weekly Master Surgical Schedule (MSS) under multiple uncertainties. The model takes into account patient priority, assignment cost and workload balancing, while also considering the constraints of the OT, surgeon availabilities, downstream resources, and the uncertainty of surgery duration and patients’ length of stay (LOS) in the ICU. To evaluate the robust solutions, a Monte Carlo simulation is used to calculate the risk of constraint violations, and an adapted ϵ -constraint algorithm is used for the four-objective problem to compute the Pareto front and calculate the hypervolume for every degree of uncertainty. This provides a comprehensive decision tool for OT decision makers and allows for the comparison of various scenarios in terms of the number of scheduled patients, canceled patients, and the utilization rate of the OT.},
  archive      = {J_EJOR},
  author       = {Salma Makboul and Alexandru-Liviu Olteanu and Marc Sevaux},
  doi          = {10.1016/j.ejor.2024.08.022},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {682-698},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A multiobjective ϵ-constraint based approach for the robust master surgical schedule under multiple uncertainties},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overseas production or domestic production? impacts of tax disparity and market difference. <em>EJOR</em>, <em>320</em>(3), 670-681. (<a href='https://doi.org/10.1016/j.ejor.2024.09.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overseas and domestic production are two commonly employed strategies for multinational firms (MNFs) to manage their global production operations. Recent tax-cutting initiatives have made domestic production an attractive option for MNFs. We aim to investigate whether such initiatives can effectively induce MNFs to produce domestically, especially when they cater to both domestic and foreign markets. In this study, we develop a game-theoretical model that considers an MNF with two subsidiaries - a production subsidiary and a retail subsidiary - located in different markets, and a third-party seller that resells the MNF's product in the domestic market. We take into account two important differences between the two markets, namely the tax disparity and the market potential difference. Our analysis reveals that a higher foreign market potential can strengthen the MNF's tax-saving incentive under the domestic production strategy. We find that a relatively lower domestic tax rate may not induce the MNF's domestic production even if the foreign market potential is low. On the other hand, when the domestic channel competition is intense, a higher domestic tax rate may still incentivize the MNF to produce domestically. These results depend on the tradeoffs among the tax-saving benefit, the third-party seller's free-riding, and the channel competition. We caution that promoting domestic production through a low tax rate may harm domestic consumer surplus, highlighting the need for policymakers to carefully consider the impact of tax policies on consumers.},
  archive      = {J_EJOR},
  author       = {Baozhuang Niu and Nan Zhang and Zihao Mu},
  doi          = {10.1016/j.ejor.2024.09.012},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {670-681},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Overseas production or domestic production? impacts of tax disparity and market difference},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A newsvendor model with multiple reference points: Target-setting for aspirational newsvendors. <em>EJOR</em>, <em>320</em>(3), 655-669. (<a href='https://doi.org/10.1016/j.ejor.2024.09.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospect theory posits that the determination of an outcome as a gain or loss hinges upon the reference points, thereby exerting a substantial influence on the decision-making processes of individuals. These reference points can encompass both external targets and internal aspirations (self-goals), forming two potential candidates. Despite a growing body of evidence showcasing the concurrent impact of multiple reference points on decision-makers’ choices, scant attention has been accorded to this phenomenon within the realm of operations management literature. In light of this, the present study delves into the influence wielded by an external target, functioning as a reference point, on the decisions and profitability of newsvendors who also harbor a well-defined aspiration (self-goal) reference point. The study introduces two distinct archetypes of newsvendors: the balanced newsvendors, adept at harmonizing personal needs with target attainment and thus navigating decisions influenced by dual reference points; and the focused newsvendors, singularly fixated on target realization and thereby governed by a sole target reference point. Our findings reveal that the effects of target-setting in boosting expected profits on both categories of newsvendors are contingent upon the specific value attributed to their self-goal. The implications of target-setting can either yield advantages or drawbacks, contingent upon the self-goal. This asymmetry in outcomes holds true for both balanced and focused newsvendors, with the former exhibiting a higher likelihood of deriving benefits from target-setting. Moreover, an exploration of scenarios wherein a newsvendor’s self-goal may undergo updates underscores the potential for further enhancement in the performance of balanced newsvendors.},
  archive      = {J_EJOR},
  author       = {Tian Bai and Gengzhong Feng and Meng Wu and Stuart X. Zhu},
  doi          = {10.1016/j.ejor.2024.09.015},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {655-669},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A newsvendor model with multiple reference points: Target-setting for aspirational newsvendors},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An unified framework for measuring environmentally-adjusted productivity change: Theoretical basis and empirical illustration. <em>EJOR</em>, <em>320</em>(3), 642-654. (<a href='https://doi.org/10.1016/j.ejor.2024.08.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to define an unified framework to analyse environmentally-adjusted productivity change. Equivalence conditions for additive and multiplicative environmentally-adjusted productivity indicators and indices are highlighted. Besides, an empirical illustration is provided considering non parametric convex neutral by-production model.},
  archive      = {J_EJOR},
  author       = {A. Abad and P. Ravelojaona},
  doi          = {10.1016/j.ejor.2024.08.014},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {642-654},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An unified framework for measuring environmentally-adjusted productivity change: Theoretical basis and empirical illustration},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reference alternatives based knockout-tournament procedure for ranking and selection. <em>EJOR</em>, <em>320</em>(3), 628-641. (<a href='https://doi.org/10.1016/j.ejor.2024.08.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knockout-tournament ( KT ) procedure is an efficient parallel procedure recently developed to solve large-scale ranking and selection (R&S) problems. The procedure adopts a selection structure which is commonly used in many sports tournaments, and eliminates alternatives by conducting “matches” between paired alternatives round-by-round. In this paper, to further improve the procedure’s performance in solving large-scale problems, we propose a major modification of the procedure. Specifically, in each round of the selection, before pairing the surviving alternatives and conducting the matches, we first choose an alternative as the reference alternative and then add the reference alternative to each match. We call the new procedure Procedure i - KT , where i - KT stands for “improved knockout-tournament”. We show that by carefully choosing the reference alternative and designing the pairing scheme for the remaining surviving alternatives in each round of the selection, Procedure i - KT can achieve significant improvements on both the average sample size required in each match and the total number of matches required during the entire selection process. In the meantime, we demonstrate that after the modifications, Procedure i - KT still fits parallel computing environments well. We compare Procedure i - KT with various procedures on different test examples and numerically justify our theoretical analysis.},
  archive      = {J_EJOR},
  author       = {Ying Zhong and Jianzhong Du and Deng-Feng Li and Zhaolin Hu},
  doi          = {10.1016/j.ejor.2024.08.031},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {628-641},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Reference alternatives based knockout-tournament procedure for ranking and selection},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simple fixes that accommodate switching costs in multi-armed bandits. <em>EJOR</em>, <em>320</em>(3), 616-627. (<a href='https://doi.org/10.1016/j.ejor.2024.09.017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When switching costs are added to the multi-armed bandit (MAB) problem where the arms’ random reward distributions are previously unknown, usually quite different techniques than those for pure MAB are required. We find that two simple fixes on the existing upper-confidence-bound (UCB) policy can work well for MAB with switching costs (MAB-SC). Two cases should be distinguished. One is with positive-gap ambiguity where the performance gap between the leading and lagging arms is known to be at least some δ > 0 . For this, our fix is to erect barriers that discourage frivolous arm switchings. The other is with zero-gap ambiguity where absolutely nothing is known. We remedy this by forcing the same arms to be pulled in increasingly prolonged intervals. As usual, the effectivenesses of our fixes are measured by the worst average regrets over long time horizons T . When the barriers are fixed at δ / 2 , we can accomplish a ln ( T ) -sized regret bound for the positive-gap case. When intervals are such that n of them occupy n 2 periods, we can achieve the best possible T 1 / 2 -sized regret bound for the zero-gap case. Other than UCB, these fixes can be applied to a learning while doing (LWD) heuristic to reach satisfactory results as well. While not yet with the best theoretical guarantees, the LWD-based policies have empirically outperformed those based on UCB and other known alternatives. Numerically competitive policies still include ones resulting from interval-based fixes on Thompson sampling (TS).},
  archive      = {J_EJOR},
  author       = {Ehsan Teymourian and Jian Yang},
  doi          = {10.1016/j.ejor.2024.09.017},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {616-627},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Simple fixes that accommodate switching costs in multi-armed bandits},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing integrated berth allocation and quay crane assignment: A distributionally robust approach. <em>EJOR</em>, <em>320</em>(3), 593-615. (<a href='https://doi.org/10.1016/j.ejor.2024.08.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we have formulated a Two-Stage Distributionally Robust Optimization (TDRO) model within the context of a mean–variance ambiguity set, specifically designed to address the challenges in the Integrated Berth Allocation and Quay Crane Assignment Problem (BACAP). A key consideration in this study is the inherent uncertainty associated with ships’ arrival times. During the initial stage, we derive a baseline schedule governing berth allocation and quay crane assignment. Anticipating potential disruptions arising from uncertain arrival delays, the second stage is meticulously formulated to determine the worst-case expectation of adjustment costs within the mean–variance ambiguity set. Subsequently, we undertake an equivalent transformation, converting the general TDRO model into a Two-Stage Robust Second-Order Cone Programming (TRO-SOCP) model. This transformation facilitates the application of the Column and Constraint Generation (C&CG) algorithm, ensuring the derivation of an exact solution. To address the computational intricacies associated with second-order cone programming, we propose two enhancement strategies for upper and lower bounds, aimed at expediting the solution process. Additionally, to contend with large-scale instances, we introduce a refinement and approximation method, transforming the TDRO model into a Mixed-Integer Programming (MIP) model. Furthermore, extensive numerical experiments are executed on both synthetic and real-life instances to validate the superior performance of our model and algorithms. In terms of the total cost, the TDRO model demonstrates superior performance compared with Two-Stage Stochastic Programming (TSP) and Two-Stage Robust Optimization (TRO) models.},
  archive      = {J_EJOR},
  author       = {Chong Wang and Qi Wang and Xi Xiang and Canrong Zhang and Lixin Miao},
  doi          = {10.1016/j.ejor.2024.08.001},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {593-615},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing integrated berth allocation and quay crane assignment: A distributionally robust approach},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A restless bandit model for dynamic ride matching with reneging travelers. <em>EJOR</em>, <em>320</em>(3), 581-592. (<a href='https://doi.org/10.1016/j.ejor.2024.07.040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a large-scale ride-matching problem with a large number of travelers who are either drivers with vehicles or riders looking for sharing vehicles. Drivers can match riders that have similar itineraries and share the same vehicle; and reneging travelers, who become impatient and leave the service system after waiting a long time for shared rides, are considered in our model. The aim is to maximize the long-run average revenue of the ride service vendor, which is defined as the difference between the long-run average reward earned by providing ride services and the long-run average penalty incurred by reneging travelers. The problem is complicated by its scale, the heterogeneity of travelers (in terms of origins, destinations, and travel preferences), and the reneging behaviors. To this end, we formulate the ride-matching problem as a specific Markov decision process and propose a scalable ride-matching policy, referred to as Bivariate Index (BI) policy. The BI policy prioritizes travelers according to a ranking of their bivariate indices, which we prove, in a special case, leads to an optimal policy to the relaxed version of the ride-matching problem. For the general case, through extensive numerical simulations for systems with real-world travel demands, it is demonstrated that the BI policy significantly outperforms baseline policies.},
  archive      = {J_EJOR},
  author       = {Jing Fu and Lele Zhang and Zhiyuan Liu},
  doi          = {10.1016/j.ejor.2024.07.040},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {581-592},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A restless bandit model for dynamic ride matching with reneging travelers},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). App release strategy in the presence of competitive platforms’ quality upgrades. <em>EJOR</em>, <em>320</em>(3), 570-580. (<a href='https://doi.org/10.1016/j.ejor.2024.09.001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile platforms such as Google Android and Apple iOS have established their app stores to entice numerous app developers into their platform ecosystem. These platform firms will deliberate on upgrading the quality of their platforms to bolster performance and security, while app developers may choose to rely on different platforms to release their developed apps as a strategic response. In this study, we explore how the quality upgrade decisions of two competitive platforms affect the app release strategies of an app developer, and reveal the optimal quality upgrade and app release strategies for supply chain members. The platform firms compete on both the platform price and platform quality, and the app developer needs to make decisions about the price and quality of the app. Our analysis suggests that, interestingly, there always exist suitable differences in upgrade efficiency, enabling one platform firm to upgrade the quality in equilibrium irrespective of the app developer's release strategy. In addition, we find that when the app developer releases the app on only one platform, the quality upgrade from the collaborative platform can yield a win-win outcome for supply chain members. Furthermore, our analysis shows that releasing the app on two platforms is not necessarily always the most beneficial compared to releasing on just one for the app developer. The findings carry implications for app developers seeking contented platform collaborators, as well as platform firms such as Google and Apple, which can cope with app release strategies.},
  archive      = {J_EJOR},
  author       = {Xiangxiang Wu and Yong Zha},
  doi          = {10.1016/j.ejor.2024.09.001},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {570-580},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {App release strategy in the presence of competitive platforms’ quality upgrades},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maintenance optimization for multi-component systems with a single sensor. <em>EJOR</em>, <em>320</em>(3), 559-569. (<a href='https://doi.org/10.1016/j.ejor.2024.08.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a multi-component system in which a single sensor monitors a condition parameter. Monitoring gives the decision maker partial information about the system state, but it does not reveal the exact state of the components. Each component follows a discrete degradation process, possibly correlated with the degradation of other components. The decision maker infers a belief about each component’s exact state from the current condition signal and the past data, and uses that to decide when to intervene for maintenance. A maintenance intervention consists of a complete and perfect inspection, and may be followed by component replacements. We model this problem as a partially observable Markov decision process. For a suitable stochastic order, we show that the optimal policy partitions in at most three regions on stochastically ordered line segments. Furthermore, we show that in some instances, the optimal policy can be partitioned into two regions on line segments. In two examples, we visualize the optimal policy. To solve the examples, we modify the incremental pruning algorithm, an exact solution algorithm for partially observable Markov decision processes. Our modification has the potential to also speed up the solution of other problems formulated as partially observable Markov decision processes.},
  archive      = {J_EJOR},
  author       = {Ragnar Eggertsson and Ayse Sena Eruguz and Rob Basten and Lisa M. Maillart},
  doi          = {10.1016/j.ejor.2024.08.016},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {559-569},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Maintenance optimization for multi-component systems with a single sensor},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equilibrium analysis of the seller’s fulfillment channels and sales channels. <em>EJOR</em>, <em>320</em>(3), 544-558. (<a href='https://doi.org/10.1016/j.ejor.2024.09.009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sellers’ products can be sold either through platform channels or through their own channels. Also, sellers’ orders can either be fulfilled by platforms (FBP) or be fulfilled by third-party merchants (FBM). We consider a platform and a representative seller that compete in selling two substitutable products. We develop an analytical framework for identifying each firm’s structure preference by comparing four modes: own channel with FBP (Mode OP), own channel with FBM (Mode OM), platform channel with FBP (Mode PP), and platform channel with FBM (Mode PM). Our research establishes several insights. When the seller’s products are sold through either his own channel or the platform channel with a low commission rate, the platform always prefers to provide fulfillment services for the seller – who may be unwilling to accept that service. In contrast, when the seller’s products are sold through the platform channel with a high commission rate, the platform has more incentive to “protect” the seller’s profit by offering (resp. not offering) fulfillment services to him when he prefers FBP (resp. FBM). These outcomes indicate that the likelihood of platform and seller agreeing on a fulfillment channel for the seller’s orders is strongly affected by the commission rate and the seller’s sales channel. Besides, even though the platform is always willing to provide sales channels for the seller, a seller’s acceptance of that offer is influenced by his own fulfillment channels. Finally, our analysis reveals that the only possible win–win equilibrium structures for the platform and the seller are Mode PP and Mode PM.},
  archive      = {J_EJOR},
  author       = {Shu Hu and Ke Fu},
  doi          = {10.1016/j.ejor.2024.09.009},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {544-558},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Equilibrium analysis of the seller’s fulfillment channels and sales channels},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retailer’s information sharing and manufacturer’s channel expansion in the live-streaming E-commerce era. <em>EJOR</em>, <em>320</em>(3), 527-543. (<a href='https://doi.org/10.1016/j.ejor.2024.09.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous manufacturers started to embrace live-streaming selling channels in addition to their preexisting retail channels during the outbreak of COVID-19. Our work investigates a retailer’s optimal strategy for sharing demand information with a manufacturer who may collaborate with a streamer to build a live-streaming selling channel. The results indicate that the manufacturer’s live-streaming selling channel expansion via an influencer does not necessarily harm the retailer because the retailer can free ride on the market expansion due to the social influence of the streamer. We also provide a rationale for the widespread voluntary information sharing observed in the era of live-streaming selling. The retailer can discourage or encourage the manufacturer to establish a live-streaming channel by sharing the demand information, which depends on the streamer’s social influence. In addition, potential changes to the information sharing policy result in several unexpected profit implications for the manufacturer, whose profit exhibits a nonmonotonic relationship to the streamer’s social influence and the channel expansion cost. In other words, our results counterintuitively show that the manufacturer may suffer from cooperating with a highly influential streamer in a live-streaming channel but benefit from choosing a less influential one.},
  archive      = {J_EJOR},
  author       = {Wei Lu and Xiang Ji and Jie Wu},
  doi          = {10.1016/j.ejor.2024.09.008},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {527-543},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Retailer’s information sharing and manufacturer’s channel expansion in the live-streaming E-commerce era},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moderate exponential-time quantum dynamic programming across the subsets for scheduling problems. <em>EJOR</em>, <em>320</em>(3), 516-526. (<a href='https://doi.org/10.1016/j.ejor.2024.09.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grover Search is currently one of the main quantum algorithms leading to hybrid quantum–classical methods that reduce the worst-case time complexity for some combinatorial optimization problems. Specifically, the combination of Quantum Minimum Finding (obtained from Grover Search) with dynamic programming has proved particularly efficient in improving the complexity of NP-hard problems currently solved by classical dynamic programming. For these problems, the classical dynamic programming complexity in O ∗ ( c n ) , where O ∗ denotes that polynomial factors are ignored, can be reduced by a hybrid algorithm to O ∗ ( c q u a n t n ) , with c q u a n t < c . In this paper, we provide a bounded-error hybrid algorithm that achieves such an improvement for a broad class of NP-hard single-machine scheduling problems for which we give a generic description. Moreover, we extend this algorithm to tackle the 3-machine flowshop problem. Our algorithm reduces the exponential-part complexity compared to the best-known classical algorithm, sometimes at the cost of an additional pseudo-polynomial factor.},
  archive      = {J_EJOR},
  author       = {Camille Grange and Michael Poss and Eric Bourreau and Vincent T’kindt and Olivier Ploton},
  doi          = {10.1016/j.ejor.2024.09.005},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {516-526},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Moderate exponential-time quantum dynamic programming across the subsets for scheduling problems},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quadratic horizontally elastic not-first/not-last filtering algorithm for cumulative constraint. <em>EJOR</em>, <em>320</em>(3), 505-515. (<a href='https://doi.org/10.1016/j.ejor.2024.09.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The not-first/not-last rule is a pendant of the edge finding rule, generally embedded in the cumulative constraint during constraint-based scheduling. It is combined with other filtering rules for more pruning of the tree search. In this paper, the Profile data structure in which tasks are scheduled in a horizontally elastic way is used to strengthen the classic not-first/not-last rule. Potential not-first task intervals are selected using criteria (specified later in the paper), and the Profile data structure is applied to selected task intervals. We prove that this new rule subsumes the classic not-first rule. A quadratic filtering algorithm is proposed for the new rule, thus improving the complexity of the horizontally elastic not-first/not-last algorithm from O ( n 3 ) to O ( n 2 ) . The fixed part of external tasks that overlap with the selected task intervals is considered during the computation of the earliest completion time of task intervals. This improvement increases the filtering power of the algorithm while remaining quadratic. Experimental results, on a well-known suite of benchmark instances of Resource-Constrained Project Scheduling Problems (RCPSPs), show that the propounded algorithms are competitive with the state-of-the-art not-first algorithms in terms of tree search and running time reduction.},
  archive      = {J_EJOR},
  author       = {Roger Kameugne and Sévérine Fetgo Betmbe and Thierry Noulamo},
  doi          = {10.1016/j.ejor.2024.09.003},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {505-515},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Quadratic horizontally elastic not-first/not-last filtering algorithm for cumulative constraint},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimization framework for solving large scale multidemand multidimensional knapsack problem instances employing a novel core identification heuristic. <em>EJOR</em>, <em>320</em>(3), 496-504. (<a href='https://doi.org/10.1016/j.ejor.2024.08.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By applying the core concept to solve a binary integer program (BIP), certain variables of the BIP are fixed to their anticipated values in the optimal solution. In contrast, the remaining variables, called core variables, are used to construct and solve a core problem (CP) instead of the BIP. A new approach for identifying CP utilizing a local branching (LB) alike constraint is presented in this article. By including the LB-like constraint in the linear programming relaxation of the BIP, this method transfers batches of variables to the set of core variables by analyzing changes to their reduced costs. This approach is sensitive to problem hardness because more variables are moved to the core set for hard problems compared to easy ones. This novel core identification approach is embedded in a multi-stage framework to solve the multidemand, multidimensional knapsack problems (MDMKP), where at each stage, more variables are added to the previous stage CP. The default branch and bound of CPLEX20.10 is used to solve the first stage, and a tabu search algorithm is used to solve subsequent stages until all variables are added to CP in the last stage. The new framework has shown equivalent to superior results compared to the state-of-the-art algorithms in solving large MDMKP instances having 500 and 1,000 variables.},
  archive      = {J_EJOR},
  author       = {Sameh Al-Shihabi},
  doi          = {10.1016/j.ejor.2024.08.025},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {496-504},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An optimization framework for solving large scale multidemand multidimensional knapsack problem instances employing a novel core identification heuristic},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified solution framework for flexible job shop scheduling problems with multiple resource constraints. <em>EJOR</em>, <em>320</em>(3), 479-495. (<a href='https://doi.org/10.1016/j.ejor.2024.08.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines flexible job shop scheduling problems with multiple resource constraints. A unified solution framework is presented for modelling various types of non-renewable, renewable and cumulative resources, such as limited capacity machine buffers, tools, utilities and work in progress buffers. We propose a Constraint Programming (CP) model and a CP-based Adaptive Large Neighbourhood Search (ALNS-CP) algorithm. The ALNS-CP uses long-term memory structures to store information about the assignment to machines of both individual operations and pairs of operations, as encountered in high-quality and diverse solutions during the search process. This information is used to create additional constraints for the CP solver, which guide the search towards promising regions of the solution space. Numerous experiments are conducted on well-known benchmark sets to assess the performance of ALNS-CP against the current state-of-the-art. Additional experiments are conducted on new instances of various sizes to study the impact of different resource types on the makespan. The computational results show that the proposed solution framework is highly competitive, while it was able to produce 39 new best solutions on well-known problem instances of the literature.},
  archive      = {J_EJOR},
  author       = {Gregory A. Kasapidis and Dimitris C. Paraskevopoulos and Ioannis Mourtos and Panagiotis P. Repoussis},
  doi          = {10.1016/j.ejor.2024.08.010},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {479-495},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A unified solution framework for flexible job shop scheduling problems with multiple resource constraints},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair integer programming under dichotomous and cardinal preferences. <em>EJOR</em>, <em>320</em>(3), 465-478. (<a href='https://doi.org/10.1016/j.ejor.2024.08.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One cannot make truly fair decisions using integer linear programs unless one controls the selection probabilities of the (possibly many) optimal solutions. For this purpose, we propose a unified framework when binary decision variables represent agents with dichotomous preferences, who only care about whether they are selected in the final solution. We develop several general-purpose algorithms to fairly select optimal solutions, for example, by maximizing the Nash product or the minimum selection probability, or by using a random ordering of the agents as a selection criterion (Random Serial Dictatorship). We also discuss in detail how to extend the proposed methods when agents have cardinal preferences. As such, we embed the “black-box” procedure of solving an integer linear program into a framework that is explainable from start to finish. Lastly, we evaluate the proposed methods on two specific applications, namely kidney exchange (dichotomous preferences), and the scheduling problem of minimizing total tardiness on a single machine (cardinal preferences). We find that while the methods maximizing the Nash product or the minimum selection probability outperform the other methods on the evaluated welfare criteria, methods such as Random Serial Dictatorship perform reasonably well in computation times that are similar to those of finding a single optimal solution.},
  archive      = {J_EJOR},
  author       = {Tom Demeulemeester and Dries Goossens and Ben Hermans and Roel Leus},
  doi          = {10.1016/j.ejor.2024.08.023},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {465-478},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fair integer programming under dichotomous and cardinal preferences},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 50 years of warehousing research—An operations research perspective. <em>EJOR</em>, <em>320</em>(3), 449-464. (<a href='https://doi.org/10.1016/j.ejor.2024.03.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Warehouses have always been an essential part of supply chains, but despite their fundamental role they were not seen as especially mission critical. With the advent of e-commerce, same-day deliveries, omni-channel retailing, and global supply chain disruptions, however, this assessment has changed, and today’s warehouses have evolved to technology-enriched fulfillment factories with strategic relevance. This paper traces the evolution of picker-to-parts and parts-to-picker warehouses from basic systems of the first generation, via extended system setups of the second generation, up to state-of-the-art robotized distribution centers of the third generation. Specifically, we highlight the most influential scientific contributions of the operations research (OR) community within each generation that have supported this evolution over the past 50 years. Beyond the historical perspective, we outline an agenda for the warehousing research of the future.},
  archive      = {J_EJOR},
  author       = {Nils Boysen and René de Koster},
  doi          = {10.1016/j.ejor.2024.03.026},
  journal      = {European Journal of Operational Research},
  month        = {2},
  number       = {3},
  pages        = {449-464},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {50 years of warehousing research—An operations research perspective},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework for integrated resource planning in surgical clinics. <em>EJOR</em>, <em>320</em>(2), 433-447. (<a href='https://doi.org/10.1016/j.ejor.2024.08.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem under study is based on the challenges faced by the Orthopaedic Clinic at St. Olav’s Hospital in Trondheim, Norway. Variations in demand and supply cause fluctuating waiting lists, and it is challenging to level the activities between the clinic’s two units, the outpatient clinic and the operating theater, to obtain short waiting times for all activities. Based on these challenges, we describe and present a planning problem referred to as the Long-term Master Scheduling Problem (LMSP), where the objective is to construct an integrated Long-term Master Schedule (LMS) that facilitates short waiting times in both units. The LMS can be separated into two schedules, one cyclic high-level schedule, and one non-cyclic low-level schedule. The demand for outpatient clinic consultations and surgeries is stochastic, as are the waiting lists. To account for this, we propose a planning framework consisting of an optimization model to solve the LMSP, and a two-level planning procedure. In the planning procedure, we first solve the LMSP to construct the LMS for the upcoming planning horizon. Then, to adjust to the fluctuating waiting lists, we periodically refine the low-level schedule by solving a constrained LMSP. We also develop a simulation-based evaluation procedure to evaluate the planning framework in a real-life setting and use this to investigate different planning strategies. We find that imposing flexible, dynamic and agile planning strategies improve waiting time outcomes and patient throughput. Furthermore, combining the strategies yields additive improvements.},
  archive      = {J_EJOR},
  author       = {Thomas Reiten Bovim and Anders N. Gullhav and Henrik Andersson and Atle Riise},
  doi          = {10.1016/j.ejor.2024.08.021},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {433-447},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A framework for integrated resource planning in surgical clinics},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing omnichannel retailer inventory replenishment using vehicle capacity-sharing with demand uncertainties and service level requirements. <em>EJOR</em>, <em>320</em>(2), 417-432. (<a href='https://doi.org/10.1016/j.ejor.2024.08.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores an inventory replenishment problem for an omnichannel retailer selling a product with demand uncertainties and service level requirements in different channels through a capacity-sharing strategy. The omnichannel retailer allows customers to order products online and then pick them up in retail stores. The capacity-sharing strategy is considered to reduce travel costs when the omnichannel retailer delivers products in different channels. With the capacity-sharing strategy, some products can be transferred from the online delivery vehicle to the retail store delivery vehicle. The expectation and the bounds on the online route travel cost of the capacity-sharing strategy are determined, based on which the capacity-sharing rules are established. Optimization models and a solution procedure are developed to solve the omnichannel retailer inventory replenishment problem. To ensure adequate performance, service level requirements are considered when making delivery decisions. Numerical experiments using randomly generated and artificial instances are performed to verify the effectiveness and practicality of the proposed optimization models and the solution procedure. The results show that the proposed models and solution procedure can effectively reduce travel costs, and can provide effective schedule and delivery quantity decision support for omnichannel retailers.},
  archive      = {J_EJOR},
  author       = {Ruozhen Qiu and Mingli Yuan and Minghe Sun and Zhi-Ping Fan and Henry Xu},
  doi          = {10.1016/j.ejor.2024.08.005},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {417-432},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing omnichannel retailer inventory replenishment using vehicle capacity-sharing with demand uncertainties and service level requirements},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of counterparty credit risk under netting agreements. <em>EJOR</em>, <em>320</em>(2), 402-416. (<a href='https://doi.org/10.1016/j.ejor.2024.08.019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate counterparty credit risk and credit valuation adjustments in portfolios including derivatives with early-exercise opportunities, under a netting agreement. We show that credit risk and netting agreements have a significant impact on the way portfolios are managed (that is, on options’ exercise strategies) and, therefore, on the value of the portfolio and on the price of counterparty risk. We derive the value of a netted portfolio as the solution of a zero-sum, finite horizon, discrete-time stochastic game. We show that this dynamic-game interpretation can be used to determine the value of the reglementary capital charges required of financial institutions to cover for counterparty credit risk and we propose a numerical valuation method. Numerical investigations show that currently used numerical approaches can grossly misestimate the value of credit valuation adjustments.},
  archive      = {J_EJOR},
  author       = {Ahmadreza Tavasoli and Michèle Breton},
  doi          = {10.1016/j.ejor.2024.08.019},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {402-416},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Evaluation of counterparty credit risk under netting agreements},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Customer and provider bounded rationality in on-demand service platforms. <em>EJOR</em>, <em>320</em>(2), 389-401. (<a href='https://doi.org/10.1016/j.ejor.2024.08.013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing literature on operations management in the context of the sharing economy typically assumes that both customers and providers are fully rational. In contrast, we consider an on-demand service platform (e.g., Didi and Uber) with boundedly rational customers and providers that sets a price charged to customers and a wage paid to providers. Both customers and providers are sensitive to the payment terms set by the platform and also to congestion in the system (given by the relative numbers of available customers and providers in the market). We capture bounded rationality using a model in which customers and providers are incapable of accurately estimating the congestion level. We examine the impact of bounded rationality on the platform profit, consumer surplus, and labor welfare. We find that both customers’ and providers’ bounded rationalities may benefit the platform. Specifically, when customers’ or providers’ bounded rationality level and service valuation are relatively large or the valuation is relatively small, more irrational customers or providers increases the platform’s profit. Moreover, we find that the platform can exploit the bounded rationality differences between customers and providers to gain profit. Counterintuitively, we also demonstrate that the high bounded rationality of customers or providers may increase consumer surplus and/or labor welfare. Finally, bounded rationality on one side (e.g., customer side) can make bounded rationality on the other side (e.g., provider side) more likely to increase the platform’s profit, consumer surplus, or labor welfare under certain conditions.},
  archive      = {J_EJOR},
  author       = {Danna Chen and Yong-Wu Zhou and Xiaogang Lin and Kangning Jin},
  doi          = {10.1016/j.ejor.2024.08.013},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {389-401},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Customer and provider bounded rationality in on-demand service platforms},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically optimal energy consumption and inventory control in a make-to-stock manufacturing system. <em>EJOR</em>, <em>320</em>(2), 375-388. (<a href='https://doi.org/10.1016/j.ejor.2024.08.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a make-to-stock manufacturing system in which a single server makes the production. The server consumes energy, and its power consumption depends on the server state: a busy server consumes more power than an idle server, and an idle server consumes more power than a turned-off server. When a server is turned on, it completes a costly set-up process that lasts a while. We jointly control the finished goods inventory and the server’s energy consumption. The objective is to minimize the long-run average inventory holding, backorder, and energy consumption costs by deciding when to produce, when to idle or turn off the server, and when to turn on a turned-off server. Because the exact analysis of the problem is challenging, we consider the asymptotic regime in which the server is in the conventional heavy-traffic regime. We formulate a Brownian control problem (BCP) with impulse and singular controls. In the BCP, the impulse control appears due to server shutdowns, and the singular control appears due to server idling. Depending on the system parameters, the optimal BCP solution is either a control-band or barrier policy. We propose a simple heuristic control policy from the optimal BCP solution that can easily be implemented in the original (non-asymptotic) system. Furthermore, we prove the asymptotic optimality of the proposed control policy in a Markovian setting. Finally, we show that our proposed policy performs close to optimal in numerical experiments.},
  archive      = {J_EJOR},
  author       = {Erhun Özkan and Barış Tan},
  doi          = {10.1016/j.ejor.2024.08.028},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {375-388},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Asymptotically optimal energy consumption and inventory control in a make-to-stock manufacturing system},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying fixed order commitment contracts in a capacitated supply chain. <em>EJOR</em>, <em>320</em>(2), 358-374. (<a href='https://doi.org/10.1016/j.ejor.2024.08.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demand uncertainty can lead to excess inventory holdings, capacity creation, emergency deliveries, and stock-outs. The costs of demand uncertainty may be directly borne by upstream suppliers, but can propagate downstream in the form of higher prices. To address these problems, we investigate a practical application of a fixed order commitment contract (FOCC) in which a manufacturer commits to a minimum fixed order quantity each period and receives a per unit price discount from the supplier for the commitment. We model a FOCC as a Stackelberg game in which the supplier offers a price discount anticipating the manufacturer’s response, and the manufacturer subsequently decides on the optimal commitment quantity. We show that a FOCC can smooth the orders received by the supplier, mitigating the negative consequences of demand uncertainty for the supplier, the manufacturer, and the supply chain. We extend the current literature by solving for an endogenous price discount instead of treating it as an exogenous value, and validate our model insights with our research partner, a large international materials handling equipment manufacturer. Using data on 863 parts, we evaluate the relationships between the model parameters, contract parameters, and the contract effectiveness, and show the conditions under which the FOCC generates greater cost savings for both the manufacturer and supplier. Our results help operations managers better understand how to obtain the optimal contract parameters for a FOCC and the circumstances under which such a contract is most beneficial for the company and its supply chain.},
  archive      = {J_EJOR},
  author       = {Christina Imdahl and Kai Hoberg and William Schmidt},
  doi          = {10.1016/j.ejor.2024.08.018},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {358-374},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Applying fixed order commitment contracts in a capacitated supply chain},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Agency selling or reselling: The role of cause marketing. <em>EJOR</em>, <em>320</em>(2), 343-357. (<a href='https://doi.org/10.1016/j.ejor.2024.07.034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cause marketing (CM) is commonly adopted to pursue profit growth or/and achieve corporate social responsibility (CSR). In online retailing, to facilitate CM for the products, e-retailers are increasingly implementing CM programs for the firms that sell products directly to consumers, i.e., suppliers under agency selling mode or themselves under reselling mode. Motivated by this, we examine how CM for the product influences the equilibrium outcomes under the two selling modes and the selling mode preferences of the supply chain members. Our results suggest that CM for the product is beneficial to both the supply chain members but can render the e-retailer to be hurt by a higher agency fee. Significantly, the supplier can prefer agency selling mode only when the agency fee is lower, whereas the e-retailer can prefer it even if the agency fee is lower. As a result of the changes in the selling mode preferences, the CM program can hurt the e-retailer when the selling mode is determined by the supplier. We further consider the cases where either of the supply chain members cares about CSR and fulfills it with CM. The results indicate that the concern for CSR of either of the supply chain members can coordinate their selling mode preferences. Specifically, the concern for CSR of the supplier (e-retailer) can induce them to prefer agency selling mode (reselling mode), thereby eliminating (further exacerbating) the adverse impact of the CM program on the supply chain members.},
  archive      = {J_EJOR},
  author       = {Lin Wei and Shengming Zheng and Shaofu Du and Baofeng Zhang},
  doi          = {10.1016/j.ejor.2024.07.034},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {343-357},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Agency selling or reselling: The role of cause marketing},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning approach to rank pricing problems in branch-and-price. <em>EJOR</em>, <em>320</em>(2), 328-342. (<a href='https://doi.org/10.1016/j.ejor.2024.07.029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach exploiting machine learning to enhance the efficiency of the branch-and-price algorithm. The focus is, specifically, on problems characterized by multiple pricing problems. Pricing problems often constitute a substantial portion of CPU time due to their repetitive nature. The primary contribution of this work includes the introduction of a machine learning-based ranker that strategically guides the search for new columns in the column generation process. The master problem solution is analyzed by the ranker, which then suggests an order for solving the pricing problems to prioritize those with the potential to improve the master problem the most. This prioritization mechanism is essential in speeding up the column generation since, by identifying new columns early in the process, we can terminate the search procedure sooner. Furthermore, our technique exhibits applicability across all nodes of the branching tree, making it a valuable tool for solving a wide range of optimization problems. We demonstrate the usefulness of this approach in the challenging domain of operating room scheduling, an area that has seen limited exploration in the context of machine learning. Extensive experimental evaluations underline the effectiveness of the developed algorithm, consistently outperforming traditional search strategies in terms of time, number of solved pricing problems, searched nodes in the branching tree, and performed column generation iterations.},
  archive      = {J_EJOR},
  author       = {Pavlína Koutecká and Přemysl Šůcha and Jan Hůla and Broos Maenhout},
  doi          = {10.1016/j.ejor.2024.07.029},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {328-342},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A machine learning approach to rank pricing problems in branch-and-price},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Industrial multi-resource flexible job shop scheduling with partially necessary resources. <em>EJOR</em>, <em>320</em>(2), 309-327. (<a href='https://doi.org/10.1016/j.ejor.2024.07.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is dedicated to the study of industrial extensions of the flexible job shop scheduling problem with multiple resources in order to propose an alternative to expensive optimization software for small to medium-sized manufacturing companies. In this context, we propose a generic model able to tackle some constraints often found in industrial scheduling problems. This model tackles partially necessary resources by decomposing operations into stages. Instances are solved by a simulated annealing metaheuristic which is further improved using efficient conditions to filter non-interesting solutions. We compare our approach to a constraint programming model using a commercial solver. Extensive experiments and statistical analysis show that our method is competitive and of practical use in the industrial context.},
  archive      = {J_EJOR},
  author       = {Quentin Perrachon and Alexandru-Liviu Olteanu and Marc Sevaux and Sylvain Fréchengues and Jean-François Kerviche},
  doi          = {10.1016/j.ejor.2024.07.023},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {309-327},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Industrial multi-resource flexible job shop scheduling with partially necessary resources},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated crew organization and work zone scheduling for network-wide daily road pavement rehabilitation. <em>EJOR</em>, <em>320</em>(2), 290-308. (<a href='https://doi.org/10.1016/j.ejor.2024.08.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study develops a new integer-programming model to address the network-wide daily road pavement rehabilitation scheduling problem. In the model, the crew organization and work zone schedule are jointly optimized daily, with the objective of minimizing both the operational cost and user travel time. A day-to-day traffic dynamics model is applied to capture the non-equilibrium traffic evolution against network supply variation over the planning horizon, which leads to a simulation-based optimization problem. To solve this challenging problem, a two-stage hybrid heuristic solution method is proposed. In the first stage, hybrid tabu search (TS) meta-heuristics are comparatively developed to identify a group of active crew work routes without time slacks. The obtained crew routes are then fed to the second stage for work zone scheduling via a discrete compass search algorithm. Some important findings are obtained from numerical experiments. First, crew routing (or crew organization) is the dominant decision in the studied problem, and a desirable work zone schedule encourages a crew to execute the assigned tasks continually. The findings can be used to develop simplified and efficient solution algorithms. Second, the hybrid TS meta-heuristics developed for crew routing exhibit superior performance compared to other solution methods. Finally, a well-defined model for the current problem should consider both user travel time and operation cost. Our model enables decision-makers to make an effective trade-off between these two objectives. An effective measure is suggested to evaluate the cost-effectiveness of budget investment decisions when budgets are limited.},
  archive      = {J_EJOR},
  author       = {Wenyi Zhang and Yanbo He and Xuan Zhang and Tao Liu and Wei Guan},
  doi          = {10.1016/j.ejor.2024.08.012},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {290-308},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integrated crew organization and work zone scheduling for network-wide daily road pavement rehabilitation},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of contextual optimization methods for decision-making under uncertainty. <em>EJOR</em>, <em>320</em>(2), 271-289. (<a href='https://doi.org/10.1016/j.ejor.2024.03.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently there has been a surge of interest in operations research (OR) and the machine learning (ML) community in combining prediction algorithms and optimization techniques to solve decision-making problems in the face of uncertainty. This gave rise to the field of contextual optimization, under which data-driven procedures are developed to prescribe actions to the decision-maker that make the best use of the most recently updated information. A large variety of models and methods have been presented in both OR and ML literature under a variety of names, including data-driven optimization, prescriptive optimization, predictive stochastic programming, policy optimization, (smart) predict/estimate-then-optimize, decision-focused learning, (task-based) end-to-end learning/forecasting/optimization, etc. This survey article unifies these models under the lens of contextual stochastic optimization, thus providing a general presentation of a large variety of problems. We identify three main frameworks for learning policies from data and present the existing models and methods under a uniform notation and terminology. Our objective with this survey is to both strengthen the general understanding of this active field of research and stimulate further theoretical and algorithmic advancements in integrating ML and stochastic programming.},
  archive      = {J_EJOR},
  author       = {Utsav Sadana and Abhilash Chenreddy and Erick Delage and Alexandre Forel and Emma Frejinger and Thibaut Vidal},
  doi          = {10.1016/j.ejor.2024.03.020},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {271-289},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A survey of contextual optimization methods for decision-making under uncertainty},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical vs virtual corporate power purchase agreements: Meeting renewable targets amid demand and price uncertainty. <em>EJOR</em>, <em>320</em>(1), 256-270. (<a href='https://doi.org/10.1016/j.ejor.2024.08.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power purchase agreements (PPAs) have become an important corporate procurement vehicle for renewable power, especially among companies that have committed to targets requiring a certain fraction of their power demand be met by renewables. PPAs are long-term contracts that provide renewable energy certificates (RECs) to the corporate buyer and take two main forms: Physical vs Virtual. Physical PPAs deliver power in addition to RECs, while virtual PPAs are financial contracts that hedge (at least partially) power price uncertainty. We compare procurement portfolios that sign physical PPAs with ones that sign virtual PPAs, focusing on fixed-volume contracts and emphasizing uncertainties in power demand and the prices of power and RECs. In particular, we first analyze a two-stage stochastic model to understand the behavior of procurement quantities and costs when using physical and virtual PPAs as well as variants that limit risk. We subsequently formulate a Markov decision process (MDP) that optimizes the multi-stage procurement of power to reach and sustain a renewable procurement target. By leveraging state-of-the-art reoptimization techniques, we solve this MDP on realistic instances to near optimality, and highlight the relative benefits of using PPA types to meet a renewable target. We underscore a trade-off between expected cost and cash flow variance that buyers should consider when choosing between physical and virtual PPAs. Moreover, advanced reoptimization techniques significantly impact the ability to manage this trade-off.},
  archive      = {J_EJOR},
  author       = {Seyed Danial Mohseni Taheri and Selvaprabu Nadarajah and Alessio Trivella},
  doi          = {10.1016/j.ejor.2024.08.002},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {256-270},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Physical vs virtual corporate power purchase agreements: Meeting renewable targets amid demand and price uncertainty},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Globally optimal sequencing of optimal reactive dispatch control adjustments to minimize operational losses in transmission systems by graph shortest path, parallel computing, and dynamic programming. <em>EJOR</em>, <em>320</em>(1), 239-255. (<a href='https://doi.org/10.1016/j.ejor.2024.07.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimizing operational losses in transmission systems through the Optimal Reactive Dispatch (ORD), a non-convex mixed-integer nonlinear programming problem, is crucial for operational cost reduction, resource optimization, and greenhouse gas emission mitigation. Besides all intricacies associated with solving ORDs, transmission system operators encounter the challenge of determining sequences in which ORD control adjustments must be implemented before significant changes occur in generators scheduled power output and system loading. Sequencing ORD control adjustments, in spite of not being novel, remains modestly scrutinized in the literature. This paper introduces a two-phase framework that tackles the globally optimal sequencing of n n ORD control adjustments over n ! n! potential paths by solving the ORD to minimize operational losses in transmission systems in the first phase, and optimally sequencing ORD control adjustments employing fast power flow calculations, graph shortest path, parallel computing, and dynamic programming in the second phase. We discuss the framework’s second phase asymptotic time complexity, which is exponential over factorial for brute-force approaches, and its capability to guarantee globally optimal paths toward minimal operational losses determined in the framework’s first phase. ORD control adjustments for transmission systems with up to 27 controllable variables are benchmarked against two mixed-integer nonlinear programming solvers: BARON, a global non-convex solver, and Knitro, a local solver (assuming convexity around local optima). Globally optimal sequences of ORD control adjustments over n ! n! potential paths (more than 1 0 28 1028 for sequencing 27 control adjustments) and average algorithm runtimes validate the straightforward application and, more importantly, effectiveness of such a comprehensive framework.},
  archive      = {J_EJOR},
  author       = {Rafael Martins Barros and Guilherme Guimarães Lage and Ricardo de Andrade Lira Rabêlo},
  doi          = {10.1016/j.ejor.2024.07.033},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {239-255},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Globally optimal sequencing of optimal reactive dispatch control adjustments to minimize operational losses in transmission systems by graph shortest path, parallel computing, and dynamic programming},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Managing oversaturation in BRT corridors: A new approach of timetabling for resilience enhancement using a tailored integer L-shaped algorithm. <em>EJOR</em>, <em>320</em>(1), 219-238. (<a href='https://doi.org/10.1016/j.ejor.2024.07.035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bus rapid transit (BRT) is a high-capacity public transport system that typically operates along urban transit corridors with dense travel demand. Maintaining the efficiency and stability of the BRT is paramount for daily transport operations. Owing to the difficulty in ensuring an exclusive right-of-way along the entire route, stochastic congestion events may occur resulting from road segments without dedicated BRT lanes. This may lead to volatility in travel time and resulting in passenger stranding, determined as common-case disruptions in this study. These common-case disruptions frequently occur in the daily operation of oversaturated BRT routes. To manage and mitigate their negative impacts, a novel timetabling problem for enhancing the resilience of a BRT system was proposed to assess the ability of the system to withstand and recover from these disruptions. We formulated the problem as a two-stage stochastic mixed-integer optimization model and designed an exact algorithm based on a tailored integer L-shaped method. We then analyzed the structural properties of our model and developed several acceleration techniques to further improve the efficiency of the algorithm. The computational results show that the proposed algorithm outperforms the commercial solver in large-scale instances and can provide near-optimal solutions when the commercial solver is invalid. Besides, external comparisons with dynamic programming also demonstrate the superiority of the proposed algorithm in solution efficiency. Compared with the benchmark timetabling problem, which aims to reduce passenger waiting time, the proposed method can efficiently reduce the duration of the oversaturation period by 35.3%.},
  archive      = {J_EJOR},
  author       = {Yiran Wang and Pengli Mo and Jingxu Chen and Zhiyuan Liu},
  doi          = {10.1016/j.ejor.2024.07.035},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {219-238},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Managing oversaturation in BRT corridors: A new approach of timetabling for resilience enhancement using a tailored integer L-shaped algorithm},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilevel optimization approach for fuel treatment planning. <em>EJOR</em>, <em>320</em>(1), 205-218. (<a href='https://doi.org/10.1016/j.ejor.2024.07.014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various fuel treatment practices involve removing all or some of the vegetation (fuel) from a landscape to reduce the potential for fires and their severity. Fuel treatments form the first line of defense against large-scale wildfires. In this study, we formulate and solve a bilevel integer programming model, where the fuel treatment planner (modeled as the leader) determines appropriate locations and types of treatments to minimize expected losses from wildfires. The follower (i.e., the lower-level decision-maker) corresponds to nature, which is adversarial to the leader and designs a wildfire attack (i.e., locations and time periods, where and when, respectively, wildfires occur) to disrupt the leader’s objective function, e.g., the total expected area burnt. Both levels in the model involve integrality restrictions for decision variables; hence, we explore the model’s difficulty from the computational complexity perspective. Then, we design specialized solution methods for general and some special cases. We perform experiments with semi-synthetic and real-life instances to illustrate the performance of our approaches. We also explore numerically the fundamental differences in the structural properties of solutions arising from bilevel model and its single-level counterpart. These disparities encompass factors like the types of treatments applied and the choice of treated areas. Additionally, we conduct various types of sensitivity analysis on the performance of the obtained policies and illustrate the value of the bilevel solutions.},
  archive      = {J_EJOR},
  author       = {Tomás Lagos and Junyeong Choi and Brittany Segundo and Jianbang Gan and Lewis Ntaimo and Oleg A. Prokopyev},
  doi          = {10.1016/j.ejor.2024.07.014},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {205-218},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Bilevel optimization approach for fuel treatment planning},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Endogenous system-wide output prices in incentive regulation. <em>EJOR</em>, <em>320</em>(1), 188-204. (<a href='https://doi.org/10.1016/j.ejor.2024.05.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simplification of frontier-based regulation currently implemented in several countries and demonstrate its potential application to the regulation of the Brazilian electricity transmission sector. A common regulation uses Data Envelopment Analysis (DEA) to estimate a cost function and mandates that firms adjust their costs to the DEA-estimated best-practice costs. Nevertheless, as DEA allows for individualized pricing of the services, such a DEA-based regulation may be challenging to understand and predict. Moreover, it may encourage strategic behavior in the selection of service profiles, thereby reducing competitive pressure on other firms with similar profiles. This could adversely affect the industry structure, leading to firms with suboptimal scales and scope. Therefore, instead of individualized output prices, we propose the use of a common set of endogenously determined prices that minimizes the total costs to end consumers, while ensuring individual rationality and incentive compatibility. We also present an extended approach that enables the regulator to strike a balance between rewarding well-performing agents and encouraging underperforming agents to save costs, while maintaining a minimal deviation from an expected aggregated payment. Achieving this balance is vital for incentivizing the agents to innovate and invest in more sustainable facilities in the electricity sector, promoting advancements in technology, energy efficiency, and environmental sustainability. Using Brazilian data, we demonstrate that the proposed incentive mechanism is operational and offers substantial advantages over the existing regulatory framework.},
  archive      = {J_EJOR},
  author       = {Mohsen Afsharian and Heinz Ahn and Peter Bogetoft and Sara Kamali and Ana Lopes-Ahn},
  doi          = {10.1016/j.ejor.2024.05.005},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {188-204},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Endogenous system-wide output prices in incentive regulation},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On indication, strict monotonicity, and efficiency of projections in a general class of path-based data envelopment analysis models. <em>EJOR</em>, <em>320</em>(1), 175-187. (<a href='https://doi.org/10.1016/j.ejor.2024.08.009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data envelopment analysis (DEA) theory formulates a number of desirable properties that DEA models should satisfy. Among these, indication, strict monotonicity, and strong efficiency of projections tend to be grouped together in the sense that, in individual models, typically, either all three are satisfied or all three fail at the same time. Specifically, in slacks-based graph models, the three properties are always met; in path-based models, such as radial models, directional distance function models, and the hyperbolic function model, the three properties, with some minor exceptions, typically all fail. Motivated by this observation, the article examines relationships among indication, strict monotonicity, and strong efficiency of projections in the class of path-based models over variable returns-to-scale technology sets. Under mild assumptions, it is shown that the property of strict monotonicity and strong efficiency of projections are equivalent, and that both properties imply indication. This paper also characterises a narrow class of technology sets and path directions for which the three properties hold in path-based models.},
  archive      = {J_EJOR},
  author       = {Margaréta Halická and Mária Trnovská and Aleš Černý},
  doi          = {10.1016/j.ejor.2024.08.009},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {175-187},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {On indication, strict monotonicity, and efficiency of projections in a general class of path-based data envelopment analysis models},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating sets of diverse and plausible scenarios through approximated multivariate normal distributions. <em>EJOR</em>, <em>320</em>(1), 160-174. (<a href='https://doi.org/10.1016/j.ejor.2024.08.003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel and broadly generalizable framework for generating diverse and plausible sets of scenarios. Potential future outcomes are decomposed using a set of uncertainties which are assumed to be multivariate normally distributed, regardless of whether the uncertainties actually present numerically quantifiable phenomena. The optimal scenarios are then chosen along the principal components of the distribution, and the results can be easily interpreted and visualized. Notably, our approach requires a relatively small number of numerical assessments, offering an efficient and practical solution for decision-makers. The framework also provides a testable setting for evaluating its performance and allows users to iteratively improve future-related assumptions and predictions. These findings are relevant for all fields that aim to understand potential future developments, such as, but not limited to, foresight, economics, business strategy and strategic intelligence analysis.},
  archive      = {J_EJOR},
  author       = {Eljas Aalto and Tuomo Kuosa and Max Stucki},
  doi          = {10.1016/j.ejor.2024.08.003},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {160-174},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Generating sets of diverse and plausible scenarios through approximated multivariate normal distributions},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust ordinal regression for subsets comparisons with interactions. <em>EJOR</em>, <em>320</em>(1), 146-159. (<a href='https://doi.org/10.1016/j.ejor.2024.07.021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is devoted to a robust ordinal method for learning the preferences of a decision maker between subsets. The decision model, derived from Fishburn and LaValle (1996) and whose parameters we learn, is general enough to be compatible with any strict weak order on subsets, thanks to the consideration of possible interactions between elements. Moreover, we accept not to predict some preferences if the available preference data are not compatible with a reliable prediction. A predicted preference is considered reliable if all the simplest models (Occam’s razor) explaining the preference data agree on it. Following the robust ordinal regression methodology, our predictions are based on an uncertainty set encompassing the possible values of the model parameters. We define a new ordinal dominance relation between subsets and design a procedure to determine whether this dominance relation holds. Numerical tests are provided on synthetic and real-world data to evaluate the richness and reliability of the preference predictions made.},
  archive      = {J_EJOR},
  author       = {Hugo Gilbert and Mohamed Ouaguenouni and Meltem Öztürk and Olivier Spanjaard},
  doi          = {10.1016/j.ejor.2024.07.021},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {146-159},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust ordinal regression for subsets comparisons with interactions},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A value-at-risk based approach to the routing problem of multi-hazmat railcars. <em>EJOR</em>, <em>320</em>(1), 132-145. (<a href='https://doi.org/10.1016/j.ejor.2024.08.006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper solves a routing problem of multi-hazmat railcars with consolidation operations in order to avoid serious consequences of hazmat accidents. We develop a bi-level optimization model for this problem, and apply a value-at-risk (VaR) approach to generate route choices. By incorporating the consolidation operations performed among different railway shipments, both the risks incurred at yards and on service legs are integratively quantified to evaluate route risks. Due to the inherent complexity of the problem, we propose an exact algorithm as well as a heuristic algorithm to solve the proposed model, and conduct extensive numerical experiments on instances generated from a real railway system in the Midwestern United States. The analysis shows that risk-seeking decision makers will benefit from consolidated transportation due to its potential to significantly reduce total transportation costs. As decision makers become more risk averse, i.e., confidence level increases, increasing the number of train services and reducing the amount of hazmat railcars and consolidation operation has a positive impact on reducing route risk. In addition, the computational results verify the effectiveness of our proposed optimization model and solution approaches, which can generate various routing plans for railway companies under different risk preferences, and our proposed heuristic algorithm gives an optimal or near-optimal solution in 1.41% to 28.22% of the time required by the exact algorithm.},
  archive      = {J_EJOR},
  author       = {Kan Fang and Enyuan Fu and Dian Huang and Ginger Y. Ke and Manish Verma},
  doi          = {10.1016/j.ejor.2024.08.006},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {132-145},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A value-at-risk based approach to the routing problem of multi-hazmat railcars},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact and heuristic approaches for the ship-to-shore problem. <em>EJOR</em>, <em>320</em>(1), 115-131. (<a href='https://doi.org/10.1016/j.ejor.2024.08.017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After a natural disaster such as a hurricane or flooding, the navy can help by bringing supplies, clearing roads, and evacuating victims. If destinations cannot be reached over land, resources can be transported using smaller ships and helicopters, called connectors. To start aid on land as soon as possible this must be done efficiently. In the ship-to-shore problem, trips with their accompanying resources are determined while minimising the makespan. Limited (un)loading capacities, heterogeneous connector characteristics and constraints posed by priority of the resources and grouping of the resources (resource sets) all require that the connector trips are carefully coordinated. Despite the criticality of this coordination, existing literature does not consider resource sets and has only developed heuristics. We provide a formulation that incorporates resource sets and develop (i) an exact branch-and-price algorithm and (ii) a tailored greedy heuristic that can provide upper bounds. We find that 84% of our 98 practical instances terminate within an hour in on average 80 s. Our greedy heuristic can find optimal solutions in two-thirds of these instances, mostly for instances that are very constrained in terms of the delivery order of resources. When improvements are found by the branch-and-price algorithm, the average gap with the makespan of the greedy solution is 40% and, in most cases, these improvements are obtained within three minutes. For the 20 artificial instances, the greedy heuristic has consistent performance on the different types of instances. For these artificial instances improvements of on average 35% are found in reasonable time.},
  archive      = {J_EJOR},
  author       = {M. Wagenvoort and P.C. Bouman and M. van Ee and T. Lamballais Tessensohn and K. Postek},
  doi          = {10.1016/j.ejor.2024.08.017},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {115-131},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Exact and heuristic approaches for the ship-to-shore problem},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alleviating poverty for common prosperity: The role of fupinguan in an E-tailing supply chain. <em>EJOR</em>, <em>320</em>(1), 101-114. (<a href='https://doi.org/10.1016/j.ejor.2024.07.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To alleviate poverty, e-tailers in China have established special web portals or channels, known as Fupinguans (FPGs), to sell products from poor suppliers in rural areas. This paper investigates the underlying mechanism of FPGs in poverty alleviation (PA) and explores their implications for the common prosperity of stakeholders. We consider an e-tailing supply chain where an e-tailer sells substitutes from a poor supplier (characterized by production inefficiency and capital shortage) and a normal supplier to PA-conscious consumers. The benchmark analyses show that the poor supplier faces hurdles in participating due to cost disadvantages in competition. The traditional PA strategy of loan interest subsidies, employed by the e-tailer, can involve the poor supplier in the supply chain by reducing its costs. However, this strategy can mitigate (not eliminate) the poor supplier's cost disadvantage while hurting the normal supplier. Instead, the FPG strategy will provide the poor supplier with a market advantage while indirectly triggering a spillover effect on the normal supplier's product. In this light, the FPG strategy can eliminate or even reverse the poor supplier's competitive disadvantage while benefiting the normal supplier and all stakeholders. Consequently, it can dominate the traditional strategy when the e-tailer has an insufficient financing capability or efficient FPG investment. The e-tailer's FPG investment incentive decreases with the cost disadvantage, whereas competition can strengthen this incentive when the cost disadvantage or competition intensity is sufficiently low. These findings position FPGs as novel business opportunities that align with the ethos of doing well by doing good.},
  archive      = {J_EJOR},
  author       = {Qingyu Zhang and Yuting Liang and Maosen Zhou},
  doi          = {10.1016/j.ejor.2024.07.026},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {101-114},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Alleviating poverty for common prosperity: The role of fupinguan in an E-tailing supply chain},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sales strategy selection for liner companies under shipping e-commerce considering canvassing ability competition. <em>EJOR</em>, <em>320</em>(1), 85-100. (<a href='https://doi.org/10.1016/j.ejor.2024.07.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of e-commerce applications has promoted the establishment of shipping e-commerce channels by many liner companies in addition to their existing traditional Non-vessel operating common carrier (NVOCC) channel. Unlike NVOCC channels, shipping e-commerce channels guarantee shippers the availability of contracted container slots. However, some problems arise, including the competition with NVOCC channels, shipping slot sales’ risk, and the increasing liner companies’ costs. Therefore, this paper addresses optimal sales strategy selection in the liner transportation industry, including a single traditional NVOCC channel (TN) strategy, and a dual channel with both e-commerce and NVOCC channels (EN) strategy. Two contract scheme models are constructed considering the channel competition on canvassing ability, overselling behavior, demand fluctuation, and the limited liner vessel capacity. Findings show that the impact of overselling behavior on the profit under the EN and TN is not always negative, which is related to the shipping capacity and probability of the high canvassing ability. Comparative analyses reveal that the EN is dominant if the unit overselling compensation cost varies small. Meanwhile, the TN is profitable if the unit overselling compensation cost increases and the canvassing cost of e-commerce channel exceeds a certain value. Otherwise, the selection of sales strategy relies on the arrival rate, the canvassing cost of the e-commerce channel and shipping capacity. The results offer new insights to both theoretical research on container slot sales and the practical selection of sales strategy since shipping e-commerce has changed the slot selling mode in the container shipping industry, which could also enhance the competitiveness of liner companies in the container shipping industry.},
  archive      = {J_EJOR},
  author       = {Heying Sun and Qingcheng Zeng and Jasmine Siu Lee Lam and Shuyi Pu and Chenrui Qu},
  doi          = {10.1016/j.ejor.2024.07.027},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {85-100},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Sales strategy selection for liner companies under shipping e-commerce considering canvassing ability competition},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protecting labor rights: Contract design and coordination between brand firms and suppliers. <em>EJOR</em>, <em>320</em>(1), 69-84. (<a href='https://doi.org/10.1016/j.ejor.2024.07.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the design of a contract aimed at coordinating brand firms and their suppliers to collectively protect the suppliers’ labor rights. It finds that a unilateral cost-sharing contract can incentivize suppliers to protect labor rights, while improving the profits of both brand firms and their suppliers. However, such a contract fails to coordinate the supply chain because of an insufficient incentive for both involved parties. Two improved contracts of this contract can coordinate brands firms and their suppliers to improve their efforts to protect labor rights under certain conditions while failing to achieve coordination. Accordingly, this research proposes a bilateral cost- and revenue-sharing contract. The results indicate that with an appropriate revenue-sharing proportion, this contract effectively encourages brand firms and their suppliers to participate in protecting labor rights and coordinate the supply chain. In addition, an increase in the proportion of prosocial consumers incentivizes brand firms and their suppliers to improve labor rights. Nonetheless, consumers’ excessive reliance on brand goodwill to evaluate the total supply chain efforts to protect labor rights will reduce their efforts. Finally, as the impact of labor rights protection efforts on reference prices increases, it can stimulate brand companies and suppliers to increase investment in improving labor rights. At this high impact level, the reference price in the centralized model may surpass that in the unilateral cost-sharing contract. However, if supply chain members invest little effort to protect labor rights, the increase in this impact could reduce consumers’ reference price.},
  archive      = {J_EJOR},
  author       = {Yanju Zhou and Hongzhen Lai and Xiaohong Chen and Chunhua Hu},
  doi          = {10.1016/j.ejor.2024.07.025},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {69-84},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Protecting labor rights: Contract design and coordination between brand firms and suppliers},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bounds and heuristic algorithms for the bin packing problem with minimum color fragmentation. <em>EJOR</em>, <em>320</em>(1), 57-68. (<a href='https://doi.org/10.1016/j.ejor.2024.08.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a recently introduced packing problem in which a given set of weighted items with colors has to be packed into a set of identical bins, while respecting capacity constraints and the number of available bins, and minimizing the total number of times that colors appear in the bins. We review exact methods from the literature and present a fast lower bounding procedure that, in some cases, can also provide an optimal solution. We theoretically study the worst-case performance of the lower bound and the effect of the number of available bins on the solution cost. Then, we computationally test our solution method on a large benchmark of instances from the literature: quite surprisingly, all of them are optimally solved by our procedure in a few seconds, including those for which the optimal solution value was still unknown. Thus, we introduce additional harder instances, which are used to evaluate the performance of a constructive heuristic method and of a tabu search algorithm. Results on the new instances show that the tabu search produces considerable improvements over the heuristic solution, with a limited computational effort.},
  archive      = {J_EJOR},
  author       = {Mathijs Barkel and Maxence Delorme and Enrico Malaguti and Michele Monaci},
  doi          = {10.1016/j.ejor.2024.08.007},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {57-68},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Bounds and heuristic algorithms for the bin packing problem with minimum color fragmentation},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilinear branch and check for unspecified parallel machine scheduling with shift consideration. <em>EJOR</em>, <em>320</em>(1), 35-56. (<a href='https://doi.org/10.1016/j.ejor.2024.08.011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the complex challenge of team formations, assignments, and job schedules within the static Unspecified Parallel Machine Flexible Resource Scheduling problem, specifically incorporating shift considerations. In existing literature, teams are often simplified as machines that operate continuously throughout the day without any interruptions. However, in reality, teams require breaks between shifts and cannot work continuously within a day. Therefore, we introduce shift considerations to ensure that teams do not work in consecutive shifts. We consider flexible workers, capable of performing any job, who are distributed among different teams in different shifts to undertake various jobs. The number of teams in each shift is a decision variable. The duration of each job is determined by the number of workers in a team assigned to it. The objective function is to minimize the makespan, representing the overall schedule completion time, while adhering to precedence constraints. We formulate an integer linear programming model for the proposed problem and develop a novel bilinear branch and check algorithm that introduces valid bilinear inequalities to accelerate convergence. The numerical results confirm that our algorithm’s performance optimally solves problems up to 35 jobs within a reasonable timeframe, surpassing the efficiency of the branch and cut method of IBM CPLEX and the classical branch and check algorithm.},
  archive      = {J_EJOR},
  author       = {Ponpot Jartnillaphand and Elham Mardaneh and Hoa T. Bui},
  doi          = {10.1016/j.ejor.2024.08.011},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {35-56},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Bilinear branch and check for unspecified parallel machine scheduling with shift consideration},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local stability in kidney exchange programs. <em>EJOR</em>, <em>320</em>(1), 20-34. (<a href='https://doi.org/10.1016/j.ejor.2024.07.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When each patient of a kidney exchange program has a preference ranking over its set of compatible donors, questions naturally arise surrounding the stability of the proposed exchanges. We extend recent work on stable exchanges by introducing and underlining the relevance of a new concept of locally stable, or L-stable, exchanges. We show that locally stable exchanges in a compatibility digraph are exactly the so-called local kernels (L-kernels) of an associated blocking digraph (whereas the stable exchanges are the kernels of the blocking digraph), and we prove that finding a nonempty L-kernel in an arbitrary digraph is NP-complete. Based on these insights, we propose several integer programming formulations for computing an L-stable exchange of maximum size. We conduct numerical experiments to assess the quality of our formulations and to compare the size of maximum L-stable exchanges with the size of maximum stable exchanges. It turns out that nonempty L-stable exchanges frequently exist in digraphs which do not have any stable exchange. All the above results and observations carry over when the concept of (locally) stable exchanges is extended to the concept of (locally) strongly stable exchanges.},
  archive      = {J_EJOR},
  author       = {Marie Baratto and Yves Crama and João Pedro Pedroso and Ana Viana},
  doi          = {10.1016/j.ejor.2024.07.031},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {20-34},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Local stability in kidney exchange programs},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unifying framework for selective routing problems. <em>EJOR</em>, <em>320</em>(1), 1-19. (<a href='https://doi.org/10.1016/j.ejor.2024.02.037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a unifying framework for Selective Routing Problems (SRPs) through a systematic analysis. The common goal in SRPs is to determine an optimal vehicle route to serve a subset of vertices while covering another subset. They arise in diverse fields such as logistics, public health, disaster response, and urban development. To establish a unifying framework for different but related problems, we associate the notion of service with coverage and argue that routing is a tool of service. We classify SRPs according to their selectiveness degree and emphasize the breadth and depth of this problem in terms of its characteristics. This SRP framework helps us identify research gaps as well as potential future research areas. We present a generic mathematical model, use it to describe the connections among these problems and identify some identical problems presented under different names.},
  archive      = {J_EJOR},
  author       = {Cagla F. Dursunoglu and Okan Arslan and Sebnem Manolya Demir and Bahar Y. Kara and Gilbert Laporte},
  doi          = {10.1016/j.ejor.2024.02.037},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A unifying framework for selective routing problems},
  volume       = {320},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
