<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DKE</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dke">DKE - 22</h2>
<ul>
<li><details>
<summary>
(2026). Knowledge graph question generation based on crucial semantic information. <em>DKE</em>, <em>161</em>, 102529. (<a href='https://doi.org/10.1016/j.datak.2025.102529'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of the knowledge graph-based question generation (KGQG) task is to generate an answerable, fluent question from a ternary knowledge graph and a target answer. Existing KGQG that study knowledge graph subgraphs and the question of target answer generation do not effectively capture the critical semantic information between tokens within nodes/edges in subgraphs and fail to make full use of target answers and answer markers. This has led to the generation of disfluent and unanswerable questions. To address these problems, we propose a model called knowledge graph question generation based on crucial semantic information (KGQG-CSI). Our proposed model utilizes the critical semantic information encoding module to dynamically learn the degree of significance of tokens within the edges and nodes of fused answers, capturing critical semantic information that would remedy disfluency. In addition, the target answers and answer markers are sufficiently integrated with the nodes to make the generated questions answerable. First, the attention mechanism is used to allow the nodes to interact with the target answers, thereby expressing the semantic information related to the answers more accurately. The nodes that have been processed through the critical semantic information encoding module are then spliced with the answer markers to reduce the ambiguous information. The experimental results on two public datasets show that the results of the proposed model outperform the existing methods.},
  archive      = {J_DKE},
  author       = {Mingtao Zhou and Juxiang Zhou and Jianhou Gan and Jun Wang and Jiatian Mei},
  doi          = {10.1016/j.datak.2025.102529},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102529},
  shortjournal = {Data Knowl. Eng.},
  title        = {Knowledge graph question generation based on crucial semantic information},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FKQG: Few-shot question generation from knowledge graph via large language model in-context learning. <em>DKE</em>, <em>161</em>, 102528. (<a href='https://doi.org/10.1016/j.datak.2025.102528'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Base Question Generation (KBQG) focuses on generating natural language questions from a set of triplets and answers, playing a crucial role in applications such as personalized question-answering systems and educational evaluation tools. Currently, most previous work trains the models based on a large-scale dataset, leading to degraded performance in low-resource scenarios. To address this issue, we propose a novel framework for KBQG using in-context learning with large language models (LLMs) in few-shot settings (FKQG). The key to in-context learning lies in selecting relevant examples to construct prompts that guide the LLM. Therefore, we introduce two strategies for example selection: (1) extracting the semantic paths of triplets as seeds to organize the data, and (2) using the relation linking to the answer as an additional seed. Based on these seeds, examples are retrieved from the data and reranked through graph edit distance, optimizing the prompt structure. This approach ensures contextually relevant question generation. We evaluate FKQG through extensive experiments on two benchmark datasets. Our framework outperforms existing KBQG models in few-shot scenarios, achieving up to a 4.73% improvement in ROUGE-L. Additionally, FKQG enhances the performance of knowledge-based question-answering systems, yielding a 1.2% increase in Hit@1.},
  archive      = {J_DKE},
  author       = {Ruishen Liu and Shaorong Xie and Xinzhi Wang and Xiangfeng Luo and Hang Yu},
  doi          = {10.1016/j.datak.2025.102528},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102528},
  shortjournal = {Data Knowl. Eng.},
  title        = {FKQG: Few-shot question generation from knowledge graph via large language model in-context learning},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An interpretable knowledge recommendation method for civil dispute mediation. <em>DKE</em>, <em>161</em>, 102527. (<a href='https://doi.org/10.1016/j.datak.2025.102527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for efficient and fair civil dispute mediation is driving the development of intelligent knowledge recommendation technologies. However, existing approaches face challenges in interpretability and reasoning over complex relationships. This study proposes an Interpretable Knowledge Recommendation Method (IKRM) that integrates deep learning and multi-hop reasoning to provide precise and transparent decision support for online mediation platforms. First, to address the extraction of specialized terms and intricate relationships in legal texts, we propose a pre-trained model-based semi-joint extraction method combined with ontology design, constructing a civil dispute knowledge graph that enables hierarchical semantic modeling of legal concepts. Second, we design a hybrid multi-hop reasoning framework that combines neural logic programming for numerical rule-based latent relation mining and cognitive graphs for multi-path reasoning, dynamically generating traceable explanations during path expansion. IKRM performs better than mainstream baseline models in terms of all key evaluation indicators, according to experiments validated using multi-source Chinese legal datasets. It additionally exhibits greater reasoning robustness for difficult queries. This study creates a new paradigm for legal knowledge recommendation systems that is modular, interpretable, and effective. It also contributes to larger social equitable governance by offering accurate decision assistance for civil dispute mediation in China.},
  archive      = {J_DKE},
  author       = {Ning Wang and Shibo Cui and Jing Zhang and Runzhe Wang and Yongping Yu},
  doi          = {10.1016/j.datak.2025.102527},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102527},
  shortjournal = {Data Knowl. Eng.},
  title        = {An interpretable knowledge recommendation method for civil dispute mediation},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A framework for purpose-guided event logs generation. <em>DKE</em>, <em>161</em>, 102526. (<a href='https://doi.org/10.1016/j.datak.2025.102526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining is a prominent discipline in business process management. It collects a variety of techniques for gathering information from event logs, each fulfilling a different mining purpose. Event logs are always necessary for assessing and validating mining techniques in relation to specific purposes. Unfortunately, event logs are hard to find and usually contain noise that can influence the validity of the results of a mining technique. In this paper, we propose a framework, named purple , for generating, through business model simulation, event logs tailored for different mining purposes, i.e., discovery, what-if analysis, and conformance checking. It supports the simulation of models specified in different languages, by projecting their execution onto a common behavioral model, i.e., a labeled transition system. We present eleven instantiations of the framework implemented in a software tool by-product of this paper. The framework is validated against reference log generators through experiments on the purposes presented in the paper.},
  archive      = {J_DKE},
  author       = {Andrea Burattin and Barbara Re and Lorenzo Rossi and Francesco Tiezzi},
  doi          = {10.1016/j.datak.2025.102526},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102526},
  shortjournal = {Data Knowl. Eng.},
  title        = {A framework for purpose-guided event logs generation},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A fine-grained multi-lingual opinion mining method on social media texts using multi-scale fused features-based adaptive residual convolutional LSTM with attention mechanism. <em>DKE</em>, <em>161</em>, 102524. (<a href='https://doi.org/10.1016/j.datak.2025.102524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilingual opinion mining has grown within Natural Language Processing (NLP), particularly in the setting of social media. Because it offers valuable data from content provided by users, social networks have grown exponentially in recent years, enabling people to express their opinions and share their ideas on a variety of subjects. Many Sentiment Analysis (SA) strategies have been developed subsequently to gather emotional information from the feedback. The primary limitations of these methods include longer training time and decreased accuracy. To solve this issue, the research work introduced a multi-lingual opinion mining model for analyzing public opinions that are useful in businesses and organizations. Firstly, the texts from social media are accumulated from standard data sources. The collected texts are pre-processed to remove unnecessary content, including promotional content and spam texts. Next, the features from pre-processed text are extracted using techniques including Bidirectional Encoder Representations from Transformers (BERT), N-gram and word2vec. Here, BERT understand the sentiment of a word based on its surrounding words, allowing for more accurate sentiment detection, particularly in complex linguistic structures present in different languages. Further, N-grams extract the features from multilingual datasets, where different languages may have unique syntactic and semantic structures. Word2Vec can effectively capture phrases and idioms that convey specific sentiments. This is particularly beneficial in multilingual contexts where expressions may vary significantly between languages. The extracted features from BERT, N-gram and word2vec are given to the developed Multi-scale Fused Features based Adaptive Residual Convolutional Long Short-Term Memory with Attention mechanism (MARCLA) for analyzing the opinion of the public in different languages. Here, the sentiments expressed in the complex and varied text data are accurately interpreted by the Residual Conv-LSTM. The multiscale mechanism captures the micro and the macro level linguistic features, and the residual connections combat the issues of vanishing gradient, which aid in effectively training the deep network. In addition, the parameters from the suggested MARCLA are optimized using a Modified Random Function-based Parrot Optimizer (MRFPO). This model is beneficial in understanding public sentiment more effectively. The suggested opinion mining model’s performance is compared with conventional techniques to ensure our model’s ability. The accuracy of the designed MRFPO- MARCLA framework is 95.12 %, which is higher than the conventional frameworks like CNN, LSTM, CoNBiLSTM and MARCLA, respectively. Thus, the experimental findings demonstrated that the developed multi-lingual opinion mining approach can effectively help the organizations to monitor sentiment changes and public reactions across different languages.},
  archive      = {J_DKE},
  author       = {D. Kavitha and Ashwin Kumar S and Divya Priya B A and M V Guru Prasadh and Sri Krishna S and Sidh Parakh and Shriram. V and Tabish Rashid},
  doi          = {10.1016/j.datak.2025.102524},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102524},
  shortjournal = {Data Knowl. Eng.},
  title        = {A fine-grained multi-lingual opinion mining method on social media texts using multi-scale fused features-based adaptive residual convolutional LSTM with attention mechanism},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An optimization enabled hierarchical attention -deep LSTM model for sentiment analysis on cloth products from customer rating. <em>DKE</em>, <em>161</em>, 102523. (<a href='https://doi.org/10.1016/j.datak.2025.102523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary aim of the study endeavour is to introduce a deep learning approaches augmented with optimization techniques to conduct sentiment analysis on apparel products, utilize customer reviews and ratings as foundational data. Consequently, a review of a clothing item is utilized as input, which undergoes pre-processing involving the elimination of stop words and stemming to eradicate superfluous information. In parallel, critical features are extracted from the pre-processed data to facilitate effective categorization. Thereafter, feature extraction is executed through execution of Term frequency-inverse document frequency (TF-IDF), SentiWordNet features, positive sentiment scores, negative sentiment scores, the count of capitalized words, and hashtags. Subsequently, feature fusion is conducted utilizing the proposed Trend factor smoothing-Siberian Tiger Optimization (TS-STO), which is innovatively premeditated by integrating trend factor smoothing within the update process of Siberian Tiger Optimization (STO). Ultimately, sentiment analysis is conducted through the implementation of HA-Deep LSTM, which is conceived by merging Hierarchical Attention Network with Deep LSTM. Experimental analysis portrayed that presented approach conquered an accuracy of 95.9 %, a sensitivity of 96.1 % and specificity of 94.2 %.},
  archive      = {J_DKE},
  author       = {Zhijun Chen and Tsungshun Hsieh and Ze Chen},
  doi          = {10.1016/j.datak.2025.102523},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102523},
  shortjournal = {Data Knowl. Eng.},
  title        = {An optimization enabled hierarchical attention -deep LSTM model for sentiment analysis on cloth products from customer rating},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Temporal knowledge graph recommendation with sequence-aware and path reasoning. <em>DKE</em>, <em>161</em>, 102522. (<a href='https://doi.org/10.1016/j.datak.2025.102522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph recommendation (KGRec) models not only alleviate the issues of data sparsity and the cold start problem encountered by traditional models but also enhance interpretability and credibility through the provision of explicit recommendation rationales. Nonetheless, existing KGRec models predominantly concentrate on extracting static structural features of user preferences from KG, often neglecting the dynamic temporal features, such as purchase time and click time. This oversight results in considerable limitations in recommendation performance. In response to this challenge, this paper introduces a novel temporal knowledge graph recommendation model (TKGRec), which fully utilizes both dynamic temporal feature and static structure feature for better recommendation. We specifically construct a temporal KG that encapsulates both static and dynamic user–item interactions. Based on the new environment, we propose a sequence-aware and path reasoning framework, in which the sequence-aware module employs a dual-attention mechanism to distill temporal features from interactions, whereas the path reasoning module utilizes reinforcement learning to extract path features. These two modules are seamlessly fused and iteratively refined to capture a more holistic understanding of user preferences. Experimental results on three real-world datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art baseline models in terms of performance.},
  archive      = {J_DKE},
  author       = {Yuanming Zhang and Ziyou He and Yongbiao Lou and Haixia Long and Fei Gao},
  doi          = {10.1016/j.datak.2025.102522},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102522},
  shortjournal = {Data Knowl. Eng.},
  title        = {Temporal knowledge graph recommendation with sequence-aware and path reasoning},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Secure data storage in multi-cloud environments using lattice-based saber with diffie-hellman cryptography and authenticate based on PUF-ECC. <em>DKE</em>, <em>161</em>, 102512. (<a href='https://doi.org/10.1016/j.datak.2025.102512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human life has become highly dependent on data in recent decades almost every facet of daily activities, leading to its storage in multi-cloud environments. To ensure data integrity, confidentiality, and privacy, it is essential to protect data from unauthorized access. This paper proposes a novel approach for securing data in multi-cloud environments for user authentication and data storage using Lattice-Based Saber Cryptography combined with PUF-ECC and the Enhanced Goose Optimization Algorithm (EGOA). The initial user authentication is achieved through the PUF-ECC digital signature algorithm, which verifies both the user's and the device's identity. Once authenticated, user data is securely transmitted to the cloud server based on Lattice-Based Saber post-quantum cryptography combined with the Diffie-Hellman key exchange protocol. The encrypted data is then stored across multiple cloud storage through a cloud controller using RAM-based chunking. For efficient data retrieval, the Enhanced Goose Optimization Algorithm (EGOA) is employed to extract encrypted data from clouds. Finally, the data is decrypted using the Lattice-Based Saber decryption algorithm and securely retrieved by the authenticated user. This method enhances both the security and efficiency of cloud data management and retrieval. The experiment is carried out with the proposed methodologies and also compared with the existing technologies. The proposed approach achieves encryption times of 9.68 ms, key generation times of 4.84 ms, and block creation times of 1.59 ms, while maintaining a 93.7 % confidentiality rate, a 98 % packet delivery ratio, a transmission delay of 0.026 ms, throughput of 407.33 MB/s, jitter of 3.26 ms, and an RTT of 0.17 ms, demonstrating its effectiveness in secure data storage and retrieval in multi-cloud environments.},
  archive      = {J_DKE},
  author       = {R. Iyswarya and R. Anitha},
  doi          = {10.1016/j.datak.2025.102512},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102512},
  shortjournal = {Data Knowl. Eng.},
  title        = {Secure data storage in multi-cloud environments using lattice-based saber with diffie-hellman cryptography and authenticate based on PUF-ECC},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ASF: A novel associative scoring function for embedded knowledge graph reasoning. <em>DKE</em>, <em>161</em>, 102511. (<a href='https://doi.org/10.1016/j.datak.2025.102511'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important tools for knowledge management is the Knowledge Graph (KG), a multi-relational graph that depicts rich factual information across entities. A KG represents entities as nodes and relations as edges, with each edge represented by a triplet: (head entity, relation, tail entity). The Scoring Function (SF) in a KG quantifies the plausibility of these triplets and is often derived from KG embeddings. However, due to the distinct relational patterns across KGs, an SF that performs well on one KG might fail on another, making the design of optimal SFs a challenging task. This study introduces the concept of an Associative Scoring Function (ASF), which leverages Association Rule Mining (ARM) to discover and incorporate patterns and characteristics of symmetric, asymmetric, inverse, and other relational types within embedded KGs. The ARM technique in ASF uses the FP-Growth algorithm to extract meaningful associations, which is enhanced further through hyperparameter tuning. Extensive experiments on benchmark datasets demonstrate that ASF is KG-independent and performs better than state-of-the-art SFs. These results highlight ASF's potential to generalize across diverse KGs, offering a significant advancement in the KG link prediction task.},
  archive      = {J_DKE},
  author       = {MVPT Lakshika and HA Caldera},
  doi          = {10.1016/j.datak.2025.102511},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102511},
  shortjournal = {Data Knowl. Eng.},
  title        = {ASF: A novel associative scoring function for embedded knowledge graph reasoning},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A graph-based model for semantic textual similarity measurement. <em>DKE</em>, <em>161</em>, 102509. (<a href='https://doi.org/10.1016/j.datak.2025.102509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring semantic similarity between sentence pairs is a fundamental problem in Natural Language Processing with applications in various domains, including machine translation, speech recognition, automatic question answering, and text summarization. Despite its significance, accurately assessing semantic similarity remains a challenging task, particularly for underrepresented languages such as Vietnamese. Existing methods have yet to fully leverage the unique linguistic characteristics of Vietnamese for semantic similarity measurement. To address this limitation, we propose GBNet-STS (Graph-Based Network for Semantic Textual Similarity), a novel framework for measuring the semantic similarity of Vietnamese sentence pairs. GBNet-STS integrates lexical-grammatical similarity scores and distributional semantic similarity scores within a multi-layered graph-based model. By capturing different semantic perspectives through multiple interconnected layers, our approach provides a more comprehensive and robust similarity estimation. Experimental results demonstrate that GBNet-STS outperforms traditional methods, achieving state-of-the-art performance in Vietnamese semantic similarity tasks.},
  archive      = {J_DKE},
  author       = {Van-Tan Bui and Quang-Minh Nguyen and Van-Vinh Nguyen and Duc-Toan Nguyen},
  doi          = {10.1016/j.datak.2025.102509},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102509},
  shortjournal = {Data Knowl. Eng.},
  title        = {A graph-based model for semantic textual similarity measurement},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Rule-guided process discovery. <em>DKE</em>, <em>161</em>, 102508. (<a href='https://doi.org/10.1016/j.datak.2025.102508'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event data extracted from information systems serves as the foundation for process mining, enabling the extraction of insights and identification of improvements. Process discovery focuses on deriving descriptive process models from event logs, which form the basis for conformance checking, performance analysis, and other applications. Traditional process discovery techniques predominantly rely on event logs, often overlooking supplementary information such as domain knowledge and process rules. These rules, which define relationships between activities, can be obtained through automated techniques like declarative process discovery or provided by domain experts based on process specifications. When used as an additional input alongside event logs, such rules have significant potential to guide process discovery. However, leveraging rules to discover high-quality imperative process models, such as BPMN models and Petri nets, remains an underexplored area in the literature. To address this gap, we propose an enhanced framework, IMr, which integrates discovered or user-defined rules into the process discovery workflow via a novel recursive approach. The IMr framework employs a divide-and-conquer strategy, using rules to guide the selection of process structures at each recursion step in combination with the input event log. We evaluate our approach on several real-world event logs and demonstrate that the discovered models better align with the provided rules without compromising their conformance to the event log. Additionally, we show that high-quality rules can improve model quality across well-known conformance metrics. This work highlights the importance of integrating domain knowledge into process discovery, enhancing the quality, interpretability, and applicability of the resulting process models.},
  archive      = {J_DKE},
  author       = {Ali Norouzifar and Marcus Dees and Wil van der Aalst},
  doi          = {10.1016/j.datak.2025.102508},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102508},
  shortjournal = {Data Knowl. Eng.},
  title        = {Rule-guided process discovery},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LQ-FJS: A logical query-digging fake-news judgment system with structured video-summarization engine using LLM. <em>DKE</em>, <em>161</em>, 102507. (<a href='https://doi.org/10.1016/j.datak.2025.102507'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of online social platforms can greatly benefit people by fostering remote relationships, but it also inevitably amplifies the impact of multimodal fake news on societal trust and ethics. Existing fake-news detection AI systems are still vulnerable to the inconspicuous and indiscernible multimodal misinformation, and often lacking interpretability and accuracy in cross-platform settings. Hence, we propose a new innovative logical query-digging fake-news judgment system (LQ-FJS) to tackle the above problem based on multimodal approach. The LQ-FJS verifies the truthfulness of claims made within multimedia news by converting video content into structured textual summaries. It then acts as an interpretable agent, explaining the reasons for identified fake news by the structured video-summarization engine (SVSE) to act as an interpretable detection intermediary agent. The SVSE generates condensed captions for raw video content, converting it into structured textual narratives. Then, LQ-FJS exploits these condensed captions to retrieve reliable information related to the video content from LLM. Thus, LQ-FJS cross-verifies external knowledge sources and internal LLM responses to determine whether contradictions exist with factual information through a multimodal inconsistency verification procedure. Our experiments demonstrate that the subtle summarization produced by SVSE can facilitate the generation of explanatory reports that mitigate large-scale trust deficits caused by opaque “black-box” models. Our experiments show that LQ-FJS improves F1 scores by 4.5% and 7.2% compared to state-of-the-art models (FactLLaMA 2023 and HiSS 2023), and increases 14% user trusts through interpretable conclusions.},
  archive      = {J_DKE},
  author       = {Jhing-Fa Wang and Din-Yuen Chan and Hsin-Chun Tsai and Bo-Xuan Fang},
  doi          = {10.1016/j.datak.2025.102507},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102507},
  shortjournal = {Data Knowl. Eng.},
  title        = {LQ-FJS: A logical query-digging fake-news judgment system with structured video-summarization engine using LLM},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ABBA: Index structure for sequential pattern-based aggregate queries. <em>DKE</em>, <em>161</em>, 102506. (<a href='https://doi.org/10.1016/j.datak.2025.102506'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern-based aggregate (PBA) queries constitute an important and widely used type of analytical queries in sequence OLAP (S-OLAP) systems. Unfortunately, finding accurate answers to PBA queries in the S-OLAP system is often very expensive both in terms of time and memory consumption. In this paper we propose an efficient and easily maintainable index structure called the ABBA Index, which addresses the problem of PBA query processing. Experiments conducted using the KDD Cup data and public transport passengers’ travel behavior data show that our index outperforms state-of-the art solutions while requiring much less memory. The ABBA Index can be easily extended to support pattern-based aggregate queries over hierarchy (PBA-H), a novel class of analytical queries which we introduce as the second main contribution of the paper. Sensitivity, scalability and complexity analysis of the ABBA Index is also provided.},
  archive      = {J_DKE},
  author       = {Witold Andrzejewski and Tadeusz Morzy and Maciej Zakrzewicz},
  doi          = {10.1016/j.datak.2025.102506},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102506},
  shortjournal = {Data Knowl. Eng.},
  title        = {ABBA: Index structure for sequential pattern-based aggregate queries},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Elevating human-machine collaboration in NLP for enhanced content creation and decision support. <em>DKE</em>, <em>161</em>, 102505. (<a href='https://doi.org/10.1016/j.datak.2025.102505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-machine collaboration in Natural Language Processing (NLP) is revolutionizing content creation and decision support by seamlessly combining the strengths of both entities for enhanced efficiency and quality. The lack of seamless integration between human creativity and machine efficiency in NLP hinders optimal content creation and decision support. The objective of this study is to explore and promote the integration of human-machine collaboration in NLP to enhance both content creation and decision support processes. Data Acquisition for NLP requests involves defining the task and target audience, identifying relevant data sources like text documents and web data, and incorporating human expertise for data curation through validation and annotation. Machine processing techniques like tokenization, stemming/lemmatization, and removal of stop words, as well as human input for tasks like data annotation and error correction, to improve data quality and relevance for NLP applications. The combination of automated processing and human feedback leads to more precise and dependable effects. Techniques such as sentiment analysis, topic modelling, and entity recognition are utilized to excerpt valued perceptions from the data and enhance collaboration between humans and machines. These techniques help to streamline the NLP process and ensure that the system is providing accurate and relevant information to users. The analysis of NLP models in machine processing involves training the models to perform specific tasks, such as summarization, sentiment analysis, information extraction, trend identification, and creative content generation. The results show that social media leads with 90% usage, pivotal for audience engagement, while blogs at 78% highlight their depth in content creation implementation using Python software. These trained models are then used to improve decision-making processes, generate creative content, and enhance the accuracy of search results. The future scope involves leveraging advanced NLP techniques to deepen the collaboration between humans and machines for more effective content creation and decision support.},
  archive      = {J_DKE},
  author       = {Priyanka V. Deshmukh and Aniket K. Shahade},
  doi          = {10.1016/j.datak.2025.102505},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102505},
  shortjournal = {Data Knowl. Eng.},
  title        = {Elevating human-machine collaboration in NLP for enhanced content creation and decision support},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ELEVATE-ID: Extending large language models for end-to-end entity linking evaluation in indonesian. <em>DKE</em>, <em>161</em>, 102504. (<a href='https://doi.org/10.1016/j.datak.2025.102504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their effectiveness in low-resource languages remains underexplored, particularly in complex tasks such as end-to-end Entity Linking (EL), which requires both mention detection and disambiguation against a knowledge base (KB). In earlier work, we introduced IndEL — the first end-to-end EL benchmark dataset for the Indonesian language — covering both a general domain (news) and a specific domain (religious text from the Indonesian translation of the Quran), and evaluated four traditional end-to-end EL systems on this dataset. In this study, we propose ELEVATE-ID, a comprehensive evaluation framework for assessing LLM performance on end-to-end EL in Indonesian. The framework evaluates LLMs under both zero-shot and fine-tuned conditions, using multilingual and Indonesian monolingual models, with Wikidata as the target KB. Our experiments include performance benchmarking, generalization analysis across domains, and systematic error analysis. Results show that GPT-4 and GPT-3.5 achieve the highest accuracy in zero-shot and fine-tuned settings, respectively. However, even fine-tuned GPT-3.5 underperforms compared to DBpedia Spotlight — the weakest of the traditional model baselines — in the general domain. Interestingly, GPT-3.5 outperforms Babelfy in the specific domain. Generalization analysis indicates that fine-tuned GPT-3.5 adapts more effectively to cross-domain and mixed-domain scenarios. Error analysis uncovers persistent challenges that hinder LLM performance: difficulties with non-complete mentions, acronym disambiguation, and full-name recognition in formal contexts. These issues point to limitations in mention boundary detection and contextual grounding. Indonesian-pretrained LLMs, Komodo and Merak, reveal core weaknesses: template leakage and entity hallucination, respectively—underscoring architectural and training limitations in low-resource end-to-end EL. 1},
  archive      = {J_DKE},
  author       = {Ria Hari Gusmita and Asep Fajar Firmansyah and Hamada M. Zahera and Axel-Cyrille Ngonga Ngomo},
  doi          = {10.1016/j.datak.2025.102504},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102504},
  shortjournal = {Data Knowl. Eng.},
  title        = {ELEVATE-ID: Extending large language models for end-to-end entity linking evaluation in indonesian},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Time-aware complex question answering over temporal knowledge graph. <em>DKE</em>, <em>161</em>, 102503. (<a href='https://doi.org/10.1016/j.datak.2025.102503'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Question Answering (KGQA) is a crucial topic in Knowledge Graphs (KGs), with the objective of retrieving the corresponding facts from KGs to answer given questions. In practical applications, facts in KGs usually have time constraints, thus, question answering on Temporal Knowledge Graphs (TKGs) has attracted extensive attention. Existing Temporal Knowledge Graph Question Answering (TKGQA) methods focus on dealing with complex questions involving multiple facts, and mainly face two challenges. First, these methods only consider matching questions with facts in TKGs to identify the answer, ignoring the temporal order between different facts, which makes it challenging to solve the questions involving temporal order. Second, they usually focus on the representation of the question text while neglecting the rich semantic information within the questions, which leads to certain limitations in understanding question. To address the above challenges, this research proposes a model named Time-Aware Complex Question Answering (TA-CQA). Specifically, we extend the Temporal Knowledge Graph Embedding (TKGE) model by incorporating temporal order information into the embedding vectors, ensuring that the model can distinguish the temporal order of different facts. To enhance the semantic representation of the question, we integrate question information using attention mechanism and learnable encoder. Different from the previous TKGQA methods, we propose time relevance measurement to further enhance the accuracy of answer prediction by better capturing the correlation between question information and time information. Multiple sets of experiments on CronQuestions and TimeQuestions demonstrate our model’s superior performance across all question types. In particular, for complex questions involving multiple facts, the hit@1 values are increased by 3.2% and 3.5% respectively.},
  archive      = {J_DKE},
  author       = {Luyi Bai and Tongyue Zhang and Guangchen Feng},
  doi          = {10.1016/j.datak.2025.102503},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102503},
  shortjournal = {Data Knowl. Eng.},
  title        = {Time-aware complex question answering over temporal knowledge graph},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Conceptual modeling of user perspectives — From data warehouses to alliance-driven data ecosystems. <em>DKE</em>, <em>161</em>, 102502. (<a href='https://doi.org/10.1016/j.datak.2025.102502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing complexity of modern information systems has highlighted the need for advanced conceptual modeling techniques that incorporate multi-perspective and view-based approaches. This paper explores the role of multi-perspective modeling and view modeling in designing distributed, heterogeneous systems while addressing diverse user requirements and ensuring semantic consistency. These methods enable the representation of multiple viewpoints, traceability, and dynamic integration across different levels of abstraction. Key advancements in schema mapping, view maintenance, and semantic metadata management are examined, illustrating how they support query optimization, data quality, and interoperability. We discuss how data management architectures, such as data ecosystems, data warehouses, and data lakes, leverage these innovations to enable flexible and sustainable data sharing. By integrating user-centric and goal-oriented modeling frameworks, the alignment of technical design with organizational and social requirements is emphasized. Future challenges include the need for enhanced reasoning capabilities and collaborative tools to manage the growing complexity of interconnected systems while maintaining adaptability and trust.},
  archive      = {J_DKE},
  author       = {Sandra Geisler and Christoph Quix and István Koren and Matthias Jarke},
  doi          = {10.1016/j.datak.2025.102502},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102502},
  shortjournal = {Data Knowl. Eng.},
  title        = {Conceptual modeling of user perspectives — From data warehouses to alliance-driven data ecosystems},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Source-free domain adaptation with complex distribution considerations for time series data. <em>DKE</em>, <em>161</em>, 102501. (<a href='https://doi.org/10.1016/j.datak.2025.102501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from a labeled source domain to an unlabeled target domain without accessing source domain data, thereby protecting source domain privacy. Although SFDA has recently been applied to time series data, the inherent complex distribution characteristics including temporal variability and distributional diversity of such data remain underexplored. Time series data exhibit significant dynamic variability influenced by collection environments, leading to discrepancies between sequences. Additionally, multidimensional time series data face distributional diversity across dimensions. These complex characteristics increase the learning difficulty for source models and widen the adaptation gap between the source and target domains. To address these challenges, this paper proposes a novel SFDA method for time series data, named Adaptive Latent Subdomain feature extraction and joint Prediction (ALSP). The method divides the source domain, which has a complex distribution, into multiple latent subdomains with relatively simple distributions, thereby effectively capturing the features of different subdistributions. It extracts latent domain-specific and domain-invariant features to identify subdomain-specific characteristics. Furthermore, it combines domain-specific classifiers and a domain-invariant classifier to enhance model performance through multi-classifier joint prediction. During target domain adaptation, ALSP reduces domain dependence by extracting invariant features, thereby narrowing the distributional gap between the source and target domains. Simultaneously, it leverages prior knowledge from the source domain distribution to support the hypothesis space and dynamically adapt to the target domain. Experiments on three real-world datasets demonstrate that ALSP achieves superior performance in cross-domain time series classification tasks, significantly outperforming existing methods.},
  archive      = {J_DKE},
  author       = {Jing Shang and Zunming Chen and Zhiwen Xiao and Zhihui Wu and Yifei Zhang and Jibing Wang},
  doi          = {10.1016/j.datak.2025.102501},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102501},
  shortjournal = {Data Knowl. Eng.},
  title        = {Source-free domain adaptation with complex distribution considerations for time series data},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Conceptual modeling: A large language model assistant for characterizing research contributions. <em>DKE</em>, <em>161</em>, 102497. (<a href='https://doi.org/10.1016/j.datak.2025.102497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The body of conceptual modeling research publications is vast and diverse, making it challenging for a single researcher or research group to fully comprehend the field’s overall development. Although some approaches have been proposed to help organize these research contributions, it is still unrealistic to expect human experts to manually comprehend and characterize all of this research. However, as generative AI tools based on large language models, such as ChatGPT, become increasingly sophisticated, it may be possible to replace or augment tedious, manual work with semi-automated approaches. In this research, we present a customized version of ChatGPT that is tuned to the task of characterizing conceptual modeling research. Experiments with this AI tool demonstrate that it is feasible to create a usable knowledge survey for the continually evolving body of conceptual modeling research contributions.},
  archive      = {J_DKE},
  author       = {Stephen W. Liddle and Heinrich C. Mayr and Oscar Pastor and Veda C. Storey and Bernhard Thalheim},
  doi          = {10.1016/j.datak.2025.102497},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102497},
  shortjournal = {Data Knowl. Eng.},
  title        = {Conceptual modeling: A large language model assistant for characterizing research contributions},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dynamic time warping for classifying long-term trends in time series. <em>DKE</em>, <em>161</em>, 102495. (<a href='https://doi.org/10.1016/j.datak.2025.102495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the potential of dynamic time warping (DTW) for recognizing different segments in time series data characterized by their long-term trends and curvature. To perform classification, a set of reference data for each class is required, where each time series in the reference set represents a typical shape of that class. The classification process involves computing the DTW distance between a given time series and each reference time series, then assigning the time series to the class with the minimum distance. Experiments on both simulated and real-world time series data from two different use cases demonstrate that DTW can correctly classify the different segments. Additionally, the paper investigates whether incorrectly classified phases could indicate data security issues. Additional experiments are performed to assess the number of data points required to reliably classify a segment correctly. These experiments highlight the limitations and emphasize the importance of selecting good reference data.},
  archive      = {J_DKE},
  author       = {Anna-Christina Glock and Klaus Chmelina and Johannes Fürnkranz and Thomas Hütter},
  doi          = {10.1016/j.datak.2025.102495},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102495},
  shortjournal = {Data Knowl. Eng.},
  title        = {Dynamic time warping for classifying long-term trends in time series},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Semantic-aware query answering with large language models. <em>DKE</em>, <em>161</em>, 102494. (<a href='https://doi.org/10.1016/j.datak.2025.102494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern data-driven world, answering queries over heterogeneous and semantically inconsistent data remains a significant challenge. Modern datasets originate from diverse sources, such as relational databases, semi-structured repositories, and unstructured documents, leading to substantial variability in schemas, terminologies, and data formats. Traditional systems, constrained by rigid syntactic matching and strict data binding, struggle to capture critical semantic connections and schema ambiguities, failing to meet the growing demand among data scientists for advanced forms of flexibility and context-awareness in query answering. In parallel, the advent of Large Language Models (LLMs) has introduced new capabilities in natural language interpretation, making them highly promising for addressing such challenges. However, LLMs alone lack the systematic rigor and explainability required for robust query processing and decision-making in high-stakes domains. In this paper, we propose Soft Query Answering (Soft QA), a novel hybrid approach that integrates LLMs as an intermediate semantic layer within the query processing pipeline. Soft QA enhances query answering adaptability and flexibility by injecting semantic understanding through context-aware, schema-informed prompts, and leverages LLMs to semantically link entities, resolve ambiguities, and deliver accurate query results in complex settings. We demonstrate its practical effectiveness through real-world examples, highlighting its ability to resolve semantic mismatches and improve query outcomes without requiring extensive data cleaning or restructuring.},
  archive      = {J_DKE},
  author       = {Paolo Atzeni and Teodoro Baldazzi and Luigi Bellomarini and Eleonora Laurenza and Emanuel Sallinger},
  doi          = {10.1016/j.datak.2025.102494},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102494},
  shortjournal = {Data Knowl. Eng.},
  title        = {Semantic-aware query answering with large language models},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An integrated requirements framework for analytical and AI projects. <em>DKE</em>, <em>161</em>, 102493. (<a href='https://doi.org/10.1016/j.datak.2025.102493'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To this day, the requirements of data warehouses, user visualizations and ML projects have been tackled in an independent manner, ignoring the possible cross-requirements, collective constraints and dependencies between the outputs of the different systems that should be taken into account to ensure a successful analytical project. In this work, we take a holistic approach and propose a methodology that supports modeling and subsequent analysis while taking into account these three aspects. This methodology has several advantages, mainly that (i) it enables us to identify possible conflicts between actors on different tasks that are overlooked if the systems are treated in an isolated manner and (ii) this holistic view enables modeling multi-company systems, where the information or even the analytical results can be provided by third-parties, identifying key participants in federated environments. After presenting the required formalism to carry out this kind of analysis, we showcase it on a real-world running example of the tourism sector.},
  archive      = {J_DKE},
  author       = {Juan Trujillo and Ana Lavalle and Alejandro Reina-Reina and Jorge García-Carrasco and Alejandro Maté and Wolfgang Maaß},
  doi          = {10.1016/j.datak.2025.102493},
  journal      = {Data & Knowledge Engineering},
  month        = {1},
  pages        = {102493},
  shortjournal = {Data Knowl. Eng.},
  title        = {An integrated requirements framework for analytical and AI projects},
  volume       = {161},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
