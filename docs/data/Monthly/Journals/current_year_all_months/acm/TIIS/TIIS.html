<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIIS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tiis">TIIS - 12</h2>
<ul>
<li><details>
<summary>
(2025). Practitioners and bias in machine learning: A study. <em>TIIS</em>, <em>15</em>(2), 1-28. (<a href='https://doi.org/10.1145/3733838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing adoption of machine learning (ML) raises ethical concerns, particularly regarding bias. This study explores how ML practitioners with limited experience in bias understand and apply bias definitions, detection measures, and mitigation methods. Through a take-home task, exercises, and interviews with 22 participants, we identified five key themes: sources of bias, selecting bias metrics, detecting bias, mitigating bias, and ethical considerations. Participants faced unresolved conflicts, such as applying fairness definitions in practice, selecting context-dependent bias metrics, addressing real-world biases, balancing model performance with bias mitigation, and relying on personal perspectives over data-driven metrics. While bias mitigation techniques helped identify biases in two datasets, participants could not fully eliminate bias, citing the oversimplification of complex processes into models with limited variables. We propose designing bias detection tools that encourage practitioners to focus on the underlying assumptions and integrating bias concepts into ML practices, such as using a harmonic mean-based approach, akin to the F1 score, to balance bias and accuracy.},
  archive      = {J_TIIS},
  author       = {Robert Cinca and Enrico Costanza and Mirco Musolesi},
  doi          = {10.1145/3733838},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {6},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Practitioners and bias in machine learning: A study},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Panda or not panda? understanding adversarial attacks with interactive visualization. <em>TIIS</em>, <em>15</em>(2), 1-31. (<a href='https://doi.org/10.1145/3725739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial machine learning (AML) studies attacks that can fool machine learning algorithms into generating incorrect outcomes as well as the defenses against worst-case attacks to strengthen model robustness. Specifically for image classification, it is challenging to understand adversarial attacks due to their use of subtle perturbations that are not human-interpretable, as well as the variability of attack impacts influenced by diverse methodologies, instance differences, and model architectures. Through a design study with AML learners, and teachers, we introduce AdvEx , a multi-level interactive visualization system that comprehensively presents the properties and impacts of evasion attacks on different image classifiers for novice AML learners. We quantitatively and qualitatively assessed AdvEx in a two-part evaluation including user studies and expert interviews. Our results show that AdvEx is not only highly effective as a visualization tool for understanding AML mechanisms but also provides an engaging and enjoyable learning experience, thus demonstrating its overall benefits for AML learners.},
  archive      = {J_TIIS},
  author       = {Yuzhe You and Jarvis Tse and Jian Zhao},
  doi          = {10.1145/3725739},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {5},
  number       = {2},
  pages        = {1-31},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Panda or not panda? understanding adversarial attacks with interactive visualization},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAIFAI: Human-AI interaction for mental face reconstruction. <em>TIIS</em>, <em>15</em>(2), 1-26. (<a href='https://doi.org/10.1145/3725891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present HAIFAI—a novel two-stage system where humans and AI interact to tackle the challenging task of reconstructing a visual representation of a face that exists only in a person’s mind. In the first stage, users iteratively rank images our reconstruction system presents based on their resemblance to a mental image. These rankings, in turn, allow the system to extract relevant image features, fuse them into a unified feature vector and use a generative model to produce an initial reconstruction of the mental image. The second stage leverages an existing face editing method, allowing users to manually refine and further improve this reconstruction using an easy-to-use slider interface for face shape manipulation. To avoid the need for tedious human data collection for training the reconstruction system, we introduce a computational user model of human ranking behaviour. For this, we collected a small face ranking dataset through an online crowd-sourcing study containing data from 275 participants. We evaluate HAIFAI and an ablated version in a 12-participant user study and demonstrate that our approach outperforms the previous state of the art regarding reconstruction quality, usability, perceived workload and reconstruction speed. We further validate the reconstructions in a subsequent face ranking study with 18 participants and show that HAIFAI achieves a new state-of-the-art identification rate of 60.6%. These findings represent a significant advancement towards developing new interactive intelligent systems capable of reliably and effortlessly reconstructing a user’s mental image.},
  archive      = {J_TIIS},
  author       = {Florian Strohm and Mihai Bâce and Andreas Bulling},
  doi          = {10.1145/3725891},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {5},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {HAIFAI: Human-AI interaction for mental face reconstruction},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The author’s Journey—Understanding and improving the authoring process of theory-driven socially intelligent agents. <em>TIIS</em>, <em>15</em>(2), 1-40. (<a href='https://doi.org/10.1145/3711672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art agent-modelling tools support the creation of powerful Socially Intelligent Agents (SIAs) capable of engaging in social interactions with participants in various roles and environments. However, their deployment demands a labourious authoring task as it is necessary to manually define behaviour rules and create content for different interaction scenarios. While Socially Intelligent Agents (SIAs) research has centred on the user experience, we shift focus to the authors. To understand the challenges faced by authors who create these agents, we performed an innovative analysis of the authoring experience in modern agent modelling tools. One key finding is that, while SIA concepts are generally understandable, emotional-based concepts are not as easily comprehended or used by authors. We propose a hybrid solution approach that culminated in the development of Authoring-Assisted FAtiMA-Toolkit. The augmented agent modelling tool incorporates a data-driven Authoring Assistant to boost author productivity while promoting transparency and authorial control. To evaluate the impact of this framework on the authoring experience, we conducted a user study. Results showed that authors using the Authoring-Assisted FAtiMA-Toolkit were on average able to create more SIA-related content in less time. Our findings suggest that data-augmented, theory-grounded agent modelling tools can support the development of affective social agents by reducing the authoring burden without sacrificing the framework’s clarity or the authors’ control over the content.},
  archive      = {J_TIIS},
  author       = {Manuel Guimarães and Joana Campos and Pedro A. Santos and João Dias and Rui Prada},
  doi          = {10.1145/3711672},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {2},
  pages        = {1-40},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {The author’s Journey—Understanding and improving the authoring process of theory-driven socially intelligent agents},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioural indicators of usability in visual analytics dashboards. <em>TIIS</em>, <em>15</em>(2), 1-35. (<a href='https://doi.org/10.1145/3715710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information presentation problems on interactive dashboards are known to hinder decision-making. Since a traditional user-centred approach to designing usable dashboards cannot fully satisfy user demands, needs and skills, we isolate behavioural indicators of usability when users conduct typical information-seeking and comparison tasks. In a first study (N = 50), we identified strategies derived from 486,435 interaction events logged in a controlled setting with synthetic dashboards. User models consisting of these user strategies and graph literacy produced strong signals indicating that usability was predictable. In a second study (N = 65), we tested the initial insights on real-world dashboards. While most of our hypotheses were confirmed, graph literacy emerged as the best predictor of usability. Usability was better predicted in dashboards with problems, suggesting promising opportunities for automated usability evaluation and real-time support for users struggling with visual analytics dashboards.},
  archive      = {J_TIIS},
  author       = {Mohammed Alhamadi and Hatim Alsayahani and Sarah Clinch and Markel Vigo},
  doi          = {10.1145/3715710},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {2},
  pages        = {1-35},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Behavioural indicators of usability in visual analytics dashboards},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MALACHITE—Enabling users to teach GUI-aware natural language interfaces. <em>TIIS</em>, <em>15</em>(2), 1-29. (<a href='https://doi.org/10.1145/3716141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users can adapt contemporary natural language interfaces (NLIs) by teaching the NLIs how to handle new natural language (NL) inputs. One promising approach is interactive task learning (ITL), which enables users to teach new NL inputs for multi-modal systems. While recent advances enable users to teach the syntactic and semantic level of the NL inputs through ITL, NLIs are still not able to learn how to consider the context, such as the current state of the graphical user interface (GUI). To address this challenge, we designed MALACHITE through three formative studies. MALACHITE enables users to successfully teach NL inputs on a semantic and syntactic level leveraging the GUI screen of a data visualization tool. With two evaluative studies, we provide evidence that with MALACHITE ’s suggestions, users significantly improve their accuracy by a factor of 2.3 in teaching GUI-dependent NL inputs in contrast to those without MALACHITE ’s suggestions.},
  archive      = {J_TIIS},
  author       = {Marcel Ruoff and Brad A. Myers and Alexander Maedche},
  doi          = {10.1145/3716141},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {2},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {MALACHITE—Enabling users to teach GUI-aware natural language interfaces},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards understanding AI delegation: The role of self-efficacy and visual processing ability. <em>TIIS</em>, <em>15</em>(1), 1-24. (<a href='https://doi.org/10.1145/3696423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work has proposed AI models that can learn to decide whether to make a prediction for a task instance or to delegate it to a human by considering both parties’ capabilities. In simulations with synthetically generated or context-independent human predictions, delegation can help improve the performance of human-AI teams—compared to humans or the AI model completing the task alone. However, so far, it remains unclear how humans perform and how they perceive the task when individual instances of a task are delegated to them by an AI model. In an experimental study with 196 participants, we show that task performance and task satisfaction improve for the instances delegated by the AI model, regardless of whether humans are aware of the delegation. Additionally, we identify humans’ increased levels of self-efficacy as the underlying mechanism for these improvements in performance and satisfaction, and one dimension of cognitive ability as a moderator to this effect. In particular, AI delegation can buffer potential negative effects on task performance and task satisfaction for humans with low visual processing ability. Our findings provide initial evidence that allowing AI models to take over more management responsibilities can be an effective form of human-AI collaboration in workplaces.},
  archive      = {J_TIIS},
  author       = {Monika Westphal and Patrick Hemmer and Michael Vössing and Max Schemmer and Sebastian Vetter and Gerhard Satzger},
  doi          = {10.1145/3696423},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Towards understanding AI delegation: The role of self-efficacy and visual processing ability},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-augmented predictions: LLM assistants improve human forecasting accuracy. <em>TIIS</em>, <em>15</em>(1), 1-25. (<a href='https://doi.org/10.1145/3707649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) match and sometimes exceed human performance in many domains. This study explores the potential of LLMs to augment human judgment in a forecasting task. We evaluate the effect on human forecasters of two LLM assistants: one designed to provide high-quality (“superforecasting”) advice, and the other designed to be overconfident and base-rate neglecting, thus providing noisy forecasting advice. We compare participants using these assistants to a control group that received a less advanced model that did not provide numerical predictions or engage in explicit discussion of predictions. Participants ( N \(=\) 991) answered a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our preregistered analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24% and 28% compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 41%, compared with 29% for the noisy assistant. We further examine whether LLM forecasting augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our data do not consistently support these hypotheses. Our results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice. However, the effects of outliers suggest that further research into the robustness of this pattern is needed.},
  archive      = {J_TIIS},
  author       = {Philipp Schoenegger and Peter S. Park and Ezra Karger and Sean Trott and Philip E. Tetlock},
  doi          = {10.1145/3707649},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {AI-augmented predictions: LLM assistants improve human forecasting accuracy},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ConverSearch: Supporting experts in human behavior analysis of conversational videos with a multimodal scene search tool. <em>TIIS</em>, <em>15</em>(1), 1-31. (<a href='https://doi.org/10.1145/3709012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal scene search of conversations is essential for unlocking valuable insights into social dynamics and enhancing our communication. While experts in conversational analysis have their own knowledge and skills to find key scenes, a lack of comprehensive, user-friendly tools that streamline the processing of diverse multimodal queries impedes efficiency and objectivity. To address this gap, we developed ConverSearch , a visual-programming-based tool based on insights for effective interface and implementation design derived from a formative study with experts. The tool allows experts to integrate various machine learning algorithms to capture human behavioral cues without the need for coding. Our user study, employing the System Usability Scale (SUS) and satisfaction metrics, demonstrated high user preference, reflecting the tool’s ease of use and effectiveness in supporting scene search tasks. Additionally, through a deployment trial within industrial organizations, we confirmed the tool’s objectivity, reusability, and potential to enhance expert workflows. This suggests the advantages of expert-AI collaboration in domains requiring human contextual understanding and demonstrates how customizable, transparent tools yielding reusable artifacts can support expert-driven tasks in complex, multimodal environments.},
  archive      = {J_TIIS},
  author       = {Riku Arakawa and Kiyosu Maeda and Hiromu Yakura},
  doi          = {10.1145/3709012},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {2},
  number       = {1},
  pages        = {1-31},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {ConverSearch: Supporting experts in human behavior analysis of conversational videos with a multimodal scene search tool},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ajna: A wearable shared perception system for extreme sensemaking. <em>TIIS</em>, <em>15</em>(1), 1-29. (<a href='https://doi.org/10.1145/3690829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the design and prototype of Ajna, a wearable shared perception system for supporting extreme sensemaking in emergency scenarios. Ajna addresses technical challenges in Augmented Reality (AR) devices, specifically the limitations of depth sensors and cameras. These limitations confine object detection to close proximity and hinder perception beyond immediate surroundings, through obstructions, or across different structural levels, impacting collaborative use. It harnesses the Inertial Measurement Unit (IMU) in AR devices to measure users’ relative distances from a set physical point, enabling object detection sharing among multiple users across obstacles like walls and over distances. We tested Ajna’s effectiveness in a controlled study with 15 participants simulating emergency situations in a multi-story building. We found that Ajna improved object detection, location awareness, and situational awareness and reduced search times by 15%. Ajna’s performance in simulated environments highlights the potential of artificial intelligence (AI) to enhance sensemaking in critical situations, offering insights for law enforcement, search and rescue, and infrastructure management.},
  archive      = {J_TIIS},
  author       = {Matthew Wilchek and Kurt Luther and Feras A. Batarseh},
  doi          = {10.1145/3690829},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Ajna: A wearable shared perception system for extreme sensemaking},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taking into account opponent’s arguments in human-agent negotiations. <em>TIIS</em>, <em>15</em>(1), 1-35. (<a href='https://doi.org/10.1145/3691643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous negotiating agents, which can interact with other agents, aim to solve decision-making problems involving participants with conflicting interests. Designing agents capable of negotiating with human partners requires considering some factors, such as emotional states and arguments. For this purpose, we introduce an extended taxonomy of argument types capturing human speech acts during the negotiation. We propose an argument-based automated negotiating agent that can extract human arguments from a chat-based environment using a hierarchical classifier. Consequently, the proposed agent can understand the received arguments and adapt its strategy accordingly while negotiating with its human counterparts. We initially conducted human-agent negotiation experiments to construct a negotiation corpus to train our classifier. According to the experimental results, it is seen that the proposed hierarchical classifier successfully extracted the arguments from the given text. Moreover, we conducted a second experiment where we tested the performance of the designed negotiation strategy considering the human opponent’s arguments and emotions. Our results showed that the proposed agent beats the human negotiator and gains higher utility than the baseline agent.},
  archive      = {J_TIIS},
  author       = {Anıl Doğru and Mehmet Onur Keskin and Reyhan Aydoğan},
  doi          = {10.1145/3691643},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-35},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Taking into account opponent’s arguments in human-agent negotiations},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ConfusionLens: Focus+Context visualization interface for performance analysis of multiclass image classifiers. <em>TIIS</em>, <em>15</em>(1), 1-21. (<a href='https://doi.org/10.1145/3700139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building higher-quality image classification models requires better performance analysis (PA) to help understand their behaviors. We propose ConfusionLens, a dynamic and interactive visualization interface that augments a conventional confusion matrix with focus+context visualization. This interface allows users to seamlessly switch table layouts among three views (overall view, class-level view, and between-class view) while observing all of the instance images in a single screen. We designed and implemented a ConfusionLens prototype that supports hundreds of instances, and then conducted a user study (N \(=\) 14) to evaluate it compared to the conventional confusion matrix with a split view of instances. Results show that ConfusionLens achieved faster task-completion time in observing instance-level performance and higher accuracy in observing between-class confusion. Moreover, we conducted an expert interview (N \(=\) 6) to investigate the applicability of our interface to practical PA tasks, and then implemented several extensions of ConfusionLens based on the feedback. Feedback on these extensions from users experienced in image classification (N \(=\) 5) demonstrated their general usefulness and highlighted their beneficial use in PA tasks.},
  archive      = {J_TIIS},
  author       = {Kazuyuki Fujita and Keito Uwaseki and Hongyu Bu and Kazuki Takashima and Yoshifumi Kitamura},
  doi          = {10.1145/3700139},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {ConfusionLens: Focus+Context visualization interface for performance analysis of multiclass image classifiers},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
