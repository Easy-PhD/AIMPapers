<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIIS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tiis">TIIS - 18</h2>
<ul>
<li><details>
<summary>
(2025). Using emotion diversification based on movie reviews to improve the user experience of movie recommender systems. <em>TIIS</em>, <em>15</em>(3), 1-27. (<a href='https://doi.org/10.1145/3743147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diversifying movie recommendations is an effective way to address choice overload, a phenomenon where recommenders generate lists with highly similar recommendations that are difficult to choose from. However, existing diversification algorithms often rely on latent features, which limits their interpretability and makes it less clear why a particular set of movies is recommended. Given that movies are designed to elicit emotional responses, researchers have suggested leveraging these responses to enhance recommender system performance. This study introduces a novel “emotion diversification” approach, which diversifies movie recommendations based on emotional signals extracted from audience reviews. We evaluate this method against latent and non-diversified baselines in a controlled user study (N = 115), finding that it significantly improves perceived taste coverage and system satisfaction without compromising recommendation quality. Going beyond the traditional rating- and/or interaction data used by traditional recommender systems, our work demonstrates the user experience benefits of extracting emotional data from rich, qualitative user feedback and using it to give users a more emotionally diverse set of recommendations.},
  archive      = {J_TIIS},
  author       = {Lior Lansman and Osnat Mokryn and Lijie Guo and Mehtab Iqbal and Bart P. Knijnenburg},
  doi          = {10.1145/3743147},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Using emotion diversification based on movie reviews to improve the user experience of movie recommender systems},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MV-crafter: An intelligent system for music-guided video generation. <em>TIIS</em>, <em>15</em>(3), 1-27. (<a href='https://doi.org/10.1145/3748515'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music videos, as a prevalent form of multimedia entertainment, deliver engaging audio-visual experiences to audiences and have gained immense popularity among singers and fans. Creators can express their interpretations of music naturally through visual elements. However, the creation process of music video demands proficiency in script design, video shooting, and music-video synchronization, posing significant challenges for non-professionals. Previous work has designed automated music video generation frameworks. However, they suffer from complexity in input and poor output quality. In response, we present MV-Crafter, a system capable of producing high-quality music videos with synchronized music-video rhythm and style. Our approach involves three technical modules that simulate the human creation process: the script generation module, video generation module, and music-video synchronization module. MV-Crafter leverages a large language model to generate scripts considering the musical semantics. To address the challenge of synchronizing short video clips with music of varying lengths, we propose a dynamic beat-matching algorithm and visual envelope-induced warping method to ensure precise, monotonic music-video synchronization. Besides, we design a user-friendly interface to simplify the creation process with intuitive editing features. Extensive experiments have demonstrated that MV-Crafter provides an effective solution for improving the quality of generated music videos.},
  archive      = {J_TIIS},
  author       = {Chuer Chen and Shengqi Dang and Yuqi Liu and Nanxuan Zhao and Yang Shi and Nan Cao},
  doi          = {10.1145/3748515},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {MV-crafter: An intelligent system for music-guided video generation},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2024 TiiS best paper announcement. <em>TIIS</em>, <em>15</em>(3), 1. (<a href='https://doi.org/10.1145/3749645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TIIS},
  author       = {Shlomo Berkovsky},
  doi          = {10.1145/3749645},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {2024 TiiS best paper announcement},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imperfections of XAI: Phenomena influencing AI-assisted decision-making. <em>TIIS</em>, <em>15</em>(3), 1-40. (<a href='https://doi.org/10.1145/3750052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing use of AI, recent research in human–computer interaction explores Explainable AI (XAI) to make AI advice more interpretable. While research addresses the effects of incorrect AI advice on AI-assisted decision-making, the impact of incorrect explanations is neglected so far. Additionally, recent work shows that not only different explanation modalities impact decision-makers, but also human factors play a critical role. To analyze relevant phenomena influencing AI-assisted decision-making, this work explores the impacting factors by conceptualizing theories of appropriate reliance and taking the first steps toward empirical evidence. We show that humans’ reliance on AI and the human–AI team performance are impacted by imperfect XAI in a study with 136 participants. Additionally, we find that cognitive styles affect decision-making in different explanation modalities. Hence, we shed light on diverse factors that impact human–AI collaboration and provide guidelines for designers to tailor such human–AI collaboration systems to individuals’ needs.},
  archive      = {J_TIIS},
  author       = {Philipp Spitzer and Katelyn Morrison and Violet Turri and Michelle Feng and Adam Perer and Niklas Kühl},
  doi          = {10.1145/3750052},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-40},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Imperfections of XAI: Phenomena influencing AI-assisted decision-making},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of personality recognition in response to virtual reality and 2D emotional stimulus using ECG signals. <em>TIIS</em>, <em>15</em>(3), 1-19. (<a href='https://doi.org/10.1145/3707648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personality primarily refers to the unique and stable way of a person’s thinking and behavior. A few studies have recently been conducted on personality recognition using physiological signals, most of which have used 2D emotional stimulus materials. Virtual reality (VR) has been utilized in many fields, and its superiority over 2D in emotion recognition has been proven. However, relevant research on VR scenes is lacking in the field of personality recognition. In this study, based on the psychological principle that emotional arousal can expose an individual’s personality, we attempt to explore the feasibility and effect of using electrocardiogram (ECG) signals in response to VR emotional stimuli for personality identification. For this purpose, a VR-2D emotion-induction experiment was conducted in which ECG signals were collected, and physiological datasets of emotional personalities were constructed through preprocessing and feature extraction. Statistical analysis of the emotion scale scores and ECG features of the participants showed that the VR group had a higher number of significantly correlated features. Meanwhile, VR- and 2D-based personality recognition models were constructed using machine learning algorithms. The results showed that the VR-based personality recognition model achieved better results for the four personality dimensions, with a maximum accuracy of 79.76%. These findings indicate that VR not only enhances the physiological correlation between emotion and personality but also improves the classification accuracy of personality recognition.},
  archive      = {J_TIIS},
  author       = {Jialan Xie and Ping Lan and Zhaonian Hu and Guangyuan Liu},
  doi          = {10.1145/3707648},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {3},
  pages        = {1-19},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Comparative analysis of personality recognition in response to virtual reality and 2D emotional stimulus using ECG signals},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What are you looking forward to? deliberate positivity as a promising strategy for conversational agents. <em>TIIS</em>, <em>15</em>(3), 1-40. (<a href='https://doi.org/10.1145/3725738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational agents (CAs) are one of the most promising technologies for helping older adults maintain independence longer by augmenting their support and social networks. Voice-based technology in particular is especially powerful in this regard due to its accessibility and ease of use. There is also a growing body of evidence supporting the potential use of such technology in mitigating common issues such as loneliness and isolation, particularly for independent older adults aging in place. One of the key challenges for smart technologies deployed in this context is the development and maintenance of long-term user engagement and adoption, which is often addressed by attempting to closely mimic human social interactions. However, the more human-like the system, the more glaring fault conditions become, and the more jarring they are for users. In this study we explore the effectiveness of an alternative conversational strategy meant to encourage users to engage in positive reflection and introspection. We detail the iterative design and implementation of a prototype CA developed to engage in social conversation with older adults on selected topics of interest. We then use this system as part of a multi-method approach to investigate the effect of deliberate positivity as a conversational strategy, including its impact on user impressions and willingness to continue using the CA. Our results from different approaches, including methods such as psycholinguistic analysis, user self-report, and researcher-based coding, paint a promising picture of this conversational design. We show that the deliberate encouragement by a CA of positive conversation and reflection in users has a measurable positive impact on both user enjoyment and desire to continue engaging with a system. We further demonstrate how some user characteristics may amplify this effect, and discuss the implications of these results for the design and testing of future conversational systems for older adults.},
  archive      = {J_TIIS},
  author       = {Libby Ferland and Risako Owan and Zachary Kunkel and Hannah Qu and Maria Gini and Wilma Koutstaal},
  doi          = {10.1145/3725738},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {3},
  pages        = {1-40},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {What are you looking forward to? deliberate positivity as a promising strategy for conversational agents},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practitioners and bias in machine learning: A study. <em>TIIS</em>, <em>15</em>(2), 1-28. (<a href='https://doi.org/10.1145/3733838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing adoption of machine learning (ML) raises ethical concerns, particularly regarding bias. This study explores how ML practitioners with limited experience in bias understand and apply bias definitions, detection measures, and mitigation methods. Through a take-home task, exercises, and interviews with 22 participants, we identified five key themes: sources of bias, selecting bias metrics, detecting bias, mitigating bias, and ethical considerations. Participants faced unresolved conflicts, such as applying fairness definitions in practice, selecting context-dependent bias metrics, addressing real-world biases, balancing model performance with bias mitigation, and relying on personal perspectives over data-driven metrics. While bias mitigation techniques helped identify biases in two datasets, participants could not fully eliminate bias, citing the oversimplification of complex processes into models with limited variables. We propose designing bias detection tools that encourage practitioners to focus on the underlying assumptions and integrating bias concepts into ML practices, such as using a harmonic mean-based approach, akin to the F1 score, to balance bias and accuracy.},
  archive      = {J_TIIS},
  author       = {Robert Cinca and Enrico Costanza and Mirco Musolesi},
  doi          = {10.1145/3733838},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {6},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Practitioners and bias in machine learning: A study},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Panda or not panda? understanding adversarial attacks with interactive visualization. <em>TIIS</em>, <em>15</em>(2), 1-31. (<a href='https://doi.org/10.1145/3725739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial machine learning (AML) studies attacks that can fool machine learning algorithms into generating incorrect outcomes as well as the defenses against worst-case attacks to strengthen model robustness. Specifically for image classification, it is challenging to understand adversarial attacks due to their use of subtle perturbations that are not human-interpretable, as well as the variability of attack impacts influenced by diverse methodologies, instance differences, and model architectures. Through a design study with AML learners, and teachers, we introduce AdvEx , a multi-level interactive visualization system that comprehensively presents the properties and impacts of evasion attacks on different image classifiers for novice AML learners. We quantitatively and qualitatively assessed AdvEx in a two-part evaluation including user studies and expert interviews. Our results show that AdvEx is not only highly effective as a visualization tool for understanding AML mechanisms but also provides an engaging and enjoyable learning experience, thus demonstrating its overall benefits for AML learners.},
  archive      = {J_TIIS},
  author       = {Yuzhe You and Jarvis Tse and Jian Zhao},
  doi          = {10.1145/3725739},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {5},
  number       = {2},
  pages        = {1-31},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Panda or not panda? understanding adversarial attacks with interactive visualization},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAIFAI: Human-AI interaction for mental face reconstruction. <em>TIIS</em>, <em>15</em>(2), 1-26. (<a href='https://doi.org/10.1145/3725891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present HAIFAI—a novel two-stage system where humans and AI interact to tackle the challenging task of reconstructing a visual representation of a face that exists only in a person’s mind. In the first stage, users iteratively rank images our reconstruction system presents based on their resemblance to a mental image. These rankings, in turn, allow the system to extract relevant image features, fuse them into a unified feature vector and use a generative model to produce an initial reconstruction of the mental image. The second stage leverages an existing face editing method, allowing users to manually refine and further improve this reconstruction using an easy-to-use slider interface for face shape manipulation. To avoid the need for tedious human data collection for training the reconstruction system, we introduce a computational user model of human ranking behaviour. For this, we collected a small face ranking dataset through an online crowd-sourcing study containing data from 275 participants. We evaluate HAIFAI and an ablated version in a 12-participant user study and demonstrate that our approach outperforms the previous state of the art regarding reconstruction quality, usability, perceived workload and reconstruction speed. We further validate the reconstructions in a subsequent face ranking study with 18 participants and show that HAIFAI achieves a new state-of-the-art identification rate of 60.6%. These findings represent a significant advancement towards developing new interactive intelligent systems capable of reliably and effortlessly reconstructing a user’s mental image.},
  archive      = {J_TIIS},
  author       = {Florian Strohm and Mihai Bâce and Andreas Bulling},
  doi          = {10.1145/3725891},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {5},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {HAIFAI: Human-AI interaction for mental face reconstruction},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The author’s Journey—Understanding and improving the authoring process of theory-driven socially intelligent agents. <em>TIIS</em>, <em>15</em>(2), 1-40. (<a href='https://doi.org/10.1145/3711672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art agent-modelling tools support the creation of powerful Socially Intelligent Agents (SIAs) capable of engaging in social interactions with participants in various roles and environments. However, their deployment demands a labourious authoring task as it is necessary to manually define behaviour rules and create content for different interaction scenarios. While Socially Intelligent Agents (SIAs) research has centred on the user experience, we shift focus to the authors. To understand the challenges faced by authors who create these agents, we performed an innovative analysis of the authoring experience in modern agent modelling tools. One key finding is that, while SIA concepts are generally understandable, emotional-based concepts are not as easily comprehended or used by authors. We propose a hybrid solution approach that culminated in the development of Authoring-Assisted FAtiMA-Toolkit. The augmented agent modelling tool incorporates a data-driven Authoring Assistant to boost author productivity while promoting transparency and authorial control. To evaluate the impact of this framework on the authoring experience, we conducted a user study. Results showed that authors using the Authoring-Assisted FAtiMA-Toolkit were on average able to create more SIA-related content in less time. Our findings suggest that data-augmented, theory-grounded agent modelling tools can support the development of affective social agents by reducing the authoring burden without sacrificing the framework’s clarity or the authors’ control over the content.},
  archive      = {J_TIIS},
  author       = {Manuel Guimarães and Joana Campos and Pedro A. Santos and João Dias and Rui Prada},
  doi          = {10.1145/3711672},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {2},
  pages        = {1-40},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {The author’s Journey—Understanding and improving the authoring process of theory-driven socially intelligent agents},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioural indicators of usability in visual analytics dashboards. <em>TIIS</em>, <em>15</em>(2), 1-35. (<a href='https://doi.org/10.1145/3715710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information presentation problems on interactive dashboards are known to hinder decision-making. Since a traditional user-centred approach to designing usable dashboards cannot fully satisfy user demands, needs and skills, we isolate behavioural indicators of usability when users conduct typical information-seeking and comparison tasks. In a first study (N = 50), we identified strategies derived from 486,435 interaction events logged in a controlled setting with synthetic dashboards. User models consisting of these user strategies and graph literacy produced strong signals indicating that usability was predictable. In a second study (N = 65), we tested the initial insights on real-world dashboards. While most of our hypotheses were confirmed, graph literacy emerged as the best predictor of usability. Usability was better predicted in dashboards with problems, suggesting promising opportunities for automated usability evaluation and real-time support for users struggling with visual analytics dashboards.},
  archive      = {J_TIIS},
  author       = {Mohammed Alhamadi and Hatim Alsayahani and Sarah Clinch and Markel Vigo},
  doi          = {10.1145/3715710},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {2},
  pages        = {1-35},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Behavioural indicators of usability in visual analytics dashboards},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MALACHITE—Enabling users to teach GUI-aware natural language interfaces. <em>TIIS</em>, <em>15</em>(2), 1-29. (<a href='https://doi.org/10.1145/3716141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users can adapt contemporary natural language interfaces (NLIs) by teaching the NLIs how to handle new natural language (NL) inputs. One promising approach is interactive task learning (ITL), which enables users to teach new NL inputs for multi-modal systems. While recent advances enable users to teach the syntactic and semantic level of the NL inputs through ITL, NLIs are still not able to learn how to consider the context, such as the current state of the graphical user interface (GUI). To address this challenge, we designed MALACHITE through three formative studies. MALACHITE enables users to successfully teach NL inputs on a semantic and syntactic level leveraging the GUI screen of a data visualization tool. With two evaluative studies, we provide evidence that with MALACHITE ’s suggestions, users significantly improve their accuracy by a factor of 2.3 in teaching GUI-dependent NL inputs in contrast to those without MALACHITE ’s suggestions.},
  archive      = {J_TIIS},
  author       = {Marcel Ruoff and Brad A. Myers and Alexander Maedche},
  doi          = {10.1145/3716141},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {2},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {MALACHITE—Enabling users to teach GUI-aware natural language interfaces},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards understanding AI delegation: The role of self-efficacy and visual processing ability. <em>TIIS</em>, <em>15</em>(1), 1-24. (<a href='https://doi.org/10.1145/3696423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work has proposed AI models that can learn to decide whether to make a prediction for a task instance or to delegate it to a human by considering both parties’ capabilities. In simulations with synthetically generated or context-independent human predictions, delegation can help improve the performance of human-AI teams—compared to humans or the AI model completing the task alone. However, so far, it remains unclear how humans perform and how they perceive the task when individual instances of a task are delegated to them by an AI model. In an experimental study with 196 participants, we show that task performance and task satisfaction improve for the instances delegated by the AI model, regardless of whether humans are aware of the delegation. Additionally, we identify humans’ increased levels of self-efficacy as the underlying mechanism for these improvements in performance and satisfaction, and one dimension of cognitive ability as a moderator to this effect. In particular, AI delegation can buffer potential negative effects on task performance and task satisfaction for humans with low visual processing ability. Our findings provide initial evidence that allowing AI models to take over more management responsibilities can be an effective form of human-AI collaboration in workplaces.},
  archive      = {J_TIIS},
  author       = {Monika Westphal and Patrick Hemmer and Michael Vössing and Max Schemmer and Sebastian Vetter and Gerhard Satzger},
  doi          = {10.1145/3696423},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Towards understanding AI delegation: The role of self-efficacy and visual processing ability},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-augmented predictions: LLM assistants improve human forecasting accuracy. <em>TIIS</em>, <em>15</em>(1), 1-25. (<a href='https://doi.org/10.1145/3707649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) match and sometimes exceed human performance in many domains. This study explores the potential of LLMs to augment human judgment in a forecasting task. We evaluate the effect on human forecasters of two LLM assistants: one designed to provide high-quality (“superforecasting”) advice, and the other designed to be overconfident and base-rate neglecting, thus providing noisy forecasting advice. We compare participants using these assistants to a control group that received a less advanced model that did not provide numerical predictions or engage in explicit discussion of predictions. Participants ( N \(=\) 991) answered a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our preregistered analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24% and 28% compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 41%, compared with 29% for the noisy assistant. We further examine whether LLM forecasting augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our data do not consistently support these hypotheses. Our results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice. However, the effects of outliers suggest that further research into the robustness of this pattern is needed.},
  archive      = {J_TIIS},
  author       = {Philipp Schoenegger and Peter S. Park and Ezra Karger and Sean Trott and Philip E. Tetlock},
  doi          = {10.1145/3707649},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {AI-augmented predictions: LLM assistants improve human forecasting accuracy},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ConverSearch: Supporting experts in human behavior analysis of conversational videos with a multimodal scene search tool. <em>TIIS</em>, <em>15</em>(1), 1-31. (<a href='https://doi.org/10.1145/3709012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal scene search of conversations is essential for unlocking valuable insights into social dynamics and enhancing our communication. While experts in conversational analysis have their own knowledge and skills to find key scenes, a lack of comprehensive, user-friendly tools that streamline the processing of diverse multimodal queries impedes efficiency and objectivity. To address this gap, we developed ConverSearch , a visual-programming-based tool based on insights for effective interface and implementation design derived from a formative study with experts. The tool allows experts to integrate various machine learning algorithms to capture human behavioral cues without the need for coding. Our user study, employing the System Usability Scale (SUS) and satisfaction metrics, demonstrated high user preference, reflecting the tool’s ease of use and effectiveness in supporting scene search tasks. Additionally, through a deployment trial within industrial organizations, we confirmed the tool’s objectivity, reusability, and potential to enhance expert workflows. This suggests the advantages of expert-AI collaboration in domains requiring human contextual understanding and demonstrates how customizable, transparent tools yielding reusable artifacts can support expert-driven tasks in complex, multimodal environments.},
  archive      = {J_TIIS},
  author       = {Riku Arakawa and Kiyosu Maeda and Hiromu Yakura},
  doi          = {10.1145/3709012},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {2},
  number       = {1},
  pages        = {1-31},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {ConverSearch: Supporting experts in human behavior analysis of conversational videos with a multimodal scene search tool},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ajna: A wearable shared perception system for extreme sensemaking. <em>TIIS</em>, <em>15</em>(1), 1-29. (<a href='https://doi.org/10.1145/3690829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the design and prototype of Ajna, a wearable shared perception system for supporting extreme sensemaking in emergency scenarios. Ajna addresses technical challenges in Augmented Reality (AR) devices, specifically the limitations of depth sensors and cameras. These limitations confine object detection to close proximity and hinder perception beyond immediate surroundings, through obstructions, or across different structural levels, impacting collaborative use. It harnesses the Inertial Measurement Unit (IMU) in AR devices to measure users’ relative distances from a set physical point, enabling object detection sharing among multiple users across obstacles like walls and over distances. We tested Ajna’s effectiveness in a controlled study with 15 participants simulating emergency situations in a multi-story building. We found that Ajna improved object detection, location awareness, and situational awareness and reduced search times by 15%. Ajna’s performance in simulated environments highlights the potential of artificial intelligence (AI) to enhance sensemaking in critical situations, offering insights for law enforcement, search and rescue, and infrastructure management.},
  archive      = {J_TIIS},
  author       = {Matthew Wilchek and Kurt Luther and Feras A. Batarseh},
  doi          = {10.1145/3690829},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Ajna: A wearable shared perception system for extreme sensemaking},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taking into account opponent’s arguments in human-agent negotiations. <em>TIIS</em>, <em>15</em>(1), 1-35. (<a href='https://doi.org/10.1145/3691643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous negotiating agents, which can interact with other agents, aim to solve decision-making problems involving participants with conflicting interests. Designing agents capable of negotiating with human partners requires considering some factors, such as emotional states and arguments. For this purpose, we introduce an extended taxonomy of argument types capturing human speech acts during the negotiation. We propose an argument-based automated negotiating agent that can extract human arguments from a chat-based environment using a hierarchical classifier. Consequently, the proposed agent can understand the received arguments and adapt its strategy accordingly while negotiating with its human counterparts. We initially conducted human-agent negotiation experiments to construct a negotiation corpus to train our classifier. According to the experimental results, it is seen that the proposed hierarchical classifier successfully extracted the arguments from the given text. Moreover, we conducted a second experiment where we tested the performance of the designed negotiation strategy considering the human opponent’s arguments and emotions. Our results showed that the proposed agent beats the human negotiator and gains higher utility than the baseline agent.},
  archive      = {J_TIIS},
  author       = {Anıl Doğru and Mehmet Onur Keskin and Reyhan Aydoğan},
  doi          = {10.1145/3691643},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-35},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Taking into account opponent’s arguments in human-agent negotiations},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ConfusionLens: Focus+Context visualization interface for performance analysis of multiclass image classifiers. <em>TIIS</em>, <em>15</em>(1), 1-21. (<a href='https://doi.org/10.1145/3700139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building higher-quality image classification models requires better performance analysis (PA) to help understand their behaviors. We propose ConfusionLens, a dynamic and interactive visualization interface that augments a conventional confusion matrix with focus+context visualization. This interface allows users to seamlessly switch table layouts among three views (overall view, class-level view, and between-class view) while observing all of the instance images in a single screen. We designed and implemented a ConfusionLens prototype that supports hundreds of instances, and then conducted a user study (N \(=\) 14) to evaluate it compared to the conventional confusion matrix with a split view of instances. Results show that ConfusionLens achieved faster task-completion time in observing instance-level performance and higher accuracy in observing between-class confusion. Moreover, we conducted an expert interview (N \(=\) 6) to investigate the applicability of our interface to practical PA tasks, and then implemented several extensions of ConfusionLens based on the feedback. Feedback on these extensions from users experienced in image classification (N \(=\) 5) demonstrated their general usefulness and highlighted their beneficial use in PA tasks.},
  archive      = {J_TIIS},
  author       = {Kazuyuki Fujita and Keito Uwaseki and Hongyu Bu and Kazuki Takashima and Yoshifumi Kitamura},
  doi          = {10.1145/3700139},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {ConfusionLens: Focus+Context visualization interface for performance analysis of multiclass image classifiers},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
