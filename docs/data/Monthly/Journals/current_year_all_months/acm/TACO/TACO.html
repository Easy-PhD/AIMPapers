<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TACO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taco">TACO - 119</h2>
<ul>
<li><details>
<summary>
(2025). HopScotch: A holistic approach to data layout-aware mapping on NPUs for high-performance DNN inference. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3711821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep neural networks (DNNs) are widely utilized across a broad range of domains, scaling rapidly and often comprising hundreds of diverse layers with varying types and configurations. To accelerate DNN execution, specialized hardware solutions, known as neural processing units (NPUs), have been developed. However, this heterogeneity of layers in a DNN model may cause performance degradation on NPUs. For example, while a layer’s execution or dataflow is generally associated with a specific data access order, the data layout in on-chip memory may not be well aligned with it, introducing bubble cycles for layout reordering. Given the hundreds of diverse layers in DNNs, this layout reordering overhead presents a new challenge for achieving efficient end-to-end DNN inference on NPUs. To address this problem, this article introduces HopScotch, a holistic approach to data layout-aware mapping of DNNs on NPUs. First, HopScotch adopts a routing interconnect between the on-chip memory and the systolic array utilizing three-input multiplexers, paired with an on-chip programmable vector processor to manage arbitrary data layout reordering at runtime. Additionally, it introduces a tailored data layout to accommodate a variety of convolutional configurations within the proposed microarchitecture. Second, HopScotch presents a novel layout mapping solver that employs a top-k selection strategy based on a beam search algorithm, facilitating the efficient exploration of the vast layout mapping space at compile time. Third, the proposed layout mapping solver is integrated into the HopScotch mapping framework (HMF) to explore the layout mapping space and evaluate the resulting performance. Experiments with popular DNN models show that HopScotch reduces layout reordering costs by up to 98.2% and 90.3%, resulting in speedups of 2.62× and 1.64× in end-to-end latency, compared to XLA and GCD 2 , respectively.},
  archive      = {J_TACO},
  author       = {Suhong Lee and Boyeal Kim and Yongseok Choi and Hyuk-Jae Lee},
  doi          = {10.1145/3711821},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {HopScotch: A holistic approach to data layout-aware mapping on NPUs for high-performance DNN inference},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LarQucut: A new cutting and mapping approach for large-sized quantum circuits in distributed quantum computing (DQC) environments. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3730585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed quantum computing (DQC) is a promising way to achieve large-scale quantum computing. However, mapping large-sized quantum circuits in DQC is a challenging job; for example, it is difficult to find an ideal cutting and mapping solution when many qubits, complicated qubit operations, and diverse QPUs are involved. In this study, we propose LarQucut, a new quantum circuit cutting and mapping approach for large-sized circuits in DQC. LarQucut has several new designs. (1) LarQucut can have cutting solutions that use fewer cuts, and it does not cut a circuit into independent sub-circuits, therefore reducing the overall cutting and computing overheads. (2) LarQucut finds isomorphic sub-circuits and reuses their execution results. So, LarQucut can reduce the number of sub-circuits that need to be executed to reconstruct the large circuit's output, reducing the time spent on sampling the sub-circuits. (3) We design an adaptive quantum circuit mapping approach, which identifies qubit interaction patterns and accordingly enables the best-fit mapping policy in DQC. The experimental results show that, for large circuits with hundreds to thousands of qubits in DQC, LarQucut can provide a better cutting and mapping solution with lower overall overheads and achieves results closer to the ground truth.},
  archive      = {J_TACO},
  author       = {Xinglei Dou and Lei Liu and Zhuohao Wang and Pengyu Li},
  doi          = {10.1145/3730585},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {LarQucut: A new cutting and mapping approach for large-sized quantum circuits in distributed quantum computing (DQC) environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards optimizing learned index for high performance, memory efficiency and NUMA awareness. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3736168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned indexes provide significant performance advantages over classical ordered indexes. However, current learned indexes face challenges regarding tradeoffs between performance and space, as well as scalability issues in platforms with multiple NUMA nodes. These limitations hinder the practical application of learned indexes in production environments. This article presents DiffLex, a learned index with high-performance, memory-efficiency, and NUMA-awareness. The core design of DiffLex is to perform differentiated management based on the popularity of data. For optimal performance, DiffLex stores newly inserted data in sparse delta arrays and frequently accessed data in sparse hot cache arrays. However, for cold data that occupy a majority of the storage space, DiffLex stores them in dense arrays and conducts compression to reduce memory costs. DiffLex ensures NUMA-awareness by partitioning sparse deltas and replicating the hot cache arrays across multiple NUMA nodes. Additionally, we propose a persistent version of DiffLex tailored for emerging persistent memory devices. Our evaluation results demonstrate that DiffLex achieving 3.88× and 1.82× performance improvements compared to state-of-the-art learned indexes, while maintaining a compact index size.},
  archive      = {J_TACO},
  author       = {Lixiao Cui and Kedi Yang and Yusen Li and Gang Wang and Xiaoguang Liu},
  doi          = {10.1145/3736168},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Towards optimizing learned index for high performance, memory efficiency and NUMA awareness},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benchmarking WebAssembly for embedded systems. <em>TACO</em>, <em>22</em>(3), 1-21. (<a href='https://doi.org/10.1145/3736169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WebAssembly is a modern, low-level virtual machine with designed for improved application performance in web browsers. Recently, WebAssembly gained interest for its use outside the web, for example as a replacement for serverless container runtimes. A number of non-web WebAssembly implementations are actively supported, some of which target microcontrollers, IoT devices, and embedded systems. Such hardware platforms have strict resource constraints which may render the usage of WebAssembly impossible or too costly, for example, due to its performance overhead and memory requirements. However, it is currently unclear what performance to expect of WebAssembly on low-resource microcontrollers compared with machine code and alternative application virtual machines. To answer this question, we evaluated the processing overhead and memory characteristics of WebAssembly application virtual machines on microcontrollers, and compared it to native execution, and the established application virtual machines: MicroPython and Lua. Furthermore, we analyzed the feature-set and architecture of the WebAssembly implementations in more detail, and measured the performance impact different runtime features have. We found that WebAssembly, despite its high extensibility and versatility in supported source languages, application paradigms, and target hardware, delivers very competitive performance. We conclude that WebAssembly can find wider industry usage for embedded systems and could replace other more costly or less flexible virtualization techniques, such as Java.},
  archive      = {J_TACO},
  author       = {Konrad Moron and Stefan Wallentowitz},
  doi          = {10.1145/3736169},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Benchmarking WebAssembly for embedded systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BigLittleMCA: A spatially-optimal tiled hardware accelerator for MCMC image processing. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3736171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov-Chain Monte-Carlo (MCMC) algorithms offer a general framework for performing interpretable inference but have high overheads due to the computational complexity of the sampling process and the large number of samples required to produce an accurate result. Computer Vision is a common class of workloads that can be performed using MCMC methods. As computer vision workloads trend toward high-resolution real-time inference, it becomes challenging to perform inference in contexts such as edge computing, which operates under strict power and area budgets. Previous work explores hardware techniques for efficient sampling; however, MCMC algorithms still require many samples. We reduce the overheads of Gibbs Sampling, an MCMC algorithm, using an approach we call mixed-resolution sampling. This approach uses low-resolution inference to provide a starting point for full-resolution sampling. We evaluate this approach on three important computer vision tasks: stereo matching, optical flow, and blind source separation. Mixed-resolution sampling reduces root mean square error (RMSE) by an average of 19.6% for stereo-matching tasks, 13% for optical flow tasks, and 6.3% for blind source separation relative to traditional Gibbs Sampling. To enable real-time, explainable MCMC inference under edge power constraints, we exploit the structure of mixed-resolution sampling to architect and implement a hardware-software co-designed accelerator architecture, BigLittleMCA ( Big - Little MC MC A ccelerator). BigLittleMCA is a tiled MCMC accelerator architecture that uses a small sampler for low-resolution sampling and a large sampler for full-resolution sampling. Our results show that the architecture sustains real-time 720p inference at 30 FPS (frames per second) using 48.5% less power than prior work.},
  archive      = {J_TACO},
  author       = {Chris Kjellqvist and Lisa Wills and Alvin Lebeck},
  doi          = {10.1145/3736171},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {BigLittleMCA: A spatially-optimal tiled hardware accelerator for MCMC image processing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attack and defense: Enhancing robustness of binary hyper-dimensional computing. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3736172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyper-Dimensional Computing (HDC) has emerged as a lightweight computational model, renowned for its robust and efficient learning capabilities, particularly suitable for resource-constrained hardware. As HDC often finds its application in edge devices, the associated security challenges pose a critical concern that cannot be ignored. In this work, we aim to quantitatively delve into the robustness of binary HDC, which is widely recognized for its robustness. Employing the bit-flip attack as our initial focal point, we meticulously devise both an attack mechanism and a corresponding defense mechanism. Our objective is to comprehensively explore the robustness of the binary hyper-dimensional computation model, aiming to gain a deeper understanding of its security vulnerabilities and potential defenses. Specifically, we introduce a novel attack framework for HDC, named HyperAttack, which is capable of compromising a robust binary HDC model by maliciously flipping a minimal number of bits within its memory system (specifically, the DRAM) that houses the associative memory. The bit-flip operation is executed through the well-known Row Hammer attack, and HyperAttack optimizes the accuracy degradation by pinpointing the most vulnerable bits in the hyper-dimensional vectors (represented as binary vectors within the associative memory) of the HDC model. The proposed HyperAttack framework is grounded in the principles of fuzziness, seamlessly integrating dimensional ranking and feature similarity analysis within hypervectors to precisely identify the bits to be flipped. Furthermore, we have developed a defense mechanism named HyperDefense, designed to bolster the robustness of binary hyper-dimensional computational models against bit-flip attacks. This defense scheme is tailored specifically for HDC models, providing a robust safeguard against potential threats. HyperDefense operates directly on the associative memory of HDC models, strengthening their defenses. By meticulously modifying selected bits, HyperDefense maintains a high level of accuracy close to the original model, even in the face of increased bit flip rates. This defense mechanism leverages redundant dimensions as backups for critical information. Through a thorough analysis of dimension importance, HyperDefense achieves superior robustness by gracefully sacrificing non-critical dimensions, thus ensuring the model’s robustness against potential attacks.},
  archive      = {J_TACO},
  author       = {Haomin Li and Fangxin Liu and Zongwu Wang and Ning Yang and Shiyuan Huang and Xiaoyao Liang and Haibing Guan and Li Jiang},
  doi          = {10.1145/3736172},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Attack and defense: Enhancing robustness of binary hyper-dimensional computing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GECC: A GPU-based high-throughput framework for elliptic curve cryptography. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3736176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elliptic Curve Cryptography (ECC) is an encryption method that provides security comparable to traditional techniques like Rivest–Shamir–Adleman (RSA) but with lower computational complexity and smaller key sizes, making it a competitive option for applications such as blockchain, secure multi-party computation, and database security. However, the throughput of ECC is still hindered by the significant performance overhead associated with elliptic curve (EC) operations, which can affect their efficiency in real-world scenarios. This article presents gECC, a versatile framework for ECC optimized for GPU architectures, specifically engineered to achieve high-throughput performance in EC operations. To maximize throughput, gECC incorporates batch-based execution of EC operations and microarchitecture-level optimization of modular arithmetic. It employs Montgomery’s trick [ 40 ] to enable batch EC computation and incorporates novel computation parallelization and memory management techniques to maximize the computation parallelism and minimize the access overhead of GPU global memory. Furthermore, we analyze the primary bottleneck in modular multiplication by investigating how the user codes of modular multiplication are compiled into hardware instructions and what these instructions’ issuance rates are. We identify that the efficiency of modular multiplication is highly dependent on the number of Integer Multiply-Add (IMAD) instructions. To eliminate this bottleneck, we propose novel techniques to minimize the number of IMAD instructions by leveraging predicate registers to pass the carry information and using addition and subtraction instructions (IADD3) to replace IMAD instructions. Our experimental results show that, for ECDSA and ECDH, the two commonly used ECC algorithms, gECC can achieve performance improvements of 5.56 × and 4.94 ×, respectively, compared to the state-of-the-art GPU-based system. In a real-world blockchain application, we can achieve performance improvements of 1.56 ×, compared to the state-of-the-art CPU-based system. gECC is completely and freely available at https://github.com/CGCL-codes/gECC .},
  archive      = {J_TACO},
  author       = {Qian Xiong and Weiliang Ma and Xuanhua Shi and Yongluan Zhou and Hai Jin and Kaiyi Huang and Haozhou Wang and Zhengru Wang},
  doi          = {10.1145/3736176},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GECC: A GPU-based high-throughput framework for elliptic curve cryptography},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging iterative applications to improve the scalability of task-based programming models on distributed systems. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3743134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed tasking models such as OmpSs-2@Cluster, StarPU-MPI, and PaRSEC express HPC applications as task graphs with explicit dependencies. The single task graph unifies the representation of parallelism across CPU cores, accelerators, and distributed-memory nodes, offering higher programmer productivity compared to traditional MPI + X. Most task-based models construct the task graph sequentially, which provides a clear and familiar programming model, simplifying code development, maintenance, and porting. However, this design introduces a bottleneck in task creation and dependency management, limiting performance and scalability. As a result, unless the tasks are very coarse-grained, current distributed sequential tasking models cannot match the performance of MPI + X. Many scientific applications, however, are iterative in nature, constructing the same directed acyclic task graph at each timestep. We exploit this structure to eliminate the sequential bottleneck and control message overhead in a sequentially-constructed distributed tasking model, while preserving its simplicity and productivity. Our approach builts on the recently proposed taskiter directive for OpenMP and OmpSs-2, allowing a single iteration to be expressed as a cyclic graph. The runtime partitions the cyclic graph across nodes, precomputes the MPI transfers, and then executes the loop body at low overhead. By integrating the MPI communications directly into the application’s task graph, our approach naturally overlaps computation and communication, in some cases exposing dramatically more parallelism than fork–join MPI + OpenMP. We define the programming model and describe the full runtime implementation, and integrate our proposal into OmpSs-2@Cluster. We evaluate it using five benchmarks on up to 128 nodes of the MareNostrum 5 supercomputer. For applications with fork–join parallelism, our approach has performance similar to fork–join MPI + OpenMP, making it a viable productive alternative, unlike the existing OmpSs-2@Cluster model, which is up to 7.7 times slower than MPI + OpenMP. For a 2D Gauss–Seidel stencil computation, our approach enables 3D wavefront computation, giving performance up to 22 times faster than fork–join MPI + OpenMP and on-a-par with state-of-the-art TAMPI + OmpSs-2. All software, comprising the compiler, runtime, and benchmarks, is released open source. 1},
  archive      = {J_TACO},
  author       = {Omar Shaaban Ibrahim ali and Juliette Fournis d'Albiat and Isabel Piedrahita and Vicenç Beltran and Xavier Martorell and Paul Carpenter and Eduard Ayguadé and Jesus Labarta},
  doi          = {10.1145/3743134},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Leveraging iterative applications to improve the scalability of task-based programming models on distributed systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scheduling language chronology: Past, present, and future. <em>TACO</em>, <em>22</em>(3), 1-31. (<a href='https://doi.org/10.1145/3743135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling languages express to a compiler—or equivalently, a code generator—a sequence of optimizations to apply. Performance tools that support a scheduling language interface allow exploration of optimizations, i.e., exploratory compilers . While scheduling languages have become a common feature of tools for experts, the proliferation of these languages without unifying common features may be confusing to users. Moreover, we recognize a need to organize the compiler developer community around common exploratory compiler infrastructure, and future advances to address, for example, data layout and data movement. To support a broader set of users may require raising the level of abstraction. This article provides a chronology of scheduling languages, discussing their origins in iterative compilation and autotuning, noting the common features that are used in existing frameworks, and calling for changes to increase their utility and portability.},
  archive      = {J_TACO},
  author       = {Mary Hall and Cosmin E. Oancea and Anne C. Elster and Ari Rasch and Sameeran Joshi and Amir Mohammad Tavakkoli and Richard Schulze},
  doi          = {10.1145/3743135},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-31},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Scheduling language chronology: Past, present, and future},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In-SRAM parallel data shuffle. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3743136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Single Instruction Multiple Data (SIMD) units are widely employed in processors for neural networks, signal processing, and high-performance computing, they suffer from expensive shuffle operations dedicated to data alignment. In fact, shuffle operations only change the layout of data and ideally should be done entirely within memory. To this end, we propose Shuffle SRAM in this article, which can shuffle multiple data elements simultaneously across SRAM banks. The key idea is exploiting inter-bank word line wise data movement to shuffle data in parallel, where all data elements on the same word line of SRAM can be shuffled simultaneously, achieving a high level of parallelism. Through suitable data layout preparation and proper control, Shuffle SRAM efficiently supports a wide range of commonly used shuffle operations. Our evaluation results show that the Shuffle SRAM can reap performance benefits of 14.3× for data reorganization only applications and 1.97× for data reorganization + computation applications over conventional shuffle architecture on general-purpose processors. With Shuffle SRAM, the state-of-the-art vector processor can obtain 2.58× energy efficiency. Compared with traditional SRAM, Shuffle SRAM only increases 3.5% additional area overhead.},
  archive      = {J_TACO},
  author       = {Chaoyang Jia and Zhang Dunbo and Qingjie Lang and Ruoxi Wang and Li Shen},
  doi          = {10.1145/3743136},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {In-SRAM parallel data shuffle},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDAS: Enabling fast data loading for GPU serverless computing. <em>TACO</em>, <em>22</em>(3), 1-23. (<a href='https://doi.org/10.1145/3743137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating GPUs into serverless computing platforms is crucial for improving efficiency. Many GPU functions, such as DNN inferences and scientific services, benefit from GPU usage, which requires only tens to hundreds of milliseconds for pure computation. Under these circumstances, fast data loading is imperative for function performance. However, existing GPU serverless systems face significant data stall issues, leading to extremely low GPU efficiency. Faced with the above problems, we observe opportunities to optimize data loading, such as data preloading and deduplicated data loading. However, these optimizations are impossible in existing GPU serverless systems due to the lack of insights into data information, such as data sizes and read-write attributes of function inputs. To address this, we propose a novel GPU serverless system, EDAS. EDAS first enhances user request specifications, allowing users to annotate data retrieved by GPU functions from the database with additional attributes. Based on this, EDAS takes over data loading from GPU functions and proposes two innovative data loading management schemes: a parallelized data loading scheme and a multi-stage resource exit scheme. Our experimental results show that EDAS reduces function duration by 16.2× and improves system throughput by 1.91× compared with the state-of-the-art serverless platform.},
  archive      = {J_TACO},
  author       = {Han Zhao and Weihao Cui and Quan Chen and Zijun Li and Zhenhua Han and Nan Wang and Yu Feng and Jieru Zhao and Chen Chen and Jingwen Leng and Minyi Guo},
  doi          = {10.1145/3743137},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {EDAS: Enabling fast data loading for GPU serverless computing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CGCGraph: Efficient CPU-GPU co-execution for concurrent dynamic graph processing. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3744904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous growth of user scale and application data, the demand for large-scale concurrent graph processing is increasing. Typically, large-scale concurrent graph processing jobs need to process corresponding snapshots of dynamically changing graph data to obtain information at different time points. To enhance the throughput of such applications, current solutions concurrently process multiple graph snapshots on the GPU. However, when dealing with rapidly changing graph data, transferring multiple snapshots of concurrent jobs to the GPU results in high data transfer overhead between CPU and GPU. Additionally, the execution mode of existing work suffers from underutilization of GPU computational resources. In this work, we introduce CGCGraph, which can be integrated into existing GPU graph processing systems like Subway, to enable efficient concurrent graph snapshot processing jobs and enhance overall system resource utilization. The key idea is to offload unshared graph data of multiple concurrent snapshots to the CPU, reducing CPU-GPU transfer overhead. By implementing CPU-GPU co-execution, there is potential for enhanced utilization of GPU computing resources. Specifically, CGCGraph leverages kernel fusion to process shared graph data concurrently on the GPU, while executing all snapshots in parallel on the CPU, with each snapshot assigned a dedicated thread. This approach enables efficient concurrent processing within a novel CPU-GPU co-execution model, incorporating three optimization strategies targeting storage, computation, and synchronization. We integrate CGCGraph with Subway, an existing system designed for out-of-GPU-memory static graph processing. Experimental results show that the integration of CGCGraph with current GPU-based systems obtains performance improvements ranging from 1.7 to 4.5 times.},
  archive      = {J_TACO},
  author       = {Yiming Sun and Jie Zhang and Huawei Cao and Yuan Zhang and Xuejun An and Junying Huang and Xiaochun Ye},
  doi          = {10.1145/3744904},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {CGCGraph: Efficient CPU-GPU co-execution for concurrent dynamic graph processing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MetaEC: An efficient and resilient erasure-coded KV store on disaggregated memory. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3744905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory KV stores have recently been migrated from traditional monolithic servers to disaggregated memory (DM) for higher resource utilization and elasticity. These works use replication-based schemes for fault tolerance, which can be replaced with erasure coding (EC) for space efficiency. However, existing EC schemes designed in KV stores on traditional monolithic architectures encounter performance constraints when directly implemented in DM due to the challenges in EC metadata management and consistent parity updating. This article proposes MetaEC, an erasure-coded KV store on DM with high efficiency and resilience. First, for organizing KV pairs to stripes, MetaEC logically forms data chunks and leverages lazy coding to remove the accumulating and coding latency from the critical path. Second, for efficient EC metadata management, MetaEC designs EC metadata structures based on accessing features, and employs a hybrid redundancy schema with deterministic distribution to provide fault tolerance with high storage efficiency. Third, for consistent parity updating, we design a parity updating protocol based on parity logging and co-design EC metadata structures to handle concurrent conflicts by allowing only concurrent reads or writes. Experimental results show that compared with the state-of-the-art replication-based KV stores on DM, MetaEC achieves up to 53.33% latency reduction, up to 31.01% throughput improvement, and 58.17% memory consumption savings.},
  archive      = {J_TACO},
  author       = {Qiliang Li and Min Lyu and Tian Liu and Liangliang Xu and Wei Wang and Yinlong Xu},
  doi          = {10.1145/3744905},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {MetaEC: An efficient and resilient erasure-coded KV store on disaggregated memory},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating parallel structures in DNNs via parallel fusion and operator co-optimization. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3744906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel structures have become a key pattern in deep neural networks (DNNs), offering improved efficiency and scalability. However, existing machine learning compilers (MLCs) face challenges in optimizing these structures due to limited parallel fusion scope and insufficient analysis of intra-operator characteristics. This article introduces Magneto, a framework designed to accelerate DNN inference by co-optimizing parallel operators. Magneto broadens the fusion scope and incorporates a specialized co-tuning algorithm to optimize operators jointly. Our approach addresses the unique challenges inherent in optimizing parallel structures, enabling significant performance improvements across various hardware platforms. Experimental results show that Magneto outperforms state-of-the-art NVIDIA TensorRT and AMD MIGraphX, achieving geometric mean speedups of 2.27× and 2.88×, respectively.},
  archive      = {J_TACO},
  author       = {Zhanyuan Di and Leping Wang and Zhaojia Ma and En Shao and Jie Zhao and Ziyi Ren and Siyuan Feng and Dingwen Tao and Guangming Tan and Ninghui Sun},
  doi          = {10.1145/3744906},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Accelerating parallel structures in DNNs via parallel fusion and operator co-optimization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage degradation-based topology reconfiguration algorithm for fault-tolerant multiprocessor arrays. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3744907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the integration density of multiprocessor arrays increases, the likelihood of permanent faults in processing elements (PEs) rises, requiring effective topology reconfiguration for system reliability. However, existing router-based multiprocessor arrays reconfiguration methods predominantly rely on redundancy techniques and lack effective degradation strategies for applications of varying sizes. To address this, we propose a two-stage degradation-based topology reconfiguration algorithm to construct a maximized and high-performance logical array. First, we introduce a novel fault compensation mechanism by defining a set of faulty PE candidates to identify locally optimal fault-free PEs for compensation, minimizing the compensation path. Building upon this, we develop a greedy bidirectional column reconfiguration algorithm that constructs an initial fault-free logical array with short interconnects and prove its maximality. Lastly, we propose a satisfiability-based reconfiguration algorithm, transforming the topology reconfiguration problem into a satisfiability problem via a SAT model, reducing interconnect redundancy, and further optimizing array performance. Experimental results demonstrate that the proposed algorithm consistently outperforms state-of-the-art methods in reducing communication latency and alleviating link congestion, especially under high fault density conditions. Furthermore, as array size and fault density increase, the effectiveness of the proposed method becomes more pronounced, showcasing excellent scalability and robustness.},
  archive      = {J_TACO},
  author       = {Hao Ding and Peiling Song and Yelin Li and Junyan Qian},
  doi          = {10.1145/3744907},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A two-stage degradation-based topology reconfiguration algorithm for fault-tolerant multiprocessor arrays},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Address/Data instruction steering in clustered general purpose processors. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3744908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although they differentiate between integer and floating-point datum, modern Instruction Set Architectures and their implementations do not differentiate integer datum used to address memory from integer datum used in purely arithmetic and logical computations. This is a perfectly reasonable choice as addresses are, in fact, integral quantities. However, in many cases, there is already a fundamental difference between addresses and integer data: Their width. As computer systems moved from 16 to 32, then to 64-bit pointers, with a potential future where 128-bit might be used for specific systems, the data width required to compute a given output with a given algorithm has remained the same, e.g., an ASCII character is still represented on a byte. This work aims to leverage this dichotomy to revisit hardware clustering, a well-known microarchitectural technique used to mitigate the cost of scaling processor backend structures by dividing the backend into several mostly independent execution clusters. We show that by treating instructions as manipulating addresses or data and steering them to a “data” or an “address” cluster accordingly, reasonable cluster load balancing can be achieved without the need for complex steering policies that can lead to performance on par with the baseline with limited hardware overhead. Moreover, we highlight two possible optimizations stemming from this distribution. First, the registers of the “address” cluster can easily be compressed thanks to address spatial and temporal locality. Second, if a processor requires a large address space but only processes narrow data (e.g., 32-bit data with 64-bit pointers or 64-bit data with 128-bit pointers), the “data” cluster datapath can be kept narrower than the “address” cluster datapath.},
  archive      = {J_TACO},
  author       = {Chandana S. Deshpande and Arthur Perais and Frédéric Pétrot},
  doi          = {10.1145/3744908},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Address/Data instruction steering in clustered general purpose processors},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D GNLM: Efficient 3D non-local means kernel with nested reuse strategies for embedded GPUs. <em>TACO</em>, <em>22</em>(3), 1-22. (<a href='https://doi.org/10.1145/3744909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3D Non-Local Means (NLM) algorithm has become a crucial preprocessing technique for 3D image datasets due to its effectiveness in denoising while preserving fine details. This method has been proven to be highly efficient in high-demand tasks within industrial applications such as medical imaging and remote sensing. The 3D NLM algorithm computes the filtered value for each voxel by calculating the weighted average of all voxels within a 3D search window, where the weights are determined by the similarity between pairs of 3D template windows. Therefore, the computational burden becomes significant, especially in embedded GPUs with limited computational power and memory resources. To address this issue, we propose an efficient GPU parallel kernel to minimize redundant computations and memory accesses. The kernel integrates three nested reuse strategies to handle redundant computations in three dimensions: for columns, we leverage the fast data exchange mechanism to reuse column computation results via on-chip registers; for rows, we use a sliding window strategy, utilizing GPU global memory as an intermediary to store and reuse similarity values between filtered rows; and for channels, we introduce a zigzag scanning strategy that enables simultaneous computation across multiple channels and employs on-chip registers to facilitate channel computation reuse. Experimental results demonstrate that our kernel achieves an average speedup of 7.7x on the embedded Jetson AGX Xavier platform across a range of 3D image datasets compared to existing methods, showcasing exceptional performance.},
  archive      = {J_TACO},
  author       = {Xiang Li and Qiong Chang and Yun Li and Jun Miyazaki},
  doi          = {10.1145/3744909},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {3D GNLM: Efficient 3D non-local means kernel with nested reuse strategies for embedded GPUs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ZNSFQ: An efficient and high-performance fair queue scheduling scheme for ZNS SSDs. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3746230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Zoned Namespace (ZNS) interface transfers most storage maintenance responsibilities from the underlying Solid-State Drives (SSDs) to the host. This shift creates new opportunities to ensure fairness and high performance in multi-tenant cloud computing environments at both hardware and software levels. However, when applications with different workloads share a single ZNS SSD hardware, traditional fair queueing schedulers fail to achieve fairness due to their limited awareness of workload characteristics. Moreover, allowing multiple outstanding requests to access the device simultaneously improves resource utilization but often leads to significant I/O interference among these requests. This interference results in over-throttling, which subsequently degrades the performance of existing fair queueing schedulers. To address the above problems, this article proposes an efficient and high-performance fair queueing scheduling scheme for ZNS SSD (ZNSFQ) on the host side. Firstly, ZNSFQ introduces a workload-aware fair scheduler that enhances fairness by accurately estimating the I/O cost for each application based on its workload characteristics. Secondly, to optimize performance while ensuring fairness, ZNSFQ designs a request dispatch parallelism adjuster. This adjuster manages the channel-level request dispatch parallelism for each application to minimize I/O interference. Finally, ZNSFQ employs a global adaptive coordinator to alleviate device-level I/O blocking, reducing tail latency and CPU consumption while satisfying fairness and performance. A comprehensive evaluation demonstrates that ZNSFQ significantly enhances fairness and performance compared to the latest fair queuing schedulers. In sequential access scenarios, ZNSFQ enhances fairness by over 38.13% and increases I/O bandwidth by more than 49.24%. Furthermore, in random access scenarios, it reduces CPU utilization by 70.22% while maintaining both fairness and high performance.},
  archive      = {J_TACO},
  author       = {Yachun Liu and Dan Feng and Jianxi Chen and Jing Hu and Zhouxuan Peng and Jinlei Hu},
  doi          = {10.1145/3746230},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ZNSFQ: An efficient and high-performance fair queue scheduling scheme for ZNS SSDs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance implications of pipelining the data transfer in CPU-GPU heterogeneous systems. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3746231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the increasing demands of machine learning, heterogeneous systems combining CPUs and GPUs have emerged as the dominant architecture for parallel computing in recent years. To optimize memory management and data transfer between CPUs and GPUs, Nvidia GPUs have introduced unified virtual memory ( UVM ) and pinned memory ( PM ) over the last decade. UVM can avoid explicit memory copies and potentially overlap GPU kernel computations with CPU-GPU data transfer. PM ensures that data with high locality remains in the main memory, preventing it from being paged out. In addition to these two techniques, asynchronous memory copy ( Async Memcpy ) was introduced recently in Nvidia GPUs to improve the CPU-GPU pipeline further. By utilizing Async Memcpy , the data transfer from GPU global memory to shared memory can be overlapped with GPU computations, adding an additional stage to the CPU-GPU data transfer pipeline. A thorough performance analysis of how Async Memcpy affects the current UVM and PM CPU-GPU data transfer scheme is desired. In this article, we provide performance implications of the combined effect of UVM , PM , and Async Memcpy , exploring which applications benefit from which combination of these features. We implement all these features on a suite of 25 workloads, including microbenchmarks and realworld applications. We observe an average performance gain of 24% when utilizing UVM and a 34% gain when employing PM on realworld applications, compared to not applying any data transfer optimization techniques. The performance benefits of Async Memcpy vary across different workloads. For workloads featuring extensive shared memory usage and high compute density (e.g., kmeans and lud ), Async Memcpy delivers around a 20% performance improvement over using UVM or PM alone. In other workloads like knn , we note a 20% performance degradation when using Async Memcpy . Furthermore, we conduct an in-depth investigation of the GPU kernel using performance counters to uncover the root causes of performance differences among various data transfer models. We also perform sensitivity analyses to examine how the number of blocks and threads, as well as the L1-cache/shared memory partitioning, impact performance. We explore future research directions aimed at enhancing the data transfer pipeline by overlapping memory allocation with data transfer and computation across GPU kernels.},
  archive      = {J_TACO},
  author       = {Ruihao Li and Bagus Hanindhito and Sanjana Yadav and Qinzhe Wu and Krishna Kavi and Gayatri Mehta and Neeraja J. Yadwadkar and Lizy K. John},
  doi          = {10.1145/3746231},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Performance implications of pipelining the data transfer in CPU-GPU heterogeneous systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partitioned scheduling and analysis for a typed DAG task on heterogeneous multi-cores. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3746232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous multi-core architectures are gaining popularity in recent years as they combine the benefits of different processors, resulting in improved execution capacity and energy efficiency. However, analyzing response times and allocating resources for the typed directed acyclic graph (DAG) task, which has complex execution logic, on heterogeneous multi-core systems poses significant challenges. Major approaches may yield overly pessimistic worst-case response time (WCRT) estimates in certain scenarios while failing to adequately address critical structural characteristics inherent to typed DAG tasks. To address these limitations, this article explores the WCRT analysis and core allocations for the typed DAG task under partitioned scheduling. In this work, we first delve into the characteristics of the topology structure of the typed DAG task and propose a novel WCRT upper bound to enhance the accuracy of WCRT analysis. Then, a subtask allocation strategy is presented, which enables an effectively utilization of the resources of multi-cores. Finally, the performance of the proposed analysis algorithm and allocation strategy are tested by implementing a verification system on a real heterogeneous multi-core platform. Experimental results demonstrate that our proposed WCRT analysis algorithm exhibits substantial improvements of 38.7% and 37.43% in the theoretical analysis performance and actual analysis accuracy, respectively. Similarly, our proposed core allocation strategy improves the theoretical and the actual execution efficiency of the system by 10.6% and 7.41%, respectively. These results substantiate the practical value of our enhanced WCRT derivation methodology and allocation scheme in improving system resource utilization efficiency.},
  archive      = {J_TACO},
  author       = {Yulong Wu and Yehan Ma and Mingdong Xie and Weizhe Zhang},
  doi          = {10.1145/3746232},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Partitioned scheduling and analysis for a typed DAG task on heterogeneous multi-cores},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCSolver: Accelerating sparse iterative solvers via divide-and-conquer on GPUs. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3746233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse iterative solvers are commonly used in various fields. However, certain essential kernels of these solvers, such as sparse triangular solves (SpTRSV), present significant challenges for efficient parallelization due to data dependencies . Previous methods, like level-scheduling or multi-coloring, typically involve creating a Task Dependency Graph (TDG) to represent data dependencies and identify independent sets from the TDG for parallel execution. However, these approaches often result in limited parallelism with substantial synchronization overheads or negatively impact the solver convergence rate. This article introduces DCSolver , a Divide-and-Conquer (DC) framework designed to efficiently parallelize sparse solvers with data dependencies on GPUs. To achieve this, we break down the solver TDG into independent subgraphs, allowing us to exploit both coarse-grained and fine-grained parallelism. To efficiently allocate GPU threads for subgraphs with varying degrees of parallelism, we have developed an adaptive in-warp scheduling strategy. Additionally, we propose a hybrid parallelization scheme in DCSolver, which involves employing different parallel approaches for different DC recursions to achieve a more optimal balance between parallelism and convergence for solvers. To evaluate the effectiveness of DCSolver, we apply it to two preconditioned Krylov subspace solvers and an unstructured mesh Computational Fluid Dynamics (CFD) solver. Our results show that when compared with the state-of-the-art methods, DCSolver accelerates the time-to-solution of solvers by an average speedup of up to 26.19X.},
  archive      = {J_TACO},
  author       = {Haozhong Qiu and Chuanfu Xu and Jianbin Fang and Jian Zhang and Liang Deng and Zhe Dai and Yue Ding and Yue Wang and Zhimeng Han and Yonggang Che and Jie Liu},
  doi          = {10.1145/3746233},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {DCSolver: Accelerating sparse iterative solvers via divide-and-conquer on GPUs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cppless: Single-source and high-performance serverless programming in c++. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3747841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of serverless computing introduced a new class of scalable, elastic, and widely available parallel workers in the cloud. Many systems and applications benefit from offloading computations and parallel tasks to dynamically allocated resources. However, the developers of C++ applications find it difficult to integrate functions due to complex deployment, lack of compatibility between client and cloud environments, and loosely typed input and output data. To enable single-source and efficient serverless acceleration in C++, we introduce Cppless , an end-to-end framework for implementing remote functions which handles the creation, deployment, and invocation of serverless functions. Cppless is built on top of LLVM and requires only two compiler extensions to automatically extract C++ function objects and deploy them to the cloud. We demonstrate that offloading parallel computations, such as from a C++ application to serverless workers, can provide up to 59x speedup with minimal cost increase while requiring only minor code modifications.},
  archive      = {J_TACO},
  author       = {Marcin Copik and Lukas Möller and Alexandru Calotoiu and Torsten Hoefler},
  doi          = {10.1145/3747841},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Cppless: Single-source and high-performance serverless programming in c++},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile-3DCNN: An acceleration framework for ultra-real-time execution of large 3D CNNs on mobile devices. <em>TACO</em>, <em>22</em>(3), 1-22. (<a href='https://doi.org/10.1145/3747842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to deploy 3D Convolutional Neural Networks (3D CNNs) on mobile devices, specifically if both real-time execution and high inference accuracy are in demand, because the increasingly large model size and complex model structure of 3D CNNs usually require tremendous computation and memory resources. Weight pruning is proposed to mitigate this challenge. However, existing pruning is either not compatible with modern parallel architectures, resulting in long inference latency or subject to significant accuracy degradation. This article proposes an end-to-end 3D CNN acceleration framework based on pruning/compilation co-design called Mobile-3DCNN that consists of two parts: a novel, fine-grained structured pruning enhanced by a prune/Winograd adaptive selection (that is mobile-hardware-friendly and can achieve high pruning accuracy), and a set of compiler optimization and code generation techniques enabled by our pruning (to fully transform the pruning benefit to real performance gains). The evaluation demonstrates that Mobile-3DCNN outperforms state-of-the-art end-to-end DNN acceleration frameworks that support 3D CNN execution on mobile devices, Alibaba Mobile Neural Networks and Pytorch-Mobile with speedup up to 34× with minor accuracy degradation, proving it is possible to execute high-accuracy large 3D CNNs on mobile devices in real-time (or even ultra-real-time).},
  archive      = {J_TACO},
  author       = {Wei Niu and Mengshu Sun and Zhengang Li and Jou-An Chen and Jiexiong Guan and Xipeng Shen and Jun Liu and Mei Zhang and Yanzhi Wang and Xue Lin and Bin Ren},
  doi          = {10.1145/3747842},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Mobile-3DCNN: An acceleration framework for ultra-real-time execution of large 3D CNNs on mobile devices},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TRACED: A temporal graph neural networks-based model for data prefetching. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3747843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern microarchitectures, machine-learning-based prefetchers use past memory requests to learn access patterns and predict memory addresses, thereby prefetching data into the cache to mitigate the processor-memory speed gap. However, they face two key challenges in capturing irregular access patterns generated by complex data structures and algorithms. One is data dispersion: the disorderliness of memory addresses makes it difficult for prefetchers to extract meaningful data features. The other is temporal and spatial complexity: existing prefetchers fail to effectively learn temporal and spatial characteristics, and thus are unable to explore more complex access patterns. To resolve these challenges, we propose TRACED, a novel temporal graph neural network-based prefetcher aimed at learning access patterns of memory addresses. TRACED consists of two key components: a dynamic clustering component and a temporal graph neural network component. In the dynamic clustering component, we introduce a similarity function to quantify the similarity of memory addresses. Based on the quantified similarity, we dynamically group unordered memory addresses into different clusters. This ensures that the memory addresses in each cluster are ordered and change smoothly, thus resolving the first challenge. The temporal graph neural network component constructs a spatiotemporal graph to represent relationships among memory addresses. This helps capture temporal and spatial characteristics both across and within clusters, thus resolving the second challenge. This article demonstrates the effectiveness of the proposed prefetcher through experiments. Specifically, in terms of accuracy, TRACED outperforms BO, SPP, DOMINO, Delta-LSTM, and VOYAGER by 2.29%–40.83% on average. Furthermore, TRACED attains remarkable coverage of 55.67% and IPC of 43.75%, outperforming all competing approaches in both metrics.},
  archive      = {J_TACO},
  author       = {He Jiang and Liuwei Fu and Dong Liu and Zhilei Ren and Yuting Chen and Lei Qiao},
  doi          = {10.1145/3747843},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {TRACED: A temporal graph neural networks-based model for data prefetching},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GenCNN: A partition-aware multi-objective mapping framework for CNN accelerators based on genetic algorithm. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3747844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) require partitioning to efficiently run on CNN accelerators, which offer multiple parallel processing dimensions, such as Processing Element (PE) array topologies and Single Instruction Multiple Data (SIMD) execution. The choice of parallelization strategy directly impacts accelerator performance. However, the vast search space for CNN partitioning and parallelization makes manual optimization costly and complex, especially when addressing both aspects simultaneously. This highlights the need for an automated framework to efficiently map CNNs onto accelerators. Our key insight is that existing approaches suffer from inadequate accelerator performance modeling and a lack of multi-objective optimization strategies that jointly consider task partitioning and convolution parallelization. To address this, we propose GenCNN, a multi-objective genetic algorithm-based mapping framework for CNN accelerators. GenCNN first constructs a fine-grained performance model that captures both off-chip data access and on-chip data processing. It then applies the Non-dominated Sorting Genetic Algorithm II improved by Multi-Objective Bayesian Optimization to derive a Pareto-optimal partitioning and parallelization strategy that balances off-chip latency and PE utilization. Finally, GenCNN optimizes scheduling and routing to minimize data transfers. Experimental results show that GenCNN achieves up to 17.66× speedup in compilation and 6.47× in execution compared with state-of-the-art mapping frameworks.},
  archive      = {J_TACO},
  author       = {Yudong Mu and Zhihua Fan and Wenming Li and Zhiyuan Zhang and Xuejun An and Dongrui Fan and Xiaochun Ye},
  doi          = {10.1145/3747844},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GenCNN: A partition-aware multi-objective mapping framework for CNN accelerators based on genetic algorithm},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supports of data cache division for computational solid-state drives. <em>TACO</em>, <em>22</em>(3), 1-20. (<a href='https://doi.org/10.1145/3747845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational SSD ( CompSSD ), with high computing capabilities, can function not only as a storage device but also as a computing node. The data cache of the CompSSD device stores both the output data from host-side tasks and the input data for tasks executed on the CompSSD . However, current cache management strategies are optimized for traditional SSDs and are incompatible with the unique requirements of CompSSD . To address the issue of cache management for CompSSD , this article proposes a novel cache division scheme, to dynamically divide the cache into two parts, for separately buffering output data from host-side tasks and input data used by CompSSD -side tasks. To this end, we construct a mathematical model that periodically estimate an optimal cache division ratio, by considering the factors of the ratios of read/write data amount, the cache hits, and the overhead of data transfer between the storage device and the host. Besides, we propose a scheme of proactive data flushing to write the output data to the underlying flash arrays, without impacts on I/O responsiveness. The trace-driven experiments show that our scheme can improve the overall I/O latency by 35.4% on average, in contrast to existing cache management schemes for CompSSD devices.},
  archive      = {J_TACO},
  author       = {Zhibing Sha and Shuaiwen Yu and Chengyong Tang and Zhigang Cai and Peng Tang and Ming Huang and Jun Li and Jianwei Liao},
  doi          = {10.1145/3747845},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Supports of data cache division for computational solid-state drives},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ephemera: Accelerating I/O-intensive serverless workloads with a harvested in-memory file system. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3747846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing has gained popularity for its ability to shift the burden of server management from developers to cloud providers, which allows providers to exercise greater control over resource management, optimizing configurations to enhance efficiency and performance. The diversity of serverless computing tasks, from short-lived, event-driven tasks to more complex workloads, highlights the growing importance of efficient file I/O performance for I/O-intensive workloads, yet effectively handling ephemeral storage for I/O-intensive tasks remains a challenge. Traditional file system approaches often introduce substantial latency and fail to fully leverage available memory resources within the execution environment, limiting performance and efficiency. Our work stems from the observation of the under-utilization of memory resources in serverless computing platforms and the potential efficiency improvement of I/O operations using an in-memory file system. Based on this observation, we propose Ephemera , a system designed to enhance ephemeral storage efficiency and memory utilization. Ephemera satisfies three design goals: transparent memory I/O integration , heterogeneous tasks resource synergy , and harmonized cluster workload orchestration . Ephemera integrates three components: the Runtime Daemon, responsible for managing a container’s in-memory file system; the Tenant Manager, facilitating memory configuration sharing across containers; and the Cluster Controller, optimizing workload balancing. Our experiments demonstrate that Ephemera significantly improves performance for I/O-intensive tasks compared to traditional file systems. Specifically, Ephemera decreases I/O processing time by 50% on average and reduces latency by up to 95.73% in certain scenarios with negligible overhead.},
  archive      = {J_TACO},
  author       = {Lingxiao Jin and Zinuo Cai and Haoxin Wang and Zongpu Zhang and Ruhui Ma and Haibing Guan and Yuan Liu and Buyya Rajkumar},
  doi          = {10.1145/3747846},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Ephemera: Accelerating I/O-intensive serverless workloads with a harvested in-memory file system},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpMARD: A sparse-sparse matrix multiplication accelerator with reconfigurable dataflow for DNN workloads. <em>TACO</em>, <em>22</em>(3), 1-23. (<a href='https://doi.org/10.1145/3747847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning becomes increasingly popular, and its main workload is Sparse-Sparse Matrix Multiplication (SpMSpM). Most SpMSpM accelerators usually only support a single dataflow. Different dataflows have different performance in different computing environments. Therefore, the single-dataflow accelerator cannot maintain the highest performance in all environments. Compared with single-dataflow accelerators, multi-dataflow accelerators provide flexible options for different workloads and improve the overall performance. Flexagon, Sparm, and SPADA are state-of-the-art multi-dataflow accelerators. However, the computation process of Flexagon and Sparm is not fully pipelined, and SPADA cannot support inner product dataflow. Additionally, Flexagon, Sparm, and SPADA cannot switch dataflows quickly and accurately. Inspired by these observations, we present SpMARD, a SpMSpM accelerator with reconfigurable dataflow. The computation process of SpMARD is fully pipelined, and SpMARD can support six dataflow variants simultaneously. Through the design of a Two-stage Pipeline Adder Network (TPAN) and a Position-based Psum Array (PPA), SpMARD can execute element-level merging, which can hide the merging overhead. Through the quantitative analysis of dataflows, we implement a Dataflow Switcher (DSwitcher), which can switch dataflows more efficiently. For the SpMSpM workload, the performance (GOPS) of the SpMARD we proposed is 1.27 times that of Flexagon, 1.18 times that of Sparm, and 1.22 times that of SPADA.},
  archive      = {J_TACO},
  author       = {Bo Wang and Sheng Ma and Yunping Zhao and Shengbai Luo and Lizhou Wu and Jianmin Zhang and Dongsheng Li and Tiejun Li and Zhuojun Chen},
  doi          = {10.1145/3747847},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SpMARD: A sparse-sparse matrix multiplication accelerator with reconfigurable dataflow for DNN workloads},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAKV: A hotness-aware zone management approach to optimizing performance of LSM-tree-based key-value stores. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3747848'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log-Structured Merge tree-based key-value (KV) stores, like LevelDB and RocksDB, are extensively applied in large-scale data storage systems. This design excels in write-intensive environments by converting random writes into sequential append operations. Despite its advantages, KV stores struggle with real-world workloads where most updates in KV pairs are infrequent. The compaction process and hierarchical data organization result in high write and read amplification. To mitigate these issues, we propose HAKV – a hotness-aware zone management approach to optimizing performance of KV stores. HAKV first separates hot KV pairs from cold KV pairs, storing hot KV pairs in dedicated zones within persistent memory (PM), enabling centralized and lightweight compaction. Second, we propose a storage zone structure in PM to achieve space optimization for cold KV pairs. Third, to bolster cache hit ratio in PM, we provide a hierarchical data framework for hot KV pairs – and a recycling strategy for invalid hot KV pairs in a zone to enhance the space utilization of PM for hot KV pairs. Finally, we design a dynamic window-based adaptive adjustment mechanism for zone pool in PM to optimize the space utilization. Thus, HAKV significantly reduces write amplification while boosting overall read and write performance. The experimental results demonstrate that HAKV achieves write amplification reduction by up to 92.3%, 79.2%, 90.2%, 41.1%, 80.6%, and 62.4% compared with LevelDB, RocksDB, NoveLSM, LightKV, Wisckey, and UniKV, respectively, with average reduction rates of 89.6%, 74.4%, 84.9% 32.3%, 63.7%, and 42.5%. Furthermore, HAKV boosts random write performance by up to 54.2×, 51.5×, 44.2×, 4.3×, 3.1×, and 4.3×, respectively—and the average improvement reaches 25.8×, 20.9×, 23.9×, 2.7×, 2.5×, and 3.4×.},
  archive      = {J_TACO},
  author       = {Hui Sun and Qianli Yue and Guanzhong Chen and Yi Zou and Yinliang Yue and Xiao Qin},
  doi          = {10.1145/3747848},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {HAKV: A hotness-aware zone management approach to optimizing performance of LSM-tree-based key-value stores},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matrix: Multi-cipher structures dataflow for parallel and pipelined TFHE accelerator. <em>TACO</em>, <em>22</em>(3), 1-23. (<a href='https://doi.org/10.1145/3750446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully homomorphic encryption over torus (TFHE) enables the execution of arbitrary functions on encrypted data through programmable bootstrapping (PBS). However, performing all operations on ciphertext during PBS results in high computational and memory requirements, limiting the deployment of PBS in real-world scenarios. Previous TFHE accelerator designs have attempted to improve performance by employing specific dataflow and functional units, but these techniques may require large off-chip bandwidth or on-chip storage when scaling up computation capacity. Additionally, the design of specialized functional units may limit the utilization of computation units when facing dynamic secure parameter settings. To address these challenges and further improve PBS throughput in TFHE, we propose Matrix , an ASIC-based architecture that balances off-chip bandwidth and on-chip storage according to the execution flow of PBS. In Matrix , we utilize a unified special-prime-based processing element (PE) that achieves high utilization with minimal resource overhead. Furthermore, we propose a hybrid PBS dataflow that can efficiently reduce computation complexity and memory requirements. Compared to state-of-the-art TFHE accelerators, Matrix achieves 1.43 × -5.66 × throughput improvement for PBS. For ZAMA Deep-NN benchmark, we achieve 525.60× and 68.06× speedup compared to CPU and GPU, respectively. 1},
  archive      = {J_TACO},
  author       = {Ling Liang and Zhen Gu and Fahong Zhang and Zhaohui Chen and Zhirui Li and Xin Fan and Dimin Niu and Meng Li and Zhiyong Li and Zongwei Wang and Hongzhong Zheng and Yimao Cai and Yuan Xie},
  doi          = {10.1145/3750446},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Matrix: Multi-cipher structures dataflow for parallel and pipelined TFHE accelerator},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SampDedup: Sampling prediction for efficient inline data deduplication on non-volatile memory. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3750447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication is an effective technique for reducing redundant data storage space in various storage systems. Generally, deduplication consists of four steps: chunking, fingerprinting, fingerprint lookup, and data management. Recently, Non-volatile Memory (NVM) as an emerging storage device has received widespread attention. Directly applying the deduplication technique on NVM for storage cost savings faces many challenges: (a) deduplication on NVM devices suffers from computation bottleneck instead of the I/O bottleneck faced by deduplication on traditional storage devices (such as HDD and SSD); (b) new fingerprint indexes and metadata are required to be re-designed to adapt to NVM characteristics; (c) inline deduplication on NVM is more sensitive to the latency. To solve these challenges, we propose a novel Samp ling prediction-based inline data Dedup lication method ( SampDedup ) on NVM devices. It aims to ensure high deduplication ratios while reducing computation costs and latency by optimizing data chunking , fingerprinting , and fingerprint lookup . (a) For data chunking , a sampling prediction-based chunking method ( SampChunk ) is proposed to leverage chunk similarity to distinguish duplicate chunks and skip them for chunking. This method can be easily integrated into most sliding-window based and non-window based CDC chunking algorithms. (b) For fingerprinting , the commonly used SHA-1 algorithm is further optimized to reduce the extra computational overhead introduced by SampChunk, and an asynchronous fingerprinting method is proposed to reduce the fingerprinting latency of unique chunks. (c) For fingerprint lookup , we design a header fingerprint index and metadata table for each data chunk constructed by SampChunk on NVM, and we use a fast-read buffer to replace the traditional slow LRU cache to improve search efficiency. Experiments on four real-world datasets demonstrate that SampDedup consistently presents high inline data deduplication ratios on NVM with different workloads and data partitioning algorithms while saving more than 90% chunking time compared with state-of-the-art deduplication baselines.},
  archive      = {J_TACO},
  author       = {Ziyue Xu and Yichen Li and Ranzhe Deng and Liping Yi and Yusen Li and Gang Wang and Xiaoguang Liu},
  doi          = {10.1145/3750447},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SampDedup: Sampling prediction for efficient inline data deduplication on non-volatile memory},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RACER: Avoiding end-to-end slowdowns in accelerated chip multi-processors. <em>TACO</em>, <em>22</em>(3), 1-22. (<a href='https://doi.org/10.1145/3750448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent chip multiprocessors incorporate several on-chip accelerators, marking the beginning of the Accelerated Chip Multi-Processor (XMP) era in datacenters. Despite the close proximity of accelerators and general-purpose cores, offloading functions to accelerators may not always be beneficial. Offloading to hardware accelerators can introduce several end-to-end overheads that can negate the speedup of the accelerable function. In this article, we design RACER, a hardware architecture and runtime system that evades the danger of end-to-end slowdowns when using hardware acceleration. RACER leverages a low-overhead interface between general-purpose cores and on-chip accelerators, fine-grained context switching, accelerator-initiated preemption, and seamless data motion between general-purpose cores and accelerators to improve the performance of workloads that use on-chip accelerators. We evaluate RACER on five representative request processing workloads featuring diverse memory access patterns, accelerable functions, and compute intensities. RACER improves the performance of hardware acceleration on a real XMP by an average of 1.31× on a range of diverse workloads and guarantees that accelerator offloads never cause slowdowns.},
  archive      = {J_TACO},
  author       = {Neel Patel and Ren Wang and Mohammad Alian},
  doi          = {10.1145/3750448},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {RACER: Avoiding end-to-end slowdowns in accelerated chip multi-processors},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sparsity-aware autonomous path planning accelerator with HW/SW co-design and multi-level dataflow optimization. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3750449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning is a critical task for autonomous driving, aiming to generate smooth, collision-free, and feasible paths based on input perception and localization information. The planning task is both highly time-sensitive and computationally intensive, posing significant challenges to resource-constrained autonomous driving hardware. In this article, we propose an end-to-end framework for accelerating path planning on FPGA platforms. This framework focuses on accelerating quadratic programming (QP) solving, which is the core of optimization-based path planning and has the most computationally-intensive workloads. Our method leverages a hardware-friendly alternating direction method of multipliers (ADMM) to solve QP problems while employing a highly parallelizable preconditioned conjugate gradient (PCG) method for solving the associated linear systems. We analyze the sparse patterns of matrix operations in QP and design customized storage schemes along with efficient sparse matrix multiplication and sparse matrix-vector multiplication units. Our customized design significantly reduces resource consumption for data storage and computation while dramatically speeding up matrix operations. Additionally, we propose a multi-level dataflow optimization strategy. Within individual operators, we achieve acceleration through parallelization and pipelining. For different operators in an algorithm, we analyze inter-operator data dependencies to enable fine-grained pipelining. At the system level, we map different steps of the planning process to the CPU and FPGA and pipeline these steps to enhance end-to-end throughput. We implement and validate our design on the AMD ZCU102 platform. Our implementation achieves state-of-the-art performance in both latency and energy efficiency compared with existing works, including an average 1.48× speedup over the best FPGA-based design, a 2.89× speedup compared with the state-of-the-art QP solver on an Intel i7-11800H CPU, a 5.62× speedup over an ARM Cortex-A57 embedded CPU, and a 1.56× speedup over state-of-the-art GPU-based work. Furthermore, our design delivers a 2.05× improvement in throughput compared with the state-of-the-art FPGA-based design.},
  archive      = {J_TACO},
  author       = {Yifan Zhang and Xiaoyu Niu and Hongzheng Tian and Yanjun Zhang and Bo Yu and Shaoshan Liu and Sitao Huang},
  doi          = {10.1145/3750449},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A sparsity-aware autonomous path planning accelerator with HW/SW co-design and multi-level dataflow optimization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TianheGraph: Topology-aware graph processing. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3750450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world graph data can have billions to trillions of edges. Processing graphs at such scales requires the efficient use of parallel computing systems. However, current graph processing engines and methods struggle to scale beyond a few dozen computing nodes because they (i) cannot efficiently store and process graph data on this scale due to the huge memory footprint incurred and (ii) do not account for the variations in communication costs across different levels of the interconnection hierarchy. We introduce TianheGraph, a software approach to reduce the memory footprint of graphs and optimize graph processing on large-scale parallel systems with complex hardware interconnection components. TianheGraph integrates a new space-time-efficient graph compression technique to reduce the memory footprint of large-scale graphs. It provides a novel graph partitioning method to improve load balancing and minimize communication overhead across various levels of the interconnection hierarchy. We evaluate TianheGraph by applying it to fundamental graph operations on synthetic and real-world graphs, using up to 79,024 computing nodes and over 1.2 million processor cores. Our extensive experiments show that TianheGraph outperforms state-of-the-art parallel graph processing engines in throughput and scalability. Moreover, TianheGraph outperformed the top-ranked systems on the Graph 500 list at the time of submission.},
  archive      = {J_TACO},
  author       = {Xinbiao Gan},
  doi          = {10.1145/3750450},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {TianheGraph: Topology-aware graph processing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A low-latency on-chip cache hierarchy for load-to-use stall reduction in GPUs. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3760782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory hierarchy in Graphics Processing Units (GPUs) is conventionally designed to provide high bandwidth rather than low latency. In particular, because of the high tolerance to load-to-use latency (i.e., the time that warps wait for data fetched by memory loads), GPU L1D caches are optimized for density, capacity, and low power with latencies that are often orders of magnitude longer than conventional CPU caches. However, there are many important classes of data-parallel applications (e.g., graph, tree, priority queue processing, and sparse deep learning applications) that benefit from lower load-to-use latency than that offered by modern GPUs due to their inherent divergence and low effective Thread-Level Parallelism (TLP). This article introduces an innovative on-chip cache hierarchy that incorporates a decoupled L1D cache with reduced latency (LoTUS) and its management scheme. LoTUS is a minimally sized fully associative cache placed in each GPU subcore that captures the primary working set of data-parallel applications. It exploits conventional high-performance low-density SRAM cells and dramatically reduces load-to-use latency. We also propose an intelligent extension of LoTUS, called LoTUSage, which employs a lightweight learning-based model to predict the utility of caching requests in LoTUS. Evaluation results show that LoTUS and LoTUSage improve the average performance by 23.9% and 35.4% and reduce the average energy consumption by 27.8% and 38.5%, respectively, for the applications suffering from high load-to-use stalls with negligible area and power overheads.},
  archive      = {J_TACO},
  author       = {Negin (Sadat) (Nematollahi zadeh) Mahani and Hajar Falahati and Sina Darabi and Ahmad Javadi-Nezhad and Yunho Oh and Mohammad Sadrosadati and Hamid Sarbazi-Azad and Babak Falsafi},
  doi          = {10.1145/3760782},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A low-latency on-chip cache hierarchy for load-to-use stall reduction in GPUs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ecmas+: Efficient circuit mapping and scheduling for surface code encoded circuit on quantum cloud platform. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3760783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the leading candidate for quantum error correction, the surface code faces substantial overhead, such as redundant physical qubits and prolonged execution time. Reducing the space-time cost of circuit execution can significantly improve the throughput of modern quantum cloud platforms. While utilizing more physical qubits can reduce execution time, different quantum circuits vary in their ability to leverage chip resources. Therefore, optimizing the compilation of surface code circuits on quantum chips becomes critical. In this work, we address the mapping and scheduling problem in compiling surface code to reduce the cost. First, we introduce a novel metric Circuit Parallelism Degree to characterize circuit properties in detail and select the most suitable chip from a list of available options. Next, we will quantitatively assess the resources to determine if they are sufficient for the circuit. We then propose a resource-adaptive mapping and scheduling method called Ecmas+ , which customizes the initialization of chip resources for each circuit. Ecmas+ significantly reduces execution time in the double defect and lattice surgery models. Extensive numerical tests on practical datasets demonstrate that Ecmas+ outperforms state-of-the-art methods, reducing execution time by an average of 46% for the double defect model and 29.7% for the lattice surgery model.},
  archive      = {J_TACO},
  author       = {Mingzheng Zhu and Hao Fu and Haishan Song and Jun Wu and Chi Zhang and Wei Xie and Xiangyang Li},
  doi          = {10.1145/3760783},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Ecmas+: Efficient circuit mapping and scheduling for surface code encoded circuit on quantum cloud platform},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augur: Semantics-aware temporal prefetching for linked data structure. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3762997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linked data structures (LDS), such as lists and trees, are widely used in modern applications. Traversing LDS typically involves a significant amount of pointer chasing. Due to the serial nature of memory access in pointer chasing, the incurred long memory latency of traversing LDS has become a critical performance bottleneck. Furthermore, the poor spatial locality in LDS makes it difficult for spatial prefetchers to predict access addresses. Although temporal prefetchers can handle irregular memory access patterns, hindered by the challenges of collecting semantic information, current state-of-the-art temporal prefetchers suffer from significant metadata redundancy and frequent metadata conflicts. Consequently, there remain substantial opportunities to enhance the LDS prefetching. To solve this problem, we propose Augur, a semantics-aware temporal prefetcher to enhance LDS performance. Augur utilizes a novel pruning method to obtain semantic information and effectively extracts node address correlations from the perspective of nodes in LDS, thereby diminishing the metadata redundancy and conflicts. Additionally, Augur employs efficient metadata management strategies that guarantee a minimal storage overhead. Evaluated on LDS workloads, Augur achieves an average performance speedup of 17.8% and 11.7% over the baseline stride prefetcher and state-of-the-art spatial prefetcher Berti, respectively. Furthermore, Augur outperforms the state-of-the-art temporal prefetcher MISB, Triage, and Triangel, by 17.4%, 12.8%, and 6.3%, respectively, with a significantly lower storage overhead of only 1.26 KB.},
  archive      = {J_TACO},
  author       = {Feng Xue and Junliang Wu and Chenji Han and Xinyu Li and Tingting Zhang and Tianyi Liu and Fuxin Zhang},
  doi          = {10.1145/3762997},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Augur: Semantics-aware temporal prefetching for linked data structure},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LitTLS: Lightweight thread-level speculation on little cores. <em>TACO</em>, <em>22</em>(2), 1-27. (<a href='https://doi.org/10.1145/3719655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thread-Level Speculation (TLS) utilizes speculative parallelization to accelerate hard-to-parallelize serial codes on multi-cores. As the heterogeneous multi-core architecture is becoming ubiquitous, it presents an opportunity for TLS to reorganize little cores for the acceleration of these serial codes instead of a big core with similar or more area and power. However, previous TLS designs significantly suffer from extended hardware overhead and costly speculative forwarding. We present LitTLS, a lightweight TLS design with versioning caches to eliminate significant extended hardware overhead by storing versions in caches without speculative write buffers and memory undo-logs. Additionally, LitTLS introduces the Speculative Address Table, a novel component to accelerate speculative forwarding with a central structure to trace memory dependencies. Evaluations on four little cores show that LitTLS achieves an average performance speedup of 2.87× compared to a little core, outperforming a big core by 94% with similar area and less power. The extended area size is only 0.07 mm 2 , and the maximum increase in dynamic power consumption is limited to 0.3%, compared to four little cores.},
  archive      = {J_TACO},
  author       = {Xin Cheng and Jinpeng Ye and Haoyu Deng and Tingting Zhang and Tianyi Liu and Jian Wang},
  doi          = {10.1145/3719655},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {LitTLS: Lightweight thread-level speculation on little cores},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSN cache: Exploiting data localities in graph computing applications. <em>TACO</em>, <em>22</em>(2), 1-26. (<a href='https://doi.org/10.1145/3721286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article finds that the reusability of vertices in the same graph in graph processing differs, and the high-reuse and low-reuse vertices are stored together. These phenomena lead to the inability of existing GPU architectures to capture the reusability of graph processing. The most advanced cache optimization strategies cannot implement different management strategies for data with different reusability, which is an essential reason for graph processing’s poor performance. Therefore, we propose a TSN cache scheme for the GPU platform. This scheme employs distinct management strategies for data with varying reusability in the cache, effectively leveraging the locality of these different data types. In addition, the TSN cache scheme can also reduce the probability of cache thrashing caused by low-reuse data. This article evaluates multiple graph algorithms and datasets and shows that the TSN cache scheme achieves an average speedup of 1.38 compared with the baseline scheme.},
  archive      = {J_TACO},
  author       = {Chaoyang Jia and Jingyu Liu and Shi Chen and Kai Lu and Li Shen},
  doi          = {10.1145/3721286},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {TSN cache: Exploiting data localities in graph computing applications},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PANDA: Adaptive prefetching and decentralized scheduling for dataflow architectures. <em>TACO</em>, <em>22</em>(2), 1-27. (<a href='https://doi.org/10.1145/3721288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataflow architectures are considered promising architecture, offering a commendable balance of performance, efficiency, and flexibility. Abundant prior works have been proposed to improve the performance of dataflow architectures. Nevertheless, these solutions can be further improved due to the lack of efficient data prefetching and flexible task scheduling. In this article, we propose a novel dataflow architecture with adaptive p refetching an d d ecentr a lized scheduling (PANDA). First, we present an application-adaptive data prefetching method and on-chip memory microarchitecture designed to overlap memory access latency. Second, we introduce a decentralized dataflow scheduling approach and processing element (PE) microarchitecture aimed at improving hardware utilization. Experimental results show that in a wide range of real-world applications, PANDA attains up to 2.53× performance improvement and 1.79× energy efficiency improvement over the state-of-the-art dataflow architectures.},
  archive      = {J_TACO},
  author       = {Shantian Qin and Zhihua Fan and Wenming Li and Zhen Wang and Xuejun An and Xiaochun Ye and Dongrui Fan},
  doi          = {10.1145/3721288},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {PANDA: Adaptive prefetching and decentralized scheduling for dataflow architectures},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BridgeGC: An efficient cross-level garbage collector for big data frameworks. <em>TACO</em>, <em>22</em>(2), 1-27. (<a href='https://doi.org/10.1145/3722110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Popular big data frameworks commonly run atop Java Virtual Machine (JVM) and rely on garbage collection (GC) mechanism to automatically allocate/reclaim in-memory objects. Existing garbage collectors are designed based on the hypothesis that most objects are short lived. However, big data frameworks usually generate many long-lived data objects, which can cause heavy GC overhead. Recent approaches have reduced GC overhead in big data frameworks but still suffer from heavy human efforts, additional runtime overhead, or suboptimal GC efficiency. This article describes the design of BridgeGC , a big-data-friendly garbage collector that significantly reduces GC overhead introduced by long-lived data objects. BridgeGC follows a cross-level co-design. At the big data framework level, BridgeGC provides two annotations for framework developers to denote the creation and release of data objects. Based on the annotations, BridgeGC tracks the lifecycles of annotated data objects and optimizes their allocation/reclamation at the GC level. At the GC level, we design a label-based allocator that stores data objects separately from other objects and balances their memory usage in the same JVM, leading to fewer GC cycles. We further design an efficient collector to eliminate unnecessary marking and copying of data objects during GC cycles, lowering the GC time. We have integrated BridgeGC into OpenJDK ZGC. The extensive evaluation, using two popular big data frameworks (Flink and Spark) and a key–value database (Cassandra), shows that BridgeGC achieves 31–82% GC time reduction compared to the baseline ZGC. BridgeGC also outperforms other traditional and academic garbage collectors in end-to-end performance.},
  archive      = {J_TACO},
  author       = {Yicheng Wang and Lijie Xu and Tian Guo and Wensheng Dou and Hongbin Zeng and Wei Wang and Jun Wei and Tao Huang},
  doi          = {10.1145/3722110},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {BridgeGC: An efficient cross-level garbage collector for big data frameworks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEED: Speculative security metadata updates for low-latency secure memory. <em>TACO</em>, <em>22</em>(2), 1-27. (<a href='https://doi.org/10.1145/3722111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Securing systems’ main memory is important for building trusted data centers. To ensure memory security, encryption and integrity verification techniques update the security metadata (e.g., encryption counters and integrity trees) during memory data writes. Existing studies are optimistic about the effect of data writes on system performance since they regard all data writes as background operations. However, we show that security metadata updates significantly increase data write latency. High-latency data writes frequently fill up write buffers in the system, forcing the system to perform the writes in the critical path. As a result, performance-critical data reads need to wait for the execution of these writes, which increases data read latency and degrades system performance. In this article, we propose SEED that improves the performance of secure memory systems by speculatively updating security metadata in the background before data writes arrive. To enable speculative updates, SEED predicts which dirty cache lines will be written to memory through natural evictions. We find that cache evictions depend on multiple factors. To decouple the dependencies for accurate predictions, we devise a two-step eviction prediction method based on our observation that the next eviction victim rarely changes in a set. The first step predicts which cache sets will evict cache lines, whereas the second step predicts which cache lines will be evicted by finding the next eviction victims in the sets. For predicted evictions, we develop a speculative updater to perform speculative updates. We analyze the invariants that must be followed by the updater to ensure the correctness of speculative updates. The updater rolls back the speculatively updated security metadata of inaccurate predictions. To reduce the rollback overhead, we devise a rollback batching and an update pausing optimization for the updater. Experimental results show that SEED reduces data write latency by 39.8%, data read latency by 44.9%, and improves performance by 40.0% on average compared with the state-of-the-art secure memory design.},
  archive      = {J_TACO},
  author       = {Xueliang Wei and Dan Feng and Wei Tong and Bing Wu and Xu Jiang},
  doi          = {10.1145/3722111},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SEED: Speculative security metadata updates for low-latency secure memory},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lock-free RDMA-friendly index in CPU-parsimonious environments. <em>TACO</em>, <em>22</em>(2), 1-25. (<a href='https://doi.org/10.1145/3722112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In CPU-parsimonious environments, such as disaggregated memory systems, the limited CPU power on the memory side constrains the ability to perform more operations. Thus, reducing CPU usage and enhancing concurrency performance are critical for indexing key-value storage in these scenarios. Current hash indexes support one-sided RDMA access and lock-free concurrency control with good performance but lack range query support. In contrast, existing tree indexes support range query but only implement expensive locks for concurrency control. Therefore, designing a tree index that supports lock-free concurrent access and one-sided RDMA is a significant challenge. To address these issues, this paper proposes a lock-free tree index based on the van Emde Boas (vEB) tree, called vBoost. vBoost inherits the vEB tree’s characteristics of index nodes without splitting or merging, and simplifies concurrency control by managing changes at a single node. It redesigns tree nodes with an 8-byte compact data structure, allowing each key-value pair to support RDMA_CAS atomic operations for lock-free concurrency. Additionally, vBoost leverages the RDMA Doorbell technique to reduce RTTs, enhancing range query and write performance. Evaluation results show that, vBoost achieves up to 3.85× higher throughput and better scalability under YCSB workloads compared to state-of-the-art tree indexes.},
  archive      = {J_TACO},
  author       = {Yuting Li and Yun Xu and Pengcheng Wang and Yonghui Xu and Weiguang Wang},
  doi          = {10.1145/3722112},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A lock-free RDMA-friendly index in CPU-parsimonious environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Koala: Efficient pipeline training through automated schedule searching on domain-specific language. <em>TACO</em>, <em>22</em>(2), 1-25. (<a href='https://doi.org/10.1145/3722113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pipeline parallelism is a crucial technique for large-scale model training, enabling parameter splitting and performance enhancement. However, creating effective pipeline schedules often requires significant manual effort and coding skills, leading to practical inconveniences and complex debugging. Major frameworks such as DeepSpeed and ColossalAI simplify the process by adopting predefined pipeline schedule strategies, such as GPipe and 1F1B. The use of predefined schedules offers limited flexibility and suboptimal training efficiency, as the limited number of manually set candidates cannot provide the optimal strategy for arbitrary model training. To deal with the issue, this article aims to automatically search for the optimal strategy with high efficiency. Since current frameworks only support a limited set of fixed strategies, lacking the technical capability to create a comprehensive strategy search space, we first design a novel domain-specific language (DSL) for pipeline schedule development. The DSL exhibits great understandability, agility, and reusability, supporting the development of all known pipeline schedule strategies and their variants. Second, we are the first to model the complete pipeline schedule strategy space via the DSL, enabling an automated end-to-end globally optimal pipeline schedule searching, while past work may get stuck in a local optimum. Finally, we propose to optimize pipeline performance by modeling and solving the pipeline schedule as a Binary-Tree-Traversing (BTT) optimization problem. Based on the formalization, we further adopt a Dynamic Try-Test Genetic Algorithm to search for the best pipeline schedule strategy, which overwhelms a variety of pre-defined ones. Experimental results show that Koala achieves an enhanced performance by up to \(1.53\times\) over state-of-the-art approaches. Besides, the pipeline schedule strategy searched by Koala outperforms pre-defined pipeline schedule strategies by \(1.10\times \sim 1.55\times\) . Moreover, Koala has superior scalability and effectiveness in combining with data parallelism and tensor parallelism.},
  archive      = {J_TACO},
  author       = {Yu Tang and Lujia Yin and Qiao Li and Hongyu Zhu and Hengjie Li and Xingcheng Zhang and Linbo Qiao and Dongsheng Li and Jiaxin Li},
  doi          = {10.1145/3722113},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Koala: Efficient pipeline training through automated schedule searching on domain-specific language},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRSparse: Generating codes for high-performance sparse matrix-vector semiring computations. <em>TACO</em>, <em>22</em>(2), 1-26. (<a href='https://doi.org/10.1145/3722114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix-vector semiring computation is a key operation in sparse matrix computations, with performance strongly dependent on both program design and the features of the sparse matrices. Given the diversity of sparse matrices, designing a tailored program for each matrix is challenging. To address this, we propose SRSparse, 1 a program generator that creates tailored programs by automatically combining program designing methods to fit specific input matrices. It provides two components: the problem definition configuration , which declares the computation, and the scheduling language , which can be leveraged by an auto-tuner to specify the program designs. The two are lowered to the intermediate representations of SRSparse, the Format IR and Kernel IR , which respectively generate format conversion routine and kernel code. We evaluate SRSparse on four representative sparse kernels and three format conversion routines. For sparse kernels, SRSparse achieves median speedups over handwritten programs: COO (3.50×), CSR-Adaptive (5.36×), CSR5 (2.06×), ELL (1.63×), Gunrock (1.57×), and GraphBLAST (1.96×); over an auto-tuner: AlphaSparse (1.16×); and over a compiler: TACO (1.71×). For format conversion routines, SRSparse achieves median speedups over handwritten implementations: Intel MKL (7.60×), SPARSKIT (2.61×), CUSP (2.77×), and Ginkgo (1.74×); and over a compiler: TACO (4.04×).},
  archive      = {J_TACO},
  author       = {Zhen Du and Ying Liu and Ninghui Sun and Huimin Cui and Xiaobing Feng and Jiajia Li},
  doi          = {10.1145/3722114},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SRSparse: Generating codes for high-performance sparse matrix-vector semiring computations},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gator: Accelerating graph attention networks by jointly optimizing attention and graph processing. <em>TACO</em>, <em>22</em>(2), 1-24. (<a href='https://doi.org/10.1145/3722219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph attention networks (GATs) have advanced performance in various application domains by introducing the attention mechanism into the graph neural networks (GNNs). The inefficiency of running GATs on CPUs or GPUs necessitates specialized hardware designs. Unfortunately, previous specialized architecture designs have focused on either the GNN architecture or the attention mechanism, resulting in limited performance and leaving ample room for improvement. This article presents Gator , a joint optimization approach with software–hardware co-designs for GAT inference. On the software level, Gator leverages degree-weighted graph partitioning and parameter-adaptive feature selection techniques to preprocess the input graph data, mining subgraph-level parallelism and mitigating the computation bottleneck of the dedicated dataflow. On the hardware level, Gator designs a unified processing engine to support various kernels by extracting a common computation pattern and a dimension-aware microarchitecture for efficient partial sum reduction. Extensive experiments show that our approach can achieve 11.5× more efficiency compared to NVIDIA RTX 4090 and provide a speedup of 3× to 9.4×, along with a 2.6× to 4.7× reduction in memory traffic, when compared to six state-of-the-art methods, with minimal accuracy loss.},
  archive      = {J_TACO},
  author       = {Xiaobo Lu and Jianbin Fang and Lin Peng and Chun Huang and Zixiao Yu and Tiejun Li},
  doi          = {10.1145/3722219},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Gator: Accelerating graph attention networks by jointly optimizing attention and graph processing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SnsBooster: Enhancing sampling-based μArch evaluation efficiency through online performance sensitivity analysis. <em>TACO</em>, <em>22</em>(2), 1-27. (<a href='https://doi.org/10.1145/3727637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sampling-based methods, such as SimPoint, are widely used for efficient pre-silicon μ Arch evaluations, where the costs are the number of simulation points multiplied by the number of evaluated μ Arch designs. However, these costs keep growing with an increasing number of simulation points and expanding μ Arch design space. Although techniques have been developed to accelerate the μ Arch design space exploration, less attention has been given to further reducing the simulation budget of each μ Arch evaluation. Common strategies like reducing simulation coverage or sampling fewer simulation points typically compromise estimation accuracy. Therefore, further reducing the simulation budget without compromising estimation accuracy remains a critical research problem. In this work, we propose SnsBooster to enhance sampling-based μ Arch evaluation efficiency, based on two insights: (a) large portions of simulation points’ performance changes are typically insensitive to the evaluated μ Arch changes, and (b) simulation points’ performance sensitivities under specific μ Arch change correlate with their inherent characteristics. By online building a μ Arch-specific performance sensitivity classifier via progressive simulation and continuous validation, SnsBooster can identify and selectively evaluate only performance-sensitive points, thus reducing the simulation budget without compromising estimation accuracy. When applied across various μ Arch changes, SnsBooster achieves an average simulation budget reduction of 39.04% with an accuracy loss of only 0.14%, compared to simulating all the sampled points. Under the same accuracy loss, SnsBooster’s simulation budgets are only 64.73% and 65.60% of those required by methods of reducing simulation coverage or sampling fewer points. Besides, under identical simulation budgets, the average accuracy losses of these methods are 1.41% and 1.23%, which is substantially higher than that of SnsBooster.},
  archive      = {J_TACO},
  author       = {Chenji Han and Zifei Zhang and Feng Xue and Xinyu Li and Yuxuan Wu and Tingting Zhang and Tianyi Liu and Qi Guo and Fuxin Zhang},
  doi          = {10.1145/3727637},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SnsBooster: Enhancing sampling-based μArch evaluation efficiency through online performance sensitivity analysis},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supporting dynamic program sizes in deep learning-based cost models for code optimization. <em>TACO</em>, <em>22</em>(2), 1-25. (<a href='https://doi.org/10.1145/3727638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic code optimization enables developers to write high-level code relying on compilers to optimize it and generate efficient code for target hardware. State-of-the-art methods for automatic code optimization leverage deep learning to build cost models that predict the impact of code optimizations on execution time. However, these models are typically limited in terms of the size and complexity of the programs they support. This research presents a novel approach to developing deep learning-based cost models that address these limitations. Our approach introduces a new program representation that efficiently represents programs with complex structures and large sizes such as varying loop depths, buffer numbers, and dimensions. Furthermore, we propose a novel deep learning architecture, that can handle this dynamic program representation. This allows the model to work on larger and more complex programs than those it was trained on. We implemented this model in Tiramisu, a state-of-the-art compiler. Our evaluation shows that our proposed model can generalize to programs larger than those seen during training, while the original Tiramisu cost model cannot. We also show that such generality does not lead to a significant increase in our proposed model’s Mean Absolute Percentage Error or a decrease in the quality of code optimizations found when the model is used for automatic code optimization. In contrast, our proposed model on average achieves a 41.89% improvement in speed compared to the original cost model when both models are trained on the same dataset, showing better generalization over unseen programs. This is a significant advantage over previous approaches, which typically do not support program sizes beyond those seen during the training.},
  archive      = {J_TACO},
  author       = {Yacine Hakimi and Riyadh Baghdadi and Yacine Challal},
  doi          = {10.1145/3727638},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Supporting dynamic program sizes in deep learning-based cost models for code optimization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleashing parallelism with elastic-barriers. <em>TACO</em>, <em>22</em>(2), 1-26. (<a href='https://doi.org/10.1145/3727639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of multi-core processors, parallel programming has become essential, and managing synchronization overheads has become crucial for efficiency. Barriers, commonly used to synchronize threads, divide the program into different phases. Existing scheduling schemes address intra-phase load imbalance to some extent but do not fully resolve the issue of thread idling, especially in the context of programs with irregular parallel for-loops. This article proposes an innovative solution called the elastic-barrier , where threads that arrive early at a barrier can execute iterations of the parallel-loop from the next phase, thereby reducing the idle time, and reduce load imbalance. The approach guarantees safety by ensuring that before executing any work W from the subsequent phase, all the works in the current phase that W depends on have been executed. The article presents a compilation scheme that integrates compile-time and runtime techniques to optimize execution. We implemented our proposed scheme in the IMOP framework. Experimental results on graph-based benchmarks show that our approach improves the performance significantly.},
  archive      = {J_TACO},
  author       = {Amit Tiwari and V. Krishna Nandivada},
  doi          = {10.1145/3727639},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Unleashing parallelism with elastic-barriers},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ModNEF: An open source modular neuromorphic emulator for FPGA for low-power in-edge artificial intelligence. <em>TACO</em>, <em>22</em>(2), 1-24. (<a href='https://doi.org/10.1145/3730581'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic computing is a novel computational paradigm that draws inspiration from the structure and function of the human brain. Spiking Neural Networks (SNNs) are a promising approach for implementing energy-efficient Artificial Neural Networks (ANNs) in embedded systems. In this article, we present ModNEF, an open-source, neuromorphic digital hardware architecture designed for Field Programmable Gate Arrays (FPGAs). ModNEF is based on a modular architecture, where independent modules communicate via point-to-point connections to emulate SNNs. Our architecture offers two neuron models based on the Leaky Integrate and Fire (LIF) model, with a different emulation strategy. The modular nature of ModNEF allows researchers to extend the architecture by developing new modules to emulate different types of neurons or implement online learning rules. ModNEF is a clock-driven emulator, meaning that the neuron state is updated at regular intervals, even in the absence of input data. We evaluated the performance of the emulator using the MNIST and NMNIST datasets, with offline, full-precision training.},
  archive      = {J_TACO},
  author       = {Aurélie Saulquin and Mazdak Fatahi and Pierre Boulet and Samy Meftali},
  doi          = {10.1145/3730581},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ModNEF: An open source modular neuromorphic emulator for FPGA for low-power in-edge artificial intelligence},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCMA: Accelerating parallel DMA transfers with a multi-port direct cached memory access in a massive-parallel vector processor. <em>TACO</em>, <em>22</em>(2), 1-25. (<a href='https://doi.org/10.1145/3730582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art applications, such as convolutional neural networks, demand specialized hardware accelerators that address performance and efficiency constraints. An efficient memory hierarchy is mandatory for such hardware systems. While the memory architectures of general-purpose processors (e.g., CPU or GPUs) are based on cache systems, dedicated accelerators have mostly adopted the DMA (Direct Memory Access) concept due to the application field of image processing. DMA features like 2D data transfers or data padding can optimize the memory accesses of image processing. However, DMA lacks the capability to exploit temporal and spatial data reuse, a feature common in cache systems, particularly when multiple DMAs operate in parallel. This article proposes a novel Direct Cached Memory Access (DCMA) architecture, combining both DMA and cache methodologies and their respective advantages. Optimized for image-based AI algorithms, the DCMA architecture facilitates enhanced memory access by integrating multiple, parallel DMA ports with caching capabilities. This design allows for efficient data reuse and parallel memory access. Optimal parameters for the DCMA are determined through a comprehensive design space exploration. The DCMA is evaluated on a state-of-the-art Xilinx UltraScale+ FPGA board coupled with a massive-parallel vertical vector co-processor, called V 2 PRO. The results show the mitigation of the vector processor’s memory bottleneck. By using the proposed DCMA, speedups of up to ×17 for the ResNet-50 CNN can be achieved.},
  archive      = {J_TACO},
  author       = {Gia Bao Thieu and Sven Gesper and Guillermo Payá-Vayá},
  doi          = {10.1145/3730582},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {DCMA: Accelerating parallel DMA transfers with a multi-port direct cached memory access in a massive-parallel vector processor},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GOLDYLOC: Global optimizations & lightweight dynamic logic for concurrency. <em>TACO</em>, <em>22</em>(2), 1-28. (<a href='https://doi.org/10.1145/3730584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern accelerators like GPUs increasingly execute independent operations concurrently to improve the device’s compute utilization. However, effectively harnessing it on GPUs for important primitives such as general matrix multiplications (GEMMs) remains challenging. Although modern GPUs have significant hardware and software GEMM support, their kernel implementations and optimizations typically assume each kernel executes in isolation and can utilize all GPU resources. This approach is highly efficient when kernels execute in isolation, but causes significant resource contention and slowdowns when kernels execute concurrently. Moreover, current approaches often only statically expose and control parallelism within an application, without considering runtime information such as varying input size and concurrent applications—often exacerbating contention. These issues limit performance benefits from concurrently executing independent operations. Accordingly, we propose GOLDYLOC, which considers the global resources across all concurrent operations to identify performant GEMM kernels, which we call globally optimized (GO)-Kernels. GOLDYLOC also introduces a lightweight dynamic logic which considers the dynamic execution environment for available parallelism and input sizes to execute performant combinations of concurrent GEMMs on the GPU. Overall, GOLDYLOC improves the performance of concurrent GEMMs on a real GPU by up to 2× (18% geomean per workload) versus the default concurrency approach and provides up to 2.5× (43% geomean per workload) speedup over sequential execution.},
  archive      = {J_TACO},
  author       = {Suchita Pati and Shaizeen Aga and Nuwan Jayasena and Matthew Sinclair},
  doi          = {10.1145/3730584},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GOLDYLOC: Global optimizations & lightweight dynamic logic for concurrency},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GNNPilot: A holistic framework for high-performance graph neural network computations on GPUs. <em>TACO</em>, <em>22</em>(2), 1-26. (<a href='https://doi.org/10.1145/3730586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have emerged as powerful tools for graph-based machine learning tasks, but their performance is often constrained by inefficient sparse operators and limited hardware utilization during multi-operator workflows. This article presents GNNPilot, a holistic optimization framework that addresses these challenges through three key innovations. First, we introduce two packing strategies for gather operators, including neighbor packing for load balancing in sparser graphs, and bin packing with a new sparse format for enhanced data locality in denser graphs. Second, we propose dynamic parallelization methods and a novel row panel-based kernel fusion technique to optimize complex multi-operator GNN models. Third, we develop a lightweight sampling-based auto-tuning mechanism that adapts the framework’s optimization strategies to varying input characteristics. Built upon tensor expression-based intermediate representations, GNNPilot maintains the flexibility to optimize both popular and customized GNN models. Extensive experiments across diverse GNN models and graph datasets demonstrate that GNNPilot achieves substantial speedups over state-of-the-art implementations in both the performance of single operators and the efficiency of end-to-end inference. These results establish GNNPilot as an efficient and adaptive solution for accelerating GNN computations on modern GPU architectures.},
  archive      = {J_TACO},
  author       = {Zhengding Hu and Jingwei Sun and Guangzhong Sun},
  doi          = {10.1145/3730586},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GNNPilot: A holistic framework for high-performance graph neural network computations on GPUs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 9Ring: A 3D-stacked memory-based accelerator for flexible and efficient deep CNN applications. <em>TACO</em>, <em>22</em>(2), 1-26. (<a href='https://doi.org/10.1145/3732940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive computational and memory requirements of deep convolutional neural networks (DCNNs) have led to the development of neural network (NN) accelerators. However, as DCNN models grow in size, the demands on NN accelerators in terms of performance, memory bandwidth, and power efficiency continue to increase. We, therefore, present 9Ring , a flexible and efficient DCNN accelerator that takes full advantage of 3D-stacked memory, focusing on its hardware architecture, software scheduling, and optimization strategy. In particular, we first show that the mismatch between DCNN accelerators and DCNN models can lead to increased energy consumption and performance bottlenecks. We then present three flexible dataflow scheduling strategies to mitigate this mismatch. Afterward, we introduce an energy efficiency analysis tool that can automatically search for the optimal scheduling scheme with respect to different DCNN models for energy efficiency. Finally, we conduct an empirical study showing that 9Ring can reduce energy consumption by 31.4% and 43.9% on average, and improve performance by 12% and 10% on average, compared with Tetris and the NN accelerators on conventional low-power DRAM memory systems, respectively.},
  archive      = {J_TACO},
  author       = {Wen Cheng and Qianya Cheng and Yi Liu and Lingfang Zeng and Andre Brinkmann and Yang Wang},
  doi          = {10.1145/3732940},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {9Ring: A 3D-stacked memory-based accelerator for flexible and efficient deep CNN applications},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ShuffleInfer: Disaggregate LLM inference for mixed downstream workloads. <em>TACO</em>, <em>22</em>(2), 1-24. (<a href='https://doi.org/10.1145/3732941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based large language model (LLM) inference serving is now the backbone of many cloud services. LLM inference consists of a prefill phase and a decode phase. However, existing LLM deployment practices often overlook the distinct characteristics of these phases, leading to significant interference. To mitigate interference, our insight is to carefully schedule and group inference requests based on their characteristics. We realize this idea in ShuffleInfer through three pillars. First, it partitions prompts into fixed-size chunks so that the accelerator always runs close to its computation-saturated limit. Second, it disaggregates prefill and decode instances so each can run independently. Finally, it uses a smart two-level scheduling algorithm augmented with predicted resource usage to avoid decode scheduling hotspots. Results show that ShuffleInfer improves time-to-first-token (TTFT), job completion time (JCT), and inference efficiency in terms of performance per dollar by a large margin, e.g., it uses 38% less resources all the while lowering average TTFT and average JCT by 97% and 47%, respectively.},
  archive      = {J_TACO},
  author       = {Cunchen Hu and Heyang Huang and Liangliang Xu and Xusheng Chen and Chenxi Wang and Jiang Xu and Shuang Chen and Hao Feng and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan},
  doi          = {10.1145/3732941},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ShuffleInfer: Disaggregate LLM inference for mixed downstream workloads},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HEngine: A high performance optimization framework on a GPU for homomorphic encryption. <em>TACO</em>, <em>22</em>(2), 1-23. (<a href='https://doi.org/10.1145/3732942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic encryption (HE) represents an encryption technology that allows for direct computation on encrypted data without requiring decryption. However, the substantial computational complexity and significant latency associated with HE has impeded its broader adoption in practical applications. To address these challenges, we propose a GPU-based acceleration framework, namely HEngine, tailored for homomorphic encryption tasks. Specifically, we first propose a warp shuffle-based optimization method for two key phases, i.e., inverse Chinese Remainder Theorem (ICRT) and number theoretic transformation (NTT), to mitigate synchronization overhead in homomorphic encryption. Secondly, we propose to fuse the NTT kernel with the inner product kernel to address the imbalance between memory access and computation. Thirdly, considering the potential difference in the amount of tasks of users in the real-world, we design two different encoding methods for small batch and large batch inference tasks to improve computational efficiency. Finally, experiments demonstrate that our proposed framework achieves a 218× speedup on homomorphic multiplication tasks compared with the CPU-based SEAL library. In addition, for convolutional neural network inference tasks on shallow network structures, our proposed framework achieves amortized inference performance at the millisecond level and sub-millisecond level on small batch and large batch data, respectively. For convolutional neural network inference tasks on deeper network structures (i.e., ResNet-20), our proposed framework achieves second-level inference.},
  archive      = {J_TACO},
  author       = {Jinghao Zhao and Hongwei Yang and Meng Hao and Weizhe Zhang and Hui He and Desheng Wang},
  doi          = {10.1145/3732942},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {HEngine: A high performance optimization framework on a GPU for homomorphic encryption},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AOBO: A fast-switching online binary optimizer on AArch64. <em>TACO</em>, <em>22</em>(2), 1-27. (<a href='https://doi.org/10.1145/3736170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the complexity of real-world server applications continues to grow, performance optimizations for large-scale applications are becoming increasingly challenging. The success of online optimization offered by OCOLOS and Dynimize proves that binary rewriting based on edge profiling data can significantly accelerate these applications. However, no similar online binary optimizer is currently available on the AArch64 platform. In response to the growing adoption of the AArch64 platform, this article introduces AOBO, a fast-switching online binary optimizer specifically designed for AArch64. In addition to providing practical and efficient engineering support for AArch64-specific features, AOBO overcomes the challenge of lacking hardware counters for edge profiling on most commercially available AArch64 servers. In particular, AOBO embraces a novel edge weight estimation scheme to deliver more accurate edge estimation, which in turn allows AOBO’s binary rewriter to generate more efficient code. Furthermore, time spent on AOBO’s online code replacement stage is optimized to work at a subsecond level, thus enabling a fast switch from running the original binary to running the optimized one. We evaluate AOBO with CINT2017, GCC, MySQL and MongoDB, measuring the accuracy and coverage of the estimated edge weights, the performance improvements of the optimized binaries, and the online optimization cost. To make a fair comparison, we are using the performance data of the binaries generated by the default compilation scripts in the software packages as a baseline. Experimental data shows that AOBO can offer a more accurate edge weight estimation and generate binaries with superior performance. Furthermore, AOBO achieves online optimization with a very small overhead and significantly improves the performance of large-scale applications. Compared with the baselines, AOBO’s online optimization can achieve 24.7% and 31.11% performance improvement respectively for MySQL and MongoDB. Notably, application pause time is reduced from 1,599.8 milliseconds to 462.1 milliseconds for MySQL, and from 1,765.9 milliseconds to 507.1 milliseconds for MongoDB.},
  archive      = {J_TACO},
  author       = {Wenlong Mu and Yue Tang and Bo Huang and Jianmei Guo},
  doi          = {10.1145/3736170},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {AOBO: A fast-switching online binary optimizer on AArch64},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cheetah: Accelerating dynamic graph mining with grouping updates. <em>TACO</em>, <em>22</em>(2), 1-26. (<a href='https://doi.org/10.1145/3736173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph pattern mining is essential for deciphering complex networks. In the real world, graphs are dynamic and evolve over time, necessitating updates in mining patterns to reflect these changes. Traditional methods use fine-grained incremental computation to avoid full re-mining after each update, which improves speed but often overlooks potential gains from examining inter-update interactions holistically, thus missing out on overall efficiency improvements. In this article, we introduce Cheetah, a dynamic graph mining system that processes updates in a coarse-grained manner by leveraging exploration domains . These domains exploit the community structure of real-world graphs to uncover data reuse opportunities typically missed by existing approaches. Exploration domains, which encapsulate extensive portions of the graph relevant to updates, allow multiple updates to explore the same regions efficiently. Cheetah dynamically constructs these domains using a management module that identifies and maintains areas of redundancy as the graph changes. By grouping updates within these domains and employing a neighbor-centric expansion strategy, Cheetah minimizes redundant data accesses. Our evaluation of Cheetah across five real-world datasets shows it outperforms current leading systems by an average factor of 2.63×.},
  archive      = {J_TACO},
  author       = {Yi Zhang and Xiaomeng Yi and Yu Huang and Jingrui Yuan and Chuangyi Gui and Dan Chen and Long Zheng and Jianhui Yue and Xiaofei Liao and Hai Jin and Jingling Xue},
  doi          = {10.1145/3736173},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Cheetah: Accelerating dynamic graph mining with grouping updates},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance, energy and NVM lifetime-aware data structure refinement and placement for heterogeneous memory systems. <em>TACO</em>, <em>22</em>(2), 1-27. (<a href='https://doi.org/10.1145/3736174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for increased memory capacity, which also needs to be affordable and sustainable, leads to the adoption of heterogeneous memory hierarchies, combining DRAM and NVM technologies. This work proposes a memory management methodology that relies on multi-objective optimization in terms of performance, energy consumption and impact on NVM’s lifetime, for applications deployed on heterogeneous (i.e., DRAM/NVM) memory systems. We propose a scalable and lightweight data structure exploration flow for supporting data type refinement based on access pattern analysis, enhanced with a weighted-based data placement decision support for multi-objective exploration and optimization. The evaluation of the methodology was performed both on emulated and real DRAM/NVM hardware for different applications and data placement algorithms. The experimental results show up to 58.7% lower execution time and 48.3% less energy consumption compared with the results obtained by the initial versions of the applications. Moreover, we observed 72.6% less NVM write operations, which can significantly extend the lifetime of the NVM memory. Finally, thorough evaluation shows that the methodology is flexible and scalable, as it can integrate different data placement algorithms and NVM technologies and requires reasonable exploration time.},
  archive      = {J_TACO},
  author       = {Manolis Katsaragakis and Christos Baloukas and Lazaros Papadopoulos and Francky Catthoor and Dimitrios Soudris},
  doi          = {10.1145/3736174},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Performance, energy and NVM lifetime-aware data structure refinement and placement for heterogeneous memory systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning workload mapping optimization on jetson platforms. <em>TACO</em>, <em>22</em>(2), 1-23. (<a href='https://doi.org/10.1145/3736175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the performance and energy efficiency of deep learning (DL) applications, recent edge computing platforms have built-in heterogeneous accelerators, such as general-purpose graphics processing units (GPUs) and neural processing units (NPUs). For example, widely used NVIDIA Jetson platforms contain CPU, GPU, and deep learning accelerator (DLA), a type of NPU. It is non-trivial to map DL workloads to suitable accelerators to improve performance, energy efficiency, or even both. This article presents JDIMO, 1 a Jetson-aware deep-learning inference workload mapping optimization framework, to simultaneously improve energy efficiency and performance. JDIMO first measures energy-performance data of the fundamental nodes and the sub-networks with energy-efficiency improvement potential according to the topology structure of a DL network. Then, under the guidance of an analytical energy-performance model, the framework exploits an algorithm based on the variable-length sliding window to find the optimal mapping configuration and the optimal number of CUDA streams. We evaluate JDIMO by applying it to seven DL applications on a Jetson Orin NX (16GB) platform. JDIMO saves 47.5% EDP (energy delay product) and 22.6% energy and improves 138.3% QPS (queries per second) on average compared to the DLA-possible configuration. JDIMO saves 22.5% EDP and 12.6% energy and improves 13.5% QPS on average compared to JEDI, the most similar work to ours. Meanwhile, JDIMO also reduces 93.8% optimization time on average compared to JEDI.},
  archive      = {J_TACO},
  author       = {Farui Wang and Meng Hao and Siyu Yang and Weizhe Zhang},
  doi          = {10.1145/3736175},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {7},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Deep learning workload mapping optimization on jetson platforms},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ceiba: An efficient and scalable DNN scheduler for spatial accelerators. <em>TACO</em>, <em>22</em>(2), 1-23. (<a href='https://doi.org/10.1145/3715123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial accelerators are domain-specific architectures to elevate performance and energy efficiency for deep neural networks (DNNs). They also bring a large number of schedule parameters to determine computation and data movement patterns of DNNs. Previous works formulate the schedule problem as design space exploration or integer linear programming. However, these advanced techniques face the challenge of efficiency or scalability. In this article, we propose Ceiba, which is a deep reinforcement learning-based DNN scheduler for spatial accelerators. Ceiba observes the running DNN computation as well as the spatial architecture to make schedule decisions. Then, Ceiba receives a reward to learn and produce the best-fit policy. To provide efficient and scalable scheduling, Ceiba constructs a DNN-architecture-specific action space. It is defined by upper and lower bounds to exclude invalid and sub-optimal schedule candidates. Extensive experiments demonstrate that Ceiba generally provides better performance for spatial accelerators under a fixed number of searching steps or a fixed amount of time. Specifically, Ceiba achieves an average 2.2× speedup for the Simba accelerator, compared with the state-of-the-art scheduler. When scaling the batch size and the hardware architecture up by 64×, the performance gains of Ceiba are 1.8× and 1.2× on average, respectively. Moreover, Ceiba exhibits better scalability for the Eyeriss accelerator.},
  archive      = {J_TACO},
  author       = {Fuyu Wang and Minghua Shen and Yutong Lu and Nong Xiao},
  doi          = {10.1145/3715123},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Ceiba: An efficient and scalable DNN scheduler for spatial accelerators},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CesASMe and staticdeps: Static detection of memory-carried dependencies for code analyzers. <em>TACO</em>, <em>22</em>(2), 1-23. (<a href='https://doi.org/10.1145/3715125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of code analyzers, such as IACA , uiCA , llvm-mca , or Ithemal , strive to statically predict the throughput of a computation kernel. Each analyzer is based on its own simplified CPU model reasoning at the scale of a basic block. Facing this diversity, evaluating their strengths and weaknesses is important to guide both their usage and their enhancement. We present CesASMe , a fully-tooled solution to evaluate code analyzers on C-level benchmarks composed of a benchmark derivation procedure that feeds an evaluation harness. We conclude that memory-carried data dependencies are a major source of imprecision for these tools. We tackle this issue with staticdeps , a static analyzer extracting memory-carried data dependencies, including across loop iterations, from an assembly basic block. We integrate its output to uiCA , a state-of-the-art code analyzer, to evaluate staticdeps ’ impact on a code analyzer’s precision through CesASMe .},
  archive      = {J_TACO},
  author       = {Théophile Bastian and Hugo Pompougnac and Alban Dutilleul and Fabrice Rastello},
  doi          = {10.1145/3715125},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {CesASMe and staticdeps: Static detection of memory-carried dependencies for code analyzers},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive evaluation and opportunity discovery for deterministic concurrency control. <em>TACO</em>, <em>22</em>(2), 1-24. (<a href='https://doi.org/10.1145/3715126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deterministic concurrency control (DCC) guarantees that the same input transactions produce the same serializable result. It offers benefits in both distributed databases and blockchain systems. Dozens of DCC algorithms have emerged in the past decade. However, there is a lack of comprehensive evaluations for them. To study the performance of existing DCC algorithms and discover further opportunities, we make the following contributions: First, we abstract five essential features from the existing DCC algorithms—generality, speculative mechanism, version strategy, batch strategy, and concurrency mode. Each distinct combination of these features corresponds to a specific algorithm. Second, we implement 13 DCC algorithms and conduct evaluations focused on their features by using 10 workloads, to conclude each feature’s strengths and weaknesses. Third, based on our feature analysis, we discover opportunities for improvement in two existing DCC algorithms, resulting in performance boosts of up to 2.3× and 3.4×.},
  archive      = {J_TACO},
  author       = {Xinyuan Wang and Xingchen Li and Yun Peng and Hejiao Huang},
  doi          = {10.1145/3715126},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Comprehensive evaluation and opportunity discovery for deterministic concurrency control},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARACHNE: Optimizing distributed parallel applications with reduced inter-process communication. <em>TACO</em>, <em>22</em>(2), 1-26. (<a href='https://doi.org/10.1145/3716871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-performance computing (HPC), parallelization is essential for improving computational efficiency as data and computation scales exceed single-node capacity. Existing methods, such as the polyhedral model used in Pluto -Distmem, focus on loop and array optimizations within shared memory but struggle with high communication overheads and inflexibility in distributed environments. These methods often fail to effectively partition computation and manage data across nodes, leading to suboptimal performance. This paper presents Arachne , an innovative system designed to address these shortcomings by generating distributed parallel code with minimized communication overhead. The system introduces a dynamic programming algorithm to optimally distribute computational tasks across multiple processes, ensuring minimal communication costs. It also incorporates user-friendly compiler directives, allowing programmers to influence code generation easily and accommodate a broader range of parallelization scenarios without needing in-depth knowledge of parallel architectures. Arachne significantly reduces the learning curve and need for extensive code modifications, making parallel programming more accessible and efficient. Evaluation of various HPC benchmarks demonstrates that Arachne outperforms existing methods by reducing communication overhead, lowering memory requirements, and supporting more complex parallel logic, thus enhancing the overall scalability and efficiency of HPC applications.},
  archive      = {J_TACO},
  author       = {Yifu He and Han Zhao and Weihao Cui and Shulai Zhang and Quan Chen and Minyi Guo},
  doi          = {10.1145/3716871},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ARACHNE: Optimizing distributed parallel applications with reduced inter-process communication},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic power management through multi-agent deep reinforcement learning for heterogeneous systems. <em>TACO</em>, <em>22</em>(2), 1-23. (<a href='https://doi.org/10.1145/3716872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power management and optimization play a significant role in modern computer systems, from battery-powered devices to servers running in data centers. Existing approaches for power capping fail to meet the requirements presented by dynamic workloads, and the situation becomes even more severe, given the divergent energy efficiency of workloads on heterogeneous hardware platforms. Adaptively optimizing energy consumption for dynamic workloads presents a great challenge to heterogeneous systems. To tackle this challenge, we present a machine learning based method to improve system-level power efficiency. We employ multi-agent deep reinforcement learning (MADRL) to automatically explore the relationship between long-term performance and the power budget for workloads of different types on classic CPU-GPU heterogeneous platforms. Our framework equips each device with an agent, enabling decentralized control over its power budget while maintaining centralized coordination to maximize the running time of applications within a power cap. We evaluate our approach against state-of-the-art methods on CPU-GPU platforms. Experimental results show that our method improves performance by an average of 8.5%. Additionally, our method is significantly more stable compared to the state-of-the-art heuristic approach.},
  archive      = {J_TACO},
  author       = {Yiming Wang and Weizhe Zhang and Meng Hao and Weizhi Kong and Yuan Wen},
  doi          = {10.1145/3716872},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Dynamic power management through multi-agent deep reinforcement learning for heterogeneous systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VersaTile: Flexible tiled architectures via associative processors. <em>TACO</em>, <em>22</em>(2), 1-26. (<a href='https://doi.org/10.1145/3716873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As modern applications demand more data, processing-in-memory (PIM) architectures have emerged to address the challenges of data movement and parallelism. In this article, we propose VersaTile, a heterogeneous, fully CMOS-based tiled architecture that combines conventional out-of-order (OoO) superscalar CPUs and associative processors (APs), a type of CAM-based PIM core. Both CPUs and APs leverage the RISC-V ISA and its standard RVV vector extension. VersaTile fosters collaboration between multiple low-latency CPUs and high-throughput APs by sharing the same software stack and adopting a common CPU programming and compilation frontend. Moreover, we introduce tile stitching, a mechanism enabling the aggregation of multiple APs into a single vector super-unit with modest hardware support and no programming effort. Tile stitching allows us to configure an architecture for optimal performance across a wide range of applications. We provide a detailed case study, including a scalable floorplan example, as well as a comprehensive evaluation of various design points. Our experiments show that, when using only AP tiles, VersaTile can achieve, on average across the Phoenix benchmark suite and 3D convolution, a \(5.7\times\) speedup with respect to area-equivalent OoO CPU cores with SIMD ALUs (up to \(23\times\) ), and \(4.6\times\) with respect to an equivalent-sized monolithic AP baseline (up to \(29\times\) ). For applications with both DLP (vector) and ILP (scalar) regions, VersaTile can use APs and OoO cores collaboratively to achieve better performance than using either one of them only, up to \(4.4\times\) .},
  archive      = {J_TACO},
  author       = {Kailin Yang and José F. Martínez},
  doi          = {10.1145/3716873},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {VersaTile: Flexible tiled architectures via associative processors},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting dynamic regular patterns in irregular programs for efficient vectorization. <em>TACO</em>, <em>22</em>(2), 1-25. (<a href='https://doi.org/10.1145/3716874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern optimizing compilers are able to exploit memory access or computation patterns to generate vectorized codes. However, such patterns in irregular programs are unknown until runtime due to the input dependence. Thus, either compiler’s static optimization or profile-guided optimization cannot represent the patterns for any common input, which leads to suboptimal vectorization. To address the above drawback, we propose DynVec , 1 a framework to automatically exploit regular patterns buried deeply inside irregular programs and apply corresponding optimizations for better vectorization. Due to the integration of workload distribution and the ability to represent instruction features and identify regular patterns with effective feature extraction and data re-arranging methods, DynVec can generate highly efficient vectorized codes for both serial and parallel irregular programs by replacing gather / scatter / reduction operations with optimized operation groups. We evaluate DynVec on optimizing irregular programs such as SpMV and graph programs with representative sparse matrix datasets. The experiment results show that DynVec achieves significant speedup compared to the state-of-the-art implementations across a range of X86 and ARM platforms.},
  archive      = {J_TACO},
  author       = {Kelun Lei and Shaokang Du and Xin You and Hailong Yang and Zhongzhi Luan and Yi Liu and Depei Qian},
  doi          = {10.1145/3716874},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Exploiting dynamic regular patterns in irregular programs for efficient vectorization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OptiFX: Automatic optimization for convolutional neural networks with aggressive operator fusion on GPUs. <em>TACO</em>, <em>22</em>(2), 1-27. (<a href='https://doi.org/10.1145/3716876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) are fundamental to advancing computer vision technologies. As CNNs become more complex and larger, optimizing model inference remains a critical challenge in both industry and academia. On modern GPU platforms, CNN operators are typically memory-bound, leading to significant performance degradation due to memory wall effects. While recent advancements have utilized operator fusion–merging multiple operators into one–to enhance inference performance, the fusion of multiple region-based operators like convolution is seldom addressed. This article introduces AFusion , a novel operator fusion technique aimed at improving inference performance, and OptiFX, an automatic optimization framework based on this approach. OptiFX employs a cost-based backtracking search to identify optimal sub-graphs for fusion and utilizes template-based code generation to create efficient kernels for these fused sub-graphs. We evaluate OptiFX across seven prominent CNN architectures–GoogLeNet, ResNet, DenseNet, MobileNet, SqueezeNet, NasNet, and UNet–on Nvidia A6000 Ada, RTX 4090, and Jetson AGX Orin platforms. Our results demonstrate that OptiFX significantly outperforms existing methods, achieving average speedups of \(2.91\times\) , \(3.30\times\) , and \(2.09\times\) in accelerating inference performance on these platforms, respectively.},
  archive      = {J_TACO},
  author       = {Xueying Wang and Shigang Li and Hao Qian and Fan Luo and Zhaoyang Hao and Tong Wu and Ruiyuan Xu and Huimin Cui and Xiaobing Feng and Guangli Li and Jingling Xue},
  doi          = {10.1145/3716876},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {OptiFX: Automatic optimization for convolutional neural networks with aggressive operator fusion on GPUs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransCL: An automatic CUDA-to-OpenCL programs transformation framework. <em>TACO</em>, <em>22</em>(2), 1-24. (<a href='https://doi.org/10.1145/3718987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rising demand for computational power and the increasing variety of computational scenarios, considerable interest has emerged in transforming existing CUDA programs into more general-purpose OpenCL programs, enabling them to run across diverse hardware platforms. However, manual methods, typically designed for specific applications, lack flexibility. Current automated conversion techniques also face considerable challenges, particularly in handling diverse programming interfaces, memory management, and so on, and are insufficient for converting large-scale, complex CUDA projects. In this article, we propose a novel source-to-source program transformation framework, TransCL, which automates the conversion of CUDA programs in four key aspects: source code, execution model, programming model, and memory model. To achieve this, we abstract a set of conversion rules aligned with the latest CUDA standards, develop a transcoder, implement an OpenCL-compatible programming interface library, and establish a memory mapping mechanism between CUDA and OpenCL. Experiments demonstrate that TransCL provides a high level of automation in converting CUDA-based applications and is effective in handling large, complex projects such as TensorFlow. Moreover, the converted AI framework successfully conducted model training for the first time. The experiment also validates that the converted program can execute correctly across multiple platforms and demonstrate good performance.},
  archive      = {J_TACO},
  author       = {Changqing Shi and Yufei Sun and Rui Chen and Jiahao Wang and Qiang Guo and Chunye Gong and Yicheng Sui and Yutong Jin and Yuzhi Zhang},
  doi          = {10.1145/3718987},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {TransCL: An automatic CUDA-to-OpenCL programs transformation framework},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shift-CIM: In-SRAM alignment to support general-purpose bit-level sparsity exploration in SRAM multiplication. <em>TACO</em>, <em>22</em>(2), 1-25. (<a href='https://doi.org/10.1145/3719654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplication plays a critical role in SRAM-based Computing-in-Memory (CIM) architectures. However, current SRAM-based CIMs face three major limitations. First, they do not fully exploit bit-level sparsity, resulting in unnecessary overhead in both latency and energy consumption. Second, the generation of numerous zero-dot products is superfluous. Third, the irregular organization of SRAM complicates the implementation. To address these issues, we propose Shift-CIM, a general-purpose approach that fully leverages bit-level sparsity within SRAM-based multiplications. Shift-CIM aligns the multipliers within the SRAM array, accumulating only the required dot products based on the non-zero bits of the multipliers. Shift-CIM achieves a regular SRAM organization by assembling two irregular SRAM arrays in a transposed manner. Our evaluations show that Shift-CIM is highly efficient, operating at a supply voltage of 0.9 V and a frequency of 833 MHz, while incurring only a 4.8% area overhead. Despite these modest requirements, Shift-CIM significantly accelerates multiplication operations, achieving up to 3.08× the performance improvement and a 60% reduction in energy consumption compared to state-of-the-art designs.},
  archive      = {J_TACO},
  author       = {Gaoyang Zhao and Qiuran Li and Rongzhen Lin and Yaohua Wang},
  doi          = {10.1145/3719654},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Shift-CIM: In-SRAM alignment to support general-purpose bit-level sparsity exploration in SRAM multiplication},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FusionFS: A contention-resilient file system for persistent CPU caches. <em>TACO</em>, <em>22</em>(2), 1-23. (<a href='https://doi.org/10.1145/3719656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Byte-addressable storage (BAS), such as persistent memory and CXL-SSDs, does not meet system designers’ expectations for data flushing and access granularity. Persistent CPU caches, enabled by recent techniques like Intel’s eADR and CXL’s Global Persistent Flush, can mitigate these issues without sacrificing consistency. However, the shared nature of CPU caches can lead to cache contention, which can result in cached data being frequently evicted to the BAS and reloaded into caches, negating the benefits of caching. If the BAS write granularity is larger than the cacheline eviction granularity, this can also lead to severe write amplification. In this article, we identify, characterize, and propose solutions to the problem of contention in persistent CPU caches, which is largely overlooked by existing systems. These systems either simply assume that cached data is hot enough to survive cache evictions or use unsupported cache allocation techniques without testing their effectiveness. We also present FusionFS, a contention-resilient kernel file system that uses persistent CPU caches to redesign data update approaches. FusionFS employs an adaptive data update approach that chooses the most effective mechanism based on file access patterns during system calls and memory mapping accesses, minimizing BAS media writes and improving throughput. FusionFS also employs contention-aware cache allocation to minimize various types of cache contention. Experimental results show that FusionFS outperforms existing file systems and effectively mitigates various types of cache contention.},
  archive      = {J_TACO},
  author       = {Congyong Chen and Shengan Zheng and Yuhang Zhang and Linpeng Huang},
  doi          = {10.1145/3719656},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {FusionFS: A contention-resilient file system for persistent CPU caches},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SimTrace: Exploiting spatial and temporal sampling for large-scale performance analysis. <em>TACO</em>, <em>22</em>(2), 1-26. (<a href='https://doi.org/10.1145/3720544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MPI tracing tools is essential to collect the communication events and performance metrics of large-scale programs for further performance analysis and optimization. However, toward the exascale era, the performance and storage overhead for tracing becomes extremely prohibitive that significantly disturbs the original execution of MPI programs, leading to distorted tracing data and thus mislead analysis results. Although process sampling can effectively reduce the tracing overhead, it can easily miss important execution information that is necessary for subsequent performance analysis. In this article, we propose SimTrace , a scalable MPI tracing tool with novel spatial and temporal sampling strategies that exploits the similarity among MPI processes to achieve both low tracing overhead as well as obtain sufficient tracing information. The experimental results demonstrate that SimTrace can significantly reduce the MPI tracing overhead compared to the state-of-the-art tracing tools, meanwhile enabling effective analysis to guide performance optimization of large-scale programs.},
  archive      = {J_TACO},
  author       = {Zhibo Xuan and Xin You and Tianyu Feng and Hailong Yang and Zhongzhi Luan and Yi Liu and Depei Qian},
  doi          = {10.1145/3720544},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SimTrace: Exploiting spatial and temporal sampling for large-scale performance analysis},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overlapping aware data placement optimizations for LSM tree-based store on ZNS SSDs. <em>TACO</em>, <em>22</em>(2), 1-25. (<a href='https://doi.org/10.1145/3721287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solid State Drives (SSDs) based on the NVMe Zoned Namespaces (ZNS) interface can notably reduce the costs of address mapping, garbage collection, and over-provisioning by dividing the storage space into multiple zones for sequential writes and random reads. The Log-Structured Merge (LSM) tree, which is extensively used in key-value storage systems, converts random writes to sequential writes, hence a suitable scenario to utilize ZNS SSDs. However, LSM tree associated data significantly varies in lifetime due to the levels and merging mechanisms of the LSM tree. Therefore, without an accurate method to estimate data lifetime, data with disparate lifetimes may be placed in the same zone, thus causing low space utilization and high write amplification within the SSD. To address these issues, the article proposes two data overlapping aware optimizations to realize intelligent data placement: a zone allocation scheme and a garbage collection scheme. The key technique of these optimizations is an accurate data-lifetime estimation by considering both the associated tree level of the data and the data overlapping ratio between the data and those in the neighboring level. Using the estimation technique, the zone allocation optimization can place data with similar lifetimes in the same zone. Besides, the garbage collection optimization can reclaim zones in an adaptive manner based on overlapping ratios to reduce the amount of data migration. Experimental results demonstrate that the optimization schemes effectively reduce garbage collection-incurred data copy by average factors of 2.11× and 1.50× in comparison to a conventional work and a state-of-the-art work, respectively. Consequently, the proposed work successfully alleviates the write amplification effect by 18% and 6%, compared to the conventional work and the state-of-the-art work, respectively.},
  archive      = {J_TACO},
  author       = {Jingcheng Shen and Lang Yang and Linbo Long and Zhenhua Tan and Congming Gao and Kan Zhong and Masao Okita and Fumihiko Ino},
  doi          = {10.1145/3721287},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Overlapping aware data placement optimizations for LSM tree-based store on ZNS SSDs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ODGS: Dependency-aware scheduling for high-level synthesis with graph neural network and reinforcement learning. <em>TACO</em>, <em>22</em>(2), 1-25. (<a href='https://doi.org/10.1145/3721289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling determines the execution order and time of operations in a program. The order is related to operation dependencies, including data and resource dependencies. Data dependency is intrinsic in a program, showing operation data flow. Resource dependency is determined by scheduling methods, resolving operation resource contention. Existing scheduling methods focus on data dependency, rather than building and exploiting operation dependency graph (ODG) with extra resource dependency. As ODG contains all dependencies determining operation execution order, it provides global program information, facilitating efficient scheduling. In this work, we propose ODGS, a dependency-aware scheduling method for high-level synthesis with graph neural network (GNN) and reinforcement learning (RL). We adopt GNN to perceive accurate relations between operations. We use the relations to guide an RL agent in building a complete ODG. We perform feedback-guided iterative scheduling with ODG to converge to a high-quality solution. Experiments show that our method reduces 16.4% latency and 26.5% resource usage on average, compared with the latest RL-based method. Moreover, we reduce an average 2.9% latency over the GNN-based method under the same resource usage. The same resource usage is obtained by improving the GNN-based method with manual resource constraint tuning. Without tuning, its basic version consumes an average 237.6% more resources than our method.},
  archive      = {J_TACO},
  author       = {Minghua Shen and Aoxiang Qin and Nong Xiao},
  doi          = {10.1145/3721289},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {6},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ODGS: Dependency-aware scheduling for high-level synthesis with graph neural network and reinforcement learning},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent scheduling approach on mobile OS for optimizing UI smoothness and power. <em>TACO</em>, <em>22</em>(1), 1-27. (<a href='https://doi.org/10.1145/3674910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices need to respond quickly to diverse user inputs. The existing approaches often heuristically raise the CPU/GPU frequency according to the empirical rules when facing burst inputs and various changes. Although doing so can be effective sometimes, the existing approaches still need improvements. For instance, raising processors’ frequency can lead to high power consumption when the frequency is over-provisioned or fail to meet user demands when the frequency is under-provisioned. To this end, we propose MobiRL, a reinforcement learning-based scheduler for intelligently adjusting the CPU/GPU frequency to satisfy user demands accurately on mobile systems. MobiRL monitors the mobile system status and autonomously learns to optimize UI smoothness and power consumption by conducting CPU/GPU frequency-adjusting actions. The experimental results on the latest delivered smartphones show that MobiRL outperforms the widely used commercial scheduler on real devices—reducing the frame drop rate by 4.1% and reducing power consumption by 42.8%, respectively. Moreover, compared with a study using Q-Learning for CPU frequency scheduling, MobiRL achieves up to a 2.5% lower frame drop rate and reduces power consumption by 32.6%, respectively. Our approach has been deployed in mobile phone products.},
  archive      = {J_TACO},
  author       = {Xinglei Dou and Lei Liu and Limin Xiao},
  doi          = {10.1145/3674910},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {An intelligent scheduling approach on mobile OS for optimizing UI smoothness and power},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DLAS: A conceptual model for across-stack deep learning acceleration. <em>TACO</em>, <em>22</em>(1), 1-28. (<a href='https://doi.org/10.1145/3688609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) are very computationally demanding, which presents a significant barrier to their deployment, especially on resource-constrained devices. Significant work from both the machine learning and computing systems communities has attempted to accelerate DNNs. However, the number of techniques available and the required domain knowledge for their exploration continue to grow, making design space exploration (DSE) increasingly difficult. To unify the perspectives from these two communities, this article introduces the Deep Learning Acceleration Stack (DLAS), a conceptual model for DNN deployment and acceleration. We adopt a six-layer representation that organizes and illustrates the key areas for DNN acceleration, from machine learning to software and computer architecture. We argue that the DLAS model balances simplicity and expressiveness, assisting practitioners from various domains in tackling co-design acceleration challenges. We demonstrate the interdependence of the DLAS layers, and thus the need for co-design, through an across-stack perturbation study, using a modified tensor compiler to generate experiments for combinations of a few parameters across the DLAS layers. Our perturbation study assesses the impact on inference time and accuracy when varying DLAS parameters across two datasets, seven popular DNN architectures, four compression techniques, three algorithmic primitives (with sparse and dense variants), untuned and auto-scheduled code generation, and four hardware platforms. The study observes significant changes in the relative performance of design choices with the introduction of new DLAS parameters (e.g., the fastest algorithmic primitive varies with the level of quantization). Given the strong evidence for the need for co-design, and the high costs of DSE, DLAS offers a valuable conceptual model for better exploring advanced co-designed accelerated deep learning solutions.},
  archive      = {J_TACO},
  author       = {Perry Gibson and Jose Cano and Elliot Crowley and Amos Storkey and Michael O’boyle},
  doi          = {10.1145/3688609},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {DLAS: A conceptual model for across-stack deep learning acceleration},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high scalability memory NoC with shared-inside hierarchical-groupings for triplet-based many-core architecture. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3688610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Innovative processor architecture designs are shifting towards Many-Core Architectures (MCAs) to meet the future demands of high-performance computing as the limits of Moore’s Law have almost been reached. Many-core processors utilize shared memory hierarchies to achieve high-speed memory systems, improving memory access efficiency. However, as the number of cores multiplies, the scalability of this system is significantly constrained by the increased proportion of long-distance and Non-Uniform Memory Access (NUMA). Improving the scalability of MCAs is crucial for achieving large/super-scale general-purpose many-core processors. This work proposes a high-scalability memory Network-on-Chip (NoC) for Triplet-Based Many-Core Architecture (TriBA), named TriBA-mNoC. TriBA-mNoC maintains a consistent core-to-core spacing as the network scale increases, effectively preventing increased long-distance memory access latency. Moreover, it leverages an inherent advantage of shared-inside hierarchical-groupings, alleviating common NUMA issues in the NoC design. Evaluations of static network characteristics show that TriBA-mNoC outperforms most classical NoCs in network diameter, average distance, and cost. TriBA-mNoC can be integrated with TriBA in the same silicon die with a tile-like floorplan, forming a novel NoC called TriBA-NoC, which can combine the strengths of both networks to maximize the architecture performance. We evaluated the memory access performance and scalability of TriBA-NoC using the mathematical evaluation models and actual simulations with real traffic (PARSEC 3.0 and SPLASH-2) at different network scales. The mathematical evaluation results indicate that TriBA-NoC achieves an aggregate speedup of approximately 3x compared with 2D-Mesh for a similar number of cores. Furthermore, TriBA-NoC’s single-core speedup efficiency remains stable as the number of cores increases under the same cache hit ratio, whereas 2D-Mesh experiences a rapid decline, highlighting TriBA-NoC’s exceptional scalability. Finally, the actual traffic simulation results show that TriBA-NoC achieves an average memory access latency and time reduction of 25.90% to 40.50% and 5.61% to 31.69%, respectively, compared with 2D-Mesh.},
  archive      = {J_TACO},
  author       = {Chunfeng Li and Feng Shi and Fei Yin and Karim Soliman and Jin Wei},
  doi          = {10.1145/3688610},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A high scalability memory NoC with shared-inside hierarchical-groupings for triplet-based many-core architecture},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient ReRAM-based accelerator for asynchronous iterative graph processing. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3689335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph processing has become a central concern for many real-world applications and is well-known for its low compute-to-communication ratios and poor data locality. By integrating computing logic into memory, resistive random access memory (ReRAM) tackles the demand for high memory bandwidth in graph processing. Despite the years’ research efforts, existing ReRAM-based graph processing approaches still face the challenges of redundant computation overhead . It is because the vertices of many subgraphs are ineffectively and repeatedly processed over the ReRAM crossbars for lots of iterations so as to update their states according to the vertices of other subgraphs regardless of the dependencies among the subgraphs. In this article, we propose ASGraph , a dependency-aware ReRAM-based graph processing accelerator that overcomes the aforementioned performance bottlenecks. Specifically, ASGraph dynamically constructs the subgraph based on the dependencies between vertices’ states and then detects constructed subgraph that owns high value (it is likely that it has accumulated many state propagations from its neighbors and is able to affect more other neighbors) to be preferentially processed. In this way, it makes the vertex states propagate along the dependencies between vertices as much as possible to reduce the redundant computation. Besides, ASGraph employs a hybrid processing scheme to accelerate the state propagations of the tightly connected subgraph, thereby minimizing the redundant computations. Experimental results show that ASGraph achieves 25.5× and 4.8× speedup and 70.8× and 2.2× energy saving on average compared with the state-of-the-art ReRAM-based graph processing accelerators, that is, GraphR and GaaS-X, respectively.},
  archive      = {J_TACO},
  author       = {Jin Zhao and Yu Zhang and Donghao He and Qikun Li and Weihang Yin and Hui Yu and Hao Qi and Xiaofei Liao and Hai Jin and Haikun Liu and Linchen Yu and Zhang Zhan},
  doi          = {10.1145/3689335},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {An efficient ReRAM-based accelerator for asynchronous iterative graph processing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GraphService: Topology-aware constructor for large-scale graph applications. <em>TACO</em>, <em>22</em>(1), 1-24. (<a href='https://doi.org/10.1145/3689341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based services are becoming integrated into everyday life through graph applications and graph learning systems. While traditional graph processing approaches boast excellent throughput with millisecond-level processing time, the construction phase before executing kernel graph operators (e.g., breadth-first search, single-source shortest path) can take up to tens of hours, severely impacting the quality of graph service. Is it feasible to develop a fast graph constructor that can complete the construction process within minutes, or even seconds? This article aims to answer this question. We present GraphService , a flexible and efficient graph constructor for fast graph applications. To facilitate graph applications with better service, we equip GraphService with a hierarchy-aware graph partitioner based on communication topology as well as a graph topology-aware compression by exploiting a huge number of identical-degree vertices within graph topology. Our evaluation, performed on a range of graph operations and datasets, shows that GraphService significantly reduces communication cost by three orders of magnitude to construct a graph. Furthermore, we tailor GraphService for downstream graph tasks and deploy it on a production supercomputer using 79,024 computing nodes, achieving a remarkable graph processing throughput that outperforms the top-ranked supercomputer on the latest Graph500 list, with construction time reduced by orders of magnitude.},
  archive      = {J_TACO},
  author       = {Xinbiao Gan},
  doi          = {10.1145/3689341},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GraphService: Topology-aware constructor for large-scale graph applications},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging the hardware resources to accelerate cryo-EM reconstruction of RELION on the new sunway supercomputer. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3701990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast development of biomolecular structure determination has enabled the fine-grained study of objects in the micro-world, such as proteins and RNAs. The world is benefited. However, as the computational algorithms are constantly developed, the enrichment of features increases the algorithmic complexity and brings more computationally unfriendly modules. It calls for efficient solutions to leverage the rich and various hardware resources from the world’s most state-of-the-art supercomputing systems, and to fully accelerate the performance of the applications. In this article, we present our efforts on porting and optimizing the 3D reconstruction of RELION, one of the most popular cryo-EM software for biomolecular structure determinations, by leveraging different resources of the latest generation of Sunway heterogeneous supercomputer. Several novel approaches are proposed to resolve different challenges faced by the complex algorithm, including a multi-level parallel scheme and operator optimizations to smartly map and scale RELION, efficient strategies to largely address the memory bottlenecks and improve data locality, lock-free writing solutions to minimize write-write conflicts, and pipelining approaches to obtain excellent computation and communication overlap. Combining all proposed optimizations, the computation time is greatly reduced to under 2 hours, achieving 11.9× and 8.9× speedups on two different datasets. The overall design scales to 131,072 cores, increasing parallel efficiency from 33% to 61% and from 46% to 70%, respectively. To the best of our knowledge, this is the first work that fully optimized and scaled the 3D reconstruction of RELION using the latest Sunway system.},
  archive      = {J_TACO},
  author       = {Jingle Xu and Jiayu Fu and Lin Gan and Yaojian Chen and Zhaoqi Sun and Zhenchun Huang and Guangwen Yang},
  doi          = {10.1145/3701990},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Leveraging the hardware resources to accelerate cryo-EM reconstruction of RELION on the new sunway supercomputer},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PARADISE: Criticality-aware instruction reordering for power attack resistance. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3701991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power side-channel attacks exploit the correlation of power consumption with the instructions and data being processed to extract secrets from a device (e.g., cryptographic keys). Prior work primarily focused on protecting small embedded micro-controllers and in-order processors rather than high-performance, out-of-order desktop and server CPUs. In this article, we present Paradise , a general-purpose out-of-order processor with always-on protection, that implements a novel dynamic instruction scheduler to provide obfuscated execution and mitigate power analysis attacks. To achieve this, we exploit the time between operand availability of critical instructions ( slack ) and create high-performance random schedules. Further, we highlight the dangers of using incorrect adversarial assumptions, which can often lead to a false sense of security. Therefore, we perform an extended security analysis on AES-128 using different levels of adversaries, from basic to advanced, including a convolution neural networks–based attack. Our advanced security evaluation assumes a strong adversary with full knowledge of the countermeasure and demonstrates a significant security improvement of 556 × when combined with Boolean Masking over a baseline only protected by masking and 62,500× over an unprotected baseline. The resulting overhead in performance, power, and area of Paradise is 3.2%, 1.2%, and 0.8% respectively. 1},
  archive      = {J_TACO},
  author       = {Yun Chen and Ali Hajiabadi and Romain Poussier and Yaswanth Tavva and Andreas Diavastos and Shivam Bhasin and Trevor E. Carlson},
  doi          = {10.1145/3701991},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {PARADISE: Criticality-aware instruction reordering for power attack resistance},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shining light on the inter-procedural code obfuscation: Keep pace with progress in binary diffing. <em>TACO</em>, <em>22</em>(1), 1-28. (<a href='https://doi.org/10.1145/3701992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software obfuscation techniques have lost their effectiveness due to the rapid development of binary diffing techniques, which can achieve accurate function matching and identification. In this paper, we propose a new inter-procedural code obfuscation mechanism KHaos , 1 which moves the code across functions to obfuscate the function by using compilation optimizations. Three obfuscation primitives are proposed to separate, aggregate, and hide the function. They can be combined to enhance the obfuscation effect further. This article also reveals distinguishing factors on obfuscation and compiler optimization and presents novel observations to gain insights into the impact of actively utilizing compiler optimization in obfuscation. A prototype of KHaos is implemented and evaluated on a large number of real-world programs. Experimental results show that KHaos outperforms existing code obfuscations and can significantly reduce the accuracy rates of six state-of-the-art binary diffing techniques with lower runtime overhead.},
  archive      = {J_TACO},
  author       = {Peihua Zhang and Chenggang Wu and Hanzhi Hu and Lichen Jia and Mingfan Peng and Jiali Xu and Mengyao Xie and Yuanming Lai and Yan Kang and Zhe Wang},
  doi          = {10.1145/3701992},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Shining light on the inter-procedural code obfuscation: Keep pace with progress in binary diffing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterating pointers: Enabling static analysis for loop-based pointers. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3701993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pointers are an integral part of C and other programming languages. They enable substantial flexibility from the programmer’s standpoint, allowing the user fine, unmediated control over data access patterns. However, accesses done through pointers are often hard to track and challenging to understand for optimizers, compilers, and sometimes, even for the developers themselves because of the direct memory access they provide. We alleviate this problem by exposing additional information to analyzers and compilers. By separating the concept of a pointer into a data container and an offset, we can optimize C programs beyond what other state-of-the-art approaches are capable of, in some cases even enabling auto-parallelization. Using this process, we are able to successfully analyze and optimize code from OpenSSL, the Mantevo benchmark suite, and the Lempel–Ziv–Oberhumer compression algorithm. We provide the only automatic approach able to find all parallelization opportunities in the HPCCG benchmark from the Mantevo suite the developers identified, and we even outperform the reference implementation by up to 18% as well as speed up the PBKDF2 algorithm implementation from OpenSSL by up to 11×.},
  archive      = {J_TACO},
  author       = {Andrea Lepori and Alexandru Calotoiu and Torsten Hoefler},
  doi          = {10.1145/3701993},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Iterating pointers: Enabling static analysis for loop-based pointers},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTAP: Accelerating strongly-typed programs with data type-aware hardware prefetching. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3701994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Queries on linked data structures, such as trees and graphs, often suffer from frequent cache misses and significant performance loss due to dependent and random pointer-chasing memory accesses. In this article, we propose a software-hardware co-designed solution for accelerating linked data structures implemented in strongly typed languages. The solution incorporates a compiler extension and a hardware prefetcher. The compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher. The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance. By doing so, the program can find these objects in the cache when it follows the prefetched pointers, thus minimizing cache misses. In the evaluation, the proposed solution achieves an average speedup of 1.37× over a set of memory-intensive benchmarks.},
  archive      = {J_TACO},
  author       = {Yingshuai Dong and Chencheng Ye and Haikun Liu and Liting Tang and Xiaofei Liao and Hai Jin and Cheng Chen and Yanjiang Li and Yi Wang},
  doi          = {10.1145/3701994},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {DTAP: Accelerating strongly-typed programs with data type-aware hardware prefetching},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepZoning: Re-accelerate CNN inference with zoning graph for heterogeneous edge cluster. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3701995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallelizing CNN inference on heterogeneous edge clusters with data parallelism has gained popularity as a way to meet real-time requirements without sacrificing model accuracy. However, existing algorithms struggle to find optimal parallel granularity for complex CNNS, the structure of which is a directed acyclic graph (DAG) rather than a chain, and the parallel dimension is inflexible. To distribute the workload of modern CNNs on heterogeneous devices is also proven as NP-hard problem. In this article, we introduce DeepZoning , a versatile and cooperative inference framework that combines both model and data parallelism to accelerate CNN inference. DeepZoning employs two algorithms at different levels: (1) a low-level Adaptive Workload Partition algorithm that uses linear programming and takes spatial and channel dimensions into optimization during the search for feature map distribution on heterogeneous devices, and (2) a high-level Model Partition algorithm that finds the optimal model granularity and organizes complex CNNs into sequential zones to balance communication and computation during execution. Our experimental evaluations show that DeepZoning is effective, achieving up to a 3.02× speed improvement on our experimental prototype compared to state-of-the-art algorithms.},
  archive      = {J_TACO},
  author       = {Jingyu Wang and Ruilong Ma and Xiang Yang and Qi Qi and Zirui Zhuang and Jing Wang and Jianxin Liao and Song Guo},
  doi          = {10.1145/3701995},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {DeepZoning: Re-accelerate CNN inference with zoning graph for heterogeneous edge cluster},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ATP: Achieving throughput peak for DNN training via smart GPU memory management. <em>TACO</em>, <em>22</em>(1), 1-27. (<a href='https://doi.org/10.1145/3701996'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limited GPU memory, the performance of large DNNs training is constrained by the unscalable batch size. Existing studies partially address the issue of GPU memory limit through tensor recomputation and swapping, but overlook the exploration of optimal performance. In response, we propose ATP, a recomputation and swapping based GPU memory management framework that aims to maximize training performance by breaking GPU memory constraints. ATP utilizes a throughput model and we propose to evaluate the theoretical peak performance achievable by DNN training on GPU, and provide the optimum memory size required for recomputation and swapping. We optimize the mechanisms for GPU memory pool and CUDA stream control, employ an optimization method to search for specific tensors requiring recomputation and swapping, thereby bringing the actual DNN training performance on ATP closer to theoretical values. Evaluations with different types of large DNN models indicate that ATP achieve throughput improvements ranging from 1.14∼ 1.49×, while support model training exceeding the GPU memory limit by up to 9.2×.},
  archive      = {J_TACO},
  author       = {Weiduo Chen and Xiaoshe Dong and Fan Zhang and Bowen Li and Yufei Wang and Qiang Wang},
  doi          = {10.1145/3701996},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ATP: Achieving throughput peak for DNN training via smart GPU memory management},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MemoriaNova: Optimizing memory-aware model inference for edge computing. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3701997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deploying deep learning models on edge devices has become pervasive, driven by the increasing demand for intelligent edge computing solutions across various industries. From industrial automation to intelligent surveillance and healthcare, edge devices are being leveraged for real-time analytics and decision-making. Existing methods face two challenges when deploying machine learning models on edge devices. The first challenge is handling the execution order of operators with a simple strategy, which can lead to a potential waste of memory resources when dealing with directed acyclic graph structure models. The second challenge is that they usually process operators of a model one by one to optimize the inference latency, which may lead to the optimization problem getting trapped in local optima. We present MemoriaNova, comprising BTSearch and GenEFlow, to solve these two problems. BTSearch is a graph state backtracking algorithm with efficient pruning and hashing strategies designed to minimize memory overhead during inference and enlarge latency optimization search space. GenEFlow, based on genetic algorithms (GA), integrates latency modeling, and memory constraints to optimize distributed inference latency. This innovative approach considers a comprehensive search space for model partitioning, ensuring robust and adaptable solutions. We implement BTSearch and GenEFlow and test them on 11 deep-learning models with different structures and scales. The results show that BTSearch can reach 12% memory optimization compared with the widely used random execution strategy. At the same time, GenEFlow reduces inference latency by 33.9% in distributed systems with four-edge devices.},
  archive      = {J_TACO},
  author       = {Renjun Zhang and Tianming Zhang and Zinuo Cai and Dongmei Li and Ruhui Ma and Buyya Rajkumar},
  doi          = {10.1145/3701997},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {MemoriaNova: Optimizing memory-aware model inference for edge computing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRAGA: A priority-aware Hardware/Software co-design for high-throughput graph processing acceleration. <em>TACO</em>, <em>22</em>(1), 1-27. (<a href='https://doi.org/10.1145/3701998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph processing is pivotal in deriving insights from complex data structures but faces performance limitations due to the irregular nature of graphs. Traditional general-purpose processors often struggle with low instruction-level parallelism and energy inefficiency when handling graph data. In response, modern graph accelerators have embraced an intra-edge-parallel model to enhance parallelization, significantly outperforming conventional processors. However, the indiscriminate processing of edges in existing systems results in substantial computational redundancy, negatively impacting overall efficiency. This article introduces PRAGA, an innovative graph accelerator designed to optimize efficiency by selectively processing edges that significantly contribute to final results while preserving high computational parallelism. PRAGA utilizes an intra-edge-sequential model, prioritizing edge processing to capitalize on coarse-grained vertex-level parallelism and minimize unnecessary computations. It incorporates a hot-value manager to alleviate network-on-chip congestion and a memory-aware coalescer to minimize redundant data accesses. Our experimental results, obtained using a Xilinx Alveo U280 FPGA accelerator card, demonstrate that PRAGA achieves speedups of 17.88× and 5.86× over state-of-the-art accelerators ScalaGraph and GraphDyns, respectively, and outperforms the advanced GPU-based system Gunrock by 22.52× on average. This substantial improvement underscores PRAGA’s potential to redefine performance benchmarks in graph processing.},
  archive      = {J_TACO},
  author       = {Long Zheng and Bing Zhu and Pengcheng Yao and Yuhang Zhou and Chengao Pan and Wenju Zhao and Xiaofei Liao and Hai Jin and Jingling Xue},
  doi          = {10.1145/3701998},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {PRAGA: A priority-aware Hardware/Software co-design for high-throughput graph processing acceleration},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constructing a supplementary benchmark suite to represent android applications with user interactions by using performance counters. <em>TACO</em>, <em>22</em>(1), 1-28. (<a href='https://doi.org/10.1145/3701999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We find existing benchmark suites for smartphone CPU micro-architecture design such as Geekbench 5.0 fail to authentically represent the micro-architecture-level performance behavior of widely used real Android applications with interactive operations such as screen sliding. It is therefore crucial to systematically construct a benchmark suite as a supplementary to Geekbench to represent the user interaction behavior of Android applications for CPU micro-architecture design. The key is to identify a small number of representative programs from a large number of real applications. To this end, a set of features used to represent a program need to be constructed, and these features should be fair for different micro-architectures and can be collected efficiently. However, this is extremely difficult for Android applications. For example, the feature collection tools for Android applications are unavailable for benchmark selection. In this article, we propose a novel benchmark suite construction approach dubbed BEMAP to efficiently build a supplementary benchmark suite from real-world Android applications to represent their user interaction behavior. 1 BEMAP innovates four techniques. The first technique, called two-stage RFC (representative feature construction), constructs program features from performance counters (events) to represent a program for selecting benchmarks from a large number of real Android applications in two stages. The first stage identifies a set of important performance events in terms of IPC (instructions per cycle) by employing a machine learning algorithm named SGBRT (Stochastic Gradient Boosted Regression Tree). The second stage constructs representative features based on the important performance events by using ICA (independent component analysis). The second technique, named SPC-MMA (source performance counters from multiple micro-architectures), collects the performance events from multiple mobile CPUs with different micro-architectures and mixes them as the source of RFC. The goal of these two innovations is to make the program features fair to different mobile CPU micro-architectures. The third technique, called ES (Elbow-Silhouette) approach, artfully leverages the synergy between the elbow method and the silhouette method to determine an optimal K when we use K-Means to group Android applications. The fourth technique is that we design a new tool named AutoProfiler to automatically profile the micro-architecture events (e.g., IPC, L1 Icache misses) of Android applications with interactive operations. Using the proposed BEMAP methodology, 2 we constructed SPBench, a novel benchmark suite supplementary to traditional mobile benchmark suites like Geekbench, for mobile CPU micro-architecture design. It consists of 15 benchmarks selected from 100 real Android applications with 3 common user interaction operations, which is the fifth innovation of this article. The experimental results on four significantly different micro-architectures show that SPBench can represent the micro-architecture performance behaviors of the 100 real-world applications with 3 common user interactive operations on each micro-architecture with significantly higher accuracy than benchmark suites produced by the state-of-the-art approaches.},
  archive      = {J_TACO},
  author       = {Chenghao Ouyang and Jinhan Xin and Siqi Zeng and Guohui Li and Jianjun Li and Zhibin Yu},
  doi          = {10.1145/3701999},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Constructing a supplementary benchmark suite to represent android applications with user interactions by using performance counters},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple function merging for code size reduction. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3702000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource-constrained environments, such as embedded devices, have limited amounts of memory and storage. Practical programming languages such as C++ and Rust tend to output multiple similar functions by monomorphizing polymorphic functions. An optimization technique called Function Merging, which merges similar functions into a single function, has been studied. However, in the state-of-the-art approach, the number of functions that can be merged at once is limited to two; thus, efficiently merging three or more functions, which are often generated from polymorphic functions, has been impossible. In this study, we propose Multiple Function Merging optimization, which targets merging three or more similar functions into a single function using a multiple sequence alignment algorithm. With multiple aligned information, Multiple Function Merging can increase merge opportunities and reduce extra branching overheads at the code generation stage. We evaluated it using the SPEC CPU benchmark suite and some large-scale C/C++ programs, and the results show that it reduces code size by as much as 7.61% compared with the state-of-the-art approach.},
  archive      = {J_TACO},
  author       = {Yuta Saito and Kazunori Sakamoto and Hironori Washizaki and Yoshiaki Fukazawa},
  doi          = {10.1145/3702000},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Multiple function merging for code size reduction},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RT-GNN: Accelerating sparse graph neural networks by tensor-CUDA kernel fusion. <em>TACO</em>, <em>22</em>(1), 1-27. (<a href='https://doi.org/10.1145/3702001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have achieved remarkable successes in various graph-based learning tasks, thanks to their ability to leverage advanced GPUs. However, GNNs currently face challenges arising from the concurrent use of advanced Tensor Cores (TCs) and CUDA Cores (CDs) in GPUs. These challenges are further exacerbated due to repeated, inefficient, and redundant aggregations in GNN that result from the high sparsity and irregular non-zero distribution of real-world graphs. We propose RT-GNN, a GNN framework based on the fusion of advanced TC and CD units, to eliminate the aforementioned redundancies by exploiting the properties of an adjacency matrix. First, a novel GNN representation technique, hierarchical embedding graph (HEG) is proposed to manage the intermediate aggregation results hierarchically, which can further avoid redundancy in intermediate aggregations elegantly. Next, to address the inherent sparsity of graphs, RT-GNN places the blocks (a.k.a. tiles) in HEG onto TCs and CDs according to their sparsity by a new block-based row-wise multiplication approach, which assembles TCs and CDs to work concurrently. Experimental results demonstrate that HEG outperforms HAG by an average speedup of 19.3× for redundancy elimination performance, especially up to 72× speedup on the dataset of ARXIV. Moreover, for overall performance, RT-GNN outperforms state-of-the-art GNN frameworks (including DGL, HAG, GNNAdvisor, and TC-GNN) by an average factor of 3.1× while maintaining or even improving the task accuracy.},
  archive      = {J_TACO},
  author       = {Jianrong Yan and Wenbin Jiang and Dongao He and Suyang Wen and Yang Li and Hai Jin and Zhiyuan Shao},
  doi          = {10.1145/3702001},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {RT-GNN: Accelerating sparse graph neural networks by tensor-CUDA kernel fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conflict management in vector register files. <em>TACO</em>, <em>22</em>(1), 1-19. (<a href='https://doi.org/10.1145/3702002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The instruction set architecture of vector processors operates on vectors stored in the vector register file which needs to handle several concurrent accesses by functional units with multiple ports. When the vector processor is running with high utilization, access conflicts become a major source of performance degradation. With a software model of a vector processor, we take a deep dive into the runtime impact of conflicts and their characteristics, and on ways to manage them, i.e., avoidance, resolution, and mitigation. For conflict avoidance, we study the existing approaches of banking with different static bank layouts and propose a dynamic bank layout to overcome their shortcomings. Our approach assigns newly written registers a temporarily unique starting bank. For conflict resolution, we compare different arbitration algorithms and optimize round-robin arbitration for mixed-width arithmetics by prioritizing wide operands. For conflict mitigation, operand queues of varying depths are studied. Our inventions are likely to increase the area efficiency of vector processors, either because they allow to use shallower operand queues while keeping the same performance, or reduce the area even further by using less banks, albeit at a performance impairment of 10% or less. The insights of the study can further be applied to other shared memory systems.},
  archive      = {J_TACO},
  author       = {Viktor Razilov and Ipek Gecin and Emil Matúš and Gerhard Fettweis},
  doi          = {10.1145/3702002},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Conflict management in vector register files},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPIRIT: Scalable and persistent in-memory indices for real-time search. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3703351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, real-time search over big microblogging data requires low indexing and query latency. Online services, therefore, prefer to host inverted indices in memory. Unfortunately, as datasets grow, indices grow proportionally, and with limited DRAM scaling, the main memory faces high pressure. Also, indices must be persisted on disks as building them is computationally intensive. Consequently, it becomes necessary to frequently move on-heap index segments to storage, slowing down indexing. Reading storage-resident index segments requires filesystem calls and disk accesses during query evaluation, leading to high and unpredictable tail latency. This work exploits hybrid DRAM and scalable non-volatile memory (NVM) to offer dynamically growing and instantly searchable (i.e., real-time) persistent indices in on-heap memory. We implement SPIRIT, a real-time text inversion engine over hybrid memory. SPIRIT exploits the byte-addressability of hybrid memory to enable direct access to the index on a pre-allocated heap, eliminating expensive block storage accesses and filesystem calls during live operation. It uses an in-memory segment descriptor table to offer: ➊ instant segment availability to query evaluators upon fresh ingestion, ➋ low-overhead segment movement across memory tiers transparent to query evaluators, and ➌ decoupled segment movement into NVM from their visibility to query evaluators, enabling different policies for mitigating NVM latency. SPIRIT accelerates compaction with zero-copy merging. It supports volatile, graceful shutdown, and crash-consistent indexing modes. The latter two modes offer instant recovery using persistent pointers. SPIRIT with hybrid memory and strong crash consistency guarantees exhibits many orders of magnitude better tail response times and query throughout than the state-of-the-art Lucene search engine. Compared against a highly optimized non-real-time evaluation of Lucene with liberal DRAM size, on average, across six query workloads, SPIRIT still delivers 2.5× better (real-time) query throughput. Our work applies to other services that will benefit from direct on-heap access to large persistent indices.},
  archive      = {J_TACO},
  author       = {Adnan Hasnat and Shoaib Akram},
  doi          = {10.1145/3703351},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SPIRIT: Scalable and persistent in-memory indices for real-time search},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ApSpGEMM: Accelerating large-scale SpGEMM with heterogeneous collaboration and adaptive panel. <em>TACO</em>, <em>22</em>(1), 1-23. (<a href='https://doi.org/10.1145/3703352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sparse General Matrix-Matrix multiplication (SpGEMM) is a fundamental component for many applications, such as algebraic multigrid methods (AMG), graphic processing, and deep learning. However, the unbearable latency of computing high-dimensional, large-scale sparse matrix multiplication on GPUs hinders the development of these applications. An effective approach is heterogeneous cores collaborative computing, but this method must address three aspects: (1) irregular non-zero elements lead to load imbalance and irregular memory access, (2) different core computing latency differences reduce computational parallelism, and (3) temporary data transfer between different cores introduces additional latency overhead. In this work, we propose an innovative framework for collaborative large-scale sparse matrix multiplication on CPU-GPU heterogeneous cores, named ApSpGEMM. ApSpGEMM is based on sparsity rules and proposes reordering and splitting algorithms to eliminate the impact of non-zero element distribution features on load and memory access. Then adaptive panels allocation with affinity constraints among cores improves computational parallelism. Finally, carefully arranged asynchronous data transmission and computation balance communication overhead. Compared with state-of-the-art SpGEMM methods, our approach provides excellent absolute performance on matrices with different sparse structures. On heterogeneous cores, the GFlops of large-scale sparse matrix multiplication is improved by 2.25 to 7.21 times.},
  archive      = {J_TACO},
  author       = {Dezhong Yao and Sifan Zhao and Tongtong Liu and Gang Wu and Hai Jin},
  doi          = {10.1145/3703352},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ApSpGEMM: Accelerating large-scale SpGEMM with heterogeneous collaboration and adaptive panel},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RaNAS: Resource-aware neural architecture search for edge computing. <em>TACO</em>, <em>22</em>(1), 1-18. (<a href='https://doi.org/10.1145/3703353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) for edge devices is often time-consuming because of long-latency deploying and testing on edge devices. The ability to accurately predict the computation cost and memory requirement for convolutional neural networks (CNNs) in advance holds substantial value. Existing work primarily relies on analytical models, which can result in high prediction errors. This article proposes a resource-aware NAS (RaNAS) model based on various features. Additionally, a new graph neural network is introduced to predict inference latency and maximum memory requirements for CNNs on edge devices. Experimental results show that, within the error bound of ±1%, RaNAS achieves an accuracy improvement of approximately 8% for inference latency prediction and about 25% for maximum memory occupancy prediction over the state-of-the-art approaches.},
  archive      = {J_TACO},
  author       = {Jianhua Gao and Zeming Liu and Yizhuo Wang and Weixing Ji},
  doi          = {10.1145/3703353},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {RaNAS: Resource-aware neural architecture search for edge computing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ShieldCXL: A practical obliviousness support with sealed CXL memory. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3703354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CXL (Compute Express Link) technology is an emerging memory interface with high-level commands. Recent studies applied the CXL memory expanding technique to mitigate the capacity limitation of the conventional DDRx memory. Unlike the prior studies to use the CXL memory as the capacity expander, this study proposes to use the CXL-based memory as a secure main memory device, while removing the conventional memory. In the conventional DDRx memory, to provide confidentiality, integrity, replay protection, and obliviousness, costly mechanisms such as counter-based integrity trees and location shuffling by ORAM (Oblivious RAM) are used. Such mechanisms incur significant performance degradation in the current DDR-based memory systems, and their costs increase as the capacity of the memory increases. To mitigate the performance degradation, the prior work proposed an obfuscated channel for a secure memory module enclosing its controller in the package. Based on the approach, we propose a secure CXL-only memory architecture called ShieldCXL . It uses the channel encryption and integrity protection mechanism of the CXL interface to provide a practical ORAM while supporting confidentiality, integrity, and replay protection from physical attacks and rowhammers. To protect the PCIe-connected memory expanding board, this study proposes to use the standard physical sealing technique to detect physical intrusion. To mitigate the increased latency with the sealed CXL memory module, the study further optimizes performance by adopting an in-package DRAM cache. In addition, this study investigates destination obfuscation when a CXL switch is used to route among multiple hosts and memory devices. The evaluation shows that ShieldCXL provides 9.16x performance improvements over the prior ORAM technique.},
  archive      = {J_TACO},
  author       = {Kwanghoon Choi and Igjae Kim and Sunho Lee and Jaehyuk Huh},
  doi          = {10.1145/3703354},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ShieldCXL: A practical obliviousness support with sealed CXL memory},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tiaozhuan: A general and efficient indirect branch optimization for binary translation. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3703355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary translation enables transparent execution, analysis, and modification of the binary program, serving as a core technology that facilitates instruction set emulation, cross-platform compatibility of software, and program instrumentation. Handling indirect branch instructions is widely recognized as a significant performance bottleneck in binary translation. While the target of a direct branch can be determined during the translation phase, an indirect branch requires a runtime lookup from the guest program counter to the host program counter, significantly influencing the performance of translator. Although several methods have been proposed to accelerate this process, each guest indirect branch instruction still translates into approximately 10 host instructions, resulting in considerable overhead. This article introduces Tiaozhuan, which addresses this issue by employing two optimization schemes. First, full address mapping uses a larger address space to store address mappings from guest to host, effectively reducing the number of instructions required to lookup the target of an indirect branch. Second, exceptionassisted branch elimination further eliminates branch instructions that check target correctness of targets in the lookup process. These two approaches enable indirect branches target lookup to be completed within one to two instructions, noticeably decreasing the overhead of indirect branches. Compared to state-of-the-art mechanisms, the SPEC CPU2006 benchmark suite showed a reduction in the number of instructions by an average of 4.2%, with the highest observed performance improvement reaching 19.4% and an average increase of 3.9%.},
  archive      = {J_TACO},
  author       = {Xinyu Li and Guangyao Guo and Yanzhi Lan and Feng Xue and Chenji Han and Gen Niu and Fuxin Zhang},
  doi          = {10.1145/3703355},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Tiaozhuan: A general and efficient indirect branch optimization for binary translation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing and understanding HGNN training on GPUs. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3703356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to their remarkable representation capabilities for heterogeneous graph data, Heterogeneous Graph Neural Networks (HGNNs) have been widely adopted in many critical real-world domains such as recommendation systems and medical analysis. Prior to their practical application, identifying the optimal HGNN model parameters tailored to specific tasks through extensive training is a time-consuming and costly process. To enhance the efficiency of HGNN training, it is essential to characterize and analyze the execution semantics and patterns within the training process to identify performance bottlenecks. In this study, we conduct a comprehensive quantification and in-depth analysis of two mainstream HGNN training scenarios, including single-GPU and multi-GPU distributed training. Based on the characterization results, we reveal the performance bottlenecks and their underlying causes in different HGNN training scenarios and propose optimization guidelines from both software and hardware perspectives.},
  archive      = {J_TACO},
  author       = {Dengke Han and Mingyu Yan and Xiaochun Ye and Dongrui Fan},
  doi          = {10.1145/3703356},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Characterizing and understanding HGNN training on GPUs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bubble-swap flow control. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3705316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deadlock-free adaptive routing is extensively adopted in both on-chip and off-chip interconnection networks to improve communication bandwidth and reduce latency. Introducing virtual channels (VCs), also known as virtual lanes (VLs). This is the mainstream technique to handle deadlocks incurred by adaptive routing and also provides VC preemption for higher priority traffic. However, existing deadlock-free flow control schemes either underutilize memory resources due to inefficient buffer management to simplify hardware implementation, or rely on complicated global coordination and synchronization with very high hardware complexity. Most hardware-friendly schemes use more VCs and memory resources to enable ease of implementation of deadlock-free flow control. In contrast, sophisticated schemes achieve deadlock freedom with minimum VC cost, even eliminating additional buffer requirement through the complicated control mechanisms. In this work, we rethink the root cause of the deadlock problem from a different perspective by considering it as a lack of credit, which makes us find an efficient solution to the deadlock problem. With minor modification of credit accumulation and return, our proposed bubble-swap flow control (BSFC) ensures atomic buffer swap between two adjacent routers only based on local credit status while making full use of the buffer space. BSFC achieves a better tradeoff between implementation complexity and memory overhead and can be easily integrated in the industrial router with no modification on buffer allocation or port arbitration. The simulation results demonstrate BSFC outperforms existing bubble-based deadlock-free methods by average 64% higher throughput. We further propose a credit reservation strategy to eliminate the escape virtual channel (VC) cost for fully adaptive routing implementation. The synthesizing results demonstrate that BSFC along with credit reservation (BSFC-CR) can reduce the area and power consumption by respectively 29% and 26% in contrast to the traditional critical bubble scheme (CBS).},
  archive      = {J_TACO},
  author       = {Yi Dai and Kai Lu and Sheng Ma and Jinshu Su and Dongsheng Li},
  doi          = {10.1145/3705316},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Bubble-swap flow control},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCNTrain+: A versatile and efficient accelerator for graph convolutional neural network training. <em>TACO</em>, <em>22</em>(1), 1-22. (<a href='https://doi.org/10.1145/3705317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph convolutional networks (GCNs) have gained wide attention due to their ability to capture node relationships in graphs. One problem appears when full-batch GCN is trained on large graph datasets, where the computational and memory requirements are unacceptable. To address this issue, mini-batch GCN training is introduced to improve the scalability of GCN training for large datasets by sampling and training only a subset of the graph in each batch. Although several acceleration techniques have been designed for boosting the efficiency of full-batch GCN, they lack attention to mini-batch GCN, which differs from full-batch GCN in terms of the sampled dynamic graph structures. Based on our previous work, GCNTrain [ 28 ], which was originally excogitated for accelerating full-batch GCN training, we devise GCNTrain+—a universal accelerator to tackle the performance bottlenecks associated with both full-batch and mini-batch GCN training. GCNTrain+ is equipped with two engines to optimize computation and memory access in GCN training, respectively. To reduce the computation overhead, we propose to dynamically reconfigure the computation order based on the varying data dimensions involved in each training batch. Moreover, we build a unified computation engine to perform the sparse-dense matrix multiplications and sparse-sparse matrix multiplications discovered in GCN training uniformly. To alleviate the memory burden, we devise a two-phased dynamic clustering mechanism to capture data locality as well as customized hardware to reduce the clustering overhead. We evaluate GCNTrain+ on seven datasets, and the result shows that GCNTrain+ achieves 136.0×, 52.6×, 2.2×, and 1.5× speedup over CPU, GPU, GCNAX, and GCNTrain in full-batch GCN training. Additionally, GCNTrain+ outperforms them with speedups of 131.6×, 67.1×, 4.4×, and 1.5× in mini-batch GCN training.},
  archive      = {J_TACO},
  author       = {Zhuoran Song and Jiabei Long and Li Jiang and Naifeng Jing and Xiaoyao Liang},
  doi          = {10.1145/3705317},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GCNTrain+: A versatile and efficient accelerator for graph convolutional neural network training},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ExZNS: Extending zoned namespace to support byte-loggable zones. <em>TACO</em>, <em>22</em>(1), 1-28. (<a href='https://doi.org/10.1145/3705318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging Zoned Namespace (ZNS) provides hosts with fine-grained, performance-predictable storage management. ZNS organizes the address space into zones composed of fixed-size, sequentially written, non-overwritable blocks, making it suitable for log-structured file systems. However, our experimental analysis reveals that ZNS’s write restrictions introduce notable persistence overhead. Firstly, out-of-place updates of data blocks require frequent small modifications to file metadata blocks, which are typically much smaller than a block, to record the latest logical block address. Secondly, some files, such as databases’ Write-Ahead Logging files, frequently execute synchronous small writes, with I/O sizes typically smaller than a logical block. The persistence of these file metadata and file data requires writing back the entire block even if it is only partially updated. This significantly increases the I/O latency and potentially reduces device lifespan. This article proposes exZNS , an innovative extension of ZNS, designed to provide both regular zones and byte-loggable zones . By exposing the persistent write buffer of the opened zones on the device to the application, the byte-loggable zone allows for appending at byte granularity through a new set of APIs. To reduce the persistence overhead described above, we built exBlzFS , a novel high-performance file system for exZNS. exBlzFS selectively records the partial updates of metadata blocks to the byte-loggable zone to ensure metadata persistence, and persists file data to the byte-loggable zone at byte granularity to absorb the frequent small writes. Evaluations show that exBlzFS increases the IOPS of RocksDB by 42.7% and 76.3%, and reduces the device’s write traffic by 86% and 94%, compared with BlzFS and F2FS, respectively. 1},
  archive      = {J_TACO},
  author       = {Wenjie Qi and Zhipeng Tan and Ziyue Zhang and Ying Yuan and Dan Feng},
  doi          = {10.1145/3705318},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ExZNS: Extending zoned namespace to support byte-loggable zones},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TPRepair: Tree-based pipelined repair in clustered storage systems. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3705895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure coding is an effective technique for guaranteeing data reliability for storage systems, yet it incurs a high repair penalty with amplified repair traffic. The repair becomes more intricate in clustered storage systems with the bandwidth diversity property. We present TPRepair , a T ree-based P ipelined Repair approach, aiming to expedite the overall repair process with the tailored pipelined repair procedure. TPRepair first prioritizes selecting racks with the current minimum load to participate in the repair process. It subsequently formulates tree-based links, tailored to align seamlessly with the pipelined repair procedure. TPRepair further designs an optimization algorithm to reduce the bottleneck load when repairing multiple chunks. Large-scale simulations demonstrate that TPRepair can increase 13.8%–41.3% of the balance ratio without amplifying cross-rack traffic. Meanwhile, Alibaba Cloud ECS experiments indicate that TPRepair can increase repair throughput by 11.3% to 72.9%.},
  archive      = {J_TACO},
  author       = {Jiahui Yang and Fulin Nan and Zhirong Shen and Zhisheng Chen and Yuhui Cai and Dmitrii Kaplun and Xiaoli Wang and Quanqing Xu and Chuanhui Yang and Jiwu Shu},
  doi          = {10.1145/3705895},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {TPRepair: Tree-based pipelined repair in clustered storage systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AIS: An active idleness I/O scheduler to reduce buffer-exhausted degradation of solid-state drives. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3708538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern solid-state drives (SSDs) continue to boost storage density and I/O bandwidth at the cost of flash-access I/O latency, especially for write, hence they prevalently deploy a build-in buffer to absorb incoming writes. However, when the buffer is used up, the applications suffer from a sudden and long performance decline, i.e., buffer-exhausted degradation (BED). To holistically understand BED and recovery, we design an automated testing toolset (SSDTest) to measure six commodity NVMe SSDs and find: (1) the occurrence of the BED strictly relies on the written-data amount, (2) BED dramatically increases I/O latency of SSDs, especially write and read-after-write, (3) BED can be conditionally reduced and recovered only after a period of idle time, and (4) a read without preceding writes is largely immune to BED, but prolongs the required idle time to recover the available buffer. Furthermore, we build a black-box SSD buffer-recovery model to quantitatively characterize the idleness-recovery behaviors and design an SSD BED predictor to make BED occurrence and buffer recovery predictable. Leveraging this model, we further design an Active Idleness I/O Scheduler (AIS) with small-sized auxiliary storage to actively regulate the I/O idle-intervals to maximize the internal buffer recovery of SSD. AIS adaptively steers incoming data to the auxiliary storage to (1) strategically keep SSD idle to reduce the occurrence of BED and (2) mitigate the tail latency of SSDs caused by read-after-writes during BED. We perform extensive evaluations under a variety of workloads. The results show that AIS improves average, 99th, 99.9th, and 99.99th-percentile latencies of SSDs by up to 29.3%, 37.3%, 78.7%, and 67.2% respectively, with up to 512MB auxiliary storage.},
  archive      = {J_TACO},
  author       = {Yekang Zhan and Xiangrui Yang and Haichuan Hu and Qiang Cao and Yifan Zhang and Jie Yao},
  doi          = {10.1145/3708538},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {AIS: An active idleness I/O scheduler to reduce buffer-exhausted degradation of solid-state drives},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consequence-based clustered architecture. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3708539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We recognize that the execution of many dynamic instructions has no consequence on the overall execution of a program. For example, the execution of a correctly predicted conditional branch instruction, as well as all the instructions leading up to it, is inconsequential. We propose a clustered architecture that steers consequential instructions to the primary cluster and inconsequential ones to the secondary cluster called the I-Pipe, which is less capable and thereby more area and power efficient. The proposed architecture also entails minimal inter-cluster communication, thereby greatly reducing the complexities of inter-cluster result buses. Such a steering policy helps increase the performance as the consequential instructions do not face any interference from the inconsequential ones. We demonstrate a 42% area reduction as compared to a baseline single cluster (Tigerlake-based) architecture, a 18.5% power reduction in the SPEC CPU2017 suite (13.7% power reduction in GAPBS), and a 5.15% performance uplift in the SPEC CPU2017 suite (10.22% in the GAPBS suite).},
  archive      = {J_TACO},
  author       = {Shruthi Karunakar and Rajshekar Kalayappan and Sandeep Chandran},
  doi          = {10.1145/3708539},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Consequence-based clustered architecture},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible and effective object tiering for heterogeneous memory systems. <em>TACO</em>, <em>22</em>(1), 1-24. (<a href='https://doi.org/10.1145/3708540'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing platforms that package multiple types of memory, each with their own performance characteristics, are quickly becoming mainstream. To operate efficiently, heterogeneous memory architectures require new data management solutions that are able to match the needs of each application with an appropriate type of memory. As the primary generators of memory usage, applications create a great deal of information that can be useful for guiding memory management, but the community still lacks tools to collect, organize, and leverage this information effectively. To address this gap, this work introduces a novel software framework that collects and analyzes object-level information to guide memory tiering. The framework includes tools to monitor the capacity and usage of individual data objects, routines that aggregate and convert this information into tier recommendations for the host platform, and mechanisms to enforce these recommendations according to user-selected policies. Moreover, the developed tools and techniques are fully automatic, work on standard Linux systems, and do not require modification or recompilation of existing software. Using this framework, this study evaluates and compares the impact of a variety of design choices for memory tiering, including different policies for prioritizing objects for the fast memory tier as well as the frequency and timing of migration events. The results, collected on a modern Intel platform with conventional DDR4 SDRAM as well as Intel Optane NVRAM, show that guiding data tiering with object-level information can enable significant performance and efficiency benefits compared with standard hardware- and software-directed data-tiering strategies for a diverse set of memory-intensive workloads.},
  archive      = {J_TACO},
  author       = {Brandon Kammerdiener and J. Zach Mcmichael and Michael Jantz and Kshitij Doshi and Terry Jones},
  doi          = {10.1145/3708540},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Flexible and effective object tiering for heterogeneous memory systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COVER: Alleviating crash-consistency error amplification in secure persistent memory systems. <em>TACO</em>, <em>22</em>(1), 1-27. (<a href='https://doi.org/10.1145/3708541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data security (including confidentiality, integrity, and availability) and crash consistency guarantees are essential for building trusted persistent memory (PM) systems. Security and consistency metadata are added to enable the guarantees. Recent studies show that errors in security metadata have the amplified effect, which significantly affects data availability. However, the impact of consistency metadata errors on data availability has rarely been discussed. We identify the crash-consistency error amplification (CCEA) problem, several errors in consistency metadata can make a large portion of data in PM possibly inconsistent. The error sensitivity of consistency metadata is higher than data and security metadata, thus requiring special attention. It is inefficient to address this problem by using the methods that are proposed to alleviate the amplified effect of security metadata errors, because security metadata are generally designed for a single purpose (e.g., integrity verification), while consistency metadata are designed for multiple purposes, including inconsistency locating and recovery. To effectively and efficiently alleviate the CCEA problem, we propose a c rash c o nsistency ver ification approach (COVER) that decouples inconsistency locating and recovery. COVER provides three design options that support different tradeoffs between effectiveness and efficiency. Experimental results show that COVER effectively alleviates the problem with only about 1.0% performance degradation on average compared with the state-of-the-art secure PM design.},
  archive      = {J_TACO},
  author       = {Xueliang Wei and Dan Feng and Wei Tong and Bing Wu and Xu Jiang},
  doi          = {10.1145/3708541},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {COVER: Alleviating crash-consistency error amplification in secure persistent memory systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MasterPlan: A reinforcement learning based scheduler for archive storage. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3708542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the sheer volume of data in today’s world, archive storage systems play a significant role in persisting the cold data. Due to stringent cost concerns, one popular design is to organize disks into groups and periodically switch them to be powered on for serving user requests. Scheduling thus becomes critical for both CapEx and performance. Unfortunately, field results indicate that existing schedulers can be often suboptimal. Our further analysis suggests that the main reason is the mismatch between the ever-changing workloads and the fixed set of coarsely-configured parameters in current heuristic-based schedulers. In this article, we propose MasterPlan , a reinforcement learning (RL) based scheduler for archive storage systems. By identifying the unique characteristics of archive storage service, we design a state space and reward function for the RL agent. MasterPlan includes a continuous action encoding approach to guarantee efficient exploration, and a meta adaptation module to extract features of workload series. Experiments show that MasterPlan can achieve 1.25× throughput, 2.16× 99 th latency and 1.47× power draw improvement compared to existing solutions.},
  archive      = {J_TACO},
  author       = {Xinqi Chen and Erci Xu and Dengyao Mo and Ruiming Lu and Haonan Wu and Dian Ding and Guangtao Xue},
  doi          = {10.1145/3708542},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {MasterPlan: A reinforcement learning based scheduler for archive storage},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Steered bubble: An interposer-based deadlock recovery algorithm for multi-chiplet systems. <em>TACO</em>, <em>22</em>(1), 1-24. (<a href='https://doi.org/10.1145/3708543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dividing a single System-on-Chip (SoC) into multiple chiplets and integrating them via an interposer can achieve an optimal balance between continuous transistor integration and monetary cost. However, potential deadlock may arise between the chiplets and the interposer. This deadlock can be avoided by applying turn restriction or injection control on the boundary routers, at the cost of additional latency and suboptimal performance. Compared to deadlock avoidance, deadlock recovery exerts less impact on network performance. Nevertheless, accurate and timely deadlock detection, along with efficient deadlock recovery, continues to pose significant challenges. Additionally, modularity is a specific concern, which involves integrating chiplets of various functions, sizes, manufacturing processes, and so on. Minimizing the negative impact of deadlock resolution while maximizing modularity is crucial for achieving the benefit of chiplets. This article proposes a modular deadlock detection strategy, Up-Down, which monitors both the upward and downward directions of vertical channels, facilitating information exchange through the congestion-sense network. When a pair of blocked upward and downward vertical channels is detected simultaneously, it is considered that an inter-chiplet deadlock has occurred. This significantly enhances the accuracy of deadlock detection by two orders of magnitude compared to time-out deadlock detection. Furthermore, this article introduces Steered Bubble, a low-cost deadlock recovery algorithm. It does so by injecting bubbles into potential deadlock cycles identified by Up-Down. These bubbles follow preset paths, ensuring efficient deadlock recovery. Experimental results indicate that the Steered Bubble results in an average performance enhancement of 1% to 10% during full-system simulations, with an area overhead of less than 2%.},
  archive      = {J_TACO},
  author       = {Zhiqiang Chen and Yongwen Wang and Hongwei Zhou and Jian Zhang},
  doi          = {10.1145/3708543},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Steered bubble: An interposer-based deadlock recovery algorithm for multi-chiplet systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IBing: An efficient interleaved bidirectional ring all-reduce algorithm for gradient synchronization. <em>TACO</em>, <em>22</em>(1), 1-23. (<a href='https://doi.org/10.1145/3711818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ring all-reduce is currently the most commonly used collective communication technique in the fields of data parallel and distributed computing. It consists of three phases: communication establishment, data transmission, and data processing at each step. However, this method may suffer from increased communication latency as the number of computation nodes increases, excessive communication steps and data processing procedures can lead to insufficient bandwidth utilization. To address this issue, this article proposes an Interleaved Bidirectional Ring (IBing) all-reduce method, which uses specially crafted communication operations to improve communication efficiency by reducing the effects of both communication establishment and data processing time. IBing reduces the number of communication steps by half compared to the Ring all-reduce. The results of extensive experiments indicate that the proposed IBing design can reduce total communication consumption by an average of 8.49% and up to 49.73%.},
  archive      = {J_TACO},
  author       = {Ruixing Zong and Jiapeng Zhang and Zhuo Tang and Kenli Li},
  doi          = {10.1145/3711818},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {IBing: An efficient interleaved bidirectional ring all-reduce algorithm for gradient synchronization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCom: Fine-grained compressors in graphics memory of mobile GPU. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3711819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, GPUs significantly boost rendering performance. However, the high memory requirements limit their use, especially on low-end mobile platforms. Compression techniques have been widely adopted to reduce memory consumption but face two primary issues when applied to mobile GPUs: (1) low repetition ratio caused by small raw data sizes and concurrency, and (2) low locality caused by unpredictable rendering behaviors. These two limitations result in a low compression ratio when compressors are applied to low-end mobile devices. This article introduces gCom , a fine-grained rendering compressor accelerated by GPUs. To improve the compression ratio, gCom incorporates the following innovations. First, unlike other compression techniques that use frames or tiles as basic processing units, gCom is the first to employ a fine-grained processing unit (i.e., the color channel), enhancing repetition amplification without increasing raw data. Second, gCom introduces two key features— Hierarchical Delta and Channel Decorrelator —which maximize the locality of adjacent channels and reduce raw data size. Third, to maintain the original GPU throughput, gCom revolutionizes the Golomb-Rice algorithm and proposes a new compression approach, the Parallel-Oriented Golomb-Rice algorithm, enabling parallel execution of both decompression and compression processes. The entire design of gCom utilizes only idle resources and existing commands on mobile GPUs, thus keeping purchasing costs low. To date, gCom has improved the channel locality by nearly 50%. The best compression achievement received by gCom has reached around 20%.},
  archive      = {J_TACO},
  author       = {Dongjie Tang and Zijun Wu and Yun Wang and Yicheng Gu and Fangxin Liu and Zhengwei Qi},
  doi          = {10.1145/3711819},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GCom: Fine-grained compressors in graphics memory of mobile GPU},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing high-throughput GPU random walks through multi-task concurrency orchestration. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3711820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random walk is a powerful tool for large-scale graph learning, but its high computational demand presents a challenge. While GPUs can accelerate random walk tasks, current frameworks fail to fully utilize GPU parallelism due to memory-to-compute bandwidth imbalance. In this article, CoWalker, an efficient GPU framework, is proposed to facilitate concurrent execution of random walks for high overall throughput. CoWalker features three novel designs. First, it incorporates a multi-level execution model that effectively orchestrates diverse walk tasks and reduces GPU stalls based on multiple graph characteristics. Second, it collaboratively manages graph data and streaming multiprocessors to minimize memory access interference and maximize core utilization under concurrent tasks. Finally, a multi-dimensional scheduler selects compatible random walk task combinations based on memory footprints to achieve maximum throughput. CoWalker significantly improves throughput over state-of-the-art baselines by mitigating concurrency overheads and effectively harnessing GPU parallelism. Our extensive evaluations on real-world workloads demonstrate that CoWalker achieves 2.75× higher overall system throughput compared with commercial tools and 1.56× over the SOTA academic system.},
  archive      = {J_TACO},
  author       = {Cheng Xu and Chao Li and Xiaofeng Hou and Junyi Mei and Jing Wang and Pengyu Wang and Shixuan Sun and Minyi Guo and Baoping Hao},
  doi          = {10.1145/3711820},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Enhancing high-throughput GPU random walks through multi-task concurrency orchestration},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LIA: Latency-improved adaptive routing for dragonfly networks. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3711914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-diameter network topologies require non-minimal routing, such as Valiant routing, to avoid network congestion under challenging traffic patterns like the so-called adversarial. However, this mechanism tends to increase the average path length, base latency, and network load. The use of shorter non-minimal paths has the potential to enhance performance, but it may also introduce congestion depending on the traffic patterns. This article introduces LIA (Latency-Improved Adaptive), a routing mechanism for Dragonfly networks which dynamically exploits minimal and non-minimal paths. LIA harnesses the traffic counters already present in contemporary switches to determine when it is safe to shorten non-minimal paths and to adjust routing decisions based on their information about the network conditions. Evaluations reveal that LIA achieves nearly optimal latency, outperforming state-of-the-art adaptive routing mechanisms by reducing latency by up to 30% while maintaining stable throughput and fairness.},
  archive      = {J_TACO},
  author       = {Mariano Benito and Enrique Vallejo and Ramón Beivide},
  doi          = {10.1145/3711914},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {LIA: Latency-improved adaptive routing for dragonfly networks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling and evaluating vulnerabilities in branch predictors via a three-step modeling methodology. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3711923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence and proliferation of microarchitectural attacks targeting branch predictors, the once-established security boundary in computer systems and architectures is facing unprecedented challenges. This article introduces an innovative branch predictor modeling methodology that abstractly characterizes 19 states and 53 operations of branch predictors, aiming to assist hardware designers in addressing overlooked security concerns during the microarchitecture design phase. Building upon this modeling discipline, we develop a symbolic execution-based framework to analyze and derive potential vulnerabilities in branch predictors. This framework finally yields 156 valid three-step attack patterns against branch predictors, including 89 novel variants not discovered in previous work. Subsequently, we extend the framework to automatically generate a benchmark suite for assessing the practical feasibility of derived attacks in real-world scenarios. Evaluation across five commercial Intel processors underscores the substantial threat posed by branch predictor attacks, with 130 of the 156 derived attacks proving viable on at least one processor. Finally, we theoretically model and evaluate 12 secure designs related to branch predictors. The evaluation results demonstrate that existing secure branch predictors can offer better security guarantees than secure speculation schemes, indicating that secure branch predictor designs are promising solutions to maintain the confidentiality and integrity of computer systems.},
  archive      = {J_TACO},
  author       = {Quancheng Wang and Ming Tang and Ke Xu and Han Wang},
  doi          = {10.1145/3711923},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Unveiling and evaluating vulnerabilities in branch predictors via a three-step modeling methodology},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KINDRED: Heterogeneous split-lock architecture for safe autonomous machines. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3711924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing practicality of autonomous vehicles and drones, the importance of reliability requirements has escalated substantially. In many instances, traditional system designs tend to overlook reliability issues, emphasizing primarily on performance constraints. However, certain designers may opt for a lock-step (redundant) system design, duplicating every component, which in turn can result in significant performance, energy, and cost overheads. In software for autonomous machines, such as self-driving vehicles, performance degradation can increase reaction time, posing safety risks and reducing mission success rates. This article introduces a novel multi-domain lock-step system design, Kindred , which places a strong emphasis on maximizing reliability while minimizing performance overhead. The proposed approach capitalizes on the inherent diversity in fault tolerance among various tasks within autonomous machine software, intelligently scheduling only the vulnerable nodes in the lock-domain. The primary challenge addressed in this study involves the intelligent task scheduling across different domains, complemented by efficient error detection and correction in the lock-domain. In a real system demonstration, we illustrate the effectiveness of Kindred , showcasing its ability to attain the same level of reliability as a full lock-step system while incurring only a mere 2.8% overhead, as opposed to a fully split system, indicating the advantages and potential of our multi-domain lock-step system design in achieving high reliability without compromising performance.},
  archive      = {J_TACO},
  author       = {Yiming Gan and Jingwen Leng and Bo Yu and Yuhao Zhu},
  doi          = {10.1145/3711924},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {KINDRED: Heterogeneous split-lock architecture for safe autonomous machines},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GHyPart: GPU-friendly end-to-end hypergraph partitioner. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3711925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraph partitioning finds practical applications in various fields, such as high-performance computing and circuit partitioning in VLSI physical design, where high-performance solutions often demand substantial parallelism beyond what existing CPU-based solutions can offer. While GPUs are promising in this regard, their potential in hypergraph partitioning remains unexplored. In this work, we first develop an end-to-end deterministic hypergraph partitioner on GPUs, ported from state-of-the-art multi-threaded CPU work, and identify three major performance challenges by characterizing its performance. We propose the first end-to-end solution, gHyPart , to unleash the potentials of hypergraph partitioning on GPUs. To overcome the challenges of GPU thread underutilization due to imbalanced workload, long critical path, and high work complexity due to excessive operations, we redesign GPU algorithms with diverse parallelization strategies thus expanding optimization space; to address the challenge of no one-size-fits-all implementation for various input hypergraphs, we propose a decision tree-based strategy to choose a suitable parallelization strategy for each kernel. Evaluation on 500 hypergraphs shows up to 125.7× (17.5× on average), 640.0× (24.2× on average), and 171.6× (1.4× on average) speedups over two CPU partitioners and our GPU baseline gHyPart-B , respectively.},
  archive      = {J_TACO},
  author       = {Zhenlin Wu and Haosong Zhao and Hongyuan Liu and Wujie Wen and Jiajia Li},
  doi          = {10.1145/3711925},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GHyPart: GPU-friendly end-to-end hypergraph partitioner},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximizing data and hardware reuse for HLS with early-stage symbolic partitioning. <em>TACO</em>, <em>22</em>(1), 1-26. (<a href='https://doi.org/10.1145/3711926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While traditional High-Level Synthesis (HLS) converts “high-level” C-like programs into hardware automatically, producing high-performance designs still requires hardware expertise. Optimizations such as data partitioning can have a large impact on performance since they directly affect data reuse patterns and the ability to reuse hardware. However, optimizing partitioning is a difficult process since minor changes in the parameter choices can lead to totally unpredictable performance. Functional array-based languages have been proposed instead of C-based approaches, as they offer stronger performance guarantees. This article proposes to follow a similar approach and exposes a divide-and-conquer primitive at the algorithmic level to let users partition any arbitrary computation. The compiler is then free to explore different partition shapes to maximize both data and hardware reuse automatically. The main challenge remains that the impact of partitioning is only known much later in the compilation flow. This is due to the hard-to-predict effects of the many optimizations applied during compilation. To solve this problem, the partitioning is expressed using a set of symbolic tunable parameters, introduced early in the compilation pipeline. A symbolic performance model is then used in the last compilation stage to predict performance based on the possible values of the tunable parameters. Using this approach, a design space exploration is conducted on an Intel Arria 10 Field Programmable Gate Arrays (FPGAs), and competitive performance is achieved on the classical VGG and TinyYolo neural networks.},
  archive      = {J_TACO},
  author       = {Tzung-Han Juang and Christophe Dubach},
  doi          = {10.1145/3711926},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Maximizing data and hardware reuse for HLS with early-stage symbolic partitioning},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taming flexible job packing in deep learning training clusters. <em>TACO</em>, <em>22</em>(1), 1-24. (<a href='https://doi.org/10.1145/3711927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Job packing is an effective technique to harvest the idle resources allocated to the deep learning (DL) training jobs but not fully utilized, especially when clusters may experience low utilization, and users may overestimate their resource needs. However, existing job packing techniques tend to be conservative due to the mismatch in scope and granularity between job packing and cluster scheduling. In particular, tapping the potential of job packing in the training cluster requires a local and fine-grained coordination mechanism. To this end, we propose a novel job-packing middleware named Gimbal , which operates between the cluster scheduler and the hardware resources. As middleware, Gimbal must not only facilitate coordination among the packed jobs but also support various scheduling objectives of different schedulers. Gimbal achieves dual functionality by introducing a set of worker calibration primitives designed to calibrate workers’ execution status in a fine-grained manner. The primitives obscure the complexity of the underlying job and resource management mechanisms, thus offering the generality and extensibility for crafting coordination policies tailored to various scheduling objectives. We implement Gimbal on a real-world GPU cluster and evaluate it with a set of representative DL training jobs. The results show that Gimbal improves different scheduling objectives up to 1.32× compared with the state-of-the-art job packing techniques.},
  archive      = {J_TACO},
  author       = {Pengyu Yang and Weihao Cui and Chunyu Xue and Han Zhao and Chen Chen and Quan Chen and Jing Yang and Minyi Guo},
  doi          = {10.1145/3711927},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Taming flexible job packing in deep learning training clusters},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScaWL: Scaling k-WL (Weisfeiler-lehman) algorithms in memory and performance on shared and distributed-memory systems. <em>TACO</em>, <em>22</em>(1), 1-25. (<a href='https://doi.org/10.1145/3715124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k -dimensional Weisfeiler-Lehman ( k -WL) algorithm—developed as an efficient heuristic for testing if two graphs are isomorphic—is a fundamental kernel for node embedding in the emerging field of graph neural networks. Unfortunately, the k -WL algorithm has exponential storage requirements, limiting the size of graphs that can be handled. This work presents a novel k -WL scheme with a storage requirement orders of magnitude lower while maintaining the same accuracy as the original k -WL algorithm. Due to the reduced storage requirement, our scheme allows for processing much bigger graphs than previously possible on a single compute node. For even bigger graphs, we provide the first distributed-memory implementation. Our k -WL scheme also has significantly reduced communication volume and offers high scalability. Our experimental results demonstrate that our approach is significantly faster and has superior scalability compared to five other implementations employing state-of-the-art techniques.},
  archive      = {J_TACO},
  author       = {Coby Soss and Aravind Sukumaran Rajam and Janet Layne and Edoardo Serra and Mahantesh Halappanavar and Assefaw H. Gebremedhin},
  doi          = {10.1145/3715124},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ScaWL: Scaling k-WL (Weisfeiler-lehman) algorithms in memory and performance on shared and distributed-memory systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating nearest neighbor search in 3D point cloud registration on GPUs. <em>TACO</em>, <em>22</em>(1), 1-24. (<a href='https://doi.org/10.1145/3716875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Iterative Closest Points (ICP) algorithm is the most widely used method for estimating rigid transformation in 3D point cloud registration. However, the ICP relies on repeatedly performing computationally intensive nearest neighbor searches (NNS) within 3D space. This dependency becomes a significant bottleneck when processing large datasets, thereby hindering the practical deployment of point cloud technologies in real-world applications. To address this issue, we propose two approximate nearest neighbor search (ANNS) acceleration strategies for efficient improvement of the processing speed of the NNS. Our strategies first voxelize target cloud points and then fill voxels in the 3D coordinate space around the source point cloud in two different ways, which can convert the global nearest neighbor search to a local search. Both the proposed methods are suited to be parallelized on GPUs with a low computational load. Extensive experiments show that our methods significantly accelerate NNS processing while maintaining high accuracy, outperforming most of the currently known approaches.},
  archive      = {J_TACO},
  author       = {Qiong Chang and Weimin Wang and Jun Miyazaki},
  doi          = {10.1145/3716875},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Accelerating nearest neighbor search in 3D point cloud registration on GPUs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
