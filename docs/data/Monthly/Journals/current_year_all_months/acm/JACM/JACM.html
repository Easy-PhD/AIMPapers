<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JACM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jacm">JACM - 37</h2>
<ul>
<li><details>
<summary>
(2025). The complexity of computing KKT solutions of quadratic programs. <em>JACM</em>, <em>72</em>(5), 1--48. (<a href='https://doi.org/10.1145/3745017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that solving a (non-convex) quadratic program is NP -hard. We show that the problem remains hard even if we are only looking for a Karush–Kuhn–Tucker (KKT) point, instead of a global optimum. Namely, we prove that computing a KKT point of a quadratic polynomial over the domain [0,1] n is complete for the class CLS = PPAD ∩ PLS .},
  archive      = {J_JACM},
  author       = {John Fearnley and Paul Goldberg and Alexandros Hollender and Rahul Savani},
  doi          = {10.1145/3745017},
  journal      = {Journal of the ACM},
  month        = {10},
  number       = {5},
  pages        = {1--48},
  shortjournal = {J. ACM},
  title        = {The complexity of computing KKT solutions of quadratic programs},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameterized inapproximability hypothesis under ETH. <em>JACM</em>, <em>72</em>(5), 1--40. (<a href='https://doi.org/10.1145/3749982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Parameterized Inapproximability Hypothesis (PIH) asserts that no fixed parameter tractable (FPT) algorithm can distinguish a satisfiable CSP instance, parameterized by the number of variables, from one where every assignment fails to satisfy an ɛ fraction of constraints for some absolute constant ɛ > 0. PIH plays the role of the PCP theorem in parameterized complexity. However, PIH has only been established under the Gap Exponential Time Hypothesis (ETH), a very strong assumption with an inherent gap. In this work, we prove PIH under the ETH. This is the first proof of PIH from a gap-free assumption. Our proof is self-contained and elementary. We identify an ETH-hard CSP whose variables take vector values, and constraints are either linear or of a special parallel structure. Both kinds of constraints can be checked with constant soundness via a “parallel PCP of proximity” based on the Walsh-Hadamard code.},
  archive      = {J_JACM},
  author       = {Venkatesan Guruswami and Bingkai Lin and Xuandi Ren and Yican Sun and Kewen Wu},
  doi          = {10.1145/3749982},
  journal      = {Journal of the ACM},
  month        = {10},
  number       = {5},
  pages        = {1--40},
  shortjournal = {J. ACM},
  title        = {Parameterized inapproximability hypothesis under ETH},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal multi-distribution learning. <em>JACM</em>, <em>72</em>(5), 1--71. (<a href='https://doi.org/10.1145/3760256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across k distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, and so on. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik–Chervonenkis (VC) dimension d , we propose a novel algorithm that yields an ɛ-optimal randomized hypothesis with a sample complexity on the order of \(\frac{d+k}{\varepsilon ^2}\) (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory are further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely through an empirical risk minimization oracle. Additionally, we establish the necessity of improper learning, revealing a large sample size barrier when only deterministic, proper hypotheses are permitted. These findings resolve three open problems presented in COLT 2023 (i.e., Awasthi et al. [ 4 , Problems 1, 3, and 4]).},
  archive      = {J_JACM},
  author       = {Zihan Zhang and Wenhao Zhan and Yuxin Chen and Simon S. Du and Jason Lee},
  doi          = {10.1145/3760256},
  journal      = {Journal of the ACM},
  month        = {10},
  number       = {5},
  pages        = {1--71},
  shortjournal = {J. ACM},
  title        = {Optimal multi-distribution learning},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coverability in VASS revisited: Improving rackoff’s bounds to obtain conditional optimality. <em>JACM</em>, <em>72</em>(5), 1--27. (<a href='https://doi.org/10.1145/3762178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seminal results establish that the coverability problem for Vector Addition Systems with States (VASS) is in EXPSPACE (Rackoff, ’78) and is EXPSPACE -hard already under unary encodings (Lipton, ’76). More precisely, Rosier and Yen later utilise Rackoff’s bounding technique to show that if coverability holds then there is a run of length at most \(n^{2^{\mathcal {O}(d \log (d))}}\) , where d is the dimension and n is the size of the given unary VASS. Earlier, Lipton showed that there exist instances of coverability in d -dimensional unary VASS that are only witnessed by runs of length at least \(n^{2^{\Omega (d)}}\) . Our first result closes this gap. We improve the upper bound by removing the twice-exponentiated \(\log (d)\) factor, thus matching Lipton’s lower bound. This closes the corresponding gap for the exact space required to decide coverability. This also yields a deterministic \(n^{2^{\mathcal {O}(d)}}\) -time algorithm for coverability. Our second result is a matching lower bound, that there does not exist a deterministic \(n^{2^{o(d)}}\) -time algorithm, conditioned upon the exponential time hypothesis. When analysing coverability, a standard proof technique is to consider VASS with bounded counters. Bounded VASS make for an interesting and popular model due to strong connections with timed automata. Withal, we study a natural setting where the counter bound is linear in the size of the VASS. Here the trivial exhaustive search algorithm runs in \(\mathcal {O}(n^{d+1})\) time. We give evidence to this being near-optimal. We prove that in dimension one this trivial algorithm is conditionally optimal, by showing that \(n^{2-o(1)}\) time is required under the k -cycle hypothesis. In general, for any fixed dimension d ≥ 4, we show that \(n^{d-2-o(1)}\) time is required under the 3-uniform hyperclique hypothesis.},
  archive      = {J_JACM},
  author       = {Marvin Künnemann and Filip Mazowiecki and Lia Schütze and Henry Sinclair-Banks and Karol Węgrzycki},
  doi          = {10.1145/3762178},
  journal      = {Journal of the ACM},
  month        = {10},
  number       = {5},
  pages        = {1--27},
  shortjournal = {J. ACM},
  title        = {Coverability in VASS revisited: Improving rackoff’s bounds to obtain conditional optimality},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The kikuchi hierarchy and tensor PCA. <em>JACM</em>, <em>72</em>(5), 1--40. (<a href='https://doi.org/10.1145/3762806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the tensor principal component analysis (tensor PCA) problem, we propose a new hierarchy of increasingly powerful algorithms with increasing runtime. Our hierarchy is analogous to the sum-of-squares (SOS) hierarchy but is instead inspired by statistical physics and related algorithms such as belief propagation and AMP (approximate message passing). Our level-ℓ algorithm can be thought of as a linearized message-passing algorithm that keeps track of ℓ-wise dependencies among the hidden variables. Specifically, our algorithms are spectral methods based on the Kikuchi Hessian , which generalizes the well-studied Bethe Hessian to the higher-order Kikuchi free energies. It is known that AMP, the flagship algorithm of statistical physics, has substantially worse performance than SOS for tensor PCA. In this work, we ‘redeem’ the statistical physics approach by showing that our hierarchy gives a polynomial-time algorithm matching the performance of SOS. Our hierarchy also yields a continuum of subexponential-time algorithms, and we prove that these achieve the same (conjecturally optimal) tradeoff between runtime and statistical power as SOS. Our proofs are much simpler than prior work, and also apply to the related problem of refuting random k -XOR formulas. The results we present here apply to tensor PCA for tensors of all orders, and to k -XOR when k is even. Our methods suggest a new avenue for systematically obtaining optimal algorithms for Bayesian inference problems, and our results constitute a step toward unifying the statistical physics and sum-of-squares approaches to algorithm design.},
  archive      = {J_JACM},
  author       = {Alexander Wein and Ahmed El Alaoui and Cristopher Moore},
  doi          = {10.1145/3762806},
  journal      = {Journal of the ACM},
  month        = {10},
  number       = {5},
  pages        = {1--40},
  shortjournal = {J. ACM},
  title        = {The kikuchi hierarchy and tensor PCA},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Envy-free cake-cutting for four agents. <em>JACM</em>, <em>72</em>(5), 1--54. (<a href='https://doi.org/10.1145/3765615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the envy-free cake-cutting problem, we are given a resource, usually called a cake and represented as the [0,1] interval, and a set of n agents with heterogeneous preferences over pieces of the cake. The goal is to divide the cake among the n agents such that no agent is envious of any other agent. Even under a very general preferences model, this fundamental fair division problem is known to always admit an exact solution where each agent obtains a connected piece of the cake; we study the complexity of finding an approximate solution, i.e., a connected ɛ-envy-free allocation. For monotone valuations of cake pieces, Deng, Qi, and Saberi (2012) gave an efficient (poly(log (1/ɛ)) queries) algorithm for three agents and posed the open problem of four (or more) monotone agents. Even for the special case of additive valuations, Brânzei and Nisan (2022) conjectured an Ω (1/ɛ) lower bound on the number of queries for four agents. We provide the first efficient algorithm for finding a connected ɛ-envy-free allocation with four monotone agents. We also prove that as soon as valuations are allowed to be non-monotone , the problem becomes hard: it becomes PPAD -hard, requires poly(1/ɛ) queries in the black-box model, and even poly(1/ɛ) communication complexity . This constitutes, to the best of our knowledge, the first intractability result for any version of the cake-cutting problem in the communication complexity model.},
  archive      = {J_JACM},
  author       = {Alexandros Hollender and Aviad Rubinstein},
  doi          = {10.1145/3765615},
  journal      = {Journal of the ACM},
  month        = {10},
  number       = {5},
  pages        = {1--54},
  shortjournal = {J. ACM},
  title        = {Envy-free cake-cutting for four agents},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian design principles for frequentist sequential learning. <em>JACM</em>, <em>72</em>(5), 1--65. (<a href='https://doi.org/10.1145/3766898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a general theory to optimize the frequentist regret for sequential learning problems, from which efficient bandit and reinforcement learning algorithms can be derived via unified Bayesian principles. Building on the recent Decision-Estimation Coefficient (DEC) framework, we propose a novel optimization approach to generate “algorithmic beliefs” at each round and use Bayesian posteriors for decision-making. The optimization objective, termed “Algorithmic Information Ratio” (AIR), represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. Although AIR’s minimax regret aligns with that provided by DEC, it additionally offers an algorithm-dependent perspective–distinct from a minimax complexity–facilitating algorithm design and analysis. Specifically, AIR enables deriving explicit algorithms via belief parameterization and provides clear approximation guidelines with provable guarantees. Moreover, the resulting algorithms have a simple structure and are computationally efficient for several representative problems. We illustrate our framework with a novel algorithm for multi-armed bandits that performs strongly across stochastic, adversarial, and non-stationary environments, and demonstrate applicability to linear bandits, convex bandits, and reinforcement learning.},
  archive      = {J_JACM},
  author       = {Yunbei Xu and Assaf Zeevi},
  doi          = {10.1145/3766898},
  journal      = {Journal of the ACM},
  month        = {10},
  number       = {5},
  pages        = {1--65},
  shortjournal = {J. ACM},
  title        = {Bayesian design principles for frequentist sequential learning},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expected constant round byzantine broadcast under dishonest majority. <em>JACM</em>, <em>72</em>(4), 1--39. (<a href='https://doi.org/10.1145/3748254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Byzantine Broadcast (BB) is a central question in distributed systems, and an important challenge is to understand its round complexity. Under the honest majority setting, it is long known that there exist randomized protocols that can achieve BB in expected constant rounds, regardless of the number of nodes n . However, whether we can match the expected constant round complexity in the corrupt majority setting —or more precisely, when \(f \ge n/2 + \omega (1)\) —remains unknown, where f denotes the number of corrupt nodes. In this article, we are the first to resolve this long-standing question. We show how to achieve BB in expected \(O((n/(n-f))^2)\) rounds. Our results hold under a weakly adaptive adversary who cannot perform “after-the-fact removal” of messages already sent by a node before it becomes corrupt. We also assume trusted setup and the Decision Linear (DLIN) assumption in bilinear groups.},
  archive      = {J_JACM},
  author       = {Jun Wan and Hanshen Xiao and Elaine Shi and Srinivas Devadas},
  doi          = {10.1145/3748254},
  journal      = {Journal of the ACM},
  month        = {8},
  number       = {4},
  pages        = {1--39},
  shortjournal = {J. ACM},
  title        = {Expected constant round byzantine broadcast under dishonest majority},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Superpolynomial lower bounds against low-depth algebraic circuits. <em>JACM</em>, <em>72</em>(4), 1--35. (<a href='https://doi.org/10.1145/3734215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An Algebraic Circuit for a polynomial \(P\in \mathbb {F}[x_1,\ldots ,x_N]\) is a computational model for constructing the polynomial P using only additions and multiplications. It is a syntactic model of computation, as opposed to the Boolean Circuit model, and hence lower bounds for this model are widely expected to be easier to prove than lower bounds for Boolean circuits. Despite this, we do not have superpolynomial lower bounds against general algebraic circuits of depth 3 (except over constant-sized finite fields) and depth 4 (over fields other than \(\mathbb {F}_2\) ), while constant-depth Boolean circuit lower bounds have been known since the early 1980s. In this article, we prove the first superpolynomial lower bounds against general algebraic circuits of all constant depths over all fields of characteristic 0 (or large). We also prove the first lower bounds against homogeneous algebraic circuits of constant depth over any field. Our approach is surprisingly simple. We first prove superpolynomial lower bounds for constant-depth Set-Multilinear circuits. While strong lower bounds were already known against such circuits, most previous lower bounds were of the form f(d) ⋅ poly( N ), where d denotes the degree of the polynomial. In analogy with Parameterized complexity, we call this an FPT lower bound. We extend a well-known technique of Nisan and Wigderson (FOCS 1995) to prove non-FPT lower bounds against constant-depth set-multilinear circuits computing the Iterated Matrix Multiplication polynomial IMM n,d (which computes a fixed entry of the product of d n × n matrices). More precisely, we prove that any set-multilinear circuit of depth Δ computing IMM n,d must have size at least \(n^{d^{\exp (-O(\Delta))}}\) . This result holds over any field, as long as d = o (log n ). We then show how to convert any constant-depth algebraic circuit of size s to a constant-depth set-multilinear circuit with a blow-up in size that is exponential in d but only polynomial in s over fields of characteristic 0. (For depths greater than 3, previous results of this form increased the depth of the resulting circuit to Ω (log s ).) This implies our constant-depth circuit lower bounds. We can also use these lower bounds to prove a Depth Hierarchy theorem for constant-depth circuits. We show that for every depth Γ , there is an explicit polynomial which can be computed by a depth Γ circuit of size s , but requires circuits of size s ω (1) if the depth is Γ -1. Finally, we observe that our superpolynomial lower bound for constant-depth circuits implies the first deterministic sub-exponential time algorithm for solving the Polynomial Identity Testing (PIT) problem for all small depth circuits using the known connection between algebraic hardness and randomness.},
  archive      = {J_JACM},
  author       = {Nutan Limaye and Srikanth Srinivasan and Sébastien Tavenas},
  doi          = {10.1145/3734215},
  journal      = {Journal of the ACM},
  month        = {7},
  number       = {4},
  pages        = {1--35},
  shortjournal = {J. ACM},
  title        = {Superpolynomial lower bounds against low-depth algebraic circuits},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Negative-weight single-source shortest paths in near-linear time. <em>JACM</em>, <em>72</em>(4), 1--34. (<a href='https://doi.org/10.1145/3742890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a randomized algorithm that computes single-source shortest paths (SSSP) in O ( m log 8 ( n ) log W ) time when edge weights are integral and can be negative. 1 This essentially resolves the classic negative-weight SSSP problem. The previous bounds are \(\tilde{O}((m+n^{1.5})\log W)\) [BLNPSSSW FOCS’20] and m 4/3+ o (1) log W [AMV FOCS’20]. Near-linear time algorithms were known previously only for the special case of planar directed graphs [Fakcharoenphol and Rao FOCS’01]. In contrast to all recent developments that rely on sophisticated continuous optimization methods and dynamic algorithms, our algorithm is simple: it requires only a simple graph decomposition and elementary combinatorial tools. In fact, ours is the first combinatorial algorithm for negative-weight SSSP to break through the classic \(\tilde{O}(m\sqrt {n}\log W)\) bound from over three decades ago [Gabow and Tarjan SICOMP’89].},
  archive      = {J_JACM},
  author       = {Aaron Bernstein and Danupon Nanongkai and Christian Wulff-Nilsen},
  doi          = {10.1145/3742890},
  journal      = {Journal of the ACM},
  month        = {7},
  number       = {4},
  pages        = {1--34},
  shortjournal = {J. ACM},
  title        = {Negative-weight single-source shortest paths in near-linear time},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vertex connectivity in poly-logarithmic max-flows. <em>JACM</em>, <em>72</em>(4), 1--34. (<a href='https://doi.org/10.1145/3743670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vertex connectivity of an m -edge n -vertex undirected graph is the smallest number of vertices whose removal disconnects the graph or leaves only a singleton vertex. In 1974, Aho Hopcroft and Ullman asked if vertex connectivity can be computed in linear time. Despite the substantial effort in the past five decades, the best-known running time is \(\tilde{O}(mn)\) by Henzinger-Rao-Gabow (FOCS 1996). Indeed, no algorithm with an o ( mn ) running time is known even if we assume a linear-time max-flow algorithm. In this article, we give an affirmative answer to this long-standing open problem (up to a sub-polynomial factor). We present a randomized reduction from the vertex connectivity problem to the max-flow problem which incurs only a poly-logarithmic overhead in runtime. Using this reduction, we can solve vertex connectivity in almost linear time by using the celebrated almost-linear-time max-flow algorithms by Chen-Kyng-Liu-Peng-Probst Gutenberg-Sachdeva (FOCS 2022) and Brand-Chen-Kyng-Liu-Peng-Probst Gutenberg-Sachdeva-Sidford (FOCS 2023). Using our new techniques, we also obtain an algorithm for directed vertex connectivity with a running time of n 2 + o (1) time which improves the best-known bound of \(\tilde{O}(mn)\) by Henzinger-Rao-Gabow (FOCS 1996).},
  archive      = {J_JACM},
  author       = {Jason Li and Danupon Nanongkai and Debmalya Panigrahi and Thatchaphol Saranurak and Sorrachai Yingchareonthawornchai},
  doi          = {10.1145/3743670},
  journal      = {Journal of the ACM},
  month        = {7},
  number       = {4},
  pages        = {1--34},
  shortjournal = {J. ACM},
  title        = {Vertex connectivity in poly-logarithmic max-flows},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive and fair transformation for recoverable mutual exclusion. <em>JACM</em>, <em>72</em>(4), 1--61. (<a href='https://doi.org/10.1145/3744236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutual exclusion is one of the most commonly used techniques to handle contention in concurrent systems. Traditionally, mutual exclusion algorithms have been designed under the assumption that a process does not fail while acquiring/releasing a lock or while executing its critical section. However, failures do occur in real life, potentially leaving the lock in an inconsistent state. This gives rise to the problem of recoverable mutual exclusion (RME) , which involves designing a mutual exclusion (ME) algorithm that can tolerate failures while maintaining required safety and liveness properties. In this work, we present a framework that transforms any algorithm that solves the RME problem into an algorithm that can also simultaneously adapt to (i) the number of processes concurrently competing for the lock, as well as (ii) the number of unresolved failures in the system, while maintaining the correctness properties and performance characteristics of the underlying RME algorithm. Additionally, the algorithm constructed as a result of this transformation adds certain desirable properties such as bounded recovery and fairness. One of the important performance measures of any ME algorithm, including an RME algorithm, is the number of remote memory references (RMRs) made by a process—to acquire and release a lock, as well as to recover the lock structure after a failure. Let R(n) denote the RMR complexity of a critical section request in the underlying RME algorithm, where n denotes the number of processes in the system. Then, our framework yields an RME algorithm for which the RMR complexity of a critical section request is given by \(\mathcal {O}(\min \lbrace \ddot{c},\sqrt {F+1},\ R(n) \rbrace)\) , where \(\ddot{c}\) denotes the point contention of the request and F denotes the failure-density of the request. We further extend our framework by presenting a novel memory reclamation algorithm to bound the space complexity of the RME algorithm. Our memory reclamation algorithm maintains the correctness, performance, and fairness properties of our transformation. Our approach is general enough that it may also be used to bound the space complexity of other RME algorithms. In addition to read and write instructions, our algorithm uses compare-and-swap ( CAS ) and fetch-and-store ( FAS ) hardware instructions, both of which are commonly available in most modern processors.},
  archive      = {J_JACM},
  author       = {Sahil Dhoked and Neeraj Mittal},
  doi          = {10.1145/3744236},
  journal      = {Journal of the ACM},
  month        = {7},
  number       = {4},
  pages        = {1--61},
  shortjournal = {J. ACM},
  title        = {Adaptive and fair transformation for recoverable mutual exclusion},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nested dissection meets IPMs: Planar min-cost flow in nearly-linear time. <em>JACM</em>, <em>72</em>(4), 1--75. (<a href='https://doi.org/10.1145/3744639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a nearly-linear time algorithm for finding a minimum-cost flow in planar graphs with polynomially-bounded integer costs and capacities. The previous fastest algorithm for this problem is based on interior point methods (IPMs) and works for general sparse graphs in O ( n 1.5 ⋅ poly (log n )) time [Daitch-Spielman, STOC’08]. Intuitively, Ω ( n 1.5 ) is a natural runtime barrier for IPM-based methods, since they require \(\sqrt {n}\) iterations, each routing a possibly-dense electrical flow. To break this barrier, we develop a new implicit representation for flows based on generalized nested dissection [Lipton-Rose-Tarjan, SINUM’79] and approximate Schur complements [Kyng-Sachdeva, FOCS’16]. This implicit representation permits us to design a data structure to route an electrical flow with sparse demands in roughly \(\sqrt {n}\) update time, resulting in a total runtime of O ( n ⋅ poly (log n )). Our results immediately extend to all families of separable graphs.},
  archive      = {J_JACM},
  author       = {Sally Dong and Yu Gao and Gramoz Goranci and Yin Tat Lee and Sushant Sachdeva and Richard Peng and Guanghao Ye},
  doi          = {10.1145/3744639},
  journal      = {Journal of the ACM},
  month        = {7},
  number       = {4},
  pages        = {1--75},
  shortjournal = {J. ACM},
  title        = {Nested dissection meets IPMs: Planar min-cost flow in nearly-linear time},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deterministic minimum cut in poly-logarithmic maximum flows. <em>JACM</em>, <em>72</em>(4), 1--18. (<a href='https://doi.org/10.1145/3744737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a deterministic algorithm for finding the minimum (weight) cut of an undirected graph on n vertices and m edges using polylog ( n ) calls to a black box maximum flow subroutine. Using the current best deterministic maximum flow algorithms, this marks the first improvement for this problem since a running time bound of \(\tilde{O}(mn)\) was established by several papers in the early 1990s. Our global minimum cut algorithm is obtained as a corollary of a deterministic minimum Steiner cut algorithm, where a minimum Steiner cut is a minimum (weight) set of edges whose removal disconnects at least one pair of vertices among a designated set of terminal vertices. We also give a remarkably simple randomized minimum Steiner cut algorithm that still improves the best known randomized algorithm for the problem. Our main technical contribution is a new tool that we call isolating cuts . Given a set of vertices R , this entails finding cuts of minimum weight that separate (or isolate) each individual vertex \(v\in R\) from the rest of the vertices \(R\setminus \lbrace v\rbrace\) . Naïvely, this can be done using |R| maximum flow calls, but we show that just O (log |R| ) calls on graphs containing O(n) vertices and O(m) edges suffice. We call this the isolating cut lemma .},
  archive      = {J_JACM},
  author       = {Jason Li and Debmalya Panigrahi},
  doi          = {10.1145/3744737},
  journal      = {Journal of the ACM},
  month        = {7},
  number       = {4},
  pages        = {1--18},
  shortjournal = {J. ACM},
  title        = {Deterministic minimum cut in poly-logarithmic maximum flows},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing a fixed point of contraction maps in polynomial queries. <em>JACM</em>, <em>72</em>(4), 1--20. (<a href='https://doi.org/10.1145/3744738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give an algorithm for finding an ε-fixed point of a contraction map f : [0, 1] k \(\mapsto\) [0, 1] k under the \(\ell _\infty\) -norm with query complexity O ( k log (1/ε).},
  archive      = {J_JACM},
  author       = {Xi Chen and Yuhao Li and Mihalis Yannakakis},
  doi          = {10.1145/3744738},
  journal      = {Journal of the ACM},
  month        = {7},
  number       = {4},
  pages        = {1--20},
  shortjournal = {J. ACM},
  title        = {Computing a fixed point of contraction maps in polynomial queries},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compressing CFI graphs and lower bounds for the weisfeiler-leman refinements. <em>JACM</em>, <em>72</em>(3), 1--27. (<a href='https://doi.org/10.1145/3727978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k -dimensional Weisfeiler–Leman ( k -WL) algorithm is a simple combinatorial algorithm that was originally designed as a graph isomorphism heuristic. It naturally finds applications in Babai’s quasipolynomial-time isomorphism algorithm, practical isomorphism solvers, and algebraic graph theory. However, it also has surprising connections to other areas such as logic, proof complexity, combinatorial optimization, and machine learning. The algorithm iteratively computes a coloring of the k -tuples of vertices of a graph. Since Fürer’s linear lower bound [ICALP 2001], it has been an open question whether there is a super-linear lower bound for the iteration number for k -WL on graphs. We answer this question affirmatively, establishing an \(\Omega (n^{k/2})\) -lower bound for all k .},
  archive      = {J_JACM},
  author       = {Martin Grohe and Moritz Lichter and Daniel Neuen and Pascal Schweitzer},
  doi          = {10.1145/3727978},
  journal      = {Journal of the ACM},
  month        = {6},
  number       = {3},
  pages        = {1--27},
  shortjournal = {J. ACM},
  title        = {Compressing CFI graphs and lower bounds for the weisfeiler-leman refinements},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Settling the sample complexity of online reinforcement learning. <em>JACM</em>, <em>72</em>(3), 1--63. (<a href='https://doi.org/10.1145/3733592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a “large-sample” regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory. We settle this problem for finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of MVP (Monotonic Value Propagation), an optimistic model-based algorithm proposed by Zhang et al. [ 82 ], achieves a regret on the order of (modulo log factors) \begin{equation*} \min \bigl \lbrace \sqrt {SAH^3K}, \,HK \bigr \rbrace , \end{equation*} where S is the number of states, A is the number of actions, H is the horizon length, and K is the total number of episodes. This regret matches the minimax lower bound for the entire range of sample size \(K\ge 1\) , essentially eliminating any burn-in requirement. It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield \(\varepsilon\) -accuracy) of \(\frac{SAH^3}{\varepsilon ^2}\) up to log factor, which is minimax-optimal for the full \(\varepsilon\) -range. Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances. The key technical innovation lies in a novel analysis paradigm (based on a new concept called “profiles”) to decouple complicated statistical dependency across the sample trajectories — a long-standing challenge facing the analysis of online RL in the sample-starved regime.},
  archive      = {J_JACM},
  author       = {Zihan Zhang and Yuxin Chen and Jason Lee and Simon S. Du},
  doi          = {10.1145/3733592},
  journal      = {Journal of the ACM},
  month        = {6},
  number       = {3},
  pages        = {1--63},
  shortjournal = {J. ACM},
  title        = {Settling the sample complexity of online reinforcement learning},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum weakest preconditions for reasoning about expected runtimes of quantum programs. <em>JACM</em>, <em>72</em>(3), 1--51. (<a href='https://doi.org/10.1145/3734516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study expected runtimes for quantum programs. Inspired by recent work on probabilistic programs, we first define expected runtime as a generalisation of quantum weakest precondition . Then, we show that the expected runtime of a quantum program should be represented as the expectation of an observable (in physics). A method for computing the expected runtimes of quantum programs in finite-dimensional state spaces is developed. Several examples are provided as applications of this method, including computing the expected runtime of quantum Bernoulli Factory —- a quantum algorithm for generating random numbers. In particular, using our new method, an open problem of computing the expected runtime of quantum random walks introduced by Ambainis et al. ( STOC 2001) is solved.},
  archive      = {J_JACM},
  author       = {Junyi Liu and Li Zhou and Gilles Barthe and Mingsheng Ying},
  doi          = {10.1145/3734516},
  journal      = {Journal of the ACM},
  month        = {6},
  number       = {3},
  pages        = {1--51},
  shortjournal = {J. ACM},
  title        = {Quantum weakest preconditions for reasoning about expected runtimes of quantum programs},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistency of relations over monoids. <em>JACM</em>, <em>72</em>(3), 1--47. (<a href='https://doi.org/10.1145/3721855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interplay between local consistency and global consistency has been the object of study in several different areas, including probability theory, relational databases, and quantum information. For relational databases, Beeri, Fagin, Maier, and Yannakakis showed that a database schema is acyclic if and only if it has the local-to-global consistency property for relations, which means that every collection of pairwise consistent relations over the schema is globally consistent. More recently, the same result has been shown under bag semantics. In this article, we carry out a systematic study of local versus global consistency for relations over positive commutative monoids, which is a common generalization of ordinary relations and bags. Let \(\mathbb {K}\) be an arbitrary positive commutative monoid. We begin by showing that acyclicity of the schema is a necessary condition for the local-to-global consistency property for \(\mathbb {K}\) -relations to hold. Unlike the case of ordinary relations and bags, however, we show that acyclicity is not always sufficient. Then, we characterize the positive commutative monoids for which acyclicity is both necessary and sufficient for the local-to-global consistency property to hold. This characterization involves a combinatorial property of monoids, which we call the transportation property . We then identify several different classes of monoids that possess the transportation property. As our final contribution, we introduce a modified notion of local consistency of \(\mathbb {K}\) -relations, which we call pairwise consistency up to the free cover . We prove that, for all positive commutative monoids \(\mathbb {K}\) , even those without the transportation property, acyclicity is both necessary and sufficient for every family of \(\mathbb {K}\) -relations that is pairwise consistent up to the free cover to be globally consistent.},
  archive      = {J_JACM},
  author       = {Albert Atserias and Phokion G. Kolaitis},
  doi          = {10.1145/3721855},
  journal      = {Journal of the ACM},
  month        = {5},
  number       = {3},
  pages        = {1--47},
  shortjournal = {J. ACM},
  title        = {Consistency of relations over monoids},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum flow and minimum-cost flow in almost-linear time. <em>JACM</em>, <em>72</em>(3), 1--103. (<a href='https://doi.org/10.1145/3728631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithm that computes exact maximum flows and minimum-cost flows on directed graphs with m edges and polynomially bounded integral demands, costs, and capacities in \(m^{1+o(1)}\) time. Our algorithm builds the flow through a sequence of \(m^{1+o(1)}\) approximate undirected minimum-ratio cycles, each of which is computed and processed in amortized \(m^{o(1)}\) time using a new dynamic graph data structure. Our framework extends to algorithms running in \(m^{1+o(1)}\) time for computing flows that minimize general edge-separable convex functions to high accuracy. This gives almost-linear time algorithms for several problems including entropy-regularized optimal transport, matrix scaling, p -norm flows, and p -norm isotonic regression on arbitrary directed acyclic graphs.},
  archive      = {J_JACM},
  author       = {Li Chen and Rasmus Kyng and Yang Liu and Richard Peng and Maximilian Probst Gutenberg and Sushant Sachdeva},
  doi          = {10.1145/3728631},
  journal      = {Journal of the ACM},
  month        = {5},
  number       = {3},
  pages        = {1--103},
  shortjournal = {J. ACM},
  title        = {Maximum flow and minimum-cost flow in almost-linear time},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Better-than-2 approximations for weighted tree augmentation and applications to steiner tree. <em>JACM</em>, <em>72</em>(2), 1--40. (<a href='https://doi.org/10.1145/3722101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the first approximation algorithms for the Weighted Tree Augmentation Problem (WTAP) that beat the longstanding approximation factor of 2, which can be achieved through standard techniques. The core of our approach is a novel decomposition theorem based on a well-chosen class of thin components . The decomposition theorem asserts that for any pair of a highly structured (but potentially expensive) WTAP solution and a cheaper WTAP solution, there is a way to decompose the cheaper solution into thin components, one of which allows for improving the structured solution. Together with the fact that we can efficiently optimize over thin components through a dynamic program, our decomposition theorem leads to a relative greedy algorithm for WTAP that is a 1 + ln 2 + ɛ-approximation. Moreover, we present an approach to improve on some relative greedy procedures by well-chosen (non-oblivious) local search algorithms. The main application of this approach leads to a 1.5 + ɛ-approximation for WTAP. Furthermore, for the Steiner Tree Problem, it provides an alternative way to obtain the currently best known approximation factor of ln 4 + ɛ . Contrary to prior methods, our approach is purely combinatorial without the need to solve an LP. Nevertheless, the solution value can still be bounded in terms of the well-known hypergraphic LP, leading to an alternative, and arguably simpler, way to bound its integrality gap by ln 4.},
  archive      = {J_JACM},
  author       = {Vera Traub and Rico Zenklusen},
  doi          = {10.1145/3722101},
  journal      = {Journal of the ACM},
  month        = {4},
  number       = {2},
  pages        = {1--40},
  shortjournal = {J. ACM},
  title        = {Better-than-2 approximations for weighted tree augmentation and applications to steiner tree},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2-approximation for prize-collecting steiner forest. <em>JACM</em>, <em>72</em>(2), 1--27. (<a href='https://doi.org/10.1145/3722551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximation algorithms for the prize-collecting Steiner forest (PCSF) problem have been a subject of research for more than three decades, starting with the seminal works of Agrawal et al. and Goemans and Williamson on Steiner forest and prize-collecting problems. In this article, we propose and analyze a natural deterministic algorithm for PCSF that achieves a 2-approximate solution in polynomial time. This represents a significant improvement compared to the previously best known algorithm with a 2.54-approximation factor developed by Hajiaghayi and Jain in 2006. Furthermore, Könemann et al. have established an integrality gap of at least 9/4 for the natural LP relaxation for PCSF. However, we surpass this gap through the utilization of an iterative algorithm and a novel analysis technique. Since 2 is the best known approximation guarantee for the Steiner forest problem, which is a special case of PCSF, our result matches this factor and closes the gap between the Steiner forest problem and its generalized version, PCSF.},
  archive      = {J_JACM},
  author       = {Ali Ahmadi and Iman Gholami and MohammadTaghi Hajiaghayi and Peyman Jabbarzade and Mohammad Mahdavi},
  doi          = {10.1145/3722551},
  journal      = {Journal of the ACM},
  month        = {4},
  number       = {2},
  pages        = {1--27},
  shortjournal = {J. ACM},
  title        = {2-approximation for prize-collecting steiner forest},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the monniaux problem in abstract interpretation. <em>JACM</em>, <em>72</em>(2), 1--51. (<a href='https://doi.org/10.1145/3704632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Monniaux Problem in abstract interpretation asks, roughly speaking, whether the following question is decidable: Given a program P , a safety (e.g., non-reachability) specification \(\varphi\) , and an abstract domain of invariants \(\mathcal {D}\) , does there exist an inductive invariant \(\mathcal {I}\) in \(\mathcal {D}\) guaranteeing that program P meets its specification φ? The Monniaux Problem is of course parameterised by the classes of programs and invariant domains that one considers. In this article, we show that the Monniaux Problem is undecidable for unguarded affine programs and semilinear invariants (unions of polyhedra). Moreover, we show that decidability is recovered in the important special case of simple linear loops.},
  archive      = {J_JACM},
  author       = {Nathanael Fijalkow and Engel Lefaucheux and Pierre Ohlmann and Joël Ouaknine and Amaury Pouly and James Worrell},
  doi          = {10.1145/3704632},
  journal      = {Journal of the ACM},
  month        = {3},
  number       = {2},
  pages        = {1--51},
  shortjournal = {J. ACM},
  title        = {On the monniaux problem in abstract interpretation},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memory checking requires logarithmic overhead. <em>JACM</em>, <em>72</em>(2), 1--43. (<a href='https://doi.org/10.1145/3707202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the complexity of memory checkers with computational security and prove the first general tight lower bound. Memory checkers, first introduced over 30 years ago by Blum, Evans, Gemmel, Kannan, and Naor (FOCS ’91, Algorithmica ’94), allow a user to store and maintain a large memory on a remote and unreliable server by using small trusted local storage. The user can issue instructions to the server and after every instruction, obtain either the correct value or a failure (but not an incorrect answer) with high probability. The main complexity measure of interest is the size of the local storage and the number of queries the memory checker makes upon every logical instruction. The most efficient known construction has query complexity \(O(\log n/\log \log n)\) and local space proportional to a computational security parameter, assuming one-way functions, where n is the logical memory size. Dwork, Naor, Rothblum, and Vaikuntanathan (TCC ’09) showed that for a restricted class of “deterministic and non-adaptive” memory checkers, this construction is optimal, up to constant factors. However, going beyond the small class of deterministic and non-adaptive constructions has remained a major open problem. In this work, we fully resolve the complexity of memory checkers by showing that any construction with local space p and query complexity q must satisfy \begin{equation*} p \ge \frac{n}{(\log n)^{O(q)}} \;. \end{equation*} This implies, as a special case, that \(q\ge \Omega (\log n/\log \log n)\) in any scheme, assuming that \(p\le n^{1-\varepsilon }\) for \(\varepsilon \gt 0\) . The bound applies to any scheme with computational security, completeness \(2/3\) , and inverse polynomial in n soundness (all of which make our lower bound only stronger). We further extend the lower bound to schemes where the read complexity \(q_r\) and write complexity \(q_w\) differ. For instance, we show the tight bound that if \(q_r=O(1)\) and \(p\le n^{1-\varepsilon }\) for \(\varepsilon \gt 0\) , then \(q_w\ge n^{\Omega (1)}\) . This is the first lower bound, for any non-trivial class of constructions, showing a read-write query complexity trade-off. Our proof is via a delicate compression argument showing that a “too good to be true” memory checker can be used to compress random bits of information. We draw inspiration from tools recently developed for lower bounds for relaxed locally decodable codes. However, our proof itself significantly departs from these works, necessitated by the differences between settings.},
  archive      = {J_JACM},
  author       = {Elette Boyle and Ilan Komargodski and Neekon Vafa},
  doi          = {10.1145/3707202},
  journal      = {Journal of the ACM},
  month        = {3},
  number       = {2},
  pages        = {1--43},
  shortjournal = {J. ACM},
  title        = {Memory checking requires logarithmic overhead},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acceleration by stepsize hedging: Multi-step descent and the silver stepsize schedule. <em>JACM</em>, <em>72</em>(2), 1--38. (<a href='https://doi.org/10.1145/3708502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can we accelerate the convergence of gradient descent without changing the algorithm—just by judiciously choosing stepsizes? Surprisingly, we show that the answer is yes. Our proposed Silver Stepsize Schedule optimizes strongly convex functions in \(\kappa ^{\log _{\rho } 2} \approx \kappa ^{0.7864}\) iterations, where \(\rho =1+\sqrt {2}\) is the silver ratio and κ is the condition number. This is intermediate between the textbook unaccelerated rate κ and the accelerated rate \(\kappa ^{1/2}\) due to Nesterov in 1983. The non-strongly convex setting is conceptually identical, and standard black-box reductions imply an analogous partially accelerated rate \(\varepsilon ^{-\log _{\rho } 2} \approx \varepsilon ^{-0.7864}\) . We conjecture and provide partial evidence that these rates are optimal among all stepsize schedules. The Silver Stepsize Schedule is constructed recursively in a fully explicit way. It is non-monotonic, fractal-like, and approximately periodic of period \(\kappa ^{\log _{\rho } 2}\) . This leads to a phase transition in the convergence rate: initially super-exponential (acceleration regime), then exponential (saturation regime). The core algorithmic intuition is hedging between individually suboptimal strategies—short steps and long steps—since bad cases for the former are good cases for the latter, and vice versa. Properly combining these stepsizes yields faster convergence due to the misalignment of worst-case functions. The key challenge in proving this speedup is enforcing long-range consistency conditions along the algorithm’s trajectory. We do this by developing a technique that recursively glues constraints from different portions of the trajectory, thus removing a key stumbling block in previous analyses of optimization algorithms. More broadly, we believe that the concepts of hedging and multi-step descent have the potential to be powerful algorithmic paradigms in a variety of contexts in optimization and beyond. This article publishes and extends the first author’s 2018 master’s thesis (advised by the second author)—which established for the first time that judiciously choosing stepsizes can enable acceleration in convex optimization. Prior to this thesis, the only such result was for the special case of quadratic optimization, due to Young in 1953.},
  archive      = {J_JACM},
  author       = {Jason M. Altschuler and Pablo A. Parrilo},
  doi          = {10.1145/3708502},
  journal      = {Journal of the ACM},
  month        = {3},
  number       = {2},
  pages        = {1--38},
  shortjournal = {J. ACM},
  title        = {Acceleration by stepsize hedging: Multi-step descent and the silver stepsize schedule},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Private low-rank approximation for covariance matrices, dyson brownian motion, and eigenvalue-gap bounds for gaussian perturbations. <em>JACM</em>, <em>72</em>(2), 1--88. (<a href='https://doi.org/10.1145/3716496'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of approximating a \(d \times d\) covariance matrix M with a rank- k matrix under \((\varepsilon ,\delta)\) -differential privacy. We present and analyze a complex variant of the Gaussian mechanism and obtain upper bounds on the Frobenius norm of the difference between the matrix output by this mechanism and the best rank- k approximation to M . Our analysis provides improvements over previous bounds, particularly when the spectrum of M satisfies natural structural assumptions. The novel insight is to view the addition of Gaussian noise to a matrix as a continuous-time matrix Brownian motion. This viewpoint allows us to track the evolution of eigenvalues and eigenvectors of the matrix, which are governed by stochastic differential equations discovered by Dyson. These equations enable us to upper bound the Frobenius distance between the best rank- k approximation of M and that of a Gaussian perturbation of M as an integral that involves inverse eigenvalue gaps of the stochastically evolving matrix, as opposed to a sum of perturbation bounds obtained via Davis-Kahan-type theorems. Subsequently, again using the Dyson Brownian motion viewpoint, we show that the eigenvalues of the matrix M perturbed by Gaussian noise have large gaps with high probability. These results also contribute to the analysis of low-rank approximations under average-case perturbations, and to an understanding of eigenvalue gaps for random matrices, both of which may be of independent interest.},
  archive      = {J_JACM},
  author       = {Oren Mangoubi and Nisheeth K. Vishnoi},
  doi          = {10.1145/3716496},
  journal      = {Journal of the ACM},
  month        = {3},
  number       = {2},
  pages        = {1--88},
  shortjournal = {J. ACM},
  title        = {Private low-rank approximation for covariance matrices, dyson brownian motion, and eigenvalue-gap bounds for gaussian perturbations},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proving as fast as computing: Succinct arguments with constant prover overhead. <em>JACM</em>, <em>72</em>(2), 1--54. (<a href='https://doi.org/10.1145/3721477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Succinct arguments are proof systems that allow a powerful, but untrusted, prover to convince a weak verifier that an input x belongs to a language \(L \in \mathsf {NP}\) , with communication that is much shorter than the \(\mathsf {NP}\) witness. Such arguments, which grew out of the theory literature, are now drawing immense interest also in practice, where a key bottleneck that has arisen is the high computational cost of proving correctness. In this work, we address this problem by constructing succinct arguments for general computations, expressed as Boolean circuits (of bounded fan-in), with a strictly linear size prover. The soundness error of the protocol is an arbitrarily small constant. Prior to this work, succinct arguments were known with a quasi- linear size prover for general Boolean circuits or with linear-size only for arithmetic circuits, defined over large finite fields. In more detail, for every Boolean circuit \(C=C(x,w)\) , we construct an \(O(\log |C|)\) -round argument-system in which the prover can be implemented by a size \(O(|C|)\) Boolean circuit (given as input both the instance x and the witness w ), with arbitrarily small constant soundness error and using \(\mathrm{poly}(\lambda ,\log |C|)\) communication, where \(\lambda\) denotes the security parameter. The verifier can be implemented by a size \(O(|x|) + \mathrm{poly}(\lambda , \log |C|)\) circuit following a size \(O(|C|)\) private pre-processing step, or, alternatively, by using a purely public-coin protocol (with no pre-processing) with a size \(O(|C|)\) verifier. The protocol can be made zero-knowledge using standard techniques (and with similar parameters). The soundness of our protocol is computational and relies on the existence of collision resistant hash functions that can be computed by linear-size circuits, such as those proposed by Applebaum et al. (ITCS, 2017). At the heart of our construction is a new information-theoretic interactive oracle proof ( \(\mathsf {IOP}\) ), an interactive analog of a \(\mathsf {PCP}\) , for circuit satisfiability, with constant prover overhead. The improved efficiency of our \(\mathsf {IOP}\) is obtained by bypassing a barrier faced by prior \(\mathsf {IOP}\) constructions, which needed to (either explicitly or implicitly) encode the entire computation using a multiplication code.},
  archive      = {J_JACM},
  author       = {Noga Ron-Zewi and Ron Rothblum},
  doi          = {10.1145/3721477},
  journal      = {Journal of the ACM},
  month        = {3},
  number       = {2},
  pages        = {1--54},
  shortjournal = {J. ACM},
  title        = {Proving as fast as computing: Succinct arguments with constant prover overhead},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integer programs with bounded subdeterminants and two nonzeros per row. <em>JACM</em>, <em>72</em>(1), 1--50. (<a href='https://doi.org/10.1145/3695985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a strongly polynomial-time algorithm for integer linear programs defined by integer coefficient matrices whose subdeterminants are bounded by a constant and that contain at most two nonzero entries in each row. The core of our approach is the first polynomial-time algorithm for the weighted stable set problem on graphs that do not contain more than k vertex-disjoint odd cycles, where k is any constant. Previously, polynomial-time algorithms were only known for k =0 (bipartite graphs) and for k =1. We observe that integer linear programs defined by coefficient matrices with bounded subdeterminants and two nonzeros per column can be also solved in strongly polynomial-time, using a reduction to b -matching.},
  archive      = {J_JACM},
  author       = {Samuel Fiorini and Gwenaël Joret and Stefan Weltge and Yelena Yuditsky},
  doi          = {10.1145/3695985},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1--50},
  shortjournal = {J. ACM},
  title        = {Integer programs with bounded subdeterminants and two nonzeros per row},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subsampling suffices for adaptive data analysis. <em>JACM</em>, <em>72</em>(1), 1--45. (<a href='https://doi.org/10.1145/3698104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring that analyses performed on a dataset are representative of the entire population is one of the central problems in statistics. Most classical techniques assume that the dataset is independent of the analyst’s query and break down in the common setting where a dataset is reused for multiple, adaptively chosen, queries. This problem of adaptive data analysis was formalized in the seminal works of Dwork et al. (STOC 2015) and Hardt and Ullman (FOCS 2014). We identify a remarkably simple set of assumptions under which the queries will continue to be representative even when chosen adaptively: the only requirements are that each query takes as input a random subsample and outputs few bits. This result shows that the noise inherent in subsampling is sufficient to guarantee that query responses generalize. The simplicity of this subsampling-based framework allows it to model a variety of real-world scenarios not covered by prior work. In addition to its simplicity, we demonstrate the utility of this framework by designing mechanisms for two foundational tasks: statistical queries and median finding. In particular, our mechanism for answering the broadly applicable class of statistical queries is both extremely simple and state of the art in many parameter regimes.},
  archive      = {J_JACM},
  author       = {Guy Blanc},
  doi          = {10.1145/3698104},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1--45},
  shortjournal = {J. ACM},
  title        = {Subsampling suffices for adaptive data analysis},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Orbit-finite linear programming. <em>JACM</em>, <em>72</em>(1), 1--39. (<a href='https://doi.org/10.1145/3703909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An infinite set is orbit-finite if, up to permutations of atoms, it has only finitely many elements. We study a generalisation of linear programming where constraints are expressed by an orbit-finite system of linear inequalities. As our principal contribution we provide a decision procedure for checking if such a system has a real solution, and for computing the minimal/maximal value of a linear objective function over the solution set. We also show undecidability of these problems in case when only integer solutions are considered. Therefore orbit-finite linear programming is decidable, while orbit-finite integer linear programming is not.},
  archive      = {J_JACM},
  author       = {Arka Ghosh and Piotr Hofman and Sławomir Lasota},
  doi          = {10.1145/3703909},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1--39},
  shortjournal = {J. ACM},
  title        = {Orbit-finite linear programming},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardness of approximate diameter: Now for undirected graphs. <em>JACM</em>, <em>72</em>(1), 1--32. (<a href='https://doi.org/10.1145/3704631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximating the graph diameter is a basic task of both theoretical and practical interest. A simple folklore algorithm can output a 2-approximation to the diameter in linear time by running BFS from an arbitrary vertex. It has been open whether a better approximation is possible in near-linear time. A series of articles on fine-grained complexity have led to strong hardness results for diameter in directed graphs, culminating in a recent tradeoff curve independently discovered by [Li, STOC’21] and [Dalirrooyfard and Wein, STOC’21], showing that under the Strong Exponential Time Hypothesis (SETH), for any integer k ≥ 2 and δ > 0, a \(2-\frac{1}{k}-\delta\) approximation for diameter in directed m -edge graphs requires \(m^{1+1/(k-1)-o(1)}\) time. In particular, the simple linear time 2-approximation algorithm is optimal for directed graphs. In this article, we prove that the same tradeoff lower bound curve is possible for undirected graphs as well, extending results of [Roditty and Vassilevska W., STOC’13], [Li’20] and [Bonnet, ICALP’21] who proved the first few cases of the curve, k =2,3, and 4, respectively. Our result shows in particular that the simple linear time 2-approximation algorithm is conditionally optimal for undirected graphs. To obtain our result, we extract the core ideas in known reductions and introduce a unification and generalization that could be useful for proving SETH-based hardness for other problems in undirected graphs related to distance computation.},
  archive      = {J_JACM},
  author       = {Mina Dalirrooyfard and Ray Li and Virginia Vassilevska Williams},
  doi          = {10.1145/3704631},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1--32},
  shortjournal = {J. ACM},
  title        = {Hardness of approximate diameter: Now for undirected graphs},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantitative coding and complexity theory of data: Part i: Motivation, definition, consequences. <em>JACM</em>, <em>72</em>(1), 1--39. (<a href='https://doi.org/10.1145/3705609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When encoding real numbers as (necessarily infinite) bit-strings, the naïve binary/decimal expansion is well-known [ doi:10.1112/plms/s2-43.6.544 ] computably “ un reasonable”, rendering, for example, tripling qualitatively discontinuous on Cantor’s sequence space. Encoding reals as sequences of (finite integer numerators and denominators, in binary, of) rational approximations does make common operations qualitatively computable, yet admits no bounds on their computational complexity/quantitative continuity. Dyadic approximations, on the other hand, are known polynomially, and signed binary expansions even linearly, “reasonable” in a rigorous sense recalled in the introduction of this work. But how to distinguish between un/suitable encodings of spaces common in Calculus beyond the reals, such as Banach or Sobolev? With respect to qualitative computability/continuity on topological spaces, the technical condition of admissibility had been identified [ doi:10.1016/0304-3975(85)90208-7 ] for an encoding over Cantor space (historically called a representation ) to be “reasonable” [ doi:10.1007/978-3-030-59234-9_9 ] . Roughly speaking, admissibility requires the representation to be (i) continuous, and to be (ii) maximal with respect to continuous reduction. Admissible representations exist for a large class of spaces. And for (precisely) these does the Kreitz–Weihrauch—sometimes aka Main —Theorem of Computable Analysis hold, which characterizes continuity of functions by continuity of mappings translating codes, so-called realizers . We refine qualitative computability/continuity on topological spaces to quantitative continuity/complexity on metric spaces by proposing a notion, and investigating the properties, of polynomially/linearly admissible representations. Roughly speaking, these are (i) close to “optimally” continuous, namely linearly/polynomially relative to the space’s entropy, and they are (ii) maximal with respect to relative linear/polynomial quantitatively continuous reductions defined in the main text. Quantitatively admissible representations are closed under composition over generalized ground spaces beyond Cantor’s. Such representations exhibit a quantitative strengthening of the qualitative Main Theorem , namely now characterizing quantitative continuity of functions by quantitative continuity of realizers. A large class of compact metric spaces is shown to admit polynomially admissible representations over compact ultra metric spaces, and some even a generalization of the linearly admissible signed binary encoding. Quantitative admissibility thus provides the desired criterion for complexity-theoretically “reasonable” encodings.},
  archive      = {J_JACM},
  author       = {Donghyun Lim and Martin Ziegler},
  doi          = {10.1145/3705609},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1--39},
  shortjournal = {J. ACM},
  title        = {Quantitative coding and complexity theory of data: Part i: Motivation, definition, consequences},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correct and complete type checking and certified erasure for , in. <em>JACM</em>, <em>72</em>(1), 1--74. (<a href='https://doi.org/10.1145/3706056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coq is built around a well-delimited kernel that performs type checking for definitions in a variant of the Calculus of Inductive Constructions ( CIC ). Although the metatheory of CIC is very stable and reliable, the correctness of its implementation in Coq is less clear. Indeed, implementing an efficient type checker for CIC is a rather complex task, and many parts of the code rely on implicit invariants which can easily be broken by further evolution of the code. Therefore, on average, one critical bug has been found every year in Coq . This article presents the first implementation of a type checker for the kernel of Coq (without the module system, template polymorphism and η-conversion), which is proven sound and complete in Coq with respect to its formal specification. Note that because of Gödel’s second incompleteness theorem, there is no hope to prove completely the soundness of the specification of Coq inside Coq (in particular strong normalization), but it is possible to prove the correctness and completeness of the implementation assuming soundness of the specification, thus moving from a trusted code base (TCB) to a trusted theory base (TTB) paradigm. Our work is based on the MetaCoq project which provides meta-programming facilities to work with terms and declarations at the level of the kernel. We verify a relatively efficient type checker based on the specification of the typing relation of the Polymorphic, Cumulative Calculus of Inductive Constructions ( PCUIC ) at the basis of Coq . It is worth mentioning that during the verification process, we have found a source of incompleteness in Coq ’s official type checker, which has then been fixed in Coq 8.14 thanks to our work. In addition to the kernel implementation, another essential feature of Coq is the so-called extraction mechanism: the production of executable code in functional languages from Coq definitions. We present a verified version of this subtle type and proof erasure step, therefore enabling the verified extraction of a safe type checker for Coq in the future.},
  archive      = {J_JACM},
  author       = {Matthieu Sozeau and Yannick Forster and Meven Lennon-Bertrand and Jakob Nielsen and Nicolas Tabareau and Théo Winterhalter},
  doi          = {10.1145/3706056},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1--74},
  shortjournal = {J. ACM},
  title        = {Correct and complete type checking and certified erasure for , in},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow-augmentation i: Directed graphs. <em>JACM</em>, <em>72</em>(1), 1--38. (<a href='https://doi.org/10.1145/3706103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show a flow-augmentation algorithm in directed graphs: There exists a randomized polynomial-time algorithm that, given a directed graph G , two vertices s, t ∈ V(G) , and an integer k , adds (randomly) to G a number of arcs such that for every minimal st -cut Z in G of size at most k , with probability 2 −poly( k ) the set Z becomes a minimum st -cut in the resulting graph. We also provide a deterministic counterpart of this procedure. The directed flow-augmentation tool allows us to prove fixed-parameter tractability of a number of problems parameterized by the cardinality of the deletion set whose parameterized complexity status was repeatedly posed as open problems: Chain SAT , defined by Chitnis, Egri, and Marx [ESA’13, Algorithmica’17], a number of weighted variants of classic directed cut problems, such as Weighted st - Cut or Weighted Directed Feedback Vertex Set . By proving that Chain SAT is FPT, we confirm a conjecture of Chitnis, Egri, and Marx that, for any graph H , if the List H - Coloring problem is polynomial-time solvable, then the corresponding vertex-deletion problem is fixed-parameter tractable. Chain SAT , defined by Chitnis, Egri, and Marx [ESA’13, Algorithmica’17], a number of weighted variants of classic directed cut problems, such as Weighted st - Cut or Weighted Directed Feedback Vertex Set .},
  archive      = {J_JACM},
  author       = {Eun Jung Kim and Stefan Kratsch and Marcin Pilipczuk and Magnus Wahlström},
  doi          = {10.1145/3706103},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1--38},
  shortjournal = {J. ACM},
  title        = {Flow-augmentation i: Directed graphs},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric embeddability of complexes is ∃ℝ-complete. <em>JACM</em>, <em>72</em>(1), 1--26. (<a href='https://doi.org/10.1145/3707201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the decision problem of determining whether a given (abstract simplicial) k -complex has a geometric embedding in ℝ d is complete for the Existential Theory of the Reals for all d ≥ 3 and k ∈ { d -1, d } by reducing from pseudoline stretchability. Consequently, the problem is polynomial time equivalent to determining whether a polynomial equation system has a real solution. Moreover, this implies NP-hardness and constitutes the first hardness result for the algorithmic problem of geometrically embedding (abstract simplicial) complexes. This complements recent breakthroughs for the computational complexity of piece-wise linear embeddability [Matoušek, Sedgwick, Tancer, and Wagner, J. ACM 2018, and de Mesmay, Rieck, Sedgwick and Tancer, J. ACM 2020] and establishes connections to computational topology.},
  archive      = {J_JACM},
  author       = {Mikkel Abrahamsen and Linda Kleist and Tillmann Miltzow},
  doi          = {10.1145/3707201},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1--26},
  shortjournal = {J. ACM},
  title        = {Geometric embeddability of complexes is ∃ℝ-complete},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient quantum factoring algorithm. <em>JACM</em>, <em>72</em>(1), 1--13. (<a href='https://doi.org/10.1145/3708471'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that n -bit integers can be factorized by independently running a quantum circuit with \(\tilde{O}(n^{3/2})\) gates for \(\sqrt {n}+4\) times, and then using polynomial-time classical post-processing. The correctness of the algorithm relies on a certain number-theoretic conjecture. It is currently not clear if the algorithm can lead to improved physical implementations in practice.},
  archive      = {J_JACM},
  author       = {Oded Regev},
  doi          = {10.1145/3708471},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1--13},
  shortjournal = {J. ACM},
  title        = {An efficient quantum factoring algorithm},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallelize single-site dynamics up to dobrushin criterion. <em>JACM</em>, <em>72</em>(1), 1--33. (<a href='https://doi.org/10.1145/3708558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-site dynamics are canonical Markov chain based algorithms for sampling from high-dimensional distributions, such as the Gibbs distributions of graphical models. We introduce a simple and generic parallel algorithm that faithfully simulates single-site dynamics. Under a much relaxed, asymptotic variant of the ℓ p -Dobrushin’s condition—where the Dobrushin’s influence matrix has a bounded ℓ p -induced operator norm for an arbitrary p ∈ [1, ∞]—our algorithm simulates N steps of single-site updates within a parallel depth of O ( N / n +log n ) on Õ( m ) processors, where n is the number of sites and m is the size of the graphical model. For Boolean-valued random variables, if the ℓ p -Dobrushin’s condition holds—specifically, if the ℓ p -induced operator norm of the Dobrushin’s influence matrix is less than 1—the parallel depth can be further reduced to O (log N + log n ), achieving an exponential speedup. These results suggest that single-site dynamics with near-linear mixing times can be parallelized into RNC sampling algorithms, independent of the maximum degree of the underlying graphical model, as long as the Dobrushin influence matrix maintains a bounded operator norm. We show the effectiveness of this approach with RNC samplers for the hardcore and Ising models within their uniqueness regimes, as well as an RNC SAT sampler for satisfying solutions of conjunctive normal form formulas in a local lemma regime. Furthermore, by employing non-adaptive simulated annealing, these RNC samplers can be transformed into RNC algorithms for approximate counting.},
  archive      = {J_JACM},
  author       = {Hongyang Liu and Yitong Yin},
  doi          = {10.1145/3708558},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1--33},
  shortjournal = {J. ACM},
  title        = {Parallelize single-site dynamics up to dobrushin criterion},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
