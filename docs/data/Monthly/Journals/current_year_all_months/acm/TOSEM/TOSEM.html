<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TOSEM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tosem">TOSEM - 189</h2>
<ul>
<li><details>
<summary>
(2025). JSimpo: Structural deobfuscation of JavaScript programs. <em>TOSEM</em>, <em>34</em>(7), 1-28. (<a href='https://doi.org/10.1145/3714460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {JavaScript (JS) obfuscation is now prevalent among popular websites and introduces challenges for malware detection and code review. Given an obfuscated JS program, existing deobfuscation techniques aim to recover the original JS program. However, these techniques overlook structural obfuscation (e.g., control-flow flattening), which causes deobfuscation to have a sub-optimal success rate. To address these challenges, in this article, we propose the first approach of structural deobfuscation named JSimpo for JS programs with two novel techniques: slice symbolic execution and dynamic code execution. We implement our JSimpo approach and evaluate it on 2,000 JS programs from the top 100 JS projects on GitHub. The evaluation results show that JSimpo can effectively conduct structural deobfuscation, boosting the average structural similarity to 78.41% (from 39.33%) between obfuscated programs and their original programs, whereas the best of the state-of-the-art/practice deobfuscators can achieve only 62.64%. The results also show JSimpo's generalization ability over programs obfuscated by various obfuscators. Additionally, JSimpo preserves the semantics of deobfuscated programs by passing all test cases that obfuscated programs have passed.},
  archive      = {J_TOSEM},
  author       = {Tianyu Chen and Ding Li and Ying Zhang and Tao Xie},
  doi          = {10.1145/3714460},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {JSimpo: Structural deobfuscation of JavaScript programs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring parameter-efficient fine-tuning techniques for code generation with large language models. <em>TOSEM</em>, <em>34</em>(7), 1-25. (<a href='https://doi.org/10.1145/3714461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this article, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.},
  archive      = {J_TOSEM},
  author       = {Martin Weyssow and Xin Zhou and Kisub Kim and David Lo and Houari Sahraoui},
  doi          = {10.1145/3714461},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Exploring parameter-efficient fine-tuning techniques for code generation with large language models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Teaching code LLMs to use autocompletion tools in repository-level code generation. <em>TOSEM</em>, <em>34</em>(7), 1-27. (<a href='https://doi.org/10.1145/3714462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent code large language models (LLMs) have shown promising performance in generating standalone functions. However, they face limitations in repository-level code generation due to their lack of awareness of repository - level dependencies (e.g., user-defined attributes), resulting in dependency errors such as undefined-variable and no-member errors. In this work, we introduce ToolGen , an approach that integrates autocompletion tools into the code LLM generation process to address these dependencies. ToolGen comprises two main phases: Trigger Insertion and Model Fine-tuning (Offline), and Tool-integrated Code Generation (Online). During the offline phase, ToolGen augments functions within a given code corpus with a special mark token, indicating positions to trigger autocompletion tools. These augmented functions, along with their corresponding descriptions, are then used to fine-tune a selected code LLM. In the online phase, ToolGen iteratively generates functions by predicting tokens step-by-step using the fine-tuned LLM. Whenever a mark token is encountered, ToolGen invokes the autocompletion tool to suggest code completions and selects the most appropriate one through constrained greedy search. We conduct comprehensive experiments to evaluate ToolGen ’s effectiveness in repository-level code generation across three distinct code LLMs: CodeGPT, CodeT5, and CodeLlama. To facilitate this evaluation, we create a benchmark comprising 671 real-world code repositories and introduce two new dependency-based metrics: Dependency Coverage and Static Validity Rate . The results demonstrate that ToolGen significantly improves Dependency Coverage by 31.4% to 39.1% and Static Validity Rate by 44.9% to 57.7% across the three LLMs, while maintaining competitive or improved performance in widely recognized similarity metrics such as BLEU-4, CodeBLEU, Edit Similarity, and Exact Match. On the CoderEval dataset, ToolGen achieves improvements of 40.0% and 25.0% in test pass rate (Pass@1) for CodeT5 and CodeLlama, respectively, while maintaining the same pass rate for CodeGPT. ToolGen also demonstrates high efficiency in repository-level code generation, with latency ranging from 0.63 to 2.34 seconds for generating each function. Furthermore, our generalizability evaluation confirms ToolGen ’s consistent performance when applied to diverse code LLMs, encompassing various model architectures and scales.},
  archive      = {J_TOSEM},
  author       = {Chong Wang and Jian Zhang and Yebo Feng and Tianlin Li and Weisong Sun and Yang Liu and Xin Peng},
  doi          = {10.1145/3714462},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Teaching code LLMs to use autocompletion tools in repository-level code generation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TAEFuzz: Automatic fuzzing for image-based deep learning systems via transferable adversarial examples. <em>TOSEM</em>, <em>34</em>(7), 1-31. (<a href='https://doi.org/10.1145/3714463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning (DL) components have been broadly applied in diverse applications. Similar to traditional software engineering, effective test case generation methods are needed by industry to enhance the quality and robustness of these DL components. To this end, we propose a novel automatic software testing technique, Fuzz-testing via T ransferable A dversarial E xamples ( TAEFuzz ), which aims to automatically assess and enhance the robustness of image-based DL systems based on test cases generated by transferable adversarial examples. TAEFuzz alleviates the over-fitting problem during optimized test case generation and prevents test cases from prematurely falling into local optima. In addition, TAEFuzz enhances the visual quality of test cases through constraining perturbations inserted into sensitive areas of the images. For a system with low robustness, TAEFuzz trains a low-cost denoising module to reduce the impact of perturbations in transferable adversarial examples on the system. Experimental results demonstrate that the test cases generated by TAEFuzz can discover up to 46.1% more errors in the targeted systems and ensure the visual quality of test cases. Compared to existing techniques, TAEFuzz also enhances the robustness of the target systems against transferable adversarial examples with the perturbation denoising module.},
  archive      = {J_TOSEM},
  author       = {Shunhui Ji and Changrong Huang and Bin Ren and Hai Dong and Lars Grunske and Yan Xiao and Pengcheng Zhang},
  doi          = {10.1145/3714463},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {TAEFuzz: Automatic fuzzing for image-based deep learning systems via transferable adversarial examples},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Signal feature coverage and testing for CPS dataflow models. <em>TOSEM</em>, <em>34</em>(7), 1-37. (<a href='https://doi.org/10.1145/3714467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design of cyber-physical systems (CPS) typically involves dataflow modeling. The structure of dataflow models differs from the traditional software, making standard coverage metrics not appropriate for measuring the thoroughness of testing. To address this limitation, this article proposes signal feature coverage as a new coverage metric for systematically testing CPS dataflow models. We derive signal feature coverage by leveraging signal features. We developed a testing framework in Simulink, a popular dataflow modeling and simulation environment, that automates the generation and execution of test cases based on the defined coverage metric. We evaluated the effectiveness of our approach by carrying out experiments on five Simulink models tested against ten Signal Temporal Logic specifications. We compared our coverage-based testing approach to adaptive random testing, falsification testing, output diversity-based approaches, and testing using MathWorks’ Simulink Design Verifier. The results demonstrate that our coverage-based testing approach outperforms the conventional techniques regarding fault detection capability.},
  archive      = {J_TOSEM},
  author       = {Ezio Bartocci and Leonardo Mariani and Dejan Nickovic and Drishti Yadav},
  doi          = {10.1145/3714467},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Signal feature coverage and testing for CPS dataflow models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Faster and better quantum software testing through specification reduction and projective measurements. <em>TOSEM</em>, <em>34</em>(7), 1-39. (<a href='https://doi.org/10.1145/3714468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing (QC) promises polynomial and exponential speedups in many domains, such as unstructured search and prime number factoring. However, quantum programs yield probabilistic outputs from exponentially growing distributions and are vulnerable to quantum-specific faults. Existing quantum software testing (QST) approaches treat quantum superpositions as classical distributions. This leads to two major limitations when applied to quantum programs: (1) an exponentially growing sample space distribution and (2) failing to detect quantum-specific faults such as phase flips. To overcome these limitations, we introduce a QST approach, which applies a reduction algorithm to a quantum program specification. The reduced specification alleviates the limitations (1) by enabling faster sampling through quantum parallelism and (2) by performing projective measurements in the mixed Hadamard basis. Our evaluation of 143 quantum programs across four categories demonstrates significant improvements in test runtimes and fault detection with our reduction approach. Average test runtimes improved from 169.9 s to 11.8 s, with notable enhancements in programs with large circuit depths (383.1 s to 33.4 s) and large program specifications (464.8 s to 7.7 s). Furthermore, our approach increases mutation scores from \(54.5\%\) to \(74.7\%\) , effectively detecting phase flip faults that non-reduced specifications miss. These results underline our approach's importance to improve QST efficiency and effectiveness.},
  archive      = {J_TOSEM},
  author       = {Noah H. Oldfield and Christoph Laaber and Tao Yue and Shaukat Ali},
  doi          = {10.1145/3714468},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Faster and better quantum software testing through specification reduction and projective measurements},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introducing phylogenetics in search-based software engineering: Phylogenetics-aware SBSE. <em>TOSEM</em>, <em>34</em>(7), 1-38. (<a href='https://doi.org/10.1145/3715002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phylogenetics studies the relationships, in terms of biological history and kinship, of a set of taxa (e.g., species). We argue that in Search-based Software Engineering (SBSE), the individuals of an evolutionary computation-driven population could be considered as taxa for which the leverage of Phylogenetic Inference might be beneficial. In this work, we present our Phylogenetics-aware SBSE approach. Our approach introduces a novel Phylogenetic Operation to promote results which are sufficiently aligned (in terms of lineage) with a certain reference given by the domain expert. Our approach is evaluated in two heterogeneous industrial case studies: Procedural Content Generation from Game Software Engineering, and Feature Location from Software Maintenance. The results are analyzed using quality-of-the-solution and acceptance-by-developers measurements. We performed a statistical analysis to determine whether the impact on the results is significant compared to baselines that do not leverage Phylogenetics. The results show that our approach significantly outperforms two baselines in both case studies. Furthermore, two focus groups confirmed the acceptance of our approach and stressed that solution acceptance may make the difference in industrial environments. Our work has the potential to motivate a new breed of research work on Phylogenetics awareness to produce better results in Software Engineering.},
  archive      = {J_TOSEM},
  author       = {Daniel Blasco and Antonio Iglesias and Jorge Echeverría and Francisca Pérez and Carlos Cetina},
  doi          = {10.1145/3715002},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Introducing phylogenetics in search-based software engineering: Phylogenetics-aware SBSE},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid automated program repair by combining large language models and program analysis. <em>TOSEM</em>, <em>34</em>(7), 1-28. (<a href='https://doi.org/10.1145/3715004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Program Repair (APR) has garnered significant attention due to its potential to streamline the bug repair process for human developers. Recently, LLM-based APR methods have shown promise in repairing real-world bugs. However, existing APR methods often utilize patches generated by LLMs without further optimization, resulting in reduced effectiveness due to the lack of program-specific knowledge. Furthermore, the evaluations of these APR methods have typically been conducted under the assumption of perfect fault localization, which may not accurately reflect their real-world effectiveness. To address these limitations, this article introduces an innovative APR approach called G iant R epair . Our approach leverages the insight that LLM-generated patches, although not necessarily correct, offer valuable guidance for the patch generation process. Based on this insight, G iant R epair first constructs patch skeletons from LLM-generated patches to confine the patch space, and then generates high-quality patches tailored to specific programs through context-aware patch generation by instantiating the skeletons. To evaluate the performance of our approach, we conduct two large-scale experiments. The results demonstrate that G iant R epair not only effectively repairs more bugs (an average of 27.78% on Defects4J v1.2 and 23.40% on Defects4J v2.0) than using LLM-generated patches directly, but also outperforms state-of-the-art APR methods by repairing at least 42 and 7 more bugs under perfect and automated fault localization scenarios, respectively.},
  archive      = {J_TOSEM},
  author       = {Fengjie Li and Jiajun Jiang and Jiajun Sun and Hongyu Zhang},
  doi          = {10.1145/3715004},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Hybrid automated program repair by combining large language models and program analysis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The good, the bad, and the monstrous: Predicting highly change-prone source code methods at their inception. <em>TOSEM</em>, <em>34</em>(7), 1-29. (<a href='https://doi.org/10.1145/3715006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cost of software maintenance often surpasses the initial development expenses, making it a significant concern for the software industry. A key strategy for alleviating future maintenance burdens is the early prediction and identification of change-prone code components, which allows for timely optimizations. While prior research has largely concentrated on predicting change-prone files and classes—an approach less favored by practitioners—this article shifts focus to predicting highly change-prone methods, aligning with the preferences of both practitioners and researchers. We analyzed 774,051 source code methods from 49 prominent open source Java projects. Our findings reveal that approximately 80% of changes are concentrated in just 20% of the methods, demonstrating the Pareto 80/20 principle. Moreover, this subset of methods is responsible for the majority of the identified bugs in these projects. After establishing their critical role in mitigating software maintenance costs, our study shows that machine learning models can effectively identify these highly change-prone methods from their inception. Additionally, we conducted a thorough manual analysis to uncover common patterns (or concepts) among the more difficult-to-predict methods. These insights can help future research develop new features and enhance prediction accuracy.},
  archive      = {J_TOSEM},
  author       = {Shaiful Chowdhury},
  doi          = {10.1145/3715006},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {The good, the bad, and the monstrous: Predicting highly change-prone source code methods at their inception},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on challenges for LLM application developers. <em>TOSEM</em>, <em>34</em>(7), 1-37. (<a href='https://doi.org/10.1145/3715007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large language models (LLMs) have seen rapid advancements, significantly impacting various fields such as computer vision, natural language processing, and software engineering. These LLMs, exemplified by OpenAI's ChatGPT, have revolutionized the way we approach language understanding and generation tasks. However, in contrast to traditional software development practices, LLM development introduces new challenges for AI developers in design, implementation, and deployment. These challenges span different areas (such as prompts, APIs, and plugins), requiring developers to navigate unique methodologies and considerations specific to LLM application development. Despite the profound influence of LLMs, to the best of our knowledge, these challenges have not been thoroughly investigated in previous empirical studies. To fill this gap, we present the first comprehensive study on understanding the challenges faced by LLM developers. Specifically, we crawl and analyze 29,057 relevant questions from a popular OpenAI developer forum. We first examine their popularity and difficulty. After manually analyzing 2,364 sampled questions, we construct a taxonomy of challenges faced by LLM developers. Based on this taxonomy, we summarize a set of findings and actionable implications for LLM-related stakeholders, including developers and providers (especially the OpenAI organization).},
  archive      = {J_TOSEM},
  author       = {Xiang Chen and Chaoyang Gao and Chunyang Chen and Guangbei Zhang and Yong Liu},
  doi          = {10.1145/3715007},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study on challenges for LLM application developers},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated detection and repair of floating-point precision problems in convolutional neural network operators. <em>TOSEM</em>, <em>34</em>(7), 1-32. (<a href='https://doi.org/10.1145/3715104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Network (CNN) operators, mostly based on mathematical linear computations, are of vital importance to developing CNN-based software. Existing studies reveal that these operators are prone to floating-point precision problems (FPPs). In a CNN-based application, such problems can be propagated and result in catastrophic consequences. Thus, it is highly desired to detect and repair the FPPs in CNN operators. Considering the FPPs in CNN operators are mainly caused by accumulated floating-point errors and diverse floating-point tensors instead of wrong codes or bad implementations, it requires much time cost and is difficult to tackle these FPPs. In this article, we propose the first method for the automated detection and repair of FPPs in CNN operators from the perspective of floating-point tensors. To generate diverse tensors with floating-point numbers, we design two levels of mutation rules, namely computation-level mutation and input-level mutation, containing a total of five mutation methods. To detect the FPPs caused by the accumulated floating-point errors, our method uses a weight matrix to guide the progressive mutation. To repair the detected FPPs, our method transforms the error-prone floating-point tensors based on the mathematical rewriting of the floating-point linear computational properties without destroying the original computation. Experimental results show that our methods can detect and repair FPPs in CNN operators effectively and efficiently and could reduce 93.32% to 100% of the FPPs in CNN operators. We conduct a case study on six different widely used CNN models and confirm that the proposed FPP method is generalizable and effective across a variety of tasks and architectures. Our detection and repair method offers an intuitive way to handle FPPs during development, allowing users to continue building and fine-tuning their models without being slowed down by numerical precision errors. We believe that our method could open up a new way to enhance the quality of CNN operators and CNN-based software.},
  archive      = {J_TOSEM},
  author       = {Jiawei Liu and Xufan Zhang and Lurong Xu and Chunrong Fang and Mingzheng Gu and Weisi Luo and Dong Chai and Jiang Wang and Zhihong Zhao and Zhenyu Chen},
  doi          = {10.1145/3715104},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automated detection and repair of floating-point precision problems in convolutional neural network operators},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding safety violations of AI-enabled control systems through the lens of synthesized proxy programs. <em>TOSEM</em>, <em>34</em>(7), 1-35. (<a href='https://doi.org/10.1145/3715105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the increasing adoption of modern AI-enabled control systems, ensuring their safety and reliability has become a critical task in software testing. One prevalent approach to testing control systems is falsification, which aims to find an input signal that causes the control system to violate a formal safety specification using optimization algorithms. However, applying falsification to AI-enabled control systems poses two significant challenges: (1) it requires the system to execute numerous candidate test inputs, which can be time-consuming, particularly for systems with AI models that have many parameters, and (2) multiple safety requirements are typically defined as a conjunctive specification, which is difficult for existing falsification approaches to comprehensively cover. This article introduces Synthify , a falsification framework tailored for AI-enabled control systems, i.e., control systems equipped with AI controllers. Our approach performs falsification in a two-phase process. At the start, Synthify synthesizes a program that implements one or a few linear controllers to serve as a proxy for the AI controller. This proxy program mimics the AI controller’s functionality but is computationally more efficient. Then, Synthify employs the \(\epsilon\) -greedy strategy to sample a promising sub-specification from the conjunctive safety specification. It then uses a Simulated Annealing-based falsification algorithm to find violations of the sampled sub-specification for the control system. To evaluate Synthify , we compare it to PSY-TaLiRo , a state-of-the-art and industrial-strength falsification tool, on eight publicly available control systems. On average, Synthify achieves a 83.5% higher success rate in falsification compared to PSY-TaLiRo with the same budget of falsification trials. Additionally, our method is 12.8 \(\times\) faster in finding a single safety violation than the baseline. The safety violations found by Synthify are also more diverse than those found by PSY-TaLiRo , covering 137.7% more sub-specifications.},
  archive      = {J_TOSEM},
  author       = {Jieke Shi and Zhou Yang and Junda He and Bowen Xu and Dongsun Kim and Donggyun Han and David Lo},
  doi          = {10.1145/3715105},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Finding safety violations of AI-enabled control systems through the lens of synthesized proxy programs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing and analyzing the correctness of GitHub copilot’s code suggestions. <em>TOSEM</em>, <em>34</em>(7), 1-32. (<a href='https://doi.org/10.1145/3715108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI programming has become a popular topic in recent years. Code suggestion, with code suggestion being a key capability of AI programming. Copilot, an “AI programmer” that provides code suggestions from natural language descriptions, has been launched by GitHub and OpenAI. By far, Copilot has been widely used by millions of developers. However, little work has systematically evaluated the correctness of Copilot’s suggestions. We conducted an empirical study on all 2,033 LeetCode problems to assess Copilot’s code generation across four mainstream languages: C, Java, JavaScript, and Python. We have found that: (1) 70.0% of problems received at least one correct suggestion, with language-specific rates of 29.7% (C), 57.7% (Java), 54.1% (JavaScript), and 41.0% (Python); (2) correctness decreases as problem difficulty increases, with acceptance rates of 89.3% (easy), 72.1% (medium), and 43.4% (hard); (3) acceptance rates vary across problem domains from 49.5% to 90.1%, while Graph problems challenge C and Python most, and Prefix Sum and Heap challenge Java and JavaScript most; (4) for the incorrect suggestions, we further summarize 17 types of error reasons accounting for their incorrectness and analyzed possible causes for why these errors occur. We believe our study can provide valuable insights into Copilot’s capabilities and limitations.},
  archive      = {J_TOSEM},
  author       = {Ran Mo and Dongyu Wang and Wenjing Zhan and Yingjie Jiang and Yepeng Wang and Yuqi Zhao and Zengyang Li and Yutao Ma},
  doi          = {10.1145/3715108},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Assessing and analyzing the correctness of GitHub copilot’s code suggestions},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HumanEvalComm: Benchmarking the communication competence of code generation for LLMs and LLM agents. <em>TOSEM</em>, <em>34</em>(7), 1-42. (<a href='https://doi.org/10.1145/3715109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. The most recent trend is using LLM-based agents to iterate the code generation process. Based on the observation that top-level software engineers often ask clarifying questions to reduce Ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. For this purpose, we define the communication skills of LLMs as “being able to ask clarifying questions when the description of the code generation problem has issues.” In this study, we restrict these issues to three matters from the software requirement engineering field: inconsistent requirements, ambiguous requirements, and incomplete requirements. By asking probing questions about the requirements of problem descriptions before generating the final code, the challenges of programming with LLMs such as unclear intent specification may be alleviated, resulting to a correct code in the initial iterations. In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues mentioned above, Inconsistency , Ambiguity , and Incompleteness . We then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Code Clarification and Generation Agent (Okanagan), to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. In the evaluation, we introduced an LLM-based evaluator and created Communication Rate and Good Question Rate as the evaluation metrics to represent the ratio of questions asked and questions with good quality in responses. We found that more than 60% of responses from Code LLMs still generate code rather than ask questions when the problem descriptions are manually modified according to different clarification categories. The Pass@1 and Test Pass Rate of most Code LLMs drop by 35% — 52% and by 17% — 35%, respectively, with statistical significance in each category for over 75% numbers. Okanagan, as an LLM agent approach that uses LLM such as ChatGPT 3.5, effectively increases the Communication Rate and Good Question Rate by an absolute 58% and 38%, respectively. Thus, Okanagan boosts Pass@1 and Test Pass Rate by an absolute 8% and 7%, respectively, when the problem descriptions are modified based on given clarification categories. This result indicates the potential for achieving more effective communication capability using LLM agent. Our benchmark and full code are publicly available at https://github.com/jie-jw-wu/human-eval-comm .},
  archive      = {J_TOSEM},
  author       = {Jie JW Wu and Fatemeh H. Fard},
  doi          = {10.1145/3715109},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-42},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {HumanEvalComm: Benchmarking the communication competence of code generation for LLMs and LLM agents},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the robustness of test selection methods for deep neural networks. <em>TOSEM</em>, <em>34</em>(7), 1-26. (<a href='https://doi.org/10.1145/3715693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regularly testing deep learning-powered systems on newly collected data is critical to ensure their reliability, robustness, and efficacy in real-world applications. This process is demanding due to the significant time and human effort required for labeling new data. While test selection methods alleviate manual labor by labeling and evaluating only a subset of data while meeting testing criteria, we observe that such methods with reported promising results are simply evaluated, e.g., testing on original test data. The question arises: are they always reliable? In this article, we explore when and to what extent test selection methods fail. First, we identify potential pitfalls of 11 selection methods based on their construction. Second, we conduct a study to empirically confirm the existence of these pitfalls. Furthermore, we demonstrate how pitfalls can break the reliability of these methods. Concretely, methods for fault detection suffer from data that are: (1) correctly classified but uncertain or (2) misclassified but confident. Remarkably, the test relative coverage achieved by such methods drops by up to 86.85%. Besides, methods for performance estimation are sensitive to the choice of intermediate-layer output. The effectiveness of such methods can be even worse than random selection when using an inappropriate layer.},
  archive      = {J_TOSEM},
  author       = {Qiang Hu and Yuejun Guo and Xiaofei Xie and Maxime Cordy and Wei Ma and Mike Papadakis and Lei Ma and Yves Le Traon},
  doi          = {10.1145/3715693},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Assessing the robustness of test selection methods for deep neural networks},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language model-aware in-context learning for code generation. <em>TOSEM</em>, <em>34</em>(7), 1-33. (<a href='https://doi.org/10.1145/3715908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have shown impressive In-Context Learning (ICL) ability in code generation. LLMs take a prompt context consisting of a few demonstration examples and a new requirement as input, and output new programs without any parameter update. Existing studies have found that the performance of ICL-based code generation heavily depends on the quality of demonstration examples and thus arises research on selecting demonstration examples: given a new requirement, a few demonstration examples are selected from a candidate pool, where LLMs are expected to learn the pattern hidden in these selected demonstration examples. Existing approaches are mostly based on heuristics or randomly selecting examples. However, the distribution of randomly selected examples usually varies greatly, making the performance of LLMs less robust. The heuristics retrieve examples by only considering textual similarities of requirements, leading to sub-optimal performance. To fill this gap, we propose a L arge language model- A ware selection approach for I n-context- L earning-based code generation named LAIL. LAIL uses LLMs themselves to select examples. It requires LLMs themselves to label a candidate example as a positive example or a negative example for a requirement. Positive examples are helpful for LLMs to generate correct programs, while negative examples are trivial and should be ignored. Based on the labeled positive and negative data, LAIL trains a model-aware retriever to learn the preference of LLMs and select demonstration examples that LLMs need. During the inference, given a new requirement, LAIL uses the trained retriever to select a few examples and feed them into LLMs to generate desired programs. We apply LAIL to four widely used LLMs and evaluate it on five code generation datasets. Extensive experiments demonstrate that LAIL outperforms the State-of-the-Art (SOTA) baselines by 11.58%, 3.33%, and 5.07% on CodeGen-Multi-16B, 1.32%, 2.29%, and 1.20% on CodeLlama-34B, and achieves 4.38%, 2.85%, and 2.74% improvements on Text-davinci-003 in terms of Pass@1 at MBJP, MBPP, and MBCPP, respectively. In addition to function-level code generation, LAIL improves the performance of LLMs on DevEval, a repository-level code generation dataset, which achieves 10.04%, 8.12%, and 4.63% improvements compared to the SOTA baselines at Pass@1, 3, and 5 on CodeLlama-7B. Human evaluation further verifies that the generated programs of LAIL are superior in correctness, code quality, and maintainability. Besides, LAIL has satisfactory transferability across different LLMs and datasets, where the retriever learned on one LLM (dataset) can be transferred to other LLMs (datasets).},
  archive      = {J_TOSEM},
  author       = {Jia Li and Chongyang Tao and Jia Li♂ and Ge Li and Zhi Jin and Huangzhao Zhang and Zheng Fang and Fang Liu},
  doi          = {10.1145/3715908},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Large language model-aware in-context learning for code generation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond decision: Android malware description generation through profiling malicious behavior trajectory. <em>TOSEM</em>, <em>34</em>(7), 1-39. (<a href='https://doi.org/10.1145/3715909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware family labels and key features used for the decision-making of Android malware detection models fall short of precise comprehension of malicious behaviors due to their coarse granularity. To solve these problems, in this article, we first introduce the concept of the malicious behavior trajectory ( MBT ) and propose an innovative approach called ProMal . ProMal aims to automatically generate malware descriptions with fine granularity through extracted MBTs from malware for users. Specifically, a labeled dataset of MBTs is constructed through substantial human efforts to build a behavioral knowledge graph ( BxKG ). The BxKG is scalable and can be automatically updated using two strategies to ensure its completeness and timeliness: (1) taking into consideration the evolution of Android SDKs and (2) mining new MBTs by leveraging the widely-used malware datasets. We highlight that the knowledge graph is essential in ProMal , which can reason new MBTs based on existing MBTs because of its structured data representation and semantic relation modeling, and thus helps effectively extract real MBTs in Android malware. We evaluated ProMal on a recent malware dataset where researcher-crafted malware descriptions are available, and the Precision, Recall, and F1-Score of MBT identification based on BxKG reached 96.97%, 91.43%, and 0.94, respectively, outperforming the state-of-the-art approaches. Taking MBTs identified from Android malware as inputs, precise, fine-grained, and human-readable descriptions can be generated using the large language model, whose readability and usability are verified through a user study. The generated descriptions play a significant role in interpreting and comprehending malware behaviors.},
  archive      = {J_TOSEM},
  author       = {Chunlian Wu and Sen Chen and Jiaming Li and Renchao Chai and Lingling Fan and Xiaofei Xie and Ruitao Feng},
  doi          = {10.1145/3715909},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Beyond decision: Android malware description generation through profiling malicious behavior trajectory},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards reliable evaluation of neural program repair with natural robustness testing. <em>TOSEM</em>, <em>34</em>(7), 1-44. (<a href='https://doi.org/10.1145/3716167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated program repair (APR) has recently gained ground, with numerous research efforts being conducted in the area that have been adopted in the industry. One notable class of APR is neural program repair (NPR), which typically employs deep learning techniques that are trained on vast amounts of historical data to fix bugs that have not been seen in the past. To study the true effectiveness of NPR on existing limited datasets, recent work augments the evaluation data by employing semantics-preserving transformations to convert original buggy programs to semantically equivalent ones. Experiments show that NPR techniques are not robust; e.g., NPR cannot repair semantically equivalent counterparts of 20%–35% of bugs that they can repair in the original dataset. However, we found that many of these transformations are unnatural, that are unlikely to occur in real-world scenarios, leading to misleading conclusions about NPR effectiveness and misguide the improvement on unrobust behaviors, which have minimal real-world impact. In this article, we propose shifting the focus of robustness evaluation for NPR techniques towards naturally occurring data transformations. To accomplish this, we first examine the naturalness of semantic-preserving transformations through a two-stage human study. This study includes: (i) interviews with senior software developers to establish concrete criteria for evaluating the naturalness of these transformations and (ii) a survey involving 10 developers to assess the naturalness of 1,178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings show that only 60% of these transformations are considered natural, while 20% are considered unnatural, with strong agreement among the annotators. Moreover, the unnaturalness of these transformations significantly impacts both their applicability to benchmarks and the conclusions drawn from robustness testing. Next, we conduct natural robustness tests on NPR techniques to assess their true effectiveness against real-world data variations. Our experimental results reveal a substantial number of prediction changes in NPR techniques, leading to significant reductions in both plausible and correct patch rates when comparing performance on the original and transformed datasets. Furthermore, we observe notable differences in performance improvements between NPR techniques, suggesting potential biases in the evaluation of NPR introduced by limited datasets. Finally, we explore automating the assessment of transformation naturalness by developing a new naturalness metric, namely RNC, using large language models. This metric effectively evaluates naturalness with an AUC of 0.7, offering a promising direction for automating the naturalness assessment of code transformations.},
  archive      = {J_TOSEM},
  author       = {Thanh Le-Cong and Dat Nguyen and Bach Le and Toby Murray},
  doi          = {10.1145/3716167},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-44},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Towards reliable evaluation of neural program repair with natural robustness testing},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on vulnerability disclosure management of open source software systems. <em>TOSEM</em>, <em>34</em>(7), 1-31. (<a href='https://doi.org/10.1145/3716822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vulnerability disclosure is critical for ensuring the security and reliability of open source software (OSS). However, in practice, many vulnerabilities are reported and discussed on public platforms before being formally disclosed, posing significant risks to vulnerability management. Inadequate vulnerability disclosure can expose users to security threats and severely impact the stability and reliability of software systems. For example, prior work shows that over 21% of CVEs are publicly discussed before a patch is released. Despite its importance, we still lack clarity on the vulnerability disclosure practices adopted by open source communities and the preferences of practitioners regarding vulnerability management. To fill this gap, we analyzed the vulnerability disclosure practices of 8,073 OSS projects spanning from 2017 to 2023. We then conducted an empirical study by surveying practitioners about their preferences and recommendations in vulnerability disclosure management. Finally, we compared the survey results with the actual vulnerability practice observed within the OSS projects. Our results show that while over 80% of practitioners support Coordinated Vulnerability Disclosure (CVD), only 55% of vulnerabilities conform to CVD in practice. Although only 20% of practitioners advocate discussions before disclosure, 42% of vulnerabilities are discussed in issue reports before their disclosure. This study reveals the vulnerability management practices in OSS, provides valuable guidance to OSS owners, and highlights potential directions to improve the security of OSS platforms.},
  archive      = {J_TOSEM},
  author       = {Shuhan Liu and Jiayuan Zhou and Xing Hu and Filipe Roseiro Cogo and Xin Xia and Xiaohu Yang},
  doi          = {10.1145/3716822},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study on vulnerability disclosure management of open source software systems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted suspiciousness and balanced aggregation to boost spectrum-based fault localization of deep learning models. <em>TOSEM</em>, <em>34</em>(7), 1-28. (<a href='https://doi.org/10.1145/3716849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) models have proven to be highly successful and are now essential to our everyday routines. However, DL models, like traditional software, inevitably contain bugs that affect their performance in real-world scenarios. Effective software engineering techniques are necessary to ensure their dependability. In recent years, fault localization methods for DL models have gained significant attention as a valuable tool for improving the reliability of DL models. Owing to the data-driven programming paradigm, traditional fault localization techniques are challenging to apply directly to DL programs. Previous studies have shown that neuron errors within models can lead to abnormal behavior, and they fix the DL model errors from the perspective of neurons. Nonetheless, there remains a significant gap between the DL program statement and model errors. To tackle this problem, this article proposes a novel fault localization method for DL models, named weiGhted sUspIciousness anD balancEd aggRegation ( GUIDER ) that revisits the idea and challenge of spectrum-based fault localization in the context of DL models. For pre-trained DL models, GUIDER utilizes neuron coverage information and test case confidence to compute weighted neuron suspiciousness values and employs balanced aggregation methods to elevate these values from the neuron level to the layer level, which establishes a bridge between the DL model and the DL program, facilitating the developers’ debugging process. We evaluate GUIDER using 161 real model bugs collected from StackOverflow and five state-of-the-art fault localization methods for DL models as baselines. The results indicate that (a) our method successfully localizes 67% of the model bugs by ranking the buggy layer to the first place (i.e., top-1), significantly outperforming all five baselines, and (b) our method maintains an acceptable time overhead compared with all baseline methods.},
  archive      = {J_TOSEM},
  author       = {Wenjie Xu and Yanhui Li and Mingliang Ma and Lin Chen and Yuming Zhou},
  doi          = {10.1145/3716849},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Weighted suspiciousness and balanced aggregation to boost spectrum-based fault localization of deep learning models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study of retrieval-augmented code generation: Challenges and opportunities. <em>TOSEM</em>, <em>34</em>(7), 1-28. (<a href='https://doi.org/10.1145/3717061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code generation aims to automatically generate code snippets of specific programming language according to natural language descriptions. The continuous advancements in deep learning, particularly pre-trained models, have empowered the code generation task to achieve remarkable performance. One main challenge of pre-trained models for code generation is the semantic gap between developers’ natural language requirements and source code. To address the issue, prior studies typically adopt a retrieval-augmented framework for the task, where the similar code snippets collected by a retrieval process can be leveraged to help understand the requirements and provide guidance for the generation process. In a retrieval-augmented framework, similar data can be retrieved from the database using a retrieval algorithm, and original input data can be fused with retrieved data by different fusion strategies. However, there is a lack of systematic study on the application of this framework for code generation, including the impact of the final generated results and the specific usage of the framework. In this article, we choose three popular pre-trained code models, namely CodeGen, UniXcoder, and CodeT5, to assess the impact of the quality and utilization of retrieved code on the retrieval-augmented framework. Our analysis shows that the retrieval-augmented framework is beneficial for improving the performance of the existing pre-trained models. We also provide suggestions on the utilization of the retrieval-augmented code generation framework: BM25 and Sequential Integration Fusion are recommended due to their convenience and superior performance. Sketch Filling Fusion, which extracts a sketch of relevant code, could help the model improve its performance further. Additionally, we conduct experiments to investigate the influence of the retrieval-augmented framework on large language models for code generation, showing the effectiveness of the framework, and we discuss the tradeoff between performance improvement and computational costs in each phase within the framework.},
  archive      = {J_TOSEM},
  author       = {Zezhou Yang and Sirong Chen and Cuiyun Gao and Zhenhao Li and Xing Hu and Kui Liu and Xin Xia},
  doi          = {10.1145/3717061},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study of retrieval-augmented code generation: Challenges and opportunities},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive study of governance issues in decentralized finance applications. <em>TOSEM</em>, <em>34</em>(7), 1-31. (<a href='https://doi.org/10.1145/3717062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized Finance (DeFi) is a prominent application of smart contracts, representing a novel financial paradigm in contrast to centralized finance. While DeFi applications are rapidly emerging on mainstream blockchain platforms, their quality varies greatly, presenting numerous challenges, particularly in terms of their governance mechanisms. In this article, we present a comprehensive study of governance issues in DeFi applications. Initially, we collected 3,165 academic papers and numerous industry reports. After thorough screening, we selected 44 academic papers and 11 industry reports for detailed analysis. Drawing upon insights from industry reports and academic research articles, we develop a taxonomy to categorize these governance issues. We collect and build a dataset of 4,446 audit reports from 17 Web3 security companies, categorizing their governance issues according to our constructed taxonomy. We conducted a thorough analysis of governance issues and identified vulnerabilities in the governance design and implementation, e.g., voting sybil attack and proposal front-running. Our statistical analysis indicates that a significant portion (35.48%) of governance-related issues is classified as severe. Within these, ownership-related problems constitute the largest share (65.38%). Despite DeFi governance being essential for the long-term success of DeFi projects, our data shows that both auditors and development teams have not fully grasped its significance. Based on audit reports, we also analyzed common vulnerabilities and issues in the governance domain. Our research identifies two primary categories of DeFi governance issues: technology-centric and human-centric. Technology-centric issues can be addressed through technology updates and iterations, whereas human-centric issues are influenced not only by the development team’s technical skills but also by their understanding of DeFi governance. Data analysis reveals that design and implementation issues are frequently overlooked; although not directly associated with vulnerabilities, these issues can impact the equitable distribution of project benefits. Furthermore, our analysis of 104 projects’ tokenomics configurations, including 15 collected from DeFi platforms, uncovered 27 inconsistent configurations, with only two projects exhibiting no issues. This suggests that such issues are relatively common. We therefore advise project teams to ensure consistency between their tokenomics design and the actual code. Our study culminates in providing several key practical implications for various DeFi stakeholders, including developers, users, researchers, and regulators, aiming to deepen the understanding of DeFi governance issues and contribute to the robust growth of DeFi systems.},
  archive      = {J_TOSEM},
  author       = {Wei Ma and Chenguang Zhu and Ye Liu and Xiaofei Xie and Yi Li},
  doi          = {10.1145/3717062},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A comprehensive study of governance issues in decentralized finance applications},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward understanding FPGA synthesis tool bugs. <em>TOSEM</em>, <em>34</em>(7), 1-37. (<a href='https://doi.org/10.1145/3718737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field Programmable Gate Array (FPGA) synthesis tools are crucial for hardware development and AI acceleration, and their bugs could compromise hardware reliability and risk downstream applications. However, it remains unknown in understanding the characteristics of these bugs. What are the root causes that trigger bugs in FPGA synthesis tools? What are the characteristics of these bugs? What are the challenges in detecting and addressing them? This article takes the first step toward answering these questions by conducting a comprehensive study of FPGA synthesis tool bugs. We analyze 551 confirmed bugs in both commercial and open-source FPGA synthesis tools, i.e., Vivado, Quartus Prime, and Yosys, covering root causes, symptoms, bug-prone components, fix characteristics, and achieve 17 valuable findings. We find that, on average, around 46.2% of bugs result from Hardware Description Language (HDL) standard non-compliance across the three tools. However, it is hard for current formal validations to fully test HDL standards compliance. Additionally, on average, over 25.8% bugs show domain-specific optimization traits due to inappropriate optimization and mapping. Meanwhile, beyond 28% of bugs trigger unexpected behavior without clear signs, making the formulation of effective test oracles challenging. These findings help addressing FPGA synthesis tool bugs and guide further research.},
  archive      = {J_TOSEM},
  author       = {Yi Zhang and He Jiang and Shikai Guo and Xiaochen Li and Hui Liu and Chongyang Shi},
  doi          = {10.1145/3718737},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Toward understanding FPGA synthesis tool bugs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing smart contract evolution. <em>TOSEM</em>, <em>34</em>(7), 1-22. (<a href='https://doi.org/10.1145/3719004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contracts are programs that permanently store and automatically execute on the blockchain system such as Ethereum. Due to the non-tamperable nature of the underlying blockchain, smart contracts are difficult to update once deployed, which requires redeploying the contracts and migrating the data. It means that the observation of smart contract evolution in the real world makes more sense. Hence, in this article, we conducted the first large-scale empirical study to characterize the evolution of smart contracts in Ethereum. For evolution identification, we presented a contract similarity-based search algorithm, digEvolution, and evaluated its effectiveness with five different search strategies. Then we applied this algorithm to 80,152 on-chain contracts we collected from Ethereum, to dig out the evolution among these contracts. We then explored three research questions. We first studied whether the evolution of smart contracts is common (RQ1), then we studied how do the Gas consumption (RQ2) and the vulnerability (RQ3) of smart contracts vary during the evolution. Our research results show that the evolution of smart contracts is not very common. There are some contract components that have vulnerability but still be called by users. The Gas consumption of most smart contracts doesn’t vary during the evolution, contract is Gas-efficient before and after the evolution. The vulnerability of most smart contracts doesn’t vary during the evolution, both are secure before and after the evolution.},
  archive      = {J_TOSEM},
  author       = {Xiangping Chen and Ziang Qian and Peiyong Liao and Yuan Huang and Changlin Yang and Zibin Zheng},
  doi          = {10.1145/3719004},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Characterizing smart contract evolution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving deep assertion generation via fine-tuning retrieval-augmented pre-trained language models. <em>TOSEM</em>, <em>34</em>(7), 1-23. (<a href='https://doi.org/10.1145/3721128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unit testing validates the correctness of the units of the software system under test and serves as the cornerstone in improving software quality and reliability. To reduce manual efforts in writing unit tests, some techniques have been proposed to generate test assertions automatically, including Deep Learning (DL)-based, retrieval-based, and integration-based ones. Among them, recent integration-based approaches inherit from both DL-based and retrieval-based approaches and are considered state-of-the-art. Despite being promising, such integration-based approaches suffer from inherent limitations, such as retrieving assertions with lexical matching while ignoring meaningful code semantics and generating assertions with a limited training corpus. In this article, we propose a novel Retrieval-Augmented Deep Assertion Generation (RetriGen) approach based on a hybrid assertion retriever and a Pre-Trained Language Model (PLM)-based assertion generator. Given a focal-test, RetriGen first builds a hybrid assertion retriever to search for the most relevant test–assert pair from external codebases. The retrieval process takes both lexical similarity and semantical similarity into account via a token-based and an embedding-based retriever, respectively. RetriGen then treats assertion generation as a sequence-to-sequence task and designs a PLM-based assertion generator to predict a correct assertion with historical test–assert pairs and the retrieved external assertion. Although our concept is general and can be adapted to various off-the-shelf encoder–decoder PLMs, we implement RetriGen to facilitate assertion generation based on the recent CodeT5 model. We conduct extensive experiments to evaluate RetriGen against six state-of-the-art approaches across two large-scale datasets and two metrics. The experimental results demonstrate that RetriGen achieves 57.66% and 73.24% in terms of accuracy and CodeBLEU, outperforming all baselines with an average improvement of 50.66% and 14.14%, respectively. Furthermore, RetriGen generates 1,598 and 1,818 unique correct assertions that all baselines fail to produce, 3.71X and 4.58X more than the most recent approach EditAS . We also demonstrate that adopting other PLMs can provide substantial advancement, e.g., four additionally utilized PLMs outperform EditAS by 7.91%–12.70% accuracy improvement, indicating the generalizability of RetriGen. Overall, our study highlights the promising future of fine-tuning off-the-shelf PLMs to generate accurate assertions by incorporating external knowledge sources.},
  archive      = {J_TOSEM},
  author       = {Quanjun Zhang and Chunrong Fang and Yi Zheng and Yaxin Zhang and Yuan Zhao and Rubing Huang and Jianyi Zhou and Yun Yang and Tao Zheng and Zhenyu Chen},
  doi          = {10.1145/3721128},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-23},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Improving deep assertion generation via fine-tuning retrieval-augmented pre-trained language models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Experimental evaluation of parameter-efficient fine-tuning for software engineering tasks. <em>TOSEM</em>, <em>34</em>(7), 1-34. (<a href='https://doi.org/10.1145/3722107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained models (PTMs) have succeeded in various software engineering (SE) tasks following the “pre-train then fine-tune” paradigm. As fully fine-tuning all parameters of PTMs can be computationally expensive, a potential solution is parameter-efficient fine-tuning (PEFT), which freezes PTMs while introducing extra parameters. Although PEFT methods have been applied to SE tasks, researchers often focus on specific scenarios and lack a comprehensive comparison of PTMs from different aspects such as field, size, and architecture. To fill this gap, we have conducted an empirical study on six PEFT methods, eight PTMs, and four SE tasks. The experimental results reveal several noteworthy findings. For example, model architecture has little impact on PTM performance when using PEFT methods. Additionally, we provide a comprehensive discussion of PEFT methods from three perspectives. First, we analyze the effectiveness and efficiency of PEFT methods. Second, we explore the impact of the scaling factor hyperparameter. Finally, we investigate the application of PEFT methods on the latest open source large language model, Llama 3.2. These findings provide valuable insights to guide future researchers in effectively applying PEFT methods to SE tasks.},
  archive      = {J_TOSEM},
  author       = {Wentao Zou and Zongwen Shen and Qi Li and Jidong Ge and Chuanyi Li and Xiang Chen and Xiaoyu Shen and Liguo Huang and Bin Luo},
  doi          = {10.1145/3722107},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-34},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Experimental evaluation of parameter-efficient fine-tuning for software engineering tasks},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging risk models to improve productivity for effective code un-freeze at scale. <em>TOSEM</em>, <em>34</em>(7), 1-24. (<a href='https://doi.org/10.1145/3722216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changing software is essential to add needed functionality and to fix problems, but changes may introduce defects that lead to outages. This motivates one of the oldest software quality control techniques: a temporary prevention of non-critical changes to the codebase—code freeze. Despite its widespread use in practice, research literature is scant. Historically, code freezes were used as a way to improve software quality by preventing changes during periods before software releases, but code freezes significantly slow down development. To address this shortcoming, we develop and evaluate a family of code un-freeze (permitting changes) strategies tailored to different occasions and products at Meta. They are designed to un-freeze the maximum amount of code without compromising quality. The three primary dimensions to un-freeze involve (a) the exact timing of (and the reasoning behind it) the code freezes, (b) the parts of the organization or the codebase where the codebase freeze is applied to, and (c) the method of screening of the code diffs during the code freeze with the aim to allow low risk diffs and prevent only the most risky diffs. To operationalize the drivers of outages, we consider the entire network of interdependencies among different parts of the source code, the engineers that modify the code, code complexity, and the coordination dependencies and authors’ expertise. Since the code freeze is a balancing act between reducing outages and allowing software development to proceed unimpeded, the performance of the various approaches to code un-freeze is evaluated based on the fraction of flagged/gated changes to measure overhead and the fraction of all outage-causing changes contained within the set of flagged set of changes to measure the ability of the code un-freeze to delay (or prevent) outages. We found that taking into account the risk posed by modifying individual files and the properties of the change we could un-freeze 2 and 2.5 times more changes correspondingly. The change level model is used by Meta in production. For example, during the winter 2023 code freeze, we see that only 16% of changes are gated. Although 42% more changes landed (were integrated into the codebase) compared to the prior year, there was a 52% decrease in outages. This reduction meant less impact on users and less strain on engineers during the holiday period. The risk model has been enormously effective at allowing low-risk changes to proceed while gating high-risk changes and reducing outages.},
  archive      = {J_TOSEM},
  author       = {Audris Mockus and Rui Abreu and Peter C. Rigby and David Amsallem and Parveen Bansal and Kaavya Chinniah and Brian Ellis and Peng Fan and Jun Ge and Bingjie He and Kelly Hirano and Sahil Kumar and Ajay Lingapuram and Andrew Loe and Megh Mehta and Venus Montes and Maher Saba and Gursharan Singh and Matt Steiner and Weiyan Sun and Siri Uppalapati and Nachiappan Nagappan},
  doi          = {10.1145/3722216},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Leveraging risk models to improve productivity for effective code un-freeze at scale},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wenwang: Toward effectively generating code beyond standalone functions via generative pre-trained models. <em>TOSEM</em>, <em>34</em>(7), 1-27. (<a href='https://doi.org/10.1145/3725213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. After being pre-trained on a large-scale corpus of code, a model is further fine-tuned with datasets specifically for the target downstream task, e.g., generating code from natural language description. The target code being generated can be classified into two types: a standalone function, i.e., a function that invokes or accesses only built-in functions and standard libraries, and a non-standalone function, i.e., a function that invokes or accesses user-defined functions or third-party libraries. To effectively generate code especially non-standalone functions (largely ignored by existing work), in this article, we present Wenwang, an approach to improving the capability of a pre-trained model on generating code beyond standalone functions. Wenwang consists of two components: a fine-tuning dataset named WenwangData and a fine-tuned model named WenwangCoder. Compared with existing fine-tuning datasets, WenwangData additionally covers non-standalone functions. Besides the docstring and code snippet for a function, WenwangData also includes its contextual information collected via program analysis. Based on PanGu-Coder, we produce WenwangCoder by fine-tuning PanGu-Coder on WenwangData with our context-aware fine-tuning technique so that the contextual information can be fully leveraged during code generation. On CoderEval and HumanEval, WenwangCoder outperforms three state-of-the-art models with similar parameter sizes (at the scale of around 300 M), namely CodeGen, PanGu-Coder, and PanGu-FT. Although WenwangCoder does not outperform ChatGPT on HumanEval, WenwangCoder with smaller model parameter sizes can achieve similar effects to ChatGPT on CoderEval. Our experimental results also shed light on a number of promising optimization directions based on existing pre-trained models.},
  archive      = {J_TOSEM},
  author       = {Hao Yu and Bo Shen and Jiaxin Zhang and Shaoxin Lin and Lin Li and Guangtai Liang and Ying Li and Qianxiang Wang and Tao Xie},
  doi          = {10.1145/3725213},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Wenwang: Toward effectively generating code beyond standalone functions via generative pre-trained models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomous driving system testing via diversity-oriented driving scenario exploration. <em>TOSEM</em>, <em>34</em>(7), 1-28. (<a href='https://doi.org/10.1145/3727875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing Autonomous Driving Systems (ADS) is critical for validating their safety in operational environments. High-fidelity simulators enable the testing of ADS through virtual driving scenarios, especially those that are hazardous to replicate in real-world settings. However, existing testing approaches suffer from inadequate coverage of real-world traffic situations due to over-simplified modeling of vehicle movements (e.g., insufficient diversity in driving styles), resulting in undetected critical ADS failures. In this article, we propose a testing framework to discover diverse failures of ADS in driving scenarios that embody real-world traffic complexity. The framework leverages advanced traffic simulation methods to encode vehicle movements and generates realistic yet safety-critical driving scenarios for ADS by mutating vehicle movements. To efficiently explore driving scenarios that pose different challenges for ADS and expose diverse ADS failures, this framework further leverages a dynamic prioritization mechanism that prioritizes vehicle movements likely to trigger unique ADS behaviors. Specifically, we propose a method to estimate the possibility based on encoded vehicle movements. We implement this framework and evaluate it with three representative ADS from the famous CARLA Leaderboard. Empirical evaluation demonstrates that the proposed approach discovers more unique failures of ADS than existing testing frameworks.},
  archive      = {J_TOSEM},
  author       = {Xinyu Ji and Lei Xue and Zhijian He and Xiapu Luo},
  doi          = {10.1145/3727875},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Autonomous driving system testing via diversity-oriented driving scenario exploration},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy in chatbot conversation-driven development: A comprehensive review and requirements proposal. <em>TOSEM</em>, <em>34</em>(7), 1-44. (<a href='https://doi.org/10.1145/3730578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring data privacy is a major challenge for software developers, especially in chatbots, where balancing privacy protection with response quality is key, given the need for conversation-driven development and data protection regulations. This research identifies privacy requirements and techniques for chatbot development through a literature review, privacy policy analysis, and a practitioner survey. The methodology includes a Systematic Literature Review (SLR), an adapted Gray Literature Review (GLR), privacy requirement formulation, and validation via a survey. Based on the SLR and GLR, eight privacy requirements are proposed, covering personal information protection, user authentication, access control, secure communication, database safety, user rights empowerment, decentralized storage, and reliable infrastructure. Survey results highlight foundational measures like secure communication and scalable infrastructures as priorities, while advanced measures such as decentralized storage or privacy rights implementation scored lower due to complexity and cost. Practitioners also stressed clarity and verifiability, citing gaps in definitions, examples, and validation criteria as challenges to adoption.},
  archive      = {J_TOSEM},
  author       = {Geovana Ramos Sousa Silva and Edna Dias Canedo},
  doi          = {10.1145/3730578},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-44},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Privacy in chatbot conversation-driven development: A comprehensive review and requirements proposal},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering agile-based generative software development through human-AI teamwork. <em>TOSEM</em>, <em>34</em>(6), 1-46. (<a href='https://doi.org/10.1145/3702987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In software development, the raw requirements proposed by users are frequently incomplete, which impedes the complete implementation of software functionalities. With the emergence of large language models, the exploration of generating software through user requirements has attracted attention. Recent methods with the top-down waterfall model employ a questioning approach for requirement completion, attempting to explore further user requirements. However, users, constrained by their domain knowledge, result in a lack of effective acceptance criteria during the requirement completion, failing to fully capture the implicit needs of the user. Moreover, the cumulative errors of the waterfall model can lead to discrepancies between the generated code and user requirements. The Agile methodologies reduce cumulative errors of the waterfall model through lightweight iteration and collaboration with users, but the challenge lies in ensuring semantic consistency between user requirements and the code generated by the agent. To address these challenges, we propose AgileGen, an agile-based generative software development through human-AI teamwork. Unlike existing questioning agents, AgileGen adopts a novel collaborative approach that breaks free from the constraints of domain knowledge by initiating the end-user perspective to complete the acceptance criteria. By introducing the Gherkin language, AgileGen attempts for the first time to use testable requirement descriptions as a bridge for semantic consistency between requirements and code, aiming to ensure that software products meet actual user requirements by defining user scenarios that include acceptance criteria. Additionally, we innovate in the human-AI teamwork model, allowing users to participate in decision-making processes they do well and significantly enhancing the completeness of software functionality. To ensure semantic consistency between requirements and generated code, we derive consistency factors from Gherkin to drive the subsequent software code generation. Finally, to improve the reliability of user scenarios, we also introduce a memory pool mechanism, collecting user decision-making scenarios and recommending them to new users with similar requirements. AgileGen, as a user-friendly interactive system, significantly outperformed existing best methods by 16.4% and garnered higher user satisfaction.},
  archive      = {J_TOSEM},
  author       = {Sai Zhang and Zhenchang Xing and Ronghui Guo and Fangzhou Xu and Lei Chen and Zhaoyuan Zhang and Xiaowang Zhang and Zhiyong Feng and Zhiqiang Zhuang},
  doi          = {10.1145/3702987},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-46},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Empowering agile-based generative software development through human-AI teamwork},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified and split symbolic execution for exposing semantic differences. <em>TOSEM</em>, <em>34</em>(6), 1-27. (<a href='https://doi.org/10.1145/3705299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software evolution is an important activity during a software development lifecycle. Understanding semantic differences between two versions of a software system is a crucial yet challenging task, especially in many safety critical sectors. Consequently, various techniques have been proposed to check semantic differences between a program and its evolution. But, many current techniques are still far from being satisfactory in terms of the accuracy and efficiency. In this article, we propose a novel framework, called US 2 E , which can efficiently and effectively generate the minimal number of test cases that reveal as many semantic differences across two versions as possible. Specifically, given a unified control flow graph that denotes two versions of a program, US 2 E executes as many common nodes as possible and leaves execution of non-common nodes separately in a single concolic execution instance. We evaluate US 2 E on 86 pairs of C programs from 4 benchmarks, and experimental results show that US 2 E can efficiently and effectively generate test cases demonstrating the semantic differences across 2 versions, with better performance than 6 baseline tools.},
  archive      = {J_TOSEM},
  author       = {Hongliang Liang and Luming Yin and Wenying Hu and Yuxiang Li and Wuwei Shen},
  doi          = {10.1145/3705299},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Unified and split symbolic execution for exposing semantic differences},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond cohesion and coupling: Integrating control flow in software modularization process for better code comprehensibility. <em>TOSEM</em>, <em>34</em>(6), 1-29. (<a href='https://doi.org/10.1145/3707452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As software systems evolve to meet the changing needs of users, understanding the source code becomes a critical step in the process. Clustering techniques, also known as modularization techniques, offer a solution to breaking down complex source code into smaller, more manageable parts. This facilitates improved analysis and understanding of the software’s structure. However, the effectiveness of clustering algorithms in code understanding heavily relies on the chosen criteria. While existing methods typically consider cohesion, coupling, and balance between clusters, we argue that these criteria alone may not fully satisfy one of the primary objectives of clustering, which is to enhance understanding. This is because spaghetti-like structures can be created even when these criteria are satisfied. To address this issue, we introduce two new criteria incorporating program control flow to regulate cluster dependencies. By controlling the uniformity of input and output directions, as well as the distribution of inputs and outputs, clustering algorithms can generate clusters that are more developer-friendly and easier to comprehend. We provide intuitive explanations and real-world projects to demonstrate the effectiveness of our approach and also incorporate feedback from academics and expert programmers. This article reveals that integrating these new criteria into existing clustering algorithms enables developers to gain deeper insights into the structure of software systems. This, in turn, leads to better design decisions and improved developer understanding of the source code.},
  archive      = {J_TOSEM},
  author       = {Babak Pourasghar and Habib Izadkhah and Maryam Akhtari},
  doi          = {10.1145/3707452},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Beyond cohesion and coupling: Integrating control flow in software modularization process for better code comprehensibility},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TEESlice: Protecting sensitive neural network models in trusted execution environments when attackers have pre-trained models. <em>TOSEM</em>, <em>34</em>(6), 1-49. (<a href='https://doi.org/10.1145/3707453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trusted Execution Environments (TEEs) are used to safeguard on-device models. However, directly employing TEEs to secure the entire DNN model is challenging due to the limited computational speed. Utilizing GPU can accelerate DNN’s computation speed but widely available commercial GPUs usually lack security protection. To this end, scholars introduce TEE-Shielded DNN Partition (TSDP), a method that protects privacy-sensitive weights within TEEs and offloads insensitive weights to GPUs. Nevertheless, current methods do not consider the presence of a knowledgeable adversary who can access abundant publicly available pre-trained models and datasets. This article investigates the security of the existing methods against such a knowledgeable adversary and reveals their inability to fulfill their security promises. Consequently, we introduce a novel partition before training strategy, which effectively separates privacy-sensitive weights from other components of the model. Our evaluation demonstrates that our approach can offer full model protection with a computational cost reduced by a factor of 10. In addition to traditional CNN models, we also demonstrate the scalability to large language models. Our approach can compress the private functionalities of the large language model to lightweight slices and achieve the same level of protection as the shielding-whole-model baseline.},
  archive      = {J_TOSEM},
  author       = {Ding Li and Ziqi Zhang and Mengyu Yao and Yifeng Cai and Yao Guo and Xiangqun Chen},
  doi          = {10.1145/3707453},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-49},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {TEESlice: Protecting sensitive neural network models in trusted execution environments when attackers have pre-trained models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging data characteristics for bug localization in deep learning programs. <em>TOSEM</em>, <em>34</em>(6), 1-29. (<a href='https://doi.org/10.1145/3708473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning (DL) is a class of machine learning algorithms that are used in a wide variety of applications. Like any software system, DL programs can have bugs. To support bug localization in DL programs, several tools have been proposed in the past. As most of the bugs that occur due to improper model structure known as structural bugs lead to inadequate performance during training, it is challenging for developers to identify the root cause and address these bugs. To support bug detection and localization in DL programs, in this article, we propose Theia, which detects and localizes structural bugs in DL programs. Unlike the previous works, Theia considers the training dataset characteristics to automatically detect bugs in DL programs developed using two DL libraries, Keras and PyTorch . Since training the DL models is a time-consuming process, Theia detects these bugs at the beginning of the training process and alerts the developer with informative messages containing the bug’s location and actionable fixes which will help them to improve the structure of the model. We evaluated Theia on a benchmark of 40 real-world buggy DL programs obtained from Stack Overflow . Our results show that Theia successfully localizes 57/75 structural bugs in 40 buggy programs, whereas NeuraLint, a state-of-the-art approach capable of localizing structural bugs before training localizes 17/75 bugs.},
  archive      = {J_TOSEM},
  author       = {Ruchira Manke and Mohammad Wardat and Foutse Khomh and Hridesh Rajan},
  doi          = {10.1145/3708473},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Leveraging data characteristics for bug localization in deep learning programs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated quantum protocol verification based on concurrent dynamic quantum logic. <em>TOSEM</em>, <em>34</em>(6), 1-36. (<a href='https://doi.org/10.1145/3708475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While constructing practical quantum computers by big companies remains a challenge, the application of quantum communication and cryptography has made remarkable progress. Therefore, it is crucial to verify quantum protocols before they can be trusted in safety and security-critical applications. We have proposed Basic Dynamic Quantum Logic (BDQL) to formalize and verify sequential models of quantum protocols with a support tool developed in Maude. However, BDQL does not support concurrency in its formalization. This article introduces Concurrent Dynamic Quantum Logic (CDQL), an extension of BDQL, to formalize and verify concurrent models of quantum protocols. CDQL is more expressive than BDQL by considering concurrent behavior and communication among participants in quantum protocols. Since CDQL is a conservative extension of BDQL, we extend the syntax of BDQL to CDQL and make a transformation from CDQL to BDQL without interrupting the semantics of BDQL. We also extend the implementation of BDQL to support CDQL, making a new support tool in Maude. The new support tool is equipped with a lazy rewriting strategy to make the verification process significantly faster. Several quantum communication protocols are successfully formalized and verified in BDQL/CDQL, demonstrating the effectiveness of our automated approach and tool in verifying quantum protocols.},
  archive      = {J_TOSEM},
  author       = {Canh Minh Do and Tsubasa Takagi and Kazuhiro Ogata},
  doi          = {10.1145/3708475},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automated quantum protocol verification based on concurrent dynamic quantum logic},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explaining explanations: An empirical study of explanations in code reviews. <em>TOSEM</em>, <em>34</em>(6), 1-30. (<a href='https://doi.org/10.1145/3708518'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code reviews are central for software quality assurance. Ideally, reviewers should explain their feedback to enable authors of code changes to understand the feedback and act accordingly. Different developers might need different explanations in different contexts. Therefore, assisting this process first requires understanding the types of explanations reviewers usually provide. The goal of this article is to study the types of explanations used in code reviews and explore the potential of Large Language Models (LLMs), specifically ChatGPT, in generating these specific types. We extracted 793 code review comments from Gerrit and manually labeled them based on whether they contained a suggestion, an explanation, or both. Our analysis shows that 42% of comments only include suggestions without explanations. We categorized the explanations into seven distinct types including rule or principle, similar examples, and future implications. When measuring their prevalence, we observed that some explanations are used differently by novice and experienced reviewers. Our manual evaluation shows that, when the explanation type is specified, ChatGPT can correctly generate the explanation in 88 out of 90 cases. This foundational work highlights the potential for future automation in code reviews, which can assist developers in sharing and obtaining different types of explanations as needed, thereby reducing back-and-forth communication.},
  archive      = {J_TOSEM},
  author       = {Ratnadira Widyasari and Ting Zhang and Abir Bouraffa and Walid Maalej and David Lo},
  doi          = {10.1145/3708518},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Explaining explanations: An empirical study of explanations in code reviews},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature review of multi-label learning in software engineering. <em>TOSEM</em>, <em>34</em>(6), 1-48. (<a href='https://doi.org/10.1145/3708532'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we provide the first systematic literature review of the intersection of two research areas: Multi-Label Learning (MLL) and Software Engineering (SE). We refer to this intersection as MLL4SE. In recent years, MLL problems have increased in many applications and research areas because real-world datasets often have a multi-label nature. For multi-label data, simplifying the assumption of traditional classification approaches that an instance can only be associated with one class only leads to worse accuracy. Thus, a better match of methods and assumptions about the data is required. We identified 50 primary studies in our systematic literature review in the MLL4SE domain. Based on this review, we identified six main SE application domains where MLL has been applied. These domains include Software Requirement Engineering, Issue Tracking and Management, Community and Knowledge Management, API Usage and Management, Code Quality and Maintenance, and Mobile Application Development . We summarized the methods used and the data nature of the MLL4SE applications. Moreover, we separately provide taxonomies of future work directions from machine learning and SE perspectives. In general, we highlight current trends, research gaps, and shortcomings.},
  archive      = {J_TOSEM},
  author       = {Joonas Hämäläinen and Teerath Das and Tommi Mikkonen},
  doi          = {10.1145/3708532},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-48},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A systematic literature review of multi-label learning in software engineering},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MeDeT: Medical device digital twins creation with few-shot meta-learning. <em>TOSEM</em>, <em>34</em>(6), 1-36. (<a href='https://doi.org/10.1145/3708534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing healthcare Internet of Things (IoT) applications at system and integration levels necessitates integrating numerous medical devices. Challenges of incorporating medical devices are: (i) their continuous evolution, making it infeasible to include all device variants and (ii) rigorous testing at scale requires multiple devices and their variants, which is time-intensive, costly, and impractical. Our collaborator, Oslo City’s health department, faced these challenges in developing automated test infrastructure, which our research aims to address. In this context, we propose a meta-learning-based approach ( MeDeT ) to generate digital twins (DTs) of medical devices and adapt DTs to evolving devices. We evaluate MeDeT in Oslo City’s context using five widely used medical devices integrated with a real-world healthcare IoT application. Our evaluation assesses MeDeT ’s ability to generate and adapt DTs across various devices and versions using different few-shot methods, the fidelity of these DTs, the scalability of operating 1,000 DTs concurrently, and the associated time costs. Results show that MeDeT can generate DTs with over 96% fidelity, adapt DTs to different devices and newer versions with reduced time cost (around one minute), and operate 1,000 DTs in a scalable manner while maintaining the fidelity level, thus serving in place of physical devices for testing.},
  archive      = {J_TOSEM},
  author       = {Hassan Sartaj and Shaukat Ali and Julie Marie Gjøby},
  doi          = {10.1145/3708534},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {MeDeT: Medical device digital twins creation with few-shot meta-learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MiniScope: Automated UI exploration and privacy inconsistency detection of MiniApps via two-phase iterative hybrid analysis. <em>TOSEM</em>, <em>34</em>(6), 1-29. (<a href='https://doi.org/10.1145/3709351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of MiniApps, operating within larger SuperApps, has revolutionized user experiences by offering a wide range of services without the need for individual app downloads. However, this convenience has raised significant privacy concerns, as these MiniApps often require access to sensitive data, potentially leading to privacy violations. Despite existing privacy regulations and platform guidelines, there is a lack of effective mechanisms to safeguard user privacy fully. To address this critical gap, we introduce MiniScope , a novel two-phase hybrid analysis approach, specifically designed for the MiniApp environment. This approach overcomes the limitations of existing static analysis techniques by incorporating UI transition states analysis, cross-package callback control flow resolution, and automated iterative UI exploration. This allows for a comprehensive understanding of MiniApps’ privacy practices, addressing the unique challenges of sub-package loading and event-driven callbacks. Our empirical evaluation of over 120K MiniApps using MiniScope demonstrates its effectiveness in identifying privacy inconsistencies. The results reveal significant issues, with 5.7% of MiniApps over-collecting private data and 33.4% overclaiming data collection. We have responsibly disclosed our findings to 2,282 developers, receiving 44 acknowledgments. These findings emphasize the urgent need for more precise privacy monitoring systems and highlight the responsibility of SuperApp operators to enforce stricter privacy measures.},
  archive      = {J_TOSEM},
  author       = {Shenao Wang and Yuekang Li and Kailong Wang and Yi Liu and Hui Li and Yang Liu and Haoyu Wang},
  doi          = {10.1145/3709351},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {MiniScope: Automated UI exploration and privacy inconsistency detection of MiniApps via two-phase iterative hybrid analysis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on common sense-violating bugs in mobile apps. <em>TOSEM</em>, <em>34</em>(6), 1-26. (<a href='https://doi.org/10.1145/3709356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile applications are widely used by billions of users in their daily work and life. Such GUI software is prone to bugs, potentially degrading user experience. Notably, many bugs in mobile apps are reported by end-users who cannot access the requirements of the app or test cases accompanied by explicitly specified test oracles. It may suggest that such bugs are not identified in the traditional way, i.e., by comparing the actual behaviors of the apps against their expected behaviors explicitly specified in the requirements or test cases. Instead, such bugs are often identified by comparing the actual behaviors against users’ common knowledge of apps, noted as common sense. We refer to such bugs as common sense-violating bugs. Although it is well-known that common sense-violating bugs are common in mobile apps, it remains unclear how popular they are and what kind of common sense principles are violated by them, let alone the relationship among the violated common sense principles. To this end, in this paper, we conduct the first large-scale empirical study on common sense-violating bugs in open-source mobile apps. We manually analyzed 2,808 real-world bug reports across 948 open-sourced mobile apps on GitHub. Our analysis results suggest that 1,006 (35.8%) out of the 2,808 bugs pertain to common sense-violating bugs. From those common sense-violating bugs, we identified a set of common sense principles violated by the buggy behaviors, and built a taxonomy for the common sense principles. Such principles fall into three categories: UI content-related common sense principles, UI layout-related common sense principles, and interaction-related common sense principles. By analyzing the frequency of the common sense principles being violated, we observed that a small set of common sense principles were frequently violated by the majority of common sense-violating bugs: 18 common sense principles, accounting for only 5% of the violated common sense principles, were violated by more than half of the common sense-violating bugs. These findings suggest that identifying the most frequent common sense-violating bugs could be achieved by using a small set of critical common sense principles, which may significantly reduce the cost of common sense-based bug detection. Finally, to demonstrate the feasibility of automated bug detection with common sense-based test oracles, we propose an automated approach to validating whether a given test run violates the most frequently violated common sense principle: No raw error message. Our evaluation results suggest that the automated approach is accurate, whose precision and recall are 91.3% and 91.6%, respectively.},
  archive      = {J_TOSEM},
  author       = {Fu Fan and Yanjie Jiang and Tianyi Chen and Hengshun Zhang and Yuxia Zhang and Nan Niu and Hui Liu},
  doi          = {10.1145/3709356},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study on common sense-violating bugs in mobile apps},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the capabilities of LLMs for code-change-related tasks. <em>TOSEM</em>, <em>34</em>(6), 1-36. (<a href='https://doi.org/10.1145/3709358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developers deal with code-change-related tasks daily, e.g., reviewing code. Pre-trained code and code-change-oriented models have been adapted to help developers with such tasks. Recently, large language models (LLMs) have shown their effectiveness in code-related tasks. However, existing LLMs for code focus on general code syntax and semantics rather than the differences between two code versions. Thus, it is an open question how LLMs perform on code-change-related tasks. To answer this question, we conduct an empirical study using \(>\) 1B parameters LLMs on three code-change-related tasks, i.e., code review generation, commit message generation, and just-in-time comment update, with in-context learning (ICL) and parameter-efficient fine-tuning (PEFT, including LoRA and prefix-tuning). We observe that the performance of LLMs is poor without examples and generally improves with examples, but more examples do not always lead to better performance. LLMs tuned with LoRA have comparable performance to the state-of-the-art small pre-trained models. Larger models are not always better, but Llama 2 and Code Llama families are always the best. The best LLMs outperform small pre-trained models on the code changes that only modify comments and perform comparably on other code changes. We suggest future work should focus more on guiding LLMs to learn the knowledge specific to the changes related to code rather than comments for code-change-related tasks.},
  archive      = {J_TOSEM},
  author       = {Lishui Fan and Jiakun Liu and Zhongxin Liu and David Lo and Xin Xia and Shanping Li},
  doi          = {10.1145/3709358},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Exploring the capabilities of LLMs for code-change-related tasks},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RuMono: Fuzz driver synthesis for rust generic APIs. <em>TOSEM</em>, <em>34</em>(6), 1-28. (<a href='https://doi.org/10.1145/3709359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzing is a popular technique for detecting bugs, which can be extended to libraries by constructing executables that call library APIs, known as fuzz drivers. Automated fuzz driver synthesis has been an important research topic in recent years since it can facilitate the library fuzzing process. Nevertheless, existing approaches generally ignore generic APIs or simply treat them as non-generic APIs. As a result, they cannot generate effective fuzz drivers for generic APIs. This article explores the challenge of automating fuzz driver synthesis for Rust libraries with generic APIs. The problem is essential because Rust prioritizes security and generic APIs are widely employed in Rust libraries. We propose a novel approach and develop a prototype, RuMono, to tackle the problem. Our approach initially infers the API reachability from the generic API dependency graph, discovering the reachable and valid monomorphic APIs within the library. Further, we apply a similarity-based filter to eliminate redundant monomorphic APIs. Experimental results from 29 popular open source libraries demonstrate that RuMono can achieve promising generic API coverage with a low rate of invalid fuzz drivers. Besides, we have identified 23 previously unknown bugs in these libraries, with 18 related to generic APIs.},
  archive      = {J_TOSEM},
  author       = {Yehong Zhang and Jun Wu and Hui Xu},
  doi          = {10.1145/3709359},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {RuMono: Fuzz driver synthesis for rust generic APIs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How are we detecting inconsistent method names? an empirical study from code review perspective. <em>TOSEM</em>, <em>34</em>(6), 1-27. (<a href='https://doi.org/10.1145/3711901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proper naming of methods can make program code easier to understand, and thus enhance software maintainability. Yet, developers may use inconsistent names due to poor communication or a lack of familiarity with conventions within the software development lifecycle. To address this issue, much research effort has been invested into building automatic tools that can check for method name inconsistency and recommend consistent names. However, existing datasets generally do not provide precise details about why a method name was deemed improper and required to be changed. Such information can give useful hints on how to improve the recommendation of adequate method names. Accordingly, we construct a sample method-naming benchmark, ReName4J, by matching name changes with code reviews. We then present an empirical study on how state-of-the-art techniques perform in detecting or recommending consistent and inconsistent method names based on ReName4J. The main purpose of the study is to reveal a different perspective based on reviewed names rather than proposing a complete benchmark. We find that the existing techniques underperform on our review-driven benchmark, both in inconsistent checking and the recommendation. We further identify potential biases in the evaluation of existing techniques, which future research should consider thoroughly.},
  archive      = {J_TOSEM},
  author       = {Kisub Kim and Xin Zhou and Dongsun Kim and Julia Lawall and Kui Liu and Tegawende F. Bissyande and Jacques Klein and Jaekwon Lee and David Lo},
  doi          = {10.1145/3711901},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {How are we detecting inconsistent method names? an empirical study from code review perspective},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding information leaks with information flow fuzzing. <em>TOSEM</em>, <em>34</em>(6), 1-18. (<a href='https://doi.org/10.1145/3711902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present information flow fuzzing , an approach that guides fuzzers towards detecting information leaks — information that reaches a third party, but should not. The approach detects information flow by means of mutations , checking whether and how mutations to (secret) data affect output and execution: — First, the fuzzer uses information flow as a leak oracle. To this end, for each input, the fuzzer first runs the program regularly. Then, it mutates secret data such as a certificate or a password, and re-runs the program giving the original input. If the output changes, the fuzzer has revealed an information leak. — Second, the fuzzer uses information flow as guidance. The fuzzer not only maximizes coverage, but also changes in coverage and changes in data between the two runs. This increases the likelihood that a mutation will spread to the output. First, the fuzzer uses information flow as a leak oracle. To this end, for each input, the fuzzer first runs the program regularly. Then, it mutates secret data such as a certificate or a password, and re-runs the program giving the original input. If the output changes, the fuzzer has revealed an information leak. Second, the fuzzer uses information flow as guidance. The fuzzer not only maximizes coverage, but also changes in coverage and changes in data between the two runs. This increases the likelihood that a mutation will spread to the output. We have implemented a tool named flowfuzz that wraps around a C program under test to provide information flow based oracles and guidance, allowing for integration with all common fuzzers for C programs. Using a set of subjects representing common information leaks, we investigate (1) whether oracles based on information flow detect information leaks in our subjects; and (2) whether guidance based on information flow improves over standard coverage guidance. All data and tools are available for replication and reproduction.},
  archive      = {J_TOSEM},
  author       = {Bernd Gruner and Clemens-Alexander Brust and Andreas Zeller},
  doi          = {10.1145/3711902},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Finding information leaks with information flow fuzzing},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Obfuscated clone search in JavaScript based on reinforcement subsequence learning. <em>TOSEM</em>, <em>34</em>(6), 1-28. (<a href='https://doi.org/10.1145/3711903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding similar code is important for software engineering, defense of intellectual property, and security, and one of the increasingly common ways adversaries use to defeat the detection of similar code is through obfuscations such as code transformation and scattering the code they wish to hide among long sequences. Moving code far enough apart poses a specific challenge for solutions with localized features (e.g., n-grams), or attention mechanisms as the code parts are distributed beyond the local context window. We introduce a neural network solution pattern called “Cybertron” that addresses this problem by utilizing reinforcement learning to train a code abstraction and summarization function; this converts arbitrarily long code into fixed-length real vectors in a way that is optimized for similarity search. The key to the design is the smart selection of important elements of the code and abstraction to preserve semantic function while minimizing syntactic feature information. We evaluated the approach on a three-challenge benchmark of obfuscated JavaScript, a scripting language that is commonly obfuscated and for which code-mixing is a rising challenge. The evaluation shows our approach identifies obfuscated code within even large scripts with an AUC of 78%, which outperforms current state-of-the-art sequence models by 7–35%.},
  archive      = {J_TOSEM},
  author       = {Leo Song and Steven H. H. Ding and Yuan Tian and Li Tao Li and Weihan Ou and Philippe Charland and Andrew Walenstein},
  doi          = {10.1145/3711903},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Obfuscated clone search in JavaScript based on reinforcement subsequence learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding information leaks with information flow Fuzzing—RCR report. <em>TOSEM</em>, <em>34</em>(6), 1-4. (<a href='https://doi.org/10.1145/3711905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is the Replicated Computational Results (RCR) report for our ACM TOSEM paper, “ Finding Information Leaks with Information Flow Fuzzing ,” in which we propose information flow fuzzing . This approach directs fuzzers toward detecting information leaks . We introduce a novel leak oracle and employ information flow as guidance for the fuzzer to identify information leaks effectively. As part of this RCR report, we provide a replication package that enables the complete replication of all our results and simplifies the reuse of our FLOWFUZZ fuzzer.},
  archive      = {J_TOSEM},
  author       = {Bernd Gruner and Clemens-Alexander Brust and Andreas Zeller},
  doi          = {10.1145/3711905},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-4},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Finding information leaks with information flow Fuzzing—RCR report},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introducing interactions in multi-objective optimization of software architectures. <em>TOSEM</em>, <em>34</em>(6), 1-39. (<a href='https://doi.org/10.1145/3712185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software architecture optimization aims to enhance non-functional attributes like performance and reliability while meeting functional requirements. Multi-objective optimization employs metaheuristic search techniques, such as genetic algorithms, to explore feasible architectural changes and propose alternatives to designers. However, this resource-intensive process may not always align with practical constraints. This study investigates the impact of designer interactions on multi-objective software architecture optimization. Designers can intervene at intermediate points in the fully automated optimization process, making choices that guide exploration towards more desirable solutions. Through several controlled experiments as well as an initial user study (14 subjects), we compare this interactive approach with a fully automated optimization process, which serves as a baseline. The findings demonstrate that designer interactions lead to a more focused solution space, resulting in improved architectural quality. By directing the search toward regions of interest, the interaction uncovers architectures that remain unexplored in the fully automated process. In the user study, participants found that our interactive approach provides a better trade-off between sufficient exploration of the solution space and the required computation time.},
  archive      = {J_TOSEM},
  author       = {Vittorio Cortellessa and Jorge Andrés Diaz-Pace and Daniele Di Pompeo and Sebastian Frank and Pooyan Jamshidi and Michele Tucci and André van Hoorn},
  doi          = {10.1145/3712185},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Introducing interactions in multi-objective optimization of software architectures},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). You don’t have to say where to edit! jLED—Joint learning to localize and edit source code. <em>TOSEM</em>, <em>34</em>(6), 1-27. (<a href='https://doi.org/10.1145/3712187'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to edit code automatically is becoming more and more feasible. Thanks to recent advances in Neural Machine Translation (NMT) , various case studies are being investigated where patches are automatically produced and assessed either automatically (using test suites) or by developers themselves. An appealing setting remains when the developer must provide a natural language input of the requirement for the code change. A recent proof of concept in the literature showed that it is indeed feasible to translate these natural language requirements into code changes. A recent advancement, MODIT, has shown promising results in code editing by leveraging natural language, code context, and location information as input. However, it struggles when location information is unavailable. While several studies have demonstrated the ability to edit source code without explicitly specifying the edit location, they still tend to generate edits with less accuracy at the line level. In this work, we address the challenge of generating code edits without precise location information, a scenario we consider crucial for the practical adoption of NMT in code development. To that end, we develop a novel joint training approach for both localization and source code editions. Building a benchmark based on over 70k commits (patches and messages), we demonstrate that our joint Localize and EDit ( jLED) approach is effective. An ablation study further demonstrates the importance of our design choice in joint training.},
  archive      = {J_TOSEM},
  author       = {Weiguo Pian and Yinghua Li and Haoye Tian and Tiezhu Sun and Yewei Song and Xunzhu Tang and Andrew Habib and Jacques Klein and Tegawendé F. Bissyandé},
  doi          = {10.1145/3712187},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {You don’t have to say where to edit! jLED—Joint learning to localize and edit source code},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDASR: Deep diverse API sequence recommendation. <em>TOSEM</em>, <em>34</em>(6), 1-39. (<a href='https://doi.org/10.1145/3712188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommending API sequences is crucial in software development, saving developers time and effort. While previous studies primarily focus on accuracy, often recommending popular APIs, they tend to overlook less frequent, or ”tail,” APIs. This oversight, often a result of limited historical data, consequently diminishes the diversity of recommender systems. In this article, we propose DDASR, a framework for recommending API sequences containing both popular and tail APIs. To accurately capture developer intent, we utilize recent Large Language Models for learning query representations. To gain a better understanding of tail APIs, DDASR clusters tail APIs with similar functionality and replaces them with cluster centers to produce a pseudo ground truth. Moreover, a loss function is defined based on learning-to-rank to achieve an equilibrium in accuracy and diversity due to the inherent tradeoff between them. To evaluate DDASR, we conduct extensive experiments on Java and Python open source datasets. Results demonstrate that DDASR significantly achieves the best diversity without sacrificing accuracy. Compared to seven state-of-the-art approaches, DDASR improves accuracy metrics BLEU, ROUGE, MAP, and NDCG and diversity metric coverage by 108.28%, 67.30%, 88.59%, and 45.83%, respectively, on the Java dataset, as well as 9.83%, 2.45%, 8.06%, and 8.03%, respectively, on the Python dataset.},
  archive      = {J_TOSEM},
  author       = {Siyu Nan and Jian Wang and Neng Zhang and Duantengchuan Li and Bing Li},
  doi          = {10.1145/3712188},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {DDASR: Deep diverse API sequence recommendation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatically learning a precise measurement for fault diagnosis capability of test cases. <em>TOSEM</em>, <em>34</em>(6), 1-28. (<a href='https://doi.org/10.1145/3712189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prevalent Fault Localization (FL) techniques rely on tests to localize buggy program elements. Tests could be treated as fuel to further boost FL by providing more debugging information. Therefore, it is highly valuable to measure the Fault Diagnosis Capability (FDC) of a test for diagnosing faults, so as to select or generate tests to better help FL (i.e., FL-oriented test selection or FL-oriented test generation). To this end, researchers have proposed many FDC metrics, which serve as the selection criterion in FL-oriented test selection or the fitness function in FL-oriented test generation. Existing FDC metrics can be classified into result-agnostic and result-aware metrics depending on whether they take test results (i.e., passing or failing) as input. Although result-aware metrics perform better in test selection, they have restricted applications due to the input of test results, e.g., they cannot be applied to guide test generation. Moreover, all the existing FDC metrics are designed based on some pre-defined heuristics and have achieved limited FL performance due to their inaccuracy. To address these issues, in this article, we reconsider result-agnostic metrics (i.e., metrics that do not take test results as input), and propose a novel result-agnostic metric RLFDC which predicts FDC values of tests through reinforcement learning. In particular, we treat FL results as reward signals, and train an FDC prediction model with the direct FL feedback to automatically learn a more accurate measurement rather than design one based on pre-defined heuristics. Finally, we evaluate the proposed RLFDC on Defects4J by applying the studied metrics to test selection and generation. According to the experimental results, the proposed RLFDC outperforms all the result-agnostic metrics in both test selection and generation, e.g., when applied to selecting human-written tests, RLFDC achieves 28.2% and 21.6% higher acc@1 and mAP values compared to the state-of-the-art result-agnostic metric TfD. Besides, RLFDC even achieves competitive performance compared to the state-of-the-art result-aware metric FDG in test selection.},
  archive      = {J_TOSEM},
  author       = {Yifan Zhao and Zeyu Sun and Guoqing Wang and Qingyuan Liang and Yakun Zhang and Yiling Lou and Dan Hao and Lu Zhang},
  doi          = {10.1145/3712189},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automatically learning a precise measurement for fault diagnosis capability of test cases},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommending variable names for extract local variable refactorings. <em>TOSEM</em>, <em>34</em>(6), 1-38. (<a href='https://doi.org/10.1145/3712191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extract local variable is one of the most popular refactorings. It is frequently employed to replace occurrences of a complex expression with simple accesses to a newly introduced variable that is initialized by the original complex expression. Consequently, most IDEs and refactoring tools provide automated support for this refactoring, e.g., to suggest names for the newly extracted variables. However, we find approximately 70% of the names recommended by these IDEs are different from what developers manually constructed, adding additional renaming burdens to developers and providing limited assistance. In this article, we introduce VarNamer , an automated approach designed to recommend variable names for extract local variable refactorings. Through a large-scale empirical study, we identify key contexts, such as variable initializations and homogeneous variables (variables whose initializations are identical to that of the newly extracted variable), that are useful for composing variable names. Leveraging these insights, we developed a set of heuristic rules through program static analysis techniques, e.g., lexical analysis, syntax analysis, control flow analysis, and data flow analysis, and employ data mining techniques, i.e., FP-growth algorithm, to recommend variable names effectively. Notably, some of our heuristic rules have been successfully integrated into Eclipse , where they are now distributed with the latest releases of the IDE. Evaluation of VarNamer on a dataset of 27,158 real-world extract local variable refactorings in Java applications demonstrates its superiority over state-of-the-art IDEs. Specifically, VarNamer significantly increases the chance of exact match by 52.6% compared to Eclipse and 40.7% compared to IntelliJ IDEA . We also evaluated the proposed approach with real-world extract local variable refactorings conducted in C \(++\) projects, and the results suggest that the approach can achieve comparable performance on programming languages besides Java. It may suggest the generalizability of VarNamer . Finally, we designed and conducted a user study to investigate the impact of VarNamer on developers’ productivity. The results of the user study suggest that our approach can speed up the refactoring by 27.8% and reduce 49.3% edits on the recommended variable names.},
  archive      = {J_TOSEM},
  author       = {Taiming Wang and Hui Liu and Yuxia Zhang and Yanjie Jiang},
  doi          = {10.1145/3712191},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Recommending variable names for extract local variable refactorings},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How low can we go? minimizing interaction samples for configurable systems. <em>TOSEM</em>, <em>34</em>(6), 1-29. (<a href='https://doi.org/10.1145/3712193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern software systems are typically configurable, a fundamental prerequisite for wide applicability and reusability. This flexibility poses an extraordinary challenge for quality assurance, as the enormous number of possible configurations makes it impractical to test each of them separately. This is where t-wise interaction sampling can be used to systematically cover the configuration space and detect unknown feature interactions. Over the last two decades, numerous algorithms for computing small interaction samples have been studied, providing improvements for a range of heuristic results; nevertheless, it has remained unclear how much these results can still be improved. We present a significant breakthrough: a fundamental framework, based on the mathematical principle of duality , for combining near-optimal solutions with provable lower bounds on the required sample size. This implies that we no longer need to work on heuristics with marginal or no improvement, but can certify the solution quality by establishing a limit on the remaining gap; in many cases, we can even prove optimality of achieved solutions. This theoretical contribution also provides extensive practical improvements: Our algorithm SampLNS was tested on 47 small- and medium-sized configurable systems from the existing literature. SampLNS can reliably find samples of smaller size than previous methods in \(85\%\) of the cases; moreover, we can achieve and prove optimality of solutions for \(63\%\) of all instances. This makes it possible to avoid cumbersome efforts of minimizing samples by researchers as well as practitioners, and substantially save testing resources for most configurable systems.},
  archive      = {J_TOSEM},
  author       = {Dominik M. Krupke and Ahmad Moradi and Michael Perk and Phillip Keldenich and Gabriel Gehrke and Sebastian Krieter and Thomas Thüm and Sándor P. Fekete},
  doi          = {10.1145/3712193},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {How low can we go? minimizing interaction samples for configurable systems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the practicability of deep learning based anomaly detection for modern online software systems: A pre-train-and-align framework. <em>TOSEM</em>, <em>34</em>(6), 1-42. (<a href='https://doi.org/10.1145/3712195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operation and maintenance are critical activities in the whole lifecycle of modern online software systems, and anomaly detection is a crucial step of these activities. Recent studies mainly develop deep learning techniques to complete this task. Notably, though these techniques have achieved promising results in experimental evaluations, there are still several practicality gaps for them to be successfully applied in a real-world online system, including the scalability gap, availability gap, and alignment gap. To bridge these gaps, we propose an anomaly detection framework, namely ShareAD , based on a pre-train-and-align paradigm. Specifically, we argue that pre-training a shared model for anomaly detection is an effective way to bridge the scalability gap and the availability gap. To support this argument, we systematically study the necessity and feasibility of model sharing for online system maintenance. We further propose a novel model based upon Transformer encoder layers and Base layers, which works well for anomaly detection pre-training. Then, to bridge the alignment gap, we propose ShareAD alignment to align the pre-trained model with operator preference by jointly considering the local observation context and sensitivity of each monitor entity. Extensive experiments on two real-world large-scale datasets demonstrate the effectiveness and practicality of ShareAD .},
  archive      = {J_TOSEM},
  author       = {Zilong He and Pengfei Chen and Zibin Zheng},
  doi          = {10.1145/3712195},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-42},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On the practicability of deep learning based anomaly detection for modern online software systems: A pre-train-and-align framework},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). System safety monitoring of learned components using temporal metric forecasting. <em>TOSEM</em>, <em>34</em>(6), 1-43. (<a href='https://doi.org/10.1145/3712196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In learning-enabled autonomous systems, safety monitoring of learned components is crucial to ensure their outputs do not lead to system safety violations, given the operational context of the system. However, developing a safety monitor for practical deployment in real-world applications is challenging. This is due to limited access to internal workings and training data of the learned component. Furthermore, safety monitors should predict safety violations with low latency, while consuming a reasonable computation resource amount. To address the challenges, we propose a safety monitoring method based on probabilistic time series forecasting. Given the learned component outputs and an operational context, we empirically investigate different Deep Learning (DL)-based probabilistic forecasting to predict the objective measure capturing the satisfaction or violation of a safety requirement ( safety metric ). We empirically evaluate safety metric and violation prediction accuracy, and inference latency and resource usage of four state-of-the-art models, with varying horizons, using autonomous aviation and autonomous driving case studies. Our results suggest that probabilistic forecasting of safety metrics, given learned component outputs and scenarios, is effective for safety monitoring. Furthermore, for both case studies, the Temporal Fusion Transformer (TFT) was the most accurate model for predicting imminent safety violations, with acceptable latency and resource consumption.},
  archive      = {J_TOSEM},
  author       = {Sepehr Sharifi and Andrea Stocco and Lionel C. Briand},
  doi          = {10.1145/3712196},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {System safety monitoring of learned components using temporal metric forecasting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WebAssembly for container runtime: Are we there yet?. <em>TOSEM</em>, <em>34</em>(6), 1-22. (<a href='https://doi.org/10.1145/3712197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To pursue more efficient software deployment with containers, WebAssembly (abbreviated as Wasm) has long been regarded as a promising alternative to native container runtime (such as Docker container) due to its features of secure memory sandbox, lightweight isolation, portability, and multi-language support. However, it remains unknown whether and how much Wasm indeed brings benefits for containerized software applications. To fill the knowledge gap, this paper presents the first measurement study on Wasm-based container runtime (i.e., Wasm container) by comparison with the Docker container and native standalone Wasm runtime for execution performance in terms of the startup, computation, system interface access, and resource consumption. Surprisingly, we find that the Wasm container does not achieve better performance versus the Docker container as expected and introduces significant overhead compared to the standalone Wasm runtime. Through comparison, we identify the main causes of performance degradation for Wasm containers. Some stem from the heavy containerization overhead similar to Docker containers, while others are inherently caused by Wasm VMs and the WASI interface. Our findings can help software developers, Wasm container developers and the Wasm community improve the efficiency of utilizing Wasm-based container runtime, ultimately optimizing software performance.},
  archive      = {J_TOSEM},
  author       = {Mugeng Liu and Haiyang Shen and Yixuan Zhang and Hong Mei and Yun Ma},
  doi          = {10.1145/3712197},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {WebAssembly for container runtime: Are we there yet?},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering community smells in machine learning-enabled systems: Causes, effects, and mitigation strategies. <em>TOSEM</em>, <em>34</em>(6), 1-48. (<a href='https://doi.org/10.1145/3712198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Successful software development hinges on effective communication and collaboration, which are significantly influenced by human and social dynamics. Poor management of these elements can lead to the emergence of ‘community smells’, i.e., negative patterns in socio-technical interactions that gradually accumulate as ‘social debt’. This issue is particularly pertinent in machine learning-enabled systems, where diverse actors such as data engineers and software engineers interact at various levels. The unique collaboration context of these systems presents an ideal setting to investigate community smells and their impact on development communities. This article addresses a gap in the literature by identifying the types, causes, effects, and potential mitigation strategies of community smells in machine learning-enabled systems. Using Partial Least Squares Structural Equation Modeling (PLS-SEM), we developed hypotheses based on existing literature and interviews, and conducted a questionnaire-based study to collect data. Our analysis resulted in the construction and validation of five models that represent the causes, effects, and strategies for five specific community smells. These models can help practitioners identify and address community smells within their organizations, while also providing valuable insights for future research on the socio-technical aspects of machine learning-enabled system communities.},
  archive      = {J_TOSEM},
  author       = {Giusy Annunziata and Stefano Lambiase and Damian A. Tamburri and Willem-Jan van den Heuvel and Fabio Palomba and Gemma Catolino and Filomena Ferrucci and Andrea De Lucia},
  doi          = {10.1145/3712198},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-48},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Uncovering community smells in machine learning-enabled systems: Causes, effects, and mitigation strategies},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating regression faults induced by feature evolution in deep learning systems. <em>TOSEM</em>, <em>34</em>(6), 1-33. (<a href='https://doi.org/10.1145/3712199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) systems have been widely utilized across various domains. However, the evolution of DL systems can result in regression faults. In addition to the evolution of DL systems through the incorporation of new data, feature evolution, such as the addition of new features, is also common and can introduce regression faults. In this work, we first investigate the underlying factors that are correlated with regression faults in feature evolution scenarios, i.e., redundancy and contribution shift. Based on our investigation, we propose a novel mitigation approach called FeaProtect, which aims to minimize the impact of these two factors. To evaluate the performance of FeaProtect, we conducted an extensive study comparing it with state-of-the-art approaches. The results show that FeaProtect outperforms the in-processing baseline approaches, with an average improvement of 50.6%–56.4% in terms of regression fault mitigation. We also show that FeaProtect can further enhance the effectiveness of mitigating regression faults by integrating with state-of-the-art post-processing approaches.},
  archive      = {J_TOSEM},
  author       = {Hanmo You and Zan Wang and Xuyang Chen and Junjie Chen and Jun Sun and Shuang Liu and Zishuo Dong},
  doi          = {10.1145/3712199},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Mitigating regression faults induced by feature evolution in deep learning systems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Making fault localization in online service systems more actionable and interpretable. <em>TOSEM</em>, <em>34</em>(6), 1-26. (<a href='https://doi.org/10.1145/3714466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online service systems struggle with accurately and quickly pinpointing and resolving failures within their intricate systems, and it therefore emerges the solutions for fault localization in the code. However, the previous fault localization models suffer from low localization accuracy and poor interpretability due to the complex dependencies among fault characteristics in industrial practice. To address this issue, challenges brought by the long-distance dependencies among fault features and the unbalanced distribution of fault knowledge, and to improve the interpretability of the model, we present a fault localization model in online service systems more actionable and interpretable, named FL-AIer. Specifically, FL-AIer consists of two components: the feature encoding component and the fault localization component. The feature encoding component utilizes graph attention networks to capture the complex spatio-temporal dependencies within fault features. Then, the fault localization component adopts a three-stage approach, leveraging a multi-attention mechanism to identify and prioritize the most relevant fault features for precise localization. Additionally, the Fault Knowledge Balancing module it contains introduces a weighted Kullback-Leibler divergence loss function to ensure that the model pays adequate attention to all fault features, addressing the issue of imbalanced fault knowledge distribution and enhancing localization performance. We conducted extensive experiments on four datasets, and the results demonstrated that FL-AIer effectively addressing the challenges of fault localization in online system environments, and consistently outperforms the state-of-the-art methods across various evaluation metrics such as A@1, A@2, A@3, A@5, and MAR. For instance, FL-AIer achieves significant improvements of 5.82%, 10.77%, 4.20%, and 15.56% on the A@1 metric, respectively. These results fully demonstrate the excellent effectiveness of FL-AIer in effectively addressing the challenges of fault localization in online system environments, surpassing the performance of existing state-of-the-art methods.},
  archive      = {J_TOSEM},
  author       = {Ke Xv and Shikai Guo and Hui Li and Chenchen Li and Rong Chen and Xiaochen Li and He Jiang},
  doi          = {10.1145/3714466},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Making fault localization in online service systems more actionable and interpretable},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable similarity-aware test suite minimization with reinforcement learning. <em>TOSEM</em>, <em>34</em>(6), 1-23. (<a href='https://doi.org/10.1145/3715008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multi-Criteria Test Suite Minimization (MCTSM) problem aims to remove redundant test cases, guided by adequacy criteria such as code coverage or fault detection capability. However, current techniques either exhibit a high loss of fault detection ability or face scalability challenges due to the NP-hard nature of the problem, which limits their practical utility. We propose TripRL , a novel technique that integrates traditional criteria such as statement coverage and fault detection ability with test coverage similarity into an Integer Linear Program (ILP), to produce a diverse reduced test suite with high test effectiveness. TripRL leverages bipartite graph representation and its embedding for concise ILP formulation and combines ILP with effective Reinforcement Learning (RL) training. This combination renders large-scale test suite minimization more scalable and enhances test effectiveness. Our empirical evaluations demonstrate that TripRL ’s runtime scales linearly with the magnitude of the MCTSM problem. Notably, for large test suites from the Defects4J dataset where existing approaches fail to provide solutions within a reasonable time frame, our technique consistently delivers solutions in less than 47 minutes. The reduced test suites produced by TripRL also maintain the original statement coverage and fault detection ability while having a higher potential to detect unknown faults.},
  archive      = {J_TOSEM},
  author       = {Sijia Gu and Ali Mesbah},
  doi          = {10.1145/3715008},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {7},
  number       = {6},
  pages        = {1-23},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Scalable similarity-aware test suite minimization with reinforcement learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic programming: Large language models and beyond. <em>TOSEM</em>, <em>34</em>(5), 1-33. (<a href='https://doi.org/10.1145/3708519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security, and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs can help produce higher assurance code from LLMs, along with evidence of assurance.},
  archive      = {J_TOSEM},
  author       = {Michael R. Lyu and Baishakhi Ray and Abhik Roychoudhury and Shin Hwei Tan and Patanamon Thongtanunam},
  doi          = {10.1145/3708519},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automatic programming: Large language models and beyond},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A road-map to readily available early validation and verification of system behaviour in model-based systems engineering using software engineering best practices. <em>TOSEM</em>, <em>34</em>(5), 1-30. (<a href='https://doi.org/10.1145/3708520'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we discuss how we can facilitate the growing need for early validation and verification (V&V) of system behaviour in Model-Based Systems Engineering (MBSyE). Several aspects, such as reducing cost and time to market, push companies towards integration of V&V methods earlier in development to support effective decision-making. One foundational methodology seeing increased attention in industry is the use of MBSyE, which brings benefits of models with well-defined syntax and semantics to support V&V activities, rather than relying on natural language text documentation. Despite their promise, industrial adoption of these practices is still challenging. This article presents a vision for readily available early V&V . We present a summary of the literature on early V&V in MBSyE and position existing challenges regarding potential solutions and future investigations towards this vision. We elaborate our vision by means of challenges with a specific emphasis on early V&V of system behaviour . We identify three specific challenge areas: Creating and managing Models , Organisational systems engineering aspects, and early V&V Methods . Finally, we outline a road-map to address these categories of challenges, in which we propose the transfer of established best practices from the software engineering domain to support emerging technologies in the systems engineering domain.},
  archive      = {J_TOSEM},
  author       = {Johan Cederbladh and Antonio Cicchetti and Robbert Jongeling},
  doi          = {10.1145/3708520},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A road-map to readily available early validation and verification of system behaviour in model-based systems engineering using software engineering best practices},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metamorphic relation generation: State of the art and research directions. <em>TOSEM</em>, <em>34</em>(5), 1-25. (<a href='https://doi.org/10.1145/3708521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metamorphic testing has become one mainstream technique to address the notorious oracle problem in software testing, thanks to its great successes in revealing real-life bugs in a wide variety of software systems. Metamorphic relations, the core component of metamorphic testing, have continuously attracted research interests from both academia and industry. In the last decade, a rapidly increasing number of studies have been conducted to systematically generate metamorphic relations from various sources and for different application domains. In this article, based on the systematic review on the state of the art for metamorphic relations’ generation, we summarize and highlight visions for further advancing the theory and techniques for identifying and constructing metamorphic relations and discuss promising research directions in related areas.},
  archive      = {J_TOSEM},
  author       = {Rui Li and Huai Liu and Pak-Lok Poon and Dave Towey and Chang-Ai Sun and Zheng Zheng and Zhi Quan Zhou and Tsong Yueh Chen},
  doi          = {10.1145/3708521},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Metamorphic relation generation: State of the art and research directions},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language model for vulnerability detection and repair: Literature review and the road ahead. <em>TOSEM</em>, <em>34</em>(5), 1-31. (<a href='https://doi.org/10.1145/3708522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair. Numerous studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks. Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair. In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs. The review encompasses research work from leading SE, AI, and Security conferences and journals, encompassing 43 papers published across 25 distinct venues, along with 15 high-quality preprint papers, bringing the total to 58 papers. By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair. Based on our findings, we have identified a series of limitations of existing studies. Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors.},
  archive      = {J_TOSEM},
  author       = {Xin Zhou and Sicong Cao and Xiaobing Sun and David Lo},
  doi          = {10.1145/3708522},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Large language model for vulnerability detection and repair: Literature review and the road ahead},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of generative AI on creativity in software development: A research agenda. <em>TOSEM</em>, <em>34</em>(5), 1-28. (<a href='https://doi.org/10.1145/3708523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As GenAI becomes embedded in developer toolchains and practices, and routine code is increasingly generated, human creativity will be increasingly important for generating competitive advantage. This article uses the McLuhan tetrad alongside scenarios of how GenAI may disrupt software development more broadly, to identify potential impacts GenAI may have on creativity within software development. The impacts are discussed along with a future research agenda comprising five connected themes that consider how individual capabilities, team capabilities, the product, unintended consequences, and society can be affected.},
  archive      = {J_TOSEM},
  author       = {Victoria Jackson and Bogdan Vasilescu and Daniel Russo and Paul Ralph and Rafael Prikladnicki and Maliheh Izadi and Sarah D’Angelo and Sarah Inman and Anielle Andrade and André van der Hoek},
  doi          = {10.1145/3708523},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {The impact of generative AI on creativity in software development: A research agenda},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Requirements are all you need: The final frontier for end-user software engineering. <em>TOSEM</em>, <em>34</em>(5), 1-22. (<a href='https://doi.org/10.1145/3708524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What if end-users could own the software development lifecycle from conception to deployment using only requirements expressed in language, images, video or audio? We explore this idea, building on the capabilities that Generative AI brings to software generation and maintenance techniques. How could designing software in this way better serve end-users? What are the implications of this process for the future of end-user software engineering and the software development lifecycle? We discuss the research needed to bridge the gap between where we are today and these imagined systems of the future.},
  archive      = {J_TOSEM},
  author       = {Diana Robinson and Christian Cabrera and Andrew D. Gordon and Neil D. Lawrence and Lars Mennen},
  doi          = {10.1145/3708524},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Requirements are all you need: The final frontier for end-user software engineering},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and green large language models for software engineering: Literature review, vision, and the road ahead. <em>TOSEM</em>, <em>34</em>(5), 1-22. (<a href='https://doi.org/10.1145/3708525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have recently shown remarkable capabilities in various software engineering tasks, spurring the rapid growth of the Large Language Models for Software Engineering (LLM4SE) area. However, limited attention has been paid to developing efficient LLM4SE techniques that demand minimal computational cost, time, and memory resources, as well as green LLM4SE solutions that reduce energy consumption, water usage, and carbon emissions. This article aims to redirect the focus of the research community toward the efficiency and greenness of LLM4SE, while also sharing potential research directions to achieve this goal. It commences with a brief overview of the significance of LLM4SE and highlights the need for efficient and green LLM4SE solutions. Subsequently, the article presents a vision for a future where efficient and green LLM4SE revolutionizes the LLM-based software engineering tool landscape, benefiting various stakeholders, including industry, individual practitioners, and society. The article then delineates a roadmap for future research, outlining specific research paths and potential solutions for the research community to pursue. While not intended to be a definitive guide, the article aims to inspire further progress, with the ultimate goal of establishing efficient and green LLM4SE as a central element in the future of software engineering.},
  archive      = {J_TOSEM},
  author       = {Jieke Shi and Zhou Yang and David Lo},
  doi          = {10.1145/3708525},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Efficient and green large language models for software engineering: Literature review, vision, and the road ahead},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A roadmap for integrating sustainability into software engineering education. <em>TOSEM</em>, <em>34</em>(5), 1-27. (<a href='https://doi.org/10.1145/3708526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world faces escalating crises: record-breaking temperatures, widespread fires, severe flooding, increased oceanic microplastics, and unequal resource distribution. Academia introduces courses around sustainability to meet the new demand, but software engineering education lags behind. While software systems contribute to environmental issues through high energy consumption, they also hold the potential for solutions, such as more efficient and equitable resource management. Yet, sustainability remains a low priority for many businesses, including those in the digital sector. Business as usual is no longer viable. A transformational change in software engineering education is urgently needed. We must move beyond traditional curriculum models and fully integrate sustainability into every aspect of software development. By embedding sustainability as a core competency, we can equip future engineers not only to minimise harm but also to innovate solutions that drive positive, sustainable change. Only with such a shift can software engineering education meet the demands of a world in crisis and prepare students to lead the next generation of sustainable technology. This article discusses a set of challenges and proposes a customisable education roadmap for integrating sustainability into the software engineering curricula. These challenges reflect our perspective on key considerations, stemming from regular, intensive discussions in regular workshops among the authors and the community, as well as our extensive research and teaching experience in the field.},
  archive      = {J_TOSEM},
  author       = {Ana Moreira and Patricia Lago and Rogardt Heldal and Stefanie Betz and Ian Brooks and Rafael Capilla and Vlad Constantin Coroamă and Leticia Duboc and João Paulo Fernandes and Ola Leifler and Ngoc-Thanh Nguyen and Shola Oyedeji and Birgit Penzenstadler and Anne-Kathrin Peters and Jari Porras and Colin C. Venters},
  doi          = {10.1145/3708526},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A roadmap for integrating sustainability into software engineering education},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contemporary software modernization: Strategies, driving forces, and research opportunities. <em>TOSEM</em>, <em>34</em>(5), 1-35. (<a href='https://doi.org/10.1145/3708527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software modernization is a common activity in software engineering, since technologies advance, requirements change, and business models evolve. Differently from conventional software evolution (e.g., adding new features, enhancing performance, or adapting to new requirements), software modernization involves re-engineering entire legacy systems (e.g., changing the technology stack, migrating to a new architecture style, or programming paradigms). Given the pervasive nature of software today, modernizing legacy systems is paramount to provide customers with competitive and innovative products and services, while keeping companies profitable. Despite the prevalent discussion of software modernization in gray literature, and the many papers in the literature, there is no work presenting a “big picture” of contemporary software modernization, describing challenges, and providing a well-defined research agenda. The goal of this work is to describe the state of the art in software modernization in the past 10 years. We collect the state of the art by performing a rapid review (searching five digital libraries), identifying potential 3,460 studies, leading to a final set of 126. We analyzed these studies to understand which strategies are employed, the driving forces that lead organizations to modernize their systems, and the challenges that need to be addressed. The results show that studies in the last 10 years have explored eight strategies for modernizing legacy systems, namely cloudification, architecture redesign, moving to a new programming language, targeting reuse optimization, software modernization for new hardware integration, practices to leverage automation, database modernization, and digital transformation. Modernization is triggered by 14 driving forces, with the most common ones being reducing operational costs, improving performance and scalability, and reducing complexity. In addition, based on the analysis of existing literature, we present a detailed discussion of research opportunities in this field. The main challenges are providing tooling support, followed by defining a modernization process and considering better evaluation metrics. The main contribution of our work is to equip practitioners and researchers with knowledge of the current state of contemporary software modernization so that they are aware of practices and challenges to be addressed when deciding to modernize legacy systems.},
  archive      = {J_TOSEM},
  author       = {Wesley K. G. Assunção and Luciano Marchezan and Lawrence Arkoh and Alexander Egyed and Rudolf Ramler},
  doi          = {10.1145/3708527},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Contemporary software modernization: Strategies, driving forces, and research opportunities},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM for mobile: An initial roadmap. <em>TOSEM</em>, <em>34</em>(5), 1-29. (<a href='https://doi.org/10.1145/3708528'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When mobile meets LLMs, mobile app users deserve to have more intelligent usage experiences. For this to happen, we argue that there is a strong need to apply LLMs for the mobile ecosystem. We therefore provide a research roadmap for guiding our fellow researchers to achieve that as a whole. In this roadmap, we sum up six directions that we believe are urgently required for research to enable native intelligence in mobile devices. In each direction, we further summarize the current research progress and the gaps that still need to be filled by our fellow researchers.},
  archive      = {J_TOSEM},
  author       = {Daihang Chen and Yonghui Liu and Mingyi Zhou and Yanjie Zhao and Haoyu Wang and Shuai Wang and Xiao Chen and Tegawendé F. Bissyandé and Jacques Klein and Li Li},
  doi          = {10.1145/3708528},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {LLM for mobile: An initial roadmap},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open source AI-based SE tools: Opportunities and challenges of collaborative software learning. <em>TOSEM</em>, <em>34</em>(5), 1-24. (<a href='https://doi.org/10.1145/3708529'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. AI code models have demonstrated their value not only in code generation but also in defect detection, enhancing security measures and improving overall software quality. They are emerging as crucial tools for both software development and maintaining software quality. Like traditional SE tools, open source collaboration is key in realizing the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximizing the sources of high-quality data. However, data, especially of high quality, often hold commercial or sensitive value, making them less accessible for open source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the SE community. Therefore, researchers need to find solutions for enabling open source AI-based SE models to tap into resources by different organizations. Addressing this challenge, our position article investigates one solution to facilitate access to diverse organizational resources for open source AI models, ensuring that privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance. We consider six different scenarios of data distributions and include four code models. We also include four most common FL algorithms. Our experimental findings highlight the potential for employing FL in the collaborative development and maintenance of AI-based SE models. We also discuss the key issues to be addressed in the co-construction process and future research directions.},
  archive      = {J_TOSEM},
  author       = {Zhihao Lin and Wei Ma and Tao Lin and Yaowen Zheng and Jingquan Ge and Jun Wang and Jacques Klein and Tegawendé F. Bissyandé and Yang Liu and Li Li},
  doi          = {10.1145/3708529},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Open source AI-based SE tools: Opportunities and challenges of collaborative software learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM app store analysis: A vision and roadmap. <em>TOSEM</em>, <em>34</em>(5), 1-25. (<a href='https://doi.org/10.1145/3708530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth and popularity of large language model (LLM) app stores have created new opportunities and challenges for researchers, developers, users, and app store managers. As the LLM app ecosystem continues to evolve, it is crucial to understand the current landscape and identify potential areas for future research and development. This article presents a forward-looking analysis of LLM app stores, focusing on key aspects such as data mining, security risk identification, development assistance, and market dynamics. Our comprehensive examination extends to the intricate relationships between various stakeholders and the technological advancements driving the ecosystem’s growth. We explore the ethical considerations and potential societal impacts of widespread LLM app adoption, highlighting the need for responsible innovation and governance frameworks. By examining these aspects, we aim to provide a vision for future research directions and highlight the importance of collaboration among stakeholders to address the challenges and opportunities within the LLM app ecosystem. The insights and recommendations provided in this article serve as a foundation for driving innovation, ensuring responsible development, and creating a thriving, user-centric LLM app landscape.},
  archive      = {J_TOSEM},
  author       = {Yanjie Zhao and Xinyi Hou and Shenao Wang and Haoyu Wang},
  doi          = {10.1145/3708530},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {LLM app store analysis: A vision and roadmap},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language model supply chain: A research agenda. <em>TOSEM</em>, <em>34</em>(5), 1-46. (<a href='https://doi.org/10.1145/3708531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, introducing unprecedented capabilities in natural language processing and multimodal content generation. However, the increasing complexity and scale of these models have given rise to a multifaceted supply chain that presents unique challenges across infrastructure, foundation models, and downstream applications. This article provides the first comprehensive research agenda of the LLM supply chain, offering a structured approach to identify critical challenges and opportunities through the dual lenses of software engineering (SE) and security and privacy (S&P). We begin by establishing a clear definition of the LLM supply chain, encompassing its components and dependencies. We then analyze each layer of the supply chain, presenting a vision for robust and secure LLM development, reviewing the current state of practices and technologies, and identifying key challenges and research opportunities. This work aims to bridge the existing research gap in systematically understanding the multifaceted issues within the LLM supply chain, offering valuable insights to guide future efforts in this rapidly evolving domain.},
  archive      = {J_TOSEM},
  author       = {Shenao Wang and Yanjie Zhao and Xinyi Hou and Haoyu Wang},
  doi          = {10.1145/3708531},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-46},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Large language model supply chain: A research agenda},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software security analysis in 2030 and beyond: A research roadmap. <em>TOSEM</em>, <em>34</em>(5), 1-26. (<a href='https://doi.org/10.1145/3708533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As our lives, our businesses, and indeed our world economy become increasingly reliant on the secure operation of many interconnected software systems, the software engineering research community is faced with unprecedented research challenges, but also with exciting new opportunities. In this roadmap article, we outline our vision of software security analysis for the systems of the future. Given the recent advances in generative AI, we need new methods to assess and maximize the security of code co-written by machines. As our systems become increasingly heterogeneous, we need practical approaches that work even if some functions are automatically generated, e.g., by deep neural networks. As software systems depend evermore on the software supply chain, we need tools that scale to an entire ecosystem. What kind of vulnerabilities exist in future systems and how do we detect them? When all the shallow bugs are found, how do we discover vulnerabilities hidden deeply in the system? Assuming we cannot find all security flaws, how can we nevertheless protect our system? To answer these questions, we start our roadmap with a survey of recent advances in software security, then discuss open challenges and opportunities, and conclude with a long-term perspective for the field.},
  archive      = {J_TOSEM},
  author       = {Marcel Böhme and Eric Bodden and Tevfik Bultan and Cristian Cadar and Yang Liu and Giuseppe Scanniello},
  doi          = {10.1145/3708533},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Software security analysis in 2030 and beyond: A research roadmap},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sustainable software engineering: Concepts, challenges, and vision. <em>TOSEM</em>, <em>34</em>(5), 1-28. (<a href='https://doi.org/10.1145/3709352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information and communication technology (ICT) offers promising opportunities to address global sustainability challenges such as climate change and social inequality by enabling energy savings and social innovations. At the same time, ICT threatens to exacerbate these crises, as evident in the increasing consumption of resources and widening digital inequalities. As one of the enablers of ICT, software engineering plays a key role to tackle the problems and explore the potentials of ICT for sustainability. However, sustainability in software engineering is still a niche topic, with little structure, a limited understanding of sustainability and few comprehensive strategies. In this article, we introduce the main concepts of Sustainable Software Engineering, critically review the state of research and identify seven future research challenges across all research areas. We further present our research vision—sustainability-driven software engineering and transdisciplinary research formats—and outline a research roadmap with the key steps to be achieved by 2030.},
  archive      = {J_TOSEM},
  author       = {Christoph König and Daniel J. Lang and Ina Schaefer},
  doi          = {10.1145/3709352},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Sustainable software engineering: Concepts, challenges, and vision},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From today’s code to tomorrow’s symphony: The AI transformation of developer’s routine by 2030. <em>TOSEM</em>, <em>34</em>(5), 1-17. (<a href='https://doi.org/10.1145/3709353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving landscape of software engineering, the integration of AI into the Software Development Lifecycle (SDLC) heralds a transformative era for developers. Recently, we have assisted to a pivotal shift toward AI-assisted programming, exemplified by tools like GitHub Copilot and OpenAI’s ChatGPT, which have become a crucial element for coding, debugging, and software design. In this article, we provide a comparative analysis between the current state of AI-assisted programming in 2024 and our projections for 2030, by exploring how AI advancements are set to enhance the implementation phase, fundamentally altering developers’ roles from manual coders to orchestrators of AI-driven development ecosystems. We envision HyperAssistant , an augmented AI tool that offers comprehensive support to 2030 developers, addressing current limitations in mental health support, fault detection, code optimization, team interaction, and skill development. We emphasize AI as a complementary force, augmenting developers’ capabilities rather than replacing them, leading to the creation of sophisticated, reliable, and secure software solutions. Our vision seeks to anticipate the evolution of programming practices, challenges, and future directions, shaping a new paradigm where developers and AI collaborate more closely, promising a significant leap in SE efficiency, security, and creativity.},
  archive      = {J_TOSEM},
  author       = {Ketai Qiu and Niccolò Puccinelli and Matteo Ciniselli and Luca Di Grazia},
  doi          = {10.1145/3709353},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-17},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {From today’s code to tomorrow’s symphony: The AI transformation of developer’s routine by 2030},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Morescient GAI for software engineering. <em>TOSEM</em>, <em>34</em>(5), 1-17. (<a href='https://doi.org/10.1145/3709354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of Generative AI (GAI) technology to automatically check, synthesize, and modify software engineering artifacts promises to revolutionize all aspects of software engineering. Using GAI for software engineering tasks is consequently one of the most rapidly expanding fields of software engineering research, with over a hundred LLM-based code models having been published since 2021. However, the overwhelming majority of existing code models share a major weakness—they are exclusively trained on the syntactic facet of software, significantly lowering their trustworthiness in tasks dependent on software semantics. To address this problem, a new class of “Morescient” GAI is needed that is “aware” of (i.e., trained on) both the semantic and static facets of software. This, in turn, will require a new generation of software observation platforms capable of generating large quantities of execution observations in a structured and readily analyzable way. In this article, we present a vision and roadmap for how such “Morescient” GAI models can be engineered, evolved, and disseminated according to the principles of open science.},
  archive      = {J_TOSEM},
  author       = {Marcus Kessel and Colin Atkinson},
  doi          = {10.1145/3709354},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-17},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Morescient GAI for software engineering},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A roadmap for software testing in open-collaborative and AI-powered era. <em>TOSEM</em>, <em>34</em>(5), 1-17. (<a href='https://doi.org/10.1145/3709355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet technology has given rise to an open-collaborative software development paradigm, necessitating the open-collaborative schema to software testing. It enables diverse and globally distributed contributions, but also presents significant challenges to efficient testing processes, coordination among personnel, and management of testing artifacts. At the same time, advancements in AI have enhanced testing capabilities and enabling automation, while also introducing new testing needs and unique challenges for AI-based systems. In this context, this article explores software testing in the open-collaborative and AI-powered era, focusing on the interrelated dimensions of process, personnel, and technology. Among them, process involves managing testing workflows and artifacts to improve efficiency, personnel emphasizes the role of individuals in ensuring testing quality through collaboration and contributions, while technology refers to AI methods that enhance testing capabilities and address challenges in AI-based systems. Furthermore, we delve into the challenges and opportunities arising from emerging technologies such as Large Language Models (LLMs) and the AI model-centric development paradigm.},
  archive      = {J_TOSEM},
  author       = {Qing Wang and Junjie Wang and Mingyang Li and Yawen Wang and Zhe Liu},
  doi          = {10.1145/3709355},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-17},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A roadmap for software testing in open-collaborative and AI-powered era},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software fairness debt: Building a research agenda for addressing bias in AI systems. <em>TOSEM</em>, <em>34</em>(5), 1-21. (<a href='https://doi.org/10.1145/3709357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring fairness in software systems has become a critical concern in software engineering. Motivated by this challenge, this article explores the multifaceted nature of bias in software systems, providing a comprehensive understanding of its origins, manifestations, and impacts. Through a scoping study, we identified the primary causes of fairness deficiencies in software development and highlighted their adverse effects on individuals and communities, including instances of discrimination and the perpetuation of inequalities. Our investigation culminated in the introduction of the concept of software fairness debt. In addition to defining fairness debt, we propose a socio-technical roadmap that addresses broader aspects of fairness in AI-driven systems. This roadmap is structured around six goals: bridging the gap between research and real-world applications, developing a framework for fairness debt, equipping practitioners with tools and knowledge, improving bias mitigation, integrating fairness tools into industry practice, and enhancing explainability and transparency in AI systems. This roadmap provides a holistic approach to managing biases in software systems through software fairness debt, offering actionable steps for both research and practice. By guiding researchers and practitioners, our roadmap aims to foster the development of more equitable and socially responsible software systems, ensuring fairness is embedded throughout the software lifecycle.},
  archive      = {J_TOSEM},
  author       = {Ronnie de Souza Santos and Felipe Fronchetti and Sávio Freire and Rodrigo Spinola},
  doi          = {10.1145/3709357},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-21},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Software fairness debt: Building a research agenda for addressing bias in AI systems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From triumph to uncertainty: The journey of software engineering in the AI era. <em>TOSEM</em>, <em>34</em>(5), 1-34. (<a href='https://doi.org/10.1145/3709360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last 10 years, the realm of AI has experienced an explosion of revolutionary breakthroughs, transforming what seemed like a far-off dream into a reality that is now deeply embedded in our everyday lives. AI’s widespread impact is revolutionizing virtually all aspects of human life, and software engineering (SE) is no exception. As we explore this changing landscape, we are faced with questions about what the future holds for SE and how AI will reshape the roles, duties, and methodologies within the field. The introduction of these groundbreaking technologies highlights the inevitable shift toward a new paradigm, suggesting a future where AI’s capabilities may redefine the boundaries of SE, potentially even more than human input. In this article, we aim at outlining the key elements that, based on our expertise, are vital for the smooth integration of AI into SE, all while preserving the intrinsic human creativity that has been the driving force behind the field. First, we provide a brief description of SE and AI evolution. Afterward, we delve into the intricate interplay between AI-driven automation and human innovation, exploring how these two components can work together to advance SE practices to new methods and standards.},
  archive      = {J_TOSEM},
  author       = {Antonio Mastropaolo and Camilo Escobar-Velásquez and Mario Linares-Vásquez},
  doi          = {10.1145/3709360},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-34},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {From triumph to uncertainty: The journey of software engineering in the AI era},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Making software development more diverse and inclusive: Key themes, challenges, and future directions. <em>TOSEM</em>, <em>34</em>(5), 1-23. (<a href='https://doi.org/10.1145/3711904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introduction : Digital products increasingly reshape industries, influencing human behavior and decision-making. However, the software development teams developing these systems often lack diversity, which may lead to designs that overlook the needs, equal treatment or safety of diverse user groups. These risks highlight the need for fostering diversity and inclusion in software development to create safer, more equitable technology. Method : This research is based on insights from an academic meeting in June 2023 involving 23 software engineering researchers and practitioners. We used the collaborative discussion method 1-2-4-ALL as a systematic research approach and identified six themes around the theme “challenges and opportunities to improve Software Developer Diversity and Inclusion (SDDI).” We identified benefits, harms, and future research directions for the four main themes. Then, we discuss the remaining two themes, AI & SDDI and AI & Computer Science education, which have a cross-cutting effect on the other themes. Results : This research explores the key challenges and research opportunities for promoting SDDI, providing a roadmap to guide both researchers and practitioners. We underline that research around SDDI requires a constant focus on maximizing benefits while minimizing harms, especially to vulnerable groups. As a research community, we must strike this balance in a responsible way.},
  archive      = {J_TOSEM},
  author       = {Sonja M. Hyrynsalmi and Sebastian Baltes and Chris Brown and Rafael Prikladnicki and Gema Rodriguez-Perez and Alexander Serebrenik and Jocelyn Simmonds and Bianca Trinkenreich and Yi Wang and Grischa Liebel},
  doi          = {10.1145/3711904},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-23},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Making software development more diverse and inclusive: Key themes, challenges, and future directions},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A roadmap for simulation-based testing of autonomous cyber-physical systems: Challenges and future direction. <em>TOSEM</em>, <em>34</em>(5), 1-9. (<a href='https://doi.org/10.1145/3711906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the era of autonomous cyber-physical systems (ACPSs), such as unmanned aerial vehicles and self-driving cars, unfolds, the demand for robust testing methodologies is key to realizing the adoption of such systems in real-world scenarios. However, traditional software testing paradigms face unprecedented challenges in ensuring the safety and reliability of these systems. In response, this article pioneers a strategic roadmap for simulation-based system-level testing of ACPSs, specifically focusing on autonomous systems. Our article discusses the relevant challenges and obstacles of ACPSs, focusing on test automation and quality assurance, hence advocating for tailored solutions to address the unique demands of autonomous systems. While providing concrete definitions of test cases within simulation environments, we also accentuate the need to create new benchmark assets and the development of automated tools tailored explicitly for autonomous systems in the software engineering community. This article not only highlights the relevant, pressing issues the software engineering community should focus on (in terms of practices, expected automation, and paradigms), but it also outlines ways to tackle them. By outlining the various domains and challenges of simulation-based testing/development for ACPSs, we provide directions for future research efforts.},
  archive      = {J_TOSEM},
  author       = {Christian Birchler and Sajad Khatiri and Pooja Rani and Timo Kehrer and Sebastiano Panichella},
  doi          = {10.1145/3711906},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-9},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A roadmap for simulation-based testing of autonomous cyber-physical systems: Challenges and future direction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum software engineering: Roadmap and challenges ahead. <em>TOSEM</em>, <em>34</em>(5), 1-48. (<a href='https://doi.org/10.1145/3712002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As quantum computers advance, the complexity of the software they can execute increases as well. To ensure this software is efficient, maintainable, reusable, and cost-effective—key qualities of any industry-grade software—mature software engineering practices must be applied throughout its design, development, and operation. However, the significant differences between classical and quantum software make it challenging to directly apply classical software engineering methods to quantum systems. This challenge has led to the emergence of Quantum Software Engineering (QSE) as a distinct field within the broader software engineering landscape. In this work, a group of active researchers analyze in depth the current state of QSE research. From this analysis, the key areas of QSE are identified and explored in order to determine the most relevant open challenges that should be addressed in the next years. These challenges help identify necessary breakthroughs and future research directions for advancing QSE.},
  archive      = {J_TOSEM},
  author       = {Juan Manuel Murillo and Jose Garcia-Alonso and Enrique Moguel and Johanna Barzen and Frank Leymann and Shaukat Ali and Tao Yue and Paolo Arcaini and Ricardo Pérez-Castillo and Ignacio García-Rodríguez de Guzmán and Mario Piattini and Antonio Ruiz-Cortés and Antonio Brogi and Jianjun Zhao and Andriy Miranskyy and Manuel Wimmer},
  doi          = {10.1145/3712002},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-48},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Quantum software engineering: Roadmap and challenges ahead},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-based multi-agent systems for software engineering: Literature review, vision, and the road ahead. <em>TOSEM</em>, <em>34</em>(5), 1-30. (<a href='https://doi.org/10.1145/3712003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Large Language Models (LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities that are competitive with human planning and reasoning. This article explores the transformative potential of integrating Large Language Models into Multi-Agent (LMA) systems for addressing complex challenges in software engineering (SE). By leveraging the collaborative and specialized abilities of multiple agents, LMA systems enable autonomous problem-solving, improve robustness, and provide scalable solutions for managing the complexity of real-world software projects. In this article, we conduct a systematic review of recent primary studies to map the current landscape of LMA applications across various stages of the software development lifecycle (SDLC). To illustrate current capabilities and limitations, we perform two case studies to demonstrate the effectiveness of state-of-the-art LMA frameworks. Additionally, we identify critical research gaps and propose a comprehensive research agenda focused on enhancing individual agent capabilities and optimizing agent synergy. Our work outlines a forward-looking vision for developing fully autonomous, scalable, and trustworthy LMA systems, laying the foundation for the evolution of Software Engineering 2.0.},
  archive      = {J_TOSEM},
  author       = {Junda He and Christoph Treude and David Lo},
  doi          = {10.1145/3712003},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {LLM-based multi-agent systems for software engineering: Literature review, vision, and the road ahead},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software engineering for collective cyber-physical ecosystems. <em>TOSEM</em>, <em>34</em>(5), 1-40. (<a href='https://doi.org/10.1145/3712004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people. While most research focuses on treating these systems as ‘composites’ (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as ‘collectives’ (i.e., uniform, collaborative and self-organising groups of entities). This article explores the motivations, state of the art and implications of this ‘collective computing paradigm’ in software engineering. In particular, it discusses its peculiar challenges, implied by characteristics like distribution, situatedness, large scale and cooperative nature. These challenges outline significant directions for future research in software engineering, touching on aspects such as macro-programming, collective intelligence, self-adaptive middleware, learning/synthesis of collective behaviour, human involvement, safety and security in collective cyber-physical ecosystems.},
  archive      = {J_TOSEM},
  author       = {Roberto Casadei and Gianluca Aguzzi and Giorgio Audrito and Ferruccio Damiani and Danilo Pianini and Giordano Scarso and Gianluca Torta and Mirko Viroli},
  doi          = {10.1145/3712004},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-40},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Software engineering for collective cyber-physical ecosystems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The current challenges of software engineering in the era of large language models. <em>TOSEM</em>, <em>34</em>(5), 1-30. (<a href='https://doi.org/10.1145/3712005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of large language models (LLMs) in the AI area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape’s challenges and opportunities. The article aims at revisiting the software development lifecycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The article first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as SE and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement and design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.},
  archive      = {J_TOSEM},
  author       = {Cuiyun Gao and Xing Hu and Shan Gao and Xin Xia and Zhi Jin},
  doi          = {10.1145/3712005},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {The current challenges of software engineering in the era of large language models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Engineering digital systems for humanity: A research roadmap. <em>TOSEM</em>, <em>34</em>(5), 1-33. (<a href='https://doi.org/10.1145/3712006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As testified by new regulations like the European AI Act, worries about the human and societal impact of (autonomous) software technologies are becoming of public concern. Human, societal, and environmental values, alongside traditional software quality, are increasingly recognized as essential for sustainability and long-term well-being. Traditionally, systems are engineered taking into account business goals and technology drivers. Considering the growing awareness in the community, in this article, we argue that engineering of systems should also consider human, societal, and environmental drivers. Then, we identify the macro and technological challenges by focusing on humans and their role while co-existing with digital systems. The first challenge considers humans in a proactive role when interacting with digital systems, i.e., taking initiative in making things happen instead of reacting to events. The second concerns humans having a reactive role in interacting with digital systems, i.e., humans interacting with digital systems as a reaction to events. The third challenge focuses on humans with a passive role, i.e., they experience, enjoy or even suffer the decisions and/or actions of digital systems. The fourth challenge concerns the duality of trust and trustworthiness, with humans playing any role. Building on the new human, societal, and environmental drivers and the macro and technological challenges, we identify a research roadmap of digital systems for humanity. The research roadmap is concretized in a number of research directions organized into four groups: development process, requirements engineering, software architecture and design, and verification and validation.},
  archive      = {J_TOSEM},
  author       = {Marco Autili and Martina De Sanctis and Paola Inverardi and Patrizio Pelliccione},
  doi          = {10.1145/3712006},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Engineering digital systems for humanity: A research roadmap},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovating for tomorrow: The convergence of software engineering and green AI. <em>TOSEM</em>, <em>34</em>(5), 1-13. (<a href='https://doi.org/10.1145/3712007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The latest advancements in machine learning, specifically in foundation models, are revolutionizing the frontiers of existing software engineering (SE) processes. This is a bi-directional phenomenon, where (1) software systems are now challenged to provide AI-enabled features to their users, and (2) AI is used to automate tasks within the software development lifecycle. In an era where sustainability is a pressing societal concern, our community needs to adopt a long-term plan enabling a conscious transformation that aligns with environmental sustainability values. In this article, we reflect on the impact of adopting environmentally friendly practices to create AI-enabled software systems and make considerations on the environmental impact of using foundation models for software development.},
  archive      = {J_TOSEM},
  author       = {Luís Cruz and Xavier Franch and Silverio Martínez-Fernández},
  doi          = {10.1145/3712007},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-13},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Innovating for tomorrow: The convergence of software engineering and green AI},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automation in model-driven engineering: A look back, and ahead. <em>TOSEM</em>, <em>34</em>(5), 1-25. (<a href='https://doi.org/10.1145/3712008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-Driven Engineering (MDE) provides a huge body of knowledge of automation for many different engineering tasks, especially those involving transitioning from design to implementation. With the huge progress made in AI, questions arise about the future of MDE, such as how existing MDE techniques and technologies can be improved or how other activities that currently lack dedicated support can also be automated. However, at the same time, it has to be revisited where and how models should be used to keep the engineers in the loop for creating, operating, and maintaining complex systems. To trigger dedicated research on these open points, we discuss the history of automation in MDE and present perspectives on how automation in MDE can be further improved and which obstacles have to be overcome in both the medium and long-term.},
  archive      = {J_TOSEM},
  author       = {Lola Burgueño and Davide Di Ruscio and Houari Sahraoui and Manuel Wimmer},
  doi          = {10.1145/3712008},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automation in model-driven engineering: A look back, and ahead},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research directions in software supply chain security. <em>TOSEM</em>, <em>34</em>(5), 1-38. (<a href='https://doi.org/10.1145/3714464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reusable software libraries, frameworks, and components, such as those provided by open source ecosystems and third-party suppliers, accelerate digital innovation. However, recent years have shown almost exponential growth in attackers leveraging these software artifacts to launch software supply chain attacks. Past well-known software supply chain attacks include the SolarWinds, log4j, and xz utils incidents. Supply chain attacks are considered to have three major attack vectors: through vulnerabilities and malware accidentally or intentionally injected into open source and third-party dependencies/components/containers ; by infiltrating the build infrastructure during the build and deployment processes; and through targeted techniques aimed at the humans involved in software development, such as through social engineering. Plummeting trust in the software supply chain could decelerate digital innovation if the software industry reduces its use of open source and third-party artifacts to reduce risks. This article contains perspectives and knowledge obtained from intentional outreach with practitioners to understand their practical challenges and from extensive research efforts. We then provide an overview of current research efforts to secure the software supply chain. Finally, we propose a future research agenda to close software supply chain attack vectors and support the software industry.},
  archive      = {J_TOSEM},
  author       = {Laurie Williams and Giacomo Benedetti and Sivana Hamer and Ranindya Paramitha and Imranur Rahman and Mahzabin Tamanna and Greg Tystahl and Nusrat Zahan and Patrick Morrison and Yasemin Acar and Michel Cukier and Christian Kästner and Alexandros Kapravelos and Dominik Wermke and William Enck},
  doi          = {10.1145/3714464},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Research directions in software supply chain security},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The future of AI-driven software engineering. <em>TOSEM</em>, <em>34</em>(5), 1-20. (<a href='https://doi.org/10.1145/3715003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A paradigm shift is underway in Software Engineering, with AI systems such as LLMs playing an increasingly important role in boosting software development productivity. This trend is anticipated to persist. In the next years, we expect a growing symbiotic partnership between human software developers and AI. The Software Engineering research community cannot afford to overlook this trend; we must address the key research challenges posed by the integration of AI into the software development process. In this article, we present our vision of the future of software development in an AI-driven world and explore the key challenges that our research community should address to realize this vision.},
  archive      = {J_TOSEM},
  author       = {Valerio Terragni and Annie Vella and Partha Roop and Kelly Blincoe},
  doi          = {10.1145/3715003},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-20},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {The future of AI-driven software engineering},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing and debugging quantum programs: The road to 2030. <em>TOSEM</em>, <em>34</em>(5), 1-46. (<a href='https://doi.org/10.1145/3715106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing has existed in the theoretical realm for several decades. Recently, quantum computing has re-emerged as a promising technology to solve problems that a classical computer could take hundreds of years to solve. However, there are challenges and opportunities for academics and practitioners regarding software engineering practices for testing and debugging quantum programs. This article presents a roadmap for addressing these challenges, pointing out the existing gaps in the literature and suggesting research directions. We discuss the limitations caused by noise, the no-cloning theorem, the lack of a standard architecture for quantum computers, among others. Regarding testing, we highlight gaps and opportunities related to transpilation, mutation analysis, input states with hybrid interfaces, program analysis, and coverage. For debugging, we present the current strategies, including classical techniques applied to quantum programs, quantum-specific assertions, and quantum-related bug patterns. We introduce a conceptual model to illustrate concepts regarding the testing and debugging of quantum programs and the relationship between them. Those concepts are used to identify and discuss research challenges to cope with quantum programs through 2030, focusing on the interfaces between classical and quantum computing and on creating testing and debugging techniques that take advantage of the unique quantum computing characteristics.},
  archive      = {J_TOSEM},
  author       = {Neilson Carlos Leite Ramalho and Higor Amario de Souza and Marcos Lordello Chaim},
  doi          = {10.1145/3715106},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-46},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Testing and debugging quantum programs: The road to 2030},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test oracle automation in the era of LLMs. <em>TOSEM</em>, <em>34</em>(5), 1-24. (<a href='https://doi.org/10.1145/3715107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effectiveness of a test suite in detecting faults highly depends on the quality of its test oracles. Large Language Models (LLMs) have demonstrated remarkable proficiency in tackling diverse software testing tasks. This article aims to present a roadmap for future research on the use of LLMs for test oracle automation. We discuss the progress made in the field of test oracle automation before the introduction of LLMs, identifying the main limitations and weaknesses of existing techniques. Additionally, we discuss recent studies on the use of LLMs for this task, highlighting the main challenges that arise from their use, e.g., how to assess quality and usefulness of the generated oracles. We conclude with a discussion about the directions and opportunities for future research on LLM-based oracle automation.},
  archive      = {J_TOSEM},
  author       = {Facundo Molina and Alessandra Gorla and Marcelo d’Amorim},
  doi          = {10.1145/3715107},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Test oracle automation in the era of LLMs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software engineering by and for humans in an AI era. <em>TOSEM</em>, <em>34</em>(5), 1-46. (<a href='https://doi.org/10.1145/3715111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The landscape of software engineering is undergoing a transformative shift driven by advancements in machine learning, Artificial Intelligence (AI), and autonomous systems. This roadmap article explores how these technologies are reshaping the field, positioning humans not only as end users but also as critical components within expansive software ecosystems. We examine the challenges and opportunities arising from this human-centered paradigm, including ethical considerations, fairness, and the intricate interplay between technical and human factors. By recognizing humans at the heart of the software lifecycle—spanning professional engineers, end users, and end user developers—we emphasize the importance of inclusivity, human-aligned workflows, and the seamless integration of AI-augmented socio-technical systems. As software systems evolve to become more intelligent and human-centric, software engineering practices must adapt to this new reality. This article provides a comprehensive examination of this transformation, outlining current trends, key challenges, and opportunities that define the emerging research and practice landscape, and envisioning a future where software engineering and AI work synergistically to place humans at the core of the ecosystem.},
  archive      = {J_TOSEM},
  author       = {Silvia Abrahão and John Grundy and Mauro Pezzè and Margaret-Anne Storey and Damian A. Tamburri},
  doi          = {10.1145/3715111},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-46},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Software engineering by and for humans in an AI era},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). With great power comes great responsibility: The role of software engineers. <em>TOSEM</em>, <em>34</em>(5), 1-21. (<a href='https://doi.org/10.1145/3715112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The landscape of Software Engineering evolves rapidly amidst digital transformation and the ascendancy of AI, leading to profound shifts in the role and responsibilities of Software Engineers. This evolution encompasses both immediate changes, such as the adoption of Large Language Model-based approaches to coding, and deeper shifts driven by the profound societal and environmental impacts of technology. Despite the urgency, there persists a lag in adapting to these evolving roles. This roadmap article proposes 10 research challenges to develop a new generation of Software Engineers equipped to navigate the technical and social complexities as well as ethical considerations inherent in their evolving profession. Furthermore, the challenges target role definition, integration of AI, education transformation, standards evolution, and impact assessment to equip future Software Engineers to skillfully and responsibly handle the obstacles within their transforming discipline.},
  archive      = {J_TOSEM},
  author       = {Stefanie Betz and Birgit Penzenstadler},
  doi          = {10.1145/3715112},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-21},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {With great power comes great responsibility: The role of software engineers},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foundation model engineering: Engineering foundation models just as engineering software. <em>TOSEM</em>, <em>34</em>(5), 1-18. (<a href='https://doi.org/10.1145/3719005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By treating data and models as source code, Foundation Models (FMs) become a new type of software. Mirroring the concept of software crisis, the increasing complexity of FMs makes FM crisis a tangible concern in the coming decade, appealing for new theories and methodologies from the field of software engineering. In this article, we outline our vision of introducing FM engineering, a strategic response to the anticipated FM crisis with principled engineering methodologies. FM engineering aims to mitigate potential issues in FM development and application through the introduction of declarative, automated, and unified programming interfaces for both data and model management, reducing the complexities involved in working with FMs by providing a more structured and intuitive process for developers. Through the establishment of FM engineering, we aim to provide a robust, automated, and extensible framework that addresses the imminent challenges, and discover new research opportunities for the software engineering field.},
  archive      = {J_TOSEM},
  author       = {Dezhi Ran and Mengzhou Wu and Wei Yang and Tao Xie},
  doi          = {10.1145/3719005},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-18},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Foundation model engineering: Engineering foundation models just as engineering software},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence for software engineering: The journey so far and the road ahead. <em>TOSEM</em>, <em>34</em>(5), 1-27. (<a href='https://doi.org/10.1145/3719006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence and recent advances in deep learning architectures, including transformer networks and large language models, change the way people think and act to solve problems. Software engineering, as an increasingly complex process to design, develop, test, deploy, and maintain large-scale software systems for solving real-world challenges, is profoundly affected by many revolutionary artificial intelligence tools in general and machine learning in particular. In this roadmap for artificial intelligence in software engineering, we highlight the recent deep impact of artificial intelligence on software engineering by discussing successful stories of applications of artificial intelligence to classic and new software development challenges. We identify the new challenges that the software engineering community has to address in the coming years to successfully apply artificial intelligence in software engineering, and we share our research roadmap toward the effective use of artificial intelligence in the software engineering profession, while still protecting fundamental human values. We spotlight three main areas that challenge the research in software engineering: the use of generative artificial intelligence and large language models for engineering large software systems, the need of large and unbiased datasets and benchmarks for training and evaluating deep learning and large language models for software engineering, and the need of a new code of digital ethics to apply artificial intelligence in software engineering.},
  archive      = {J_TOSEM},
  author       = {Iftekhar Ahmed and Aldeida Aleti and Haipeng Cai and Alexander Chatzigeorgiou and Pinjia He and Xing Hu and Mauro Pezzè and Denys Poshyvanyk and Xin Xia},
  doi          = {10.1145/3719006},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Artificial intelligence for software engineering: The journey so far and the road ahead},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When ChatGPT meets smart contract vulnerability detection: How far are we?. <em>TOSEM</em>, <em>34</em>(4), 1-30. (<a href='https://doi.org/10.1145/3702973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of blockchain technology, smart contracts have become an important component of blockchain applications. Despite their crucial role, the development of smart contracts may introduce vulnerabilities and potentially lead to severe consequences, such as financial losses. Meanwhile, large language models, represented by ChatGPT, have gained great attention, showcasing great capabilities in code analysis tasks. In this article, we presented an empirical study to investigate the performance of ChatGPT in identifying smart contract vulnerabilities. Initially, we evaluated ChatGPT’s effectiveness using a publicly available smart contract dataset. Our findings discover that while ChatGPT achieves a high recall rate, its precision in pinpointing smart contract vulnerabilities is limited. Furthermore, ChatGPT’s performance varies when detecting different vulnerability types. We delved into the root causes for the false positives generated by ChatGPT, and categorized them into four groups. Second, by comparing ChatGPT with other state-of-the-art smart contract vulnerability detection tools, we found that ChatGPT’s F-score is lower than others for 3 out of the 7 vulnerabilities. In the case of the remaining 4 vulnerabilities, ChatGPT exhibits a slight advantage over these tools. Finally, we analyzed the limitation of ChatGPT in smart contract vulnerability detection, revealing that the robustness of ChatGPT in this field needs to be improved from two aspects: its uncertainty in answering questions; and the limited length of the detected code. In general, our research provides insights into the strengths and weaknesses of employing large language models, specifically ChatGPT, for the detection of smart contract vulnerabilities.},
  archive      = {J_TOSEM},
  author       = {Chong Chen and Jianzhong Su and Jiachi Chen and Yanlin Wang and Tingting Bi and Jianxing Yu and Yanli Wang and Xingwei Lin and Ting Chen and Zibin Zheng},
  doi          = {10.1145/3702973},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {When ChatGPT meets smart contract vulnerability detection: How far are we?},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FunFuzz: Greybox fuzzing with function significance. <em>TOSEM</em>, <em>34</em>(4), 1-34. (<a href='https://doi.org/10.1145/3702974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Greybox fuzzing is dedicated to revealing software bugs by maximizing code coverage. Concentrating on code coverage, greybox fuzzing effectively exposes bugs in real-world programs by continuously executing the program under test (PUT) with the test inputs generated from initial seeds, making it a popular software testing technique. Although powerful, the effectiveness of greybox fuzzing can be restricted in some cases. Ignoring the significant degrees of executed functions, traditional greybox fuzzing usually fails to identify significant seeds that execute more significant functions, and thus may assign similar energy to significant and trivial seeds when conducting power scheduling. As a result, the effectiveness of greybox fuzzing can be degraded due to wasting too much energy on trivial seeds. In this paper, we introduce function significance (FS) to measure the significant degrees of functions. Our key insight is that the influential functions that connect to many other functions are significant to greybox fuzzing as they provide more probabilities to reach previously unexplored code regions. To quantify FS, we conduct influence analysis upon the call graphs extracted from the PUTs to obtain the centrality values of function nodes. With FS as the significance measurement, we further propose FunFuzz , an FS-aware greybox fuzzing technique, to optimize significant seeds and tackle the aforementioned restriction. To this end, FunFuzz dynamically tracks the functions executed by a seed during fuzzing, and computes the significance score for the seed by accumulating the FS values of the functions executed by it. Based on the computed FS values, FunFuzz then takes an estimation-based power scheduling to assign more (or less) energy to seeds that achieve over-estimated (or under-estimated) significance scores. Specifically, the seed energy is adjusted by multiplying with a scale factor computed regarding the ratio of the actual significance score achieved by executing the seed and the estimated significance score predicted by a linear model constructed on-the-fly. To evaluate FunFuzz , we prototype it on top of AFL++ and conduct experiments with 15 programs, of which 10 are from common real-world projects and five are from Magma, and compare it to seven popular fuzzers. The experimental results obtained through fuzzing exceeding 40,800 CPU hours show that: (1) In terms of covering code, FunFuzz outperforms AFL++ by achieving 0.1%–18.4% more region coverage on 13 out of 15 targets. (2) In terms of finding bugs, FunFuzz unveils 114 unique crashes and 25 Magma bugs (which are derived from CVEs) in 20 trials of 24-hour fuzzing, which are the most compared to the competitor fuzzers and include 32 crashes and 1 Magma bug that the other fuzzers fail to discover. Besides the experiments focusing on code coverage and bug finding, we evaluate the key components of FunFuzz , namely the FS-centered estimation-based power scheduling and the lazy FS computation mechanism. The extensive evaluation not only suggests FunFuzz ’s superiority in code coverage and bug finding, but also demonstrates the effectiveness of the two components.},
  archive      = {J_TOSEM},
  author       = {Ruixiang Qian and Quanjun Zhang and Chunrong Fang and Lihua Guo and Zhenyu Chen},
  doi          = {10.1145/3702974},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-34},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {FunFuzz: Greybox fuzzing with function significance},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demystifying react native android apps for static analysis. <em>TOSEM</em>, <em>34</em>(4), 1-33. (<a href='https://doi.org/10.1145/3702977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {React Native, an open source framework, simplifies cross-platform app development by allowing JavaScript-side code to interact with native-side code. Previous studies disregarded React Native, resulting in insufficient static analysis of React Native app code. This study initiates the investigation of challenges when statically analyzing React Native apps. We propose ReuNify to improve Soot-based static analysis coverage for JavaScript-side and native-side code. ReuNify converts Hermes bytecode to Soot’s intermediate representation. Hermes bytecode, compiled from JavaScript code and integrated into React Native apps, possesses a unique syntax that eludes current JavaScript analyzers. Additionally, we investigate opcode distribution and conduct in-depth analyses of the usage of opcode between popular apps and malware. We also propose a benchmark consisting of 97 control flow-related cases to validate the control flow recovery of the generated intermediate representation. Furthermore, we model the cross-language communication mechanisms of React Native to expand the static analysis coverage for native-side code. Our evaluation demonstrates that ReuNify enables an average increase of 84% in reached nodes within the callgraph and further identifies an average of two additional privacy leaks in taint analysis. In summary, this article demonstrates that ReuNify significantly improves the static analysis for the React Native Android apps.},
  archive      = {J_TOSEM},
  author       = {Yonghui Liu and Xiao Chen and Pei Liu and Jordan Samhi and John Grundy and Chunyang Chen and Li Li},
  doi          = {10.1145/3702977},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Demystifying react native android apps for static analysis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ZS4C: Zero-shot synthesis of compilable code for incomplete code snippets using LLMs. <em>TOSEM</em>, <em>34</em>(4), 1-30. (<a href='https://doi.org/10.1145/3702979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technical Q&A sites are valuable for software developers seeking knowledge, but the code snippets they provide are often uncompilable and incomplete due to unresolved types and missing libraries. This poses a challenge for users who wish to reuse or analyze these snippets. Existing methods either do not focus on creating compilable code or have low success rates. To address this, we propose ZS4C, a lightweight approach for zero-shot synthesis of compilable code from incomplete snippets using Large Language Models (LLMs). ZS4C operates in two stages: first, it uses an LLM, like GPT-3.5, to identify missing import statements in a snippet; second, it collaborates with a validator (e.g., compiler) to fix compilation errors caused by incorrect imports and syntax issues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset, Python-SO, which includes 539 Python snippets from Stack Overflow across the 20 most popular Python libraries. ZS4C significantly outperforms existing methods, improving the compilation rate from 63% to 95.1% compared to the state-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer more accurate import statements (with an F1 score of 0.98) than SnR, with an improvement of 8.5% in the F1.},
  archive      = {J_TOSEM},
  author       = {Azmain Kabir and Shaowei Wang and Yuan Tian and Tse-Hsun (Peter) Chen and Muhammad Asaduzzaman and Wenbin Zhang},
  doi          = {10.1145/3702979},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {ZS4C: Zero-shot synthesis of compilable code for incomplete code snippets using LLMs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trained without my consent: Detecting code inclusion in language models trained on code. <em>TOSEM</em>, <em>34</em>(4), 1-46. (<a href='https://doi.org/10.1145/3702980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers’ codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM’s training dataset. We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM. In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%. In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pairwise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets.},
  archive      = {J_TOSEM},
  author       = {Vahid Majdinasab and Amin Nikanjam and Foutse Khomh},
  doi          = {10.1145/3702980},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-46},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Trained without my consent: Detecting code inclusion in language models trained on code},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unifying model execution and deductive verification with interaction trees in Isabelle/HOL. <em>TOSEM</em>, <em>34</em>(4), 1-40. (<a href='https://doi.org/10.1145/3702981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model execution allows us to prototype and analyse software engineering models by stepping through their possible behaviours, using techniques like animation and simulation. On the other hand, deductive verification allows us to construct formal proofs demonstrating satisfaction of certain critical properties in support of high-assurance software engineering. To ensure coherent results between execution and proof, we need unifying semantics and automation. In this article, we mechanise Interaction Trees (ITrees) in Isabelle/HOL to produce an execution and verification framework. ITrees are coinductive structures that allow us to encode infinite labelled transition systems, yet they are inherently executable. We use ITrees to create verification tools for stateful imperative programs, concurrent programs with message passing in the form of the CSP and Circus languages, and abstract system models in the style of the Z and B methods. We demonstrate how ITrees can account for diverse semantic presentations, such as structural operational semantics, a relational program model, and CSP's failures-divergences trace model. Finally, we demonstrate how ITrees can be executed using the Isabelle code generator to support the animation of models.},
  archive      = {J_TOSEM},
  author       = {Simon Foster and Chung-Kil Hur and Jim Woodcock},
  doi          = {10.1145/3702981},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-40},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Unifying model execution and deductive verification with interaction trees in Isabelle/HOL},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Twenty years later: Evaluating the adoption of control flow integrity. <em>TOSEM</em>, <em>34</em>(4), 1-30. (<a href='https://doi.org/10.1145/3702982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory corruption vulnerabilities still allow compromising computers through software written in a memory-unsafe language such as C/C++. This highlights that mitigation techniques to prevent such exploitations are not all widely deployed. In this article, we introduce SeeCFI , a tool to detect the presence of a memory corruption mitigation technique called Control Flow Integrity (CFI). We leverage SeeCFI to investigate to what extent the mitigation has been deployed in complex software systems such as Android and specific Linux distributions (Ubuntu and Debian). Our results indicate that the overall adoption of CFI (forward- and backward-edge) is increasing across Android versions (~30% in Android 13) but remains the same low ( \(\lt\) 1%) throughout different Linux versions. Our tool, SeeCFI , offers the possibility to identify which binaries in a system were compiled using the CFI option. This can be deployed by external security researchers to efficiently decide which binaries to prioritize when fixing vulnerabilities and how to fix them. Therefore, SeeCFI can help to make software systems more secure.},
  archive      = {J_TOSEM},
  author       = {Sabine Houy and Alexandre Bartel},
  doi          = {10.1145/3702982},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Twenty years later: Evaluating the adoption of control flow integrity},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Repairs and breaks prediction for deep neural networks. <em>TOSEM</em>, <em>34</em>(4), 1-42. (<a href='https://doi.org/10.1145/3702983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing prevalence of software incorporating deep neural networks (DNNs), quality assurance for these software systems has become a crucial concern. To this end, various methods have been proposed to repair the misbehavior of DNNs by modifying their weights. However, these repair methods may not meet the developer’s needs for a given dataset and model. In this study, we build prediction models for repair outcomes (i.e., repairs and breaks) to help determine whether the repair method is likely to work. By using our prediction models, developers and operators of DNNs can decide whether or not to apply a repair method, and if so, which method to use. Our prediction models utilize four metrics as explanatory metrics that represent the confidence or ambiguity in the DNN predictions. We experimented with four repair methods and 10 datasets. The experimental results demonstrate that our prediction models successfully select a repair method that meets developers’ needs in 16 out of 24 cases, resulting in an average time saving of 16.29% compared to the naive method. Based on these results, our prediction models can reduce costs for developers and operators when deciding whether to employ repair methods for real-world applications of DNNs.},
  archive      = {J_TOSEM},
  author       = {Yuta Ishimoto and Masanari Kondo and Lei Ma and Naoyasu Ubayashi and Yasutaka Kamei},
  doi          = {10.1145/3702983},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-42},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Repairs and breaks prediction for deep neural networks},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multivariate time series anomaly detection through transfer learning for large-scale software systems. <em>TOSEM</em>, <em>34</em>(4), 1-25. (<a href='https://doi.org/10.1145/3702984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely anomaly detection of multivariate time series (MTS) is of vital importance for managing large-scale software systems. However, many deep learning-based MTS anomaly detection models require long-term MTS training data to achieve optimal performance, which often conflicts with the frequent pattern changes observed in software systems. Moreover, the training overhead of vast MTS in large-scale software systems is unacceptably high. To address these issues, we design OmniTransfer , a model-agnostic framework that combines weighted hierarchical agglomerative clustering with an adaptive transfer learning strategy, making many state-of-the-art (SOTA) MTS anomaly detection models efficient and effective. Extensive experiments using real-world data from a large web content service provider and a network operator show that OmniTransfer significantly reduces the model initialization time by 46.49% and the training cost by 74.51%, while maintaining high accuracy in detecting anomalies.},
  archive      = {J_TOSEM},
  author       = {Yongqian Sun and Minghan Liang and Shenglin Zhang and Zeyu Che and Zhiyao Luo and Dongwen Li and Yuzhi Zhang and Dan Pei and Lemeng Pan and Liping Hou},
  doi          = {10.1145/3702984},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Efficient multivariate time series anomaly detection through transfer learning for large-scale software systems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Llasm: Naming functions in binaries by fusing encoder-only and decoder-only LLMs. <em>TOSEM</em>, <em>34</em>(4), 1-22. (<a href='https://doi.org/10.1145/3702988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting function names in stripped binaries, which requires succinctly summarizing semantics of binary code in natural languages, is a crucial but challenging task. Recently, many machine learning based solutions have been proposed. However, they have poor generalizability, i.e., fail to handle unseen binaries. To advance the state of the art, we present large assembly language Model ( llasm ) , a novel framework which fuses encoder-only and decoder-only LLMs for function name prediction. It refines encoder-only models to preserve more binary information and learn better binary representations. Then it adopts a novel architecture to project the encoding to the input space of a decoder-only natural language model, which enables it to have better capability of inferring general knowledge and better generalizability. We have evaluated llasm in the BinaryCorp and Debin datasets. llasm outperforms the state-of-the-art function name prediction tools by up to 19.9%, 40.7%, and 36.5% in precision, recall, and F1 score, with significantly better generalizability in unseen binaries. Our case studies further demonstrate the practical use cases of llasm in analyzing real-world malware, showing the usefulness of function name prediction.},
  archive      = {J_TOSEM},
  author       = {Zihan Sha and Hao Wang and Zeyu Gao and Hui Shu and Bolun Zhang and Ziqing Wang and Chao Zhang},
  doi          = {10.1145/3702988},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Llasm: Naming functions in binaries by fusing encoder-only and decoder-only LLMs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal-incremental learning for android malware detection. <em>TOSEM</em>, <em>34</em>(4), 1-30. (<a href='https://doi.org/10.1145/3702990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware classification is a specific and refined task within the broader malware detection problem. Effective classification aids in understanding attack techniques and developing robust defenses, ensuring application security and timely mitigation of software vulnerabilities. The dynamic nature of malware demands adaptive classification techniques that can handle the continuous emergence of new families. Traditionally, this is done by retraining models on all historical samples, which requires significant resources in terms of time and storage. An alternative approach is Class-Incremental Learning (CIL), which focuses on progressively learning new classes (malware families) while preserving knowledge from previous training steps. However, CIL assumes that each class appears only once in training and is not revisited, an assumption that does not hold for malware families, which often persist across multiple time intervals. This leads to shifts in the data distribution for the same family over time, a challenge that is not addressed by traditional CIL methods. We formulate this problem as Temporal-Incremental Malware Learning (TIML), which adapts to these shifts and effectively classifies new variants. To support this, we organize the MalNet dataset, consisting of over a million entries of Android malware data collected over a decade, in chronological order. We first adapt state-of-the-art CIL approaches to meet TIML’s requirements, serving as baseline methods. Then, we propose a novel multimodal TIML approach that leverages multiple malware modalities for improved performance. Extensive evaluations show that our TIML approaches outperform traditional CIL methods and demonstrate the feasibility of periodically updating malware classifiers at a low cost. This process is efficient and requires minimal storage and computational resources, with only a slight dip in performance compared to full retraining with historical data.},
  archive      = {J_TOSEM},
  author       = {Tiezhu Sun and Nadia Daoudi and Weiguo Pian and Kisub Kim and Kevin Allix and Tegawendé F. Bissyandé and Jacques Klein},
  doi          = {10.1145/3702990},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Temporal-incremental learning for android malware detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward better comprehension of breaking changes in the NPM ecosystem. <em>TOSEM</em>, <em>34</em>(4), 1-23. (<a href='https://doi.org/10.1145/3702991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code evolution is prevalent in software ecosystems, which can provide many benefits, such as new features, bug fixes, security patches, while still introducing breaking changes that make downstream projects fail to work. Breaking changes cause a lot of effort to both downstream and upstream developers: downstream developers need to adapt to breaking changes and upstream developers are responsible for identifying and documenting them. In the NPM ecosystem, characterized by frequent code changes and a high tolerance for making breaking changes, the effort is larger. For better comprehension of breaking changes in the NPM ecosystem and to enhance breaking change detection tools, we conduct a large-scale empirical study to investigate breaking changes in the NPM ecosystem. We construct a dataset of explicitly documented breaking changes from 381 popular NPM projects. We find that 95.4% of the detected breaking changes can be covered by developers’ documentation, and 19% of the breaking changes cannot be detected by regression testing. Then in the process of investigating source code of our collected breaking changes, we yield a taxonomy of JavaScript- and TypeScript-specific syntactic breaking changes and a taxonomy of major types of behavioral breaking changes. Additionally, we investigate the reasons why developers make breaking changes in NPM and find three major reasons, i.e., to reduce code redundancy, to improve identifier names, and to improve API design, and each category contains several sub-items. We provide actionable implications for future research, e.g., automatic naming and renaming techniques should be applied in JavaScript projects to improve identifier names, future research can try to detect more types of behavioral breaking changes. By presenting the implications, we also discuss the weakness of automatic renaming and breaking change detection approaches, such as the lack of support for public identifiers and various types of breaking changes.},
  archive      = {J_TOSEM},
  author       = {Dezhen Kong and Jiakun Liu and Lingfeng Bao and David Lo},
  doi          = {10.1145/3702991},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Toward better comprehension of breaking changes in the NPM ecosystem},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DREAM: Debugging and repairing AutoML pipelines. <em>TOSEM</em>, <em>34</em>(4), 1-29. (<a href='https://doi.org/10.1145/3702992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning models have become an integrated component of modern software systems. In response to the challenge of model design, researchers proposed Automated Machine Learning (AutoML) systems, which automatically search for model architecture and hyperparameters for a given task. Like other software systems, existing AutoML systems have shortcomings in their design. We identify two common and severe shortcomings in AutoML, performance issue (i.e., searching for the desired model takes an unreasonably long time) and ineffective search issue (i.e., AutoML systems are not able to find an accurate enough model). After analyzing the workflow of AutoML, we observe that existing AutoML systems overlook potential opportunities in search space, search method, and search feedback, which results in performance and ineffective search issues. Based on our analysis, we design and implement DREAM , an automatic and general-purpose tool to alleviate and repair the shortcomings of AutoML pipelines and conduct effective model searches for diverse tasks. It monitors the process of AutoML to collect detailed feedback and automatically repairs shortcomings by expanding search space and leveraging a feedback-driven search strategy. Our evaluation results show that DREAM can be applied on two state-of-the-art AutoML pipelines and effectively and efficiently repair their shortcomings.},
  archive      = {J_TOSEM},
  author       = {Xiaoyu Zhang and Juan Zhai and Shiqing Ma and Xiaohong Guan and Chao Shen},
  doi          = {10.1145/3702992},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {DREAM: Debugging and repairing AutoML pipelines},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GUing: A mobile GUI search engine using a vision-language model. <em>TOSEM</em>, <em>34</em>(4), 1-30. (<a href='https://doi.org/10.1145/3702993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical User Interfaces (GUIs) are central to app development projects. App developers may use the GUIs of other apps as a means of requirements refinement and rapid prototyping or as a source of inspiration for designing and improving their own apps. Recent research has thus suggested retrieving relevant GUI designs that match a certain text query from screenshot datasets acquired through crowdsourced or automated exploration of GUIs. However, such text-to-GUI retrieval approaches only leverage the textual information of the GUI elements, neglecting visual information such as icons or background images. In addition, retrieved screenshots are not steered by app developers and lack app features that require particular input data. To overcome these limitations, this article proposes GUing, a GUI search engine based on a vision-language model called GUIClip, which we trained specifically for the problem of designing app GUIs. For this, we first collected from Google Play app introduction images which display the most representative screenshots and are often captioned (i.e., labeled) by app vendors. Then, we developed an automated pipeline to classify, crop, and extract the captions from these images. This resulted in a large dataset which we share with this article: including 303k app screenshots, out of which 135k have captions. We used this dataset to train a novel vision-language model, which is, to the best of our knowledge, the first of its kind for GUI retrieval. We evaluated our approach on various datasets from related work and in a manual experiment. The results demonstrate that our model outperforms previous approaches in text-to-GUI retrieval achieving a Recall@10 of up to 0.69 and a HIT@10 of 0.91. We also explored the performance of GUIClip for other GUI tasks, including GUI classification and sketch-to-GUI retrieval with encouraging results.},
  archive      = {J_TOSEM},
  author       = {Jialiang Wei and Anne-Lise Courbis and Thomas Lambolais and Binbin Xu and Pierre Louis Bernard and Gérard Dray and Walid Maalej},
  doi          = {10.1145/3702993},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {GUing: A mobile GUI search engine using a vision-language model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time rectifying flight control misconfiguration using intelligent agent. <em>TOSEM</em>, <em>34</em>(4), 1-29. (<a href='https://doi.org/10.1145/3702994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Configurations are supported by most flight control systems, allowing users to control a flying drone adapted to complexities such as environmental changes or mission alterations. Such an advanced functionality also introduces a significant problem—misconfiguration settings. It may cause drone instability, threaten drone safety, and potentially lead to substantial financial loss. However, detecting and rectifying misconfigurations across different flight control systems is challenging because (1) (mis)configuration-related code snippets might be syntactically correct and thus hard to identify through traditional code analysis; (2) the response to each configuration varies under different flying scenarios. In this article, we propose and implement a novel rectification approach, Nyctea , to detect instability caused by misconfigurations and conduct an on-the-fly rectification. Nyctea first continuously inspects state changes over consecutive time intervals and calculates the overall deviations to determine whether a drone is in a transition of instability to control loss. When a potential instability is reported, Nyctea instantly invokes a pre-trained intelligent agent to automatically generate proper configurations and then re-configure the drone against entering a state of loss of control. This process of reconfiguration is conducted iteratively until the instability is eliminated. We integrated Nyctea with the widely used flight control system, Ardupilot and PX4 . The simulated and practical experiment results showed that Nyctea successfully eliminates instabilities caused by 85% of misconfigurations. For each misconfiguration, Nyctea averagely generated 4 to 5 configurations to achieve a successful rectification.},
  archive      = {J_TOSEM},
  author       = {Ruidong Han and Shangzhi Xu and Juanru Li and Elisa Bertino and David Lo and JianFeng Ma and Siqi Ma},
  doi          = {10.1145/3702994},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Real-time rectifying flight control misconfiguration using intelligent agent},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distinguishing LLM-generated from human-written code by contrastive learning. <em>TOSEM</em>, <em>34</em>(4), 1-31. (<a href='https://doi.org/10.1145/3705300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs), such as ChatGPT released by OpenAI, have attracted significant attention from both industry and academia due to their demonstrated ability to generate high-quality content for various tasks. Despite the impressive capabilities of LLMs, there are growing concerns regarding their potential risks in various fields, such as news, education, and software engineering. Recently, several commercial and open source LLM-generated content detectors have been proposed, which, however, are primarily designed for detecting natural language content without considering the specific characteristics of program code. This article aims to fill this gap by proposing a novel ChatGPT-generated code detector, CodeGPTSensor, based on a contrastive learning framework and a semantic encoder built with UniXcoder. To assess the effectiveness of CodeGPTSensor on differentiating ChatGPT-generated code from human-written code, we first curate a large-scale Human and Machine comparison Corpus (HMCorp), which includes 550k pairs of human-written and ChatGPT-generated code (i.e., 288k Python code pairs and 222k Java code pairs). Based on the HMCorp dataset, our qualitative and quantitative analysis of the characteristics of ChatGPT-generated code reveals the challenge and opportunity of distinguishing ChatGPT-generated code from human-written code with their representative features. Our experimental results indicate that CodeGPTSensor can effectively identify ChatGPT-generated code, outperforming all selected baselines.},
  archive      = {J_TOSEM},
  author       = {Xiaodan Xu and Chao Ni and Xinrong Guo and Shaoxuan Liu and Xiaoya Wang and Kui Liu and Xiaohu Yang},
  doi          = {10.1145/3705300},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Distinguishing LLM-generated from human-written code by contrastive learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IATT: Interpretation analysis-based transferable test generation for convolutional neural networks. <em>TOSEM</em>, <em>34</em>(4), 1-34. (<a href='https://doi.org/10.1145/3705301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have been widely used in various fields. However, it is essential to perform sufficient testing to detect internal defects before deploying CNNs, especially in security-sensitive scenarios. Generating error-inducing inputs to trigger erroneous behavior is the primary way to detect CNN model defects. However, in practice, when the model under test is a black-box CNN model without accessible internal information, in some scenarios it is still necessary to generate high-quality test inputs within a limited testing budget. In such a new scenario, a potential approach is to generate transferable test inputs by analyzing the internal knowledge of other white-box CNN models similar to the model under test, and then use transferable test inputs to test the black-box CNN model. The main challenge in generating transferable test inputs is how to improve their error-inducing capability for different CNN models without changing the test oracle. We found that different CNN models make predictions based on features of similar important regions in images. Adding targeted perturbations to important regions will generate transferable test inputs with high realism. Therefore, we propose the Interpretable Analysis-based Transferable Test (IATT) Generation method for CNNs, which employs interpretation methods of CNN models to explain and localize important regions in test inputs, using backpropagation optimizer and perturbation mask process to add targeted perturbations to these important regions, thereby generating transferable test inputs. This process is repeated to iteratively optimize the transferability and realism of the test inputs. To verify the effectiveness of IATT, we perform experimental studies on nine deep learning models, including ResNet-50 and Vit-B/16, and commercial computer vision system Google Cloud Vision , and compared our method with four state-of-the-art baseline methods. Experimental results show that transferable test inputs generated by IATT can effectively cause black-box target models to output incorrect results. Compared to existing testing and adversarial attack methods, the average Error-inducing Success Rate (ESR) in different testing scenarios is 18.1%–52.7% greater than the baseline methods. Additionally, the test inputs generated by IATT achieve high ESR while maintaining high realism.},
  archive      = {J_TOSEM},
  author       = {Ruilin Xie and Xiang Chen and Qifan He and Bixin Li and Zhanqi Cui},
  doi          = {10.1145/3705301},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-34},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {IATT: Interpretation analysis-based transferable test generation for convolutional neural networks},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-based transfer learning for structuring fault localization and program repair automation. <em>TOSEM</em>, <em>34</em>(4), 1-32. (<a href='https://doi.org/10.1145/3705302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated software debugging plays a crucial role in aiding software developers to swiftly identify and attempt to rectify faults, thereby significantly reducing developers’ workload. Previous researches have predominantly relied on simplistic semantic deep learning or statistical analysis methods to locate faulty statements in diverse projects. However, code repositories often consist of lengthy sequences with long-distance dependencies, posing challenges for accurately modeling fault localization using these methods. In addition, the lack of joint reasoning among various faults prevents existing models from deeply capturing fault information. To address these challenges, we propose a method named CodeHealer to achieve accurate fault localization and program repair. CodeHealer comprises three components: a Deep Semantic Information Extraction Component that effectively extracts deep semantic features from suspicious code statements using classifiers based on Joint-attention mechanisms; a Suspicious Statement Ranking Component that combines various fault localization features and employs multilayer perceptrons to derive multidimensional vectors of suspicion values; and a Fault Repair Component that, based on ranked suspicious statements generated by fault localization, adopts a top-down approach using multiple classifiers based on Co-teaching mechanisms to select repair templates and generate patches. The experimental results indicate that when applied to fault localization, CodeHealer outperforms the best baseline method with improvements of 11.4%, 2.7%, and 1.6% on Top-1/3/5 metrics, respectively. It also reduces the MFR and MAR by 9.8% and 2.1%, where lower values denote better fault localization effectiveness. Additionally, in automated software debugging, CodeHealer fixes an additional 6 faults compared to the current best method, totaling 53 faults repaired.},
  archive      = {J_TOSEM},
  author       = {Lehuan Zhang and Shikai Guo and Yi Guo and Hui Li and Yu Chai and Rong Chen and Xiaochen Li and He Jiang},
  doi          = {10.1145/3705302},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Context-based transfer learning for structuring fault localization and program repair automation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Killing two birds with one stone: Malicious package detection in NPM and PyPI using a single model of malicious behavior sequence. <em>TOSEM</em>, <em>34</em>(4), 1-28. (<a href='https://doi.org/10.1145/3705304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open source software (OSS) supply chain enlarges the attack surface of a software system, which makes package registries attractive targets for attacks. Recently, multiple package registries have received intensified attacks with malicious packages. Of those package registries, NPM and PyPI are two of the most severe victims. Existing malicious package detectors are developed with features from a list of packages of the same ecosystem and deployed within the same ecosystem exclusively, which is infeasible to utilize the knowledge of a new malicious NPM package detected recently to detect the new malicious package in PyPI. Moreover, existing detectors lack support to model malicious behavior of OSS packages in a sequential way. To address the two limitations, we propose a single detection model using malicious behavior sequence, named Cerebro , to detect malicious packages in NPM and PyPI. We curate a feature set based on a high-level abstraction of malicious behavior to enable multi-lingual knowledge fusing. We organize extracted features into a behavior sequence to model sequential malicious behavior. We fine-tune the pre-trained language model to understand the semantics of malicious behavior. Extensive evaluation has demonstrated the effectiveness of Cerebro over the state-of-the-art as well as the practically acceptable efficiency. Cerebro has detected 683 and 799 new malicious packages in PyPI and NPM, and received 707 thank letters from the official PyPI and NPM teams.},
  archive      = {J_TOSEM},
  author       = {Junan Zhang and Kaifeng Huang and Yiheng Huang and Bihuan Chen and Ruisi Wang and Chong Wang and Xin Peng},
  doi          = {10.1145/3705304},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Killing two birds with one stone: Malicious package detection in NPM and PyPI using a single model of malicious behavior sequence},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROSE: An IDE-based interactive repair framework for debugging. <em>TOSEM</em>, <em>34</em>(4), 1-39. (<a href='https://doi.org/10.1145/3705306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Debugging is costly. Automated program repair (APR) holds the promise of reducing its cost by automatically fixing errors. However, current techniques are not easily applicable in a realistic debugging scenario because they assume a high-quality test suite and frequent program re-execution, have low repair efficiency, and only handle a limited set of errors. To improve the practicality of APR for debugging, we propose ROSE, an interactive repair framework that is able to suggest quick and effective repairs of semantic errors while debugging in an Integrated Development Environment (IDE). ROSE allows an easy integration of existing APR patch generators and can do program repair without assuming the existence of a test suite and without requiring program re-execution. It works in conjunction with an IDE debugger and assumes a debugger stopping point where a problem symptom is observed. ROSE asks the developer to quickly describe the symptom. Then it uses the stopping point, the identified symptom, and the current environment to identify potentially faulty lines, uses a variety of APR techniques to suggest repairs at those lines, and validates those repairs without re-executing the program. Finally, it presents the results so the developer can examine, select, and make the appropriate repair. ROSE uses novel approaches to achieve effective fault localization and patch validation without a test suite or program re-execution. For fault localization, ROSE builds on a fast abstract interpretation-based flow analysis to compute a static backward slice approximating the real dynamic slice while taking into account the symptom and the current execution. For patch validation without re-running the program, ROSE generates simulated traces based on a live-programming system for both the original and repaired executions and compares the traces with respect to the problem symptoms to infer patch correctness. We implemented a prototype of ROSE that works in an Eclipse-based IDE and evaluated its potency and utility with an effectiveness study and a user study. We found that ROSE’s fault localization and validation are highly effective and a ROSE-based tool using existing APR patch generators generated correct repair suggestions for many errors in only seconds. Moreover, the user study demonstrated that ROSE was helpful for debugging and developers liked to use it.},
  archive      = {J_TOSEM},
  author       = {Steven P. Reiss and Xuan Wei and Jiahao Yuan and Qi Xin},
  doi          = {10.1145/3705306},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {ROSE: An IDE-based interactive repair framework for debugging},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpectAcle: Fault localisation of AI-enabled CPS by exploiting sequences of DNN controller inferences. <em>TOSEM</em>, <em>34</em>(4), 1-35. (<a href='https://doi.org/10.1145/3705307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-physical systems (CPSs) are increasingly adopting deep neural networks (DNNs) as controllers, giving birth to AI-enabled CPSs . Despite their advantages, many concerns arise about the safety of DNN controllers. Numerous efforts have been made to detect system executions that violate safety specifications; however, once a violation is detected, to fix the issue, it is necessary to localise the parameters of the DNN controller responsible for the wrong decisions leading to the violation. This is particularly challenging, as it requires to consider a sequence of control decisions, rather than a single one, preceding the violation. To tackle this problem, we propose SpectAcle , that can localise the faulty parameters in DNN controllers. SpectAcle considers the DNN inferences preceding the specification violation and uses forward impact to determine the DNN parameters that are more relevant to the DNN outputs. Then, it identifies which of these parameters are responsible for the specification violation, by adapting classic suspiciousness metrics. Moreover, we propose two versions of SpectAcle , that consider differently the timestamps that precede the specification violation. We experimentally evaluate the effectiveness of SpectAcle on 6,067 faulty benchmarks, spanning over different application domains. The results show that SpectAcle can detect most of the faults.},
  archive      = {J_TOSEM},
  author       = {Deyun Lyu and Zhenya Zhang and Paolo Arcaini and Xiao-Yi Zhang and Fuyuki Ishikawa and Jianjun Zhao},
  doi          = {10.1145/3705307},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {SpectAcle: Fault localisation of AI-enabled CPS by exploiting sequences of DNN controller inferences},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scuzer: A scheduling optimization fuzzer for TVM. <em>TOSEM</em>, <em>34</em>(4), 1-28. (<a href='https://doi.org/10.1145/3705308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of Deep Learning (DL) compiler was proposed to deploy DL models more efficiently on diverse hardware through optimization techniques. As one of the most popular DL compilers, TVM incorporates three levels (high-level, schedule, and low-level) of optimizations, which can inadvertently introduce code logic bugs and build failure bugs. Among these optimizations, scheduling optimization is the core component of DL compilers, which ensures the acceleration of models on all devices. However, the existing works only focus on the testing of high-level and low-level optimizations in TVM, fail to take the most important and challenging intermediate scheduling optimization layer into consideration. To fill the gap, we propose a Scheduling Optimization Oriented Fuzzer ( Scuzer ) for TVM, which is specially designed to effectively detect bugs introduced by the scheduling optimization. In particular, Scuzer first proposes a set of schedule-triggering mutators to actively trigger many scheduling optimizations. Meanwhile, observing that scheduling optimization is closely coupled with program dataflow and operator type, Scuzer additionally proposes a set of structure-enriching mutators to enrich the structure of dataflows and operators. Based on these carefully designed mutators, Scuzer then devises a multi-objective algorithm that can adaptively select different combinations of objectives at each period to guide the selection of seeds and mutators during fuzzing. We conduct extensive experiments comparing with three state-of-the-art fuzzers that can be applied in testing scheduling optimization to evaluate the effectiveness of Scuzer . The experimental results demonstrate that Scuzer outperforms the 2nd-best state-of-the-art fuzzer by 7.4% in edge coverage and achieves 7 \(\times\) improvement in rule-operator coverage. Scuzer has successfully detected 17 previously unknown bugs (9 are inconsistent results and 5 are inconsistent compilations) in TVM, out of which 10 have been confirmed and 5 been fixed.},
  archive      = {J_TOSEM},
  author       = {Xiangxiang Chen and Xingwei Lin and Jingyi Wang and Jun Sun and Jiashui Wang and Wenhai Wang},
  doi          = {10.1145/3705308},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Scuzer: A scheduling optimization fuzzer for TVM},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prioritizing speech test cases. <em>TOSEM</em>, <em>34</em>(4), 1-27. (<a href='https://doi.org/10.1145/3707450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Automated Speech Recognition (ASR) systems gain widespread acceptance, there is a pressing need to rigorously test and enhance their performance. Nonetheless, the process of collecting and executing speech test cases is typically both costly and time-consuming. This presents a compelling case for the strategic prioritization of speech test cases, which consist of a piece of audio and the corresponding reference text . The central question we address is: In what sequence should speech test cases be collected and executed to identify the maximum number of errors at the earliest stage ? In this study, we introduce PRiOritizing sPeecH tEsT ( Prophet ) cases, a tool designed to predict the likelihood that speech test cases will identify errors. Consequently, Prophet can assess and prioritize these test cases without having to run the ASR system, facilitating large-scale analysis. Our evaluation encompasses \(6\) distinct prioritization techniques across \(3\) ASR systems and \(12\) datasets. When constrained by the same test budget, our approach identified \(15.44\%\) more misrecognized words than the leading state-of-the-art method. We select top-ranked speech test cases from the prioritized list to fine-tune ASR systems and analyze how our approach can improve the ASR system performance. Statistical evaluations show that our method delivers a considerably higher performance boost for ASR systems compared to established baseline techniques. Moreover, our correlation analysis confirms that fine-tuning an ASR system with a dataset where the model initially underperforms tends to yield greater performance improvements.},
  archive      = {J_TOSEM},
  author       = {Zhou Yang and Jieke Shi and Muhammad Hilmi Asyrofi and Bowen Xu and Xin Zhou and Donggyun Han and David Lo},
  doi          = {10.1145/3707450},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Prioritizing speech test cases},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vulnerability repair via concolic execution and code mutations. <em>TOSEM</em>, <em>34</em>(4), 1-27. (<a href='https://doi.org/10.1145/3707454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security vulnerabilities detected via techniques like greybox fuzzing are often fixed with a significant time lag. This increases the exposure of the software to vulnerabilities. Automated fixing of vulnerabilities where a tool can generate fix suggestions is thus of value. In this work, we present such a tool, called CrashRepair , to automatically generate fix suggestions using concolic execution, specification inference, and search techniques. Our approach avoids generating fix suggestions merely at the crash location because such fixes often disable the manifestation of the error instead of fixing the error. Instead, based on sanitizer-guided concolic execution, we infer desired constraints at specific program locations and then opportunistically search for code mutations that help respect those constraints. Our technique only requires a single detected vulnerability or exploit as input; it does not require any user-provided properties. Evaluation results on a wide variety of CVEs in the VulnLoc benchmark, show CrashRepair achieves greater efficacy than state-of-the-art vulnerability repair tools like Senx. The repairs suggested come in the form of a ranked set of patches at different locations, and we show that on most occasions, the desired fix is among the top-3 fixes reported by CrashRepair .},
  archive      = {J_TOSEM},
  author       = {Ridwan Shariffdeen and Christopher S. Timperley and Yannic Noller and Claire Le Goues and Abhik Roychoudhury},
  doi          = {10.1145/3707454},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Vulnerability repair via concolic execution and code mutations},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging modular architecture for bug characterization and analysis in automated driving software. <em>TOSEM</em>, <em>34</em>(4), 1-31. (<a href='https://doi.org/10.1145/3707455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of automated driving technology, numerous manufacturers deploy vehicles with auto-driving features. This highlights the importance of ensuring the quality of automated driving software. To achieve this, characterizing bugs in automated driving software is important, as it can facilitate bug detection and bug fixes, thereby ensuring software quality. Automated driving software typically has a modular architecture, where software is divided into multiple modules, each designed for its own functionality for automated driving. This may lead to varying bug characteristics. Additionally, our recent study has shown a correlation between bugs caused by code clones and the functionalities of modules in automated driving software. Hence, we consider the modular structure when analyzing bug characteristics. In this article, we analyze 3,078 bugs from two representative open-source Level-4 automated driving systems, Apollo and Autoware. By analyzing the bug report description, title, and developers’ discussions, we have identified 20 bug symptoms and 17 bug-fixing strategies and analyzed their relationships with the respective modules. Our analysis achieves 12 main findings offering a comprehensive view of bug characteristics in automated driving software. We believe our findings can help developers better understand and manage bugs in automated driving software, thereby improving software quality and reliability.},
  archive      = {J_TOSEM},
  author       = {Yingjie Jiang and Ran Mo and Wenjing Zhan and Dongyu Wang and Zengyang Li and Yutao Ma},
  doi          = {10.1145/3707455},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Leveraging modular architecture for bug characterization and analysis in automated driving software},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A catalog of data smells for coding tasks. <em>TOSEM</em>, <em>34</em>(4), 1-32. (<a href='https://doi.org/10.1145/3707457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are increasingly becoming fundamental in supporting software developers in coding tasks. The massive datasets used for training LLMs are often collected automatically, leading to the introduction of data smells. Previous work addressed this issue by using quality filters to handle some specific smells. Still, the literature lacks a systematic catalog of the data smells for coding tasks currently known. This article presents a Systematic Literature Review (SLR) focused on articles that introduce LLMs for coding tasks. We first extracted the quality filters adopted for training and testing such LLMs, inferred the root problem behind their adoption (data smells for coding tasks), and defined a taxonomy of such smells. Our results highlight discrepancies in the adoption of quality filters between pre-training and fine-tuning stages and across different coding tasks, shedding light on areas for improvement in LLM-based software development support.},
  archive      = {J_TOSEM},
  author       = {Antonio Vitale and Rocco Oliveto and Simone Scalabrino},
  doi          = {10.1145/3707457},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A catalog of data smells for coding tasks},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards effective detection of ponzi schemes on ethereum with contract runtime behavior graph. <em>TOSEM</em>, <em>34</em>(4), 1-32. (<a href='https://doi.org/10.1145/3707458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ponzi schemes, a form of scam, have been discovered in Ethereum smart contracts in recent years, causing massive financial losses. Existing detection methods primarily focus on rule-based approaches and machine learning techniques that utilize static information as features. However, these methods have significant limitations. Rule-based approaches rely on pre-defined rules with limited capabilities and domain knowledge dependency. Using static information like opcodes for machine learning fails to effectively characterize Ponzi contracts, resulting in poor reliability and interpretability. Our research shows no significant difference between Ponzi and non-Ponzi contracts at the opcode level. Moreover, relying on static information like transactions for machine learning requires a certain number of transactions to achieve detection, which limits the scalability of detection and hinders the identification of 0-day Ponzi schemes. In this article, we propose PonziGuard , an efficient Ponzi scheme detection approach based on contract runtime behavior. Inspired by the observation that a contract’s runtime behavior is more effective in disguising Ponzi contracts from the innocent contracts, PonziGuard establishes a comprehensive graph representation called contract runtime behavior graph (CRBG), to accurately depict the behavior of Ponzi contracts. Furthermore, it formulates the detection process as a graph classification task on CRBG, enhancing its overall effectiveness. The experiment results show that PonziGuard surpasses the current state-of-the-art approaches in the ground-truth dataset, achieving a precision of 96.9%, recall of 98.2%, and F1-score of 97.5%. It also exhibits the highest level of interpretability among the current tools. We applied PonziGuard to Ethereum Mainnet and demonstrated its effectiveness in real-world scenarios. Using PonziGuard , we identified 805 Ponzi contracts on Ethereum Mainnet, which have resulted in an estimated economic loss of 281,700 Ether or approximately \($\) 500 million USD. We also found 0-day Ponzi schemes in the recently deployed 10,000 smart contracts.},
  archive      = {J_TOSEM},
  author       = {Ruichao Liang and Jing Chen and Cong Wu and Kun He and Yueming Wu and Weisong Sun and Ruiying Du and Qingchuan Zhao and Yang Liu},
  doi          = {10.1145/3707458},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Towards effective detection of ponzi schemes on ethereum with contract runtime behavior graph},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grammar mutation for testing input parsers. <em>TOSEM</em>, <em>34</em>(4), 1-21. (<a href='https://doi.org/10.1145/3708517'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grammar-based fuzzing is an effective method for testing programs that consume structured inputs, particularly input parsers. However, if the available grammar does not accurately represent the input format, or if the system under test (SUT) does not conform strictly to the grammar, there may be an impedance mismatch between inputs generated via grammars and inputs accepted by the SUT. Even if the SUT has been designed to strictly conform to the grammar, the SUT parser may exhibit vulnerabilities that would only be triggered by slightly invalid inputs. Grammar-based generation, by construction, will not yield such edge case inputs. To overcome these limitations, we present two mutational-based approaches: Gmutator and G+M . Both approaches are built upon Grammarinator , a grammar-based generator. Gmutator applies mutations to the grammar input of Grammarinator , while G+M directly applies byte-level mutations to Grammarinator -generated inputs. To evaluate the effectiveness of these techniques ( Grammarinator , Gmutator , G+M ) in testing programs that parse various input formats, we conducted an experimental evaluation over four different input formats and twelve SUTs (three per input format). Our findings suggest that both Gmutator and G+M excel in generating edge case inputs, facilitating the detection of disparities between input specifications and parser implementations.},
  archive      = {J_TOSEM},
  author       = {Bachir Bendrissou and Cristian Cadar and Alastair F. Donaldson},
  doi          = {10.1145/3708517},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Grammar mutation for testing input parsers},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI for DevSecOps: A landscape and future opportunities. <em>TOSEM</em>, <em>34</em>(4), 1-61. (<a href='https://doi.org/10.1145/3712190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DevOps has emerged as one of the most rapidly evolving software development paradigms. With the growing concerns surrounding security in software systems, the DevSecOps paradigm has gained prominence, urging practitioners to incorporate security practices seamlessly into the DevOps workflow. However, integrating security into the DevOps workflow can impact agility and impede delivery speed. Recently, the advancement of artificial intelligence (AI) has revolutionized automation in various software domains, including software security. AI-driven security approaches, particularly those leveraging machine learning or deep learning, hold promise in automating security workflows. They have the potential to reduce manual efforts and can be incorporated into DevOps practices to support consistent delivery speed while aligning with the principles of the DevSecOps paradigm. This article seeks to contribute to the critical intersection of AI and DevSecOps by presenting a comprehensive landscape of AI-driven security techniques applicable to DevOps and identifying avenues for enhancing security, trust, and efficiency in software development processes. We analyzed 99 research papers spanning from 2017 to 2023. Specifically, we address two key research questions (RQs). In RQ1, we identified 12 security tasks associated with the DevSecOps process and reviewed existing AI-driven security approaches, the problems they addressed, and the 65 benchmarks used to evaluate those approaches. Drawing insights from our findings, in RQ2, we discussed state-of-the-art AI-driven security approaches, highlighted 15 challenges in existing research, and proposed 15 corresponding avenues for future opportunities.},
  archive      = {J_TOSEM},
  author       = {Michael Fu and Jirat Pasuksmit and Chakkrit Tantithamthavorn},
  doi          = {10.1145/3712190},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-61},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {AI for DevSecOps: A landscape and future opportunities},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grammar mutation for testing input Parsers—RCR report. <em>TOSEM</em>, <em>34</em>(4), 1-5. (<a href='https://doi.org/10.1145/3712192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This document presents the artefact that was used to run experiments and produce results reported in the article ‘Grammar Mutation for Testing Input Parsers’. The artefact includes a docker image and a dockerfile. The image can be reconstructed by executing the provided dockerfile. The dockerfile includes all instructions needed to reconstruct the image. Files stored in the image include scripts, systems under test and grammar files. We also list the steps and instructions required to reproduce the results of the experiment.},
  archive      = {J_TOSEM},
  author       = {Bachir Bendrissou and Cristian Cadar and Alastair F. Donaldson},
  doi          = {10.1145/3712192},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-5},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Grammar mutation for testing input Parsers—RCR report},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying affected third-party java libraries from textual descriptions of vulnerabilities and libraries. <em>TOSEM</em>, <em>34</em>(4), 1-27. (<a href='https://doi.org/10.1145/3717060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address security vulnerabilities arising from third-party libraries, security researchers maintain databases monitoring and curating vulnerability reports. Application developers can identify libraries affected by vulnerability reports (in short, affected libraries) by directly querying the databases with their used libraries. However, the querying results of affected libraries are not reliable due to the incompleteness of vulnerability reports. Thus, current approaches model the task of identifying affected libraries as a named-entity-recognition (NER) task or an extreme multi-label learning (XML) task. These approaches suffer from highly inaccurate results in identifying affected libraries with complex and similar names, e.g., Java libraries. To address these limitations, in this article, we propose VulLibMiner, the first to identify affected libraries from textual descriptions of both vulnerabilities and libraries, together with VulLib, a Java vulnerability dataset with their affected libraries. VulLibMiner consists of a TF-IDF matcher to efficiently screen out a small set of candidate libraries and a BERT-FNN model to effectively identify affected libraries from these candidates. We evaluate VulLibMiner using four state-of-the-art/practice approaches of identifying affected libraries on both their dataset named VeraJava and our VulLib dataset. Our evaluation results show that VulLibMiner can effectively identify affected libraries with an average F1 score of 0.669 while the state-of-the-art/practice approaches achieve only 0.547.},
  archive      = {J_TOSEM},
  author       = {Tianyu Chen and Lin Li and Bingjie Shan and Guangtai Liang and Ding Li and Qianxiang Wang and Tao Xie},
  doi          = {10.1145/3717060},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Identifying affected third-party java libraries from textual descriptions of vulnerabilities and libraries},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CodeScore: Evaluating code generation by learning code execution. <em>TOSEM</em>, <em>34</em>(3), 1-22. (<a href='https://doi.org/10.1145/3695991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A proper code evaluation metric (CEM) profoundly impacts the evolution of code generation, which is an important research field in NLP and software engineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU) suffer from two significant drawbacks. 1. They primarily measure the surface differences between codes without considering their functional equivalence. However, functional equivalence is pivotal in evaluating the effectiveness of code generation, as different codes can perform identical operations. 2. They are predominantly designed for the Ref-only input format. However, code evaluation necessitates versatility in input formats. Aside from Ref-only, there are NL-only and Ref and NL formats, which existing match-based CEMs cannot effectively accommodate. In this article, we propose CodeScore, a large language model (LLM)-based CEM, which estimates the functional correctness of generated code on three input types. To acquire CodeScore, we present UniCE, a unified code generation learning framework, for LLMs to learn code execution (i.e., learning PassRatio and Executability of generated code) with unified input. Extensive experimental results on multiple code evaluation datasets demonstrate that CodeScore absolutely improves up to 58.87% correlation with functional correctness compared to other CEMs, achieves state-of-the-art performance, and effectively handles three input formats.},
  archive      = {J_TOSEM},
  author       = {Yihong Dong and Jiazheng Ding and Xue Jiang and Ge Li and Zhuo Li and Zhi Jin},
  doi          = {10.1145/3695991},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {CodeScore: Evaluating code generation by learning code execution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective hard negative mining for contrastive learning-based code search. <em>TOSEM</em>, <em>34</em>(3), 1-35. (<a href='https://doi.org/10.1145/3695994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background . Code search aims to find the most relevant code snippet in a large codebase based on a given natural language query. An accurate code search engine can increase code reuse and improve programming efficiency. The focus of code search is how to represent the semantic similarity of code and query. With the development of code pre-trained models, the pattern of using numeric feature vectors (embeddings) to represent code semantics and using vector distance to represent semantic similarity has replaced traditional string matching methods. The quality of semantic representations is critical to the effectiveness of downstream tasks such as code search. Currently, the state-of-the-art (SOTA) learning method uses the contrastive learning paradigm. The objective of contrastive learning is to maximize the similarity between matching code and query (positive samples) and minimize the similarity between mismatched pairs (negative samples). To increase the reusing of negative samples, prior contrastive learning approaches use a large queue (memory bank) to store embeddings. Problem . However, there is still a lot of room for improvement in using negative examples for code search: ① Due to the random selection of negative samples, semantic representations learned by existing models cannot distinguish similar codes well. ② Since semantic vectors in the memory bank are reused from previous inference results and then directly used for loss function calculation without gradient descent, the model cannot effectively learn the negative sample semantic information. Method . To solve the above problems, we propose a contrastive learning code search model with hard negative mining called CoCoHaNeRe: ❶ To enable the model to distinguish similar codes, we introduce hard negative examples into contrastive training, which are negative examples in the codebase that are most similar to positive examples. As a result, hard negative examples are most likely to make the model make mistakes. ❷ To improve the learning efficiency of negative samples during training, we add all hard negative examples to the model's gradient descent process. Result . To verify the effectiveness of CoCoHaNeRe, we conducted experiments on large code search datasets with six programming languages, as well as similar retrieval tasks code clone detection and code question answering. Experimental results show that our model achieves SOTA performance. In the code search task, the average MRR score of CoCoHaNeRe exceeds CodeBERT, GraphCodeBERT, and UniXcoder by 11.25%, 8.13%, and 7.38%, respectively. It has also made great progress in code clone detection and code question answering. In addition, our method performs well in different programming languages and code pre-training models. Furthermore, qualitative analysis shows that our model effectively distinguishes high-order semantic differences between similar codes.},
  archive      = {J_TOSEM},
  author       = {Ye Fan and Chuanyi Li and Jidong Ge and LiGuo Huang and Bin Luo},
  doi          = {10.1145/3695994},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Effective hard negative mining for contrastive learning-based code search},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PROV-IDEA: Supporting interoperable schema and data provenance within database evolution. <em>TOSEM</em>, <em>34</em>(3), 1-27. (<a href='https://doi.org/10.1145/3697008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Database evolution and data provenance are two closely related research fields. On the one hand, the registry (via provenance) of the schema evolution allows the maintenance of its version record. On the other hand, the origin of the data (i.e., its provenance) will always be affected by modifications (i.e., the evolution) in the schema on which they are based. Despite these interrelationships, there are few works in the literature that have proposed advances in that direction. In particular, to the best of our knowledge, there is no research that has resulted in a general and interoperable solution to the problem of managing database evolution using provenance. In this article, we present PROV-IDEA: a PROV-Interoperable Database Evolution Approach. This is a proposal that allows the simultaneous management of the provenance of schemas (of relational databases) and data, using the PROV standard as a way to guarantee interoperability. Furthermore, it is an adaptable and expandable approach (by using PROV templates), which allows a non-intrusive and seamless integration with existing applications, as well as different aspects of provenance information generation. These properties are demonstrated in the article by presenting a proof of concept built on top of a third-party relational database evolution tool.},
  archive      = {J_TOSEM},
  author       = {Beatriz Pérez and Ángel Luis Rubio Garcia and María A. Zapata},
  doi          = {10.1145/3697008},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {PROV-IDEA: Supporting interoperable schema and data provenance within database evolution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting sentiment analysis for software engineering in the era of large language models. <em>TOSEM</em>, <em>34</em>(3), 1-30. (<a href='https://doi.org/10.1145/3697009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software development involves collaborative interactions where stakeholders express opinions across various platforms. Recognizing the sentiments conveyed in these interactions is crucial for the effective development and ongoing maintenance of software systems. For software products, analyzing the sentiment of user feedback, e.g., reviews, comments, and forum posts can provide valuable insights into user satisfaction and areas for improvement. This can guide the development of future updates and features. However, accurately identifying sentiments in software engineering datasets remains challenging. This study investigates bigger large language models (bLLMs) in addressing the labeled data shortage that hampers fine-tuned smaller large language models (sLLMs) in software engineering tasks. We conduct a comprehensive empirical study using five established datasets to assess three open source bLLMs in zero-shot and few-shot scenarios. Additionally, we compare them with fine-tuned sLLMs, using sLLMs to learn contextual embeddings of text from software platforms. Our experimental findings demonstrate that bLLMs exhibit state-of-the-art performance on datasets marked by limited training data and imbalanced distributions. bLLMs can also achieve excellent performance under a zero-shot setting. However, when ample training data are available or the dataset exhibits a more balanced distribution, fine-tuned sLLMs can still achieve superior results.},
  archive      = {J_TOSEM},
  author       = {Ting Zhang and Ivana Clairine Irsan and Ferdian Thung and David Lo},
  doi          = {10.1145/3697009},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Revisiting sentiment analysis for software engineering in the era of large language models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the effectiveness of large language models in domain-specific code generation. <em>TOSEM</em>, <em>34</em>(3), 1-22. (<a href='https://doi.org/10.1145/3697012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) such as ChatGPT have shown remarkable capabilities in code generation. Despite significant achievements, they rely on enormous training data to acquire a broad spectrum of open-domain knowledge. Besides, their evaluation revolves around open-domain benchmarks like HumanEval, which primarily consist of programming contests. Therefore, it is hard to fully characterize the intricacies and challenges associated with particular domains (e.g., Web, game, and math). In this article, we conduct an in-depth study of the LLMs in domain-specific code generation. Our results demonstrate that LLMs exhibit sub-optimal performance in generating domain-specific code, due to their limited proficiency in utilizing domain-specific libraries. We further observe that incorporating API knowledge as prompts can empower LLMs to generate more professional code. Based on these findings, we further investigate how to effectively incorporate API knowledge into the code generation process. We experiment with three strategies for incorporating domain knowledge, namely, external knowledge inquirer, chain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these strategies as a new code generation approach called DomCoder . Experimental results show that all strategies of DomCoder improve the effectiveness of domain-specific code generation under certain settings.},
  archive      = {J_TOSEM},
  author       = {Xiaodong Gu and Meng Chen and Yalan Lin and Yuhan Hu and Hongyu Zhang and Chengcheng Wan and Zhao Wei and Yong Xu and Juhong Wang},
  doi          = {10.1145/3697012},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On the effectiveness of large language models in domain-specific code generation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Divide-and-conquer: Automating code revisions via localization-and-revision. <em>TOSEM</em>, <em>34</em>(3), 1-26. (<a href='https://doi.org/10.1145/3697013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its effectiveness in ensuring software quality, code review remains a labor-intensive and time-consuming task. In order to alleviate this burden on developers, researchers have proposed the automation of code review activities, particularly focusing on automating code revisions. This automation can benefit both code authors, as they are relieved from the manual task of code revision, and code reviewers, as they are spared from addressing minor code flaws through manual comments. While current code revision approaches have shown promising results, they typically operate within a single phase, in which the code requiring revision is treated as the input of a deep learning model, and the revised code is directly generated through a sequence-to-sequence transformation. Consequently, these approaches tackle both the challenges of localization (i.e., where to revise) and revision (i.e., how to revise) simultaneously. Attempting to handle the entire complex process with a single model goes against the principle of “Divide-and-Conquer,” which encourages breaking down complex problems into smaller sub-problems and addressing them individually. In fact, we have observed that existing code revision approaches often yield inaccurate results in both the localization and revision phases. In this article, we present a two-phase code revision approach that aims to overcome the aforementioned limitations by adhering to the “Divide-and-Conquer” principle. Our approach comprises two key components: a localizer, responsible for identifying the specific parts of the input code that require revisions, and a reviser, tasked with generating the revised code based on the localization result. Extensive experiments conducted on two widely used datasets demonstrate the substantial superiority of our approach over existing code revision approaches. For instance, when revising code based on the code reviewer’s comments, our approach achieves a success rate of over 20% in implementing the ground-truth code revisions. In comparison, the widely used pre-trained model CodeT5 achieves a success rate of less than 16% on the same test set, which contains 16K+ cases.},
  archive      = {J_TOSEM},
  author       = {Shangwen Wang and Bo Lin and Liqian Chen and Xiaoguang Mao},
  doi          = {10.1145/3697013},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Divide-and-conquer: Automating code revisions via localization-and-revision},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instance space analysis of testing of autonomous vehicles in critical scenarios. <em>TOSEM</em>, <em>34</em>(3), 1-36. (<a href='https://doi.org/10.1145/3699596'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Before being deployed on roads, Autonomous Vehicles (AVs) must undergo comprehensive testing. Safety-critical situations, however, are infrequent in usual driving conditions, so simulated scenarios are used to create them. A test scenario comprises static and dynamic features related to the AV and the test environment; the representation of these features is complex and makes testing a heavy process. A test scenario is effective if it identifies incorrect behaviors of the AV. In this article, we present a technique for identifying key features of test scenarios associated with their effectiveness using Instance Space Analysis (ISA). ISA generates a ( \(2D\) ) representation of test scenarios and their features. This visualization helps to identify combinations of features that make a test scenario effective. We present a graphical representation of each feature that helps identify how well each testing technique explores the search space. While identifying key features is a primary goal, this study specifically seeks to determine the critical features that differentiate the performance of algorithms. Finally, we present metrics to assess the robustness of testing algorithms and the scenarios generated. Collecting essential features in combination with their values associated with effectiveness can be used for selection and prioritization of effective test cases.},
  archive      = {J_TOSEM},
  author       = {Victor Crespo-Rodriguez and Neelofar and Aldeida Aleti and Burak Turhan},
  doi          = {10.1145/3699596},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Instance space analysis of testing of autonomous vehicles in critical scenarios},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automating comment generation for smart contract from bytecode. <em>TOSEM</em>, <em>34</em>(3), 1-31. (<a href='https://doi.org/10.1145/3699597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, smart contracts have played a vital role in automatic financial and business transactions. To help end users without programming background to better understand the logic of smart contracts, previous studies have proposed models for automatically translating smart contract source code into their corresponding code summaries. However, in practice, only 13% of smart contracts deployed on the Ethereum blockchain are associated with source code. The practical usage of these existing tools is significantly restricted. Considering that bytecode is always necessary when deploying smart contracts, in this article, we first introduce the task of automatically generating smart contract code summaries from bytecode. We propose a novel approach, named Smart Contract Bytecode Translator ( SmartBT ) for automatically translating smart contract bytecode into fine-grained natural language description directly. Two key challenges are posed for this task: structural code logic hidden in bytecode and the huge semantic gap between bytecode and natural language descriptions. To address the first challenge, we transform bytecode into Control-Flow Graph (CFG) to learn code structural and logic details. Regarding the second challenge, we introduce an information retrieval component to fetch similar comments for filling the semantic gap. Then, the structural input and semantic input are used to build an attentional sequence-to-sequence neural network model. The copy mechanism is employed to copy rare words directly from similar comments, and the coverage mechanism is employed to eliminate repetitive outputs. The automatic evaluation results show that SmartBT outperforms a set of baselines by a large margin, and the human evaluation results show the effectiveness and potential of SmartBT in producing meaningful and accurate comments for smart contract code from bytecode directly.},
  archive      = {J_TOSEM},
  author       = {Jianhang Xiang and Zhipeng Gao and Lingfeng Bao and Xing Hu and Jiayuan Chen and Xin Xia},
  doi          = {10.1145/3699597},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automating comment generation for smart contract from bytecode},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring automated assertion generation via large language models. <em>TOSEM</em>, <em>34</em>(3), 1-25. (<a href='https://doi.org/10.1145/3699598'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unit testing aims to validate the correctness of software system units and has become an essential practice in software development and maintenance. However, it is incredibly time-consuming and labor-intensive for testing experts to write unit test cases manually, including test inputs (i.e., prefixes) and test oracles (i.e., assertions). Very recently, some techniques have been proposed to apply Large Language Models (LLMs) to generate unit assertions and have proven the potential in reducing manual testing efforts. However, there has been no systematic comparison of the effectiveness of these LLMs, and their pros and cons remain unexplored. To bridge this gap, we perform the first extensive study on applying various LLMs to automated assertion generation. The experimental results on two independent datasets show that studied LLMs outperform six state-of-the-art techniques with a prediction accuracy of 51.82%–58.71% and 38.72%–48.19%. The improvements achieve 29.60% and 12.47% on average. Besides, as a representative LLM, CodeT5 consistently outperforms all studied LLMs and all baselines on both datasets, with an average improvement of 13.85% and 26.64%, respectively. We also explore the performance of generated assertions in detecting real-world bugs, and find LLMs are able to detect 32 bugs from Defects4J on average, with an improvement of 52.38% against the most recent approach EditAS . Inspired by the findings, we construct a simplistic retrieval-and-repair-enhanced LLM-based approach by transforming the assertion generation problem into a program repair task for retrieved similar assertions. Surprisingly, such a simplistic approach can further improve the prediction accuracy of LLMs by 9.40% on average, leading to new records on both datasets. Besides, we provide additional discussions from different aspects (e.g., the impact of assertion types and test lengths) to illustrate the capacity and limitations of LLM-based approaches. Finally, we further pinpoint various practical guidelines (e.g., the improvement of multiple candidate assertions) for advanced LLM-based assertion generation in the near future. Overall, our work underscores the promising future of adopting off-the-shelf LLMs to generate accurate and meaningful assertions in real-world test cases and reduce the manual efforts of unit testing experts in practical scenarios.},
  archive      = {J_TOSEM},
  author       = {Quanjun Zhang and Weifeng Sun and Chunrong Fang and Bowen Yu and Hongyan Li and Meng Yan and Jianyi Zhou and Zhenyu Chen},
  doi          = {10.1145/3699598},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Exploring automated assertion generation via large language models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving source code pre-training via type-specific masking. <em>TOSEM</em>, <em>34</em>(3), 1-34. (<a href='https://doi.org/10.1145/3699599'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Masked Language Modeling (MLM) task is widely recognized as one of the most effective pre-training tasks and currently derives many variants in the Software Engineering (SE) field. However, most of these variants mainly focus on code representation without distinguishing between different code token types, while some focus on a specific type, such as code identifiers. Indeed, various code token types exist, and there is no evidence that only identifiers can improve PTMs. Thus, to improve PTMs through different types, we conducted an extensive study to evaluate how different type-specific masking tasks can affect PTMs. First, we extract five code token types, convert them into type-specific masking tasks, and generate their combinations. Second, we pre-train CodeBERT and PLBART using combinations and fine-tuned them on four SE downstream tasks. Experimental results show that type-specific masking tasks can enhance CodeBERT and PLBART on all downstream tasks. Furthermore, we discuss topics related to low-resource datasets, conflicting PTMs that original pre-training tasks conflict with our methods, the cost and performance of our methods, factors that impact the performance of our methods, and applying our methods on state-of-the-art PTMs. These discussions comprehensively analyze the strengths and weaknesses of different type-specific masking tasks.},
  archive      = {J_TOSEM},
  author       = {Wentao Zou and Qi Li and Chuanyi Li and Jidong Ge and Xiang Chen and LiGuo Huang and Bin Luo},
  doi          = {10.1145/3699599},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-34},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Improving source code pre-training via type-specific masking},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on governance in bitcoin’s consensus evolution. <em>TOSEM</em>, <em>34</em>(3), 1-47. (<a href='https://doi.org/10.1145/3699600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus rule changes in public permissionless blockchains are challenging. Changes can be contentious, and getting all participants to agree could be tedious. Notably, Bitcoin has seen centralisation tendencies in mining and development. However, how these tendencies influence governance processes of consensus evolution has received minimal attention. We explore how the evolution of blockchain systems and the governance of consensus intertwine from socio-technical aspects. Our study analyses the governmental structures in blockchain by looking into Bitcoin. We investigate consensus change processes through grounded theory, comprising quantitative and qualitative data from 34 consensus forks in two different blockchains, Bitcoin Core and Bitcoin Cash. We explore how decentralisation and governance unfold in practice. In contrast to existing studies, we revealed that centralisation tendencies among miners and developers have no direct control over consensus rules in a blockchain. Furthermore, centralisation tendencies do not affect decision-making for consensus evolution governance in the same way as they facilitate consensus attacks, such as 51% attacks. We also discovered that consensus governance is constrained by the technicalities of change and deployment techniques. Consequently, even though miners have the authority to make consensus changes and propose new blocks, they are restricted by deployment techniques and dependence on user adoption.},
  archive      = {J_TOSEM},
  author       = {Jakob Svennevik Notland and Mariusz Nowostawski and Jingyue Li},
  doi          = {10.1145/3699600},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-47},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study on governance in bitcoin’s consensus evolution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Less is more: Unlocking semi-supervised deep learning for vulnerability detection. <em>TOSEM</em>, <em>34</em>(3), 1-37. (<a href='https://doi.org/10.1145/3699602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has demonstrated its effectiveness in software vulnerability detection, but acquiring a large number of labeled code snippets for training deep learning models is challenging due to labor-intensive annotation. With limited labeled data, complex deep learning models often suffer from overfitting and poor performance. To address this limitation, semi-supervised deep learning offers a promising approach by annotating unlabeled code snippets with pseudo-labels and utilizing limited labeled data together as training sets to train vulnerability detection models. However, applying semi-supervised deep learning for accurate vulnerability detection comes with several challenges. One challenge lies in how to select correctly pseudo-labeled code snippets as training data, while another involves mitigating the impact of potentially incorrectly pseudo-labeled training code snippets during model training. To address these challenges, we propose the semi-supervised vulnerability detection (SSVD) approach. SSVD leverages the information gain of model parameters as the certainty of the correctness of pseudo-labels and prioritizes high-certainty pseudo-labeled code snippets as training data. Additionally, it incorporates the proposed noise-robust triplet loss to maximize the separation between vulnerable and non-vulnerable code snippets to better propagate labels from labeled code snippets to nearby unlabeled snippets and utilizes the proposed noise-robust cross-entropy loss for gradient clipping to mitigate the error accumulation caused by incorrect pseudo-labels. We evaluate SSVD with nine semi-supervised approaches on four widely-used public vulnerability datasets. The results demonstrate that SSVD outperforms the baselines with an average of 29.82% improvement in terms of F1-score and 56.72% in terms of MCC. In addition, SSVD trained on a certain proportion of labeled data can outperform or closely match the performance of fully supervised LineVul and ReVeal vulnerability detection models trained on 100% labeled data in most scenarios. This indicates that SSVD can effectively learn from limited labeled data to enhance vulnerability detection performance, thereby reducing the effort required for labeling a large number of code snippets.},
  archive      = {J_TOSEM},
  author       = {Xiao Yu and Guancheng Lin and Xing Hu and Jacky Wai Keung and Xin Xia},
  doi          = {10.1145/3699602},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Less is more: Unlocking semi-supervised deep learning for vulnerability detection},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on the suitability of test-based patch acceptance criteria. <em>TOSEM</em>, <em>34</em>(3), 1-20. (<a href='https://doi.org/10.1145/3702971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we empirically study the suitability of tests as acceptance criteria for automated program fixes, by checking patches produced by automated repair tools using a bug-finding tool, as opposed to previous works that used tests or manual inspections. We develop a number of experiments in which faulty programs from IntroClass , a known benchmark for program repair techniques, are fed to the program repair tools GenProg, Angelix, AutoFix, and Nopol, using test suites of varying quality, including those accompanying the benchmark. We then check the produced patches against formal specifications using a bug-finding tool. Our results show that, in the studied scenarios, automated program repair tools are significantly more likely to accept a spurious program fix than producing an actual one. Using bounded-exhaustive suites larger than the originally given ones (with about 100 and 1,000 tests) we verify that overfitting is reduced but (a) few new correct repairs are generated and (b) some tools see their performance reduced by the larger suites and fewer correct repairs are produced. Finally, by comparing with previous work, we show that overfitting is underestimated in semantics-based tools and that patches not discarded using held-out tests may be discarded using a bug-finding tool.},
  archive      = {J_TOSEM},
  author       = {Luciano Zemín and Ariel Godio and César Cornejo and Renzo Degiovanni and Simón Gutiérrez Brida and Germán Regis and Nazareno Aguirre and Marcelo Fabián Frias},
  doi          = {10.1145/3702971},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study on the suitability of test-based patch acceptance criteria},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive modelling languages: Abstract syntax and model migration. <em>TOSEM</em>, <em>34</em>(3), 1-54. (<a href='https://doi.org/10.1145/3702975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling languages are heavily used in many disciplines, including software engineering. However, current languages are rigid , since they do not get adapted to fit the users’ expertise, the modelling task or the usage platform. This may turn some languages unsuitable for a range of users (from unexperienced to experts), goals (from informal discussion to precise specification) and platforms (from desktops to mobile phones). We claim that making languages adaptive to the modelling scenario would alleviate these issues and help simplifying recurring tasks such as language evolution or inter-operability between the languages of a family. In this article, we propose the new notion of adaptive modelling language . This concept combines meta-modelling and product lines to support variants of a given language, and encompasses contextual conditions triggering language reconfigurations, and mechanisms for model migration across the language variants. The article presents a theory and its realisation atop the Eclipse Modelling Framework. Our tool includes an Eclipse workbench to specify adaptive languages and produce Eclipse modelling editors with adaptation support. We report on an evaluation demonstrating the advantages of using our framework to express migrations across the variants of adaptive languages, which moreover have generally fast execution times.},
  archive      = {J_TOSEM},
  author       = {Juan de Lara and Esther Guerra},
  doi          = {10.1145/3702975},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-54},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Adaptive modelling languages: Abstract syntax and model migration},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying performance issues in cloud service systems based on relational-temporal features. <em>TOSEM</em>, <em>34</em>(3), 1-31. (<a href='https://doi.org/10.1145/3702978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud systems, typically comprised of various components (e.g., microservices), are susceptible to performance issues, which may cause service-level agreement violations and financial losses. Identifying performance issues is thus of paramount importance for cloud vendors. In current practice, crucial metrics, i.e., Key Performance Indicators (KPIs), are monitored periodically to provide insight into the operational status of components. Identifying performance issues is often formulated as an anomaly detection problem, which is tackled by analyzing each metric independently. However, this approach overlooks the complex dependencies existing among cloud components. Some graph neural network-based methods take both temporal and relational information into account; however, the correlation violations in the metrics that serve as indicators of underlying performance issues are difficult for them to identify. Furthermore, a large volume of components in a cloud system results in a vast array of noisy metrics. This complexity renders it impractical for engineers to fully comprehend the correlations, making it challenging to identify performance issues accurately. To address these limitations, we propose Identifying Performance Issues based on Relational-Temporal Features (ISOLATE), a learning-based approach that leverages both the relational and temporal features of metrics to identify performance issues. In particular, it adopts a graph neural network with attention to characterizing the relations among metrics and extracts long-term and multi-scale temporal patterns using a GRU and a convolution network, respectively. The learned graph attention weights can be further used to localize the correlation-violated metrics. Moreover, to relieve the impact of noisy data, ISOLATE utilizes a Positive Unlabeled (PU) Learning strategy that tags pseudo-labels based on a small portion of confirmed negative examples. Extensive evaluation on both public and industrial datasets shows that ISOLATE outperforms all baseline models with 0.945 F1 score and 0.920 Hit rate@3. The ablation study also proves the effectiveness of the relational-temporal features and the PU-Learning strategy. Furthermore, we share the success stories of leveraging ISOLATE to identify performance issues in Huawei Cloud, which demonstrates its superiority in practice.},
  archive      = {J_TOSEM},
  author       = {Wenwei Gu and Jinyang Liu and Zhuangbin Chen and Jianping Zhang and Yuxin Su and Jiazhen Gu and Cong Feng and Zengyin Yang and Yongqiang Yang and Michael R. Lyu},
  doi          = {10.1145/3702978},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Identifying performance issues in cloud service systems based on relational-temporal features},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bias behind the wheel: Fairness testing of autonomous driving systems. <em>TOSEM</em>, <em>34</em>(3), 1-24. (<a href='https://doi.org/10.1145/3702989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article conducts fairness testing of automated pedestrian detection, a crucial but under-explored issue in autonomous driving systems. We evaluate eight state-of-the-art deep learning-based pedestrian detectors across demographic groups on large-scale real-world datasets. To enable thorough fairness testing, we provide extensive annotations for the datasets, resulting in 8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone labels. Our findings reveal significant fairness issues, particularly related to age. The proportion of undetected children is 20.14% higher compared to adults. Furthermore, we explore how various driving scenarios affect the fairness of pedestrian detectors. We find that pedestrian detectors demonstrate significant gender biases during night time, potentially exacerbating the prevalent societal issue of female safety concerns during nighttime out. Moreover, we observe that pedestrian detectors can demonstrate both enhanced fairness and superior performance under specific driving conditions, which challenges the fairness-performance tradeoff theory widely acknowledged in the fairness literature. We publicly release the code, data, and results to support future research on fairness in autonomous driving.},
  archive      = {J_TOSEM},
  author       = {Xinyue Li and Zhenpeng Chen and Jie M. Zhang and Federica Sarro and Ying Zhang and Xuanzhe Liu},
  doi          = {10.1145/3702989},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Bias behind the wheel: Fairness testing of autonomous driving systems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Certified cost bounds for abstract programs. <em>TOSEM</em>, <em>34</em>(3), 1-33. (<a href='https://doi.org/10.1145/3705298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A program containing placeholders for unspecified statements or expressions is called an abstract (or schematic) program. Placeholder symbols occur naturally in program transformation rules, as used in refactoring, compilation or optimization. Static cost analysis derives the precise cost—or upper and lower bounds for it—of executing programs, as functions in terms of the program's input data size. We present a generalization of automated cost analysis that can handle abstract programs and, hence, can analyze the impact on the cost effect of program transformations . This kind of relational property requires provably precise cost bounds which are not always produced by cost analysis. Therefore, we certify by deductive verification that the inferred abstract cost bounds are correct and sufficiently precise. It is the first approach solving this problem. Both, abstract cost analysis and certification, are based on quantitative abstract execution (QAE) which in turn is a variation of abstract execution, a recently developed symbolic execution technique for abstract programs. To realize QAE the new concept of a cost invariant is introduced. QAE is implemented and runs fully automatically on a benchmark set consisting of representative optimization rules.},
  archive      = {J_TOSEM},
  author       = {Elvira Albert and Reiner Hähnle and Alicia Merayo and Dominic Steinhöfel},
  doi          = {10.1145/3705298},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Certified cost bounds for abstract programs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding the OSS communities of deep learning frameworks: A comparative case study of PyTorch and TensorFlow. <em>TOSEM</em>, <em>34</em>(3), 1-30. (<a href='https://doi.org/10.1145/3705303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past two decades, deep learning has received tremendous success in developing software systems across various domains. Deep learning frameworks have been proposed to facilitate the development of such software systems, among which, P y T orch and T ensor F low stand out as notable examples. Considerable attention focuses on exploring software engineering practices and addressing diverse technical aspects in developing and deploying deep learning frameworks and software systems. Despite these efforts, little is known about the open source software communities involved in the development of deep learning frameworks. In this article, we perform a comparative investigation into the open source software communities of the two representative deep learning frameworks, P y T orch and T ensor F low . To facilitate the investigation, we compile a dataset of 2,792 and 3,288 code commit authors, along with 9,826 and 19,750 participants engaged in issue events on GitHub , from the two communities, respectively. With the dataset, we first characterize the structures of the two communities by employing four operationalizations to classify contributors into various roles and inspect the contributions made by common contributors across the two communities. We then conduct a longitudinal analysis to characterize the evolution of the two communities across various releases, in terms of the numbers of contributors with various roles and role transitions among contributors. Finally, we explore the causal effects between community characteristics and the popularity of the two frameworks. We find that the T ensor F low community harbors a larger base of contributors, encompassing a higher proportion of core developers and a more extensive cohort of active users compared to the P y T orch community. In terms of the technical background of the developers, 64.4% and 56.1% developers in the P y T orch and T ensor F low communities are employed by the leading companies of the corresponding open source software projects, Meta and Google, respectively; 25.9% and 21.9% core developers in the P y T orch and T ensor F low communities possess Ph.D. degrees, while 77.2% and 77.7% contribute to other machine learning or deep learning open source projects, respectively. Developers contributing to both communities demonstrate spatial and temporal similarities to some extent in their pull requests across the respective projects. The evolution of contributors with various roles exhibits a consistent upward trend over time in the P y T orch community. Conversely, a noticeable turning point in the growth of contributors characterizes the evolution of the T ensor F low community. Both communities show a statistically significant decreasing trend in the inflow rates of core developers. Furthermore, we observe statistically significant causal effects between the expansion of communities and retention of core developers and the popularity of deep learning frameworks. Based on our findings, we discuss implications, provide recommendations for sustaining open source software communities of deep learning frameworks, and outline directions for future research.},
  archive      = {J_TOSEM},
  author       = {Yunqi Chen and Zhiyuan Wan and Yifei Zhuang and Ning Liu and David Lo and Xiaohu Yang},
  doi          = {10.1145/3705303},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Understanding the OSS communities of deep learning frameworks: A comparative case study of PyTorch and TensorFlow},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Validity-preserving delta debugging via generator trace reduction. <em>TOSEM</em>, <em>34</em>(3), 1-33. (<a href='https://doi.org/10.1145/3705305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing test inputs that trigger bugs is crucial for efficient debugging. Delta debugging is the most popular approach for this purpose. When test inputs need to conform to certain specifications, existing delta debugging practice encounters a validity problem: it blindly applies reduction rules, producing a large number of invalid test inputs that do not satisfy the required specifications. This overall diminishing effectiveness and efficiency becomes even more pronounced when the specifications extend beyond syntactical structures. Our key insight is that we should leverage input generators, which are aware of these specifications, to generate valid reduced inputs, rather than straightforwardly performing reduction on test inputs. In this article, we propose a generator-based delta debugging method, namely GReduce, which derives validity-preserving reducers. Specifically, given a generator and its execution, demonstrating how the bug-inducing test input is generated, GReduce searches for other executions on the generator that yield reduced, valid test inputs. The evaluation results on five benchmarks (i.e., graphs, DL models, JavaScript programs, SymPy, and algebraic data types) show that GReduce substantially outperforms state-of-the-art syntax-based reducers including Perses and T-PDD, and also outperforms QuickCheck, SmartCheck, as well as the state-of-the-art choice-sequence-based reducer Hypothesis, demonstrating the effectiveness, efficiency, and versatility of GReduce.},
  archive      = {J_TOSEM},
  author       = {Luyao Ren and Xing Zhang and Ziyue Hua and Yanyan Jiang and Xiao He and Yingfei Xiong and Tao Xie},
  doi          = {10.1145/3705305},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Validity-preserving delta debugging via generator trace reduction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting refactoring commits in machine learning python projects: A machine learning-based approach. <em>TOSEM</em>, <em>34</em>(3), 1-25. (<a href='https://doi.org/10.1145/3705309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Refactoring aims to improve the quality of software without altering its functional behaviors. Understanding developers’ refactoring activities is essential to improve software maintainability. The use of machine learning (ML) libraries and frameworks in software systems has significantly increased in recent years, making the maximization of their maintainability crucial. Due to the data-driven nature of ML libraries and frameworks, they often undergo a different development process compared to traditional projects. As a result, they may experience various types of refactoring, such as those related to the data. The state-of-the-art refactoring detection tools have not been tested in the ML technical domain, and they are not specifically designed to detect ML-specific refactoring types (e.g., data manipulation) in ML projects; therefore, they may not adequately find all potential refactoring operations, specifically the ML-specific refactoring operations. Furthermore, a vast number of ML libraries and frameworks are written in Python, which has limited tooling support for refactoring detection. PyRef, a rule-based and state-of-the-art tool for Python refactoring detection, can identify 11 types of refactoring operations with relatively high precision. In contrast, for other languages such as Java, state-of-the-art tools are capable of detecting a much more comprehensive list of refactorings. For example, Rminer can detect 99 types of refactoring for Java projects. Inspired by previous work that leverages commit messages to detect refactoring, we introduce MLRefScanner, a prototype tool that applies ML techniques to detect refactoring commits in ML Python projects. MLRefScanner detects commits involving both ML-specific refactoring operations and additional refactoring operations beyond the scope of state-of-the-art refactoring detection tools. To demonstrate the effectiveness of our approach, we evaluate MLRefScanner on 199 ML open source libraries and frameworks and compare MLRefScanner against other refactoring detection tools for Python projects. Our findings show that MLRefScanner outperforms existing tools in detecting refactoring-related commits, achieving an overall precision of 94% and recall of 82% for identifying refactoring-related commits. MLRefScanner can identify commits with ML-specific and additional refactoring operations compared to state-of-the-art refactoring detection tools. When combining MLRefScanner with PyRef, we can further increase the precision and recall to 95% and 99%, respectively. MLRefScanner provides a valuable contribution to the Python ML community, as it allows ML developers to detect refactoring-related commits more effectively in their ML Python projects. Our study sheds light on the promising direction of leveraging machine learning techniques to detect refactoring activities for other programming languages or technical domains where the commonly used rule-based refactoring detection approaches are not sufficient.},
  archive      = {J_TOSEM},
  author       = {Shayan Noei and Heng Li and Ying Zou},
  doi          = {10.1145/3705309},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Detecting refactoring commits in machine learning python projects: A machine learning-based approach},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EffFix: Efficient and effective repair of pointer manipulating programs. <em>TOSEM</em>, <em>34</em>(3), 1-27. (<a href='https://doi.org/10.1145/3705310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces EffFix, a tool that applies a novel static analysis-driven automated program repair (APR) technique for fixing memory errors. APR tools typically rely on a given test-suite to guide the repair process. Apart from the need to provide test oracles, this reliance is also one of the main contributors to the over-fitting problem. Static analysis based APR techniques bypass these issues only to introduce new ones, such as soundness, scalability, and generalizability. This work demonstrates how we can overcome these challenges and achieve sound memory bug repair at scale by leveraging static analysis (specifically incorrectness separation logic (ISL)) to guide repair. This is the first repair approach to use ISL. Our key insight is that the abstract domain used by static analysis to detect the bugs also contains key information to derive correct patches. Our proposed approach learns what a desirable patch is by inspecting how close a patch is to fixing the bug based on the feedback from ISL based static analysis (specifically the Pulse analyzer), and turning this information into a distribution of probabilities over context free grammars. This approach to repair is generic in that its learning strategy allows for finding patches without relying on the commonly used patch templates. Furthermore, to achieve efficient program repair, instead of focusing on heuristics for reducing the search space of patches, we make repair scalable by creating classes of equivalent patches according to the effect they have on the symbolic heap. We then conduct candidate patch validation only once per patch equivalence class. This allows EffFix to efficiently discover quality repairs even in the presence of a large pool of patch candidates. Experimental evaluation of fixing real world memory errors in medium to large scale subjects like OpenSSL, Linux Kernel, swoole, shows the efficiency and effectiveness of EffFix— in terms of automatically producing repairs from large search spaces. In particular, EffFix has a fix ratio of 66% for memory leak bugs and 83% for Null Pointer Dereferences for the considered dataset.},
  archive      = {J_TOSEM},
  author       = {Yuntong Zhang and Andreea Costea and Ridwan Shariffdeen and Davin McCall and Abhik Roychoudhury},
  doi          = {10.1145/3705310},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {EffFix: Efficient and effective repair of pointer manipulating programs},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bounded verification of atomicity violations for interrupt-driven programs via lazy sequentialization. <em>TOSEM</em>, <em>34</em>(3), 1-33. (<a href='https://doi.org/10.1145/3705311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting atomicity violations effectively in interrupt-driven programs is difficult due to the asymmetric concurrency interleaving of interrupts. Current approaches face two main challenges: (1) A large number of false positives are generated by efficient static analysis techniques. (2) Loops with large or unknown bounds in these programs limit the scalability of the bounded verification techniques. To address these challenges, we present NIChecker, a new bounded verification tool designed to detect atomicity violations in interrupt-driven programs. The key ideas are: (1) Transforming an interrupt-driven program into a bounded sequential C program through lazy sequentialization technique. This sequential program accurately models interrupt masking and nested interrupt execution. (2) Combining a refined loop abstraction technique with our sequentialization to enhance the efficiency of detecting programs with intractable loops. (3) Integrating slicing and an interleaving path reduction technique known as preemption point reduction in NIChecker to shrink the explored state space. We prove the bounded correctness of our translation and discuss the impact of our optimizations. We evaluate NIChecker on 31 academic benchmark programs and 18 real-world interrupt-driven programs. Our results show that NIChecker achieves better precision, a lower false positive rate, and a significant verification speed-up than related state-of-the-art tools.},
  archive      = {J_TOSEM},
  author       = {Yuan Zhang and Lei Qu and Yifei Wu and Leihuan Wu and Tingting Yu and Rui Chen and Weiqiang Kong},
  doi          = {10.1145/3705311},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Bounded verification of atomicity violations for interrupt-driven programs via lazy sequentialization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What could possibly go wrong: Undesirable patterns in collective development. <em>TOSEM</em>, <em>34</em>(3), 1-50. (<a href='https://doi.org/10.1145/3707451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software development, often perceived as a technical endeavor, is fundamentally a social activity requiring collaboration among team members. Acknowledging this, the software development community has devised strategies to address possible collaboration-related shortcomings. Various studies have attempted to capture the social dynamics within software engineering. These studies developed methods to identify numerous teamwork issues and proposed various approaches to address them. However, there is a need for a comprehensive bottom-up exploration from practitioner’s perceptions to common patterns. This article introduces the concept of undesirable patterns in collective development, referring to potential teamwork problems that may escalate if unaddressed. Through 38 in-depth exploratory interviews, we identify and classify 42 patterns, revealing their origins and consequences. To the best of our knowledge, some patterns, like Teamwork pipeline bottleneck , were never reported before. Subsequent surveys, 436 and 968 participants each, explore the significance and frequency of the undesirable patterns and evaluate potential tools and features to manage these patterns. The study contributes a nuanced understanding of undesirable patterns, evaluating their impact and proposing pragmatic tools and features for industrial application. The findings provide a valuable foundation for further in-depth studies and the development of tools to enhance collaborative software engineering practices.},
  archive      = {J_TOSEM},
  author       = {Mikhail Evtikhiev and Ekaterina Koshchenko and Vladimir Kovalenko},
  doi          = {10.1145/3707451},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-50},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {What could possibly go wrong: Undesirable patterns in collective development},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Refining code-line-level bugginess identification: Getting the best of both worlds by fusing syntactic and semantic features. <em>TOSEM</em>, <em>34</em>(3), 1-43. (<a href='https://doi.org/10.1145/3707456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background : Code-line-level bugginess identification (CLBI) is an important area within software quality assurance, aiming to pinpoint potential buggy source code lines in a given software product. Recently, two concurrent approaches, GLANCE and DeepLineDP, have showcased impressive performance by respectively leveraging syntactic and semantic features compared with the existing state-of-the-art (SOTA) approaches in this field. Problem : Yet, the literature lacks a thorough investigation that fuses these two types of features to enhance CLBI. Such fusion holds the promise of significantly improving the efficacy of identifying defective lines. Objective : We aim to advance CLBI by fusing syntactic and semantic features, thereby harnessing their respective strengths. Method : We propose to build a CLBI approach, booSting DeePLineDP wIth syntaCtic fEatures (SPLICE) , by fusing syntactic features from GLANCE and semantic features from DeepLineDP. SPLICE comprises three variants—SPLICE-S, SPLICE-G, and SPLICE-F—each utilizing a unique line-level sorting approach. We make a comprehensive comparison with the existing SOTA approaches using six performance metrics. Result : Through an analysis of nine open source projects, our experimental results reveal that SPLICE is competitive with current SOTA CLBI approaches. Notably, SPLICE-F demonstrates superiority over all SOTA CLBI approaches, including GLANCE and DeepLineDP, across all six metrics, indicating a substantial improvement. Conclusion : This discovery underscores the critical importance of future CLBI research in fusing syntactic and semantic features to construct more effective bugginess identification approaches. It is worth noting that the analysis was conducted within the context of Java programs, which highlights the potential for exploring similar methods in other programming languages in future research.},
  archive      = {J_TOSEM},
  author       = {Yufei Zhou and Haihua Tang and Longtao Zhu and Hao Ding and Junyan Qian},
  doi          = {10.1145/3707456},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Refining code-line-level bugginess identification: Getting the best of both worlds by fusing syntactic and semantic features},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TG-CUP: A transformer and GNN-based multi-modal comment updating method. <em>TOSEM</em>, <em>34</em>(3), 1-22. (<a href='https://doi.org/10.1145/3708474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comments play a crucial role in code comprehension and maintenance. This is particularly vital when the code is changed, as comments should be promptly updated to maintain consistency between the code and the comments. Existing comment update methods usually treat code as natural language text, ignore the information of code structure, and often fail when code changes are not associated with comment updates (called a non-code-indicative update (NCIU)). Therefore, we propose a Transformer and Graph neural network-based Comment UPdate method (TG-CUP). The model integrates the information of old comment, code edit sequence, and AST-Difference Graph to update the outdated comments. The experimental results show that TG-CUP increased by 5.16% and 2.23% compared with the most advanced methods on Accuracy and Recall@5, and the performance on NCIUs is improved as well.},
  archive      = {J_TOSEM},
  author       = {Yinan Chen and Yuan Huang and Xiangping Chen and Zibin Zheng},
  doi          = {10.1145/3708474},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {TG-CUP: A transformer and GNN-based multi-modal comment updating method},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DistMeasure: A framework for runtime characterization and quality assessment of distributed software via interprocess communications. <em>TOSEM</em>, <em>34</em>(3), 1-53. (<a href='https://doi.org/10.1145/3708476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A defining, unique aspect of distributed systems lies in interprocess communication (IPC) through which distributed components interact and collaborate toward the holistic system behaviors. This highly decoupled construction intuitively contributes to the scalability, performance, and resiliency advantages of distributed software, but also adds largely to their greater complexity, compared to centralized software. Yet despite the importance of IPC in distributed systems, little is known about how to quantify IPC-induced behaviors in these systems through IPC measurement and how such behaviors may be related to the quality of distributed software . To answer these questions, in this article, we present DistMeasure , a framework for measuring distributed software systems via the lens of IPC hence enabling the study of its correlation with distributed system quality. Underlying DistMeasure is a novel set of IPC metrics that focus on gauging the coupling and cohesion of distributed processes. Through these metrics, DistMeasure quantifies relevant runtime characteristics of distributed systems and their quality relevance, covering a range of quality aspects each via respective direct quality metrics. Further, DistMeasure enables predictive assessment of distributed system quality in those aspects via learning-based anomaly detection with respect to the corresponding quality metrics based on their significant correlations with related IPC metrics. Using DistMeasure , we demonstrated the practicality and usefulness of IPC measurement against 11 real-world distributed systems and their diverse execution scenarios. Among other findings, our results revealed that IPC has a strong correlation with distributed system complexity, performance efficiency, and security. Higher IPC coupling between distributed processes tended to be negatively indicative of distributed software quality, while more cohesive processes have positive quality implications. Yet overall IPC-induced behaviors are largely independent of the system scale, and higher (lower) process coupling does not necessarily come with lower (higher) process cohesion. We also show promising merits (with 98% precision/recall/F1) of IPC measurement (e.g., class-level coupling and process-level cohesion) for predictive anomaly assessment of various aspects (e.g., attack surface and performance efficiency) of distributed system quality.},
  archive      = {J_TOSEM},
  author       = {Xiaoqin Fu and Asif Zaman and Haipeng Cai},
  doi          = {10.1145/3708476},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-53},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {DistMeasure: A framework for runtime characterization and quality assessment of distributed software via interprocess communications},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-powered static binary taint analysis. <em>TOSEM</em>, <em>34</em>(3), 1-36. (<a href='https://doi.org/10.1145/3711816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes LATTE , the first static binary taint analysis that is powered by a large language model (LLM). LATTE is superior to the state of the art (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully automated while prior static binary taint analyzers need rely on human expertise to manually customize taint propagation rules and vulnerability inspection rules. Second, LATTE is significantly effective in vulnerability detection, demonstrated by our comprehensive evaluations. For example, LATTE has found 37 new bugs in real-world firmware, which the baselines failed to find. Moreover, 10 of them have been assigned CVE numbers. Lastly, LATTE incurs remarkably low engineering cost, making it a cost-efficient and scalable solution for security researchers and practitioners. We strongly believe that LATTE opens up a new direction to harness the recent advance in LLMs to improve vulnerability analysis for binary programs.},
  archive      = {J_TOSEM},
  author       = {Puzhuo Liu and Chengnian Sun and Yaowen Zheng and Xuan Feng and Chuan Qin and Yuncheng Wang and Zhenyang Xu and Zhi Li and Peng Di and Yu Jiang and Limin Sun},
  doi          = {10.1145/3711816},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {LLM-powered static binary taint analysis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novelty not found: Exploring input shadowing in fuzzing through adaptive fuzzer restarts. <em>TOSEM</em>, <em>34</em>(3), 1-32. (<a href='https://doi.org/10.1145/3712186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Greybox fuzzing enhances software security through unprecedented effectiveness in automated fault detection. Its success lies in the coverage feedback extracted from the system under test, guiding the fuzzer to explore different program parts. The most prominent way to use this feedback is novelty search , where the fuzzer keeps only new inputs exercising a new program edge. However, this approach—by design—ignores input shadowing , in which interesting inputs are discarded if they do not contribute to new coverage. This limits the accepted input space and may overlook bugs that shadowed inputs could trigger with mutations. In this work, we present a comprehensive analysis of input shadowing and demonstrate that multiple fuzzing runs of the same target exhibit a different basic block hit frequency distribution despite overlapping code coverage. We propose fuzzer restarts to effectively redistribute basic block hit frequencies and show that this increases the overall achieved coverage on 15 evaluated targets on average by \(9.5\%\) and up to \(25.0\%\) . Furthermore, restarts help to find more bugs and trigger them more reliably. Overall, our results highlight the importance of considering input shadowing in the fuzzers’ design and the potential benefits of a restart-based strategy to enhance the performance of complex fuzzing methods.},
  archive      = {J_TOSEM},
  author       = {Nico Schiller and Xinyi Xu and Lukas Bernhard and Nils Bars and Moritz Schloegel and Thorsten Holz},
  doi          = {10.1145/3712186},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Novelty not found: Exploring input shadowing in fuzzing through adaptive fuzzer restarts},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Backsolver: Adapting preceding execution paths to solve constraints for concolic execution. <em>TOSEM</em>, <em>34</em>(3), 1-30. (<a href='https://doi.org/10.1145/3712194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concolic execution follows the execution paths of concrete inputs, capable of generating new inputs for unexplored code by solving negated path constraints. However, implicit flows can hinder concolic execution, reducing the code coverage. Implicit flows occur when inputs influence control flow, and the control flow variation affects the values of some variables. During concolic execution, the preceding path selections limit the potential values of these variables. This limitation may result in unsolvable constraints, subsequently restricting the generation of new inputs for unexplored paths. Our insight is that following the same preceding paths is unnecessary, and we can adapt preceding paths to make the latest constraints solvable. We divide states into general states and implicit-flow-solving states (IFSSs). We utilize the general states to perform concolic execution. When solving constraints influenced by implicit flows, we switch to the IFSSs. We use the IFSSs to explore the relevant code region and adapt paths. To mitigate path explosion and construct the relation between inputs and the variables, we merge the IFSSs. State merging does not burden the general states, and we limit the code regions for the IFSSs to minimize the introduced overhead. Finally, we replace the variable symbols in the target constraints with new expressions and attempt to solve the new constraints. We implement our approach in Backsolver and build a test suite to evaluate it. Backsolver successfully identifies all the implicit flows in the test suite and resolves most of them. When evaluated on six real-world binaries, Backsolver resolves the highest number of branches related to implicit flows in total. Besides, Backsolver has the highest code coverage in PlutoSVG and finds a 0-day vulnerability. We reported the vulnerability and obtained a CVE ID.},
  archive      = {J_TOSEM},
  author       = {Yicheng Zeng and Zhanwei Song and Guo Lv and Yu Zhou and Hongsong Zhu and Limin Sun},
  doi          = {10.1145/3712194},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Backsolver: Adapting preceding execution paths to solve constraints for concolic execution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novelty not found: Exploring input shadowing in fuzzing through adaptive fuzzer Restarts—RCR report. <em>TOSEM</em>, <em>34</em>(3), 1-3. (<a href='https://doi.org/10.1145/3712590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is the Replicated Computational Results (RCR) Report for our ACM TOSEM paper, “Novelty Not Found: Exploring Input Shadowing in Fuzzing through Adaptive Fuzzer Restarts”. In this paper, we demonstrate how input shadowing can impact the fuzzing process and propose a mitigation strategy: restarting the fuzzing process when progress stagnates. As part of this RCR, we provide a replication package to facilitate the reproduction of our results.},
  archive      = {J_TOSEM},
  author       = {Nico Schiller and Xinyi Xu and Lukas Bernhard and Nils Bars and Moritz Schloegel and Thorsten Holz},
  doi          = {10.1145/3712590},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {3},
  pages        = {1-3},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Novelty not found: Exploring input shadowing in fuzzing through adaptive fuzzer Restarts—RCR report},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relevant information in TDD experiment reporting. <em>TOSEM</em>, <em>34</em>(2), 1-41. (<a href='https://doi.org/10.1145/3688837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experiments are a commonly used method of research in software engineering (SE). Researchers report their experiments following detailed guidelines. However, researchers do not, in the field of test-driven development (TDD) at least, specify how they operationalized the response variables and, particularly, the measurement process. This article has three aims: (i) identify the response variable operationalization components in TDD experiments that study external quality; (ii) study their influence on the experimental results; (iii) determine if the experiment reports describe the measurement process components that have an impact on the results. We used two-part sequential mixed methods research. The first part of the research adopts a quantitative approach applying a statistical analysis of the impact of the operationalization components on the experimental results. The second part follows with a qualitative approach applying a systematic mapping study (SMS). The test suites, intervention types and measurers have an influence on the measurements and results of the statistical analysis of TDD experiments in SE. The test suites have a major impact on both the measurements and the results of the experiments. The intervention type has less impact on the results than on the measurements. While the measurers have an impact on the measurements, this is not transferred to the experimental results. On the other hand, the results of our SMS confirm that TDD experiments do not usually report either the test suites, the test case generation method, or the details of how external quality was measured. A measurement protocol should be used to ensure that the measurements made by different measurers are similar. It is necessary to report the test cases, the experimental task and the intervention type in order to be able to reproduce the measurements and statistical analyses, as well as to replicate experiments and build dependable families of experiments.},
  archive      = {J_TOSEM},
  author       = {Fernando Uyaguari and Silvia T. Acuña and John W. Castro and Davide Fucci and Oscar Dieste and Sira Vegas},
  doi          = {10.1145/3688837},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-41},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Relevant information in TDD experiment reporting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QuanTest: Entanglement-guided testing of quantum neural network systems. <em>TOSEM</em>, <em>34</em>(2), 1-32. (<a href='https://doi.org/10.1145/3688840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum Neural Network (QNN) combines the deep learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration. Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems. There is an urgent need for ways to test their correctness and security. However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing. These challenges include the inapplicability of traditional quantum software testing methods to QNN systems due to differences in programming paradigms and decision logic representations, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons. In this article, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems. We design a quantum entanglement adequacy criterion to quantify the entanglement acquired by the input quantum states from the QNN system, along with two similarity metrics to measure the proximity of generated quantum adversarial examples to the original inputs. Subsequently, QuanTest formulates the problem of generating test inputs that maximize the quantum entanglement adequacy and capture incorrect behaviors of the QNN system as a joint optimization problem and solves it in a gradient-based manner to generate quantum adversarial examples. Experimental results demonstrate that QuanTest possesses the capability to capture erroneous behaviors in QNN systems (generating 67.48–96.05% more high-quality test samples than the random noise under the same perturbation size constraints). The entanglement-guided approach proves effective in adversarial testing, generating more adversarial examples (maximum increase reached 21.32%).},
  archive      = {J_TOSEM},
  author       = {Jinjing Shi and Zimeng Xiao and Heyuan Shi and Yu Jiang and Xuelong Li},
  doi          = {10.1145/3688840},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {QuanTest: Entanglement-guided testing of quantum neural network systems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). My fuzzers won’t build: An empirical study of fuzzing build failures. <em>TOSEM</em>, <em>34</em>(2), 1-30. (<a href='https://doi.org/10.1145/3688842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzing is an automated software testing technique used to find software vulnerabilities that works by sending large amounts of inputs to a software system to trigger bad behaviors. In recent years, the open source software ecosystem has seen a significant increase in the adoption of fuzzing to avoid spreading vulnerabilities throughout the ecosystem. While fuzzing can uncover vulnerabilities, there is currently a lack of knowledge regarding the challenges of conducting fuzzing activities over time. Specifically, fuzzers are very complex tools to set up and build before they can be used. We set out to empirically find out how challenging is build maintenance in the context of fuzzing. We mine over 1.2 million build logs from Google’s OSS-Fuzz service to investigate fuzzing build failures. We first conduct a quantitative analysis to quantify the prevalence of fuzzing build failures. We then manually investigate 677 failing fuzzing builds logs and establish a taxonomy of 25 root causes of build failures. We finally train a machine learning model to recognize common failure patterns in failing build logs. Our taxonomy can serve as a reference for practitioners conducting fuzzing build maintenance. Our modeling experiment shows the potential of using automation to simplify the process of fuzzing.},
  archive      = {J_TOSEM},
  author       = {Olivier Nourry and Yutaro Kashiwa and Weiyi Shang and Honglin Shu and Yasutaka Kamei},
  doi          = {10.1145/3688842},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {My fuzzers won’t build: An empirical study of fuzzing build failures},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anatomizing deep learning inference in web browsers. <em>TOSEM</em>, <em>34</em>(2), 1-43. (<a href='https://doi.org/10.1145/3688843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web applications have increasingly adopted Deep Learning (DL) through in-browser inference , wherein DL inference performs directly within Web browsers. The actual performance of in-browser inference and its impacts on the Quality of Experience ( QoE ) remain unexplored, and urgently require new QoE measurements beyond traditional ones, e.g., mainly focusing on page load time. To bridge this gap, we make the first comprehensive performance measurement of in-browser inference to date. Our approach proposes new metrics to measure in-browser inference: responsiveness, smoothness, and inference accuracy. Our extensive analysis involves 9 representative DL models across Web browsers of 50 popular PC devices and 20 mobile devices. The results reveal that in-browser inference exhibits a substantial latency gap, averaging 16.9 times slower on CPU and 4.9 times slower on GPU compared to native inference on PC devices. The gap on mobile CPU and mobile GPU is 15.8 times and 7.8 times, respectively. Furthermore, we identify contributing factors to such latency gap, including underutilized hardware instruction sets, inherent overhead in the runtime environment, resource contention within the browser, and inefficiencies in software libraries and GPU abstractions. Additionally, in-browser inference imposes significant memory demands, at times exceeding 334.6 times the size of the DL models themselves, partly attributable to suboptimal memory management. We also observe that in-browser inference leads to a significant 67.2% increase in the time it takes for GUI components to render within Web browsers, significantly affecting the overall user QoE of Web applications reliant on this technology.},
  archive      = {J_TOSEM},
  author       = {Qipeng Wang and Shiqi Jiang and Zhenpeng Chen and Xu Cao and Yuanchun Li and Aoyu Li and Yun Ma and Ting Cao and Xuanzhe Liu},
  doi          = {10.1145/3688843},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Anatomizing deep learning inference in web browsers},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward the fractal dimension of classes. <em>TOSEM</em>, <em>34</em>(2), 1-50. (<a href='https://doi.org/10.1145/3688844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fractal property has been regarded as a fundamental property of complex networks, characterizing the self-similarity of a network. Such a property is usually numerically characterized by the fractal dimension metric, and it not only helps the understanding of the relationship between the structure and function of complex networks but also finds a wide range of applications in complex systems. The existing literature shows that class-level software networks (i.e., class dependency networks) are complex networks with the fractal property. However, the fractal property at the feature (i.e., methods and fields) level has never been investigated, although it is useful for measuring class complexity and predicting bugs in classes. Furthermore, existing studies on the fractal property of software systems were all performed on un-weighted software networks and have not been used in any practical quality assurance tasks such as bug prediction. Generally, considering the weights on edges can give us more accurate representations of the software structure and thus help us obtain more accurate results. The illustration of an approach’s practical use can promote its adoption in practice. In this article, we examine the fractal property of classes by proposing a new metric. Specifically, we build a Feature-Level Software Network (FLSN) for each class to represent the methods/fields and their couplings (including coupling frequencies) within the class and propose a new metric, Fractal Dimension for Classes (FDC) , to numerically describe the fractal property of classes using FLSNs, which captures class complexity. We evaluate FDC theoretically against Weyuker’s nine properties, and the results show that FDC adheres to eight of the nine properties. Empirical experiments performed on a set of 12 large open source Java systems show that (i) for most classes (larger than \(96\%\) ), there exists the fractal property in their FLSNs, (ii) FDC is capable of capturing additional aspects of class complexity that have not been addressed by existing complexity metrics, (iii) FDC significantly correlates with both the existing class-level complexity metrics and the number of bugs in classes, and (iv) FDC , when used together with existing class-level complexity metrics, can significantly improve bug prediction in classes in three scenarios (i.e., bug-count , bug-classification , and effort-aware ) of the cross-project context, but in the within-project context, it cannot.},
  archive      = {J_TOSEM},
  author       = {Weifeng Pan and Wei Wu and Hua Ming and Dae-Kyoo Kim and Zijiang Yang and Yutao Ma},
  doi          = {10.1145/3688844},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-50},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Toward the fractal dimension of classes},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). T-rec: Fine-grained language-agnostic program reduction guided by lexical syntax. <em>TOSEM</em>, <em>34</em>(2), 1-31. (<a href='https://doi.org/10.1145/3690631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Program reduction strives to eliminate bug-irrelevant code elements from a bug-triggering program, so that (1) a smaller and more straightforward bug-triggering program can be obtained, (2) and the difference among duplicates (i.e., different programs that trigger the same bug) can be minimized or even eliminated. With such reduction and canonicalization functionality, program reduction facilitates debugging for software, especially language toolchains, such as compilers, interpreters, and debuggers. While many program reduction techniques have been proposed, most of them (especially the language-agnostic ones) overlooked the potential reduction opportunities hidden within tokens. Therefore, their capabilities in terms of reduction and canonicalization are significantly restricted. To fill this gap, we propose \(\mathsf{T}\) - \(\mathsf{Rec}\) , a fine-grained language-agnostic program reduction technique guided by lexical syntax. Instead of treating tokens as atomic and irreducible components, \(\mathsf{T}\) - \(\mathsf{Rec}\) introduces a fine-grained reduction process that leverages the lexical syntax of programming languages to effectively explore the reduction opportunities in tokens. Through comprehensive evaluations with versatile benchmark suites, we demonstrate that \(\mathsf{T}\) - \(\mathsf{Rec}\) significantly improves the reduction and canonicalization capability of two existing language-agnostic program reducers (i.e., Perses and Vulcan). \(\mathsf{T}\) - \(\mathsf{Rec}\) enables Perses and Vulcan to further eliminate 1,294 and 1,315 duplicates in a benchmark suite that contains 3,796 test cases that trigger 46 unique bugs. Additionally, \(\mathsf{T}\) - \(\mathsf{Rec}\) can also reduce up to 65.52% and 53.73% bytes in the results of Perses and Vulcan on our multi-lingual benchmark suite, respectively.},
  archive      = {J_TOSEM},
  author       = {Zhenyang Xu and Yongqiang Tian and Mengxiao Zhang and Jiarui Zhang and Puzhuo Liu and Yu Jiang and Chengnian Sun},
  doi          = {10.1145/3690631},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {T-rec: Fine-grained language-agnostic program reduction guided by lexical syntax},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic literature review of commercial participation in open source software. <em>TOSEM</em>, <em>34</em>(2), 1-31. (<a href='https://doi.org/10.1145/3690632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open source software (OSS) has been playing a fundamental role in not only information technology but also our social lives. Attracted by various advantages of OSS, increasing commercial companies are participating extensively in open source development, and this has had a broad impact. Enormous research efforts have been devoted to understanding this phenomenon and trying to pursue a win-win result. To characterize the current research achievement and identify challenges, this article provides a comprehensive systematic literature review (SLR) of existing research on company participation in OSS. We collected 105 papers and organized them based on their research topics, which cover three main directions, i.e., participation motivation, contribution model, and impact on OSS development. We found that companies have diverse motivations from economic, technological, and social aspects, and no one study covered all the motivation categories. Existing studies categorize five main companies’ contribution models in OSS projects through their objectives and how they shape OSS communities. Researchers also explored how commercial participation affects OSS development, including companies, developers, and OSS projects. This study contributes to a comprehensive understanding of commercial participation in OSS development. Based on our findings, we present a set of research challenges and promising directions for companies’ better participation in OSS.},
  archive      = {J_TOSEM},
  author       = {Xuetao Li and Yuxia Zhang and Cailean Osborne and Minghui Zhou and Zhi Jin and Hui Liu},
  doi          = {10.1145/3690632},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Systematic literature review of commercial participation in open source software},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness concerns in app reviews: A study on AI-based mobile apps. <em>TOSEM</em>, <em>34</em>(2), 1-30. (<a href='https://doi.org/10.1145/3690633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairness is one of the socio-technical concerns that must be addressed in software systems. Considering the popularity of mobile software applications (apps) among a wide range of individuals worldwide, mobile apps with unfair behaviors and outcomes can affect a significant proportion of the global population, potentially more than any other type of software system. Users express a wide range of socio-technical concerns in mobile app reviews. This research aims to investigate fairness concerns raised in mobile app reviews. Our research focuses on AI-based mobile app reviews as the chance of unfair behaviors and outcomes in AI-based mobile apps may be higher than in non-AI-based apps. To this end, we first manually constructed a ground-truth dataset, including 1,132 fairness and 1,473 non-fairness reviews. Leveraging the ground-truth dataset, we developed and evaluated a set of machine learning and deep learning models that distinguish fairness reviews from non-fairness reviews. Our experiments show that our best-performing model can detect fairness reviews with a precision of 94%. We then applied the best-performing model on approximately 9.5M reviews collected from 108 AI-based apps and identified around 92K fairness reviews. Next, applying the K-means clustering technique to the 92K fairness reviews, followed by manual analysis, led to the identification of six distinct types of fairness concerns (e.g., “receiving different quality of features and services in different platforms and devices” and “lack of transparency and fairness in dealing with user-generated content” ). Finally, the manual analysis of 2,248 app owners’ responses to the fairness reviews identified six root causes (e.g., “copyright issues”) that app owners report to justify fairness concerns.},
  archive      = {J_TOSEM},
  author       = {Ali Rezaei Nasab and Maedeh Dashti and Mojtaba Shahin and Mansooreh Zahedi and Hourieh Khalajzadeh and Chetan Arora and Peng Liang},
  doi          = {10.1145/3690633},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Fairness concerns in app reviews: A study on AI-based mobile apps},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoRIC: Automated neural network repairing based on constrained optimization. <em>TOSEM</em>, <em>34</em>(2), 1-29. (<a href='https://doi.org/10.1145/3690634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are important computational models used in the domains of artificial intelligence and software engineering. Parameters of a neural network are obtained via training it against a specific dataset with a standard process, which guarantees each sample within that set is mapped to the correct class. In general, for a trained neural network, there is no warranty of high-level properties, such as fairness, robustness, and so forth. In this case, one need to tune the parameters in an alternative manner, and it is called repairing. In this paper, we present AutoRIC ( Auto mated R epair w I th C onstraints), an analytical-approach-based white-box repairing framework against general properties that could be quantitatively measured. Our approach is mainly based on constrained optimization, namely, we treat the properties of neural network as the optimized objective described by a quadratic formula about the faulty parameters. To ensure the classification accuracy of the repaired neural network, we impose linear inequality constraints to the inputs that obtain incorrect outputs from the neural network. In general, this may generate a huge amount of constraints, resulting in the prohibitively high cost in the problem solving, or even making the problem unable to be solved by the constraint solver. To circumvent this, we present a selection strategy to diminish the restrictions, i.e., we always select the most ‘strict’ ones into the constraint set each time. Experimental results show that repairing with constraints performs efficiently and effectively. AutoRIC tends to achieve a satisfactory repairing result whereas brings in a negligible accuracy drop. AutoRIC enjoys a notable time advantage and this advantage becomes increasingly evident as the network complexity rises. Moreover, experiment results also demonstrate that repairing based on unconstrained optimizations are not stable, which embodies the necessity of constraints.},
  archive      = {J_TOSEM},
  author       = {Xinyu Sun and Wanwei Liu and Shangwen Wang and Tingyu Chen and Ye Tao and Xiaoguang Mao},
  doi          = {10.1145/3690634},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {AutoRIC: Automated neural network repairing based on constrained optimization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structured chain-of-thought prompting for code generation. <em>TOSEM</em>, <em>34</em>(2), 1-23. (<a href='https://doi.org/10.1145/3690635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have shown impressive abilities in code generation. Chain-of-Thought (CoT) prompting is the state-of-the-art approach to utilizing LLMs. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, the accuracy of CoT prompting still cannot satisfy practical applications. For example, gpt-3.5-turbo with CoT prompting only achieves 53.29% Pass@1 in HumanEval. In this article, we propose Structured CoTs (SCoTs) and present a novel prompting technique for code generation named SCoT prompting. Our motivation is that human developers follow structured programming. Developers use three programming structures (i.e., sequential, branch, and loop) to design and implement structured programs. Thus, we ask LLMs to use three programming structures to generate SCoTs (structured reasoning steps) before outputting the final code. Compared to CoT prompting, SCoT prompting explicitly introduces programming structures and unlocks the structured programming thinking of LLMs. We apply SCoT prompting to two LLMs (i.e., gpt-4-turbo, gpt-3.5-turbo, and DeepSeek Coder-Instruct- \(\{\) 1.3B, 6.7B, 33B \(\}\) ) and evaluate it on three benchmarks (i.e., HumanEval, MBPP, and MBCPP). SCoT prompting outperforms CoT prompting by up to 13.79% in Pass@1. SCoT prompting is robust to examples and achieves substantial improvements. The human evaluation also shows human developers prefer programs from SCoT prompting.},
  archive      = {J_TOSEM},
  author       = {Jia Li and Ge Li and Yongmin Li and Zhi Jin},
  doi          = {10.1145/3690635},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Structured chain-of-thought prompting for code generation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large-scale study of IoT security weaknesses and vulnerabilities in the wild. <em>TOSEM</em>, <em>34</em>(2), 1-40. (<a href='https://doi.org/10.1145/3691628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) is defined as the connection between places and physical objects (i.e., things) over the internet/network via smart computing devices. IoT is a rapidly emerging paradigm that now encompasses almost every aspect of our modern life. As these devices differ from traditional computing, it is important to understand the challenges IoT developers face while implementing proper security measures in their IoT devices. We observed that IoT software developers share solutions to programming questions as code examples on three Stack Exchange Q & A sites: Stack Overflow (SO), Arduino, and Raspberry Pi. Previous research studies found vulnerabilities/weaknesses in C/C++ code examples shared in SO. However, the studies did not investigate C/C++ code examples related to IoT. The studies investigated SO code examples only. In this article, we conduct a large-scale empirical study of all IoT C/C++ code examples shared in the three Stack Exchange sites, i.e., SO, Arduino, and Raspberry Pi. From the 11,329 obtained code snippets from the three sites, we identify 29 distinct Common Weakness Enumeration (CWE) types in 609 snippets. These CWE types can be categorized into eight general weakness categories, and we observe that evaluation, memory, and initialization-related weaknesses are the most common to be introduced by users when posting programming solutions. Furthermore, we find that 39.58% of the vulnerable code snippets contain instances of CWE types that can be mapped to real-world occurrences of those CWE types (i.e., CVE instances). The most number vulnerable IoT code examples was found in Arduino, followed by SO, and Raspberry Pi. Memory type vulnerabilities are on the rise in the sites. For example, from the 3,595 mapped CVE instances, we find that 28.99% result in Denial of Service (DoS) errors, which is particularly harmful for network reliant IoT devices such as smart cars. Our study results can guide various IoT stakeholders to be aware of such vulnerable IoT code examples and to inform IoT researchers during their development of tools that can help prevent developers the sharing of such vulnerable code examples in the sites.},
  archive      = {J_TOSEM},
  author       = {Madhu Selvaraj and Gias Uddin},
  doi          = {10.1145/3691628},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-40},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A large-scale study of IoT security weaknesses and vulnerabilities in the wild},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NLPLego: Assembling test generation for natural language processing applications. <em>TOSEM</em>, <em>34</em>(2), 1-36. (<a href='https://doi.org/10.1145/3691631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Deep Learning, Natural Language Processing (NLP) applications have reached or even exceeded human-level capabilities in certain tasks. Although NLP applications have shown good performance, they can still have bugs like traditional software and even lead to serious consequences. Inspired by Lego blocks and syntax structure analysis, we propose an assembling test generation method for NLP applications or models and implement it in NLPLego . The key idea of NLPLego is to assemble the sentence skeleton and adjuncts in order by simulating the building of Lego blocks to generate multiple grammatically and semantically correct sentences based on one seed sentence. The sentences generated by NLPLego have derivation relations and different degrees of variation. These characteristics make it well-suited for integration with metamorphic testing theory, addressing the challenge of test oracle absence in NLP application testing. To validate NLPLego , we conduct experiments on three commonly used NLP tasks (i.e., machine reading comprehension, sentiment analysis, and semantic similarity measures), focusing on the efficiency of test generation and the quality and effectiveness of generated tests. We select five advanced NLP models and one popular industrial NLP software as the tested subjects. Given seed tests from SQuAD 2.0, SST, and QQP, NLPLego successfully detects 1,732, 3,140, and 261,879 incorrect behaviors with around 93.1% precision in three tasks, respectively. The experiment results show that NLPLego can efficiently generate high-quality tests for multiple NLP tasks to detect erroneous behaviors effectively. In the case study, we analyze the testing results provided by NLPLego to obtain intuitive representations of the different NLP capabilities of the tested subjects. The case study confirms that NLPLego can provide developers with clarity on the direction to improve NLP models or applications, laying the foundation for enhancing performance.},
  archive      = {J_TOSEM},
  author       = {Pin Ji and Yang Feng and Ruohao Zhang and Ruichen Xue and Yichi Zhang and Weitao Huang and Jia Liu and Zhihong Zhao},
  doi          = {10.1145/3691631},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {NLPLego: Assembling test generation for natural language processing applications},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software product line engineering via software transplantation. <em>TOSEM</em>, <em>34</em>(2), 1-27. (<a href='https://doi.org/10.1145/3695987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Product Lines (SPLs) improve time-to-market, enhance software quality, and reduce maintenance costs. Current SPL reengineering practices are largely manual and require domain knowledge. Thus, adopting and, to a lesser extent, maintaining SPLs are expensive tasks, preventing many companies from enjoying their benefits. To address these challenges, we introduce Foundry , an approach utilising software transplantation to reduce the manual effort of SPL adoption and maintenance. Foundry enables integrating features across different codebases, even codebases that are unaware that they are contributing features to a software product line. Each product produced by Foundry is pure code, without variability annotation, unlike feature flags, which eases variability management and reduces code bloat. We realise Foundry in prodScalpel , a tool that transplants multiple organs (i.e., a set of interesting features) from donor systems into an emergent product line for codebases written in C. Given tests and lightweight annotations identifying features and implantation points, prodScalpel automates feature extraction and integration. To evaluate its effectiveness, our evaluation compares feature transplantation using prodScalpel to the current state of practice: on our dataset, prodScalpel ’s use speeds up feature migration by an average of 4.8 times when compared to current practice.},
  archive      = {J_TOSEM},
  author       = {Leandro Oliveria de Souza and Eduardo Santana de Almeida and Paulo Anselmo da Mota Silveira Neto and Earl T. Barr and Justyna Petke},
  doi          = {10.1145/3695987},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Software product line engineering via software transplantation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-flaky and nearly optimal time-based treatment of asynchronous wait web tests. <em>TOSEM</em>, <em>34</em>(2), 1-29. (<a href='https://doi.org/10.1145/3695989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous waits are a common root cause of flaky tests and a major time-influential factor of Web application testing. We build a dataset of 49 reproducible asynchronous wait flaky tests and their fixes from 26 open source projects to study their characteristics in Web testing. Our study reveals that developers adjusted wait time to address asynchronous wait flakiness in about 63% of cases (31 out of 49), even when the underlying causes lie elsewhere. From this, we introduce TRaf , an automated time-based repair for asynchronous wait flakiness in Web applications. TRaf determines appropriate wait times for asynchronous calls in Web applications by analyzing code similarity and past change history. Its key insight is that efficient wait times can be inferred from the current or past codebase since developers tend to repeat similar mistakes. Our analysis shows that TRaf can statically suggest a shorter wait time to alleviate async wait flakiness immediately upon the detection, reducing test execution time by 11.1% compared to the timeout values initially chosen by developers. With optional dynamic tuning, TRaf can reduce the execution time by 16.8% in its initial refinement compared to developer-written patches and by 6.2% compared to the post-refinements of these original patches. Overall, we sent 16 pull requests from our dataset, each fixing one test, to the developers. So far, three have been accepted by the developers.},
  archive      = {J_TOSEM},
  author       = {Yu Pei and Jeongju Sohn and Sarra Habchi and Mike Papadakis},
  doi          = {10.1145/3695989},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Non-flaky and nearly optimal time-based treatment of asynchronous wait web tests},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying the failure-revealing test cases in metamorphic testing: A statistical approach. <em>TOSEM</em>, <em>34</em>(2), 1-26. (<a href='https://doi.org/10.1145/3695990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metamorphic testing, thanks to its high failure-detection effectiveness especially in the absence of test oracle, has been widely applied in both the traditional context of software testing and other relevant fields such as fault localization and program repair. Its core element is a set of metamorphic relations, which are the necessary properties of the target algorithm in the form of the relationships among multiple inputs and corresponding expected outputs. When a relation is violated by the outputs of a group of test cases, namely metamorphic group of test cases, that are constructed based on the relation, a failure is said to be revealed. Traditionally, the primary task of software testing is to reveal failures. Therefore, from the perspective of software testing, it may not need to know which test case(s) in the metamorphic group cause the violation and thus the failure. However, such information is definitely helpful for other software engineering activities, such as software debugging. The current literature of metamorphic testing lacks a systematic mechanism of identifying the actual failure-revealing test cases, which hinders its applicability and effectiveness in other relevant fields. In this article, we propose a new technique for the FAILure-revealing Test case Identification in Metamorphic testing, namely FAILTIM. The approach is based on a novel application of statistical methods. More specifically, we leverage and adapt the basic ideas of spectrum-based techniques, which are originally used in fault localization, and propose the utilization of a set of risk formulas to estimate the suspiciousness of each individual test case in metamorphic groups. Failure-revealing test cases are then suggested according to their suspiciousness. A series of experiments have been conducted to evaluate the effectiveness and efficiency of FAILTIM using 9 subject programs and 30 risk formulas. The experimental results showed that the new approach can achieve a high accuracy in identifying the actual failure-revealing test cases in metamorphic testing. Consequently, our study will help boost the applicability and performance of metamorphic testing beyond testing to other software engineering areas. The present work also unfolds a number of research directions for further advancing the theory of metamorphic testing and more broadly, software testing.},
  archive      = {J_TOSEM},
  author       = {Zheng Zheng and Daixu Ren and Huai Liu and Tsong Yueh Chen and Tiancheng Li},
  doi          = {10.1145/3695990},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Identifying the failure-revealing test cases in metamorphic testing: A statistical approach},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic identification of game stuttering via gameplay videos analysis. <em>TOSEM</em>, <em>34</em>(2), 1-29. (<a href='https://doi.org/10.1145/3695992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern video games are extremely complex software systems and, as such, they might suffer from several types of post-release issues. A particularly insidious issue is constituted by drops in the frame rate (i.e., stuttering events), which might have a negative impact on the user experience. Stuttering events are frequently documented in the million of hours of gameplay videos shared by players on platforms such as Twitch or YouTube. From the developers’ perspective, these videos represent a free source of documented “testing activities.” However, especially for popular games, the quantity and length of these videos make impractical their manual inspection. We introduce HASTE, an approach for the automatic detection of stuttering events in gameplay videos that can be exploited to generate candidate bug reports. HASTE firstly splits a given video into visually coherent slices, with the goal of filtering-out those that not representing actual gameplay (e.g., navigating the game settings). Then, it identifies the subset of pixels in the video frames which actually show the game in action excluding additional elements on screen such as the logo of the YouTube channel, on-screen chats, and so forth. In this way, HASTE can exploit state-of-the-art image similarity metrics to identify candidate stuttering events, namely subsequent frames being almost identical in the pixels depicting the game. We evaluate the different steps behind HASTE on a total of 105 videos showing that it can correctly extract video slices with a 76% precision, and can correctly identify the slices related to gameplay with a recall and precision higher than 77%. Overall, HASTE achieves 71% recall and 89% precision for the identification of stuttering events in gameplay videos.},
  archive      = {J_TOSEM},
  author       = {Emanuela Guglielmi and Gabriele Bavota and Rocco Oliveto and Simone Scalabrino},
  doi          = {10.1145/3695992},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automatic identification of game stuttering via gameplay videos analysis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep API sequence generation via golden solution samples and API seeds. <em>TOSEM</em>, <em>34</em>(2), 1-21. (<a href='https://doi.org/10.1145/3695995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic API recommendation can accelerate developers’ programming and has been studied for years. There are two orthogonal lines of approaches for this task, i.e., information retrieval-based (IR-based) approaches and sequence to sequence (seq2seq) model-based approaches. Although these approaches were reported to have remarkable performance, our observation finds two major drawbacks, i.e., IR-based approaches lack the consideration of relations among the recommended APIs, and seq2seq models do not model the API’s semantic meaning. To alleviate the above two problems, we propose APIGens, which is a retrieval-enhanced large language model (LLM)-based API recommendation approach to recommend an API sequence for a natural language query. The approach first retrieves similar programming questions in history based on the input natural language query, and then scores the results based on API documents via a scorer model. Finally, these results are used as samples for few-shot learning of LLM. To reduce the risk of encountering local optima, we also extract API seeds from the retrieved results to increase the search scope during the LLM generation process. The results show that our approach can achieve 48.41% ROUGE@10 on API sequence recommendation and the 82.61% MAP on API set recommendation, largely outperforming the state-of-the-art baselines.},
  archive      = {J_TOSEM},
  author       = {Yuekai Huang and Junjie Wang and Song Wang and Moshi Wei and Lin Shi and Zhe Liu and Qing Wang},
  doi          = {10.1145/3695995},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-21},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Deep API sequence generation via golden solution samples and API seeds},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is it hard to generate holistic commit message?. <em>TOSEM</em>, <em>34</em>(2), 1-28. (<a href='https://doi.org/10.1145/3695996'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commit messages are important for developers to understand the content and the reason for code changes. However, poor and even empty commit messages widely exist. To improve the quality of commit messages and development efficiency, many commit message generation methods have been proposed. Nevertheless, previous methods mainly focus on a brief generation problem, where both the input code change and the output commit messages are restricted to short. This may initiate a debate on the performance of these methods in practice. In this article, we attempt to remove the restrictions and move the needle forward to a holistic commit message generation problem. In particular, we conduct experiments to evaluate the performance of existing commit message generation methods in holistic commit message generation. In the experiments, we choose seven state-of-the-art commit generation methods and focus on two important scenarios in commit message generation (i.e., the within-project scenario and the cross-project scenario). To conduct our experiments, we publish a holistic commit message dataset HORDA with test data manually labeled. In our evaluations, we find that in generating holistic commit messages, the IR-based method has a better performance than non-pre-trained generation-based methods in the within-project scenario, contradicting previous research findings. Further, while the pre-trained generation-based methods are better than non-pre-trained generation-based methods, they are still constrained by the limitations of generation models.},
  archive      = {J_TOSEM},
  author       = {Guoqing Wang and Zeyu Sun and Jinhao Dong and Yuxia Zhang and Mingxuan Zhu and Qingyuan Liang and Dan Hao},
  doi          = {10.1145/3695996},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Is it hard to generate holistic commit message?},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving fault localization with external oracle by using counterfactual execution. <em>TOSEM</em>, <em>34</em>(2), 1-22. (<a href='https://doi.org/10.1145/3695997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Flex , a new approach to improve fault localization with external oracles. Spectrum-based fault localization techniques estimate suspicious statements based on the execution trace of the test suite. State-of-the-art techniques rely on test oracles that internally exist in the program. However, programs often have external oracles that observe their behavior from outside. This in turn hinders fine-grained and accurate estimation of suspicious statements in practice because the correctness of each execution can only be observed at termination. In this article, we aim to address this problem by observing counterfactual execution traces, which enable fine-grained estimation even without precise internal oracles. We observe two types of counterfactual scenarios related to different types of test cases: When the branch condition is set to a Boolean constant, (1) if most of the passing test cases still pass, we consider the newly executed statements in the branch statement as unrelated to the failure; (2) if failing test case still fails, we also consider the originally executed statements as unrelated to the failure. We evaluated the performance on widely used C and Java programs. Flex improves the accuracy of state-of-the-art SBFL techniques on C and Java programs by 24% and 22% on average, respectively.},
  archive      = {J_TOSEM},
  author       = {Jongchan Park and Tae Eun Kim and Dongsun Kim and Kihong Heo},
  doi          = {10.1145/3695997},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Improving fault localization with external oracle by using counterfactual execution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward enhancing privacy preservation of a federated learning CNN intrusion detection system in IoT: Method and empirical study. <em>TOSEM</em>, <em>34</em>(2), 1-48. (<a href='https://doi.org/10.1145/3695998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enormous risks and hidden dangers of information security exist in the applications of Internet of Things (IoT) technologies. To secure IoT software systems, software engineers have to deploy advanced security software such as Intrusion Detection Systems (IDS) that are able to keep track of how the IoT devices behave within the network and detect any malicious activity that may be occurring. Considering that IoT devices generate large amounts of data, Artificial Intelligence (AI) is often regarded as the best method for implementing IDS, thanks to AI’s high capability in processing large amounts of IoT data. To tackle these security concerns, specifically the ones tied to the privacy of data used in IoT systems, the software implementation of a Federated Learning (FL) method is often used to improve both privacy preservation (PP) and scalability in IoT networks. In this article, we present an FL IDS that leverages a 1-Dimensional Convolutional Neural Network (CNN) for efficient and accurate intrusion detection in IoT networks. To address the critical issue of PP in FL, we incorporate three techniques: Differential Privacy, Diffie–Hellman Key Exchange, and Homomorphic Encryption. To evaluate the effectiveness of our solution, we conduct experiments on seven publicly available IoT datasets: TON-IoT, IoT-23, BoT-IoT, CIC IoT 2023, CIC IoMT 2024, RT-IoT 2022, and EdgeIIoT. Our CNN-based approach achieves outstanding performance with an average accuracy, precision, recall, and F1-score of 97.31%, 95.59%, 92.43%, and 92.69%, respectively, across these datasets. These results demonstrate the effectiveness of our approach in accurately identifying and detecting intrusions in IoT networks. Furthermore, our experiments reveal that implementing all three PP techniques only incurs a minimal increase in computation time, with a 10% overhead compared to our solution without any PP mechanisms. This finding highlights the feasibility and efficiency of our solution in maintaining privacy while achieving high performance. Finally, we show the effectiveness of our solution through a comparison study with other recent IDS trained and tested on the same datasets we use.},
  archive      = {J_TOSEM},
  author       = {Damiano Torre and Anitha Chennamaneni and JaeYun Jo and Gitika Vyas and Brandon Sabrsula},
  doi          = {10.1145/3695998},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-48},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Toward enhancing privacy preservation of a federated learning CNN intrusion detection system in IoT: Method and empirical study},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable failure localization for microservice systems based on graph autoencoder. <em>TOSEM</em>, <em>34</em>(2), 1-28. (<a href='https://doi.org/10.1145/3695999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient localization of root cause instances in large-scale microservice systems is of paramount importance. Unfortunately, prevailing methods face several limitations. Notably, some recent methods rely on supervised learning which necessitates a substantial amount of labeled data. However, labeling root cause instances is time-consuming and laborious, especially with multiple modalities of data including logs, traces, metrics, and so on. Moreover, some approaches favor deep learning for localization but lack interpretability and continuous improvement mechanisms. To address the above challenges, we propose DeepHunt , a novel root cause localization method based on multimodal data analysis. Firstly, DeepHunt introduces root cause score (RCS) by integrating reconstruction errors and failure propagation patterns (upstream–downstream relationships), imparting interpretability to the localization of root causes. Then, it embraces graph autoencoder (GAE) to address the limitation imposed by scarce labeled data. It employs data augmentation to mitigate the adverse effects of insufficient historical training samples. We evaluate DeepHunt on two open source datasets, and it outperforms existing methods when facing a zero-label cold start. DeepHunt can be further improved by continuously fine-tuning through a feedback mechanism.},
  archive      = {J_TOSEM},
  author       = {Yongqian Sun and Zihan Lin and Binpeng Shi and Shenglin Zhang and Shiyu Ma and Pengxiang Jin and Zhenyu Zhong and Lemeng Pan and Yicheng Guo and Dan Pei},
  doi          = {10.1145/3695999},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Interpretable failure localization for microservice systems based on graph autoencoder},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KBX: Verified model synchronization via formal bidirectional transformation. <em>TOSEM</em>, <em>34</em>(2), 1-40. (<a href='https://doi.org/10.1145/3696000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex safety-critical systems require multiple models for a comprehensive description, resulting in error-prone development and laborious verification. Bidirectional transformation (BX) is an approach to automatically synchronizing these models. However, existing BX frameworks lack formal verification to enforce these models’ consistency rigorously. This paper introduces KBX, a formal bidirectional transformation framework for verified model synchronization. First, we present a matching logic-based BX model, providing a logical foundation for constructing BX definitions within the \(\mathbb{K}\) framework. Second, we propose algorithms to synthesize formal BX definitions from unidirectional ones, which allows developers to focus on crafting the unidirectional definitions while disregarding the reverse direction and missing information recovery for synchronization. Afterward, we harness \(\mathbb{K}\) to generate a formal synchronizer from the synthesized definitions for consistency maintenance and verification. To evaluate the effectiveness of KBX, we conduct a comparative analysis against existing BX frameworks. Furthermore, we demonstrate the application of KBX in constructing a BX between UML and HCSP for real-world scenarios, showcasing an 72% reduction in BX development effort compared to manual specification writing in \(\mathbb{K}\) .},
  archive      = {J_TOSEM},
  author       = {Jianhong Zhao and Yongwang Zhao and Peisen Yao and Fanlang Zeng and Bohua Zhan and Kui Ren},
  doi          = {10.1145/3696000},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-40},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {KBX: Verified model synchronization via formal bidirectional transformation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demo2Test: Transfer testing of agent in competitive environment with failure demonstrations. <em>TOSEM</em>, <em>34</em>(2), 1-28. (<a href='https://doi.org/10.1145/3696001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The competitive game between agents exists in many critical applications, such as military unmanned aerial vehicles. It is urgent to test these agents to reduce the significant losses caused by their failures. Existing studies mainly are to construct a testing agent that competes with the target agent to induce its failures. These approaches usually focus on a single task, requiring much more time for multi-task testing. However, if the previously tested tasks (source tasks) and the task to be tested (target task) share similar agents or task objectives, the transferable knowledge in source tasks can potentially increase the effectiveness of testing in the target task. We propose Demo2Test for conducting transfer testing of agents in the competitive environment, i.e., leveraging the demonstrations of failure scenarios from the source task to boost the testing effectiveness in the target task. It trains a testing agent with demonstrations and incorporates the action perturbation at key states to balance the number of revealed failures and their diversity. We conduct experiments in the simulated robotics competitive environments of MuJoCo. The results indicate that Demo2Test outperforms the best-performing baseline with improvements ranging from \(22.38\%\) to \(87.98\%\) , and \(12.69\%\) to \(60.98\%\) , in terms of the number and diversity of discovered failure scenarios, respectively.},
  archive      = {J_TOSEM},
  author       = {Jianming Chen and Yawen Wang and Junjie Wang and Xiaofei Xie and Dandan Wang and Qing Wang and Fanjiang Xu},
  doi          = {10.1145/3696001},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Demo2Test: Transfer testing of agent in competitive environment with failure demonstrations},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel refactoring and semantic aware abstract syntax tree differencing tool and a benchmark for evaluating the accuracy of diff tools. <em>TOSEM</em>, <em>34</em>(2), 1-63. (<a href='https://doi.org/10.1145/3696002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software undergoes constant changes to support new requirements, address bugs, enhance performance, and ensure maintainability. Thus, developers spend a great portion of their workday trying to understand and review the code changes of their teammates. Abstract Syntax Tree (AST) diff tools were developed to overcome the limitations of line-based diff tools, which are used by the majority of developers. Despite the notable improvements brought by AST diff tools in understanding complex changes, they still suffer from serious limitations, such as (1) lacking multi-mapping support, (2) matching semantically incompatible AST nodes, (3) ignoring language clues to guide the matching process, (4) lacking refactoring awareness, and (5) lacking commit-level diff support. We propose a novel AST diff tool based on RefactoringMiner that resolves all aforementioned limitations. First, we improved RefactoringMiner to increase its statement mapping accuracy, and then we developed an algorithm that generates AST diff for a given commit or pull request based on the refactoring instances and pairs of matched program element declarations provided by RefactoringMiner. To evaluate the accuracy of our tool and compare it with the state-of-the-art tools, we created the first benchmark of AST node mappings, including 800 bug-fixing commits and 188 refactoring commits. Our evaluation showed that our tool achieved a considerably higher precision and recall, especially for refactoring commits, with an execution time that is comparable with that of the faster tools.},
  archive      = {J_TOSEM},
  author       = {Pouria Alikhanifard and Nikolaos Tsantalis},
  doi          = {10.1145/3696002},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-63},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A novel refactoring and semantic aware abstract syntax tree differencing tool and a benchmark for evaluating the accuracy of diff tools},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study of the non-determinism of ChatGPT in code generation. <em>TOSEM</em>, <em>34</em>(2), 1-28. (<a href='https://doi.org/10.1145/3697010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; non-deterministically returning very different code for the same prompt. Such non-determinism affects the correctness and consistency of the generated code, undermines developers’ trust in LLMs, and yields low reproducibility in LLM-based papers. Nevertheless, there is no work investigating how serious this non-determinism threat is. To fill this gap, this article conducts an empirical study on the non-determinism of ChatGPT in code generation. We chose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems across three code generation benchmarks (i.e., CodeContests, APPS and HumanEval) with three aspects of code similarities: semantic similarity, syntactic similarity, and structural similarity. Our results reveal that ChatGPT exhibits a high degree of non-determinism under the default setting: the ratio of coding tasks with zero equal test output across different requests is 75.76%, 51.00% and 47.56% for three different code generation datasets (i.e., CodeContests, APPS and HumanEval), respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration ( temperature \(=\) 1). In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.},
  archive      = {J_TOSEM},
  author       = {Shuyin Ouyang and Jie M. Zhang and Mark Harman and Meng Wang},
  doi          = {10.1145/3697010},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study of the non-determinism of ChatGPT in code generation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ZigZagFuzz: Interleaved fuzzing of program options and files. <em>TOSEM</em>, <em>34</em>(2), 1-31. (<a href='https://doi.org/10.1145/3697014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Command-line options (e.g., -l , -F , -R for ls ) given to a command-line program can significantly alternate the behaviors of the program. Thus, fuzzing not only file input but also program options can improve test coverage and bug detection. In this article, we propose ZigZagFuzz which achieves higher test coverage and detects more bugs than the state-of-the-art fuzzers by separately mutating program options and file inputs in an iterative/interleaving manner. ZigZagFuzz applies the following three core ideas. First, to utilize different characteristics of the program option domain and the file input domain, ZigZagFuzz separates phases of mutating program options from ones of mutating file inputs and performs two distinct mutation strategies on the two different domains. Second, to reach deep segments of a target program that are accessed through an interleaving sequence of program option checks and file inputs checks, ZigZagFuzz continuously interleaves phases of mutating program options with phases of mutating file inputs. Finally, to improve fuzzing performance further, ZigZagFuzz periodically shrinks input corpus by removing similar test inputs based on their function coverage. The experiment results on the 20 real-world programs show that ZigZagFuzz improves test coverage and detects 1.9 to 10.6 times more bugs than the state-of-the-art fuzzers that mutate program options such as AFL++-argv, AFL++-all, Eclipser, CarpetFuzz, ConfigFuzz, and POWER. We have reported the new bugs detected by ZigZagFuzz, and the original developers confirmed our bug reports.},
  archive      = {J_TOSEM},
  author       = {Ahcheong Lee and Youngseok Choi and Shin Hong and Yunho Kim and Kyutae Cho and Moonzoo Kim},
  doi          = {10.1145/3697014},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {ZigZagFuzz: Interleaved fuzzing of program options and files},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Patch correctness assessment: A survey. <em>TOSEM</em>, <em>34</em>(2), 1-50. (<a href='https://doi.org/10.1145/3702972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most automated program repair methods rely on test cases to determine the correctness of the generated patches. However, due to the incompleteness of available test suites, some patches that pass all the test cases may still be incorrect. This issue is known as the patch overfitting problem. Overfitting problem is a longstanding problem in automated program repair. Due to overfitting patches, the patches obtained by automated program repair tools require further validation to determine their correctness. Researchers have proposed many methods to automatically assess the correctness of patches, but no systematic review provides a detailed introduction to this problem, the existing solutions, and the challenges. To address this deficiency, we systematically review the existing approaches to patch correctness assessment. We first offer a few examples of overfitting patches to acquire a more detailed understanding of this problem. We then propose a comprehensive categorization of publicly available techniques and datasets, examine the commonly used evaluation metrics, and perform an in-depth analysis of the effectiveness of the existing models in addressing the challenge of overfitting. Based on our analysis, we provided the difficulties encountered by current methodologies, alongside the possible avenues for future research exploration.},
  archive      = {J_TOSEM},
  author       = {Zhiwei Fei and Jidong Ge and Chuanyi Li and Tianqi Wang and Yuning Li and Haodong Zhang and LiGuo Huang and Bin Luo},
  doi          = {10.1145/3702972},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-50},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Patch correctness assessment: A survey},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-linear software documentation with interactive code examples. <em>TOSEM</em>, <em>34</em>(2), 1-32. (<a href='https://doi.org/10.1145/3702976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Documentation enables sharing knowledge between the developers of a technology and its users. Creating quality documents, however, is challenging: Documents must satisfy the needs of a large audience without being overwhelming for individuals. We address this challenge with a new document format named Casdoc. Casdoc documents are interactive resources centered around code examples for programmers. Explanations of the code elements are presented as annotations that the readers reveal based on their needs. We evaluated Casdoc in a field study with over 300 participants who used 126 documents as part of a software design course. During the study, the majority of participants adopted Casdoc instead of a baseline format and used interactive annotations to reveal additional information about the code example. Although participants collectively viewed the majority of the documents’ content, they individually revealed a minority of the annotations they saw. We gathered insights into five aspects of Casdoc that can be applied to other formats and highlighted five lessons learned to improve navigability in online documents.},
  archive      = {J_TOSEM},
  author       = {Mathieu Nassif and Martin P. Robillard},
  doi          = {10.1145/3702976},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Non-linear software documentation with interactive code examples},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning approach for automated filling of categorical fields in data entry Forms—RCR report. <em>TOSEM</em>, <em>34</em>(2), 1-7. (<a href='https://doi.org/10.1145/3702985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article represents the Replicated Computational Results (RCR) related to our TOSEM paper “A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms,” where we proposed LAFF, an approach to automatically suggest possible values of categorical fields in data entry forms, which is a common user interface feature in many software systems. In this RCR report, we provide details about our replication package. We make available the different scripts needed to fully replicate the results obtained in our paper.},
  archive      = {J_TOSEM},
  author       = {Hichem Belgacem and Xiaochen Li and Domenico Bianculli and Lionel Briand},
  doi          = {10.1145/3702985},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {2},
  pages        = {1-7},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A machine learning approach for automated filling of categorical fields in data entry Forms—RCR report},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision support model for selecting the optimal blockchain oracle platform: An evaluation of key factors. <em>TOSEM</em>, <em>34</em>(1), 1-35. (<a href='https://doi.org/10.1145/3697011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contract-based applications are executed in a blockchain environment, and they cannot directly access data from external systems, which is required for the service provision of these applications. Instead, smart contracts use agents known as blockchain oracles to collect and provide data feeds to the contracts. The functionality and compatibility with smart contract applications need to be considered when selecting the best-fit oracle platform. As the number of oracle alternatives and their features increases, the decision-making process becomes increasingly complex. Selecting the wrong or sub-optimal oracle is costly and may lead to severe security risks. This article provides a decision support model for the oracle selection problem. The model supports smart contract decision-makers in selecting a secure, cost-effective, and feasible oracle platform for their applications. We interviewed oracle co-founders and smart contracts experts to refine and validate the decision model. Two real-world smart contract application case studies were used to evaluate the model. Our model prioritises and suggests more than one possible oracle platform based on the developer’s required criteria, security assessment and cost analysis. Moreover, this guided decision model serves to reveal issues that may go unnoticed if done haphazardly, reduce decision-making efforts and provide a cost-effective solution.},
  archive      = {J_TOSEM},
  author       = {Sabreen Ahmadjee and Carlos Mera-Gómez and Siamak Farshidi and Rami Bahsoon and Rick Kazman},
  doi          = {10.1145/3697011},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {1},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Decision support model for selecting the optimal blockchain oracle platform: An evaluation of key factors},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). : Distance-based seed prioritization for greybox Fuzzing—RCR report. <em>TOSEM</em>, <em>34</em>(1), 1-13. (<a href='https://doi.org/10.1145/3701298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This replicated computational results (RCR) report describes how to (1) set up DiPri and (2) replicate the experimental results. The primary artifact is the C/C++ prototype of DiPri , which is essentially an extension of the state-of-the-art greybox fuzzer AFL++ (version 4.06). Other artifacts include the Java implementation of DiPri on Zest, the materials for integrating DiPri into FuzzBench and Magma, and the scripts for running docker and processing data. All artifacts can be found at our GitHub repository 1 and Zenodo archive. 2},
  archive      = {J_TOSEM},
  author       = {Ruixiang Qian and Quanjun Zhang and Chunrong Fang and Ding Yang and Shun Li and Binyu Li and Zhenyu Chen},
  doi          = {10.1145/3701298},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {: Distance-based seed prioritization for greybox Fuzzing—RCR report},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
