<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TELO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="telo">TELO - 22</h2>
<ul>
<li><details>
<summary>
(2025). Hardening active directory graphs via evolutionary diversity optimization-based policies. <em>TELO</em>, <em>5</em>(3), 1-36. (<a href='https://doi.org/10.1145/3688401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Active Directory (AD) is the default security management system for Windows domain networks. An AD environment can be described as a cyber-attack graph, with nodes representing computers, accounts, and so forth, and edges indicating existing accesses or known exploits that enable attackers to move from one node to another. This article explores a Stackelberg game model between one attacker and one defender on an AD attack graph. The attacker’s goal is to maximize their chances of successfully reaching the destination before getting detected. The defender’s aim is to block a constant number of edges to minimize the attacker’s chance of success. The article shows that the problem is #P-hard and, therefore, intractable to solve exactly. To defend the AD graph from cyberattackers, this article proposes two defensive approaches. In the first approach, we convert the attacker’s problem to an exponential-sized Dynamic Program that is approximated by a neural network (NN). Once trained, the NN serves as an efficient fitness function for defender’s Evolutionary Diversity Optimization-based defensive policy. The diversity emphasis on the defender’s solution provides a diverse set of training samples, improving the training accuracy of our NN for modeling the attacker. In the second approach, we propose a RL-based policy to solve the attacker’s problem and Critic network-assisted Evolutionary Diversity Optimization-based defensive policy to solve defender’s problem. Experimental results on synthetic AD graphs show that the proposed defensive policies are scalable, highly effective, approximate attacker’s problem accurately and generate good defensive plans.},
  archive  = {J},
  author   = {Diksha Goel and Max Ward and Aneta Neumann and Frank Neumann and Hung Nguyen and Mingyu Guo},
  doi      = {10.1145/3688401},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {8},
  number   = {3},
  pages    = {1-36},
  title    = {Hardening active directory graphs via evolutionary diversity optimization-based policies},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrogate modeling to address the absence of protected membership attributes in fairness evaluation. <em>TELO</em>, <em>5</em>(3), 1-25. (<a href='https://doi.org/10.1145/3700145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {It is imperative to ensure that AI models perform well for all groups including those from underprivileged populations. By comparing the performance of models for the protected group with respect to the rest of the population, we can uncover and prevent unwanted bias. However, a significant drawback of such binary fairness evaluation is its dependency on protected group membership attributes. In various real-world scenarios, protected status for individuals is sparse, unavailable, or even illegal to collect. This article extends the previous work on binary fairness metrics to relax the requirement on deterministic membership to its surrogate counterpart under a probabilistic setting. We show how to conduct binary fairness evaluation when exact protected attributes are not available, but their surrogates as likelihoods are accessible. In theory, we prove that inferred metrics calculated from surrogates are valid under standard statistical assumptions. In practice, we demonstrate the effectiveness of our approach using publicly available data from the Home Mortgage Disclosure Act and simulated benchmarks that mimic real-world conditions under different levels of model disparity. We extend the results from previous work to include comparisons with alternative model-based methods and we develop further practical guidance based on our extensive simulation. Finally, we embody our method in open source software that is readily available for use in other applications.},
  archive  = {J},
  author   = {Serdar Kadioğlu and Melinda Thielbar},
  doi      = {10.1145/3700145},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {8},
  number   = {3},
  pages    = {1-25},
  title    = {Surrogate modeling to address the absence of protected membership attributes in fairness evaluation},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining multi-objective bayesian optimization with reinforcement learning for TinyML. <em>TELO</em>, <em>5</em>(3), 1-21. (<a href='https://doi.org/10.1145/3715012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deploying deep neural networks (DNNs) on microcontrollers (TinyML) is a common trend to process the increasing amount of sensor data generated at the edge, but in practice, resource and latency constraints make it difficult to find optimal DNN candidates. Neural architecture search (NAS) is an excellent approach to automate this search and can easily be combined with DNN compression techniques commonly used in TinyML. However, many NAS techniques are not only computationally expensive, especially hyperparameter optimization (HPO), but also often focus on optimizing only a single objective, e.g., maximizing accuracy, without considering additional objectives such as memory requirements or computational complexity of a DNN, which are key to making deployment at the edge feasible. In this article, we propose a novel NAS strategy for TinyML based on multi-objective Bayesian optimization (MOBOpt) and an ensemble of competing parametric policies trained using augmented random search (ARS) reinforcement learning (RL) agents. Our methodology aims at efficiently finding tradeoffs between a DNN’s predictive accuracy, memory requirements on a given target system, and computational complexity. Our experiments show that we consistently outperform existing MOBOpt approaches on different datasets and architectures such as ResNet-18 and MobileNetv3.},
  archive  = {J},
  author   = {Mark Deutel and Georgios Kontes and Christopher Mutschler and Jürgen Teich},
  doi      = {10.1145/3715012},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {8},
  number   = {3},
  pages    = {1-21},
  title    = {Combining multi-objective bayesian optimization with reinforcement learning for TinyML},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing batch diversity in surrogate optimization: A determinantal point processes approach. <em>TELO</em>, <em>5</em>(3), 1-30. (<a href='https://doi.org/10.1145/3721296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The exploration–exploitation tradeoff poses a significant challenge in surrogate optimization for expensive black-box functions, particularly when dealing with batch evaluation settings. Despite efforts to develop batch sampling techniques, they often fall short of sufficiently prioritizing diversity within the selected batch. In this article, we propose a fundamentally novel approach called Determinantal Point Processes (DPP)-Based Surrogate Optimization (DPPSO), which serves as a consolidated framework. DPPSO introduces a novel discretization scheme and sampling algorithm that fuses exploration and exploitation objectives by harnessing the power of DPP decomposition. An essential aspect of this project is the development of effective scoring functions to incorporate the quality of the sampled points in the decomposition. We provide theoretical guarantees achieving lower bounds on the probability of convergence. We demonstrate the effectiveness of DPPSO across different benchmarks, comparing its performance against various baseline methods.},
  archive  = {J},
  author   = {Nazanin Nezami and Hadis Anahideh},
  doi      = {10.1145/3721296},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {8},
  number   = {3},
  pages    = {1-30},
  title    = {Enhancing batch diversity in surrogate optimization: A determinantal point processes approach},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to cut generation in branch-and-cut algorithms for combinatorial optimization. <em>TELO</em>, <em>5</em>(3), 1-27. (<a href='https://doi.org/10.1145/3728371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Branch-and-cut is one of the most successful methods to exactly solve combinatorial optimization problems. A key decision problem in branch-and-cut is cut generation —the problem of deciding whether to generate cuts or to branch at each node of the search tree. This decision significantly impacts performance: generating efficient cuts can remove a substantial portion of the infeasible region and reduce tree size. However, in many cases, generating cuts slows runtime as separation routines could be time-consuming, and the violated cuts found by these routines could be inefficient. Hence, a smart strategy for generating cuts is crucial for the efficiency of branch-and-cut algorithms. There are two main types of cuts: generic cuts derived from the integrality of variables and combinatorial cuts based on the facial structure of the convex hull of feasible solutions. Combinatorial cuts are particularly determinant in branch-and-cut for many NP-hard combinatorial optimization problems, e.g., the Traveling Salesman Problem and the Max-Cut problem. In this article, we propose a framework combining supervised learning and deep reinforcement learning to learn strategies for generating combinatorial cuts in branch-and-cut. Our framework contains two components: a cut detector to predict the cut existence and a cut evaluator to choose between generating cuts and branching. We conduct experiments on two well-known combinatorial cut classes: subtour elimination constraints for the Traveling Salesman problem and cycle inequalities for the Max-Cut problem. Our results show that the proposed framework outperforms the commonly used strategies for cut generation, even on instances larger than those used for training.},
  archive  = {J},
  author   = {Trang Vo and Mourad Baiou and Viet Hung Nguyen and Paul Weng},
  doi      = {10.1145/3728371},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {8},
  number   = {3},
  pages    = {1-27},
  title    = {Learning to cut generation in branch-and-cut algorithms for combinatorial optimization},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RunAndSchedule2Survive: Algorithm scheduling based on Run2Survive. <em>TELO</em>, <em>5</em>(3), 1-17. (<a href='https://doi.org/10.1145/3737705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The algorithm selection problem aims to identify the most suitable algorithm for a given problem instance under specific time constraints, where suitability typically refers to a performance metric such as algorithm runtime. While previous work has employed machine learning techniques to tackle this challenge, methods from survival analysis have proven particularly effective. This article presents RunAndSchedule2Survive to address the more general and complex problem of algorithm scheduling, where the objective is to allocate computational resources across multiple algorithms to maximize performance within specified time constraints. Our approach combines survival analysis with evolutionary algorithms to optimize algorithm schedules by leveraging runtime distributions modeled as survival functions. Experimental results across various standard benchmarks demonstrate that our approach significantly outperforms previous methods for algorithm scheduling and yields more robust results than its algorithm selection variant. More specifically, RunAndSchedule2Survive achieves superior performance in 20 out of 25 benchmark scenarios, surpassing hitherto state-of-the-art approaches.},
  archive  = {J},
  author   = {Valentin Margraf and Tom Koerner and Alexander Tornede and Marcel Wever},
  doi      = {10.1145/3737705},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {8},
  number   = {3},
  pages    = {1-17},
  title    = {RunAndSchedule2Survive: Algorithm scheduling based on Run2Survive},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction to the special issue on learning and intelligent optimization. <em>TELO</em>, <em>5</em>(3), 1-2. (<a href='https://doi.org/10.1145/3757070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Kevin Tierney and Meinolf Sellmann},
  doi     = {10.1145/3757070},
  journal = {ACM Transactions on Evolutionary Learning},
  month   = {8},
  number  = {3},
  pages   = {1-2},
  title   = {Introduction to the special issue on learning and intelligent optimization},
  volume  = {5},
  year    = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). First steps toward a runtime analysis when starting with a good solution. <em>TELO</em>, <em>5</em>(2), 1-41. (<a href='https://doi.org/10.1145/3675783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The mathematical runtime analysis of evolutionary algorithms traditionally regards the time an algorithm needs to find a solution of a certain quality when initialized with a random population. In practical applications it may be possible to guess solutions that are better than random ones. We start a mathematical runtime analysis for such situations. We observe that different algorithms profit to a very different degree from a better initialization. We also show that the optimal parameterization of an algorithm can depend strongly on the quality of the initial solutions. To overcome this difficulty, self-adjusting and randomized heavy-tailed parameter choices can be profitable. Finally, we observe a larger gap between the performance of the best evolutionary algorithm we found and the corresponding black-box complexity. This could suggest that evolutionary algorithms better exploiting good initial solutions are still to be found. These first findings stem from analyzing the performance of the \((1+1)\) evolutionary algorithm and the static, self-adjusting, and heavy-tailed \((1+(\lambda,\lambda))\) genetic algorithms on the OneMax benchmark. We are optimistic that the question of how to profit from good initial solutions is interesting beyond these first examples.},
  archive  = {J},
  author   = {Denis Antipov and Maxim Buzdalov and Benjamin Doerr},
  doi      = {10.1145/3675783},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {5},
  number   = {2},
  pages    = {1-41},
  title    = {First steps toward a runtime analysis when starting with a good solution},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable optimisation through online and offline hyper-heuristics. <em>TELO</em>, <em>5</em>(2), 1-29. (<a href='https://doi.org/10.1145/3701236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Research in the explainability of optimisation techniques has largely focused on metaheuristics and their movement of solutions around the search landscape. Hyper-heuristics create a different challenge for explainability as they make use of many more operators, or low-level heuristics and learning algorithms which modify their probability of selection online. This article describes a set of methods for explaining hyper-heuristics decisions in both online and offline scenarios using selection hyper-heuristics as an example. These methods help to explain various aspects of the function of hyper-heuristics both at a particular juncture in the optimisation process and through time. Visualisations of each method acting on sequences provide an understanding of which operators are being utilised and when, and in which combinations to produce a greater understanding of the algorithm-problem nexus in hyper-heuristic search. These methods are demonstrated on a range of problems including those in operational research and water distribution network optimisation. They demonstrate the insight that can be generated from optimisation using selection hyper-heuristics, including building an understanding of heuristic usage, useful combinations of heuristics and heuristic parameterisations. Furthermore the dynamics of heuristic utility are explored throughout an optimisation run and we show that it is possible to cluster problem instances according to heuristic selection alone, providing insight into the perception of problems from a hyper-heuristic perspective.},
  archive  = {J},
  author   = {William B. Yates and Edward C. Keedwell and Ahmed Kheiri},
  doi      = {10.1145/3701236},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {5},
  number   = {2},
  pages    = {1-29},
  title    = {Explainable optimisation through online and offline hyper-heuristics},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EVOTER: Evolution of transparent explainable rule-sets. <em>TELO</em>, <em>5</em>(2), 1-30. (<a href='https://doi.org/10.1145/3702651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most AI systems are black boxes generating reasonable outputs for given inputs. Some domains, however, have explainability and trustworthiness requirements that cannot be directly met by these approaches. Various methods have therefore been developed to interpret black-box models after training. This article advocates an alternative approach where the models are transparent and explainable to begin with. This approach, EVOTER, evolves rule-sets based on extended propositional logic expressions. The approach is evaluated in several prediction/classification and prescription/policy search domains with and without a surrogate. It is shown to discover meaningful rule-sets that perform similarly to black-box models. The rules can provide insight into the domain and make hidden biases explicit. It may also be possible to edit the rules directly to remove biases and add constraints. EVOTER thus forms a promising foundation for building trustworthy AI systems for real-world applications in the future.},
  archive  = {J},
  author   = {Hormoz Shahrzad and Babak Hodjat and Risto Miikkulainen},
  doi      = {10.1145/3702651},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {5},
  number   = {2},
  pages    = {1-30},
  title    = {EVOTER: Evolution of transparent explainable rule-sets},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objectivising acquisition functions in bayesian optimisation. <em>TELO</em>, <em>5</em>(2), 1-33. (<a href='https://doi.org/10.1145/3716504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian optimisation (BO) is an efficient approach for solving expensive optimisation problems, where acquisition functions play a major role in achieving the tradeoff between exploitation and exploration. The exploitation–exploration tradeoff is challenging; excessive focus on exploitation can stagnate the search, while too much exploration can slow convergence. Multi-objectivisation has been explored as an effective approach to mitigate the exploitation–exploration tradeoff problem. Along this line, in this article, we propose a Multi-Objectivisation-Based Adaptive Exploitation–Exploration Tradeoff Framework (MOEE) to balance exploitation and exploration in BO. MOEE considers the nondominated front formed by the exploitation and exploration objectives and adaptively switches the focus on exploration and exploitation on the basis of the search status. We verify our method on the 19 synthetic and practical problem instances with 1–20 dimensions, and the results show that our proposed multi-objectivisation framework can achieve a good balance between exploitation and exploration.},
  archive      = {J_TELO},
  author       = {Chao Jiang and Miqing Li},
  doi          = {10.1145/3716504},
  journal      = {ACM Transactions on Evolutionary Learning and Optimization},
  month        = {5},
  number       = {2},
  pages        = {1-33},
  shortjournal = {ACM Trans. Evol. Learn. Optim.},
  title        = {Multi-objectivising acquisition functions in bayesian optimisation},
  volume       = {5},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable benchmarking for iterative optimization heuristics. <em>TELO</em>, <em>5</em>(2), 1-30. (<a href='https://doi.org/10.1145/3716638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.},
  archive  = {J},
  author   = {Niki van Stein and Diederick Vermetten and Anna V. Kononova and Thomas Bäck},
  doi      = {10.1145/3716638},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {5},
  number   = {2},
  pages    = {1-30},
  title    = {Explainable benchmarking for iterative optimization heuristics},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards explainable metaheuristics: Feature mining of search trajectories through principal component projection. <em>TELO</em>, <em>5</em>(2), 1-30. (<a href='https://doi.org/10.1145/3731456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While population-based metaheuristics have proven useful for refining and improving explainable AI systems, they are seldom the focus of explanatory approaches themselves. This stems from their inherently stochastic, population-driven searches, which complicate the use of standard explainability techniques. In this article, we present a method to identify which decision variables have the greatest impact during an algorithm’s trajectory from random initialsation to convergence. We apply Principal Component Analysis to project each population onto a lower-dimensional space, then introduce two metrics—Mean Variable Contribution and Proportion of Aligned Variables—to identify the variables most responsible for guiding the search. Using four different population-based methods (Particle Swarm Optimisation, Genetic Algorithm, Differential Evolution, and Covariance Matrix Adaptation Evolution Strategy) on 24 BBOB benchmark functions in 10 dimensions, we find that these metrics highlight meaningful variable relationships and provide a window into each method’s search dynamics. By comparing the features extracted across algorithms and problems, we illustrate how certain variable subsets consistently drive major improvements in solution quality. In doing so, new evolutionary algorithm variants can be designed to take advantage of these influential variables, while also identifying underutilised variables that may benefit alternative search strategies.},
  archive  = {J},
  author   = {Martin Fyvie and John A. W. McCall and Lee A. Christie},
  doi      = {10.1145/3731456},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {5},
  number   = {2},
  pages    = {1-30},
  title    = {Towards explainable metaheuristics: Feature mining of search trajectories through principal component projection},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction to the special issue on explainable AI in evolutionary Computation—Part 2. <em>TELO</em>, <em>5</em>(2), 1-2. (<a href='https://doi.org/10.1145/3733611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Jaume Bacardit and Alexander Brownlee and Stefano Cagnoni and Giovanni Iacca and John McCall and David Walker},
  doi     = {10.1145/3733611},
  journal = {ACM Transactions on Evolutionary Learning},
  month   = {5},
  number  = {2},
  pages   = {1-2},
  title   = {Introduction to the special issue on explainable AI in evolutionary Computation—Part 2},
  volume  = {5},
  year    = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Body and brain quality-diversity in robot swarms. <em>TELO</em>, <em>5</em>(1), 1-27. (<a href='https://doi.org/10.1145/3664656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In biological societies, complex interactions between the behavior and morphology of evolving organisms and their environment have given rise to a wide range of complex and diverse social structures. Similarly, in artificial counterparts such as swarm robotics systems, collective behaviors emerge via the interconnected dynamics of robot morphology (sensory-motor configuration), behavior (controller), and environment (task). Various studies have demonstrated morphological and behavioral diversity enables biological groups to exhibit adaptive, robust, and resilient collective behavior across changing environments. However, in artificial (swarm robotic) systems there is little research on the impact of changing environments on morphological and behavioral (body-brain) diversity in emergent collective behavior, and the benefits of such diversity. This study uses evolutionary collective robotics as an experimental platform to investigate the impact of increasing task environment complexity (collective behavior task difficulty) on the evolution and benefits of morphological and behavioral diversity in robotic swarms. Results indicate that body-brain evolution using coupled behavior and morphology diversity maintenance yields higher behavioral and morphological diversity, which is beneficial for collective behavior task performance across task environments. Results also indicate that such behavioral and morphological diversity maintenance coupled with body-brain evolution produces neuro-morpho complexity that does not increase concomitantly with task complexity.},
  archive  = {J},
  author   = {Sindiso Mkhatshwa and Geoff Nitschke},
  doi      = {10.1145/3664656},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {3},
  number   = {1},
  pages    = {1-27},
  title    = {Body and brain quality-diversity in robot swarms},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariance matrix adaptation MAP-annealing: Theory and experiments. <em>TELO</em>, <em>5</em>(1), 1-53. (<a href='https://doi.org/10.1145/3665336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Single-objective optimization algorithms search for the single highest quality solution with respect to an objective. Quality diversity (QD) optimization algorithms, such as Covariance Matrix Adaptation MAP-Elites (CMA-ME), search for a collection of solutions that are both high quality with respect to an objective and diverse with respect to specified measure functions. However, CMA-ME suffers from three major limitations highlighted by the QD community: prematurely abandoning the objective in favor of exploration, struggling to explore flat objectives, and having poor performance for low-resolution archives. We propose a new QD algorithm, CMA MAP-Annealing (CMA-MAE), and its differentiable QD variant, CMA-MAE via a Gradient Arborescence (CMA-MAEGA), that address all three limitations. We provide theoretical justifications for the new algorithm with respect to each limitation. Our theory informs our experiments, which support the theory and show that CMA-MAE achieves state-of-the-art performance and robustness on standard QD benchmark and reinforcement learning domains.},
  archive  = {J},
  author   = {Shihan Zhao and Bryon Tjanaka and Matthew C. Fontaine and Stefanos Nikolaidis},
  doi      = {10.1145/3665336},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {3},
  number   = {1},
  pages    = {1-53},
  title    = {Covariance matrix adaptation MAP-annealing: Theory and experiments},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MA-BBOB: A problem generator for black-box optimization using affine combinations and shifts. <em>TELO</em>, <em>5</em>(1), 1-19. (<a href='https://doi.org/10.1145/3673908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Choosing a set of benchmark problems is often a key component of any empirical evaluation of iterative optimization heuristics. In continuous, single-objective optimization, several sets of problems have become widespread, including the well-established BBOB suite. While this suite is designed to enable rigorous benchmarking, it is also commonly used for testing methods such as algorithm selection, which the suite was never designed around. We present the MA-BBOB function generator, which uses the BBOB suite as component functions in an affine combination. In this work, we describe the full procedure to create these affine combinations and highlight the tradeoffs of several design decisions, specifically the choice to place the optimum uniformly at random in the domain. We then illustrate how this generator can be used to gain more low-level insight into the function landscapes through the use of exploratory landscape analysis. Finally, we show a potential use-case of MA-BBOB in generating a wide set of training and testing data for algorithm selectors. Using this setup, we show that the basic scheme of using a set of landscape features to predict the best algorithm does not lead to optimal results, and that an algorithm selector trained purely on the BBOB functions generalizes poorly to the affine combinations.},
  archive  = {J},
  author   = {Diederick Vermetten and Furong Ye and Thomas Bäck and Carola Doerr},
  doi      = {10.1145/3673908},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {3},
  number   = {1},
  pages    = {1-19},
  title    = {MA-BBOB: A problem generator for black-box optimization using affine combinations and shifts},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergizing quality-diversity with descriptor-conditioned reinforcement learning. <em>TELO</em>, <em>5</em>(1), 1-35. (<a href='https://doi.org/10.1145/3696426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A hallmark of intelligence is the ability to exhibit a wide range of effective behaviors. Inspired by this principle, Quality-Diversity algorithms, such as MAP-Elites, are evolutionary methods designed to generate a set of diverse and high-fitness solutions. However, as a genetic algorithm, MAP-Elites relies on random mutations, which can become inefficient in high-dimensional search spaces, thus limiting its scalability to more complex domains, such as learning to control agents directly from high-dimensional inputs. To address this limitation, advanced methods like PGA-MAP-Elites and DCG-MAP-Elites have been developed, which combine actor-critic techniques from Reinforcement Learning with MAP-Elites, significantly enhancing the performance and efficiency of Quality-Diversity algorithms in complex, high-dimensional tasks. While these methods have successfully leveraged the trained critic to guide more effective mutations, the potential of the trained actor remains underutilized in improving both the quality and diversity of the evolved population. In this work, we introduce DCRL-MAP-Elites, an extension of DCG-MAP-Elites that utilizes the descriptor-conditioned actor as a generative model to produce diverse solutions, which are then injected into the offspring batch at each generation. Additionally, we present an empirical analysis of the fitness and descriptor reproducibility of the solutions discovered by each algorithm. Finally, we present a second empirical analysis shedding light on the synergies between the different variations operators and explaining the performance improvement from PGA-MAP-Elites to DCRL-MAP-Elites.},
  archive  = {J},
  author   = {Maxence Faldor and Félix Chalumeau and Manon Flageat and Antoine Cully},
  doi      = {10.1145/3696426},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {3},
  number   = {1},
  pages    = {1-35},
  title    = {Synergizing quality-diversity with descriptor-conditioned reinforcement learning},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMA-ES with learning rate adaptation. <em>TELO</em>, <em>5</em>(1), 1-28. (<a href='https://doi.org/10.1145/3698203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The covariance matrix adaptation evolution strategy (CMA-ES) is one of the most successful methods for solving continuous black-box optimization problems. A practically useful aspect of CMA-ES is that it can be used without hyperparameter tuning. However, the hyperparameter settings still have a considerable impact on performance, especially for difficult tasks, such as solving multimodal or noisy problems. This study comprehensively explores the impact of learning rate on CMA-ES performance and demonstrates the necessity of a small learning rate by considering ordinary differential equations. Thereafter, it discusses the setting of an ideal learning rate. Based on these discussions, we develop a novel learning rate adaptation mechanism for CMA-ES that maintains a constant signal-to-noise ratio. Additionally, we investigate the behavior of CMA-ES with the proposed learning rate adaptation mechanism through numerical experiments and compare the results with those obtained for CMA-ES with a fixed learning rate and with population size adaptation. The results show that CMA-ES with the proposed learning rate adaptation works well for multimodal and/or noisy problems without extremely expensive learning rate tuning.},
  archive  = {J},
  author   = {Masahiro Nomura and Youhei Akimoto and Isao Ono},
  doi      = {10.1145/3698203},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {3},
  number   = {1},
  pages    = {1-28},
  title    = {CMA-ES with learning rate adaptation},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evolutionary algorithm for expensive mixed-integer black-box optimization with explicit constraints. <em>TELO</em>, <em>5</em>(1), 1-16. (<a href='https://doi.org/10.1145/3707465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Determining the feasibility of a candidate solution to a constrained black-box optimization problem may be similarly expensive compared to the process of determining its quality, or it may be much cheaper. Constraints that allow obtaining degrees of feasibility or constraint violation without incurring significant computational costs are referred to as explicit. We present an evolutionary algorithm for solving mixed-integer black-box optimization problems where objective function evaluations are expensive but constraints are explicit. We do not assume relaxability of the constraints. The method wraps active-set evolution strategies, an algorithm for solving continuous black-box problems with explicit constraints, in a branching mechanism that allows enforcing integrality constraints. In computer experiments we demonstrate that the algorithm solves a set of mixed-integer problems with significantly fewer objective function evaluations than several algorithms that do not exploit the explicitness of the constraints.},
  archive  = {J},
  author   = {Yuan Hong and Dirk V. Arnold},
  doi      = {10.1145/3707465},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {3},
  number   = {1},
  pages    = {1-16},
  title    = {An evolutionary algorithm for expensive mixed-integer black-box optimization with explicit constraints},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully autonomous programming using iterative multi-agent debugging with large language models. <em>TELO</em>, <em>5</em>(1), 1-37. (<a href='https://doi.org/10.1145/3719351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Program synthesis with Large Language Models (LLMs) suffers from a “near-miss syndrome”: The generated code closely resembles a correct solution but fails unit tests due to minor errors. We address this with a multi-agent framework called Synthesize, Execute, Instruct, Debug, and Repair (SEIDR). Effectively applying SEIDR to instruction-tuned LLMs requires determining (a) optimal prompts for LLMs, (b) what ranking algorithm selects the best programs in debugging rounds, and (c) balancing the repair of unsuccessful programs with the generation of new ones. We empirically explore these tradeoffs by comparing replace-focused, repair-focused, and hybrid debug strategies. We also evaluate lexicase and tournament selection to rank candidates in each generation. On Program Synthesis Benchmark 2 (PSB2), our framework outperforms both conventional use of OpenAI Codex without a repair phase and traditional genetic programming approaches. SEIDR outperforms the use of an LLM alone, solving 18 problems in C++ and 20 in Python on PSB2 at least once across experiments. To assess generalizability, we employ GPT-3.5 and Llama 3 on the PSB2 and HumanEval-X benchmarks. Although SEIDR with these models does not surpass current state-of-the-art methods on the Python benchmarks, the results on HumanEval-C++ are promising. SEIDR with Llama 3-8B achieves an average pass@100 of 84.2%. Across all SEIDR runs, 163 of 164 problems are solved at least once with GPT-3.5 in HumanEval-C++, and 162 of 164 with the smaller Llama 3-8B. We conclude that SEIDR effectively overcomes the near-miss syndrome in program synthesis with LLMs.},
  archive  = {J},
  author   = {Anastasiia Grishina and Vadim Liventsev and Aki Härmä and Leon Moonen},
  doi      = {10.1145/3719351},
  journal  = {ACM Transactions on Evolutionary Learning},
  month    = {3},
  number   = {1},
  pages    = {1-37},
  title    = {Fully autonomous programming using iterative multi-agent debugging with large language models},
  volume   = {5},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction to the “Best of GECCO 2023” special issue. <em>TELO</em>, <em>5</em>(1), 1-2. (<a href='https://doi.org/10.1145/3721461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Luís Paquete and Sara Silva},
  doi     = {10.1145/3721461},
  journal = {ACM Transactions on Evolutionary Learning},
  month   = {3},
  number  = {1},
  pages   = {1-2},
  title   = {Introduction to the “Best of GECCO 2023” special issue},
  volume  = {5},
  year    = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
