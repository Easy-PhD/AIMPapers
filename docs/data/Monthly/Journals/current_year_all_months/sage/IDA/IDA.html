<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IDA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ida">IDA - 90</h2>
<ul>
<li><details>
<summary>
(2025). Deep learning approaches for robust prediction of large-scale renewable energy generation: A comprehensive comparative study from a national context. <em>IDA</em>, <em>29</em>(6), 1615--1635. (<a href='https://doi.org/10.1177/1088467X251325068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise forecasting of renewable energy generation is crucial for ensuring grid stability and enhancing the efficiency of energy management systems. This research develops and rigorously evaluates a range of deep learning models—such as Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRUs), and Bidirectional LSTM (BiLSTM) architectures—for predicting solar, wind, and total renewable energy production at a national scale. These models are systematically benchmarked against traditional machine learning approaches and gradient boosting methods to determine their predictive capabilities. The findings demonstrate that deep learning models incorporating memory mechanisms consistently surpass conventional methods, with BiLSTM standing out as the most precise and dependable model. Furthermore, the study investigates fully connected artificial neural networks (ANNs) and ConvLSTM2D models, reinforcing the advantages of memory-based architectures in modeling temporal relationships. By introducing a robust deep learning framework for large-scale renewable energy forecasting, this research represents a considerable leap forward compared to traditional machine learning techniques. The results highlight the transformative potential of deep learning in improving forecasting accuracy, thereby facilitating more effective energy planning and the smooth integration of renewable energy into national power grids.},
  archive      = {J_IDA},
  author       = {Necati Aksoy and Istemihan Genc},
  doi          = {10.1177/1088467X251325068},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1615--1635},
  shortjournal = {Intell. Data Anal.},
  title        = {Deep learning approaches for robust prediction of large-scale renewable energy generation: A comprehensive comparative study from a national context},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-related desynchronization detection and electroencephalography motor imagery classification using vision transformer. <em>IDA</em>, <em>29</em>(6), 1598--1614. (<a href='https://doi.org/10.1177/1088467X251324336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary objective of this research is to enhance the classification accuracy of motor imagery (MI) electroencephalography signals to improve brain–computer interfaces (BCIs) for communication among individuals with mobility limitations. The main challenge is finding the correct frequency bands, efficient time-frequency representations, and accurately classifying these representations. Various classification models are available but have not achieved high accuracy, which is a key evaluation matrix. This research provides a method that first identifies the best frequency range utilizing several fast-converging optimization approaches. After filtering with the identified band, a continuous wavelet transform was used with complex Morlet to obtain temporal frequency representations, which are effective sources of feature representation for MI tasks. These scalograms were then classified using the advanced deep learning model vision transformer, which is well-known for its ability to extract and select features using the attention mechanism. The proposed technique achieved remarkable accuracy, attaining 97.33% on a widely recognized dataset and 89.89% on another dataset, outperforming comparable research. Integrating modern signal processing and a cutting-edge deep model enhances accuracy, allows for neuroprosthetic device control, and offers up new avenues for research in the BCI arena.},
  archive      = {J_IDA},
  author       = {Vaishali Shirodkar and Damodar Reddy Edla and Annu Kumari and Sridhar Chintala},
  doi          = {10.1177/1088467X251324336},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1598--1614},
  shortjournal = {Intell. Data Anal.},
  title        = {Event-related desynchronization detection and electroencephalography motor imagery classification using vision transformer},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An inner-inter city graph neural network for predicting the course of COVID-19 cases. <em>IDA</em>, <em>29</em>(6), 1582--1597. (<a href='https://doi.org/10.1177/1088467X251356798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Highly infectious diseases like COVID-19 have affected millions worldwide, with transmission clearly linked to crowd mobility. However, research on the predictive power of city-level mobility remains limited. Most existing forecasting methods focus on state or national level, making it difficult for policymakers to quickly respond to emerging threats. To address this gap, we propose the Inner-Inter City Learner (IIL) for short- and mid-term COVID-19 case predictions at the city level, based on the correlation between human interaction and new cases. IIL consists of two key components: an inter-city transmission learner and an inner-city propagation learner. The first uses city mobility graphs and graph neural networks to learn how transmission in one city is influenced by others. The second, leveraging the highly contagious nature of the virus, captures key features of COVID-19 spread within cities. To overcome limited inner-city mobility data, we apply model-agnostic meta-learning to transfer common features across cities. We conduct various experiments and compare our methods with the state-of-art baselines. The results show the superiority of our method across various forecast horizons.},
  archive      = {J_IDA},
  author       = {Moussa Ndiaye and Tong Liu and Yangguang Cui and Yuhang Li and Hang Yu},
  doi          = {10.1177/1088467X251356798},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1582--1597},
  shortjournal = {Intell. Data Anal.},
  title        = {An inner-inter city graph neural network for predicting the course of COVID-19 cases},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-path neural network based on mp-MRI for predicting muscle-invasive bladder cancer. <em>IDA</em>, <em>29</em>(6), 1568--1581. (<a href='https://doi.org/10.1177/1088467X241313324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  author       = {Jie Yu and Lingkai Cai and Chunxiao Chen and Xue Fu and Yueyue Xiao and Liang Wang and Qinghua Zeng and Xueying Sun and Gongcheng Wang and Qiang Shao and Xiao Yang and Qiang Lu},
  doi          = {10.1177/1088467X241313324},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1568--1581},
  shortjournal = {Intell. Data Anal.},
  title        = {Multi-path neural network based on mp-MRI for predicting muscle-invasive bladder cancer},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved performance on melanoma skin cancer classification using deep learning based ensemble technique. <em>IDA</em>, <em>29</em>(6), 1549--1567. (<a href='https://doi.org/10.1177/1088467X251320265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer, particularly melanoma, arises from DNA damage that leads to abnormal cell growth in the epidermis. Early detection is crucial as melanoma can spread rapidly, but it is highly curable if identified promptly. Detecting and diagnosing melanoma early are essential to reduce mortality rates associated with this type of cancer. In the literature, various ensemble techniques have been proposed to improve the performance. This paper introduces a deep learning based ensemble method aimed at enhancing the accuracy of melanoma skin cancer detection. Additionally, it presents a thorough performance evaluation of five ensemble techniques. Initially, the dataset underwent pre-processing, involving the removal of artifacts through hair removal, and achieving a balance in the distribution of images for each class through image augmentation techniques. Then, the architecture of 16 pre-trained models was modified by adding additional layers to improve their performance. The models that achieved the highest melanoma accuracy were selected for ensembling. Since VGG16, MobileNetV2, and DenseNet169 achieved the highest melanoma accuracy, they were chosen for ensembling. Five ensemble techniques, namely, weighted average, voting, bagging, boosting, and stacking, were applied to the modified architectures of fine-tuned pre-trained models such as VGG16, MobileNetV2, and DenseNet169 to classify skin cancer images. The experiments were performed on a combined dataset of HAM10000 and ISIC2019, which contains images of seven skin lesion classes. The results demonstrate that the weighted average ensemble model achieves highest overall accuracy of 81.99% and melanoma classification accuracy of 89.85%. The positive outcomes affirm that employing ensemble techniques with adjusted model architectures enhances performance, thereby demonstrating their potential utility in the classification of skin cancer images.},
  archive      = {J_IDA},
  author       = {Naga Swetha R and Vimal K Shrivastava and Mohammad Farukh Hashmi},
  doi          = {10.1177/1088467X251320265},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1549--1567},
  shortjournal = {Intell. Data Anal.},
  title        = {Improved performance on melanoma skin cancer classification using deep learning based ensemble technique},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive generative knowledge distillation for dense object detection with reused detector. <em>IDA</em>, <em>29</em>(6), 1536--1548. (<a href='https://doi.org/10.1177/1088467X251325364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation has shown its potential in the field of computer vision, significantly reducing model parameters and memory usage with minimal impact on model performance. Although current mainstream methods face issues related to inconsistencies between distillation targets and real targets, as well as insufficient learning of teacher features. To address these issues, this paper presents a novel knowledge distillation framework, termed ADKD, designed to mitigate the limitations of current methodologies. It consists of two modules: AGD (Adaptive Generative Distillation) and RHD (Reused Head Distillation). Through AGD, the teacher guides the learning of student network, enabling student to achieve stronger representational power. Meanwhile, RHD effectively addresses the discrepancies between real targets and distillation targets. By masking and reusing feature maps and utilizing the teacher detection head, a more effective object detection model is obtained. The proposed approach is straightforward to implement and effective, having undergone extensive experimentation on datasets PASCAL VOC and MS COCO to demonstrate its efficacy. The results indicate that our method outperforms other knowledge distillation techniques.},
  archive      = {J_IDA},
  author       = {Hong Liang and Shuaiqing Wang and Qian Zhang and Mingwen Shao},
  doi          = {10.1177/1088467X251325364},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1536--1548},
  shortjournal = {Intell. Data Anal.},
  title        = {Adaptive generative knowledge distillation for dense object detection with reused detector},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantics-based oversampling for imbalanced named entity recognition datasets using Word2Vec embeddings. <em>IDA</em>, <em>29</em>(6), 1520--1535. (<a href='https://doi.org/10.1177/1088467X251322099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a persistent challenge in deep learning, often leading to suboptimal performance for underrepresented classes. This challenge is particularly pronounced in natural language processing (NLP) tasks, such as named entity recognition (NER), where traditional oversampling methods may overlook important linguistic nuances. In this study, we introduce a novel synonym-based oversampling technique that employs pre-trained Word2Vec embeddings to generate semantically coherent examples. This approach augments minority classes using contextually appropriate synonyms. Experiments on an imbalanced social media NER dataset demonstrate enhanced model performance, with improved recognition of named entities across diverse categories. By generating synthetic samples that closely mirror the original data’s semantic characteristics, our method offers a compelling solution to data imbalance in semantically driven NLP tasks. This research highlights the potential of semantic-based oversampling in enhancing the generalization capabilities of deep learning models for NER challenges.},
  archive      = {J_IDA},
  author       = {Adel Belbekri and Wissem Bouarroudj and Fouzia Benchikha and Zizette Boufaida},
  doi          = {10.1177/1088467X251322099},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1520--1535},
  shortjournal = {Intell. Data Anal.},
  title        = {Semantics-based oversampling for imbalanced named entity recognition datasets using Word2Vec embeddings},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable threatening tweets identification using abstract meaning representation graph and pattern structure approaches. <em>IDA</em>, <em>29</em>(6), 1501--1519. (<a href='https://doi.org/10.1177/1088467X251330977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms have become an integral part of people's lives, generating a huge amount of data. Threatening speech detection has gained serious attention in recent years. To control the dissemination of such content on social media, the design of an artificial intelligence-based pipeline is an active area of research. The majority of prior studies handled under-resourced languages for detecting threatening speech and very limited research is observed on the English language. Furthermore, there is no work on the explainability of prediction inference for the said task. This study proposes an inherently interpretable threatening speech detection framework for the Twitter platform. The strengths of Pattern Structures and Abstract Meaning Representations graphs are explored first time to support classification and explainability tasks for threatening speech detection. The proposed system automatically mines the context of threatening speech and provides intermediate and final explanations for the prediction inference. A new English corpus is built for the experimental evaluation of the proposed framework for the Twitter platform. The experimental evaluation demonstrates that the proposed framework outperformed the standard baselines and obtained benchmark performance with an accuracy of 73.15% stably on the newly built corpus. In addition, the intermediate and final interpretations offered by the inherently explainable proposed framework provide meaningful and trustworthy prediction inference. The findings of this study have several implications for regulating social media and future research.},
  archive      = {J_IDA},
  author       = {Muhammad Shahid Iqbal Malik and Anna Nazarova and Mona Mamdouh Jamjoom},
  doi          = {10.1177/1088467X251330977},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1501--1519},
  shortjournal = {Intell. Data Anal.},
  title        = {Explainable threatening tweets identification using abstract meaning representation graph and pattern structure approaches},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VyAnG-net: A novel multi-modal sarcasm recognition model by uncovering visual, acoustic and glossary features. <em>IDA</em>, <em>29</em>(6), 1478--1500. (<a href='https://doi.org/10.1177/1088467X251315637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various linguistic and non-linguistic clues, such as excessive emphasis on a word, a shift in the tone of voice, or an awkward expression, frequently convey sarcasm. The computer vision problem of sarcasm recognition in conversation aims to identify hidden sarcastic, criticizing, and metaphorical information embedded in everyday dialogue. Prior, sarcasm recognition has focused mainly on text. Still, it is critical to consider all textual information, audio stream, facial expression, and body position for reliable sarcasm identification. Hence, we propose a novel approach that combines a lightweight depth attention module with a self-regulated ConvNet to concentrate on the most crucial features of visual data and an attentional tokenizer-based strategy to extract the most critical context-specific information from the textual data. The following is a list of the key contributions that our experimentation has made in response to performing the task of Multi-modal Sarcasm Recognition: an attentional tokenizer branch to get beneficial features from the glossary content provided by the subtitles; a visual branch for acquiring the most prominent features from the video frames; an utterance-level feature extraction from acoustic content and a multi-headed attention based feature fusion branch to blend features obtained from multiple modalities. Extensive testing on one of the benchmark video datasets, MUSTaRD, yielded an accuracy of 79.86% for speaker\; dependent and 76.94% for speaker\; independent configuration demonstrating that our approach is superior to the existing methods. We have also conducted a cross-dataset analysis to test the adaptability of VyAnG-Net with unseen samples of another dataset MUStARD++.},
  archive      = {J_IDA},
  author       = {Ananya Pandey and Dinesh Kumar Vishwakarma},
  doi          = {10.1177/1088467X251315637},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1478--1500},
  shortjournal = {Intell. Data Anal.},
  title        = {VyAnG-net: A novel multi-modal sarcasm recognition model by uncovering visual, acoustic and glossary features},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated ensemble learning on long-tailed data with prototypes. <em>IDA</em>, <em>29</em>(6), 1459--1477. (<a href='https://doi.org/10.1177/1088467X251317420'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning enables multiple participants to train models without sharing their raw data. However, long-tailed data with imbalanced sample sizes among clients deteriorates the model’s performance in federated learning. Additionally, existing studies on class prototypes are less effective for federated long-tailed issues, as the difference in class prototype representation between the head classes and tail classes exacerbates global model updates and instability among clients. Therefore, we propose a Federated Ensemble Prototypes Learning (FedEP) approach that employs ensemble class prototypes instead of local class prototypes to alleviate class representation bias. Specifically, each client partitions its local dataset into multiple subsets to derive subset class prototypes and filters biased subset class prototypes using a threshold to obtain ensemble class prototypes. The server then aggregates these ensemble prototypes to develop novel global ones, which guide local training without extra data. Concurrently, we track category probability differences to assess the degree of deviation among class prototypes during the iterative process. Furthermore, our method has proven effective and outperforms baseline approaches on long-tailed data across various experimental settings.},
  archive      = {J_IDA},
  author       = {Yang Li and Xin Liu and Kan Li},
  doi          = {10.1177/1088467X251317420},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1459--1477},
  shortjournal = {Intell. Data Anal.},
  title        = {Federated ensemble learning on long-tailed data with prototypes},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A responsible AI approach for designing resilient classifier to handle incomplete data. <em>IDA</em>, <em>29</em>(6), 1438--1458. (<a href='https://doi.org/10.1177/1088467X251314758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values can greatly affect analyses and decision-making in many fields. In the context of Responsible Artificial Intelligence (AI), ensuring the robustness of machine learning models is essential because Responsible AI emphasizes reliability and interpretability in decision-making processes. However, traditional imputation and ensemble learning methods often fail to preserve critical relationships between independent and dependent variables, introducing bias or noise into the data and undermining the development of robust classification models. To address these challenges, we propose a novel classification approach that aligns with Responsible AI principles. Our Resilient Decision Tree classifier is specifically designed to handle incomplete datasets. We employ subspace classifiers that operate on different non overlapping subsets of features without relying on imputation. By combining these subspace models into a weighted ensemble classifier, we enhance prediction accuracy for test datasets with missing values. The experimental results obtained on real-life and synthetic datasets demonstrate that our methodology produces an effective ensemble classifier.},
  archive      = {J_IDA},
  author       = {Sairam Utukuru and P Radha Krishna},
  doi          = {10.1177/1088467X251314758},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1438--1458},
  shortjournal = {Intell. Data Anal.},
  title        = {A responsible AI approach for designing resilient classifier to handle incomplete data},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable deep classification of time series based on class discriminative prototype learning. <em>IDA</em>, <em>29</em>(6), 1419--1437. (<a href='https://doi.org/10.1177/1088467X251319188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prototypes help to explain the predictions of deep classification models for time series. However, most models learn prototypes by randomly initializing an uncertain number of low-discriminative prototypes, which may lead to unstable models and unreliable results. To address these issues, we propose a new class D iscriminative P rototype L earning Net work (DPL-Net), which learns an appropriate number of class-discriminative prototypes, thus improving classification performance. Specifically, the proposed P rototype I nitialization M echanism (PIM) introduces a new proximity metric based on the silhouette coefficient and statistical metrics. It facilitates the automatic determination of the class-discriminative prototypes for each class. Then, the encoder layer encodes the prototypes derived from PIM and the input series using one-dimensional convolutional neural networks (1D-CNN). Finally, the prototype classification layer optimizes the prototypes according to the regularization terms, while simultaneously classifying the input sequence based on its similarity to the updated prototypes. The comparison experiments are conducted on 26 UCR datasets compared with 10 baselines. The results show that our proposed approach achieves the best accuracy on 11 datasets. Specifically, our method outperforms PIP, CSSL, and LSS by an average of 16.33%, 9.77% and 5.96% on 22, 14 and 16 datasets, respectively. The interpretability experimental results and the application analysis on spectral data indicate that the learned prototypes can provide reasonable explanations for the classification results of the model.},
  archive      = {J_IDA},
  author       = {Yupeng Wang and Jianghui Cai and Haifeng Yang and Chenhui Shi and Min Zhang and Jie Wang and Ran Zhang and Xujun Zhao},
  doi          = {10.1177/1088467X251319188},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1419--1437},
  shortjournal = {Intell. Data Anal.},
  title        = {Interpretable deep classification of time series based on class discriminative prototype learning},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSDNet: A two-stage decomposition-based hybrid deep neural network for long-term time series forecasting. <em>IDA</em>, <em>29</em>(6), 1399--1418. (<a href='https://doi.org/10.1177/1088467X241308796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to recent developments in deep learning models, the field of time series forecasting has undergone significant advancements related to forecasting accuracy and reliability across various industrial sectors. Unfortunately, traditional deep learning models often encounter long-term and multivariate forecasting challenges due to complex temporal patterns. To overcome this, decomposition-based approaches have been proposed. However, there have been few attempts to utilize an appropriate network type for each decomposed component. In this paper, we propose the two-stage decomposition-based hybrid deep neural network (TSDNet) for enhancing the accuracy of long-term time series forecasting. To effectively manage complicated time series data with varying periodicities, TSDNet accommodates a single linear layer for forecasting smooth trend components and a convolutional module for complex seasonal components. Extensive experiments on various benchmark and real-world financial datasets show that TSDNet mostly improves the forecasting accuracy compared to the existing methods considered, particularly in long-term forecasting scenarios. Furthermore, ablation studies were conducted to examine the impact of the number of decomposition stages and the implementation of different modules on the decomposed elements, suggesting the effectiveness of the proposed approach.},
  archive      = {J_IDA},
  author       = {Sukhyun Cho and Dokyun Kim and Jonghun Park and In-Beom Park},
  doi          = {10.1177/1088467X241308796},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1399--1418},
  shortjournal = {Intell. Data Anal.},
  title        = {TSDNet: A two-stage decomposition-based hybrid deep neural network for long-term time series forecasting},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph guided local structure propagation for tensorial multi-view subspace clustering. <em>IDA</em>, <em>29</em>(6), 1379--1398. (<a href='https://doi.org/10.1177/1088467X251315633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering (MSC) is widely studied owing to the ability to capture the diverse and complementary information hidden in multiple views. As a representative model, tensorial MSC can capture global information by leveraging high-order correlations across various perspectives, leading to promising results. However, this approach fails to reveal the local structure in the specific view and ignores the prior information of the self-representation tensor. To address the problems, we propose the novel graph-guided local structure propagation (GGLSP) for tensorial MSC. First, we improve the adaptive graph model to acquire a fused graph similarity matrix for extracting the relationships between samples and propagating the local structure information to the self-representation tensor. Subsequently, we introduce the weighted tensor Schatten p-norm to approximate the tensor rank function by exploiting the contributions of different singular values so that the self-representation tensor can better reveal the global low-rank structure information. Finally, we develop two efficient algorithms to solve the optimization problems. Large numbers of experiments on seven popular datasets confirm the superiority of our proposed GGLSP.},
  archive      = {J_IDA},
  author       = {Tao Zhang and Yan Qin and Yizhang Wang and Xiaobo Shen and Fan Liu},
  doi          = {10.1177/1088467X251315633},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1379--1398},
  shortjournal = {Intell. Data Anal.},
  title        = {Graph guided local structure propagation for tensorial multi-view subspace clustering},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank multi-view subspace clustering via adaptive weight. <em>IDA</em>, <em>29</em>(6), 1367--1378. (<a href='https://doi.org/10.1177/1088467X251314322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering, which aims to partition a set of multi-source data into a common space, has recently attracted wide attention in the field of data analysis and machine learning. Traditional algorithms may face the problem of high complexity in calculating the self-expression matrix and in turning parameter. This paper proposes a novel multi-view subspace clustering model termed as Low-rank Multi-view Subspace Clustering via Adaptive Weight (LMSCAW). LMSCAW decomposes the self-expression matrix into the product of two low-rank representation matrices and thus can fix the rank of the self-expression matrix of each view to increase the stability of the algorithm. In addition, in order to learn the common representation matrix better, LMSCAW fuses the self-expression matrices among multiple views and implicitly weights each view by the Frobenius norm without additional parameters. Extensive experimental results on multiple benchmark datasets are provided to show the effectiveness of the proposed algorithm and its superior performance over other state-of-the-art methods.},
  archive      = {J_IDA},
  author       = {Yuanyuan Jiao and Xiao Ouyang and Ruidong Fan and Chenping Hou},
  doi          = {10.1177/1088467X251314322},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1367--1378},
  shortjournal = {Intell. Data Anal.},
  title        = {Low-rank multi-view subspace clustering via adaptive weight},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A detection of multi-level co-location patterns based on column calculation and HDBSCAN clustering. <em>IDA</em>, <em>29</em>(6), 1349--1366. (<a href='https://doi.org/10.1177/1088467X241308765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A spatial co-location pattern is a subset of a spatial feature set whose instances prevalently appear in nearby locations in space. The objective of spatial co-location pattern mining is to detect co-location patterns that are non-obvious, informative, or predictive. Due to the heterogeneity in the distribution of spatial instances, spatial co-location patterns are classified into global co-location patterns (GCPs) and local co-location patterns (LCPs). The technique that discovers both simultaneously is termed multi-level co-location pattern mining (MLCPM). However, existing MLCPM methods have room for improvement in efficiently identifying GCPs and perform poorly in discovering prevalent sub-areas of LCPs. To address these issues, we propose a novel MLCPM framework called ML-CCHDB. This framework enhances GCP mining efficiency by optimizing the column calculation method tailored for MLCPM. Furthermore, it utilizes the HDBSCAN clustering method to identify potential prevalent sub-areas of LCPs and develops an adaptive approach for generating input parameters to enhance detection efficiency and quality. Experimental results on both synthetic and real datasets demonstrate that the column calculation optimizations in ML-CCHDB effectively enhance efficiency. Moreover, HDBSCAN strikes a balance between efficiency and quality in prevalent sub-area mining. These results fully validate the proposed framework's effectiveness and efficiency.},
  archive      = {J_IDA},
  author       = {Ting Yang and Lizhen Wang and Lihua Zhou and Hui Chen},
  doi          = {10.1177/1088467X241308765},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1349--1366},
  shortjournal = {Intell. Data Anal.},
  title        = {A detection of multi-level co-location patterns based on column calculation and HDBSCAN clustering},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial. <em>IDA</em>, <em>29</em>(6), 1347--1348. (<a href='https://doi.org/10.1177/1088467X251372297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.1177/1088467X251372297},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1347--1348},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-strategy improved african vulture optimization algorithm for global optimization and engineering design problems. <em>IDA</em>, <em>29</em>(5), 1313--1344. (<a href='https://doi.org/10.1177_1088467X241301637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a multi-strategy improved African Vulture Optimization Algorithm (MIAVOA) is proposed address the drawback of premature convergence of the African Vulture Optimization Algorithm (AVOA). Firstly, the gaussian quasi reflection-based learning strategy is introduced, which improves the vulture initial population’s randomness and diversity. Then, the adaptive control strategy is used to enhance the search ability of the algorithm and avoid premature convergence. Furthermore, the elite candidate pooling strategy is designed in the exploitation phase, which expands the discovery fields for the optimal solution and reinforces the ability to escape from local optima. Finally, the formula of starvation factor is modified to balance the exploitative and explorative abilities of algorithm. MIAVOA is compared with seven state-of-the-art meta-heuristics on CEC 2022 and 23 classical test functions. It is observed that the proposed algorithm significant outperforms the other compared algorithms in terms of convergence and accuracy on the majority of benchmark functions. In addition, four engineering design problems and mobile robot path planning problem are utilized to evaluate the performance of MIAVOA. The experimental results demonstrate MIAVOA is effective and can achieve better applicability in real-world scenarios.},
  archive      = {J_IDA},
  author       = {Xinzhe Li and Qingyang Zhang and Shengxiang Yang and Yongquan Dong},
  doi          = {10.1177_1088467X241301637},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1313--1344},
  shortjournal = {Intell. Data Anal.},
  title        = {Multi-strategy improved african vulture optimization algorithm for global optimization and engineering design problems},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Livernet based segmentation of lesions from computed tomography scan for liver tumor detection. <em>IDA</em>, <em>29</em>(5), 1289--1312. (<a href='https://doi.org/10.1177/1088467X241301660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millions of lives could be saved annually if liver tumours could be detected early with computed tomography. But it's a huge strain for radiologists to read hundreds or even tens of these CT scans. Therefore, developing an autonomous, rapid, and reliable method of reading, detecting, and assessing CT scans is important. However, extracting the liver region from CT scans is a bottleneck for any approach. This paper introduces a three-part automatic process. Initial processing includes noise suppression and image enhancement. Optimized Bi-lateral Filtering is used to carry it out; in this case, the process's control parameters are optimized using the Monarch butterfly optimization method. After that, automatic liver segmentation and lesion identification are performed. Mask-Region-based Convolutional Neural Network segment liver from the pre-processed images. Then a new generator network named LiverNet is used to detect tumors within the liver. Finally, an Enhanced Swin Transformer Network employing Adversarial Propagation distinguishes between malignant and benign liver lesions. Positive developments were discovered as a result of the inquiry. Expert results are associated with the consequences of segmentation and analysis. The classifier makes a relatively accurate tumour differentiation and gives the radiologist a second opinion.},
  archive      = {J_IDA},
  author       = {Priyan Malarvizhi Kumar and Hardik Gohel and Jeeva Selvaraj and Balasubramanian Prabhu Kavin},
  doi          = {10.1177/1088467X241301660},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1289--1312},
  shortjournal = {Intell. Data Anal.},
  title        = {Livernet based segmentation of lesions from computed tomography scan for liver tumor detection},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YONET: A unified model for simultaneous identification and segmentation of structures in medical images. <em>IDA</em>, <em>29</em>(5), 1275--1288. (<a href='https://doi.org/10.1177/1088467X241301679'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proposed novel model, YONET, performs the identification and segmentation of medical images collaboratively. Many types of image-processing software are being developed to extract meaningful information from medical photos to enhance patient diagnosis. Simultaneous segmentation and object detection is a computer vision task that combines object detection and image segmentation, two related functions usually done separately. It involves detecting objects in an image and generating a segmentation mask for each object. This assortment of tasks can enhance the precision and velocity of object detection by providing more accurate object boundaries and, in some cases, eliminating the need for post-processing steps. The image segmentation module in YONET handles intermediate abstract representations and utilizes them as input for object detection. YONET will be trained on bounding boxes that delineate the detected objects and pixel-wise segmentation information. The resultant system is optimized for segmenting an optionally distinct class of structures and detecting a class of objects.},
  archive      = {J_IDA},
  author       = {M Bhavani and Prithi Samuel},
  doi          = {10.1177/1088467X241301679},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1275--1288},
  shortjournal = {Intell. Data Anal.},
  title        = {YONET: A unified model for simultaneous identification and segmentation of structures in medical images},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-prompt complementary fusion network for RGBT tracking. <em>IDA</em>, <em>29</em>(5), 1261--1274. (<a href='https://doi.org/10.1177/1088467X241308764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGBT target tracking is a significant downstream task in the field of object tracking. However, compared to visible light target tracking, RGBT target tracking faces the challenge of smaller datasets, making it difficult to achieve performance levels comparable to those achieved in visible light target tracking. To address how to effectively combine the complementary characteristics of visible and thermal modalities, as well as how to fully leverage the superior performance of models trained on visible light target tracking tasks, while also aiming for lower computational costs and higher tracking effectiveness, a dual-prompt complementary fusion strategy for an RGBT tracking network is proposed. Drawing on the concept of prompt learning, this network aims to extend the efficient performance of visible light target tracking to the RGBT target tracking domain. In its implementation, the prompt module inputs both visible and thermal modality information as dual prompts into the backbone network, where the network utilizes these prompts to generate new, enriched prompt information at each layer. Subsequently, an information enhancement fusion module enhances the acquired prompt information and refeeds it into the backbone network, aiming to improve the tracking accuracy and robustness. Experimental results on GTOT, RGBT234 and LasHeR datasets show that the tracking accuracy (PR) and success rate (SR) of the network reach 93.1%/76.8%, 84.4%/62.4% and 66.8%/53.8%, respectively, which is improved compared with the current mainstream RGBT target tracking network, which verifies the effectiveness of the network.},
  archive      = {J_IDA},
  author       = {Xihui Wu and Hongwei Ge and Ting Li and Shuzhi Su},
  doi          = {10.1177/1088467X241308764},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1261--1274},
  shortjournal = {Intell. Data Anal.},
  title        = {Dual-prompt complementary fusion network for RGBT tracking},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilingual generated text detection through semantic and statistical analysis. <em>IDA</em>, <em>29</em>(5), 1248--1260. (<a href='https://doi.org/10.1177_1088467X241307192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The release of Large Language Models (LLMs) has achieved human-level text generation, leading to malicious uses such as disinformation propagation and academic dishonesty. Existing research has faced substantial challenges in low detection rates and poor generalization on multilingual generated text and short text. To fill these gaps, in this paper, we propose a generic bilingual generated text detection model to integrate semantic and statistical features, which exhibits proficiency in English and Chinese. To obtain fine-grained features, we employ the multilingual pre-trained language model xlm-RoBERTa to extract the CLS vector as overall semantic features, integrating with statistical features log rank, probability, and cumulative probability for detection. Moreover, Shapley additive explanations (SHAP) serves to interpret the decision-making process. The experimental results demonstrate significant advancements over baselines, notably with the F1 score improvements exceeding 10% and 5% on the English and Chinese HC3 sentence-level datasets, respectively. Our proposed method exhibits higher generalization for advanced LLMs and out-of-domain datasets with a 91.13% F1 score, thereby providing a more robust solution for detecting generated text.},
  archive      = {J_IDA},
  author       = {Chenxi Min and Ru Zhang and Jianyi Liu},
  doi          = {10.1177_1088467X241307192},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1248--1260},
  shortjournal = {Intell. Data Anal.},
  title        = {Bilingual generated text detection through semantic and statistical analysis},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring data augmentation techniques for advancing emotion recognition in text - An ANOVA-RFE fusion approach with emotion fusion ensemble. <em>IDA</em>, <em>29</em>(5), 1219--1247. (<a href='https://doi.org/10.1177/1088467X251325354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in text is a complex challenge essential for enhancing human-computer interaction. This study introduces an advanced system utilizing the EmoBank dataset, which contains text annotated with six basic emotions from diverse sources like social media, narratives, news articles, and movie scripts, ensuring accurate labeling through automated algorithms and manual annotation. Extensive preprocessing techniques, including tokenization, lowercasing, stop words removal, stemming, lemmatization, punctuation removal, handling special characters and numbers, text normalization, and encoding, prepare the text for analysis. Data augmentation methods such as random insertion, deletion, swapping, synonym replacement, and leveraging Large Language Models (LLMs) like GPT-3, BERT, and RoBERTa enrich the dataset. Feature extraction combines word embeddings with self-attention mechanisms to capture contextual and semantic information. The ANOVA-RFE Fusion technique is applied for feature selection, while the Emotion Fusion Ensemble (EFE) method enhances classification by combining Random Forest, Gradient Boosting, AdaBoost, XGBoost, Extra Trees, SVM, and K-NN. Systematic experimentation and hyperparameter tuning using grid search validate the system's performance. Notably, the combination of GPT-3+WE+ANOVA-RFE+EFE achieved 89% accuracy before tuning and 94% after tuning. This research underscores the critical role of integrated processing, augmentation, and ensemble learning in advancing emotion recognition, suggesting future exploration of emerging language models, novel augmentation techniques, and domain specific adaptations for developing more accurate and robust systems.},
  archive      = {J_IDA},
  author       = {Nirmal Varghese Babu and E. Grace Mary Kanaga},
  doi          = {10.1177/1088467X251325354},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1219--1247},
  shortjournal = {Intell. Data Anal.},
  title        = {Exploring data augmentation techniques for advancing emotion recognition in text - An ANOVA-RFE fusion approach with emotion fusion ensemble},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel HVAC fuzzy controller based on improved snake optimizer algorithm. <em>IDA</em>, <em>29</em>(5), 1199--1218. (<a href='https://doi.org/10.1177_1088467X241303347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heating, ventilation, and air conditioning (HVAC) systems are characterized by time-varying, nonlinear, and parametric coupling, making it challenging to design their controllers. As a major source of energy consumption in buildings, the control schemes for HVAC systems need to be optimized to improve energy efficiency. To address the above issues, a fuzzy controller was designed to regulate the HVAC system, and an improved snake optimizer (ISO) was proposed to optimize the membership function of the fuzzy controller in this study. In the ISO algorithm, the parameter Threshold of the snake optimizer (SO) was dynamically adjusted, and the snake egg-hatching formula was improved. Benchmark function tests show that the convergence speed and optimization accuracy of ISO are superior to those of the established comparison algorithms. Furthermore, simulation experiments indicate that the ISO-optimized HVAC system has a temperature error of 0 and a humidity ratio error of less than 1.5% compared to competing methods. In addition, the ISO-optimized HVAC system achieves a 40.8% reduction in annual energy consumption.},
  archive      = {J_IDA},
  author       = {Zuqiang Long and Jiaying Gu and Zhiyong Hu and Ke Sun},
  doi          = {10.1177_1088467X241303347},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1199--1218},
  shortjournal = {Intell. Data Anal.},
  title        = {A novel HVAC fuzzy controller based on improved snake optimizer algorithm},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance comparison: Cloud-based vs. open-source NER for english and polish. <em>IDA</em>, <em>29</em>(5), 1187--1198. (<a href='https://doi.org/10.1177_1088467X241305521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Entity Recognition (NER) plays a vital role in Natural Language Processing (NLP) tasks, extracting valuable information from textual data. This study addresses a gap in NER research by comparing the effectiveness of cloud-based NER tools (Azure NER and Google Cloud NER) and a popular open-source tool (SpaCy) for recognizing named entities in both English and Polish text. Text data is imported into a PostgreSQL database and processed by each NER tool. The extracted entities and their labels are stored in a dedicated SQL Entity table, enabling performance evaluation across different languages and entity types. This research contributes to the field of NLP by investigating the suitability of cloud-based NER tools for multilingual tasks, particularly those involving Polish text, which presents unique linguistic challenges. By analyzing the performance of these NER approaches, the study provides valuable insights for selecting the most effective NER technique for specific NLP applications, especially when dealing with multilingual content.},
  archive      = {J_IDA},
  author       = {Lukasz Pawlik},
  doi          = {10.1177_1088467X241305521},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1187--1198},
  shortjournal = {Intell. Data Anal.},
  title        = {Performance comparison: Cloud-based vs. open-source NER for english and polish},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Some novel probabilistic dual hesitant fuzzy information measures and their applications to multi-attribute decision-making. <em>IDA</em>, <em>29</em>(5), 1153--1186. (<a href='https://doi.org/10.1177/1088467X241308761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic dual hesitant fuzzy set (PDHFS), as an extension of dual hesitant fuzzy set (DHFS) and probabilistic hesitant fuzzy set (PHS), can better describe the uncertainty and fuzziness in the real world through membership degree (MD) and non-membership degree (NMD) and their respective probability information. Therefore, it can collect more comprehensive fuzzy information in decision-making issues, which makes PDHFS has a wide range of applications in practice. The distance, similarity and entropy measures (EMs) of PDHFSs were studied. Due to the particularity of PDHFS, the length of MD and NMD of two probabilistic dual hesitation fuzzy elements (PDHFEs) are often different. To define the distance measure more expediently, many current studies need to add fresh elements to ensure that the length of corresponding elements in MD and NMD are the same, which will destroy the original information of the elements. In order to overcome this disadvantage, this paper defined some novel PDHF distance measures without adding elements. Some novel PDHF similarity measures were defined through the complementary relationship between distance and similarity measures based on the newly proposed PDHF distance measures. The fuzziness of a given PDHFS can be measured by the similarity measure between it and its complement. Based on the property, some novel PDHF EMs were proposed. Finally, three numerical examples are given to illustrate the effectiveness and rationality of these novel information measures.},
  archive      = {J_IDA},
  author       = {Baoquan Ning and Cun Wei and Guiwu Wei},
  doi          = {10.1177/1088467X241308761},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1153--1186},
  shortjournal = {Intell. Data Anal.},
  title        = {Some novel probabilistic dual hesitant fuzzy information measures and their applications to multi-attribute decision-making},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VLinear: Enhanced linear complexity time series forecasting model. <em>IDA</em>, <em>29</em>(5), 1142--1152. (<a href='https://doi.org/10.1177_1088467X241303376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel time series prediction model(VLinear), designed with linear computational complexity, demonstrating superior predictive performance compared to DLiner without increasing parameter count or computational demands. Our model introduces two key innovations: first, a Feature Pyramid Network (FPN) is employed to effectively capture time series data characteristics, bypassing the traditional decomposition into trend and seasonal components. Second, a multi-layer fusion structure is developed to integrate deep and shallow features seamlessly. Empirically, VLinear outperforms DLiner in 31 out of 32 test cases on eight open-source datasets, with an average reduction of 16.8% in mean squared error (MSE) and 11.8% in mean absolute error (MAE). Additionally, compared to the transformer-based PatchTST, VLinear achieves 10 best MSE and 15 best MAE results, using only 8% of PatchTST’s total computational load in the 32 test projects.},
  archive      = {J_IDA},
  author       = {Chu Li and Bingjia Xiao and Qiping Yuan},
  doi          = {10.1177_1088467X241303376},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1142--1152},
  shortjournal = {Intell. Data Anal.},
  title        = {VLinear: Enhanced linear complexity time series forecasting model},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task intent recommendation based on dynamic and static intent integration and disentanglement. <em>IDA</em>, <em>29</em>(5), 1122--1141. (<a href='https://doi.org/10.1177/1088467X241301915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems that mine users’ intentions to explore their potential interaction preferences have received increasing attention. However, the existing research on intent recommendation has some limitations. On one hand, the extant studies only consider users’ historical interaction information and the sparse interactions cannot reflect users’ potential interaction intention; on the other hand, they do not consider the changes in users’ kinematic and static intentions over time and the importance of users’ intention disentanglement representation, which makes it impossible for the general intention recommendation model to obtain a better representation of the intention. We propose a multitask recommendation model with dynamic and static intent integration and de-entanglement. The model mines users’ dynamic and static intents and then combines them with regularization to model the independence of the intents, encouraging the differences between the intents. Meanwhile, to further alleviate the data sparsity problem, this study additionally constructs user–user and item–item graphs using four different similarity measures, such as cosine similarity and mutual information, applies graph convolutional networks to learn about the three graphs, and then captures the complementarity between different graphs using a graph-level cross-attention mechanism. Extensive comparative experiments and ablation studies on three public datasets demonstrate that DSI-ID consistently outperforms all baseline methods, achieving a 3.5%–7.4% improvement in recommendation performance over the best baseline.},
  archive      = {J_IDA},
  author       = {Xiao Huang and Xianyi Zhang and Tao Huang and Lin Liu and Junhao Wen},
  doi          = {10.1177/1088467X241301915},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1122--1141},
  shortjournal = {Intell. Data Anal.},
  title        = {Multi-task intent recommendation based on dynamic and static intent integration and disentanglement},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imbalanced data prediction model based on self-attention mechanism and generative adversarial network. <em>IDA</em>, <em>29</em>(5), 1106--1121. (<a href='https://doi.org/10.1177_1088467X241301698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data distribution causes the traditional machine learning classification algorithms to be affected by the characteristics of the majority class, resulting in poor classification performance for the minority-class data. To improve the classification accuracy of minority classes in imbalanced data, this study has proposed a novel model—a generative adversarial network with self-attention mechanism oversampling based on a convolutional neural network (GAN-SAMO-CNN). The self-attention mechanism (SAM) of this model focused on the correlations among data elements of the minority class. The degree of correlation was first obtained by calculating the attention scores, which enabled the effective extraction of the distribution characteristics of the data. Subsequently, a generative adversarial network (GAN) was used to generate samples with high similarity to reduce data imbalances. Finally, a CNN classification model was constructed to train and predict the samples. The experimental results showed that the F1-score , G-mean , and area under PRC curve ( AUPRC ) of the model were considerably better than those of the other imbalanced data classification methods. The proposed method was then validated using multiple independent test datasets to demonstrate the model's generalizability and robustness.},
  archive      = {J_IDA},
  author       = {Hui Li and Fengxin Zhang and Dechang Pi and Dongyan Ding},
  doi          = {10.1177_1088467X241301698},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1106--1121},
  shortjournal = {Intell. Data Anal.},
  title        = {Imbalanced data prediction model based on self-attention mechanism and generative adversarial network},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted bonferroni mean-based classifiers with factorization machines for binary classification. <em>IDA</em>, <em>29</em>(5), 1085--1105. (<a href='https://doi.org/10.1177_1088467X241301694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature interaction plays a very significant role in binary classification. In this paper, we study binary classification with weighted feature interactions by means of weighted Bonferroni mean (WBM) operators. After analyzing the characteristics of interactions in the (dependently weighted) Bonferroni mean (BM) operators, the quadratic-polynomial (POLY2)-based classifier, and the factorization-machine (FM)-based classifier, we introduce the WBM operators to capture the independent importance of weighted feature interactions among input arguments. These operators are a general generalization of the existing BM operators and the crossover parts of the POLY2-based and FM-based classifiers. Then, the WBM-based classifiers and their special cases, namely the WBM-based classifiers based on the factorization machines (abbreiated as FM _ WBM-based classifiers), are constructed to handle the (in)dependent importance of weighted feature interactions. Extensive experimental results on the synthetic datasets and four UCI machine-learning datasets verify the effectiveness of these proposed classifiers.},
  archive      = {J_IDA},
  author       = {Yong Qiao Zhou and Wei Yang and Zhen Ming Ma and Zeshui Xu},
  doi          = {10.1177_1088467X241301694},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1085--1105},
  shortjournal = {Intell. Data Anal.},
  title        = {Weighted bonferroni mean-based classifiers with factorization machines for binary classification},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial. <em>IDA</em>, <em>29</em>(5), 1083--1084. (<a href='https://doi.org/10.1177/1088467X251355035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  author       = {J.M. Peña},
  doi          = {10.1177/1088467X251355035},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1083--1084},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ABSOD: Attention based small object detector for outfalls inspection in aerial images. <em>IDA</em>, <em>29</em>(4), 1062--1080. (<a href='https://doi.org/10.1177/1088467X251348766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strengthening the inspection of outfalls into rivers and oceans can help monitor pollutant emissions to the natural environment. Unmanned aerial vehicle (UAV) with high spatial resolution imagery has become a more efficient method for outfall surveys. At present, outfalls retrieval from UAV images relies on visual interpretation by skilled experts. However, long periods of concentration on detecting outfalls in high-resolution images for an expert easily increase mental load and stress, resulting in missing and false detection. Therefore, we develop a deep learning model, called Attention Based Small Object Detector (ABSOD), to perform outfalls detection in aerial images. In this model, an adaptive spatial correlation pyramid attention (ASCPA) network is proposed to establish long-distance region-to-region relationships between the outfall and its surrounding information more effectively. This network is mainly composed of SPE (Spatial Pyramid Extractor) and SCFM (Spatial Correlation Fusion Module). The purpose of the SPE is to extract multi-scale spatial information on the feature map. The SCFM is used to perform spatial correlation feature recalibration to selectively emphasized informative features. Experimental results show that the proposed network outperforms the state-of-the-art small object detection model in detecting outfalls, and reaches 45.9%, 92.8%, 86.5% and 34.4% in the four metrics of Precision, Recall, AP 0.5 , and AP 0.5:0.95 , respectively. To show the superiority of the ASCPA network, we compared our results with other attention mechanisms, all of them show that the ASCPA network has a competitive performance for outfalls detection. Moreover, based on visualization analysis, the ASCPA network is able to pay more attention on true outfall objects with respect to other attention mechanisms. These promising results demonstrate that the deep learning algorithm can be a feasible solution to assist experts in detecting outfalls with UAV imagery. The model and code are available at https://github.com/ISCLab-Bistu/ASCPA-Attention .},
  archive      = {J_IDA},
  author       = {Zhenjia Li and Shengjun Liang and Mingxin Yu},
  doi          = {10.1177/1088467X251348766},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1062--1080},
  shortjournal = {Intell. Data Anal.},
  title        = {ABSOD: Attention based small object detector for outfalls inspection in aerial images},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shifted split-merge segmentation and fuzzy-guided generative adversarial network underwater object detection. <em>IDA</em>, <em>29</em>(4), 1037--1061. (<a href='https://doi.org/10.1177/1088467X241290657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marine object localization from UAVs is required for different areas, including marine research, environment monitoring and topographic surveys. Herein, we introduce a new undersea object detection method; synthesizing shifted split-merge segmentation and fuzzy-oriented generative adversarial networks. Split-merge segmentation is the region-based model, where this process is achieved via splitting and merging regions based on specified similarity measures. The previous split-merge segmentation algorithm was modified to employ a shifted window approach that is better at detecting undersea objects with changing shapes and sizes to address this issue. In addition to Guiding the segmentation quality of underwater object detection, a Fuzzy-Guided Generative Adversarial Network (FG-GAN) is proposed. The generator network aimed to produce artificially photographed images beneath the water, and the discriminator network was used to differentiate between real and fabricated pictures. The generator system is trained to use a fuzzy loss function with fuzzy membership functions to explain the level of uncertainty and vagueness in the underwater environment by controlling the behaviour of underwater entities. We positioned the above-described method and compared it to the traditionally used image partition and object detection approaches. The outcomes from these experiments indicate that our proposed method is more accurate than the existing approaches in segmenting the objects and identifying the objects accurately with 95% and has a reduced loss of 0.3. The proposed approach could be applied in a broad spectrum of underwater facilities such as marine hydrology, work with remote sensing equipment and underwater robotics.},
  archive      = {J_IDA},
  author       = {K Selva Sheela and S Vinoth Kumar and Saman M Almufti and R Lakshmana Kumar},
  doi          = {10.1177/1088467X241290657},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1037--1061},
  shortjournal = {Intell. Data Anal.},
  title        = {Shifted split-merge segmentation and fuzzy-guided generative adversarial network underwater object detection},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy efficient metaheuristic cluster-based routing protocol for underwater sensor networks. <em>IDA</em>, <em>29</em>(4), 1021--1036. (<a href='https://doi.org/10.1177/1088467X241301385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, underwater wireless sensor networks (UWSNs) have been employed in the marine environment to forecast landslides by determining the amount of water and soil conditions, such as soil salinity, wetness, and movement. It is used in a variety of applications, including data collecting, disaster prediction, resource inquiry, marine surveillance, and so on. Energy efficiency becomes a challenge in UWSN since the nodes function on inherent energy and it can be difficult to exchange the power supply. This study focuses on developing an energy-efficient metaheuristic cluster-based routing protocol for UWSN, known as the EEMCBR-UWSN approach. The primary goal of the EEMCBR-UWSN technology is to improve energy efficiency through clustering and routing procedures. EEMCBR-UWSN technique uses a two-stage process. Initially, the spotted hyena optimization algorithm-based clustering (SHOA-C) method was used to arrange nodes in UWSN and pick appropriate cluster heads. The SHOA-C approach creates a fitness function for selecting CHs based on energy usage. Furthermore, the tumbleweed optimization algorithm-based routing (TWOA-R) approach was employed in the second stage to find the best routes in the UWSN. For route selection, the TWOA-R approach creates a fitness function based on energy, distance, and node degree. The simulation results were suggested that the EEMCBR-UWSN method outperforms traditional techniques by effectively enhancing the cluster head selection and routing processes.},
  archive      = {J_IDA},
  author       = {J Maheswari and J Senthil Kumar},
  doi          = {10.1177/1088467X241301385},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1021--1036},
  shortjournal = {Intell. Data Anal.},
  title        = {Energy efficient metaheuristic cluster-based routing protocol for underwater sensor networks},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault identification of rolling bearing based on improved salp swarm algorithm. <em>IDA</em>, <em>29</em>(4), 999--1020. (<a href='https://doi.org/10.3233/IDA-230994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid development of industrial manufacturing technology, modern mechanical equipment involves complex operating conditions and structural characteristics of hardware systems. Therefore, the state of components directly affects the stable operation of mechanical parts. To ensure engineering reliability improvement and economic benefits, bearing diagnosis has always been a concern in the field of mechanical engineering. Therefore, this article studies an effective machine learning method to extract useful fault feature information from actual bearing vibration signals and identify bearing faults. Firstly, variational mode decomposition decomposes the source signal into several intrinsic mode functions according to the actual situation. The vibration signal of the bearing is decomposed and reconstructed. By iteratively solving the variational model, the optimal modulus function can be obtained, which can better describe the characteristics of the original signal. Then, the feature subset is efficiently searched using the wrapper method of feature selection and the improved binary salp swarm algorithm (IBSSA) to effectively reduce redundant feature vectors, thereby accurately extracting fault feature frequency signals. Finally, support vector machines are used to classify and identify fault types, and the advantages of support vector machines are verified through extensive experiments, improving the ability of global search potential solutions. The experimental findings demonstrate the superior fault recognition performance of the IBSSA algorithm, with a highest recognition accuracy of 97.5%. By comparing different recognition methods, it is concluded that this method can accurately identify bearing failure.},
  archive      = {J_IDA},
  author       = {Hongwei Chen and Man Zhang and Fangrui Liu and Zexi Chen},
  doi          = {10.3233/IDA-230994},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {999--1020},
  shortjournal = {Intell. Data Anal.},
  title        = {Fault identification of rolling bearing based on improved salp swarm algorithm},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VMD-LSTM-BMA: A hybrid model for enhancing fresh food sales forecasting and uncertainty estimation. <em>IDA</em>, <em>29</em>(4), 982--998. (<a href='https://doi.org/10.1177_1088467X241296754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate sales forecasting of fresh food is imperative for retailers. It facilitates maintaining optimal inventory levels, thereby enhancing customer satisfaction, boosting revenue, and minimizing waste. However, sales sequences of fresh food are subject to multiple compounded factors, exhibiting nonlinearity and non-stationarity, posing challenges for prediction. This paper proposes a novel multi-variable hybrid model, VMD-LSTM-BMA, based on variational mode decomposition (VMD), long short-term memory (LSTM) neural networks, and Bayesian model averaging (BMA), for daily fresh food sales forecasting. Utilizing the posterior distribution generated by BMA, we calculate prediction intervals at various confidence levels to quantify the uncertainty of the forecasting outcomes. Employing a daily banana sales dataset from a retail chain supermarket, we validate the predictive performance of the proposed hybrid model at different aggregation levels. The results demonstrate that our VMD-LSTM-BMA framework achieves superior point forecasting accuracy compared to other models. In most instances, the prediction intervals provided by VMD-LSTM-BMA exhibit a higher prediction interval coverage probability (PICP) and a narrower interval width. Our proposed hybrid model operates robustly and efficiently, capable of providing reliable guidance for retailers’ replenishment and ordering processes, thereby mitigating the risks of out-of-stock and excess inventory.},
  archive      = {J_IDA},
  author       = {Shangxue Luo and Zhenkun Zhou and Tao Ren},
  doi          = {10.1177_1088467X241296754},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {982--998},
  shortjournal = {Intell. Data Anal.},
  title        = {VMD-LSTM-BMA: A hybrid model for enhancing fresh food sales forecasting and uncertainty estimation},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecast combination with multivariate grey prediction for tourism demand forecasting. <em>IDA</em>, <em>29</em>(4), 969--981. (<a href='https://doi.org/10.3233/IDA-230565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical evidence has shown that forecast combination can improve the prediction accuracy of tourism demand forecasting. This paper aimed to develop a more accurate grey forecast combination method (GFCM) with multivariate grey prediction models In light of the practical applicability of grey prediction, which is not required to apply any statistical test to examine data series this research features the use of multivariate grey models through the genetic algorithm to synthesize forecasts from univariate grey prediction models commonly used in tourism forecasting into composite forecasts Empirical results showed that the proposed GFCM significantly outperformed the other combination methods considered. The results also suggested that the risk of forecast failures caused by selecting an inappropriate single model for tourism demand forecasting can be reduced by using the GFCM.},
  archive      = {J_IDA},
  author       = {Yi-Chung Hu and Geng Wu},
  doi          = {10.3233/IDA-230565},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {969--981},
  shortjournal = {Intell. Data Anal.},
  title        = {Forecast combination with multivariate grey prediction for tourism demand forecasting},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal fake news detection model using improved heuristic approach with optimal weighted integration and dilated adaptive deep learning. <em>IDA</em>, <em>29</em>(4), 944--968. (<a href='https://doi.org/10.1177/1088467X251313530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A lot of fake news is published with the help of television and social media. As a consequence, we are affected by the disinformation and misinformation spreading in our community. It is implemented to identify fake news creators to prevent spreading misinformation about the people. Thus, it is published like real news for the reputation and finances of an individual. It is challenging, because that is created by the integration of real and false details, and images are attached like originals to confuse the public. Few fake news detection tools are available to detect fake information. To solve this fake news spreading problem, an effectual multimodal fake news detection system is proposed based on the deep learning technique. In the beginning, the input image and text are gathered from benchmark data sources. Consequently, the deep features are extracted from raw images and text. The dilated Visual Geometry Group 16 (VGG16) is adopted to extract the image features and similarly, the text features are retrieved from the Dilated Text Convolutional Neural Network (DTCNN). After achieving two different features, it is upgraded into weighted fused features, in which the weight is tuned by an Adaptive Controlling Parameter-based Chameleon Swarm Algorithm (ACP-CSA). Finally, the fused features are fed as given to the Dilated Adaptive Deep Temporal Convolution Network with Bi-directional Long Short-Term Memory (DADTCN-Bi-LSTM) for predicting the fake news. The analysis is further performed by tuning the parameters in the model using developed ACP-CSA. The efficiency of the model is investigated and results are conducted. Thus, the analysis of the suggested system shows 95 regarding accuracy, sensitivity, and specificity. The analysis of the F1-score achieves the value of 90 in the developed model. Hence, the findings demonstrate that it achieves a better detection process to evade the existence of misinformation.},
  archive      = {J_IDA},
  author       = {Vikash Kishore and Mukesh Kumar},
  doi          = {10.1177/1088467X251313530},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {944--968},
  shortjournal = {Intell. Data Anal.},
  title        = {Multimodal fake news detection model using improved heuristic approach with optimal weighted integration and dilated adaptive deep learning},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Construction of motion phase classification algorithm using ankle joint physical signals. <em>IDA</em>, <em>29</em>(4), 927--943. (<a href='https://doi.org/10.1177/1088467X241296750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantification of human muscular activity is crucial, especially the activity of the lower limb muscles, which plays a significant role in motor control and rehabilitation. Devices such as electromyographs, motion capture systems, and smartwatches have been used to achieve this purpose. These methodologies have been reported to contribute to the understanding of motor control mechanisms, elucidation of movement disorders, and optimization of rehabilitation interventions and programs among others. However, they present several challenges. This study proposed the selector circuit type LstpR-HMM, a universal algorithm for estimating movement states using the physical signals obtained from a device we developed to measure vibrations and angles around the ankle joint. The algorithm optimizes specificity and sensitivity by transforming the logistic function, which can thus prevent from the convergence to erroneous states due to transient noise. Moreover, it possesses interpretable internal mechanisms. The efficacy of this method was demonstrated by evaluating it on subjects with various physical characteristics who performed heel-raising exercises. The results thus highlighted the effectiveness of our proposed method.},
  archive      = {J_IDA},
  author       = {Tatsuhiko Matsumoto and Yutaka Kano},
  doi          = {10.1177/1088467X241296750},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {927--943},
  shortjournal = {Intell. Data Anal.},
  title        = {Construction of motion phase classification algorithm using ankle joint physical signals},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotional privacy-preserving of speech based on generative adversarial networks. <em>IDA</em>, <em>29</em>(4), 913--926. (<a href='https://doi.org/10.1177/1088467X241301384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consumer electronic devices with voice assistants are becoming increasingly popular in modern intelligent home. Nevertheless, directly uploading unprocessed speech data, which may contain sensitive attributes, to a cloud server poses a significant risk to user privacy. To address this privacy issue, this paper proposes a privacy-enhancing model to protect speech emotions based on generative adversarial networks (PSEGAN). The model aims to prevent the inference of emotional attributes while maintaining the accuracy and utility of speech features. PSEGAN benefits from three modules: (1) A pre-trained speaker matcher imposes generative constraints on the model during the training phase to ensure that the generated speech retains the essential information needed for speaker recognition. (2) Attribute adversarial networks can generate perturbed speech that transforms emotional attributes while preserving the utility of the speech. (3) Gated Recurrent Networks (GRN) can handle the long-short term dependencies of speech signals. PSEGAN model solves the problem of utility loss in traditional speech privacy preservation methods based on generative adversarial networks (GAN). Experimental results show that on the RAVDESS dataset, PSEGAN reduces emotion recognition accuracy by 80.7%, while speaker recognition accuracy only decreases by 1.1%. These findings demonstrate that PSEGAN effectively mitigates the leakage of emotional attributes while maintaining high utility.},
  archive      = {J_IDA},
  author       = {Jinsen Lin and Zhiqiang Yao and Biao Jin and Zheyu Chen},
  doi          = {10.1177/1088467X241301384},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {913--926},
  shortjournal = {Intell. Data Anal.},
  title        = {Emotional privacy-preserving of speech based on generative adversarial networks},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentiment analysis: A machine learning utilisation for analyzing the sentiments of facebook and twitter posts. <em>IDA</em>, <em>29</em>(4), 889--912. (<a href='https://doi.org/10.1177/1088467X241301389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article describes a novel sentiment analysis framework for social media platforms, based on a combination of five machine learning (ML) algorithms—Multinomial Naive Bayes, Random Forest Classifier, Gradient Boosting Classifier, K-Nearest Neighbors, and Decision Tree—and three deep learning (DL) algorithms—LSTM, MLP, and CNN. Using comprehensive datasets from Facebook and Twitter, the authors achieved remarkable results, with LSTM demonstrating superior performance, achieving an accuracy of 0.99, and excelling particularly with Facebook data. The authors illustrate the proposed method’s effectiveness through detailed performance metrics, comparing it against existing models. The proposed framework allows for improved accuracy by up to 20.9%, precision by 1.23%, recall by 11.11%, and F1-score by 3.61%. The new method’s effectiveness is confirmed by extensive evaluation on real-world datasets. These new research results enhance sentiment analysis and can be used for better public opinion understanding, business strategy formulation, and decision-making processes. The novelty and scientific contribution lie in integrating diverse algorithms to achieve higher accuracy and more reliable sentiment detection in social media contexts.},
  archive      = {J_IDA},
  author       = {Deema Mohammed Alsekait and Hana Fathi and Shimaa Abdallah Ibrahim and Ahmed Younes Shdefat and Ahmed Saleh Alattas and Diaa Salama AbdElminaam},
  doi          = {10.1177/1088467X241301389},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {889--912},
  shortjournal = {Intell. Data Anal.},
  title        = {Sentiment analysis: A machine learning utilisation for analyzing the sentiments of facebook and twitter posts},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WAGRank: A word ranking model based on word attention graph for keyphrase extraction. <em>IDA</em>, <em>29</em>(4), 866--888. (<a href='https://doi.org/10.1177/1088467X241296257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyphrase extraction is an essential task of identifying representative words or phrases in document processing. Main traditional models rely on each word frequency feature in a document and its associated corpus. There are two major limitations of the word frequency method: first, it fails to fully exploit semantic information in the document, that is, it is a bag-of-word method; second, it tends to be influenced by local word frequency in the short current text when the linked corpus is not available or incomplete. This paper proposes WAGRank, a novel unsupervised ranking model on a word attention graph, where nodes are words and edges are semantic relations between words. To assign edge weights, two interpretable statistical methods of assessing correlation strength between words are designed using attention mechanism. WAGRank depends on word semantics rather than frequency only in the current text, using external knowledge stored in a pre-trained language model. WAGRank was evaluated on two publicly available datasets against twelve baselines, presenting its effectiveness and robustness. Besides, the Granger causality test illustrated that word attention has a statistically significant predictive effect on word frequency, providing a more reasonable explanation for word frequency analysis.},
  archive      = {J_IDA},
  author       = {Rong Bian and Bing Cheng},
  doi          = {10.1177/1088467X241296257},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {866--888},
  shortjournal = {Intell. Data Anal.},
  title        = {WAGRank: A word ranking model based on word attention graph for keyphrase extraction},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Session-based recommendation with quaternion-enhanced attention calculation. <em>IDA</em>, <em>29</em>(4), 850--865. (<a href='https://doi.org/10.1177_1088467X241301378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A sound session-based Recommendation system is an important thing for users and enterprises. Session data from anonymous users is used to infer subsequent behavior and make high-quality recommendations. Graph neural networks have been extensively used to represent and learn information about the graph structure of session data in many existing studies, which have achieved significant progress. However, certain limitations still exist. This part of the model mainly focuses on how to enhance the expressiveness of the model while ignoring the quality of embedding. And previous studies have shown that complex deep learning models do not always have an edge over relatively simple algorithms in terms of prediction accuracy. To complement this aspect, this paper proposes a model for session recommendation based on quaternion-enhanced attention computation. Distinct from Euclidean space, quaternions perform calculations and analogy deductions in a hyper-complex vector space. The Hamilton product employed by quaternions provides a highly significant and meaningful computational method for enhancing session representation and reducing model parameters. A quaternion weight calculation fusion mechanism was designed, which uses quaternion calculations in key calculation steps, such as calculating relevant learnable weights for item representation and session representation. Experimental results on the benchmark database show that QEAC-SR outperforms some of the existing state-of-the-art methods, with Precision scores improving over the best-performing baseline method by 1.70 % to 1.80 % on Diginetica and 21.51 % to 23.49 % on Tmall; MRR scores improve over the best-performing baseline method by 2.31 % to 2.36 % on Diginetica and 19.56 % to 19.65 % on Tmall.},
  archive      = {J_IDA},
  author       = {F Yuanbiao Guo and S Yishan Liu and T Canta Zheng and T Wenming Cao},
  doi          = {10.1177_1088467X241301378},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {850--865},
  shortjournal = {Intell. Data Anal.},
  title        = {Session-based recommendation with quaternion-enhanced attention calculation},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision variables to be discovered in modelling high-dimensional omics data for cancer studies*. <em>IDA</em>, <em>29</em>(4), 835--849. (<a href='https://doi.org/10.1177_1088467X241290621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional omics data are often contaminated by sources of unwanted variations caused by platforms, batches, or other external factors. These interferences and noise can obscure critical signals related to cancer. Contaminated data are modeled as a combination of variables derived from the phenotype of interest (POI) and confounding factors. To identify these variables, a novel method called Decision Variable Analysis (DVA) is proposed. The novelty of DVA is to iteratively extract independent decisive variables for modeling the data. Specifically, a priori knowledge introduced as the definite variable linked with POI is removed from data through a residual operation. The number of variables is estimated from the residual matrix based on the zero gradient of singular values, rather than relying on random matrix theory or principal components analysis, which can produce unreliable results when the number of features exceeds the number of samples. Applications of DVA to both synthetic and real data demonstrate superior performance in identifying variables compared to conventional approaches. Improvements offered by DVA are illustrated across high-dimensional omics datasets, particularly those with smaller sample sizes relative to the number of features on different platforms. The results indicate that DVA is an effective method for dissecting sources of variation in high-dimensional data with disturbances.},
  archive      = {J_IDA},
  author       = {Feng Xie and Cheng Li and Weike Lu and Zhen Yang and Hanling Zhang and Jie Xie},
  doi          = {10.1177_1088467X241290621},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {835--849},
  shortjournal = {Intell. Data Anal.},
  title        = {Decision variables to be discovered in modelling high-dimensional omics data for cancer studies*},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial. <em>IDA</em>, <em>29</em>(4), 833--834. (<a href='https://doi.org/10.1177/1088467X251337522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  author       = {JM Peña},
  doi          = {10.1177/1088467X251337522},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {833--834},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of solar and wind power plants production through a parallel fusion approach with modified hybrid machine and deep learning models. <em>IDA</em>, <em>29</em>(3), 808--830. (<a href='https://doi.org/10.1177/1088467X241312592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) is becoming increasingly indispensable across diverse domains as technology rapidly advances. As traditional energy sources dwindle, there's a noticeable pivot towards renewable energy sources (RES). However, to effectively meet energy demands, integrating these RES into smart grids to bolster efficiency is imperative. Despite the transition, ongoing technical challenges persist, specifically in accurately predicting and optimizing smart grid parameters. To tackle these hurdles and enhance smart grid efficiency, various AI techniques are being harnessed. This study leverages real-time energy generation data (MWh) from solar and wind plants over a year, dependent on parameters such as POA and wind speed, respectively. Prediction outcomes are derived using three machine learning (ML) models (XGBoost, CatBoost, and LightGBM) and three deep learning (DL) models (LSTM, BiLSTM, and GRU). From these individual models, two hybrid ML and DL models are developed, yielding promising results. Subsequently, these outcomes are further refined through a parallel fusion approach (PFA), resulting in heightened accuracy and reliability. The implementation of this technique notably reduces error rates to 15.05% for hybrid ML, 19.18% for hybrid DL, and 8.1432% for PFA. This methodology holds substantial potential for future research endeavors, supplementing existing AI models for enhanced efficiency.},
  archive      = {J_IDA},
  author       = {Muhammad Abubakar and Yanbo Che and Ahsan Zafar and Mahmoud Ahmad Al-Khasawneh and Muhammad Shoaib Bhutta},
  doi          = {10.1177/1088467X241312592},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {808--830},
  shortjournal = {Intell. Data Anal.},
  title        = {Optimization of solar and wind power plants production through a parallel fusion approach with modified hybrid machine and deep learning models},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OpTunedSMOTE: A novel model for automated hyperparameter tuning of SMOTE in software defect prediction. <em>IDA</em>, <em>29</em>(3), 787--807. (<a href='https://doi.org/10.1177/1088467X241301390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Defect Prediction plays a crucial role in quality assurance by identifying potential defects early in the software development lifecycle. It is an essential aspect of modern software engineering that significantly contributes to improving software quality and reliability. It utilizes a variety of techniques, including machine learning algorithms like decision trees, support vector machines, and neural networks, to predict defects. A lot of research tried to improve the prediction accuracy but had problems with imbalanced data and hyperparameter tuning of the algorithms. To deal with this, we proposed a novel approach by tuning the hyperparameters of the Synthetic Minority Over-sampling Technique using the Tree-structured Parzen Estimator algorithm within the Optuna framework. Through an analysis of seventeen imbalanced datasets from a different public database, we compare our technique with existing SDP models using K-Nearest Neighbors, Multi-Layer Perceptron, Random Forest, Support Vector Machine, and Extreme Gradient Boosting classifiers. Our findings reveal that optimizing the Synthetic Minority Over-sampling Technique significantly improves the performance of SDP models, resulting in enhanced performance metrics. We have statistically validated our results using Friedman's test.},
  archive      = {J_IDA},
  author       = {Ruchika Malhotra and Kishwar Khan},
  doi          = {10.1177/1088467X241301390},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {787--807},
  shortjournal = {Intell. Data Anal.},
  title        = {OpTunedSMOTE: A novel model for automated hyperparameter tuning of SMOTE in software defect prediction},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test scheduling of network-on-chip using hybrid WOA-GWO algorithm. <em>IDA</em>, <em>29</em>(3), 769--786. (<a href='https://doi.org/10.3233/IDA-240878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The promising Network-on-Chip (NoC) model replaces the existing system-on-chip (SoC) model for complex VLSI circuits. Testing the embedded cores using NoC incurs additional costs in these SoC models. NoC models consist of network interface controllers, Internet Protocol (IP) data centers, routers, and network connections. Technological advancements enable the production of more complex chips, but longer testing times pose a potential problem. NoC packet switching networks provide high-performance interconnection, a significant benefit for IP cores. A multi-objective approach is created by integrating the benefits of the Whale Optimization Algorithm (WOA) and Grey Wolf Optimization (GWO). In order to minimize the duration of testing, the approach implements optimization algorithms that are predicated on the behavior of grey wolves and whales. The P22810 and D695 benchmark circuits are under consideration. We compare the test time with existing optimization techniques. We assess the effectiveness of the suggested hybrid WOA-GWO algorithm using fourteen established benchmark functions and an NP-hard problem. This proposed method minimizes the time needed to test the P22810 benchmark circuit by 69%, 46%, 60%, 19%, and 21% compared to the Modified Ant Colony Optimization, Modified Artificial Bee Colony, WOA, and GWO algorithms. In the same vein, the proposed method reduces the testing time for the d695 benchmark circuit by 72%, 49%, 63%, 21%, and 25% in comparison to the same algorithms. We experimented to determine the time savings achieved by adhering to the suggested procedure throughout the testing process.},
  archive      = {J_IDA},
  author       = {Sadesh S and Gokul Chandrasekaran and Rajasekaran Thangaraj and Neelam Sanjeev Kumar},
  doi          = {10.3233/IDA-240878},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {769--786},
  shortjournal = {Intell. Data Anal.},
  title        = {Test scheduling of network-on-chip using hybrid WOA-GWO algorithm},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social media user geolocation based on geographically compact subgraphs. <em>IDA</em>, <em>29</em>(3), 748--768. (<a href='https://doi.org/10.1177/1088467X241301379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining geographic location of social media users is a crucial technology for realizing the mapping of cyberspace to geographical world, which can provide strong support for wide-ranging location-based services. As a typical approach, user geolocation methods based on relationships rely on the assumption of location homophily between users and their neighbors. However, these methods only utilize the geographic influence between pair-wise relationship, resulting in undesired geolocation performance. In this paper, a social media user geolocation method based on geographically compact social subgraphs (SMUG-GCS) is proposed. Firstly, we analyze the relationship pattern among users in geographic proximity, and find a phenomenon that users who are geographically close tend to have tightly social groups. Based on this finding, a subgraph partitioning algorithm is presented which integrates structure compactness and geographical credibility to identify a set of subgraphs, whose nodes are more tightly connected and geographically proximity. Finally, user locations are inferred using the propagation of user information only based on the geographically compact subgraph. Extensive experiments are conducted on three real-world social media datasets. The results show that, compared with 5 typical relationship-based methods, SMUG-GCS improves the geolocating accuracy while reducing storage costs, leading to a significant reduction in median error distance ranging from 26.7% to 82.9%, as well as decrease in storage requirements by up to 56.5%.},
  archive      = {J_IDA},
  author       = {Meng Zhang and Xiangyang Luo and Ningbo Huang and Ruixiang Li},
  doi          = {10.1177/1088467X241301379},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {748--768},
  shortjournal = {Intell. Data Anal.},
  title        = {Social media user geolocation based on geographically compact subgraphs},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trust enhanced POI recommendation with collaborative learning. <em>IDA</em>, <em>29</em>(3), 732--747. (<a href='https://doi.org/10.3233/IDA-230897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development and popularization of smart mobile devices, users tend to share their visited points-of-interest (POIs) on the network with attached location information, which forms a location-based social network (LBSN). LBSNs contain a wealth of valuable information, including the geographical coordinates of POIs and the social connections among users. Nowadays, lots of trust-enhanced approaches have fused the trust relationships of users together with other auxiliary information to provide more accurate recommendations. However, in the traditional trust-aware approaches, the embedding processes of the information on different graphs with different properties (e.g., user-user graph is an isomorphic graph, user-POI graph is a heterogeneous graph) are independent of each other and different embedding information is directly fused together without guidance, which limits their performance. More effective information fusion strategies are needed to improve the performance of trust-enhanced recommendation. To this end, we propose a T rust E nhanced POI recommendation approach with C ollaborative L earning (TECL) to merge geographic information and social influence. Our proposed model integrates two modules, a GAT-based graph autoencoder as trust relationships embedding module and a multi-layer deep neural network as a user-POI graph learning module. By applying collaborative learning strategy, these two modules can interact with each other. The trust embedding module can guide the selection of user’s potential features, and in turn the user-POI graph learning module enhances the embedding process of trust relationships. Different information is fused through the two-way interaction of information, instead of travelling in one direction. Extensive experiments are conducted using real-world datasets, and results illustrate that our suggested approach outperforms state-of-the-art methods.},
  archive      = {J_IDA},
  author       = {Renhui Wu and Hui Xu and Xiaobin Rui and Zhixiao Wang},
  doi          = {10.3233/IDA-230897},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {732--747},
  shortjournal = {Intell. Data Anal.},
  title        = {Trust enhanced POI recommendation with collaborative learning},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced network for brain MR image denoising. <em>IDA</em>, <em>29</em>(3), 720--731. (<a href='https://doi.org/10.3233/IDA-240613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance Imaging (MRI) is a cornerstone of modern medical diagnosis due to its ability to visualize intricate soft tissues without ionizing radiation. However, noise artifacts significantly degrade image quality, hindering accurate diagnosis. Traditional denoising methods struggle to preserve details while effectively reducing noise. While deep learning approaches show promise, they often focus on local information, neglecting long-range dependencies. To address these limitations, this study proposes the deep and shallow feature fusion denoising network (DAS-FFDNet) for MRI denoising. DAS-FFDNet combines shallow and deep feature extraction with a tailored fusion module, effectively capturing both local and global image information. This approach surpasses existing methods in preserving details and reducing noise, as demonstrated on publicly available T1-weighted and T2-weighted brain image datasets. The proposed model offers a valuable tool for enhancing MRI image quality and subsequent analyses.},
  archive      = {J_IDA},
  author       = {Qian Wang and Tie-Qiang Li and Haicheng Sun and Hao Yang and Xia Li},
  doi          = {10.3233/IDA-240613},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {720--731},
  shortjournal = {Intell. Data Anal.},
  title        = {An enhanced network for brain MR image denoising},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of intelligent data analysis: Machine learning approaches for addressing class imbalance in healthcare - Challenges and perspectives. <em>IDA</em>, <em>29</em>(3), 699--719. (<a href='https://doi.org/10.1177/1088467X241305509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent data analysis rapidly transforms healthcare care by improving patient care and predicting health outcomes through machine learning (ML) techniques. These advanced analytical methods allow intelligent healthcare systems to process large amounts of health data, improving diagnosis, treatment, and patient monitoring. The success of these systems is highly dependent on the quality and balance of the data they analyze. Class imbalance, a situation where certain classes dominate the dataset, can significantly affect the accuracy and effectiveness of ML models. In healthcare, it is not only crucial, but urgent, to accurately represent all conditions, including rare diseases, to ensure proper diagnosis and treatment. For this analysis, data was gathered from six reputable academic databases: ScienceDirect, IEEE Xplore, Scopus, Web of Science, Google Scholar, and PubMed. This review offers a comprehensive overview of current approaches to handling class imbalance, including data preprocessing methods like oversampling, undersampling, hybrid techniques, and ensemble learning strategies such as bagging, boosting, and AdaBoost. It also addresses the limitations of these methods and the ongoing challenges in effectively managing class imbalance in healthcare data. Furthermore, the review explores innovative and promising strategies that have shown success in overcoming class imbalance, with a particular emphasis on fairness, diversity, and ethical considerations, offering a hopeful outlook for the future of healthcare data analysis. The discussion highlights how class imbalance can impact the accuracy and reliability of intelligent healthcare systems, underscoring its significance in improving patient care, healthcare delivery, and the broader medical community.},
  archive      = {J_IDA},
  author       = {Bashar Hamad Aubaidan and Rabiah Abdul Kadir and Mohamed Taha Lajb and Muhammad Anwar and Kashif Naseer Qureshi and Bakr Ahmed Taha and Kayhan Ghafoor},
  doi          = {10.1177/1088467X241305509},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {699--719},
  shortjournal = {Intell. Data Anal.},
  title        = {A review of intelligent data analysis: Machine learning approaches for addressing class imbalance in healthcare - Challenges and perspectives},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient algorithm for high utility sequential pattern mining over data streams based on sliding window model. <em>IDA</em>, <em>29</em>(3), 673--698. (<a href='https://doi.org/10.1177/1088467X241301387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of the digital economy, the exploration of useful knowledge from data streams has garnered significant attention due to its wide-ranging applications. However, the rapid and infinite nature of data streams poses challenges for efficiently mining high utility sequential patterns, including strong spatio-temporal constraints and the combinatorial explosion of sequence data search spaces. To address this and adapt to a variety of application scenarios, this paper delves into the investigation and design of an efficient algorithm for high utility sequential pattern mining over data streams based on the sliding window model (HUSP_DS). This algorithm utilizes a projection mechanism within a sliding window to recursively search for all interesting patterns. Additionally, it introduces a novel structure called the dynamic utility index table, which stores information such as the utility and index positions of data stream sequences. Notably, this structure proves highly effective in recursive search processes and utility updates. Comprehensive experimentation, conducted on both real-world and synthetic datasets, have shown that the superior performance of the HUSP_DS algorithm compared to state-of-the-art algorithms. This superiority is particularly evident in terms of temporal and spatial efficiency. Furthermore, the algorithm demonstrates suitability for mining sliding windows of arbitrary sizes, showcasing stable scalability.},
  archive      = {J_IDA},
  author       = {Meng Han and Ruihua Zhang and Feifei He and Fanxing Meng and Chunpeng Li},
  doi          = {10.1177/1088467X241301387},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {673--698},
  shortjournal = {Intell. Data Anal.},
  title        = {An efficient algorithm for high utility sequential pattern mining over data streams based on sliding window model},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale hierarchical model for long-term time series forecasting. <em>IDA</em>, <em>29</em>(3), 654--672. (<a href='https://doi.org/10.3233/IDA-240455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term time series forecasting (LTSF) has become an urgent requirement in many applications, such as wind power supply planning. This is a highly challenging task because it requires considering both the complex frequency-domain and time-domain information in long-term time series simultaneously. However, existing work only considers potential patterns in a single domain (e.g., time or frequency domain), whereas a large amount of time-frequency domain information exists in real-world LTSFs. In this paper, we propose a multi-scale hierarchical network (MHNet) based on time-frequency decomposition to solve the above problem. MHNet first introduces a multi-scale hierarchical representation, extracting and learning features of time series in the time domain, and gradually builds up a global understanding and representation of the time series at different time scales, enabling the model to process time series over lengthy periods of time with lower computational complexity. Then, the robustness to noise is enhanced by employing a transformer that leverages frequency-enhanced decomposition to model global dependencies and integrates attention mechanisms in the frequency domain. Meanwhile, forecasting accuracy is further improved by designing a periodic trend decomposition module for multiple decompositions to reduce input-output fluctuations. Experiments on five real benchmark datasets show that the forecasting accuracy and computational efficiency of MHNet outperform state-of-the-art methods.},
  archive      = {J_IDA},
  author       = {Jie Xu and Luo Jia Zhang and De Chun Zhao and Gen Lin Ji and Pei Heng Li},
  doi          = {10.3233/IDA-240455},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {654--672},
  shortjournal = {Intell. Data Anal.},
  title        = {Multi-scale hierarchical model for long-term time series forecasting},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two stages data mining analytics for food intentional and behavioral recommendations. <em>IDA</em>, <em>29</em>(3), 631--653. (<a href='https://doi.org/10.3233/IDA-240664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recommender system is an information filtering system used to predict a user’s rating or preference for an item. Dietary preferences are often influenced by various etiquettes and culture, such as appetite, the selection of ingredients, menu development, cooking methods, choice of tableware, seating arrangement of diners, order of eating, etc. Food delivery service is a courier service in that delivers food to customers by restaurants, stores, or independent delivery companies. With the continuous advances in information systems and data science, recommender systems are gradually developing towards to intentional and behavioral recommendations. Behavioral recommendation is an extension of peer-to-peer recommendation, where merchants find the people who want to buy the product and deliver it. Intentional recommendation is a mindset that seeks to understand the life of consumers; by continuously collecting information about their actions on the internet and displaying events and information that match the life and purchase preferences of consumers. This study considers that data targeting is a method by which food delivery service platforms can understand consumers’ dietary preferences and individual lifestyles so that the food delivery service platform can effectively recommend food to the consumer. Thus, this study implements two stages data mining analytics, including clustering analysis and association rules, to investigate Taiwanese food consumers ( n = 2,138) to investigate dietary and food delivery services behaviors and preferences to find knowledge profiles/patterns/rules for food intentional and behavioral recommendations. Finally, discussion and implications are presented.},
  archive      = {J_IDA},
  author       = {Shu-Hsien Liao and Retno Widowati and Shu-Ting Liao},
  doi          = {10.3233/IDA-240664},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {631--653},
  shortjournal = {Intell. Data Anal.},
  title        = {Two stages data mining analytics for food intentional and behavioral recommendations},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TLSTSRec: Time-aware long short-term attention neural network for sequential recommendation. <em>IDA</em>, <em>29</em>(3), 613--630. (<a href='https://doi.org/10.3233/IDA-240051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, sequential recommendation has received widespread attention for its role in enhancing user experience and driving personalized content recommendations. However, it also encounters challenges, including the limitations of modeling information and the variability of user preferences. A novel time-aware Long-Short Term Transformer (TLSTSRec) for sequential recommendation is introduced in this paper to address these challenges. TLSTSRec has two major innovative features. (1) Accurate modeling of users is achieved by fully leveraging temporal information. Time information is modeled by creating a trainable timestamp matrix from both the perspectives of time duration and time spectrum. (2) A novel time-aware Transformer model is proposed. To address the inherent variability of user preferences over time, the model combines long-term and short-term temporal information and adjusts the personalized trade-offs between long-term and short-term sequences using adaptive fusion layers. Subsequently, newly designed encoders and decoders are employed to model timestamps and interaction items. Finally, extensive experiments substantiate the effectiveness of TLSTSRec relative to various state-of-the-art sequential recommendation models based on MC/RNN/GNN/SA across a spectrum of widely used metrics. Furthermore, experiments are conducted to validate the rationality of the TLSTSRec structure.},
  archive      = {J_IDA},
  author       = {Hongwei Chen and Luanxuan Liu and Zexi Chen},
  doi          = {10.3233/IDA-240051},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {613--630},
  shortjournal = {Intell. Data Anal.},
  title        = {TLSTSRec: Time-aware long short-term attention neural network for sequential recommendation},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable paper classification system using topic modeling and SHAP. <em>IDA</em>, <em>29</em>(3), 590--612. (<a href='https://doi.org/10.3233/IDA-240075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of academic papers necessitates sophisticated classification systems to effectively manage and navigate vast information repositories. Despite the proliferation of such systems, traditional approaches often rely on embeddings that do not allow for easy interpretation of classification decisions, creating a gap in transparency and understanding. To address these challenges, we propose an innovative explainable paper classification system that combines Latent Semantic Analysis (LSA) for topic modeling with explainable artificial intelligence (XAI) techniques. Our objective is to identify which topics significantly influence the classification outcomes, incorporating Shapley additive explanations (SHAP) as a key XAI technique. Our system extracts topic assignments and word assignments from paper abstracts using LSA topic modeling. Topic assignments are then employed as embeddings in a multilayer perceptron (MLP) classification model, with the word assignments further utilized alongside SHAP for interpreting the classification results at the corpus, document, and word levels, enhancing interpretability and providing a clear rationale for each classification decision. We applied our model to a dataset from the Web of Science, specifically focusing on the field of nanomaterials. Our model demonstrates superior classification performance compared to several baseline models. Ultimately, our proposed model offers a significant advancement in both the performance and explainability of the system, validated by case studies that illustrate its effectiveness in real-world applications.},
  archive      = {J_IDA},
  author       = {Nakyung Shin and Yulhee Lee and Heesung Moon and Joonhui Kim and Hohyun Jung},
  doi          = {10.3233/IDA-240075},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {590--612},
  shortjournal = {Intell. Data Anal.},
  title        = {Explainable paper classification system using topic modeling and SHAP},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on weight initialization of CNN student models based on knowledge distillation. <em>IDA</em>, <em>29</em>(3), 566--589. (<a href='https://doi.org/10.1177/1088467X241301381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a process of weight compression where a complex teacher model trains a simplified student model, making the weights and their distribution crucial. This paper investigates the weight distribution in convolutional and fully connected layers of both teacher and student models. For convolutional layers, it was discovered that both teacher and student models exhibit a piecewise power-law distribution. A verification method based on the piecewise power law distribution of convolutional layers was proposed, and the correctness of this law was confirmed. Detailed analysis of the breakpoints and power exponents reveals that the teacher model has smaller breakpoints than the student model; for weights smaller than the breakpoint, the teacher model’s power exponent is lower than that of the student model, whereas, for weights larger than the breakpoint, the teacher model’s power exponent is higher. Based on these findings, a new weight initialization algorithm for convolutional layers was proposed. For fully connected layers, both models demonstrate a skewed distribution. A verification method based on the skewed distribution of fully connected layers was proposed, and the correctness of this law was confirmed. Analysis of the kurtosis and skewness indicates that the student model exhibits higher kurtosis and skewness than the teacher model. Based on these observations, a new weight initialization algorithm for fully connected layers was proposed. Experimental results show that both initialization methods improve the initial and final accuracy of the student model compared to the He initialization method.},
  archive      = {J_IDA},
  author       = {Chengzhi Fei and Zhenghong Zhong and Kaiwen Jiang and Changheng Shao and Yi Sui and Hanning Liu and Rencheng Sun},
  doi          = {10.1177/1088467X241301381},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {566--589},
  shortjournal = {Intell. Data Anal.},
  title        = {Research on weight initialization of CNN student models based on knowledge distillation},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial. <em>IDA</em>, <em>29</em>(3), 563--565. (<a href='https://doi.org/10.1177/1088467X251326433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.1177/1088467X251326433},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {563--565},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pharmachain IoT with internal attack classification framework using PBFT-MI-RIB-RBF technique in healthcare. <em>IDA</em>, <em>29</em>(2), 539--559. (<a href='https://doi.org/10.3233/IDA-240087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pharmaceutical supply chain, which ensures that drugs are accessible to patients in a trusted process, is a complex arrangement in the healthcare industry. For that, a secure pharmachain framework is proposed. Primarily, the users register their details. Then, the details are converted into cipher text and stored in the blockchain. If a user requests an order, the manufacturer receives the request, and the order is handed to the distributor. Labeling is performed through Hypergeometric Distribution Centroid Selection K-Medoids Clustering (HDCS-KMC) to track the drugs. The healthcare Pharmachain architecture uses IoT to control the supply chain and provide safe medication tracking. The framework includes security with a classifier and block mining consensus method, boosts performance with a decision controller, and protects user and medication information with encryption mechanisms. After that, the drugs are assigned to vehicles, where the vehicle ID and Internet of Things (IoT) sensor data are collected and pre-processed. Afterward, the pre-processed data is analyzed in the fog node by utilizing a decision controller. Now, the status ID is generated based on vehicle id and location. The generated status ID is meant for fragmentation, encryption, and block mining processes. If a user requests to view the drug’s status ID, then the user needs to get authentication. The user’s forking behavior and request activities were extracted and given to the classifier present in the block-mining consensus algorithm for authentication purposes. Block mining happens after authentication, thereby providing the status ID. Furthermore, the framework demonstrates an efficaciousness in identifying assaults with a low False Positive Rate (FPR) of 0.022483% and a low False Negative Rate (FNR) of 1.996008%. Additionally, compared to traditional methods, the suggested strategy exhibits good precision (97.869%), recall (97.0039%), accuracy (98%), and F-measure (97.999%).},
  archive      = {J_IDA},
  author       = {M Anbarasan and K Ramesh},
  doi          = {10.3233/IDA-240087},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {539--559},
  shortjournal = {Intell. Data Anal.},
  title        = {A pharmachain IoT with internal attack classification framework using PBFT-MI-RIB-RBF technique in healthcare},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bone feature quantization and systematized attention gate UNet-based deep learning framework for bone fracture classification. <em>IDA</em>, <em>29</em>(2), 513--538. (<a href='https://doi.org/10.3233/IDA-240431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bones collaborate with muscles and joints to sustain and maintain our freedom of mobility. The proper musculoskeletal activity of bone protects and strengthens the brain, heart, and lung function. When a bone is subjected to a force greater than its structural capacity, it fractures. Bone fractures should be detected with the appropriate type and should be treated early to avoid acute neurovascular complications. The manual detection of bone fracture may lead to highly delayed complications like malunion, Joint stiffness, Contractures, Myositis ossificans, and Avascular necrosis. A proper classification system must be integrated with deep learning technology to classify bone fractures accurately. This motivates me to propose a Systematized Attention Gate UNet (SAG-UNet) that classifies the type of bone fracture with high accuracy. The main contribution of this research is two-fold. The first contribution focuses on dataset preprocessing through feature extraction using unsupervised learning by adapting the Growing Neural Gas (GNG) method. The second contribution deals with refining the supervised learning Attention UNet model that classifies the ten types of bone fracture. The attention gate of the Attention UNet model is refined and applied to the upsampling decoding layer of Attention UNet. The KAGGLE Bone Break Classification dataset was processed to extract only the essential features using GNG extraction. The quantized significant feature RGB X-ray image was divided into 900 training and 230 testing images in the ratio of 80:20. The training images are fitted with the existing CNN models like DenseNet, VGG, AlexNet, MobileNet, EfficientNet, Inception, Xception, UNet and Attention UNet to choose the best CNN model. Experiment results portray that Attention UNet offers the classification of bone fractures with an accuracy of 89% when testing bone break images. Now, the Attention UNet was chosen to refine the Attention gate of the Decoding upsampling layer that occurs after the encoding layer. The Attention Gate of the proposed SAG-UNet forms the gating coefficient from the input feature map and gate signal. The gating coefficient is then processed with batch normalization that centers the aligned features in the active region, thereby leaving the focus on the unaligned weights of feature maps. Then, the ReLU activation function is applied to introduce the nonlinearity in the aligned features, thereby learning the complex representation in the feature vector. Then, dropout is used to exclude the error noise in the aligned weights of the feature map. Then, 1 × 1 linear convolution transformation was done to form the vector concatenation-based attention feature map. This vector has been applied to the sigmoid activation to create the attention coefficient feature map with weights assigned as ‘1’ for the aligned features. The attention coefficient feature map was grid resampled using trilinear interpolation to form the spatial attention weight map, which is passed to the skip connection of the next decoding layer. The implementation results reveal that the proposed SAG-UNet deep learning model classifies the bone fracture types with a high accuracy of 98.78% compared to the existing deep learning models.},
  archive      = {J_IDA},
  author       = {M Shyamala Devi and R Aruna and Saman Almufti and P Punitha and R Lakshmana Kumar},
  doi          = {10.3233/IDA-240431},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {513--538},
  shortjournal = {Intell. Data Anal.},
  title        = {Bone feature quantization and systematized attention gate UNet-based deep learning framework for bone fracture classification},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A medical assistant decision-making method based on interval belief rule base with explainability. <em>IDA</em>, <em>29</em>(2), 490--512. (<a href='https://doi.org/10.3233/IDA-230648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical assisted decision-making plays a key role in providing accurate and reliable medical advice. But in medical decision-making, various uncertainties are often accompanied. The belief rule base (BRB) has a strong nonlinear modeling capability and can handle uncertainties well. However, BRB suffers from combinatorial explosion and tends to influence explainability during the optimization process. Therefore, an interval belief rule base with explainability (IBRB-e) is explored in this paper. Firstly, pre-processing using extreme gradient boosting (XGBoost) is performed to filter out features with lower importance. Secondly, based on the filtered features, explainability criterion is defined. Thirdly, evidence reasoning (ER) rule is chosen as an inference tool, while projection covariance matrix adaptive evolutionary strategy (P-CMA-ES) algorithm with explainability constraints is chosen as an optimization algorithm. Lastly, the validation of the model is performed through a breast cancer case. The experimental results show that IBRB-e has good explainability while maintaining high accuracy.},
  archive      = {J_IDA},
  author       = {Lingkai Kong and Boying Zhao and Hongyu Li and Wei He and You Cao and Guohui Zhou},
  doi          = {10.3233/IDA-230648},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {490--512},
  shortjournal = {Intell. Data Anal.},
  title        = {A medical assistant decision-making method based on interval belief rule base with explainability},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework for classifying breast cancer via heterogenetic attention mechanism and optimized feature selection. <em>IDA</em>, <em>29</em>(2), 459--489. (<a href='https://doi.org/10.3233/IDA-240334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer poses a significant threat to women’s health, emphasizing the crucial role of timely detection. Traditional pathology reports, though widely used, face challenges prompting the development of automated Deep Learning (DL) tools. DL models, gaining traction in radiology, offer precise diagnoses; however, issues with generalization on varying dataset sizes persist. This paper introduces a computationally efficient DL framework, addressing dataset imbalance through a hybrid model design, ensuring both accuracy and speed in breast cancer image classification. Proposed model novel design excels in accuracy and generalization across medical imaging datasets, providing a robust tool for precise diagnostics. The proposed model integrates features from two classifiers, Inception ResNet V2 and Vision Transformers (ViT), to enhance the classification of breast cancer. This synergistic blend enhances adaptability, ensuring consistent performance across diverse dataset scales. A key contribution is the introduction of an Efficient Attention Mechanism within one of the classifiers, optimizing focus on critical features for improved accuracy and computational efficiency. Further, a Resource-Efficient Optimization model through feature selection is proposed, streamlining computational usage without compromising accuracy. Addressing the inherent heterogeneity within classifiers, our framework integrates high dimensional features comprehensively, leading to more accurate tumor class predictions. This consideration of heterogeneity marks a significant leap forward in precision for breast cancer diagnosis. An extensive analysis on datasets, BreakHis and BACH, that are imbalanced in nature is conducted by evaluating complexity, performance, and resource usage. Comprehensive evaluation using the datasets and standard performance metrics accuracy, precision, Recall, F1-score, MCC reveals the model’s high efficacy, achieving a testing accuracy of 0.9936 and 0.994, with precision, recall, F1-score and MCC scores of 0.9919, 0.987, 0.9898, 0.9852 and 0.989, 1.0, 0.993, 0.988 on the BreakHis and BACH datasets, respectively. Our proposed model outperforms state-of-the-art techniques, demonstrating superior accuracy across different datasets, with improvements ranging from 0.25% to 15% on the BACH dataset and from 0.36% to 15.02% on the BreakHis dataset. Our results position the framework as a promising solution for advancing breast cancer prediction in both clinical and research applications. The collective contributions, from framework and hybrid model design to feature selection and classifier heterogeneity consideration, establish a holistic and state-of-the-art approach, significantly improving accuracy and establishing optimization in breast cancer classification from MRI images. Future research for the DL framework in breast cancer image classification includes enhancing interpretability, integrating multi-modal data, and developing personalized treatments.},
  archive      = {J_IDA},
  author       = {AVS Swetha and Manju Bala and Kapil Sharma},
  doi          = {10.3233/IDA-240334},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {459--489},
  shortjournal = {Intell. Data Anal.},
  title        = {A framework for classifying breast cancer via heterogenetic attention mechanism and optimized feature selection},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrative framework for brain tumor segmentation and classification using neuraclassnet. <em>IDA</em>, <em>29</em>(2), 435--458. (<a href='https://doi.org/10.3233/IDA-240108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological developments in medical image processing have created a state-of-the-art framework for accurately identifying and classifying brain tumors. To improve the accuracy of brain tumor segmentation, this study introduced VisioFlow FusionNet, a robust neural network architecture that combines the best features of DeepVisioSeg and SegFlowNet. The proposed system uses deep learning to identify the cancer site from medical images and provides doctors with valuable information for diagnosis and treatment planning. This combination provides a synergistic effect that improves segmentation performance and addresses challenges encountered across various tumor shapes and sizes. In parallel, robust brain tumor classification is achieved using NeuraClassNet, a classification component optimized with a dedicated catfish optimizer. NeuraClassNet’s convergence and generalization capabilities are powered by the Cat Fish optimizer, which draws inspiration from the adaptive properties of aquatic predators. By complementing a comprehensive diagnostic pipeline, this classification module helps clinicians accurately classify brain tumors based on various morphological and histological features. The proposed framework outperforms current approaches regarding segmentation accuracy (99.2%) and loss (2%) without overfitting.},
  archive      = {J_IDA},
  author       = {CP ThamilSelvi and S Vinoth Kumar and Renas Rajab Asaad and Punitha Palanisamy and Lakshmana Kumar Rajappan},
  doi          = {10.3233/IDA-240108},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {435--458},
  shortjournal = {Intell. Data Anal.},
  title        = {An integrative framework for brain tumor segmentation and classification using neuraclassnet},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning and deep learning methods in brain tumor classification: A decade: Systematic literature review. <em>IDA</em>, <em>29</em>(2), 408--434. (<a href='https://doi.org/10.3233/IDA-240069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been extensive use of machine learning (ML) based tools for mathematical symbol and phrase categorization and prediction. Aiming to thoroughly analyze the existing methods for categorizing brain tumors, this paper considers both machine-learning and non-machine-learning approaches. From 2013 to 2023, the writers compiled and reviewed research papers on brain tumor detection. Wiley, IEEE-Explore, Science-Direct, Scopus, ACM-Digital Library, and others provide the relevant data. A systematic literature review examines the efficacy of research methodologies over the last ten years or more by compiling relevant publications and studies from various sources. Accuracy, sensitivity, specificity, and computing efficiency are some of the criteria that researchers use to evaluate these methods. The availability of labeled data, the required degree of automation and accuracy in the classification process, and the unique dataset are generally the deciding factors in the method choice. This work integrates previous research findings to summarize the current state of brain tumor categorization. This paper summarizes the 169 research papers in brain tumor detection between 2013–2023 and explores the application and development of machine learning methods in brain tumor detection, which has significant research implications and value in the field of brain tumor classification research. All research findings of previous studies are arranged in this paper in the form of research questions and answers format.},
  archive      = {J_IDA},
  author       = {Sachin Jain and Vishal Jain},
  doi          = {10.3233/IDA-240069},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {408--434},
  shortjournal = {Intell. Data Anal.},
  title        = {Machine learning and deep learning methods in brain tumor classification: A decade: Systematic literature review},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep hierarchical spectral-spatial feature fusion for hyperspectral image classification based on convolutional neural network. <em>IDA</em>, <em>29</em>(2), 385--407. (<a href='https://doi.org/10.3233/IDA-230927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint spectral-spatial feature extraction has been proven to be the most effective part of hyperspectral image (HSI) classification. But, due to the mixing of informative and noisy bands in HSI, joint spectral-spatial feature extraction using convolutional neural network (CNN) may lead to information loss and high computational cost. More specifically, joint spectral-spatial feature extraction from excessive bands may cause loss of spectral information due to the involvement of convolution operation on non-informative spectral bands. Therefore, we propose a simple yet effective deep learning model, named deep hierarchical spectral-spatial feature fusion (DHSSFF), where spectral-spatial features are exploited separately to reduce the information loss and fuse the deep features to learn the semantic information. It makes use of abundant spectral bands and few informative bands of HSI for spectral and spatial feature extraction, respectively. The spectral and spatial features are extracted through 1D CNN and 3D CNN, respectively. To validate the effectiveness of our model, the experiments have been performed on five well-known HSI datasets. Experimental results demonstrate that the proposed method outperforms other state-of-the-art methods and achieved 99.17%, 98.84%, 98.70%, 99.18%, and 99.24% overall accuracy on Kennedy Space Center, Botswana, Indian Pines, University of Pavia, and Salinas datasets, respectively.},
  archive      = {J_IDA},
  author       = {Somenath Bera and Naushad Varish and Syed irfan Yaqoob and Mudassir Rafi and Vimal K. Shrivastava},
  doi          = {10.3233/IDA-230927},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {385--407},
  shortjournal = {Intell. Data Anal.},
  title        = {Deep hierarchical spectral-spatial feature fusion for hyperspectral image classification based on convolutional neural network},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Missing data imputation using correlation coefficient and min-max normalization weighting. <em>IDA</em>, <em>29</em>(2), 372--384. (<a href='https://doi.org/10.3233/IDA-230140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data is one of the challenges a researcher encounters while attempting to draw information from data. The first step in solving this issue is to have the data stage ready for processing. Much effort has been made in this area; removing instances with missing data is a popular method for handling missing data, but it has drawbacks, including bias. It will be impacted negatively on the results. How missing values are handled depends on several vectors, including data types, missing rates, and missing mechanisms. It covers missing data patterns as well as missing at random, missing at completely random, and missing not at random. Other suggestions include using numerous imputation techniques divided into various categories, such as statistical and machine learning methods. One strategy to improve a model’s output is to weight the feature values to better the performance of classification or regression approaches. This research developed a new imputation technique called correlation coefficient min-max weighted imputation (CCMMWI). It combines the correlation coefficient and min-max normalization techniques to balance the feature values. The proposed technique seeks to increase the contribution of features by considering how those elements relate to the desired functionality. We evaluated several established techniques to assess the findings, including statistical techniques, mean and EM imputation, and machine learning imputation techniques, including k -NNI, and MICE. The evaluation also used the imputation techniques CBRL, CBRC, and ExtraImpute. We use various sizes of datasets, missing rates, and random patterns. To compare the imputed datasets and original data, we finally provide the findings and assess them using the root mean squared error (RMSE), mean absolute error (MAE), and R 2 . According to the findings, the proposed CCMMWI performs better than most other solutions in practically all missing-rate scenarios.},
  archive      = {J_IDA},
  author       = {Mohammed Shantal and Zalinda Othman and Azuraliza Abu Bakar},
  doi          = {10.3233/IDA-230140},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {372--384},
  shortjournal = {Intell. Data Anal.},
  title        = {Missing data imputation using correlation coefficient and min-max normalization weighting},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stable top-k periodic high-utility patterns mining over multi-sequence. <em>IDA</em>, <em>29</em>(2), 351--371. (<a href='https://doi.org/10.3233/IDA-230672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periodic high-utility sequential patterns (PHUSPs) mining is one of the research hotspots in data mining, which aims to discover patterns that not only have high utility but also regularly appear in sequence datasets. Traditional PHUSP mining mainly focuses on mining patterns from a single sequence, which often results in some interesting patterns being discarded due to strict constraints, and most of the discovered patterns are unstable and difficult to use for decision-making. In response to this issue, a novel algorithm called TKSPUS (top-k stable periodic high-utility sequential pattern mining) is proposed to discover stable top-k periodic high-utility sequential patterns that co-occur in multi-sequences. TKSPUS extends the traditional periodic high-utility sequential patterns mining, and designs two new metrics, namely utility stability coefficient (usc) and periodic stability coefficient (sr), to determine the periodic stability and utility stability of patterns in multi-sequences respectively. Additionally, the TKSPUS algorithm adopts the projection mechanism to mine stable periodic high-utility patterns over multi-sequence, while a new data structure called pusc and two corresponding pruning strategies are also introduced to boost the mining process. Experiments show that compared with the other four related algorithms, the TKSPUS algorithm has better performance in memory consumption and execution time, and the stability of the mining results is improved by 47% on average compared with the traditional periodic high-utility patterns mining algorithm.},
  archive      = {J_IDA},
  author       = {Ziqian Ren and Yaling Xun and Jianghui Cai and Haifeng Yang},
  doi          = {10.3233/IDA-230672},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {351--371},
  shortjournal = {Intell. Data Anal.},
  title        = {Stable top-k periodic high-utility patterns mining over multi-sequence},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on the artistic form and image expression effect of public sculpture based on intelligent and parametric design. <em>IDA</em>, <em>29</em>(2), 332--350. (<a href='https://doi.org/10.3233/IDA-240497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the modernization of cities, public sculptures are constantly being designed and constructed. The artistic form and image expression effect of sculpture based on intelligent and parametric design needs to be designed and developed to guide and assist the construction of sculpture. This paper applies the NAS architecture search method to explore the field of image expression effect models. Through the end-to-end search of the experiment designed in this paper, the separable convolution lightweight design is used, and the new model AestheticNet is used to predict the image form effect score distribution. Secondly, this paper proposes optimization strategies combining image expression effect theory and convolutional neural network, including improvement of Loss function self-weighted Loss, two-dimensional Attention mechanism – introduction of CBAM, and adaptive pooling layer. Optimization of several aspects, such as adaptive input. Finally, the validation set is compared with other existing image-morphological effect model algorithms, which proves the effectiveness of the customized search scheme. It demonstrates the efficacy of the AestheticNet model compared to other algorithms by validating its prediction of public sculpture image form effect ratings. The artistic form using intelligent and parametric design methodologies may improve. Image expression of sculptures may be enhanced by applying the image form effect model, which should be pervasive. We can use it to intelligently and parametrically guide the design and manufacture of sculptures.},
  archive      = {J_IDA},
  author       = {Wei Li},
  doi          = {10.3233/IDA-240497},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {332--350},
  shortjournal = {Intell. Data Anal.},
  title        = {Research on the artistic form and image expression effect of public sculpture based on intelligent and parametric design},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning-based neural style transfer optimization approach. <em>IDA</em>, <em>29</em>(2), 320--331. (<a href='https://doi.org/10.3233/IDA-230765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural style transfer is used as an optimization technique that combines two different images – a content image and a style reference image – to produce an output image that retains the appearance of the content image but has been modified to match the actual style of the style reference image. This is achieved by fine-tuning the output image to match the style reference images and the statistics for both content and style in the content image. These statistics are extracted from the images using a convolutional network. Primitive models such as WCT were improved upon by models such as PhotoWCT, whose spatial and temporal limitations were improved upon by Deep Photo Style Transfer. Eventually, wavelet transforms were introduced to perform photorealistic style transfer. A wavelet-corrected transfer based on whitening and colouring transforms, i.e., WCT 2 , was proposed that allowed the preservation of core content and eliminated the need for any post-processing steps and constraints. A model called Domain-Aware Universal Style Transfer also came into the picture. It supported both artistic and photorealistic style transfer. This study provides an overview of the neural style transfer technique. The recent advancements and improvements in the field, including the development of multi-scale and adaptive methods and the integration of semantic segmentation, are discussed and elaborated upon. Experiments have been conducted to determine the roles of encoder-decoder architecture and Haar wavelet functions. The optimum levels at which these can be leveraged for effective style transfer are ascertained. The study also highlights the contrast between VGG-16 and VGG-19 structures and analyzes various performance parameters to establish which works more efficiently for particular use cases. On comparing quantitative metrics across Gatys, AdaIN, and WCT, a gradual upgrade was seen across the models, as AdaIN was performing 99.92 percent better than the primitive Gatys model in terms of processing time. Over 1000 iterations, we found that VGG-16 and VGG-19 have comparable style loss metrics, but there is a difference of 73.1 percent in content loss. VGG-19, however, is displaying a better overall performance since it can keep both content and style losses at bay.},
  archive      = {J_IDA},
  author       = {Priyanshi Sethi and Rhythm Bhardwaj and Nonita Sharma and Deepak Kumar Sharma and Gautam Srivastava},
  doi          = {10.3233/IDA-230765},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {320--331},
  shortjournal = {Intell. Data Anal.},
  title        = {A deep learning-based neural style transfer optimization approach},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute graph clustering via transformer and graph attention autoencoder. <em>IDA</em>, <em>29</em>(2), 306--319. (<a href='https://doi.org/10.3233/IDA-230647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clustering is a crucial technique for partitioning graph data. Recent research has concentrated on integrating topology and attribute information from attribute graphs to generate node embeddings, which are subsequently clustered using classical algorithms. However, these methods have some limitations, such as insufficient information inheritance in shallow networks or inadequate quality of reconstructed nodes, leading to suboptimal clustering performance. To tackle these challenges, we introduce two normalization techniques within the graph attention autoencoder framework, coupled with an MSE loss, to facilitate node embedding learning. Furthermore, we integrate Transformers into the self-optimization module to refine node embeddings and clustering outcomes. Our model can induce appropriate node embeddings for graph clustering in a shallow network. Our experimental results demonstrate that our proposed approach outperforms the state-of-the-art in graph clustering over multiple benchmark datasets. In particular, we achieved 76.3% accuracy on the Pubmed dataset, an improvement of at least 7% compared to other methods.},
  archive      = {J_IDA},
  author       = {Wei Weng and Fengxia Hou and Shengchao Gong and Fen Chen and Dongsheng Lin},
  doi          = {10.3233/IDA-230647},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {306--319},
  shortjournal = {Intell. Data Anal.},
  title        = {Attribute graph clustering via transformer and graph attention autoencoder},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cd-MBRec: Enhancing multi-behavior recommendation by explicitly modeling commonality and diversity. <em>IDA</em>, <em>29</em>(2), 292--305. (<a href='https://doi.org/10.3233/IDA-240393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-behavior recommendation models excel in extracting abundant information from user-item interactions to enhance performance; however, they encounter challenges in accuracy due to noise disturbance and ambiguous weight allocation. In this paper, we propose cd-MBRec, a novel model designed to amplify commonality among various behaviors, thereby minimizing noise interference while preserving behavior diversity to highlight semantic variations in feedback across distinct scenarios. Specifically, the model begins by constructing behavior matrices that models separate behaviors, along with an interaction matrix offering a broad overview of user behaviors. It employs graph neural networks to extract higher-order semantic and structural information from input data. Concurrently, the model integrates principles of Weber-Fechner Law for the adaptive allocation of initial weights to the multiple behaviors and utilizes matrix factorization techniques for efficient behavior embedding. Extensive experiments on two real-world datasets demonstrate that cd-MBRec surpasses existing state-of-the-art models in recommendation performance, achieving notable average improvements of 4.96% in HR@10 and 7.75% in NDCG@10.},
  archive      = {J_IDA},
  author       = {Cairong Yan and Ziyang Zhu and Yiwei Zhang and Xiaopeng Guan and Yongquan Wan},
  doi          = {10.3233/IDA-240393},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {292--305},
  shortjournal = {Intell. Data Anal.},
  title        = {Cd-MBRec: Enhancing multi-behavior recommendation by explicitly modeling commonality and diversity},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViSSR: A visual analytics system for student high-order social relationships at campus. <em>IDA</em>, <em>29</em>(2), 271--291. (<a href='https://doi.org/10.3233/IDA-230263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social relationships among students at campus are closely related to their mental health and academic performance. Therefore, it is a very important task for educators to analyze students’ social relationships. However, existing studies have focused on one-to-one social relationships between students, few ones have explored the high-order community relationships hidden in social networks, especially in a visual manner. To solve this problem, a visual analysis system called ViSSR is proposed in this paper, which utilizes the Louvain algorithm to detect the hierarchical community structure of students’ social network at campus, and then provides four coordinated views to visualize the detection results. Among the views, the hierarchical hypergraph view is to visualize the hierarchical community structure that greatly breaks through the limitations of first-order relationships available in a traditional node-link social network, the community analysis view and individual analysis view show the social characteristics of a community and individual student respectively, and the matrix view displays the behavioral features of students. Case studies and experts evaluation have been conducted to demonstrate the usability of the system.},
  archive      = {J_IDA},
  author       = {Xiaoyong Li and Huimin Cheng and Sufang An and Yanjun Zhang and Yong Zhang},
  doi          = {10.3233/IDA-230263},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {271--291},
  shortjournal = {Intell. Data Anal.},
  title        = {ViSSR: A visual analytics system for student high-order social relationships at campus},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAGCN: A relation extraction model based on heterogeneous graph convolutional neural network and graph attention. <em>IDA</em>, <em>29</em>(2), 257--270. (<a href='https://doi.org/10.3233/IDA-240083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction is one of the core tasks of natural language processing, which aims to identify entities in unstructured text and judge the semantic relationships between them. In the traditional methods, the extraction of rich features and the judgment of complex semantic relations are inadequate. Therefore, in this paper, we propose a relation extraction model, HAGCN, based on heterogeneous graph convolutional neural network and graph attention mechanism. We have constructed two different types of nodes, words and relations, in a heterogeneous graph convolutional neural network, which are used to extract different semantic types and attributes and further extract contextual semantic representations. By incorporating the graph attention mechanism to distinguish the importance of different information, and the model has stronger representation ability. In addition, an information update mechanism is designed in the model. Relation extraction is performed after iteratively fusing the node semantic information to obtain a more comprehensive node representation. The experimental results show that the HAGCN model achieves good relation extraction performance, and its F1 value reaches 91.51% in the SemEval-2010 Task 8 dataset. In addition, the HAGCN model also has good results in the WebNLG dataset, verifying the generalization ability of the model.},
  archive      = {J_IDA},
  author       = {Xinyu He and Manfei Kan and Yonggong Ren},
  doi          = {10.3233/IDA-240083},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {257--270},
  shortjournal = {Intell. Data Anal.},
  title        = {HAGCN: A relation extraction model based on heterogeneous graph convolutional neural network and graph attention},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial. <em>IDA</em>, <em>29</em>(2), 255--256. (<a href='https://doi.org/10.1177/1088467X241311041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  author       = {Dr. J.M. Peña},
  doi          = {10.1177/1088467X241311041},
  journal      = {Intelligent Data Analysis},
  month        = {3},
  number       = {2},
  pages        = {255--256},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight unmanned aerial vehicle object detection algorithm based on improved YOLOv8. <em>IDA</em>, <em>29</em>(1), 235--252. (<a href='https://doi.org/10.3233/IDA-230929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of unmanned aerial vehicle (UAV) technology and computer vision, real-time object detection in UAV aerial images has become a current research hotspot. However, the detection tasks in UAV aerial images face challenges such as disparate object scales, numerous small objects, and mutual occlusion. To address these issues, this paper proposes the ASM-YOLO model, which enhances the original model by replacing the Neck part of YOLOv8 with an efficient bidirectional cross-scale connections and adaptive feature fusion (ABiFPN) . Additionally, a Structural Feature Enhancement Module (SFE) is introduced to inject features extracted by the backbone network into the Neck part, enhancing inter-network information exchange. Furthermore, the MPDIoU bounding box loss function is employed to replace the original CIoU bounding box loss function. A series of experiments was conducted on the VisDrone-DET dataset, and comparisons were made with the baseline network YOLOv8s. The experimental results demonstrate that the proposed model in this study achieved reductions of 26.1% and 24.7% in terms of parameter count and model size, respectively. Additionally, during testing on the evaluation set, the proposed model exhibited improvements of 7.4% and 4.6% in the AP50 and mAP metrics, respectively, compared to the YOLOv8s baseline model, thereby validating the practicality and effectiveness of the proposed model. Subsequently, the generalizability of the algorithm was validated on the DOTA and DIOR datasets, which share similarities with aerial images captured by drones. The experimental results indicate significant enhancements on both datasets.},
  archive      = {J_IDA},
  author       = {Zhaolin Zhao and Kaiming Bo and Chih-Yu Hsu and Lyuchao Liao},
  doi          = {10.3233/IDA-230929},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {235--252},
  shortjournal = {Intell. Data Anal.},
  title        = {Lightweight unmanned aerial vehicle object detection algorithm based on improved YOLOv8},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stacked collaborative transformer network with contrastive learning for video moment localization. <em>IDA</em>, <em>29</em>(1), 220--234. (<a href='https://doi.org/10.3233/IDA-240138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video grounding intends to perform temporal localization in multimedia information retrieval. The temporal bounds of the target video span are determined for the given input query. A novel interactive multi-head self-attention (IMSA) transformer is proposed to localize an unseen moment in the untrimmed video for the given image. A new semantic-trained self-supervised approach is considered in this paper to perform cross-domain learning to match the image query – video segment. It normalizes the convolution function enabling efficient correlation and collecting of semantically related video segments across time based on the image query. A double hostile Contrastive learning with Gaussian distribution parameters method is advanced to learn the representations of video. The proposed approach performs dynamically on various video components to achieve exact semantic synchronization and localization among queries and video. In the proposed approach, the IMSA model localizes frames greatly compared to other approaches. Experiments on benchmark datasets show that the proposed model can significantly increase temporal grounding accuracy. The moment occurrence is identified in the video with a start and end boundary ascertains an average recall of 86.45% and a mAP of 59.3%.},
  archive      = {J_IDA},
  author       = {G Megala and P Swarnalatha},
  doi          = {10.3233/IDA-240138},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {220--234},
  shortjournal = {Intell. Data Anal.},
  title        = {Stacked collaborative transformer network with contrastive learning for video moment localization},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secureimagesec: A privacy-preserving framework for outsourced picture representation with content-based image retrieval. <em>IDA</em>, <em>29</em>(1), 202--219. (<a href='https://doi.org/10.3233/IDA-240265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-Based Image Retrieval (CBIR) uses complicated algorithms to analyze visual attributes and retrieve relevant photos from large databases. CBIR is essential to a privacy-preserving feature extraction and protection method for outsourced picture representation. SecureImageSec combines essential methods with the system’s key entities to ensure secure, private and protected image feature processing during outsourcing. For a system to be implemented effectively, these techniques must be seamlessly integrated across critical entities, such as the client, the cloud server that is being outsourced, the component that protects secure features, the component that maintains privacy in communication, access control, and authorization, and the integration and system evaluation. The client entity initiates outsourcing using advanced encryption techniques to protect privacy. SecureImageSec protects outsourced data by using cutting-edge technologies like Fully Homomorphic Encryption (FHE) and Secure Multi-Party Computation (SMPC). Cloud servers hold secure feature protection entities and protect outsourced features’ privacy and security. SecureImageSec uses AES and FPE to protect data format. SecureImageSec’s cloud-outsourced privacy-preserving communication uses SSL/TLS and QKD to protect data transmission. Attribute-Based Encryption (ABE) and Functional Encryption (FE) in SecureImageSec limit access to outsourced features based on user attributes and allow fine-grained access control over decrypted data. SecureImageSec’s Information Leakage Rate (ILR) of 0.02 for a 1000-feature dataset shows its efficacy. SecureImageSec also achieves 4.5 bits of entropy, ensuring the encrypted feature set’s muscular cryptographic strength and randomness. Finally, SecureImageSec provides secure and private feature extraction and protection, including CBIR capabilities, for picture representation outsourcing.},
  archive      = {J_IDA},
  author       = {Vijay K and K Jayashree},
  doi          = {10.3233/IDA-240265},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {202--219},
  shortjournal = {Intell. Data Anal.},
  title        = {Secureimagesec: A privacy-preserving framework for outsourced picture representation with content-based image retrieval},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing 3D medical image registration with cross attention, residual skips, and cascade attention. <em>IDA</em>, <em>29</em>(1), 186--201. (<a href='https://doi.org/10.3233/IDA-230692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the core of Deep Learning-based Deformable Medical Image Registration (DMIR) lies a strong foundation. Essentially, this network compares features in two images to identify their mutual correspondence, which is necessary for precise image registration. In this paper, we use three novel techniques to increase the registration process and enhance the alignment accuracy between medical images. First, we propose cross attention over multi-layers of pairs of images, allowing us to take out the correspondences between them at different levels and improve registration accuracy. Second, we introduce a skip connection with residual blocks between the encoder and decoder, helping information flow and enhancing overall performance. Third, we propose the utilization of cascade attention with residual block skip connections, which enhances information flow and empowers feature representation. Experimental results on the OASIS data set and the LPBA40 data set show the effectiveness and superiority of our proposed mechanism. These novelties contribute to the enhancement of 3D DMIR-based on unsupervised learning with potential implications in clinical practice and research.},
  archive      = {J_IDA},
  author       = {Muhammad Anwar and Zhiquan He and Wenming Cao},
  doi          = {10.3233/IDA-230692},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {186--201},
  shortjournal = {Intell. Data Anal.},
  title        = {Enhancing 3D medical image registration with cross attention, residual skips, and cascade attention},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EFS-XGBoost: A robust framework for precision classification of COVID-19 cases. <em>IDA</em>, <em>29</em>(1), 171--185. (<a href='https://doi.org/10.3233/IDA-230854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of the novel COVID-19 virus has had a profound impact on global healthcare systems and economies, underscoring the imperative need for the development of precise and expeditious diagnostic tools. Machine learning techniques have emerged as a promising avenue for augmenting the capabilities of medical professionals in disease diagnosis and classification. In this research, the EFS-XGBoost classifier model, a robust approach for the classification of patients afflicted with COVID-19 is proposed. The key innovation in the proposed model lies in the Ensemble-based Feature Selection (EFS) strategy, which enables the judicious selection of relevant features from the expansive COVID-19 dataset. Subsequently, the power of the eXtreme Gradient Boosting (XGBoost) classifier to make precise distinctions among COVID-19-infected patients is harnessed.The EFS methodology amalgamates five distinctive feature selection techniques, encompassing correlation-based, chi-squared, information gain, symmetric uncertainty-based, and gain ratio approaches. To evaluate the effectiveness of the model, comprehensive experiments were conducted using a COVID-19 dataset procured from Kaggle, and the implementation was executed using Python programming. The performance of the proposed EFS-XGBoost model was gauged by employing well-established metrics that measure classification accuracy, including accuracy, precision, recall, and the F1-Score. Furthermore, an in-depth comparative analysis was conducted by considering the performance of the XGBoost classifier under various scenarios: employing all features within the dataset without any feature selection technique, and utilizing each feature selection technique in isolation. The meticulous evaluation reveals that the proposed EFS-XGBoost model excels in performance, achieving an astounding accuracy rate of 99.8%, surpassing the efficacy of other prevailing feature selection techniques. This research not only advances the field of COVID-19 patient classification but also underscores the potency of ensemble-based feature selection in conjunction with the XGBoost classifier as a formidable tool in the realm of medical diagnosis and classification.},
  archive      = {J_IDA},
  author       = {Mustufa Haider Abidi and Neelu Khare and Preethi D. and Hisham Alkhalefah and Usama Umer},
  doi          = {10.3233/IDA-230854},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {171--185},
  shortjournal = {Intell. Data Anal.},
  title        = {EFS-XGBoost: A robust framework for precision classification of COVID-19 cases},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning based car accident risk prediction for usage-based insurance. <em>IDA</em>, <em>29</em>(1), 156--170. (<a href='https://doi.org/10.3233/IDA-230971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Usage-Based Insurance paradigm, which is receiving a lot of attention in recent years, envisages computing the car policy premium based on accident risk probability, evaluated observing the past driving history and habits. However, Usage-Based Insurance strategies are usually based on simple empirical decision rules built on travelled distance. The development of intelligent systems for smart risk prediction using the stored overall driving behaviour, without the need of other insurance or socio-demographic information, is still an open challenge. This work aims at exploring a comprehensive machine learning-based approach solely based on driving-related data of private vehicles. The anonymized dataset employed in this study is provided by the telematics company UnipolTech, and contains space/time densely measured data related to trips of almost 100000 vehicles uniformly spread on the Italian territory, recorded every 2 km by on-board telematics fix devices (black boxes), from February 2018 to February 2020. An innovative feature engineering process is proposed, with the aim of uncovering novel informative quantities able to disclose complex aspects of driving behaviour. Recent and powerful learning techniques are explored to develop advanced predictive models, able to provide a reliable accident probability for each vehicle, automatically managing the critical imbalance intrinsically peculiar this kind of datasets.},
  archive      = {J_IDA},
  author       = {Silvia Strada and Emanuele Costantini and Simone Formentin and Sergio M. Savaresi},
  doi          = {10.3233/IDA-230971},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {156--170},
  shortjournal = {Intell. Data Anal.},
  title        = {Machine learning based car accident risk prediction for usage-based insurance},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal patterns assisted deep learning model for PM2.5 prediction (STEEP). <em>IDA</em>, <em>29</em>(1), 141--155. (<a href='https://doi.org/10.3233/IDA-240028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution is an alarming problem in many cities and countries around the globe. The ability to forecast air pollutant levels plays a crucial role in implementing necessary prevention measures to curb its effects in advance. There are many statistical, machine learning, and deep learning models available to predict air pollutant values, but only a limited number of models take into account the spatio-temporal factors that influence pollution. In this study a novel Deep Learning model that is augmented with Spatio-Temporal Co-Occurrence Patterns (STEEP) is proposed. The deep learning model uses the Closed Spatio-Temporal Co-Occurrence Pattern mining (C-STCOP) algorithm to extract non-redundant/closed patterns and the Diffusion Convolution Recurrent Neural Network (DCRNN) for time series prediction. By constructing a graph based on the co-occurrence patterns obtained from C-STCOP, the proposed model effectively addresses the spatio-temporal association among monitoring stations. Furthermore, the sequence-to-sequence encoder-decoder architecture captures the temporal dependencies within the time series data. The STEEP model is evaluated using the Delhi air pollutants dataset and shows an average improvement of 8%–13% in RMSE, MAE and MAPE metric compared to the baseline models.},
  archive      = {J_IDA},
  author       = {Sharmiladevi S and Siva Sathya S},
  doi          = {10.3233/IDA-240028},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {141--155},
  shortjournal = {Intell. Data Anal.},
  title        = {Spatio-temporal patterns assisted deep learning model for PM2.5 prediction (STEEP)},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining decomposition and graph capsule network for multi-objective vehicle routing optimization. <em>IDA</em>, <em>29</em>(1), 116--140. (<a href='https://doi.org/10.3233/IDA-230480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to alleviate urban congestion, improve vehicle mobility, and improve logistics delivery efficiency, this paper establishes a practical multi-objective and multi constraint logistics delivery mathematical model based on graphs, and proposes a solution algorithm framework that combines decomposition strategy and deep reinforcement learning (DRL). Firstly, taking into account the actual multiple constraints such as customer distribution, vehicle load constraints, and time windows in urban logistics distribution regions, a multi constraint and multi-objective urban logistics distribution mathematical model was established with the goal of minimizing the total length, cost, and maximum makespan of urban logistics distribution paths. Secondly, based on the decomposition strategy, a DRL framework for optimizing urban logistics delivery paths based on Graph Capsule Network (G-Caps Net) was designed. This framework takes the node information of VRP as input in the form of a 2D graph, modifies the graph attention capsule network by considering multi-layer features, edge information, and residual connections between layers in the graph structure, and replaces probability calculation with the module length of the capsule vector as output. Then, the baseline REINFORCE algorithm with rollout is used for network training, and a 2-opt local search strategy and sampling search strategy are used to improve the quality of the solution. Finally, the performance of the proposed method was evaluated on standard examples of problems of different scales. The experimental results showed that the constructed model and solution framework can improve logistics delivery efficiency. This method achieved the best comprehensive performance, surpassing the most advanced distress methods, and has great potential in practical engineering.},
  archive      = {J_IDA},
  author       = {Haifei Zhang and Hongwei Ge and Ting Li and Lujie Zhou and Shuzhi Su and Yubing Tong},
  doi          = {10.3233/IDA-230480},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {116--140},
  shortjournal = {Intell. Data Anal.},
  title        = {Combining decomposition and graph capsule network for multi-objective vehicle routing optimization},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised contrastive learning with decomposition-based data augmentation for time series classification. <em>IDA</em>, <em>29</em>(1), 94--115. (<a href='https://doi.org/10.3233/IDA-240002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While time series data are prevalent across diverse sectors, data labeling process still remains resource-intensive. This results in a scarcity of labeled data for deep learning, emphasizing the importance of semi-supervised learning techniques. Applying semi-supervised learning to time series data presents unique challenges due to its inherent temporal complexities. Efficient contrastive learning for time series requires specialized methods, particularly in the development of tailored data augmentation techniques. In this paper, we propose a single-step, semi-supervised contrastive learning framework named nearest neighbor contrastive learning for time series (NNCLR-TS). Specifically, the proposed framework incorporates a support set to store representations including their label information, enabling a pseudo-labeling of the unlabeled data based on nearby samples in the latent space. Moreover, our framework presents a novel data augmentation method, which selectively augments only the trend component of the data, effectively preserving their inherent periodic properties and facilitating effective training. For training, we introduce a novel contrastive loss that utilizes the nearest neighbors of augmented data for positive and negative representations. By employing our framework, we unlock the ability to attain high-quality embeddings and achieve remarkable performance in downstream classification tasks, tailored explicitly for time series. Experimental results demonstrate that our method outperforms the state-of-the-art approaches across various benchmarks, validating the effectiveness of our proposed method.},
  archive      = {J_IDA},
  author       = {Dokyun Kim and Sukhyun Cho and Heewoong Chae and Jonghun Park and Jaeseok Huh},
  doi          = {10.3233/IDA-240002},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {94--115},
  shortjournal = {Intell. Data Anal.},
  title        = {Semi-supervised contrastive learning with decomposition-based data augmentation for time series classification},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Products ranking through two-stage online customer reviews information. <em>IDA</em>, <em>29</em>(1), 74--93. (<a href='https://doi.org/10.3233/IDA-230865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of product ranking considering online reviews, they often are based on initial reviews and do not consider additional consumer reviews, but additional review information can sometimes directly affect consumers’ final decisions. To fully characterize the rich emotional preferences of consumers embedded in two-stage online customer reviews information, considering consumers’ individual preferences and product objective evaluation information, we construct a combination weighting method to calculate comprehensive weights of product attributes, and then exploit the sentiment analysis technique, interval-valued probabilistic linguistic term set (IVPLTS) and preference ranking organization method for enrichment evaluations (PROMETHEE) to establish a products ranking method based on compound reviews, and then we use it to identify the sentiment orientation of reviews and the results. Finally, a real-life case illustrates a real-world application of the proposed method.},
  archive      = {J_IDA},
  author       = {Shi-Tong Liu and Yong Liu and Jia-Ming Ding},
  doi          = {10.3233/IDA-230865},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {74--93},
  shortjournal = {Intell. Data Anal.},
  title        = {Products ranking through two-stage online customer reviews information},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-layer dynamic graph convolutional recurrent neural network for traffic flow prediction. <em>IDA</em>, <em>29</em>(1), 59--73. (<a href='https://doi.org/10.3233/IDA-230174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction can improve transportation efficiency, which is an important part of intelligent transportation systems. In recent years, the prediction method based on graph convolutional recurrent neural network has been widely used in traffic flow prediction. However, in real application scenarios, the spatial dependence of graph signals will change with time, and the filter using a fixed graph displacement operator cannot accurately predict traffic flow at the current moment. To improve the accuracy of traffic flow prediction, a two-layer graph convolutional recurrent neural network based on the dynamic graph displacement operator is proposed. The framework of our proposal is to use the first layer of static graph convolutional recurrent neural network to generate the sequence wave vector of the graph displacement operator. The sequence wave vector is passed through the deconvolutional neural network to obtain the sequence dynamic graph displacement operator, and then the second layer dynamic graph convolutional recurrent neural network is used to predict the traffic flow at the next moment. The model is evaluated on the METR-LA and PEMS-BAY datasets. Experimental results demonstrate that our model significantly outperforms other baseline models.},
  archive      = {J_IDA},
  author       = {Xiangyi Lu and Feng Tian and Yumeng Shen and Xuejun Zhang},
  doi          = {10.3233/IDA-230174},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {59--73},
  shortjournal = {Intell. Data Anal.},
  title        = {Two-layer dynamic graph convolutional recurrent neural network for traffic flow prediction},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LaplaceConfidence: A graph-based approach for learning with noisy labels. <em>IDA</em>, <em>29</em>(1), 45--58. (<a href='https://doi.org/10.3233/IDA-230818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world machine learning applications seldom provide perfect labeled data, posing a challenge in developing models robust to noisy labels. Recent methods prioritize noise filtering based on the discrepancies between model predictions and the provided noisy labels, assuming samples with minimal classification losses to be clean. In this work, we capitalize on the consistency between the learned model and the complete noisy dataset, employing the data’s rich representational and topological information. We introduce LaplaceConfidence, a method that to obtain label confidence (i.e., clean probabilities) utilizing the Laplacian energy. Specifically, it first constructs graphs based on the feature representations of all noisy samples and minimizes the Laplacian energy to produce a low-energy graph. Clean labels should fit well into the low-energy graph while noisy ones should not, allowing our method to determine data’s clean probabilities. Furthermore, LaplaceConfidence is embedded into a holistic method for robust training, where co-training technique generates unbiased label confidence and label refurbishment technique better utilizes it. We also explore the dimensionality reduction technique to accommodate our method on large-scale noisy datasets. Our experiments demonstrate that LaplaceConfidence outperforms state-of-the-art methods on benchmark datasets under both synthetic and real-world noise. Code available at https://github.com/chenmc1996/LaplaceConfidence .},
  archive      = {J_IDA},
  author       = {Mingcai Chen and Yuntao Du and Wei Tang and Baoming Zhang and Chongjun Wang},
  doi          = {10.3233/IDA-230818},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {45--58},
  shortjournal = {Intell. Data Anal.},
  title        = {LaplaceConfidence: A graph-based approach for learning with noisy labels},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cracking black-box models: Revealing hidden machine learning techniques behind their predictions. <em>IDA</em>, <em>29</em>(1), 28--44. (<a href='https://doi.org/10.3233/IDA-230707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quest for transparency in black-box models has gained significant momentum in recent years. In particular, discovering the underlying machine learning technique type (or model family) from the performance of a black-box model is a real important problem both for better understanding its behaviour and for developing strategies to attack it by exploiting the weaknesses intrinsic to the learning technique. In this paper, we tackle the challenging task of identifying which kind of machine learning model is behind the predictions when we interact with a black-box model. Our innovative method involves systematically querying a black-box model (oracle) to label an artificially generated dataset, which is then used to train different surrogate models using machine learning techniques from different families (each one trying to partially approximate the oracle’s behaviour). We present two approaches based on similarity measures, one selecting the most similar family and the other using a conveniently constructed meta-model. In both cases, we use both crisp and soft classifiers and their corresponding similarity metrics. By experimentally comparing all these methods, we gain valuable insights into the explanatory and predictive capabilities of our model family concept. This provides a deeper understanding of the black-box models and increases their transparency and interpretability, paving the way for more effective decision making.},
  archive      = {J_IDA},
  author       = {Raül Fabra-Boluda and Cèsar Ferri and José Hernández-Orallo and M. José Ramrez-Quintana and Fernando Martnez-Plumed},
  doi          = {10.3233/IDA-230707},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {28--44},
  shortjournal = {Intell. Data Anal.},
  title        = {Cracking black-box models: Revealing hidden machine learning techniques behind their predictions},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on learning based image reflection removal algorithms. <em>IDA</em>, <em>29</em>(1), 5--27. (<a href='https://doi.org/10.3233/IDA-230904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing images through semi-reflective surfaces, such as glass windows and transparent enclosures, often leads to a reduction in visual quality and can adversely affect the performance of computer vision algorithms. As a result, image reflection removal has garnered significant attention among computer vision researchers. With the growing application of deep learning methods in various computer vision tasks, such as super-resolution, inpainting, and denoising, convolutional neural networks (CNNs) have become an increasingly popular choice for image reflection removal. The purpose of this paper is to provide a comprehensive review of learning-based algorithms designed for image reflection removal. Firstly, we provide an overview of the key terminology and essential background concepts in this field. Next, we examine various datasets and data synthesis methods to assist researchers in selecting the most suitable options for their specific needs and targets. We then review existing methods with qualitative and quantitative results, highlighting their contributions and significance in this field. Finally, some considerations about challenges and future scope in image reflection removal techniques are discussed.},
  archive      = {J_IDA},
  author       = {Xin Wang and Yong Zhang and Junfeng Xu and Jun Gao},
  doi          = {10.3233/IDA-230904},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {5--27},
  shortjournal = {Intell. Data Anal.},
  title        = {A review on learning based image reflection removal algorithms},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial. <em>IDA</em>, <em>29</em>(1), 3--4. (<a href='https://doi.org/10.1177/1088467X241303248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.1177/1088467X241303248},
  journal      = {Intelligent Data Analysis},
  month        = {1},
  number       = {1},
  pages        = {3--4},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
