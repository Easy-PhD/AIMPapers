<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EAI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="eai">EAI - 36</h2>
<ul>
<li><details>
<summary>
(2025). Federated learning strategies for integrating composite meta-consistency loss with multi-head attention. <em>EAI</em>, <em>38</em>(4), 725-742. (<a href='https://doi.org/10.1177/30504554251340238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem of accuracy decline caused by feature redundancy, this paper designs a federated learning strategy that combines composite meta-consistency loss and multi-head attention. Firstly, this paper decorrelates the features based on the dual theory of constraints to eliminate redundant information, and improves the stability of the model through gradient-based regularization. Composite meta-consistency loss is constructed based on these two optimization methods. Experiments show that compared with the latest algorithms, the maximum accuracy of CIFAR-10 and Oxford-Pets in this paper is improved by 0.82% and 2.19%, respectively. After that, this paper introduces multi-head attention into the framework of federated learning. After capturing richer context information in the process of feature extraction, the combination of inner-layer update and outer-layer update of the meta-learning method enables the federated learning framework to effectively cope with the data distribution of different clients and finally accelerate the convergence speed. Compared with other algorithms, the average accuracy of the first 40 rounds in the MINIST, CIFAR-10 and CIFAR-100 data sets is higher. In CIFAR-10, SVHN, Oxford-Pets, taking Robust-HDP as the benchmark, the speedup ratio reaches 1.5, 1.42, and 1.34, respectively, which is faster than other algorithms.},
  archive      = {J_EAI},
  author       = {Afei Li and Xiaolei Yang and Li Ma and Lu Yu and Liyu Hao and Yongshan Liu},
  doi          = {10.1177/30504554251340238},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {725-742},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Federated learning strategies for integrating composite meta-consistency loss with multi-head attention},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASNet: Multi-attention sparse network for light detection and ranging (LIDAR)-based three-dimensional (3D) object detection. <em>EAI</em>, <em>38</em>(4), 707-724. (<a href='https://doi.org/10.1177/30504554251340241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that pillar-based detectors perform better in terms of both accuracy and speed, but these detectors perform poorly for detecting small objects such as pedestrians and cyclists. To solve this problem, we propose a highly efficient pillar-based model named MASNet, which mainly consists of a pillar mix attention (PMA) module, an attention-pooling operation, and a focal sparse network (FSN) module. The PMA module encodes the pillar features by fusion of the point-wise attention module and the channel-wise attention module. The attention-pooling operation aggregates the attention-encoded pillar features in a more comprehensive way to obtain the most expressive pillar features. In addition, the FSN module exploits the intrinsic sparsity of the data by introducing focal sparse convolution, which enriches the learned pillar features in the foreground without adding redundant pillars in other regions. On the KITTI three-dimensional (3D) Object Detection Benchmark, it achieves a 3D average precision of (77.81%, 60.30%, and 53.92%) in easy, moderate, and hard levels, which outperforms other pillar-based methods for the detection of cyclists. Additionally, our method is only 0.52% lower than the top-ranked method (pillar feature network (PIFENet)) on the KITTI Bird's Eye View pedestrian leaderboard, but our inference speed reaches 41 frames per second ahead of PIFENet by 57.69%.},
  archive      = {J_EAI},
  author       = {Fuqiang You and H Ziheng Zhang and Hao Chen},
  doi          = {10.1177/30504554251340241},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {707-724},
  shortjournal = {Eur. Artif. Intell.},
  title        = {MASNet: Multi-attention sparse network for light detection and ranging (LIDAR)-based three-dimensional (3D) object detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal coupling prompt learning for image classification tasks. <em>EAI</em>, <em>38</em>(4), 684-706. (<a href='https://doi.org/10.1177/30504554251335569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, vision-language pretraining (VLP) models have become a crucial driving force in the advancement of artificial intelligence. Besides, studies such as contrastive language-image pretraining (CLIP) have demonstrated that incorporating prompt learning within VLP models can significantly enhance the performance of downstream tasks. However, we believe that CLIP’s visual encoder suffers from feature extraction bias in image classification tasks, which is because of the uneven quantity and distribution of image features CLIP learned between the pretraining and fine-tuning stages. This can be further summarized as an inherent bias in feature extraction for differently distributed samples during the pretraining phase. To address the above problem this paper proposes (i) text-semantic hierarchical injection prompt learning method, which constructs self-attention layers and prompt mapping structures and injects text semantic features into the visual encoder layer by layer to generate visual prompt features and (ii) visual-semantic attention interactive prompt learning method, which further integrates text embeddings with the output features of the visual encoder through cross-attention and constructs instance-level text prompt features for each image. Based on the two above methods, this paper further proposes the multimodal coupling prompt learning CLIP (MCPL-CLIP) to enhance CLIP’s performance in image classification tasks. Experiments conducted on 15 image classification datasets demonstrate that MCPL-CLIP outperforms baseline models such as MaPLe, CoCoOp, and CoOp in cross-dataset transfer, domain generalization, and base-to-novel class generalization tasks, showcasing its superior text semantic representation and visual feature extraction capabilities.},
  archive      = {J_EAI},
  author       = {Yufei Liu and Hua Cheng and Yiquan Fang and Yiming Pan and Zehong Qian and Xiaoning Chen},
  doi          = {10.1177/30504554251335569},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {684-706},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Multimodal coupling prompt learning for image classification tasks},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KANDiff: Low-light image enhancement based on diffusion models. <em>EAI</em>, <em>38</em>(4), 666-683. (<a href='https://doi.org/10.1177/30504554251342571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement technique aims to improve the contrast and brightness of low-light images. Diffusion models, attributed to adeptness at capturing intricate details, have achieved good results in image enhancement, but there are problems such as inadequate estimation of noise characteristics and the emergence of color bias in the enhanced images. To address the aforementioned problems, this paper proposes a diffusion models-based method for low-light image enhancement, termed KANDiff. In the diffusion model architecture, this paper adds the nonlinear learnable activation function Kolmogorov–Arnold network to the noise estimation network U-Net to generate higher quality enhanced images. Additionally, KANDiff mitigates color bias in the enhanced images through a joint loss function and employs a patch-based image restoration strategy to significantly enhance the model generalization capability. The experimental results show that the KANDiff algorithm proposed in this paper can achieve high-quality image enhancement and achieve better enhancement effects compared to other algorithms.},
  archive      = {J_EAI},
  author       = {Yuanxin Ren and Minghui Yue and Yuxuan He and Liye Zhang},
  doi          = {10.1177/30504554251342571},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {666-683},
  shortjournal = {Eur. Artif. Intell.},
  title        = {KANDiff: Low-light image enhancement based on diffusion models},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-aspect graph representation feature integration for recommender dialogue system. <em>EAI</em>, <em>38</em>(4), 630-648. (<a href='https://doi.org/10.1177/30504554251347451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation-based dialogue systems aim to capture user preferences via interactive conversations for personalized recommendations. While existing studies focus on modeling user preferences, real-time dialog scenarios face challenges in balancing historical conversation contexts and immediate interests. This study proposes MGIRD, a multi-aspect graph representation approach integrating ordinary graphs and hypergraphs. We use graph structures to model users’ current interests and hypergraphs for historical conversation features, while incorporating historical behaviors in the recommendation module to balance context relevance. A novel item selection mechanism is introduced during dialog generation to naturally integrate recommended items. Experiments on Chinese TG-Redial and English Redial datasets show MGIRD outperforms most state-of-the-art methods in recommendation accuracy and dialog diversity, validating its effectiveness in enhancing recommendation quality and conversational fluency.},
  archive      = {J_EAI},
  author       = {Shi Li and Qing Yang Bai},
  doi          = {10.1177/30504554251347451},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {630-648},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Multi-aspect graph representation feature integration for recommender dialogue system},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic graph anomaly detection model combining dual behavior contrast. <em>EAI</em>, <em>38</em>(4), 617-629. (<a href='https://doi.org/10.1177/30504554251347752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current dynamic graph anomaly detection models learn multibehavior patterns for abnormal edges poorly and rely too much on the differences in long-term snapshots. Aiming at the above problems, combine dual behavior contrast dynamic graph anomaly detection model is proposed. Firstly, a dual behavior learning module is designed, where the role-based behavior learning submodule constructs graphlet degree vector by identifying four self-isomorphic orbits to capture deep structural features, while the attribute-based behavior learning submodule obtains attribute vectors through graph convolutional network. Then, the results are combined in the dynamic edge representation module to form the dynamic representations of edges to capture dual behavior patterns. Lastly, the anomaly detection module is designed to detect newly generated edges by combining contrastive learning with gated recurrent unit. We conduct experiments from four perspectives: anomaly detection accuracy, parameter sensitivity, robustness of module variants, and model runtime efficiency. The results demonstrate that the model achieves a peak accuracy of 92.05% in the task of dynamic edge anomaly detection.},
  archive      = {J_EAI},
  author       = {Jian Feng and Xiaotian Zhao},
  doi          = {10.1177/30504554251347752},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {617-629},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Dynamic graph anomaly detection model combining dual behavior contrast},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Action recognition based virtual panorama live broadcasting system. <em>EAI</em>, <em>38</em>(4), 601-616. (<a href='https://doi.org/10.1177/30504554251347439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the significant success of transformers in natural language processing, an increasing number of researchers are introducing them into the field of computer vision, particularly for action recognition. As a crucial task in video understanding, action recognition has significant applications in live broadcasting, autonomous driving, and medical diagnostics. The attention mechanisms in transformers mimicking human visual attention allocation, thereby enhancing the processing capabilities and comprehension of long video sequences. However, they often overlook the aggregation of multiscale detail features and the hierarchical representations of early visual information. Additionally, attention networks are computationally intensive and parameter-heavy, complicating model training and extending inference times, rendering them unsuitable for real-time applications. To crack these nuts, we propose a lightweight multiscale action recognition model based on convolutional enhancement block (ConvEB) and multiscale average pooling encoder. The ConvEB aims to establish long-range dependencies among multiscale local features in the early stages of the network, providing effective inductive biases for the attention network to compensate for the loss of detailed information. Moreover, we introduce a parallel pooling mixer to replace the original attention mixer, ensuring model lightweight while maintaining recognition accuracy. Finally, we deploy this model in the construction of a virtual panorama live broadcasting system. Experimental results demonstrate that our action recognition algorithm achieves competitive performance, and the constructed panoramic system basically meets the needs of daily live broadcasting.},
  archive      = {J_EAI},
  author       = {Lichuan Geng and Zihao Zhao and Qiaohong Hou},
  doi          = {10.1177/30504554251347439},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {601-616},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Action recognition based virtual panorama live broadcasting system},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BSNet: Boundary-location network based on deep multi-scale modulation for camouflaged object detection. <em>EAI</em>, <em>38</em>(4), 581-600. (<a href='https://doi.org/10.1177/30504554251328322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to identify objects seamlessly embedded in the surrounding environment. Due to the high inherent similarity between the texture of the camouflaged object and its complex background, making COD far more challenging than traditional target detection. To solve these problems, we propose a method that uses holistic boundary information to optimize COD through a two-stage strategy. Specifically, the feature enhancement module is initially implemented to refine features at different scales and emphasize boundary details of camouflaged entities. Then, our network employs a boundary localization module to guide low-level local edge features through high-level global semantic. Furthermore, the boundary-embedded feature aggregation module is introduced to achieve cross-level fusion of multi-scale features, by embedding and effectively activating boundary information, which reduces the interference from cluttered backgrounds. Extensive experiments on four benchmark datasets demonstrate that our proposed model outperforms the other 17 state-of-the-art COD methods. The source code and results of our method are available at https://github.com/WObaibai/BSNet .},
  archive      = {J_EAI},
  author       = {Yuhong Chen and Meng Dai and Qing Zhang and Jiayun Wu},
  doi          = {10.1177/30504554251328322},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {581-600},
  shortjournal = {Eur. Artif. Intell.},
  title        = {BSNet: Boundary-location network based on deep multi-scale modulation for camouflaged object detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ME2FNet: Muti-level edge-enhanced fusion network for camouflaged object detection. <em>EAI</em>, <em>38</em>(4), 530-542. (<a href='https://doi.org/10.1177/30504554251351219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is an emerging research direction in computer vision in recent years, aiming to segment objects that are visually integrated with the background, which is a valuable task and has attracted increasing interest from researchers. Since camouflaged objects are integrated with their surroundings, their boundaries are also very blurred, and it becomes an important issue in COD to segment the edges of the objects accurately and completely. To address the above issues, in this article, we propose a novel multi-level edge-enhanced fusion for camouflaged object detection network (ME 2 FNet). Specifically, we design a residual texture enhanced module to obtain more refined features from the noise-filled backbone features. Then, we design an edge extraction module (EEM), which aims to extract effective edge semantic information from low-level features and high-level features by a simple local channel attention mechanism. Finally, we design a boundary-guided fusion module, which aims to fuse the previously obtained prior information. It can fuse the edge information extracted by EEM with the features at different levels of the backbone network, and guide the learning under the supervision of ground truth. At the same time, it fuses the high-level global information with the features at different levels, so that the final predicted edge is clearer and the overall structure is more complete. Extensive experiments on three challenging benchmark datasets have shown that ME 2 FNet outperforms multiple leading-edge models in recent years and achieves advanced results under four widely used evaluation metrics.},
  archive      = {J_EAI},
  author       = {Xuwei Tong and Guangjian Zhang and Yuhao Yang},
  doi          = {10.1177/30504554251351219},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {530-542},
  shortjournal = {Eur. Artif. Intell.},
  title        = {ME2FNet: Muti-level edge-enhanced fusion network for camouflaged object detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging context-sensitivity for user-centered explainability. <em>EAI</em>, <em>38</em>(4), 496-529. (<a href='https://doi.org/10.1177/30504554251331568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread integration of artificial intelligence (AI) into our daily lives has spurred an escalating demand for explainable AI (XAI). This demand is particularly pronounced in critical domains such as healthcare and finance, where understanding the decision-making processes of AI models is paramount. Despite noteworthy strides in XAI, prevailing approaches often neglect the crucial dimension of context, resulting in explanations that are challenging to comprehend and act upon for different stakeholders. This paper advocates for a paradigm shift towards context-sensitive explainability, tailoring explanations to users’ specific needs and understanding promoting inclusivity and accessibility. We propose a novel context taxonomy and a versatile framework, “ConEX” for developing context-sensitive explanations using any state-of-the-art post hoc explainer. Our empirical user study highlights diverse preferences for contextualization levels, emphasizing the importance of catering to these preferences to build trust and satisfaction in AI systems. Our contributions extend beyond the theoretical realm, offering practical guidance for developing context-sensitive explanations that are tailored to the specific needs of diverse stakeholders. By embracing context-sensitive explainability, we can unlock the true potential of AI, fostering trust, transparency, and informed decision-making across various domains.},
  archive      = {J_EAI},
  author       = {Yasmeen Khaled and Nourhan Ehab},
  doi          = {10.1177/30504554251331568},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {496-529},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Leveraging context-sensitivity for user-centered explainability},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A conversational agent that learns to be aligned with the moral value of respect. <em>EAI</em>, <em>38</em>(4), 457-473. (<a href='https://doi.org/10.1177/30504554241311168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Videogame developers typically conduct user experience surveys to gather feedback from users once they have played. Nevertheless, as users may not recall all the details once finished, we propose an ethical conversational agent that respectfully conducts the survey during gameplay. To achieve this without hindering user’s engagement, we resort to reinforcement learning and an ethical embedding algorithm. Specifically, we transform the learning environment so that it guarantees that the agent learns to be respectful (i.e. aligned with the moral value of respect) while pursuing its individual objective of eliciting as much feedback information as possible. When applying this approach to a simple videogame, our comparative tests between the two agents (ethical and unethical) empirically demonstrate that endowing a survey-oriented conversational agent with this moral value of respect avoids disturbing user’s engagement while still pursuing its individual objective, which is to gather as much information as possible.},
  archive      = {J_EAI},
  author       = {Eric Roselló-Marín and Inmaculada Rodríguez and Maite Lopez-Sanchez and Manel Rodríguez-Soto and Juan Antonio Rodríguez-Aguilar},
  doi          = {10.1177/30504554241311168},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {457-473},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A conversational agent that learns to be aligned with the moral value of respect},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated data bias mitigation technique for algorithmic fairness. <em>EAI</em>, <em>38</em>(3), 436-453. (<a href='https://doi.org/10.1177/30504554251328351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning fairness enhancement methods based on data bias correction are usually divided into two processes: The determination of sensitive attributes (such as race and gender) and the correction of data bias. In terms of determining sensitive attributes, existing studies tend to rely too heavily on sociological knowledge and neglect the importance of exploring potential sensitive attributes directly from the data itself. The accuracy of this approach is limited when dealing with data that cannot be fully explained by sociological factors. Regarding data bias correction, existing methods are primarily categorized into causality-based and association-based methods. The former requires a deep understanding of the underlying causal structure in the dataset, which is often difficult to achieve in practice. The latter method correlates sensitive attributes with algorithmic results through statistical measures, but this approach often tends to ignore the impact of sensitive attributes on other attributes. In this paper, we formalize the identification of sensitive attributes as a problem solvable through data analysis, without relying on commonly recognized knowledge in social science. We also propose a data pre-processing method that considers the effects of attributes correlated with sensitive attributes to enhance algorithmic fairness by combining the association-based bias reduction method. We evaluated our proposed method on a public dataset. The evaluation results indicate that our method can accurately identify sensitive attributes and improve the fairness of machine learning algorithms compared to existing methods.},
  archive      = {J_EAI},
  author       = {Jiale Shi and Chuitian Rong},
  doi          = {10.1177/30504554251328351},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {436-453},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Automated data bias mitigation technique for algorithmic fairness},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Small targets detection in LIDAR point clouds based on deep learning. <em>EAI</em>, <em>38</em>(3), 417-435. (<a href='https://doi.org/10.1177/30504554251328462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multifeature fusion small-target detection network (MF-Net) is proposed based on PointRCNN, aimed at enhancing the detection accuracy of small targets in vehicle-mounted LiDAR systems. Semantically controlled farthest point sampling and multisampling strategies are presented to achieve uniform sampling and retain a greater number of small target points. Additionally, a local feature aggregation module is utilized to learn the intensity features of small target point clouds through spatial intensity encoding. Furthermore, PointPillars technology is implemented to convert the three-dimensional point cloud into a pseudo-image, allowing for the extraction of features at various scales using a feature pyramid network. Experimental results demonstrate that MF-Net improves the mean average precision for pedestrian and cyclist detection by 2.49% and 2.88%, respectively, compared to the baseline network PointRCNN. The false detection rate is reduced significantly and the detection accuracy is enhanced across diverse scenarios.},
  archive      = {J_EAI},
  author       = {Zhipeng Zhai and Jinju Shao and Meng Zhang and Jinlei Zhang and Zhibing Duan and Lei Wang},
  doi          = {10.1177/30504554251328462},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {417-435},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Small targets detection in LIDAR point clouds based on deep learning},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep node embeddings for the graph coloring problem. <em>EAI</em>, <em>38</em>(3), 405-416. (<a href='https://doi.org/10.1177/30504554251325151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing trend in utilizing deep learning techniques to solve various NP-hard combinatorial optimization problems, mostly using deep neural networks to generate the solutions directly. In this work, we address a famous combinatorial optimization problem on graphs, the graph coloring problem (GCP), and propose novel ways that train and utilize deep node embeddings to facilitate the problem’s solving. Specifically, we propose to use Transformer to learn the correlation between nodes in graphs. The Transformer learns the node embeddings (feature vectors) such that nodes that might be in the same color in (near-)optimal solutions have close embeddings. To generate the labels, we use a typical GCP heuristic called Tabucol to solve each small training instance multiple times. In this way, the labels are generated more efficiently and robustly as compared to using an exact solver. We then apply the learned embeddings to guide several construction and searching algorithms for the GCP, including Tabucol. Empirical results show that all the algorithms could be improved by utilizing the learned node embeddings, and our methods generalize well to graphs on much larger scales than the training graphs.},
  archive      = {J_EAI},
  author       = {Jiongzhi Zheng and Mingming Jin and Kun He and Jinghui Xue and Li Zhao and Lei Song and Jiang Bian},
  doi          = {10.1177/30504554251325151},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {405-416},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Deep node embeddings for the graph coloring problem},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-to-image synthesis combining cascade structure and joint attention. <em>EAI</em>, <em>38</em>(3), 390-404. (<a href='https://doi.org/10.1177/30504554251319450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the text-to-image synthesis with non-stacked network structure only models the global features of the image and the semantic features of the text, it is easy to cause problems such as semantic inconsistency, loss of detail features, and incomplete main content in the generated images. To solve the above problems, this paper proposes a text-to-image generation method (cascaded structure and joint attention generative adversarial network (CSGAN)) that combines cascaded structure and joint attention. After encoding the text, this method utilizes a conditional enhancement module to process it, which enhances the expressive ability of the text features. In order to make the local area of the image better fit the text features, this method designs a joint attention module to solve the problem that the image generation process cannot fully reflect the local details of the text content. By using affine transform mapping visual features and cascade structure to fuse the features of different modules, the integrity of the whole content of the image is effectively guaranteed and the semantic consistency of the text image is improved. Experimental results on the CUB dataset show that compared with the current mainstream non-stacked network model DF-GAN model, the inception score index of the CSGAN model is improved by about 4.01%, and the Fréchet inception distance index is reduced by about 13.36%. These data indicators and visualization results fully demonstrate the effectiveness of the CSGAN model.},
  archive      = {J_EAI},
  author       = {Chao Zhang and Mi Zhou and Cheng Han and Chuanao Bai},
  doi          = {10.1177/30504554251319450},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {390-404},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Text-to-image synthesis combining cascade structure and joint attention},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arithmetic optimization algorithm with mathematical operator for solving spherical minimum spanning tree problem. <em>EAI</em>, <em>38</em>(3), 364-389. (<a href='https://doi.org/10.1177/30504554251319447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to effectively strengthen the exploration and exploitation capabilities of the arithmetic optimization algorithm (AOA) and the balance of search ability between the two is realized, a novel mathematical operator-based arithmetic optimization algorithm (MAOA) is proposed. Firstly, the exploitation and exploration abilities of the population are improved through mathematical symmetry operators and median operators, respectively. Secondly, the balance between exploration and exploitation of AOA algorithm is effectively strengthened by using sine–cosine operator. Finally, the MAOA algorithm is used to solve the spherical mining spanning tree (sphere MST) and communication network problems. Experimental results show that the proposed MAOA has achieved excellent results in terms of accuracy, robustness, and convergence speed.},
  archive      = {J_EAI},
  author       = {Qifang Luo and Xiaodong Mi and Yuanfei Wei and Yongquan Zhou},
  doi          = {10.1177/30504554251319447},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {364-389},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Arithmetic optimization algorithm with mathematical operator for solving spherical minimum spanning tree problem},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Theater scene description for human-scene interaction. <em>EAI</em>, <em>38</em>(3), 348-363. (<a href='https://doi.org/10.1177/30504554251319445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entertainment venues like theaters are key to social engagement, but the lack of assistive technologies for people with visual impairments limits their participation. Our research aims to enhance theater accessibility using computer vision techniques to detect visual information about objects and actor gestures and convey them to blind audiences through non-visual modalities. In this work, we focus on providing a novel dataset, TS-RGBD, containing theater scenes to guide the development of computer-vision-based systems for theater scene description. It includes RGB, depth, and skeleton sequences captured by Microsoft Kinect in a theatrical setting. The dataset features untrimmed theater scenes as well as trimmed sequences consisting of individual gestures for actor gesture recognition. We test state-of-the-art image captioning models on untrimmed scenes, revealing that dense captioning models generate redundant captions with fixed numbers, leading to imprecise descriptions and a lack of context. These challenges hinder visually impaired individuals from comprehending theater scenes descriptions effectively. Additionally, we assess the performance of skeleton-based graph convolution networks for human action recognition in a theater environment using trimmed skeleton sequences. The results highlight limitations in recognizing human actions in this setting. Based on these findings, we propose solutions for overcoming these challenges, paving the way for future improvements in making theater performances more accessible to individuals with visual impairments.},
  archive      = {J_EAI},
  author       = {Khadidja Delloul and Leyla Benhamida and Slimane Larabi},
  doi          = {10.1177/30504554251319445},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {348-363},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Theater scene description for human-scene interaction},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic paraphrase generation at phrasal, and sentence level for urdu language: Data and methods. <em>EAI</em>, <em>38</em>(3), 330-347. (<a href='https://doi.org/10.1177/30504554251319449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paraphrasing involves rewording a text to maintain its meaning while using different language. In recent years, there has been growing interest among researchers in automatic paraphrase generation (APG). Previous studies have primarily focused on developing corpora and methods for APG tasks in English and other languages. However, there is a lack of comprehensive benchmark corpora and standardized methods specifically designed for APG in Urdu. To address this gap, this study introduces two extensive benchmark corpora: the Urdu Phrasal Paraphrase corpus (UPP-22 corpus) at the phrasal level and the Urdu Sentential Paraphrase corpus (USP-22 corpus) at the sentence level. The UPP-22 corpus contains 3,50,000 paraphrased pairs, while the USP-22 corpus consists of 11,000 paraphrased pairs, both specifically curated for the Urdu APG task. As a second major contribution, this research developed and applied various state-of-the-art deep learning models, including sequence-to-sequence models (using long short-term memory [LSTM], gated recurrent unit [GRU], bidirectional LSTM, and bidirectional GRU) and sequence-to-sequence models with attention mechanisms as baseline methods for the proposed Urdu Paraphrase corpora. Additionally, the study introduced a bidirectional and auto-regressive transformer-based model specifically tailored to these corpora as a third contribution. A fourth significant contribution was the development of a test corpus at the phrasal level, consisting of 10,000 instances, created through automatic translation, followed by manual inspection and correction, to evaluate the performance of APG tasks for the Urdu language. As a fifth major contribution, the study applied and fine-tuned a large language model (GPT-4-Mini) for APG in Urdu, resulting in significant performance improvements. The evaluation was carried out using the standard bilingual evaluation understudy (BLEU) metric. The best results were achieved with BLEU-1 = 75.89 for the UPP-22 corpus and BLEU-1 = 75.16 for the USP-22 corpus using the GPT-4-Mini model, representing a significant improvement over all other models including baseline. These corpora and methodologies will be made publicly available to encourage and promote further research in APG for under-resourced languages such as Urdu.},
  archive      = {J_EAI},
  author       = {Zara Khan and Iqra Muneer and Rao Muhammad Adeel Nawab and Ahmad Mahmood},
  doi          = {10.1177/30504554251319449},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {330-347},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Automatic paraphrase generation at phrasal, and sentence level for urdu language: Data and methods},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentential cross-lingual paraphrase detection for english-urdu language pair. <em>EAI</em>, <em>38</em>(3), 309-329. (<a href='https://doi.org/10.1177/30504554251319446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to vast digital data collections and paraphrasing tools, researchers have shown growing interest in Cross-lingual Paraphrase Detection (CLPD). Open-access data and tools make paraphrasing easier and detection more challenging. Translation tools further exacerbate the issue by enabling effortless text translation across languages, leading to increased cross-lingual paraphrasing. Most existing CLPD studies focus on European languages, particularly English, while the English-Urdu language pair remains underexplored due to limited standard approaches and benchmark corpora.This study addresses this gap by developing the CLPD Corpus for English-Urdu (CLPD-EU), a gold-standard benchmark corpus at the sentence level. The corpus includes 5,801 sentence pairs, comprising 3,900 paraphrased and 1,901 non-paraphrased instances. Additionally, the study implements classical machine learning methods based on bilingual dictionaries, cross-lingual word embeddings, and transfer learning using sentence transformers.The research further incorporates state-of-the-art Large Language Models (LLMs) such as Mistral and LLaMA, significantly improving detection accuracy. Our proposed Feature Fusion Approach, ‘Comb-ST+BD,’ demonstrates strong performance with an F1 score of 0.739 for the CLPD task. The CLPD-EU corpus will be publicly available to encourage further research in CLPD, especially for under-resourced languages like Urdu.},
  archive      = {J_EAI},
  author       = {Iqra Muneer and Nida Waheed and Muhammad Adnan Ashraf and Rao M Adeel Nawab},
  doi          = {10.1177/30504554251319446},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {309-329},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Sentential cross-lingual paraphrase detection for english-urdu language pair},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BiMSA: Multimodal sentiment analysis based on BiGRU and bidirectional interactive attention. <em>EAI</em>, <em>38</em>(3), 296-308. (<a href='https://doi.org/10.1177/30504554251319444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the significance of multimodal sentiment analysis is progressively increasing. However, the heterogeneity of multimodal signals poses a challenge in learning modality representation and fusing information. To address it, the BiMSA (multimodal sentiment analysis based on BiGRU and bidirectional interactive attention) is proposed in this paper. In the modality representation learning layer, BiMSA incorporates a structure consisting of BiGRU to extract contextual information from video and audio inputs. Subsequently, modality features are projected into modality internal representations and interactive representations for extracting information. This practice allows the model to take into account more aspects of modality information. In terms of the modality fusion layer, a bidirectional interactive attention mechanism is used to focus on the key representations of key modalities, and integrate multimodal information flexibly and efficiently. Attention weights are concentrated on modality representations that synergistically contribute toward overall sentiment orientation. Additionally, the constraints of similarity loss and difference loss are introduced to align with representations while mitigating redundant information and achieving a better fusion effect. Experimental results on public datasets (CMU-MOSI and CMU-MOSEI) demonstrate the effectiveness of the BiMSA model.},
  archive      = {J_EAI},
  author       = {Qi Wang and Haizheng Yu and Yao Wang and Hong Bian},
  doi          = {10.1177/30504554251319444},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {296-308},
  shortjournal = {Eur. Artif. Intell.},
  title        = {BiMSA: Multimodal sentiment analysis based on BiGRU and bidirectional interactive attention},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAM-YOLO: An improved small object detection model for vehicle detection. <em>EAI</em>, <em>38</em>(3), 279-295. (<a href='https://doi.org/10.1177/30504554251319452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection using computer vision plays a crucial role in accurately recognizing and responding to various road conditions, targets, and signals, particularly within autonomous driving technology. However, traditional vehicle detection algorithms suffer from slow detection speed, low accuracy, and poor robustness. To address these challenges, this paper proposes the simple attention mechanism-you only look once (SAM-YOLO) algorithm. SAM-YOLO incorporates the simple attention mechanism into the YOLOv7 network, allowing for the capture of more detailed information without introducing additional parameters. In this study, we experimentally redesigned the backbone network of SAM-YOLO by replacing the redundant part of the network layer with the C3 module, resulting in improved model performance while maintaining accuracy. The experimental results show that the SAM-YOLO algorithm performs excellently in several evaluation metrics under conventional conditions, especially outperforming other algorithms in accuracy and mean average precision values. In tests on the ExLight dataset facing extreme lighting conditions, SAM-YOLO similarly demonstrated optimal detection capabilities, especially in terms of robustness when dealing with complex lighting variations. These findings emphasize the potential of the SAM-YOLO algorithm for real-time and accurate target detection tasks, especially in environments with highly variable lighting conditions.},
  archive      = {J_EAI},
  author       = {JiaWang Liao and SuYu Jiang and MingHua Chen and ChengJiao Sun},
  doi          = {10.1177/30504554251319452},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {279-295},
  shortjournal = {Eur. Artif. Intell.},
  title        = {SAM-YOLO: An improved small object detection model for vehicle detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt scoring system for dialog summarization. <em>EAI</em>, <em>38</em>(3), 261-278. (<a href='https://doi.org/10.1177/30504554251321829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in language processing have demonstrated the advanced capabilities of language models. Particularly noteworthy is the heightened prowess of pre-trained large language models in tackling tasks that were a real challenge a few years ago, such as the abstractive summarization of dialogs. An approach to generating summaries involves engineering prompt templates. The easiest way would be by using a static prompt, but it can lead to unreliable outcomes for different classes of dialogs. We implemented a scoring system to enhance the performance of a few-shot training. This involves constructing finely tuned prompts composed of dialog samples with the highest scores. The scoring process is grounded in a set of heuristics that specifically assess the structure and content of the dialogs. The use of the scoring system resulted in enhanced ROUGE scores and positive evaluations from human assessors. These promising results were consistently validated across all three large-scale datasets used in the testing phase.},
  archive      = {J_EAI},
  author       = {George P Prodan and Elena Pelican},
  doi          = {10.1177/30504554251321829},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {261-278},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Prompt scoring system for dialog summarization},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSNet: Food image classification model based on lightweight convolutional neural network. <em>EAI</em>, <em>38</em>(2), 238-257. (<a href='https://doi.org/10.1177/30504554251319448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of technology, the demand for healthy eating has increased, making food classification a research hotspot. Existing deep learning-based food image classification models demonstrate high accuracy but require substantial computational resources, limiting their use on resource-constrained devices. In this study, a lightweight convolutional neural network model named MSNet for food classification is proposed. MSNet mainly consists of M Blocks and S Blocks. The M Block uses improved depthwise convolution to reduce the computational cost of conventional convolutions, and the S Block uses channel shuffle techniques to enhance feature information flow between channels without increasing additional computation, effectively capturing relationships between different channel features. Experimental results on three benchmark datasets (ETHZ Food-101, Vireo Food-172, and ISIA Food-500) show that MSNet achieves top-1 accuracies of 86.24%, 87.98%, and 65.70%, with model sizes of 13.8 MB, 15.9 MB, and 25.4 MB, respectively, outperforming mainstream models in terms of computational efficiency. Further quantization produces two MSNet-Lite variants with competitive model size, while maintaining high accuracy and significantly improving inference speed. Additionally, visualization analysis indicates that MSNet effectively extracts essential features of food images, offering good interpretability and generalization across datasets of varying complexity. The proposed MSNet model provides a feasible solution for practical deployment in food classification tasks on mobile and embedded devices.},
  archive      = {J_EAI},
  author       = {Congyuan Xu and Donghui Li and Zihao Liu and Jun Yang},
  doi          = {10.1177/30504554251319448},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {238-257},
  shortjournal = {Eur. Artif. Intell.},
  title        = {MSNet: Food image classification model based on lightweight convolutional neural network},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-branch super-resolution (DBSR): Lightweight DBSR network for enhancing remote sensing images. <em>EAI</em>, <em>38</em>(2), 217-237. (<a href='https://doi.org/10.1177/30504554241311178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning-based super-resolution (SR) techniques for remote sensing images (RSIs) have shown significant promise. However, these performance improvements often come at a high computational cost, which limits their practical application. To address this issue, this paper proposes a dual-branch SR model (DBSR) that enhances both model performance and efficiency through primary and auxiliary branches. The primary branch integrates the advantages of channel recalibration, a separable swin transformer (SST), and a spatial refinement module to achieve fine-grained feature extraction. The SST serves as the core of the primary branch, employing hierarchical window attention calculations to facilitate lightweight and effective multiscale feature representation. Conversely, the auxiliary branch enhances shallow features through a global information enhancement module, which mitigates the misleading effects of directly upsampling these shallow features on the SR results. Comparative and ablation experiments conducted on four RSI datasets and five SR benchmark datasets demonstrate that our DBSR method effectively balances the number of parameters with performance, showcasing its potential for application in RSI processing.},
  archive      = {J_EAI},
  author       = {Tianjun Tang and Yuheng Ren and Shuwan Feng},
  doi          = {10.1177/30504554241311178},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {217-237},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Dual-branch super-resolution (DBSR): Lightweight DBSR network for enhancing remote sensing images},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learned lossless image compression based on optimal kernel transformer approach. <em>EAI</em>, <em>38</em>(2), 198-216. (<a href='https://doi.org/10.1177/30504554241311177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image compression is crucial for reducing storage and transmission costs, particularly in applications involving high-resolution and complex imagery. Traditional compression methods, such as JPEG, PNG, and newer lossless formats like JPEG XL and WebP, often suffer from suboptimal compression ratios (CRs) and image quality when handling modern high-definition content. To overcome these limitations, this paper proposes a novel deep learning-based lossless image compression method. The symmetrical transformer (STF) model is introduced, integrating transformer blocks in both the downsampling encoder and upsampling decoder to enhance the capture of local and global features. The model also includes a multivariate mixture distribution channel conditioning (MMCC) entropy model, which improves pixel dependency predictions by modeling complex relationships within image channels. Additionally, an automated searching of optimal kernel shapes (SOKS) is employed to dynamically configure kernel sizes, optimizing the convolutional layers for different image regions. The system also applies stripe-wise pruning (SWP), which selectively prunes unimportant features during compression, reducing computational complexity and memory usage without compromising image quality. Extensive evaluations on standard datasets, including Kodak, CLIC, and DIV2K, demonstrate the effectiveness of the proposed model. Specifically, the approach achieves significant compression efficiency, with bits-per-dimension (BPD) values of 3.06 on Kodak, 3.91 on CLIC, and 3.63 on DIV2K, outperforming traditional methods such as iVPF (3.20 BPD on Kodak), GOLLIC (3.15 BPD on Kodak), and MSPSM (3.12 BPD on Kodak). In addition to compression efficiency, the proposed method excels in inference speed, with encoding times as low as 7.80 ms per sample on Kodak, significantly faster than competing methods. These results demonstrate a substantial improvement in compression rates and image reconstruction quality, highlighting the model's potential for real-world applications, including medical imaging, remote sensing, and real-time streaming, offering a significant advancement in lossless image compression.},
  archive      = {J_EAI},
  author       = {Pitchumony Sherly Kanaga Priya and Chellam Helen Sulochana},
  doi          = {10.1177/30504554241311177},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {198-216},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Learned lossless image compression based on optimal kernel transformer approach},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Triangle mesh reconstruction by fusion of residual graph convolution and edge contraction pooling. <em>EAI</em>, <em>38</em>(2), 181-197. (<a href='https://doi.org/10.1177/30504554241301393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle meshes are a crucial and powerful data type for three-dimensional (3D) shapes, extensively studied in the fields of computer vision and computer graphics. In this paper, we delve into the challenge of analyzing deforming 3D triangle meshes using deep neural networks. While existing methods extend graph-based deep learning to 3D triangle meshes using graph convolution, the lack of effective graph convolutional structures and pooling operations limits the learning capacity of their networks. We propose a variational autoencoder structure that integrates graph convolutional residual blocks with multilayer pooling to explore the latent space of 3D shapes for generation. This framework introduces graph convolutional residual blocks to address the issue of gradient vanishing in deep networks. By employing multilayer pooling and unpooling structures based on triangle mesh simplification, gradually reducing the spatial dimensions of the input, the model can extract more general features. This enables it to handle denser mesh models effectively. Extensive experiments demonstrate that our generalized framework can learn reasonable representations of deformable shape collections with minimal training examples. It produces competitive results across various applications, including shape generation and interpolation, requiring fewer training samples and outperforming state-of-the-art techniques.},
  archive      = {J_EAI},
  author       = {Yang Ding and Huamin Yang and Cheng Han and Xinhui Zhong},
  doi          = {10.1177/30504554241301393},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {181-197},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Triangle mesh reconstruction by fusion of residual graph convolution and edge contraction pooling},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-stream feature extraction and multiscale attention generative adversarial network (BM-GAN): Colorization of GrayScale images based on bi-stream feature fusion and multiscale attention generative adversarial network. <em>EAI</em>, <em>38</em>(2), 159-180. (<a href='https://doi.org/10.1177/30504554241297613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image colorization is one of the core issues in computer vision that has attracted significant attention in recent years. Colorization technique improves the human eye’s ability to recognize grayscale images and understand scenes, particularly in low-light-level (LLL) images. However, current colorization methods still face issues, such as semantic confusion, color bleeding, and loss of image details. To address these issues, a bi-stream feature extraction and multiscale attention generative adversarial network (BM-GAN) is proposed. The bi-stream feature extraction block combines global and local features extracted from two parallel encoders. This combination improves the ability of the network to extract deep features from images. The multiscale attention block enhances key features related to the colorization target across channels and spatial dimensions. This results in higher-quality color images. The proposed method is evaluated on ImageNet, Summer2winter validation set, and LLL images. Experimental results show that BM-GAN reduces the feature-aware evaluation metrics learned perceptual image patch similarity and Fréchet inception distance by 5.2% and 7.5%, respectively.},
  archive      = {J_EAI},
  author       = {Xiaoning Gao and Liju Yin and Yulin Deng and Feng Wang and Yiming Qin and Meng zhang},
  doi          = {10.1177/30504554241297613},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {159-180},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Bi-stream feature extraction and multiscale attention generative adversarial network (BM-GAN): Colorization of GrayScale images based on bi-stream feature fusion and multiscale attention generative adversarial network},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A haze intensity sensing and texture enhanced transformer for image dehazing. <em>EAI</em>, <em>38</em>(2), 145-158. (<a href='https://doi.org/10.1177/30504554241296445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The degradation of images captured in hazy weather can severely affect practical applications. However, most existing learning-based methods ignore the varied haze distribution in an image, resulting in incomplete dehazing of some areas. Also, the presence of haze can blur the textures and details, which will heavily impact the subsequent image processing. In this paper, we propose a transformer-based framework for dehazing tasks called HITFormer. Firstly, we introduce a texture recovery and enhance module as a preprocess to strengthen details. Then, we propose an adaptive haze intensity prediction subnet to predict the haze intensity of different areas. Lastly, we use a semantic-based luminance and chrominance adjustment module to fuse the feature maps in YUV color space and form a transformation coefficient to get a recovery image. The extensive experiments demonstrate that our HITFormer achieves state-of-the-art performance on several image dehazing datasets.},
  archive      = {J_EAI},
  author       = {Tianli Zhang and Hao Zeng},
  doi          = {10.1177/30504554241296445},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {145-158},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A haze intensity sensing and texture enhanced transformer for image dehazing},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bateson-inspired model for the generation of semantic concepts from sensory data. <em>EAI</em>, <em>38</em>(2), 113-144. (<a href='https://doi.org/10.1177/30504554251314334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although neural-symbolic techniques for symbol grounding in sensory data have shown significant effectiveness, they still require substantial training. This article revisits symbolic-only approaches by introducing a new algorithm for creating hierarchical concept structures from spatial sensory data. The method is based on Bateson’s idea of difference as the fundamental element of concept formation. By leveraging this principle, the algorithm extracts atomic features from raw data through basic sequential comparisons within a stream of multivariate numerical values. Experiments carried out on a set of common objects indicate that the method can successfully discriminate and assimilate categories as needed without training. The results show that the model improves on the neural networks it has been tested against, which required more than 400 training examples for the same task. The results also show that the model can generate rich conceptual structures and human-like representations, which (i) facilitate high composability, (ii) support formal reasoning, (iii) inherently enable generalisation, and (iv) possess potential for generative behaviour. Consequently, this approach offers a compelling contribution to symbol grounding and neural-symbolic research by providing a seamless algorithm to bridge perception and conceptual knowledge.},
  archive      = {J_EAI},
  author       = {Jaime de Miguel Rodríguez and Fernando Sancho Caparrini},
  doi          = {10.1177/30504554251314334},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {113-144},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A bateson-inspired model for the generation of semantic concepts from sensory data},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional diffusion model for missing value imputation. <em>EAI</em>, <em>38</em>(1), 96-109. (<a href='https://doi.org/10.1177/30504554241311182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid proliferation of social data, the prevalence of missing values has become increasingly common. Various factors, including human error and machine failure, contribute to the emergence of missing values in datasets. Datasets containing missing values not only consume storage space but also pose a significant obstacle to direct utilization, resulting in substantial resource wastage. Consequently, accurately imputing missing values has emerged as a focal point in research. Generative missing value imputation methods, leveraging generative models, have demonstrated notable efficacy in recent years by directly generating values for missing components based on observable data values. This paper introduces a novel generative method for missing value imputation based on a diffusion denoising model, termed the conditional diffusion model for missing value imputation (CDMVI). Specifically, CDMVI trains a conditional diffusion model using complete data samples (samples devoid of missing values) and subsequently utilizes the trained model to impute missing values in datasets. During the training stage (i.e., the forward process of the diffusion model), a subset of features is randomly selected from complete data samples, and varying levels of random noise are introduced as condition inputs to the noise predictor within the diffusion model. In the imputation stage (i.e., the backward process of the diffusion model), the missing segments of the data are initially replaced with random noise, serving as a guide for the diffusion model to generate complete samples. Experimental evaluations across multiple datasets demonstrate the competitive performance of our proposed CDMVI method.},
  archive      = {J_EAI},
  author       = {Binyi Li and Long Long and Xi Zuo and Long Chen},
  doi          = {10.1177/30504554241311182},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {96-109},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Conditional diffusion model for missing value imputation},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image manipulation detection based on irrelevant information suppression and critical information enhancement. <em>EAI</em>, <em>38</em>(1), 79-95. (<a href='https://doi.org/10.1177/30504554241301395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image manipulation detection localization task differs from traditional computer vision tasks in that we focus more on capturing subtle and generic manipulation detection features in images. In this paper, we propose a novel method called irrelevant visual information suppression, which aims to alleviate the interference of irrelevant visual information in images on manipulation detection feature extraction, thereby obtaining generic manipulation traces that are more subtle and unrelated to semantic visual information. In general, most manipulation operations leave traces at manipulation edges. Therefore, we introduce a specially designed manipulated edge information enhancement branch aimed at identifying these edge artifacts more accurately. We construct a dual-branch network, where each branch uses ResNet-50 as the backbone to capture as many multi-scale manipulation features as possible. Finally, we adopt a multi-view feature learning method that combines the manipulated edge information enhancement branch with the irrelevant visual information suppression branch and is trained with multi-scale (pixel/edge/image/irrelevant visual information suppression) supervision. To validate the effectiveness of the proposed method, we conducted extensive experiments using five image manipulation localization datasets, including CASIAv1, CASIAv2, COVER, Columbia, and NIST16. The experimental results demonstrate that our proposed method can outperform state-of-the-art methods by a significant margin in terms of F1 score. Taking CASIAv1, COVER, and Columbia datasets as examples, compared with MVSS-Net published in ICCV 2021, our method has improved F1 scores by 7.1%, 6.3%, and 12.5%, respectively. The code used in this paper can be found at the following URL: https://github.com/ginwins/ISIE-Net .},
  archive      = {J_EAI},
  author       = {Yunxue Shao and Tingting Wang and Lingfeng Wang},
  doi          = {10.1177/30504554241301395},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {79-95},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Image manipulation detection based on irrelevant information suppression and critical information enhancement},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised temporal action localization with contrastive learning-based action salience network. <em>EAI</em>, <em>38</em>(1), 64-78. (<a href='https://doi.org/10.1177/30504554241301394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the wide application of weakly supervised temporal action localization (WTAL) technology has accelerated the efficiency of video analysis. However, this domain continues to confront numerous challenges, especially due to the lack of precise temporal annotations. Consequently, this technique becomes highly susceptible to contextual background noise and overly reliant on prominent action segments, leading to less-than-ideal action localization. To alleviate this problem, we propose the contrastive learning-based action salience network (CLASNet), comprising two pivotal modules: feature contrast separation module (FCSM) and boundary refinement module (BRM). FCSM utilizes a contrastive learning approach to effectively separate action features from background features, thereby enhancing the discriminability of features. Concurrently, BRM introduces boundary refinement loss to rectify the temporal boundaries of actions, thereby further elevating the precision of temporal localization. The collaborative functioning of these two key modules effectively resolves the ambiguity issues in temporal action localization under weak supervision, markedly enhancing localization accuracy. Furthermore, CLASNet is versatile and can be integrated into different WTAL frameworks, achieving enhanced localization performance while preserving the original end-to-end training manner. Utilizing three large-scale benchmark action localization datasets, THUMOS14, ActivityNet v1.2, and ActivityNet v1.3, we embed CLASNet into various cutting-edge weakly supervised temporal action localization methods, such as CO2-Net, DELU, and ACRNet, for empirical substantiation. The experimental outcomes reveal that CLASNet significantly enhances the efficacy of these methods in action localization, offering novel perspectives for the advancement of temporal action localization technology.},
  archive      = {J_EAI},
  author       = {Sun Jingtao and Shi Weipeng and Hao Shaoyang and Wang Jialin},
  doi          = {10.1177/30504554241301394},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {64-78},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Weakly supervised temporal action localization with contrastive learning-based action salience network},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel news recommendation model with knowledge enhancement and stability. <em>EAI</em>, <em>38</em>(1), 50-63. (<a href='https://doi.org/10.1177/30504554241301391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of information technology, today’s society has higher and higher requirements for the recommendation system, especially regarding recommendation accuracy. A significant feature of news recommendation is that it has high timeliness, and the popularity of a news article will decline exponentially in a week. The effectiveness of traditional recommendation methods in news recommendation could be more optimistic. In order to further improve the accuracy of news recommendations, a large number of knowledge graphs are applied to news recommendations, and the nodes and edges of the knowledge graph can better represent the relationship between entities in the article; compared with traditional recommendation methods, it can better solve the problems of data sparsity and cold start. This paper proposes a relational entity credibility discrimination model, eliminating the relational entities without credibility to improve news recommendations accuracy, the existence of some relational entities in the triad of the knowledge graph may distort the meaning of the article or have a near-zero impact on the article, which is considered untrustworthy for these two types of relational entities. Experimental results show the effectiveness and efficiency of the model.},
  archive      = {J_EAI},
  author       = {Huajing Huang and Yongquan Fan and Linsen Li and Jiabao Chen and Tianyi Zhou},
  doi          = {10.1177/30504554241301391},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {50-63},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A novel news recommendation model with knowledge enhancement and stability},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Registration and convolutional multidimensional attention module for few-shot anomaly detection. <em>EAI</em>, <em>38</em>(1), 35-49. (<a href='https://doi.org/10.1177/30504554241297616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the shortcomings of most current anomaly detection models, such as low detection accuracy and poor generalization performance, this paper proposes a few-shot anomaly detection model based on a convolutional multidimensional attention module to achieve feature registration (abbreviated as RCM-FSAD), which enhances the model’s perception of the overall image perception ability, using spatial transformer network to obtain the spatial transformation features of the image, improving the sensitivity of the relevant features, so that the whole model learns the commonality between the categories, and enhancing the generalization ability of the model. The spatial transformations and local structures of the input data are captured by deformable convolutional networks v2 to ensure the spatial invariance of the input data. The model is trained with only normal samples to accomplish anomalous regions’ localization and anomaly detection. On the challenging MVTec AD dataset, the unsupervised model not only improves the anomaly detection accuracy but also shows better generalization compared to current state-of-the-art unsupervised anomaly detection methods.},
  archive      = {J_EAI},
  author       = {Xin Xie and Shenping Xiong and Wenbin Zheng and Tijian Cai},
  doi          = {10.1177/30504554241297616},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {35-49},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Registration and convolutional multidimensional attention module for few-shot anomaly detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-lingual English–Urdu semantic word similarity using sentence transformers. <em>EAI</em>, <em>38</em>(1), 21-34. (<a href='https://doi.org/10.1177/30504554241297614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic word similarity is a quantitative method of determining how much two terms are contextually identical, which is a considerable challenge for computational linguistics. The research community has examined a range of approaches to address this issue. However, most of these approaches are for a comparatively limited set of languages, especially English. Research on semantic word similarity for South Asian languages, particularly Urdu, is immature. In recent years, transformer-based approaches have proved extremely successful for a range of language processing tasks. The primary aim of this study is to develop and compare a variety of transformer-based approaches to the cross-lingual English–Urdu semantic word similarity task. This study evaluated a publicly available benchmark USWS-19 corpus that comprises 518 word pairs. This study mainly explored four types of transformer-based approaches: (a) cross-lingual sentence transformer-based approaches using the original dataset, (b) cross-lingual sentence transformer-based approaches using the translated dataset (translation plus monolingual analysis [T+MA] approach), (c) the feature fusion approach (mixture of features), and large language models. In addition, this study also explores the word embedding-based approach using the translated dataset (T+MA approach). In total, this study developed 29 transformer-based models, with the highest results (Pearson correlation = 0.788) achieved using a feature fusion approach, that is, Best-Two-SBERT (where SBERT stands for sentence-bidirectional encoder representations from transformers; using T+MA) + BEST Baseline (with Bing translator) + Best cross-lingual SBERT. This approach improved by 7% over previously reported results on the same corpus.},
  archive      = {J_EAI},
  author       = {Iqra Muneer and Ali Saeed and Rao Muhammad Adeel Nawab},
  doi          = {10.1177/30504554241297614},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {21-34},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Cross-lingual English–Urdu semantic word similarity using sentence transformers},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The 12th IJCAR automated theorem proving system Competition—CASC-j12. <em>EAI</em>, <em>38</em>(1), 3-20. (<a href='https://doi.org/10.1177/30504554241305110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CADE ATP System Competition (CASC) is the annual evaluation of fully automatic, classical logic, automated theorem proving (ATP) systems—the world championship for such systems. CASC-J12 was the 29th competition in the CASC series. Nineteen ATP systems competed in the various divisions. This paper presents an outline of the competition design and a commentated summary of the results.},
  archive      = {J_EAI},
  author       = {Geoff Sutcliffe},
  doi          = {10.1177/30504554241305110},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {3-20},
  shortjournal = {Eur. Artif. Intell.},
  title        = {The 12th IJCAR automated theorem proving system Competition—CASC-j12},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
