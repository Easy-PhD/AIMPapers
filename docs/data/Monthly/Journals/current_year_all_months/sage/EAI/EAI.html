<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EAI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="eai">EAI - 25</h2>
<ul>
<li><details>
<summary>
(2025). Automated data bias mitigation technique for algorithmic fairness. <em>EAI</em>, <em>38</em>(3), 436-453. (<a href='https://doi.org/10.1177/30504554251328351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning fairness enhancement methods based on data bias correction are usually divided into two processes: The determination of sensitive attributes (such as race and gender) and the correction of data bias. In terms of determining sensitive attributes, existing studies tend to rely too heavily on sociological knowledge and neglect the importance of exploring potential sensitive attributes directly from the data itself. The accuracy of this approach is limited when dealing with data that cannot be fully explained by sociological factors. Regarding data bias correction, existing methods are primarily categorized into causality-based and association-based methods. The former requires a deep understanding of the underlying causal structure in the dataset, which is often difficult to achieve in practice. The latter method correlates sensitive attributes with algorithmic results through statistical measures, but this approach often tends to ignore the impact of sensitive attributes on other attributes. In this paper, we formalize the identification of sensitive attributes as a problem solvable through data analysis, without relying on commonly recognized knowledge in social science. We also propose a data pre-processing method that considers the effects of attributes correlated with sensitive attributes to enhance algorithmic fairness by combining the association-based bias reduction method. We evaluated our proposed method on a public dataset. The evaluation results indicate that our method can accurately identify sensitive attributes and improve the fairness of machine learning algorithms compared to existing methods.},
  archive      = {J_EAI},
  author       = {Jiale Shi and Chuitian Rong},
  doi          = {10.1177/30504554251328351},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {436-453},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Automated data bias mitigation technique for algorithmic fairness},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Small targets detection in LIDAR point clouds based on deep learning. <em>EAI</em>, <em>38</em>(3), 417-435. (<a href='https://doi.org/10.1177/30504554251328462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multifeature fusion small-target detection network (MF-Net) is proposed based on PointRCNN, aimed at enhancing the detection accuracy of small targets in vehicle-mounted LiDAR systems. Semantically controlled farthest point sampling and multisampling strategies are presented to achieve uniform sampling and retain a greater number of small target points. Additionally, a local feature aggregation module is utilized to learn the intensity features of small target point clouds through spatial intensity encoding. Furthermore, PointPillars technology is implemented to convert the three-dimensional point cloud into a pseudo-image, allowing for the extraction of features at various scales using a feature pyramid network. Experimental results demonstrate that MF-Net improves the mean average precision for pedestrian and cyclist detection by 2.49% and 2.88%, respectively, compared to the baseline network PointRCNN. The false detection rate is reduced significantly and the detection accuracy is enhanced across diverse scenarios.},
  archive      = {J_EAI},
  author       = {Zhipeng Zhai and Jinju Shao and Meng Zhang and Jinlei Zhang and Zhibing Duan and Lei Wang},
  doi          = {10.1177/30504554251328462},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {417-435},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Small targets detection in LIDAR point clouds based on deep learning},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep node embeddings for the graph coloring problem. <em>EAI</em>, <em>38</em>(3), 405-416. (<a href='https://doi.org/10.1177/30504554251325151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing trend in utilizing deep learning techniques to solve various NP-hard combinatorial optimization problems, mostly using deep neural networks to generate the solutions directly. In this work, we address a famous combinatorial optimization problem on graphs, the graph coloring problem (GCP), and propose novel ways that train and utilize deep node embeddings to facilitate the problem’s solving. Specifically, we propose to use Transformer to learn the correlation between nodes in graphs. The Transformer learns the node embeddings (feature vectors) such that nodes that might be in the same color in (near-)optimal solutions have close embeddings. To generate the labels, we use a typical GCP heuristic called Tabucol to solve each small training instance multiple times. In this way, the labels are generated more efficiently and robustly as compared to using an exact solver. We then apply the learned embeddings to guide several construction and searching algorithms for the GCP, including Tabucol. Empirical results show that all the algorithms could be improved by utilizing the learned node embeddings, and our methods generalize well to graphs on much larger scales than the training graphs.},
  archive      = {J_EAI},
  author       = {Jiongzhi Zheng and Mingming Jin and Kun He and Jinghui Xue and Li Zhao and Lei Song and Jiang Bian},
  doi          = {10.1177/30504554251325151},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {405-416},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Deep node embeddings for the graph coloring problem},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-to-image synthesis combining cascade structure and joint attention. <em>EAI</em>, <em>38</em>(3), 390-404. (<a href='https://doi.org/10.1177/30504554251319450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the text-to-image synthesis with non-stacked network structure only models the global features of the image and the semantic features of the text, it is easy to cause problems such as semantic inconsistency, loss of detail features, and incomplete main content in the generated images. To solve the above problems, this paper proposes a text-to-image generation method (cascaded structure and joint attention generative adversarial network (CSGAN)) that combines cascaded structure and joint attention. After encoding the text, this method utilizes a conditional enhancement module to process it, which enhances the expressive ability of the text features. In order to make the local area of the image better fit the text features, this method designs a joint attention module to solve the problem that the image generation process cannot fully reflect the local details of the text content. By using affine transform mapping visual features and cascade structure to fuse the features of different modules, the integrity of the whole content of the image is effectively guaranteed and the semantic consistency of the text image is improved. Experimental results on the CUB dataset show that compared with the current mainstream non-stacked network model DF-GAN model, the inception score index of the CSGAN model is improved by about 4.01%, and the Fréchet inception distance index is reduced by about 13.36%. These data indicators and visualization results fully demonstrate the effectiveness of the CSGAN model.},
  archive      = {J_EAI},
  author       = {Chao Zhang and Mi Zhou and Cheng Han and Chuanao Bai},
  doi          = {10.1177/30504554251319450},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {390-404},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Text-to-image synthesis combining cascade structure and joint attention},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arithmetic optimization algorithm with mathematical operator for solving spherical minimum spanning tree problem. <em>EAI</em>, <em>38</em>(3), 364-389. (<a href='https://doi.org/10.1177/30504554251319447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to effectively strengthen the exploration and exploitation capabilities of the arithmetic optimization algorithm (AOA) and the balance of search ability between the two is realized, a novel mathematical operator-based arithmetic optimization algorithm (MAOA) is proposed. Firstly, the exploitation and exploration abilities of the population are improved through mathematical symmetry operators and median operators, respectively. Secondly, the balance between exploration and exploitation of AOA algorithm is effectively strengthened by using sine–cosine operator. Finally, the MAOA algorithm is used to solve the spherical mining spanning tree (sphere MST) and communication network problems. Experimental results show that the proposed MAOA has achieved excellent results in terms of accuracy, robustness, and convergence speed.},
  archive      = {J_EAI},
  author       = {Qifang Luo and Xiaodong Mi and Yuanfei Wei and Yongquan Zhou},
  doi          = {10.1177/30504554251319447},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {364-389},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Arithmetic optimization algorithm with mathematical operator for solving spherical minimum spanning tree problem},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Theater scene description for human-scene interaction. <em>EAI</em>, <em>38</em>(3), 348-363. (<a href='https://doi.org/10.1177/30504554251319445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entertainment venues like theaters are key to social engagement, but the lack of assistive technologies for people with visual impairments limits their participation. Our research aims to enhance theater accessibility using computer vision techniques to detect visual information about objects and actor gestures and convey them to blind audiences through non-visual modalities. In this work, we focus on providing a novel dataset, TS-RGBD, containing theater scenes to guide the development of computer-vision-based systems for theater scene description. It includes RGB, depth, and skeleton sequences captured by Microsoft Kinect in a theatrical setting. The dataset features untrimmed theater scenes as well as trimmed sequences consisting of individual gestures for actor gesture recognition. We test state-of-the-art image captioning models on untrimmed scenes, revealing that dense captioning models generate redundant captions with fixed numbers, leading to imprecise descriptions and a lack of context. These challenges hinder visually impaired individuals from comprehending theater scenes descriptions effectively. Additionally, we assess the performance of skeleton-based graph convolution networks for human action recognition in a theater environment using trimmed skeleton sequences. The results highlight limitations in recognizing human actions in this setting. Based on these findings, we propose solutions for overcoming these challenges, paving the way for future improvements in making theater performances more accessible to individuals with visual impairments.},
  archive      = {J_EAI},
  author       = {Khadidja Delloul and Leyla Benhamida and Slimane Larabi},
  doi          = {10.1177/30504554251319445},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {348-363},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Theater scene description for human-scene interaction},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic paraphrase generation at phrasal, and sentence level for urdu language: Data and methods. <em>EAI</em>, <em>38</em>(3), 330-347. (<a href='https://doi.org/10.1177/30504554251319449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paraphrasing involves rewording a text to maintain its meaning while using different language. In recent years, there has been growing interest among researchers in automatic paraphrase generation (APG). Previous studies have primarily focused on developing corpora and methods for APG tasks in English and other languages. However, there is a lack of comprehensive benchmark corpora and standardized methods specifically designed for APG in Urdu. To address this gap, this study introduces two extensive benchmark corpora: the Urdu Phrasal Paraphrase corpus (UPP-22 corpus) at the phrasal level and the Urdu Sentential Paraphrase corpus (USP-22 corpus) at the sentence level. The UPP-22 corpus contains 3,50,000 paraphrased pairs, while the USP-22 corpus consists of 11,000 paraphrased pairs, both specifically curated for the Urdu APG task. As a second major contribution, this research developed and applied various state-of-the-art deep learning models, including sequence-to-sequence models (using long short-term memory [LSTM], gated recurrent unit [GRU], bidirectional LSTM, and bidirectional GRU) and sequence-to-sequence models with attention mechanisms as baseline methods for the proposed Urdu Paraphrase corpora. Additionally, the study introduced a bidirectional and auto-regressive transformer-based model specifically tailored to these corpora as a third contribution. A fourth significant contribution was the development of a test corpus at the phrasal level, consisting of 10,000 instances, created through automatic translation, followed by manual inspection and correction, to evaluate the performance of APG tasks for the Urdu language. As a fifth major contribution, the study applied and fine-tuned a large language model (GPT-4-Mini) for APG in Urdu, resulting in significant performance improvements. The evaluation was carried out using the standard bilingual evaluation understudy (BLEU) metric. The best results were achieved with BLEU-1 = 75.89 for the UPP-22 corpus and BLEU-1 = 75.16 for the USP-22 corpus using the GPT-4-Mini model, representing a significant improvement over all other models including baseline. These corpora and methodologies will be made publicly available to encourage and promote further research in APG for under-resourced languages such as Urdu.},
  archive      = {J_EAI},
  author       = {Zara Khan and Iqra Muneer and Rao Muhammad Adeel Nawab and Ahmad Mahmood},
  doi          = {10.1177/30504554251319449},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {330-347},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Automatic paraphrase generation at phrasal, and sentence level for urdu language: Data and methods},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentential cross-lingual paraphrase detection for english-urdu language pair. <em>EAI</em>, <em>38</em>(3), 309-329. (<a href='https://doi.org/10.1177/30504554251319446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to vast digital data collections and paraphrasing tools, researchers have shown growing interest in Cross-lingual Paraphrase Detection (CLPD). Open-access data and tools make paraphrasing easier and detection more challenging. Translation tools further exacerbate the issue by enabling effortless text translation across languages, leading to increased cross-lingual paraphrasing. Most existing CLPD studies focus on European languages, particularly English, while the English-Urdu language pair remains underexplored due to limited standard approaches and benchmark corpora.This study addresses this gap by developing the CLPD Corpus for English-Urdu (CLPD-EU), a gold-standard benchmark corpus at the sentence level. The corpus includes 5,801 sentence pairs, comprising 3,900 paraphrased and 1,901 non-paraphrased instances. Additionally, the study implements classical machine learning methods based on bilingual dictionaries, cross-lingual word embeddings, and transfer learning using sentence transformers.The research further incorporates state-of-the-art Large Language Models (LLMs) such as Mistral and LLaMA, significantly improving detection accuracy. Our proposed Feature Fusion Approach, ‘Comb-ST+BD,’ demonstrates strong performance with an F1 score of 0.739 for the CLPD task. The CLPD-EU corpus will be publicly available to encourage further research in CLPD, especially for under-resourced languages like Urdu.},
  archive      = {J_EAI},
  author       = {Iqra Muneer and Nida Waheed and Muhammad Adnan Ashraf and Rao M Adeel Nawab},
  doi          = {10.1177/30504554251319446},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {309-329},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Sentential cross-lingual paraphrase detection for english-urdu language pair},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BiMSA: Multimodal sentiment analysis based on BiGRU and bidirectional interactive attention. <em>EAI</em>, <em>38</em>(3), 296-308. (<a href='https://doi.org/10.1177/30504554251319444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the significance of multimodal sentiment analysis is progressively increasing. However, the heterogeneity of multimodal signals poses a challenge in learning modality representation and fusing information. To address it, the BiMSA (multimodal sentiment analysis based on BiGRU and bidirectional interactive attention) is proposed in this paper. In the modality representation learning layer, BiMSA incorporates a structure consisting of BiGRU to extract contextual information from video and audio inputs. Subsequently, modality features are projected into modality internal representations and interactive representations for extracting information. This practice allows the model to take into account more aspects of modality information. In terms of the modality fusion layer, a bidirectional interactive attention mechanism is used to focus on the key representations of key modalities, and integrate multimodal information flexibly and efficiently. Attention weights are concentrated on modality representations that synergistically contribute toward overall sentiment orientation. Additionally, the constraints of similarity loss and difference loss are introduced to align with representations while mitigating redundant information and achieving a better fusion effect. Experimental results on public datasets (CMU-MOSI and CMU-MOSEI) demonstrate the effectiveness of the BiMSA model.},
  archive      = {J_EAI},
  author       = {Qi Wang and Haizheng Yu and Yao Wang and Hong Bian},
  doi          = {10.1177/30504554251319444},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {296-308},
  shortjournal = {Eur. Artif. Intell.},
  title        = {BiMSA: Multimodal sentiment analysis based on BiGRU and bidirectional interactive attention},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAM-YOLO: An improved small object detection model for vehicle detection. <em>EAI</em>, <em>38</em>(3), 279-295. (<a href='https://doi.org/10.1177/30504554251319452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection using computer vision plays a crucial role in accurately recognizing and responding to various road conditions, targets, and signals, particularly within autonomous driving technology. However, traditional vehicle detection algorithms suffer from slow detection speed, low accuracy, and poor robustness. To address these challenges, this paper proposes the simple attention mechanism-you only look once (SAM-YOLO) algorithm. SAM-YOLO incorporates the simple attention mechanism into the YOLOv7 network, allowing for the capture of more detailed information without introducing additional parameters. In this study, we experimentally redesigned the backbone network of SAM-YOLO by replacing the redundant part of the network layer with the C3 module, resulting in improved model performance while maintaining accuracy. The experimental results show that the SAM-YOLO algorithm performs excellently in several evaluation metrics under conventional conditions, especially outperforming other algorithms in accuracy and mean average precision values. In tests on the ExLight dataset facing extreme lighting conditions, SAM-YOLO similarly demonstrated optimal detection capabilities, especially in terms of robustness when dealing with complex lighting variations. These findings emphasize the potential of the SAM-YOLO algorithm for real-time and accurate target detection tasks, especially in environments with highly variable lighting conditions.},
  archive      = {J_EAI},
  author       = {JiaWang Liao and SuYu Jiang and MingHua Chen and ChengJiao Sun},
  doi          = {10.1177/30504554251319452},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {279-295},
  shortjournal = {Eur. Artif. Intell.},
  title        = {SAM-YOLO: An improved small object detection model for vehicle detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt scoring system for dialog summarization. <em>EAI</em>, <em>38</em>(3), 261-278. (<a href='https://doi.org/10.1177/30504554251321829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in language processing have demonstrated the advanced capabilities of language models. Particularly noteworthy is the heightened prowess of pre-trained large language models in tackling tasks that were a real challenge a few years ago, such as the abstractive summarization of dialogs. An approach to generating summaries involves engineering prompt templates. The easiest way would be by using a static prompt, but it can lead to unreliable outcomes for different classes of dialogs. We implemented a scoring system to enhance the performance of a few-shot training. This involves constructing finely tuned prompts composed of dialog samples with the highest scores. The scoring process is grounded in a set of heuristics that specifically assess the structure and content of the dialogs. The use of the scoring system resulted in enhanced ROUGE scores and positive evaluations from human assessors. These promising results were consistently validated across all three large-scale datasets used in the testing phase.},
  archive      = {J_EAI},
  author       = {George P Prodan and Elena Pelican},
  doi          = {10.1177/30504554251321829},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {8},
  number       = {3},
  pages        = {261-278},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Prompt scoring system for dialog summarization},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSNet: Food image classification model based on lightweight convolutional neural network. <em>EAI</em>, <em>38</em>(2), 238-257. (<a href='https://doi.org/10.1177/30504554251319448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of technology, the demand for healthy eating has increased, making food classification a research hotspot. Existing deep learning-based food image classification models demonstrate high accuracy but require substantial computational resources, limiting their use on resource-constrained devices. In this study, a lightweight convolutional neural network model named MSNet for food classification is proposed. MSNet mainly consists of M Blocks and S Blocks. The M Block uses improved depthwise convolution to reduce the computational cost of conventional convolutions, and the S Block uses channel shuffle techniques to enhance feature information flow between channels without increasing additional computation, effectively capturing relationships between different channel features. Experimental results on three benchmark datasets (ETHZ Food-101, Vireo Food-172, and ISIA Food-500) show that MSNet achieves top-1 accuracies of 86.24%, 87.98%, and 65.70%, with model sizes of 13.8 MB, 15.9 MB, and 25.4 MB, respectively, outperforming mainstream models in terms of computational efficiency. Further quantization produces two MSNet-Lite variants with competitive model size, while maintaining high accuracy and significantly improving inference speed. Additionally, visualization analysis indicates that MSNet effectively extracts essential features of food images, offering good interpretability and generalization across datasets of varying complexity. The proposed MSNet model provides a feasible solution for practical deployment in food classification tasks on mobile and embedded devices.},
  archive      = {J_EAI},
  author       = {Congyuan Xu and Donghui Li and Zihao Liu and Jun Yang},
  doi          = {10.1177/30504554251319448},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {238-257},
  shortjournal = {Eur. Artif. Intell.},
  title        = {MSNet: Food image classification model based on lightweight convolutional neural network},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-branch super-resolution (DBSR): Lightweight DBSR network for enhancing remote sensing images. <em>EAI</em>, <em>38</em>(2), 217-237. (<a href='https://doi.org/10.1177/30504554241311178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning-based super-resolution (SR) techniques for remote sensing images (RSIs) have shown significant promise. However, these performance improvements often come at a high computational cost, which limits their practical application. To address this issue, this paper proposes a dual-branch SR model (DBSR) that enhances both model performance and efficiency through primary and auxiliary branches. The primary branch integrates the advantages of channel recalibration, a separable swin transformer (SST), and a spatial refinement module to achieve fine-grained feature extraction. The SST serves as the core of the primary branch, employing hierarchical window attention calculations to facilitate lightweight and effective multiscale feature representation. Conversely, the auxiliary branch enhances shallow features through a global information enhancement module, which mitigates the misleading effects of directly upsampling these shallow features on the SR results. Comparative and ablation experiments conducted on four RSI datasets and five SR benchmark datasets demonstrate that our DBSR method effectively balances the number of parameters with performance, showcasing its potential for application in RSI processing.},
  archive      = {J_EAI},
  author       = {Tianjun Tang and Yuheng Ren and Shuwan Feng},
  doi          = {10.1177/30504554241311178},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {217-237},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Dual-branch super-resolution (DBSR): Lightweight DBSR network for enhancing remote sensing images},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learned lossless image compression based on optimal kernel transformer approach. <em>EAI</em>, <em>38</em>(2), 198-216. (<a href='https://doi.org/10.1177/30504554241311177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image compression is crucial for reducing storage and transmission costs, particularly in applications involving high-resolution and complex imagery. Traditional compression methods, such as JPEG, PNG, and newer lossless formats like JPEG XL and WebP, often suffer from suboptimal compression ratios (CRs) and image quality when handling modern high-definition content. To overcome these limitations, this paper proposes a novel deep learning-based lossless image compression method. The symmetrical transformer (STF) model is introduced, integrating transformer blocks in both the downsampling encoder and upsampling decoder to enhance the capture of local and global features. The model also includes a multivariate mixture distribution channel conditioning (MMCC) entropy model, which improves pixel dependency predictions by modeling complex relationships within image channels. Additionally, an automated searching of optimal kernel shapes (SOKS) is employed to dynamically configure kernel sizes, optimizing the convolutional layers for different image regions. The system also applies stripe-wise pruning (SWP), which selectively prunes unimportant features during compression, reducing computational complexity and memory usage without compromising image quality. Extensive evaluations on standard datasets, including Kodak, CLIC, and DIV2K, demonstrate the effectiveness of the proposed model. Specifically, the approach achieves significant compression efficiency, with bits-per-dimension (BPD) values of 3.06 on Kodak, 3.91 on CLIC, and 3.63 on DIV2K, outperforming traditional methods such as iVPF (3.20 BPD on Kodak), GOLLIC (3.15 BPD on Kodak), and MSPSM (3.12 BPD on Kodak). In addition to compression efficiency, the proposed method excels in inference speed, with encoding times as low as 7.80 ms per sample on Kodak, significantly faster than competing methods. These results demonstrate a substantial improvement in compression rates and image reconstruction quality, highlighting the model's potential for real-world applications, including medical imaging, remote sensing, and real-time streaming, offering a significant advancement in lossless image compression.},
  archive      = {J_EAI},
  author       = {Pitchumony Sherly Kanaga Priya and Chellam Helen Sulochana},
  doi          = {10.1177/30504554241311177},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {198-216},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Learned lossless image compression based on optimal kernel transformer approach},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Triangle mesh reconstruction by fusion of residual graph convolution and edge contraction pooling. <em>EAI</em>, <em>38</em>(2), 181-197. (<a href='https://doi.org/10.1177/30504554241301393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle meshes are a crucial and powerful data type for three-dimensional (3D) shapes, extensively studied in the fields of computer vision and computer graphics. In this paper, we delve into the challenge of analyzing deforming 3D triangle meshes using deep neural networks. While existing methods extend graph-based deep learning to 3D triangle meshes using graph convolution, the lack of effective graph convolutional structures and pooling operations limits the learning capacity of their networks. We propose a variational autoencoder structure that integrates graph convolutional residual blocks with multilayer pooling to explore the latent space of 3D shapes for generation. This framework introduces graph convolutional residual blocks to address the issue of gradient vanishing in deep networks. By employing multilayer pooling and unpooling structures based on triangle mesh simplification, gradually reducing the spatial dimensions of the input, the model can extract more general features. This enables it to handle denser mesh models effectively. Extensive experiments demonstrate that our generalized framework can learn reasonable representations of deformable shape collections with minimal training examples. It produces competitive results across various applications, including shape generation and interpolation, requiring fewer training samples and outperforming state-of-the-art techniques.},
  archive      = {J_EAI},
  author       = {Yang Ding and Huamin Yang and Cheng Han and Xinhui Zhong},
  doi          = {10.1177/30504554241301393},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {181-197},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Triangle mesh reconstruction by fusion of residual graph convolution and edge contraction pooling},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-stream feature extraction and multiscale attention generative adversarial network (BM-GAN): Colorization of GrayScale images based on bi-stream feature fusion and multiscale attention generative adversarial network. <em>EAI</em>, <em>38</em>(2), 159-180. (<a href='https://doi.org/10.1177/30504554241297613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image colorization is one of the core issues in computer vision that has attracted significant attention in recent years. Colorization technique improves the human eye’s ability to recognize grayscale images and understand scenes, particularly in low-light-level (LLL) images. However, current colorization methods still face issues, such as semantic confusion, color bleeding, and loss of image details. To address these issues, a bi-stream feature extraction and multiscale attention generative adversarial network (BM-GAN) is proposed. The bi-stream feature extraction block combines global and local features extracted from two parallel encoders. This combination improves the ability of the network to extract deep features from images. The multiscale attention block enhances key features related to the colorization target across channels and spatial dimensions. This results in higher-quality color images. The proposed method is evaluated on ImageNet, Summer2winter validation set, and LLL images. Experimental results show that BM-GAN reduces the feature-aware evaluation metrics learned perceptual image patch similarity and Fréchet inception distance by 5.2% and 7.5%, respectively.},
  archive      = {J_EAI},
  author       = {Xiaoning Gao and Liju Yin and Yulin Deng and Feng Wang and Yiming Qin and Meng zhang},
  doi          = {10.1177/30504554241297613},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {159-180},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Bi-stream feature extraction and multiscale attention generative adversarial network (BM-GAN): Colorization of GrayScale images based on bi-stream feature fusion and multiscale attention generative adversarial network},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A haze intensity sensing and texture enhanced transformer for image dehazing. <em>EAI</em>, <em>38</em>(2), 145-158. (<a href='https://doi.org/10.1177/30504554241296445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The degradation of images captured in hazy weather can severely affect practical applications. However, most existing learning-based methods ignore the varied haze distribution in an image, resulting in incomplete dehazing of some areas. Also, the presence of haze can blur the textures and details, which will heavily impact the subsequent image processing. In this paper, we propose a transformer-based framework for dehazing tasks called HITFormer. Firstly, we introduce a texture recovery and enhance module as a preprocess to strengthen details. Then, we propose an adaptive haze intensity prediction subnet to predict the haze intensity of different areas. Lastly, we use a semantic-based luminance and chrominance adjustment module to fuse the feature maps in YUV color space and form a transformation coefficient to get a recovery image. The extensive experiments demonstrate that our HITFormer achieves state-of-the-art performance on several image dehazing datasets.},
  archive      = {J_EAI},
  author       = {Tianli Zhang and Hao Zeng},
  doi          = {10.1177/30504554241296445},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {145-158},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A haze intensity sensing and texture enhanced transformer for image dehazing},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bateson-inspired model for the generation of semantic concepts from sensory data. <em>EAI</em>, <em>38</em>(2), 113-144. (<a href='https://doi.org/10.1177/30504554251314334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although neural-symbolic techniques for symbol grounding in sensory data have shown significant effectiveness, they still require substantial training. This article revisits symbolic-only approaches by introducing a new algorithm for creating hierarchical concept structures from spatial sensory data. The method is based on Bateson’s idea of difference as the fundamental element of concept formation. By leveraging this principle, the algorithm extracts atomic features from raw data through basic sequential comparisons within a stream of multivariate numerical values. Experiments carried out on a set of common objects indicate that the method can successfully discriminate and assimilate categories as needed without training. The results show that the model improves on the neural networks it has been tested against, which required more than 400 training examples for the same task. The results also show that the model can generate rich conceptual structures and human-like representations, which (i) facilitate high composability, (ii) support formal reasoning, (iii) inherently enable generalisation, and (iv) possess potential for generative behaviour. Consequently, this approach offers a compelling contribution to symbol grounding and neural-symbolic research by providing a seamless algorithm to bridge perception and conceptual knowledge.},
  archive      = {J_EAI},
  author       = {Jaime de Miguel Rodríguez and Fernando Sancho Caparrini},
  doi          = {10.1177/30504554251314334},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {5},
  number       = {2},
  pages        = {113-144},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A bateson-inspired model for the generation of semantic concepts from sensory data},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional diffusion model for missing value imputation. <em>EAI</em>, <em>38</em>(1), 96-109. (<a href='https://doi.org/10.1177/30504554241311182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid proliferation of social data, the prevalence of missing values has become increasingly common. Various factors, including human error and machine failure, contribute to the emergence of missing values in datasets. Datasets containing missing values not only consume storage space but also pose a significant obstacle to direct utilization, resulting in substantial resource wastage. Consequently, accurately imputing missing values has emerged as a focal point in research. Generative missing value imputation methods, leveraging generative models, have demonstrated notable efficacy in recent years by directly generating values for missing components based on observable data values. This paper introduces a novel generative method for missing value imputation based on a diffusion denoising model, termed the conditional diffusion model for missing value imputation (CDMVI). Specifically, CDMVI trains a conditional diffusion model using complete data samples (samples devoid of missing values) and subsequently utilizes the trained model to impute missing values in datasets. During the training stage (i.e., the forward process of the diffusion model), a subset of features is randomly selected from complete data samples, and varying levels of random noise are introduced as condition inputs to the noise predictor within the diffusion model. In the imputation stage (i.e., the backward process of the diffusion model), the missing segments of the data are initially replaced with random noise, serving as a guide for the diffusion model to generate complete samples. Experimental evaluations across multiple datasets demonstrate the competitive performance of our proposed CDMVI method.},
  archive      = {J_EAI},
  author       = {Binyi Li and Long Long and Xi Zuo and Long Chen},
  doi          = {10.1177/30504554241311182},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {96-109},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Conditional diffusion model for missing value imputation},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image manipulation detection based on irrelevant information suppression and critical information enhancement. <em>EAI</em>, <em>38</em>(1), 79-95. (<a href='https://doi.org/10.1177/30504554241301395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image manipulation detection localization task differs from traditional computer vision tasks in that we focus more on capturing subtle and generic manipulation detection features in images. In this paper, we propose a novel method called irrelevant visual information suppression, which aims to alleviate the interference of irrelevant visual information in images on manipulation detection feature extraction, thereby obtaining generic manipulation traces that are more subtle and unrelated to semantic visual information. In general, most manipulation operations leave traces at manipulation edges. Therefore, we introduce a specially designed manipulated edge information enhancement branch aimed at identifying these edge artifacts more accurately. We construct a dual-branch network, where each branch uses ResNet-50 as the backbone to capture as many multi-scale manipulation features as possible. Finally, we adopt a multi-view feature learning method that combines the manipulated edge information enhancement branch with the irrelevant visual information suppression branch and is trained with multi-scale (pixel/edge/image/irrelevant visual information suppression) supervision. To validate the effectiveness of the proposed method, we conducted extensive experiments using five image manipulation localization datasets, including CASIAv1, CASIAv2, COVER, Columbia, and NIST16. The experimental results demonstrate that our proposed method can outperform state-of-the-art methods by a significant margin in terms of F1 score. Taking CASIAv1, COVER, and Columbia datasets as examples, compared with MVSS-Net published in ICCV 2021, our method has improved F1 scores by 7.1%, 6.3%, and 12.5%, respectively. The code used in this paper can be found at the following URL: https://github.com/ginwins/ISIE-Net .},
  archive      = {J_EAI},
  author       = {Yunxue Shao and Tingting Wang and Lingfeng Wang},
  doi          = {10.1177/30504554241301395},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {79-95},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Image manipulation detection based on irrelevant information suppression and critical information enhancement},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised temporal action localization with contrastive learning-based action salience network. <em>EAI</em>, <em>38</em>(1), 64-78. (<a href='https://doi.org/10.1177/30504554241301394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the wide application of weakly supervised temporal action localization (WTAL) technology has accelerated the efficiency of video analysis. However, this domain continues to confront numerous challenges, especially due to the lack of precise temporal annotations. Consequently, this technique becomes highly susceptible to contextual background noise and overly reliant on prominent action segments, leading to less-than-ideal action localization. To alleviate this problem, we propose the contrastive learning-based action salience network (CLASNet), comprising two pivotal modules: feature contrast separation module (FCSM) and boundary refinement module (BRM). FCSM utilizes a contrastive learning approach to effectively separate action features from background features, thereby enhancing the discriminability of features. Concurrently, BRM introduces boundary refinement loss to rectify the temporal boundaries of actions, thereby further elevating the precision of temporal localization. The collaborative functioning of these two key modules effectively resolves the ambiguity issues in temporal action localization under weak supervision, markedly enhancing localization accuracy. Furthermore, CLASNet is versatile and can be integrated into different WTAL frameworks, achieving enhanced localization performance while preserving the original end-to-end training manner. Utilizing three large-scale benchmark action localization datasets, THUMOS14, ActivityNet v1.2, and ActivityNet v1.3, we embed CLASNet into various cutting-edge weakly supervised temporal action localization methods, such as CO2-Net, DELU, and ACRNet, for empirical substantiation. The experimental outcomes reveal that CLASNet significantly enhances the efficacy of these methods in action localization, offering novel perspectives for the advancement of temporal action localization technology.},
  archive      = {J_EAI},
  author       = {Sun Jingtao and Shi Weipeng and Hao Shaoyang and Wang Jialin},
  doi          = {10.1177/30504554241301394},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {64-78},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Weakly supervised temporal action localization with contrastive learning-based action salience network},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel news recommendation model with knowledge enhancement and stability. <em>EAI</em>, <em>38</em>(1), 50-63. (<a href='https://doi.org/10.1177/30504554241301391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of information technology, today’s society has higher and higher requirements for the recommendation system, especially regarding recommendation accuracy. A significant feature of news recommendation is that it has high timeliness, and the popularity of a news article will decline exponentially in a week. The effectiveness of traditional recommendation methods in news recommendation could be more optimistic. In order to further improve the accuracy of news recommendations, a large number of knowledge graphs are applied to news recommendations, and the nodes and edges of the knowledge graph can better represent the relationship between entities in the article; compared with traditional recommendation methods, it can better solve the problems of data sparsity and cold start. This paper proposes a relational entity credibility discrimination model, eliminating the relational entities without credibility to improve news recommendations accuracy, the existence of some relational entities in the triad of the knowledge graph may distort the meaning of the article or have a near-zero impact on the article, which is considered untrustworthy for these two types of relational entities. Experimental results show the effectiveness and efficiency of the model.},
  archive      = {J_EAI},
  author       = {Huajing Huang and Yongquan Fan and Linsen Li and Jiabao Chen and Tianyi Zhou},
  doi          = {10.1177/30504554241301391},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {50-63},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A novel news recommendation model with knowledge enhancement and stability},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Registration and convolutional multidimensional attention module for few-shot anomaly detection. <em>EAI</em>, <em>38</em>(1), 35-49. (<a href='https://doi.org/10.1177/30504554241297616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the shortcomings of most current anomaly detection models, such as low detection accuracy and poor generalization performance, this paper proposes a few-shot anomaly detection model based on a convolutional multidimensional attention module to achieve feature registration (abbreviated as RCM-FSAD), which enhances the model’s perception of the overall image perception ability, using spatial transformer network to obtain the spatial transformation features of the image, improving the sensitivity of the relevant features, so that the whole model learns the commonality between the categories, and enhancing the generalization ability of the model. The spatial transformations and local structures of the input data are captured by deformable convolutional networks v2 to ensure the spatial invariance of the input data. The model is trained with only normal samples to accomplish anomalous regions’ localization and anomaly detection. On the challenging MVTec AD dataset, the unsupervised model not only improves the anomaly detection accuracy but also shows better generalization compared to current state-of-the-art unsupervised anomaly detection methods.},
  archive      = {J_EAI},
  author       = {Xin Xie and Shenping Xiong and Wenbin Zheng and Tijian Cai},
  doi          = {10.1177/30504554241297616},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {35-49},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Registration and convolutional multidimensional attention module for few-shot anomaly detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-lingual English–Urdu semantic word similarity using sentence transformers. <em>EAI</em>, <em>38</em>(1), 21-34. (<a href='https://doi.org/10.1177/30504554241297614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic word similarity is a quantitative method of determining how much two terms are contextually identical, which is a considerable challenge for computational linguistics. The research community has examined a range of approaches to address this issue. However, most of these approaches are for a comparatively limited set of languages, especially English. Research on semantic word similarity for South Asian languages, particularly Urdu, is immature. In recent years, transformer-based approaches have proved extremely successful for a range of language processing tasks. The primary aim of this study is to develop and compare a variety of transformer-based approaches to the cross-lingual English–Urdu semantic word similarity task. This study evaluated a publicly available benchmark USWS-19 corpus that comprises 518 word pairs. This study mainly explored four types of transformer-based approaches: (a) cross-lingual sentence transformer-based approaches using the original dataset, (b) cross-lingual sentence transformer-based approaches using the translated dataset (translation plus monolingual analysis [T+MA] approach), (c) the feature fusion approach (mixture of features), and large language models. In addition, this study also explores the word embedding-based approach using the translated dataset (T+MA approach). In total, this study developed 29 transformer-based models, with the highest results (Pearson correlation = 0.788) achieved using a feature fusion approach, that is, Best-Two-SBERT (where SBERT stands for sentence-bidirectional encoder representations from transformers; using T+MA) + BEST Baseline (with Bing translator) + Best cross-lingual SBERT. This approach improved by 7% over previously reported results on the same corpus.},
  archive      = {J_EAI},
  author       = {Iqra Muneer and Ali Saeed and Rao Muhammad Adeel Nawab},
  doi          = {10.1177/30504554241297614},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {21-34},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Cross-lingual English–Urdu semantic word similarity using sentence transformers},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The 12th IJCAR automated theorem proving system Competition—CASC-j12. <em>EAI</em>, <em>38</em>(1), 3-20. (<a href='https://doi.org/10.1177/30504554241305110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CADE ATP System Competition (CASC) is the annual evaluation of fully automatic, classical logic, automated theorem proving (ATP) systems—the world championship for such systems. CASC-J12 was the 29th competition in the CASC series. Nineteen ATP systems competed in the various divisions. This paper presents an outline of the competition design and a commentated summary of the results.},
  archive      = {J_EAI},
  author       = {Geoff Sutcliffe},
  doi          = {10.1177/30504554241305110},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {3-20},
  shortjournal = {Eur. Artif. Intell.},
  title        = {The 12th IJCAR automated theorem proving system Competition—CASC-j12},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
