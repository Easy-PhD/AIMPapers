<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SII</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sii">SII - 20</h2>
<ul>
<li><details>
<summary>
(2025). Bayesian variable selection for semiparametric zero-inflated longitudinal count data in the presence of non-ignorable missingness. <em>SII</em>, <em>18</em>(4), 569-582. (<a href='https://doi.org/10.4310/SII.250515195908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the issue of zero-inflated longitudinal data in the presence of non-ignorable missingness. To account for the presence of additional zeros, a zero-inflated power series mixed effects model is utilized. This model incorporates spline functions, such as B-spline, to capture the non-linear impact of time on the longitudinal outcome. Also, missing data is a common problem in longitudinal studies. To address this issue, a shared random effects model is utilized. Further, the proposed model addresses a variable selection technique using Dirac spike priors. The MCMC method is used for posterior sampling, which involves the Metropolis-Hastings algorithm within Gibbs sampling. Some simulation studies are performed to investigate the performance of the proposed approach, and it is also applied to analyze a real dataset in a RAND health insurance experiment.},
  archive      = {J_SII},
  author       = {Nawar, Alsalim and Taban, Baghfalaki and Mojtaba, Ganjali and Sultan, Salem},
  doi          = {10.4310/SII.250515195908},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {569-582},
  shortjournal = {Stat. Interface},
  title        = {Bayesian variable selection for semiparametric zero-inflated longitudinal count data in the presence of non-ignorable missingness},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sequential estimation with dorfman and halving pooling. <em>SII</em>, <em>18</em>(4), 551-567. (<a href='https://doi.org/10.4310/SII.250522055246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group testing has recently shown great potential to screen for infectious diseases in a large population with low prevalence, which can considerably reduce costs compared to testing specimens individually. However, a common problem encountered in most existing studies is that it is impractical to obtain additional massive specimens to increase the precision of the estimates. This paper establishes a mixture model while considering continuous biomarker levels (e.g., antibody concentrations) from group samples, which adopts different grouping strategies that incorporate retesting results, Dorfman and Halving, for estimation. We demonstrate the effectiveness of our proposal through several simulation cases, and with application to the plasma glucose data collected from the National Health and Nutrition Examination Survey.},
  archive      = {J_SII},
  author       = {Zhong, Yunning and Wu, Tao and Wei, Hongyu and Chen, Lifei},
  doi          = {10.4310/SII.250522055246},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {551-567},
  shortjournal = {Stat. Interface},
  title        = {A sequential estimation with dorfman and halving pooling},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multivariate robust integrated test for genetic pleiotropy. <em>SII</em>, <em>18</em>(4), 539-550. (<a href='https://doi.org/10.4310/SII.250522054924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic pleiotropy means that a single gene influences more than one trait and the detecting of the genetic pleiotropy can help the biological understanding of a gene among others. Although several multivariate test procedures have been proposed for the problem in the literature, most of them apply only to the situation where the error follows a multivariate normal distribution. In other words, they cannot detect the pleiotropy if the data contain outliers or the error follows a heavy-tailed distribution. To overcome these difficulties, we propose a new multivariate robust integrated test and establish its theoretical properties. An extensive simulation study is conducted to illustrate the efficiency and robustness of the proposled procedure. In particular, it suggests that the method can reach high power and control the asymptotic type I error well.},
  archive      = {J_SII},
  author       = {Huang, Qiyue and Tong, Xingwei and Sun, Jianguo},
  doi          = {10.4310/SII.250522054924},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {539-550},
  shortjournal = {Stat. Interface},
  title        = {A multivariate robust integrated test for genetic pleiotropy},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel subsampling strategies for heavily censored reliability data. <em>SII</em>, <em>18</em>(4), 511-538. (<a href='https://doi.org/10.4310/SII.250522054738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational capability often falls short when confronted with massive data, posing a common challenge in establishing a statistical model or statistical inference method dealing with big data. While subsampling techniques have been extensively developed to downsize the data volume, there is a notable gap in addressing the unique challenge of handling extensive reliability data, in which a common situation is that a large proportion of data is censored. In this article, we propose an efficient subsampling method for reliability analysis in the presence of censoring data, intending to estimate the parameters of lifetime distribution. Moreover, a novel subsampling method for subsampling from severely censored data is proposed, i.e., only a tiny proportion of data is complete. The subsamplingbased estimators are given, and their asymptotic properties are derived. The optimal subsampling probabilities are derived through the L-optimality criterion, which minimizes the trace of the product of the asymptotic covariance matrix and a constant matrix. Efficient algorithms are proposed to implement the proposed subsampling methods to address the challenge that optimal subsampling strategy depends on unknown parameter estimation from full data. Real-world hard drive dataset case and simulative empirical studies are employed to demonstrate the superior performance of the proposed methods.},
  archive      = {J_SII},
  author       = {Ruan, Yixiao and Li, Zan and Li, Zhaohui and Lin, Dennis K. J. and Hu, Qingpei and Yu, Dan},
  doi          = {10.4310/SII.250522054738},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {511-538},
  shortjournal = {Stat. Interface},
  title        = {Novel subsampling strategies for heavily censored reliability data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new gamma frailty joint modeling approach for recurrent event and terminal event. <em>SII</em>, <em>18</em>(4), 497-509. (<a href='https://doi.org/10.4310/SII.250522053733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new joint gamma frailty modeling approach for recurrent event data with informative terminal event by adopting a new type of double exponential Cox model to the terminal event. The proposed model overcomes the drawback that the marginal effects of covariates die out over time in gamma frailty Cox models. A sieve maximum likelihood approach is carried out for parameter estimation, and the Bernstein polynomials are employed to approximate the non-decreasing cumulative baseline functions. The EM algorithm is utilized for optimization. Asymptotic properties of the estimators are provided. Simulation studies are conducted to evaluate the finite sample behavior of the proposed estimators. A real dataset of readmissions of patients diagnosed with colorectal cancer is analyzed for illustration.},
  archive      = {J_SII},
  author       = {Zhou, Jie and Xie, Mengqi and Liu, Lei},
  doi          = {10.4310/SII.250522053733},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {497-509},
  shortjournal = {Stat. Interface},
  title        = {A new gamma frailty joint modeling approach for recurrent event and terminal event},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture of partially functional linear regression. <em>SII</em>, <em>18</em>(4), 487-496. (<a href='https://doi.org/10.4310/SII.250522053330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces a new mixture of partially functional linear models to characterize the relationship between a scalar response and mixture predictors of functional and covariate vector. The mixing proportions are allowed to change with a covariate, enhancing the flexibility of the proposed model. Utilizing basis function expansion, we develop a modified backfitting EM algorithm to estimate the regression functions. Furthermore, based on Fisher's method, a maximum form test statistics, combining the p -values of each component of mixtures, is proposed for depicting whether the mixing proportions actually depend on the covariate. The asymptotic properties of the resulting estimators are established under mild conditions. To assess the finite sample performance of the estimators, several simulation studies are conducted. Finally, we analyze the motivating datasets to illustrate the powerfulness of the proposed procedure.},
  archive      = {J_SII},
  author       = {Ren, Pengcheng and Yan, Xingyu and Zhao, Peng},
  doi          = {10.4310/SII.250522053330},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {487-496},
  shortjournal = {Stat. Interface},
  title        = {Mixture of partially functional linear regression},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subgroup analysis and homogeneous effect selection in longitudinal data. <em>SII</em>, <em>18</em>(4), 459-486. (<a href='https://doi.org/10.4310/SII.250522013545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a penalized regression approach to simultaneously identify subgroups and homogeneity structures in longitudinal data analysis. Unlike conventional subgroup models, the proposed approach can partition all the effects into homogeneous effects shared by all individuals and heterogeneous effects that have group structures, without requiring prior knowledge to differentiate between heterogeneous and homogeneous effects. Theoretically, we show that the group structure and the homogeneous patterns can be recovered with probability approaching one. An efficient algorithm based on the alternating direction method of multipliers (ADMM) algorithm is developed for computational feasibility. Simulation studies demonstrate the effectiveness of the proposed approach with finite samples, and a real data example is provided for illustration.},
  archive      = {J_SII},
  author       = {Wang, Han and Wu, Jiaqi and Zhang, Weiping},
  doi          = {10.4310/SII.250522013545},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {459-486},
  shortjournal = {Stat. Interface},
  title        = {Subgroup analysis and homogeneous effect selection in longitudinal data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional bayesian mediation analysis with adaptive laplace priors. <em>SII</em>, <em>18</em>(4), 445-457. (<a href='https://doi.org/10.4310/SII.250521220937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mediation analysis method is used to investigate effects of mediators that intervene in the pathways between an exposure variable and an outcome variable. Bayesian methods are naturally used in mediation analysis due to the hierarchical structure of Bayesian models. This paper introduces an innovative adaptive Bayesian mediation analysis method that incorporates adaptive Laplace priors into the predictive model to account for high-dimensional mediators. This approach introduces a penalization function on the estimated direct and indirect effects rather than solely on the coefficients of predictive models. Consequently, estimated effects that lack statistical significance may shrink to zero, facilitating a more robust analysis. We demonstrate the efficacy of our adaptive mediation analysis method on simulations and on a Louisiana triple negative breast cancer (TNBC) dataset to examine racial disparity in diagnosed stage among TNBC patients diagnosed between 2010 and 2017. The dataset is linked to the 2017 hazardous air pollutant emissions burden estimation database using patients' residential address. We effectively explain a portion of the disparity using currently collected variables. The analysis identifies crucial mediators and confounders, highlighting the significance of variables such as age of diagnosis, insurance status, tumor grades, and the concentration of Naphtha in the air.},
  archive      = {J_SII},
  author       = {Yu, Qingzhao and Hagan, Joseph and Wu, Xiaocheng and Richmond-Bryant, Jennifer and Urbanek, Norman and Li, Bin},
  doi          = {10.4310/SII.250521220937},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {445-457},
  shortjournal = {Stat. Interface},
  title        = {High-dimensional bayesian mediation analysis with adaptive laplace priors},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence intervals for ratios of proportions in stratified bilateral correlated data. <em>SII</em>, <em>18</em>(4), 411-443. (<a href='https://doi.org/10.4310/SII.250522013240'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidence interval (CI) methods for stratified bilateral studies use intraclass correlation to avoid misleading results. In this article, we propose four CI methods (sample-size weighted global MLE-based Wald-type CI, complete MLE-based Wald-type CI, profile likelihood CI, and complete MLE-based score CI) to investigate CIs of proportion ratios to clinical trial design with stratified bilateral data under Dallal's intraclass model. Monte Carlo simulations are performed, and the complete MLE-based score confidence interval (CS) method yields a robust outcome. Lastly, two real data examples are conducted to illustrate the proposed four CIs.},
  archive      = {J_SII},
  author       = {Tian, Wanqing and Ma, Chang-Xing},
  doi          = {10.4310/SII.250522013240},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {411-443},
  shortjournal = {Stat. Interface},
  title        = {Confidence intervals for ratios of proportions in stratified bilateral correlated data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic tests for serial correlation and ARCH effect of high-dimensional time series. <em>SII</em>, <em>18</em>(4), 399-410. (<a href='https://doi.org/10.4310/SII.250522012841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a norm-rank-based automatic test for detecting serial correlation and ARCH effect in high-dimensional time series (HDTS). The proposed automatic test is based on the Spearman's rank autocorrelations of the L u -norm of the HDTS up to lag m , where the values of u and m are chosen by a completely data-driven method. The asymptotic null distribution of this automatic test is established without assuming any moment condition of HDTS, so this automatic test has a large application scope covering the commonly observed heavy-tailed data. To account for the possible scaling effect, another standardized norm-rankbased automatic test is further proposed. Simulations and one real example are given to demonstrate the advantage of these two automatic tests over the portmanteau tests, which seek the rejection evidence only from the L 1 -norm of HDTS, perform unstably across the user-chosen value of m , and have unsatisfactory power for small sample sizes.},
  archive      = {J_SII},
  author       = {Zhang, Bingbing and Liu, Mengya and Yan, Ting and Zhu, Ke},
  doi          = {10.4310/SII.250522012841},
  journal      = {Statistics and Its Interface},
  month        = {5},
  number       = {4},
  pages        = {399-410},
  shortjournal = {Stat. Interface},
  title        = {Automatic tests for serial correlation and ARCH effect of high-dimensional time series},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Poisson inverse regression for sufficient dimension reduction in text data. <em>SII</em>, <em>18</em>(3), 389-397. (<a href='https://doi.org/10.4310/SII.250118023912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we present a sufficient dimension reduction (SDR) method for the analysis of text data. We use Poisson distribution to model word occurrences and therefore the method is called Poisson Inverse Regression. We provide the motivation for our example and adjust previously developed algorithms for sufficient dimension reduction for predictors from an exponential family distributions to develop our method. We demonstrate the validity of the proposed method using numerical experiments.},
  archive      = {J_SII},
  author       = {Gaba, Amarjit and Artemiou, Andreas},
  doi          = {10.4310/SII.250118023912},
  journal      = {Statistics and Its Interface},
  month        = {1},
  number       = {3},
  pages        = {389-397},
  shortjournal = {Stat. Interface},
  title        = {Poisson inverse regression for sufficient dimension reduction in text data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous treatment effect on survival endpoint in clinical trials via restricted mean survival time. <em>SII</em>, <em>18</em>(3), 379-387. (<a href='https://doi.org/10.4310/SII.250118022051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-to-event endpoints are common in randomized clinical trials. Compared to hazard ratio, the restricted mean survival time (RMST) has more straightforward clinical interpretation, its definition is free of model assumption, and it allows the evaluation of treatment effect that is heterogeneous across participants. We assess the heterogeneous causal effect of treatment on RMST through a nonparametric regression with dimension reduction. This approach allows simultaneous identification of the “baseline score”, a linear combination of baseline characteristics that determines the potential benefit an individual can receive from the treatment as compared to the control, and the estimation of treatment benefit. In the motivating example of antiretroviral regimens against HIV, the proposed estimation indicates the experimental regimen is not universally more effective than the standard regimen in speeding up viral suppression but its benefit depends on a patient’s baseline medical condition. The baseline score uncovered from the study may assist regimen decision in a future patient.},
  archive      = {J_SII},
  author       = {Hu, Zonghui and Zhang, Zhiwei},
  doi          = {10.4310/SII.250118022051},
  journal      = {Statistics and Its Interface},
  month        = {1},
  number       = {3},
  pages        = {379-387},
  shortjournal = {Stat. Interface},
  title        = {Heterogeneous treatment effect on survival endpoint in clinical trials via restricted mean survival time},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal subsampling proportional subdistribution hazards regression with rare events in big data. <em>SII</em>, <em>18</em>(3), 361-377. (<a href='https://doi.org/10.4310/SII.250118022821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proportional subdistribution hazards (PSH) model has been widely employed for analyzing competing risks data which have mutually exclusive events with multiple causes and commonly occur in clinical research. With the rapid development of healthcare industry, massively sized survival data sets are becoming increasingly prevalent and classical PSH models are computationally intensive with large data sets. In this article, we propose the optimal subsampling estimators and two-step algorithm for the Fine-Gray model. Asymptotic properties of the proposed estimators are established and an extensive simulation study is conducted to demonstrate the efficiency of the estimators. Our proposed methodology is then illustrated with the large dataset from the SEER (Surveillance, Epidemiology, and End Results) database.},
  archive      = {J_SII},
  author       = {Li, Erqian and Tang, Man-Lai and Tian, Maozai and Yu, Keming},
  doi          = {10.4310/SII.250118022821},
  journal      = {Statistics and Its Interface},
  month        = {1},
  number       = {3},
  pages        = {361-377},
  shortjournal = {Stat. Interface},
  title        = {Optimal subsampling proportional subdistribution hazards regression with rare events in big data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sequential stepwise screening procedure for sparse recovery in high-dimensional multiresponse models with complex group structures. <em>SII</em>, <em>18</em>(3), 349-359. (<a href='https://doi.org/10.4310/SII.250118022546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiresponse data with complex group structures in both responses and predictors arise in many fields, while the identification of such group structures is often challenging in practice. We propose a novel algorithm called sequential stepwise screening procedure (SeSS) for sparse recovery. This algorithm encourages the grouping effect on both responses and predictors and each response group is allowed to relate to multiple predictor groups. To obtain a correct model under the complex group structures, the proposed procedure first chooses the nonzero block and the nonzero row by the canonical correlation measure (CC) and then selects the relevant entries by the extended Bayesian information Criterion (EBIC).We show that this method obtains accurate estimates in extremely sparse models and greatly reduces the computational cost. The theoretical property of SeSS is established. We conduct simulation studies and consider a real example to compare its performance with existing methods.},
  archive      = {J_SII},
  author       = {Liang, Weixiong and Yang, Yuehan},
  doi          = {10.4310/SII.250118022546},
  journal      = {Statistics and Its Interface},
  month        = {1},
  number       = {3},
  pages        = {349-359},
  shortjournal = {Stat. Interface},
  title        = {A sequential stepwise screening procedure for sparse recovery in high-dimensional multiresponse models with complex group structures},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A conditional semiparametric approach for the analysis of semi-competing risk data. <em>SII</em>, <em>18</em>(3), 333-347. (<a href='https://doi.org/10.4310/SII.250118022331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-competing risk data arise in many situations and many methods have been proposed for their analysis. However, most of the existing methods are joint modeling procedures and require the specification of the underlying correlation between the non-terminal event and terminal event. In this paper, we propose an alternative conditional approach, which is more attractive and natural if the non-terminal event is of main interest. In the proposed method, a class of flexible additive and multiplicative models and the additive hazards model are employed to model the non-terminal and terminal events, respectively. For inference, an estimating equation-based procedure is developed and the asymptotic properties of the resulting estimators are established. In addition, a model checking procedure is provided. The numerical results indicate that the proposed methodology works well in practical situations and it is applied to a real set of data that motivated this study.},
  archive      = {J_SII},
  author       = {Zheng, Tiange and Li, Yang and Sun, Jianguo},
  doi          = {10.4310/SII.250118022331},
  journal      = {Statistics and Its Interface},
  month        = {1},
  number       = {3},
  pages        = {333-347},
  shortjournal = {Stat. Interface},
  title        = {A conditional semiparametric approach for the analysis of semi-competing risk data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distributed minimum average variance estimation for sufficient dimension reduction. <em>SII</em>, <em>18</em>(3), 315-332. (<a href='https://doi.org/10.4310/SII.250118021553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimum average variance estimation (MAVE) is a powerful method to estimate the dimension reduction directions of the central mean subspace. However, when analyzing the Boeing 737 data with more than 600000 samples, the classical MAVE method would suffer from significant computational burden. To address this issue, we introduce a distributed minimum average variance estimation (distMAVE) methodology for sufficient dimension reduction by adopting a distributed computing system. The proposed distMAVE method is able to handle large datasets and make a balance between estimation accuracy and computation efficiency. The asymptotic properties of the distributed estimators are proved, and a practical algorithm is provided for implementation. In addition to the theoretical results, we also demonstrate the effectiveness of the distMAVE method through simulation studies and Boeing 737 dataset.},
  archive      = {J_SII},
  author       = {Dai, Shuang and Ming, Jingsi and Yu, Zhou},
  doi          = {10.4310/SII.250118021553},
  journal      = {Statistics and Its Interface},
  month        = {1},
  number       = {3},
  pages        = {315-332},
  shortjournal = {Stat. Interface},
  title        = {A distributed minimum average variance estimation for sufficient dimension reduction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-sample test for stochastic block models via maximum entry-wise deviation. <em>SII</em>, <em>18</em>(3), 299-313. (<a href='https://doi.org/10.4310/SII.250118021110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stochastic block model is a popular tool for detecting community structures in network data. Detecting the difference between two community structures is an important issue for stochastic block models. However, the two-sample test has been a largely under-explored domain, and too little work has been devoted to it. In this article, based on the maximum entry-wise deviation of the two centered and rescaled adjacency matrices, we propose a novel test statistic to test two samples of stochastic block models. We prove that the null distribution of the proposed test statistic converges in distribution to a Gumbel distribution, and we show the change of the two samples from stochastic block models can be tested via the proposed method. Then, we show that the proposed test has an asymptotic power guarantee against alternative models. One noticeable advantage of the proposed test statistic is that the number of communities can be allowed to grow linearly up to a logarithmic factor. Further, we extend the proposed method to the degreecorrected stochastic block model. Both simulation studies and real-world data examples indicate that the proposed method works well.},
  archive      = {J_SII},
  author       = {Fu, Kang and Hu, Jianwei and Keita, Seydou and Liu, Hao},
  doi          = {10.4310/SII.250118021110},
  journal      = {Statistics and Its Interface},
  month        = {1},
  number       = {3},
  pages        = {299-313},
  shortjournal = {Stat. Interface},
  title        = {Two-sample test for stochastic block models via maximum entry-wise deviation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composition estimation via shrinkage. <em>SII</em>, <em>18</em>(3), 291-297. (<a href='https://doi.org/10.4310/SII.250118020253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this note, we explore a simple approach to composition estimation, using penalized likelihood density estimation on a nominal discrete domain. Practical issues such as smoothing parameter selection and the effective use of prior information are investigated in simulations, and the asymptotic convergence rate is established in theoretical analysis. The practical performances also compare favorably against a few existing methods. The method has been implemented in a pair of R functions for use by practitioners.},
  archive      = {J_SII},
  author       = {Gu, Chong},
  doi          = {10.4310/SII.250118020253},
  journal      = {Statistics and Its Interface},
  month        = {1},
  number       = {3},
  pages        = {291-297},
  shortjournal = {Stat. Interface},
  title        = {Composition estimation via shrinkage},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Varying-coefficients additive hazards regression with current status data and time-dependent treatment. <em>SII</em>, <em>18</em>(3), 281-290. (<a href='https://doi.org/10.4310/SII.250118020005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses regression analysis of current status data with time-dependent treatment. We propose a varyingcoefficients additive hazards model that investigates the efficacy of a time-dependent treatment. A global partiallikelihood procedure is constructed to estimate the varying coefficient covariate effect. The asymptotic properties, including consistency and asymptotic normality, of the proposed estimators are established. Extensive simulation studies for the finite sample performance of the proposed method are conducted. The simulated results show that the proposed method is effective in practical application. A real data study is provided to illustrate the performance of the proposed method.},
  archive      = {J_SII},
  author       = {Feng, Yanqin and Zhou, Xiuqing and Ma, Shuang},
  doi          = {10.4310/SII.250118020005},
  journal      = {Statistics and Its Interface},
  month        = {1},
  number       = {3},
  pages        = {281-290},
  shortjournal = {Stat. Interface},
  title        = {Varying-coefficients additive hazards regression with current status data and time-dependent treatment},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conjugate prior to the wishart shape parameter: The generalized gamcon distributions. <em>SII</em>, <em>18</em>(3), 267-280. (<a href='https://doi.org/10.4310/SII.250118001733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of this work is to bring to light the generalized Gamcon distributions from different perspectives. We first suggest them as a new family of conjugate priors to the Wishart shape parameter. The law of the first kind presents the conjugate prior for the Wishart distribution with a known scale parameter. The generalized Gamcon distribution of the second kind confers the conjugate prior for the Wishart distribution with an unknown scale parameter. A conjugate prior to the Wishart scale parameter is also investigated to deal with the latter case. Then, we provide the Bayes estimators of the Wishart parameters. Simulation studies are devoted to the accuracy evaluation of these estimators. We finally prove that the generalized Gamcon distributions could be closely investigated to fit the Covid-19 data. The model reveals interesting results that exceed the widely considered Gaussian model on the studied data sets within this work.},
  archive      = {J_SII},
  author       = {Kessentini, Sameh and Ktari, Fatma and Zine, Raoudha},
  doi          = {10.4310/SII.250118001733},
  journal      = {Statistics and Its Interface},
  month        = {1},
  number       = {3},
  pages        = {267-280},
  shortjournal = {Stat. Interface},
  title        = {Conjugate prior to the wishart shape parameter: The generalized gamcon distributions},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
