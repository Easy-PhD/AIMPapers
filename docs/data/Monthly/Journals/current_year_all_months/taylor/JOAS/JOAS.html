<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JOAS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="joas">JOAS - 105</h2>
<ul>
<li><details>
<summary>
(2025). Statistical methods for dynamic disease screening and spatio-temporal disease surveillance. <em>JOAS</em>, <em>52</em>(12), 2354-2355. (<a href='https://doi.org/10.1080/02664763.2025.2458134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {Jong-Min Kim},
  doi          = {10.1080/02664763.2025.2458134},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2354-2355},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical methods for dynamic disease screening and spatio-temporal disease surveillance},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gene mutation estimations via mutual information and ewens sampling based CNN & machine learning algorithms. <em>JOAS</em>, <em>52</em>(12), 2321-2353. (<a href='https://doi.org/10.1080/02664763.2025.2460076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct gene mutation rate estimations via developing mutual information and Ewens sampling based convolutional neural network (CNN) and machine learning algorithms. More precisely, we develop a systematic methodology through constructing a CNN. Meanwhile, we develop two machine learning algorithms to study protein production with target gene sequences and protein structures. The core of the CNN and machine learning approach is to address a two-stage optimization problem to balance gene mutation rates during protein production. To wit, we try to optimally coordinate the consistency between the given input DNA sequences and the given (or optimally computed) target ones through controlling their intermediate gene mutation rates. The purposes in doing so are aimed to conduct gene editing and protein structure prediction. For example, after the gene mutation rates are estimated, the computing complexity of protein structure prediction will be reduced to a reasonable degree. Our developed CNN numerical optimization scheme consists of two newly designed machine learning algorithms. The stochastic gradients for the two algorithms are designed according to the Kuhn-Tucker conditions with boundary constraints and with the support of Ewens sampling, multi-input multi-output (MIMO) mutual information, and codon optimization techniques. The associated learning rate bounds are explicitly derived from the method and the two algorithms are numerically implemented. The convergence and optimality of the algorithms are mathematically proved. To illustrate the usage of our study, we also conduct a real-world data implementation.},
  archive      = {J_JOAS},
  author       = {Wanyang Dai},
  doi          = {10.1080/02664763.2025.2460076},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2321-2353},
  shortjournal = {J. Appl. Stat.},
  title        = {Gene mutation estimations via mutual information and ewens sampling based CNN & machine learning algorithms},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathway-based genetic association analysis for overdispersed count data. <em>JOAS</em>, <em>52</em>(12), 2306-2320. (<a href='https://doi.org/10.1080/02664763.2025.2460073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overdispersion is a common phenomenon in genetic data, such as gene expression count data. In genetic association studies, it is important to investigate the association between a gene expression and a set of genetic variants from a pathway. However, existing approaches for pathway analysis are primarily designed for continuous and binary outcomes and are not applicable to overdispersed count data. In this paper, we propose a hierarchical approach to analyze the association between an overdispersed count response and a set of low-frequency genetic variants in negative binomial regression. We derive score-type test statistics for both fixed and random effects of genetic variants, and further introduce a novel procedure for efficiently combining these two statistics for global testing. Through simulation studies, we demonstrate that the proposed method tends to be more powerful than existing methods under a wide range of scenarios. Additionally, we apply the proposed method to a colorectal cancer study, demonstrating its power in identifying associations between gene expression and somatic mutations.},
  archive      = {J_JOAS},
  author       = {Yang Liu},
  doi          = {10.1080/02664763.2025.2460073},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2306-2320},
  shortjournal = {J. Appl. Stat.},
  title        = {Pathway-based genetic association analysis for overdispersed count data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On use of adaptive cluster sampling for variance estimation. <em>JOAS</em>, <em>52</em>(12), 2291-2305. (<a href='https://doi.org/10.1080/02664763.2025.2460072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive cluster sampling is particularly helpful whenever the target population is unique, dispersed unevenly, concealed or difficult to find. In the current investigation, under an adaptive cluster sampling approach, we propose a ratio-product-logarithmic type estimator employing a single auxiliary variable for the estimation of finite population variance. The bias and mean square error of the proposed estimator are developed by using simulation as well as real data sets. The study results show that for estimating the finite population variance, the proposed estimator outperforms the competing estimators.},
  archive      = {J_JOAS},
  author       = {Shameem Alam and Javid Shabbir and Malaika Nadeem},
  doi          = {10.1080/02664763.2025.2460072},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2291-2305},
  shortjournal = {J. Appl. Stat.},
  title        = {On use of adaptive cluster sampling for variance estimation},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the improved estimation of the normal mixture components for longitudinal data. <em>JOAS</em>, <em>52</em>(12), 2271-2290. (<a href='https://doi.org/10.1080/02664763.2025.2459293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When analyzing real data sets, statisticians often face the question that the data are heterogeneous and it may not necessarily be possible to model this heterogeneity directly. One natural option in this case is to use the methods based on finite mixtures. The key question in these techniques often is what is the best number of mixtures or, depending on the focus of the analysis, the best number of sub-populations when the model is otherwise fixed. Moreover, when the distribution of the response variable deviates from meeting the assumptions, it's common to employ an appropriate transformation to align the distribution with the model's requirements. To solve the problem in the mixture regression context we propose a technique based on the scaled Box-Cox transformation for normal mixtures. The specific focus here is on mixture regression for longitudinal data, the so-called trajectory analysis. We present interesting practical results as well as simulation experiments to demonstrate that our method yields reasonable results. Associated R-programs are also provided.},
  archive      = {J_JOAS},
  author       = {Tapio Nummi and Jyrki Möttönen and Pasi Väkeväinen and Janne Salonen and Timothy E. O'Brien},
  doi          = {10.1080/02664763.2025.2459293},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2271-2290},
  shortjournal = {J. Appl. Stat.},
  title        = {On the improved estimation of the normal mixture components for longitudinal data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the use and misuse of time-rescaling to assess the goodness-of-fit of self-exciting temporal point processes. <em>JOAS</em>, <em>52</em>(12), 2247-2270. (<a href='https://doi.org/10.1080/02664763.2025.2459245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper first highlights important drawbacks and biases related to the common use of time-rescaling to assess the goodness-of-fit (Gof) of self-exciting temporal point process (SETPP) models. Then it presents a new predictive time-rescaling approach leading to an asymptotically unbiased Gof framework for general SETPPs in the case of single observed trajectories. The predictive approach focuses on forecasting accuracy and addresses the bias problem resulting from the plugged-in estimated parameters. Dawid's prequential approach is used and the models' checking is mainly based on the forecasting accuracy of arrival times. These times are transformed, using sequentially estimated parameters, into random vectors which are proved to converge in probability under the null hypothesis and standard regulatory conditions to vectors of iid Exponential(1) rv's. Numerical experiments are used to compare the performances of the standard and predictive time-rescaling for Gof assessment of non-homogeneous Poisson and Hawkes self-exciting temporal processes. Data of Japanese seismic events are also used to illustrate the dynamic aspect of the proposed model-checking approach.},
  archive      = {J_JOAS},
  author       = {M.-A. El-Aroui},
  doi          = {10.1080/02664763.2025.2459245},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2247-2270},
  shortjournal = {J. Appl. Stat.},
  title        = {On the use and misuse of time-rescaling to assess the goodness-of-fit of self-exciting temporal point processes},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parametric estimation of quantile versions of zenga and d inequality curves: Methodology and application to weibull distribution. <em>JOAS</em>, <em>52</em>(12), 2226-2246. (<a href='https://doi.org/10.1080/02664763.2025.2458126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inequality (concentration) curves such as Lorenz, Bonferroni, Zenga curves, as well as a new inequality curve – the D curve, are broadly used to analyse inequalities in wealth and income distribution in certain populations. Quantile versions of these inequality curves are more robust to outliers. We discuss several parametric estimators of quantile versions of the Zenga and D curves. A minimum distance (MD) estimator is proposed for these two curves and the indices related to them. The consistency and asymptotic normality of the MD estimator is proved. The MD estimator can also be used to estimate the inequality measures corresponding to the quantile versions of the inequality curves. The estimation methods considered are illustrated in the case of the Weibull model, which has many applications in life sciences, for example, to fit the precipitation data. In econometrics it is also considered to fit incomes, especially in the case when a significant share of population have low incomes, for example, in less developed countries or among low-paid jobs.},
  archive      = {J_JOAS},
  author       = {Sylwester Pia̧tek},
  doi          = {10.1080/02664763.2025.2458126},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2226-2246},
  shortjournal = {J. Appl. Stat.},
  title        = {Parametric estimation of quantile versions of zenga and d inequality curves: Methodology and application to weibull distribution},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Derivation of a multivariate longitudinal causal effects model. <em>JOAS</em>, <em>52</em>(12), 2207-2225. (<a href='https://doi.org/10.1080/02664763.2025.2457013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a causal inference estimation method for longitudinal observational studies with multiple outcomes. The method uses marginal structural models with inverse probability treatment weights (MSM-IPTWs). In developing the proposed method, we re-define the weights as a product of inverse weights at each time point, accounting for time-varying confounders and treatment exposures and possible correlation between and within (serial) the multiple outcomes. The proposed method is evaluated by simulation studies and with an application to estimate the effect of HIV positivity awareness on condom use and multiple sexual partners using the Malawi Longitudinal Study of Families and Health (MLSFH) data. The simulation study shows that the joint MSM-IPTW performs well with coverage within the expected 95% level for a large sample size ( n = 1000) and moderate to strong between and within outcome correlation strength ( ρ j = 0.3 , 0.75, ρ k = 0.4 , 0.8) when the effects are similar. The joint MSM-IPTW performed relatively the same as the adjusted standard joint model when the treatment effect estimate was the same for the outcomes. In the application, HIV positivity awareness increased the usage of condoms and did not affect the number of sexual partners. We recommend using the proposed MSM-IPTWs to correctly control for time-varying treatment and confounders when estimating causal effects for longitudinal observational studies with multiple outcomes.},
  archive      = {J_JOAS},
  author       = {Halima S. Twabi and Samuel O. M. Manda and Dylan S. Small and Hans-Peter Kohler},
  doi          = {10.1080/02664763.2025.2457013},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2207-2225},
  shortjournal = {J. Appl. Stat.},
  title        = {Derivation of a multivariate longitudinal causal effects model},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized functional regression using r package PFLR. <em>JOAS</em>, <em>52</em>(11), 2191-2205. (<a href='https://doi.org/10.1080/02664763.2025.2457011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Penalized functional regression is a useful tool to estimate models for applications where the effect/coefficient function is assumed to be truncated. The truncated coefficient function occurs when the functional predictor does not influence the response after a certain cutoff point on the time domain. The R package PFLR offers an extensive suite of methods for advanced functional regression techniques with penalization. The package implements four distinct methods, each tailored to different models, effectively addressing a range of scenarios. This is demonstrated through simulations as well as an application to particulate matter emissions data. Generic S3 methods are also implemented for each model to help with summary, visualization and interpretation.},
  archive      = {J_JOAS},
  author       = {Rob Cameron and Tianyu Guan and Haolun Shi and Zhenhua Lin},
  doi          = {10.1080/02664763.2025.2457011},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2191-2205},
  shortjournal = {J. Appl. Stat.},
  title        = {Penalized functional regression using r package PFLR},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient test to assess homogeneity of probabilities in discrete-time transition models with application in agricultural science data. <em>JOAS</em>, <em>52</em>(11), 2172-2190. (<a href='https://doi.org/10.1080/02664763.2025.2457008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal studies in discrete or continuous time involving categorical data are common in agricultural sciences. Transition models can be used as a means to analyse the resulting data, especially when the aim is to describe category changes over time, as well as to accommodate covariates due to experimental design. Here we focus on discrete-time models, for which it is critical to assess whether the underlying process is stationary or not. Tests based on likelihood procedures are very useful, and here we propose the Gradient test to assess stationary, or homogeneity of transition probabilities. We carried out simulation studies to evaluate the performance of the proposed test, which indicated a good performance regarding type-I error and power when compared to other classical tests available in the literature. As motivation we present two studies with agricultural data, the first one applied to entomology with nominal responses and the second application refers to the degree of injury in pigs. Using our proposed test, stationarity and non-stationarity were verified respectively in the applications. Since the gradient test to assess stationarity has a simplified structure when compared to other tests, it is therefore a useful alternative when carrying out inference in these types of models.},
  archive      = {J_JOAS},
  author       = {Laura Vicuña Torres de Paula and Idemauro Antonio Rodrigues de Lara and Cesar Auguto Taconeli and Carolina Reigada and Rafael de Andrade Moral},
  doi          = {10.1080/02664763.2025.2457008},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2172-2190},
  shortjournal = {J. Appl. Stat.},
  title        = {Gradient test to assess homogeneity of probabilities in discrete-time transition models with application in agricultural science data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Upper quantile-based CUSUM-type control chart for detecting small changes in image data. <em>JOAS</em>, <em>52</em>(11), 2156-2171. (<a href='https://doi.org/10.1080/02664763.2025.2456614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image monitoring is an important research problem that has wide applications in various fields, including manufacturing industries, satellite imaging, medical diagnostics, and so forth. Traditional image monitoring control charts perform rather poorly when the changes occur at very small regions of the image, and when the changes of image intensity values are small in those regions. Their performances get worse if the images contain noise, and the changes occur near the edges of image objects. In applications such as manufacturing industries, the changes in the images are often too small to be detected by human eyes. In this article, we propose a CUSUM-type control chart for online monitoring of grayscale images. Depending on what kind of changes we wish to detect, big or small, we propose to use a certain upper quantile of the local CUSUM statistics. We incorporate a state-of-the-art jump preserving image smoothing technique in the proposed chart that ensures good performance even in presence of low to moderate noise. Theoretical justifications, and superior performance in numerical comparisons ensure that the proposed control chart can be useful to many researchers and practitioners.},
  archive      = {J_JOAS},
  author       = {Anik Roy and Partha Sarathi Mukherjee},
  doi          = {10.1080/02664763.2025.2456614},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2156-2171},
  shortjournal = {J. Appl. Stat.},
  title        = {Upper quantile-based CUSUM-type control chart for detecting small changes in image data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Change point detection to analyze air pollution and its economic effects: An exponentially weighted moving average perspective. <em>JOAS</em>, <em>52</em>(11), 2113-2155. (<a href='https://doi.org/10.1080/02664763.2025.2455636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution has a direct impact on every society, leading to consequential effects on the economy of a nation. Poor air quality adversely affects human health, resulting in various economic outcomes such as rising healthcare costs, diminished labor productivity, negative impacts on tourism and living standards, increased regulatory expenses for businesses, and heightened economic disparities. Effective control methods are essential to monitor factors influencing the economy, including air quality. The presence of toxic substances in the air reduces air quality, necessitating its monitoring through indices like PM10. Among statistical process control tools, control charts are the most prominent for efficient change point detection. This study introduces a new process monitoring tool that incorporates additional auxiliary information, if available, alongside the main variable of interest. The proposed methodology ensures detection ability remains robust, even under disturbances in the auxiliary variable. Furthermore, mathematical analyses reveal that many existing statistical quality control tools become special cases of the proposed structure for specific sensitivity parameter values. Evaluated through properties of run length distribution, the proposed chart allows control of the robustness-efficiency balance by adjusting its sensitivity parameter. A practical implementation demonstrates the effectiveness of the chart in monitoring air quality data.},
  archive      = {J_JOAS},
  author       = {Shabbir Ahmad and Muhammad Riaz and Tahir Mahmood and Nasir Abbas},
  doi          = {10.1080/02664763.2025.2455636},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2113-2155},
  shortjournal = {J. Appl. Stat.},
  title        = {Change point detection to analyze air pollution and its economic effects: An exponentially weighted moving average perspective},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal effect estimation for competing risk data in randomized trial: Adjusting covariates to gain efficiency. <em>JOAS</em>, <em>52</em>(11), 2094-2112. (<a href='https://doi.org/10.1080/02664763.2025.2455626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The double-blinded randomized trial is considered the gold standard to estimate the average causal effect (ACE). The naive estimator without adjusting any covariate is consistent. However, incorporating the covariates that are strong predictors of the outcome could reduce the issue of unbalanced covariate distribution between the treated and controlled groups and can improve efficiency. Recent work has shown that thanks to randomization, for linear regression, an estimator under risk consistency (e.g. Random Forest) for the regression coefficients could maintain the convergence rate even when a nonparametric model is assumed for the effect of covariates. Also, such an adjusted estimator will always lead to efficiency gain compared to the naive unadjusted estimator. In this paper, we extend this result to the competing risk data setting and show that under similar assumptions, the augmented inverse probability censoring weighting (AIPCW) based adjusted estimator has the same convergence rate and efficiency gain. Extensive simulations were performed to show the efficiency gain in the finite sample setting. To illustrate our proposed method, we apply it to the Women's Health Initiative (WHI) dietary modification trial studying the effect of a low-fat diet on cardiovascular disease (CVD) related mortality among those who have prior CVD.},
  archive      = {J_JOAS},
  author       = {Youngjoo Cho and Cheng Zheng and Lihong Qi and Ross L. Prentice and Mei-Jie Zhang},
  doi          = {10.1080/02664763.2025.2455626},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2094-2112},
  shortjournal = {J. Appl. Stat.},
  title        = {Causal effect estimation for competing risk data in randomized trial: Adjusting covariates to gain efficiency},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated change point detection and online monitoring approach for the ratio of two variables using clustering-based control charts. <em>JOAS</em>, <em>52</em>(11), 2060-2093. (<a href='https://doi.org/10.1080/02664763.2025.2455625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online monitoring of the ratio of two random characteristics rather than monitoring their individual behaviors has many applications. For this aim, there are various control charts, known as RZ charts in the literature, e.g. Shewhart, memory-type and adaptive monitoring schemes, have been designed to detect the ratio’s abnormal patterns as soon as possible. Most of the existing RZ charts rely on two assumptions about the process: ( i ) both individual characteristics are normally distributed, and ( ii ) the direction (upward or downward) of the RZ’s deviation from its in-control (IC) state to an out-of-control (OC) condition is known. However, these assumptions can be violated in many practical situations. In recent years, applying the machine learning (ML) models in the Statistical Process Monitoring (SPM) area has provided several contributions compared to traditional statistical methods. However, ML-based control charts have not yet been discussed in the RZ monitoring literature. To this end, this study introduces a novel clustering-based control chart for monitoring RZ in Phase II. This method avoids making any assumptions about the direction of RZ’s deviation and does not need to assume a specific distribution for the two random characteristics. Furthermore, it can estimate the Change Point (CP) in the process.},
  archive      = {J_JOAS},
  author       = {Adel Ahmadi Nadi and Ali Yeganeh and Sandile Charles Shongwe and Alireza Shadman},
  doi          = {10.1080/02664763.2025.2455625},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2060-2093},
  shortjournal = {J. Appl. Stat.},
  title        = {An integrated change point detection and online monitoring approach for the ratio of two variables using clustering-based control charts},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clustering of recurrent events data. <em>JOAS</em>, <em>52</em>(11), 2031-2059. (<a href='https://doi.org/10.1080/02664763.2025.2452966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays data are often timestamped, thus, when analysing the events which may occur several times (recurrent events), it is desirable to model the whole dynamics of the counting process rather than to focus on a total number of events. Such kind of data can be encountered in hospital readmissions, disease recurrences or repeated failures of industrial systems. Recurrent events can be analysed in the counting process framework, as in the Andersen–Gill model, assuming that the baseline intensity depends on time and on covariates, as in the Cox model. However, observed covariates are often insufficient to explain the observed heterogeneity in the data. We propose a mixture model for recurrent events, allowing to account for the unobserved heterogeneity and to perform clustering of individuals (unsupervised classification allowing to partition of the heterogeneous data according to unobserved, or latent, variables). Within each cluster, the recurrent event process intensity is specified parametrically and is adjusted for covariates. Model parameters are estimated by maximum likelihood using the EM algorithm; the BIC criterion is adopted to choose an optimal number of clusters. The model feasibility is checked on simulated data. Real data on hospital readmissions of elderly people, which motivated the development of the proposed clustering model, are analysed. The obtained results allow a fine understanding of the recurrent event process in each cluster.},
  archive      = {J_JOAS},
  author       = {G. Babykina and V. Vandewalle and J. Carretero-Bravo},
  doi          = {10.1080/02664763.2025.2452966},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2031-2059},
  shortjournal = {J. Appl. Stat.},
  title        = {Clustering of recurrent events data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrative rank-based regression for multi-source high-dimensional data with multi-type responses. <em>JOAS</em>, <em>52</em>(11), 2011-2030. (<a href='https://doi.org/10.1080/02664763.2025.2452964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical scenarios often present instances where the types of responses are different between multi-source different datasets, reflecting distinct attributes or characteristics. In this paper, an integrative rank-based regression is proposed to facilitate information sharing among varied datasets with multi-type responses. Taking advantage of the rank-based regression, our proposed approach adeptly tackles differences in the magnitude of loss functions. In addition, it can robustly handle outliers and data contamination, and effectively mitigate model misspecification. Extensive numerical simulations demonstrate the superior and competitive performance of the proposed approach in model estimation and variable selection. Analysis of genetic data on HNSC and LUAD yields results with biological explanations and confirms its practical usefulness.},
  archive      = {J_JOAS},
  author       = {Fuzhi Xu and Shuangge Ma and Qingzhao Zhang},
  doi          = {10.1080/02664763.2025.2452964},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2011-2030},
  shortjournal = {J. Appl. Stat.},
  title        = {Integrative rank-based regression for multi-source high-dimensional data with multi-type responses},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovations in multivariate statistical modeling: Navigating theoretical and multidisciplinary domains. <em>JOAS</em>, <em>52</em>(10), 2007-2009. (<a href='https://doi.org/10.1080/02664763.2024.2440590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {Cacu Cacu and Elfi Rindiana},
  doi          = {10.1080/02664763.2024.2440590},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {2007-2009},
  shortjournal = {J. Appl. Stat.},
  title        = {Innovations in multivariate statistical modeling: Navigating theoretical and multidisciplinary domains},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The slashed lomax distribution: New properties and mellin-type statistical measures for inference. <em>JOAS</em>, <em>52</em>(10), 1984-2006. (<a href='https://doi.org/10.1080/02664763.2025.2451977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several continuous distributions have been proposed recently to provide more flexibility in modeling lifetime data. Among these, the Slashed class of models, particularly the Slashed Lomax ( SL ) distribution, has gained special attention. This asymmetric model has positive support and it is notable for its stochastic representation and ability to fit heavy-tailed datasets. Despite the increasing number of new continuous models catering to specific samples, there have been few statistical tools introduced to evaluate their goodness-of-fits. To address this deficit, we employ the methodology outlined in J.M. Nicolas [ Introduction aux statistiques de deuxième espèce: Applications des logs-moments et des logs-cumulants à l'analyse des lois d'images radar, TS , Trait. Signal 19 (2002), pp. 139–167] derived from the Mellin Transform (MT) to provide new goodness-of-fit measures for the SL distribution. These measures consider both qualitative and quantitative aspects. We derive the MT for the SL distribution, calculate the log-cumulants, and construct the log-cumulant diagram. Further, we introduce a test statistic using a combination of Hotelling's T 2 statistic and the multivariate Delta method to test hypotheses about the log-cumulants. We apply the new methodology to two real databases in the context of survival analysis to show its effectiveness in evaluating the fit criteria. We conduct bootstrap experiments to assess the power of the proposed test and to evaluate the performance of the estimators. The results revealed that the adjustment tools performed well and that the log-cumulant method proved to be an effective estimation criterion.},
  archive      = {J_JOAS},
  author       = {Jaine de Moura Carvalho and Frank Gomes-Silva and Josimar M. Vasconcelos and Gauss M. Cordeiro},
  doi          = {10.1080/02664763.2025.2451977},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1984-2006},
  shortjournal = {J. Appl. Stat.},
  title        = {The slashed lomax distribution: New properties and mellin-type statistical measures for inference},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference under multivariate size-biased sampling. <em>JOAS</em>, <em>52</em>(10), 1968-1983. (<a href='https://doi.org/10.1080/02664763.2025.2451972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present research deals with statistical inference for the expectation of a function of a random vector based on biased samples. After highlighting with the help of a motivating example the need for conducting this study, using the concept of multivariate weighted distributions, a consistent and asymptotically normally distributed estimator is proposed and utilized for developing statistical inference. A Monte Carlo study is carried out to examine the performance of the estimator proposed. Finally, the analysis of a real-world data set illustrates the benefits of using the proposed methods for statistical inference.},
  archive      = {J_JOAS},
  author       = {A. Batsidis and G. Tzavelas and P. Economou},
  doi          = {10.1080/02664763.2025.2451972},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1968-1983},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference under multivariate size-biased sampling},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted portmanteau statistics for testing for zero autocorrelation in dependent data. <em>JOAS</em>, <em>52</em>(10), 1950-1967. (<a href='https://doi.org/10.1080/02664763.2024.2449413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero autocorrelation test statistics of the portmanteau type are studied under dependence. The asymptotic distribution of statistics formed with weighted averages of the autocorrelation and partial autocorrelation functions is theoretically obtained and its accuracy is then analyzed via simulation and in an empirical application. In the simulation study, we find that the proposed statistics provide test with sizes quite close to their nominal, intended sizes and with power functions which show high sensitivity to deviations from the null. It also reveals, for all the lags studied, that the tests are increasingly precise as the sample size increases. An application to financial time series modeling is given where the importance of using robust portmanteau statistics is illustrated. Specifically, we show that traditional tests incur in large deviations from their nominal size, whereas robust tests do not.},
  archive      = {J_JOAS},
  author       = {N. Muriel},
  doi          = {10.1080/02664763.2024.2449413},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1950-1967},
  shortjournal = {J. Appl. Stat.},
  title        = {Weighted portmanteau statistics for testing for zero autocorrelation in dependent data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian doubly robust estimation of causal effects for clustered observational data. <em>JOAS</em>, <em>52</em>(10), 1931-1949. (<a href='https://doi.org/10.1080/02664763.2024.2449396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational data often exhibit clustered structure, which leads to inaccurate estimates of exposure effect if such structure is ignored. To overcome the challenges of modelling the complex confounder effects in clustered data, we propose a Bayesian doubly robust estimator of causal effects with random intercept BART to enhance the robustness against model misspecification. The proposed approach incorporates the uncertainty in the estimation of the propensity score, potential outcomes and the distribution of individual-level and cluster-level confounders into the exposure effect estimation, thereby improving the coverage probability of interval estimation. We evaluate the proposed method in the simulation study compared with frequentist doubly robust estimators with parametric and nonparametric multilevel modelling strategies. The proposed method is applied to estimate the effect of limited food access on the mortality of cardiovascular disease in the senior population.},
  archive      = {J_JOAS},
  author       = {Qi Zhou and Haonan He and Jie Zhao and Joon Jin Song},
  doi          = {10.1080/02664763.2024.2449396},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1931-1949},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian doubly robust estimation of causal effects for clustered observational data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric model averaging prediction in nested case-control studies. <em>JOAS</em>, <em>52</em>(10), 1904-1930. (<a href='https://doi.org/10.1080/02664763.2024.2447324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival predictions for patients are becoming increasingly important in clinical practice as they play a crucial role in aiding healthcare professionals to make more informed diagnoses and treatment decisions. The nested case-control designs have been extensively utilized as a cost-effective solution in many large cohort studies across epidemiology and other research fields. To achieve accurate survival predictions of individuals from nested case-control studies, we propose a semiparametric model averaging approach based on the partly linear additive proportional hazards structure to avoid the curse of dimensionality. The inverse probability weighting method is considered to estimate the parameters of submodels used in model averaging. We choose the weights by maximizing the pseudo-likelihood function constructed for the aggregated model and discuss the asymptotic optimality of selected weights. Simulation studies are conducted to assess the performance of our proposed model averaging method in the nested case-control study. Furthermore, we apply the proposed approach to real data to demonstrate its superiority.},
  archive      = {J_JOAS},
  author       = {Mengyu Li and Xiaoguang Wang},
  doi          = {10.1080/02664763.2024.2447324},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1904-1930},
  shortjournal = {J. Appl. Stat.},
  title        = {Semiparametric model averaging prediction in nested case-control studies},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for dependent competing risks data under adaptive type-II progressive hybrid censoring. <em>JOAS</em>, <em>52</em>(10), 1871-1903. (<a href='https://doi.org/10.1080/02664763.2024.2445237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider statistical inference based on dependent competing risks data from Marshall–Olkin bivariate Weibull distribution. The maximum likelihood estimates of the unknown model parameters have been computed by using Newton–Raphson method under adaptive Type II progressive hybrid censoring with partially observed failure causes. Existence and uniqueness of maximum likelihood estimates are derived. Approximate confidence intervals have been constructed via the observed Fisher information matrix using asymptotic normality property of the maximum likelihood estimates. Bayes estimates and highest posterior density credible intervals have been calculated under gamma-Dirichlet prior distribution by using Markov chain Monte Carlo technique. Convergence of Markov chain Monte Carlo samples is tested. In addition, a Monte Carlo simulation is carried out to compare the effectiveness of the proposed methods. Further, three different optimality criteria have been taken into account to obtain the most effective censoring plans. From these simulation study results it has been concluded that Bayesian technique produces superior outcomes. Finally, a real-life data set has been analyzed to illustrate the operability and applicability of the proposed methods.},
  archive      = {J_JOAS},
  author       = {Subhankar Dutta and Suchandan Kayal},
  doi          = {10.1080/02664763.2024.2445237},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1871-1903},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical inference for dependent competing risks data under adaptive type-II progressive hybrid censoring},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapting and evaluating deep-pseudo neural network for survival data with time-varying covariates. <em>JOAS</em>, <em>52</em>(10), 1847-1870. (<a href='https://doi.org/10.1080/02664763.2024.2444649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Extended Cox model provides an alternative to the proportional hazard Cox model for modelling data including time-varying covariates. Incorporating time-varying covariates is particularly beneficial when dealing with survival data, as it can improve the precision of survival function estimation. Deep learning methods, in particular, the Deep-pseudo survival neural network (DSNN) model have demonstrated a high potential for accurately predicting right-censored survival data when dealing with time-invariant variables. The DSNN's ability to discretise survival times makes it a natural choice for extending its application to scenarios involving time-varying covariates. This study adapts the DSNN to predict survival probabilities for data with time-varying covariates. To demonstrate this, we considered two scenarios: significant and non-significant time-varying covariates. For significant covariates, the Brier scores were below 0.25 at all considered specific time points, while, in the non-significant case, the Brier scores were above 0.25. The results illustrate that the DSNN performed comparably to the extended Cox, the Dynamic-DeepHit and mulitivariate joint models and on the simulated data. A real-world data application further confirms the predictive potential of the DSNN model in modelling survival data with time-varying covariates.},
  archive      = {J_JOAS},
  author       = {Albert Whata and Justine B. Nasejje and Najmeh Nakhaei Rad and Tshilidzi Mulaudzi and Ding-Geng Chen},
  doi          = {10.1080/02664763.2024.2444649},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1847-1870},
  shortjournal = {J. Appl. Stat.},
  title        = {Adapting and evaluating deep-pseudo neural network for survival data with time-varying covariates},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation for time-varying coefficient smoothed quantile regression. <em>JOAS</em>, <em>52</em>(9), 1825-1846. (<a href='https://doi.org/10.1080/02664763.2024.2440056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying coefficient regression is commonly used in the modeling of nonstationary stochastic processes. In this paper, we consider a time-varying coefficient con volution-type smoothed qu antil e r egression ( conquer ). The covariates and errors are assumed to belong to a general class of locally stationary processes. We propose a local linear conquer estimator for the varying-coefficient function, and obtain the global Bahadur–Kiefer representation, which yields the asymptotic normality. Furthermore, statistical inference on simultaneous confidence bands is also studied. We investigate the finite-sample performance of the conquer estimator and confirm the validity of our asymptotic theory by conducting extensive simulation studies. We also consider financial volatility data as an example of a real-world application.},
  archive      = {J_JOAS},
  author       = {Lixia Hu and Jinhong You and Qian Huang and Shu Liu},
  doi          = {10.1080/02664763.2024.2440056},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1825-1846},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation for time-varying coefficient smoothed quantile regression},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-valued scalar-on-function linear quantile regression based on the bivariate center and radius method. <em>JOAS</em>, <em>52</em>(9), 1791-1824. (<a href='https://doi.org/10.1080/02664763.2024.2440035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-valued functional data, a new type of data in symbolic data analysis, depicts the characteristics of a variety of big data and has drawn the attention of many researchers. Mean regression is one of the important methods for analyzing interval-valued functional data. However, this method is sensitive to outliers and may lead to unreliable results. As an important complement to mean regression, this paper proposes an interval-valued scalar-on-function linear quantile regression model. Specifically, we constructed two linear quantile regression models for the interval-valued response and interval-valued functional regressors based on the bivariate center and radius method. The proposed model is more robust and efficient than mean regression methods when the data contain outliers as well as the error does not follow the normal distribution. Numerical simulations and real data analysis of a climate dataset demonstrate the effectiveness and superiority of the proposed method over the existing methods.},
  archive      = {J_JOAS},
  author       = {Kaiyuan Liu and Min Xu and Jiang Du and Tianfa Xie},
  doi          = {10.1080/02664763.2024.2440035},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1791-1824},
  shortjournal = {J. Appl. Stat.},
  title        = {Interval-valued scalar-on-function linear quantile regression based on the bivariate center and radius method},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rollout designs for lump-sum data. <em>JOAS</em>, <em>52</em>(9), 1777-1790. (<a href='https://doi.org/10.1080/02664763.2024.2440031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work studies rollout design problems with a focus of suitable choices of rollout rate under the standard Type I and Type II error probabilities control framework. The main challenge of rollout design is that data is often observed in a lump-sum manner from a spatio-temporal point of view: (1) temporally, only the sum of data in a given sliding time window can be observed; (2) spatially, there are two subgroups for the data at each time step: control and treatment, but one can only observe the total values instead of individual values from each subgroup. We develop rollout tests of lump-sum data under both fixed-sample-size and sequential settings, subject to the constraints on Type I and Type II error probabilities. Numerical studies are conducted to validate our theoretical results.},
  archive      = {J_JOAS},
  author       = {Qunzhi Xu and Hongzhen Tian and Ananda Sarkar and Yajun Mei},
  doi          = {10.1080/02664763.2024.2440031},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1777-1790},
  shortjournal = {J. Appl. Stat.},
  title        = {Rollout designs for lump-sum data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrative analysis of high-dimensional quantile regression with contrasted penalization. <em>JOAS</em>, <em>52</em>(9), 1760-1776. (<a href='https://doi.org/10.1080/02664763.2024.2438799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big data, the simultaneous analysis of multiple high-dimensional, heavy-tailed datasets has become essential. Integrative analysis offers a powerful approach to combine and synthesize information from these various datasets, and often outperforming traditional meta-analysis and single-dataset analysis. In this paper, we introduce a novel high-dimensional integrative quantile regression that can accommodate the complexities inherent in multi-dataset analysis. A contrast penalty that smooths regression coefficients is introduced to account for across-dataset structures and improve variable selection. To ease the computational burden associated with high-dimensional quantile regression, a new algorithm is developed that is effective at computing solution paths and selecting significant variables. Monte Carlo simulations demonstrate its competitive performance. Additionally, the proposed method is applied to data from the China Health and Retirement Longitudinal Study, illustrating its practical utility in identifying influential factors affecting support income for the elderly. Findings indicate that adult children's individual characteristics and emotional comfort are primary factors of support income, and the extent of their impact varies across regions.},
  archive      = {J_JOAS},
  author       = {Panpan Ren and Xu Liu and Xiao Zhang and Peng Zhan and Tingting Qiu},
  doi          = {10.1080/02664763.2024.2438799},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1760-1776},
  shortjournal = {J. Appl. Stat.},
  title        = {Integrative analysis of high-dimensional quantile regression with contrasted penalization},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). To impute or not? testing multivariate normality on incomplete dataset: Revisiting the BHEP test. <em>JOAS</em>, <em>52</em>(9), 1742-1759. (<a href='https://doi.org/10.1080/02664763.2024.2438798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on testing multivariate normality using the BHEP test with data that are missing completely at random. Our objective is twofold: first, to gain insight into the asymptotic behavior of the BHEP test statistics under two widely used approaches for handling missing data, namely complete-case analysis and imputation, and second, to compare the power performance of the test statistic under these approaches. Since complete-case approach removes all elements of the sample with at least one missing component, it might lead to the loss of information. On the other hand, we note that performing the test on imputed data as if they were complete, Type I error becomes severely distorted. To address these issues, we propose an appropriate bootstrap algorithm for approximating p -values. Extensive simulation studies demonstrate that both mean and median approaches exhibit greater power compared to testing with complete-case analysis, and open some questions for further research. The proposed methodology is illustrated with real-data examples.},
  archive      = {J_JOAS},
  author       = {Danijel G. Aleksić and Bojana Milošević},
  doi          = {10.1080/02664763.2024.2438798},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1742-1759},
  shortjournal = {J. Appl. Stat.},
  title        = {To impute or not? testing multivariate normality on incomplete dataset: Revisiting the BHEP test},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A control chart for bivariate discrete data monitoring. <em>JOAS</em>, <em>52</em>(9), 1713-1741. (<a href='https://doi.org/10.1080/02664763.2024.2438795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control charts are sophisticated graphical tools used to detect and control aberrant variations. Different control schemes are designed to continuously monitor and improve the process stability and performance. This study proposes a bivariate exponentially weighted moving average chart for joint monitoring of the mean vector of Gumbel's bivariate geometric (GBG) data. The performance of the proposed chart is compared with Hotelling's T 2 chart. The results of the study indicated that the proposed control chart performs uniformly and substantially better than Hotelling's T 2 chart. In addition to two real-life examples, an example based on simulated data is also considered and compared to existing charts to verify the superiority of the proposed chart. Based on the comparisons, it turns out that the MEWMA (GBG) chart outperforms Hotelling's T 2 chart and individual EWMA control chart.},
  archive      = {J_JOAS},
  author       = {Ayesha Talib and Sajid Ali and Ismail Shah},
  doi          = {10.1080/02664763.2024.2438795},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1713-1741},
  shortjournal = {J. Appl. Stat.},
  title        = {A control chart for bivariate discrete data monitoring},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable bayesian inference for bradley–Terry models with ties: An application to honour based abuse. <em>JOAS</em>, <em>52</em>(9), 1695-1712. (<a href='https://doi.org/10.1080/02664763.2024.2436608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Honour-based abuse covers a wide range of family abuse including female genital mutilation and forced marriage. Safeguarding professionals need to identify where abuses are happening in their local community to the best support those at risk of these crimes and take preventative action. However, there is little local data about these kinds of crime. To tackle this problem, we ran comparative judgement surveys to map abuses at the local level, where participants where shown pairs of wards and asked which had a higher rate of honour based abuse. In previous comparative judgement studies, participants reported fatigue associated with comparisons between areas with similar levels of abuse. Allowing for tied comparisons reduces fatigue, but increase the computational complexity when fitting the model. We designed an efficient Markov Chain Monte Carlo algorithm to fit a model with ties, allowing for a wide range of prior distributions on the model parameters. Working with South Yorkshire Police and Oxford Against Cutting, we mapped the risk of honour-based abuse at the community level in two counties in the UK.},
  archive      = {J_JOAS},
  author       = {Rowland G. Seymour and Fabian Hernandez},
  doi          = {10.1080/02664763.2024.2436608},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1695-1712},
  shortjournal = {J. Appl. Stat.},
  title        = {Scalable bayesian inference for bradley–Terry models with ties: An application to honour based abuse},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust and efficient change point detection method for high-dimensional linear models. <em>JOAS</em>, <em>52</em>(9), 1671-1694. (<a href='https://doi.org/10.1080/02664763.2024.2436008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of linear models, a key problem of interest is to estimate the regression coefficient. Nevertheless, in certain instances, the vector of unknown coefficient parameters in a linear regression model differs from one segment to another. In this paper, when the dimension of covariates is high, a new method is proposed to examine a linear model in which the regression coefficient of two subpopulations may be different. To achieve robustness and efficiency, we introduce modal linear regression as a means of estimating the unknown coefficient parameters. Furthermore, our proposed method is capable of selecting variables and checking change points. Under certain mild assumptions, the limiting behavior of our proposed method can be established. Additionally, an estimation algorithm based on kick-one-off and SCAD approach is developed to implement in practice. For illustration, simulation studies and a real data are considered to assess the performance of our proposed method.},
  archive      = {J_JOAS},
  author       = {Zhong-Cheng Han and Kong-Sheng Zhang and Yan-Yong Zhao},
  doi          = {10.1080/02664763.2024.2436008},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1671-1694},
  shortjournal = {J. Appl. Stat.},
  title        = {A robust and efficient change point detection method for high-dimensional linear models},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delaying bud-break on pecan trees: A bayesian longitudinal multinomial regression approach. <em>JOAS</em>, <em>52</em>(8), 1649-1669. (<a href='https://doi.org/10.1080/02664763.2024.2436007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multivariate Bayesian Probit model is adapted to analyze a longitudinal multiclass-ordinal response, with a linear plateau as the longitudinal model. Measurements on pecan bud growth were collected on irregular time intervals, about a week apart from late March to mid April, using a six-level ordinal scale. The data are from two randomized complete block designs with four blocks each. The experiments were setup and initiated in 2018 in a pecan orchard, at two different locations, to evaluate the effect of two sets of four treatments on delaying growth of recently broken pecan buds to minimize bud loss due to low temperatures. A simulation study was successfully carried out to validate the model implementation. Treatment 3 of Experiment 1 was associated with the greatest reduction in bud growth rate. In Experiment 2, Treatments 2 and 3 had some effect on delaying bud growth. Although treatment effects were not statistically different in either experiment, this paper presents a practical and efficient modeling technique for longitudinal multinomial ordinal data, a common data type in applied agricultural research studies.},
  archive      = {J_JOAS},
  author       = {Dayna P. Saldaña Zepeda and Richard Heerema and Ciro Velasco Cruz and William Giese and Joshua Sherman},
  doi          = {10.1080/02664763.2024.2436007},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1649-1669},
  shortjournal = {J. Appl. Stat.},
  title        = {Delaying bud-break on pecan trees: A bayesian longitudinal multinomial regression approach},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent class profile model with time-dependent covariates: A study on symptom patterning of patients for head and neck cancer. <em>JOAS</em>, <em>52</em>(8), 1628-1648. (<a href='https://doi.org/10.1080/02664763.2024.2435997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The latent class profile model (LCPM) is a widely used technique for identifying distinct subgroups within a sample based on observations' longitudinal responses to categorical items. This paper proposes an expanded version of LCPM by embedding time-specific structures. Such development allows analysts to investigate associations between latent class memberships and time-dependent predictors at specific time points. We suggest a simultaneous estimation of latent class measurement parameters via the expectation-maximization (EM) algorithm, which yields valid point and interval estimators of associations between latent class memberships and covariates. We illustrate the validity of our estimation strategy via numerical studies. In addition, we demonstrate the novelty of the proposed model by analyzing the head and neck cancer data set.},
  archive      = {J_JOAS},
  author       = {Jung Wun Lee and Hayley Dunnack Yackel},
  doi          = {10.1080/02664763.2024.2435997},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1628-1648},
  shortjournal = {J. Appl. Stat.},
  title        = {Latent class profile model with time-dependent covariates: A study on symptom patterning of patients for head and neck cancer},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter estimation for stable distributions and their mixture. <em>JOAS</em>, <em>52</em>(8), 1594-1627. (<a href='https://doi.org/10.1080/02664763.2024.2434627'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider estimating the parameters of univariate α -stable distributions and their mixtures. First, using a Gaussian kernel density distribution estimator, we propose an estimation method based on the characteristic function. The optimal bandwidth parameter was selected using a plug-in method. We highlight another estimation procedure for the Maximum Likelihood framework based on the False position algorithm to find a numerical root of the log-likelihood through the score functions. For mixtures of α -stable distributions, the EM algorithm and the Bayesian estimation method have been modified to propose an efficient and valuable tool for parameter estimation. The proposed methods can be generalised to multiple mixtures, although we have limited the mixture study to two components. A simulation study is carried out to evaluate the performance of our methods, which are then applied to real data. Our results appear to accurately estimate mixtures of α -stable distributions. Applications concern the estimation of the number of replicates in the Mayotte COVID-19 dataset and the distribution of the N-acetyltransferase activity of the Bechtel et al. data for a urinary caffeine metabolite implicated in carcinogens. We compare the proposed methods, together with a detailed discussion. We conclude with the limitations of this study, together with other forthcoming work and a future implementation of an R package or Python library for the proposed methods in data modelling.},
  archive      = {J_JOAS},
  author       = {Omar Hajjaji and Solym Mawaki Manou-Abi and Yousri Slaoui},
  doi          = {10.1080/02664763.2024.2434627},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1594-1627},
  shortjournal = {J. Appl. Stat.},
  title        = {Parameter estimation for stable distributions and their mixture},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tests of covariate effects under finite gaussian mixture regression models. <em>JOAS</em>, <em>52</em>(8), 1571-1593. (<a href='https://doi.org/10.1080/02664763.2024.2433567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture of regression model is widely used to cluster subjects from a suspected heterogeneous population due to differential relationships between response and covariates over unobserved subpopulations. In such applications, statistical evidence pertaining to the significance of a hypothesis is important yet missing to substantiate the findings. In this case, one may wish to test hypotheses regarding the effect of a covariate such as its overall significance. If confirmed, a further test of whether its effects are different in different subpopulations might be performed. This paper is motivated by the analysis of Chiroptera dataset, in which, we are interested in knowing how forearm length development of bat species is influenced by precipitation within their habitats and living regions using finite Gaussian mixture regression (GMR) model. Since precipitation may have different effects on the evolutionary development of the forearm across the underlying subpopulations among bat species worldwide, we propose several testing procedures for hypotheses regarding the effect of precipitation on forearm length under finite GMR models. In addition to the real analysis of Chiroptera data, through simulation studies, we examine the performances of these testing procedures on their type I error rate, power, and consequently, the accuracy of clustering analysis.},
  archive      = {J_JOAS},
  author       = {Chong Gan and Jiahua Chen and Zeny Feng},
  doi          = {10.1080/02664763.2024.2433567},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1571-1593},
  shortjournal = {J. Appl. Stat.},
  title        = {Tests of covariate effects under finite gaussian mixture regression models},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Change-point detection of the kumaraswamy skew-t distribution based on modified information criterion. <em>JOAS</em>, <em>52</em>(8), 1561-1570. (<a href='https://doi.org/10.1080/02664763.2024.2431743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the change-point problem of the Kumaraswamy skew-t distribution. An approach based on the modified information criterion is proposed to detect the changes of the parameters of this distribution. Simulations have been conducted to investigate the performance of the proposed method. The proposed method is applied to real data to illustrate the detection procedure.},
  archive      = {J_JOAS},
  author       = {Jun Wang and Wei Ning},
  doi          = {10.1080/02664763.2024.2431743},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1561-1570},
  shortjournal = {J. Appl. Stat.},
  title        = {Change-point detection of the kumaraswamy skew-t distribution based on modified information criterion},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A clustering approach to integrative analyses of multiomic cancer data. <em>JOAS</em>, <em>52</em>(8), 1539-1560. (<a href='https://doi.org/10.1080/02664763.2024.2431742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid technological advances have allowed for molecular profiling across multiple omics domains for clinical decision-making in many diseases, especially cancer. However, as tumor development and progression are biological processes involving composite genomic aberrations, key challenges are to effectively assimilate information from these domains to identify genomic signatures and druggable biological entities, develop accurate risk prediction profiles for future patients, and identify novel patient subgroups for tailored therapy and monitoring. We propose integrative frameworks for high-dimensional multiple-domain cancer data. These Bayesian mixture model-based approaches coherently incorporate dependence within and between domains to accurately detect tumor subtypes, thus providing a catalog of genomic aberrations associated with cancer taxonomy. The flexible and scalable Bayesian nonparametric strategy performs simultaneous bidirectional clustering of the tumor samples and genomic probes to achieve dimension reduction. We describe an efficient variable selection procedure that can identify relevant genomic aberrations and potentially reveal underlying drivers of disease. Although the work is motivated by lung cancer datasets, the proposed methods are broadly applicable in a variety of contexts involving high-dimensional data. The success of the methodology is demonstrated using artificial data and lung cancer omics profiles publicly available from The Cancer Genome Atlas.},
  archive      = {J_JOAS},
  author       = {Dongyan Yan and Subharup Guha},
  doi          = {10.1080/02664763.2024.2431742},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1539-1560},
  shortjournal = {J. Appl. Stat.},
  title        = {A clustering approach to integrative analyses of multiomic cancer data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust bayesian latent position approach for community detection in networks with continuous attributes. <em>JOAS</em>, <em>52</em>(8), 1513-1538. (<a href='https://doi.org/10.1080/02664763.2024.2431736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of multiplex networks has spurred a critical need to take into account potential dependencies across different layers, especially when the goal is community detection, which is a fundamental learning task in network analysis. We propose a full Bayesian mixture model for community detection in both single-layer and multi-layer networks. A key feature of our model is the joint modeling of the nodal attributes that often come with the network data as a spatial process over the latent space. In addition, our model for multi-layer networks allows layers to have different strengths of dependency in the unique latent position structure and assumes that the probability of a relation between two actors (in a layer) depends on the distances between their latent positions (multiplied by a layer-specific factor) and the difference between their nodal attributes. Under our prior specifications, the actors' positions in the latent space arise from a finite mixture of Gaussian distributions, each corresponding to a cluster. Simulated examples show that our model outperforms existing benchmark models and exhibits significantly greater robustness when handling datasets with missing values. The model is also applied to a real-world three-layer network of employees in a law firm.},
  archive      = {J_JOAS},
  author       = {Zhumengmeng Jin and Juan Sosa and Shangchen Song and Brenda Betancourt},
  doi          = {10.1080/02664763.2024.2431736},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1513-1538},
  shortjournal = {J. Appl. Stat.},
  title        = {A robust bayesian latent position approach for community detection in networks with continuous attributes},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A defective cure rate quantile regression model for male breast cancer data. <em>JOAS</em>, <em>52</em>(8), 1485-1512. (<a href='https://doi.org/10.1080/02664763.2024.2428272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we particularly address the problem of assessing the impact of different prognostic factors, such as clinical stage and age, on the specific survival times of men with breast cancer when cure is a possibility. To this end, we developed a quantile regression model for survival data in the presence of long-term survivors based on the generalized Gompertz distribution in a defective version, which is conveniently reparametrized in terms of the q -th quantile and then linked to covariates via a logarithm link function. This proposal allows us to obtain how each variable affects the survival times in different quantiles. In addition, we are able to study the effects of covariates on the cure rate as well. We consider Markov Chain Monte Carlo methods to develop a Bayesian analysis in the proposed model and we evaluate its performance through Monte Carlo simulation studies. Finally, we illustrate the application of our model in a data set about male breast cancer from Brazil analyzed for the very first time.},
  archive      = {J_JOAS},
  author       = {Agatha Rodrigues and Patrick Borges and Bruno Santos},
  doi          = {10.1080/02664763.2024.2428272},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1485-1512},
  shortjournal = {J. Appl. Stat.},
  title        = {A defective cure rate quantile regression model for male breast cancer data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bootstrap procedure to estimate the causal effect of a public policy, considering overlap and imperfect compliance. <em>JOAS</em>, <em>52</em>(7), 1470-1484. (<a href='https://doi.org/10.1080/02664763.2024.2428994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a nonparametric bootstrap method for estimating the causal effects of public policy under the circumstances of imperfect compliance and overlap. It focuses on business investment subsidies in Sardinia by comparing firms eligible for the 1999 subsidies to those not, amid issues of imperfect compliance and overlapping programs. Bootstrap confidence intervals (CI) are proposed for the average effect of treatment on the sub-population of compliers. The obtained CIs are consistent across nominal levels and robust against data nonnormality; they show coverages of credible intervals close to nominal, suggesting effectiveness for assessing causal effects. Compared to other methods, the results of the new combination of a specific estimator for incompliance and the bootstrap align with those of more modern approaches such as Bayesian Additive Regression Trees and Causal forest.},
  archive      = {J_JOAS},
  author       = {Stefano Cabras},
  doi          = {10.1080/02664763.2024.2428994},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1470-1484},
  shortjournal = {J. Appl. Stat.},
  title        = {A bootstrap procedure to estimate the causal effect of a public policy, considering overlap and imperfect compliance},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bivariate load-sharing model. <em>JOAS</em>, <em>52</em>(7), 1446-1469. (<a href='https://doi.org/10.1080/02664763.2024.2428267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motivation of this work came from a data set obtained from an experiment performed on diabetic patients, with diabetic retinopathy disorder. The aim of this experiment is to test whether there is any significant difference between two different treatments which are being used for this disease. The two eyes can be considered as a two-component load-sharing system. In a two-component load-sharing system after the failure of one component, the surviving component has to shoulder extra load. Hence, it is prone to failure at an earlier time than what is expected under the original model. It may also happen sometimes that the failure of one component may release extra resources to the survivor, thus delaying the failure. In most of the existing literature, it has been assumed that at the beginning the lifetime distributions of the two components are independently distributed, which may not be very reasonable in this case. In this paper, we have introduced a new bivariate load-sharing model where the independence assumptions of the lifetime distributions of the two components at the beginning have been relaxed. In this present model, they may be dependent. Further, there is a positive probability that the two components may fail simultaneously. If the two components do not fail simultaneously, it is assumed that the lifetime of the surviving component changes based on the tampered failure rate assumption. The proposed bivariate distribution has a singular component. The likelihood inference of the unknown parameters has been provided. Simulation results and the analysis of the data set have been presented to show the effectiveness of the proposed model.},
  archive      = {J_JOAS},
  author       = {Debasis Kundu},
  doi          = {10.1080/02664763.2024.2428267},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1446-1469},
  shortjournal = {J. Appl. Stat.},
  title        = {A bivariate load-sharing model},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric regression analysis of panel binary data with a dependent failure time. <em>JOAS</em>, <em>52</em>(7), 1423-1445. (<a href='https://doi.org/10.1080/02664763.2024.2428266'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In health and clinical research, panel binary data from recurrent events arise when subjects are surveyed to report occurrence statuses of recurrent events over fixed observation windows. In practice, such data can be cut short by a dependent failure event such as death. For the analysis of panel binary data, tools from generalized linear models overlook the recurrence nature of panel binary data, and other relevant literature does not accommodate the failure time. Motivated by the hospitalization data surveyed from the Health and Retirement Study, we propose a semiparametric joint-modeling-based procedure for analyzing panel binary data with a dependent failure time. For model fitting, we develop a computationally efficient EM algorithm and show the resulting estimates are consistent and asymptotically normal. Theoretical results are provided to enable valid inferences. Simulation studies have confirmed the performance of the proposed method in practical settings. The method is applied to assess important risk factors associated with incidences of hospitalization among the working elderly.},
  archive      = {J_JOAS},
  author       = {Lei Ge and Yang Li and Jianguo Sun},
  doi          = {10.1080/02664763.2024.2428266},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1423-1445},
  shortjournal = {J. Appl. Stat.},
  title        = {Semiparametric regression analysis of panel binary data with a dependent failure time},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient non-parametric estimation of variable productivity hawkes processes. <em>JOAS</em>, <em>52</em>(7), 1405-1422. (<a href='https://doi.org/10.1080/02664763.2024.2426019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several approaches to estimating the productivity function for a Hawkes point process with variable productivity are discussed, improved upon, and compared in terms of their root-mean-squared error and computational efficiency for various data sizes, and for binned as well as unbinned data. We find that for unbinned data, a regularized version of the analytic maximum likelihood estimator proposed by Schoenberg is the most accurate but is computationally burdensome. The unregularized version of the estimator is faster to compute but has lower accuracy, though both estimators outperform empirical or binned least squares estimators in terms of root-mean-squared error, especially when the mean productivity is 0.2 or greater. For binned data, binned least squares estimates are highly efficient both in terms of computation time and root-mean-squared error. An extension to estimating transmission time density is discussed, and an application to estimating the productivity of Covid-19 in the United States as a function of time from January 2020 to July 2022 is provided.},
  archive      = {J_JOAS},
  author       = {Sophie Phillips and Frederic Schoenberg},
  doi          = {10.1080/02664763.2024.2426019},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1405-1422},
  shortjournal = {J. Appl. Stat.},
  title        = {Efficient non-parametric estimation of variable productivity hawkes processes},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unit-level one-inflated beta model for small area prediction of seat-belt use rates. <em>JOAS</em>, <em>52</em>(7), 1381-1404. (<a href='https://doi.org/10.1080/02664763.2024.2426016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a unit-level one-inflated beta model for the purpose of small area estimation. Our specific interest is in estimation of seat-belt use rates for Iowa counties using data from the Iowa Seat-Belt Use Survey. As a result of small county sample sizes, small area estimation methods are needed. We propose frequentist and Bayesian implementations of a unit-level one-inflated beta model. We compare the Bayesian and frequentist predictors to simpler alternatives through simulation. We apply the proposed Bayesian and frequentist procedures to data from the Iowa Seat-Belt Use Survey.},
  archive      = {J_JOAS},
  author       = {Zirou Zhou and Emily Berg},
  doi          = {10.1080/02664763.2024.2426016},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1381-1404},
  shortjournal = {J. Appl. Stat.},
  title        = {A unit-level one-inflated beta model for small area prediction of seat-belt use rates},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture mean residual life model for competing risks data with mismeasured covariates. <em>JOAS</em>, <em>52</em>(7), 1361-1380. (<a href='https://doi.org/10.1080/02664763.2024.2426015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a mixture regression model for competing risks data, where the logistic regression model is specified for the marginal probabilities of the failure types and the mean residual lifetime (MRL) model is assumed for the failure time given the failure of interest. The estimating equations (EEs) are derived to infer the logistic regression and MRL model separately. We further consider the situation where the covariates are subject to measurement error. The presence of measurement error imposes extra challenges for the analysis of complex time-to-event data. By using the above EEs as the correction-amenable original estimating functions, we propose a corrected score estimation, which does not require specifying the distributions for unobserved error-prone covariates. The proposed estimators are shown to be consistent and asymptotically normally distributed. The performance of the method is investigated by intensive simulation studies and two real examples are presented to illustrate the proposed methods.},
  archive      = {J_JOAS},
  author       = {Chyong-Mei Chen and Chih-Ching Lin and Chih-Cheng Wu and Jia-Ren Tsai},
  doi          = {10.1080/02664763.2024.2426015},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1361-1380},
  shortjournal = {J. Appl. Stat.},
  title        = {Mixture mean residual life model for competing risks data with mismeasured covariates},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting economic growth by combining local linear and standard approaches. <em>JOAS</em>, <em>52</em>(7), 1342-1360. (<a href='https://doi.org/10.1080/02664763.2024.2424920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, developing economies are of major importance for global macroeconomic development. However, the empirical analysis and especially the forecasting of macroeconomic time series remain difficult due to a lack of sufficient data, data frequency, high volatility, and non-linear developments. These difficulties require more sophisticated approaches to obtain reliable forecasts. Therefore, we propose an improved forecasting method especially for growth data based on a data-driven local linear trend estimation with an extended iterative plug-in algorithm for determining the bandwidth endogenously. This approach allows a smooth trend estimation that takes care of temporary changes in trend processes. Further, the naïve random walk model is extended for forecasting by including a local linear, time-varying drift. We apply this method to GDP development for six developing and two advanced economies and compare different forecast combinations. The combinations that include the local linear approach and the random walk with a local linear trend improve forecasting accuracy and reduce variance.},
  archive      = {J_JOAS},
  author       = {Marlon Fritz and Sarah Forstinger and Yuanhua Feng and Thomas Gries},
  doi          = {10.1080/02664763.2024.2424920},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1342-1360},
  shortjournal = {J. Appl. Stat.},
  title        = {Forecasting economic growth by combining local linear and standard approaches},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimal subsampling design for large-scale cox model with censored data. <em>JOAS</em>, <em>52</em>(7), 1315-1341. (<a href='https://doi.org/10.1080/02664763.2024.2423234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling designs are useful for reducing computational load and storage cost for large-scale data analysis. For massive survival data with right censoring, we propose a class of optimal subsampling designs under the widely-used Cox model. The proposed designs utilize information from both the outcome and the covariates. Different forms of the design can be derived adaptively to meet various targets, such as optimizing the overall estimation accuracy or minimizing the variation of specific linear combination of the estimators. Given the subsampled data, the inverse probability weighting approach is employed to estimate the model parameters. The resultant estimators are shown to be consistent and asymptotically normally distributed. Simulation results indicate that the proposed subsampling design yields more efficient estimators than the uniform subsampling by using subsampled data of comparable sample sizes. Additionally, the subsampling estimation significantly reduces the computational load and storage cost relative to the full data estimation. An analysis of a real data example is provided for illustration.},
  archive      = {J_JOAS},
  author       = {Shiqi Liu and Zilong Xie and Ming Zheng and Wen Yu},
  doi          = {10.1080/02664763.2024.2423234},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1315-1341},
  shortjournal = {J. Appl. Stat.},
  title        = {An optimal subsampling design for large-scale cox model with censored data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient fully bayesian approach to brain activity mapping with complex-valued fMRI data. <em>JOAS</em>, <em>52</em>(6), 1299-1314. (<a href='https://doi.org/10.1080/02664763.2024.2422392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional magnetic resonance imaging (fMRI) enables indirect detection of brain activity changes via the blood-oxygen-level-dependent (BOLD) signal. Conventional analysis methods mainly rely on the real-valued magnitude of these signals. In contrast, research suggests that analyzing both real and imaginary components of the complex-valued fMRI (cv-fMRI) signal provides a more holistic approach that can increase power to detect neuronal activation. We propose a fully Bayesian model for brain activity mapping with cv-fMRI data. Our model accommodates temporal and spatial dynamics. Additionally, we propose a computationally efficient sampling algorithm, which enhances processing speed through image partitioning. Our approach is shown to be computationally efficient via image partitioning and parallel computation while being competitive with state-of-the-art methods. We support these claims with both simulated numerical studies and an application to real cv-fMRI data obtained from a finger-tapping experiment.},
  archive      = {J_JOAS},
  author       = {Zhengxin Wang and Daniel B. Rowe and Xinyi Li and D. Andrew Brown},
  doi          = {10.1080/02664763.2024.2422392},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1299-1314},
  shortjournal = {J. Appl. Stat.},
  title        = {Efficient fully bayesian approach to brain activity mapping with complex-valued fMRI data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust distance-based approach for detecting multidimensional outliers. <em>JOAS</em>, <em>52</em>(6), 1278-1298. (<a href='https://doi.org/10.1080/02664763.2024.2422403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying outliers in data analysis is a critical task, as outliers can significantly influence the results and conclusions drawn from a dataset. This study explores the use of the Mahalanobis distance metric for detecting outliers in multivariate data, focusing on a novel approach inspired by the work of M. Falk, [ On mad and comedians , Ann. Inst. Stat. Math. 49 (1997), pp. 615–644]. The proposed method is rigorously tested through extensive simulation analysis, where it demonstrates high True Positive Rates (TPR) and low False Positive Rates (FPR) when compared to other existing outlier detection techniques. Through extensive simulation analysis, we empirically evaluate the affine equivariance and breakdown properties of our proposed distance measure and it is evident from the outputs that our robust distance measure demonstrates effective results with respect to the measures FPR and TPR. The proposed method was applied to seven different datasets, showing promising true positive rates (TPR) and false positive rates (FPR), and it outperformed several well-known outlier identification approaches. We can effectively use our proposed distance measure in fields demanding outlier detection.},
  archive      = {J_JOAS},
  author       = {R. Lakshmi and T. A. Sajesh},
  doi          = {10.1080/02664763.2024.2422403},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1278-1298},
  shortjournal = {J. Appl. Stat.},
  title        = {A robust distance-based approach for detecting multidimensional outliers},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction intervals and bands with improved coverage for functional data under noisy discrete observation. <em>JOAS</em>, <em>52</em>(6), 1258-1277. (<a href='https://doi.org/10.1080/02664763.2024.2420223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the classic situation in functional data analysis in which curves are observed at discrete, possibly sparse and irregular, arguments with observation noise. We focus on the reconstruction of individual curves by prediction intervals and bands. The standard approach consists of two steps: first, one estimates the mean and covariance function of curves and observation noise variance function by, e.g. penalized splines, and second, under Gaussian assumptions, one derives the conditional distribution of a curve given observed data and constructs prediction sets with required properties, usually employing sampling from the predictive distribution. This approach is well established, commonly used and theoretically valid but practically, it surprisingly fails in its key property: prediction sets constructed this way often do not have the required coverage. The actual coverage is lower than the nominal one. We investigate the cause of this issue and propose a computationally feasible remedy that leads to prediction regions with a much better coverage. Our method accounts for the uncertainty of the predictive model by sampling from the approximate distribution of its spline estimators whose covariance is estimated by a novel sandwich estimator. Our approach also applies to the important case of covariate-adjusted models.},
  archive      = {J_JOAS},
  author       = {David Kraus},
  doi          = {10.1080/02664763.2024.2420223},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1258-1277},
  shortjournal = {J. Appl. Stat.},
  title        = {Prediction intervals and bands with improved coverage for functional data under noisy discrete observation},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust estimation of the incubation period and the time of exposure using γ-divergence. <em>JOAS</em>, <em>52</em>(6), 1239-1257. (<a href='https://doi.org/10.1080/02664763.2024.2420221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the exposure time to single infectious pathogens and the associated incubation period, based on symptom onset data, is crucial for identifying infection sources and implementing public health interventions. However, data from rapid surveillance systems designed for early outbreak warning often come with outliers originated from individuals who were not directly exposed to the initial source of infection (i.e. tertiary and subsequent infection cases), making the estimation of exposure time challenging. To address this issue, this study uses a three-parameter lognormal distribution and proposes a new γ -divergence-based robust approach for estimating the parameter corresponding to exposure time with a tailored optimization procedure using the majorization-minimization algorithm, which ensures the monotonic decreasing property of the objective function. Comprehensive numerical experiments and real data analyses suggest that our method is superior to conventional methods in terms of bias, mean squared error, and coverage probability of 95% confidence intervals.},
  archive      = {J_JOAS},
  author       = {Daisuke Yoneoka and Takayuki Kawashima and Yuta Tanoue and Shuhei Nomura and Akifumi Eguchi},
  doi          = {10.1080/02664763.2024.2420221},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1239-1257},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust estimation of the incubation period and the time of exposure using γ-divergence},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating the choice of the duration in DDMS models through a parametric link. <em>JOAS</em>, <em>52</em>(6), 1219-1238. (<a href='https://doi.org/10.1080/02664763.2024.2419505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important hyper-parameters in duration-dependent Markov-switching (DDMS) models is the duration of the hidden states. Because there is currently no procedure for estimating this duration or testing whether a given duration is appropriate for a given data set, an ad hoc duration choice must be heuristically justified. In this paper, we propose and examine a methodology that mitigates the choice of duration in DDMS models when forecasting is the goal. The novelty of this paper is the use of the asymmetric Aranda-Ordaz parametric link function to model transition probabilities in DDMS models, instead of the commonly applied logit link. The idea behind this approach is that any incorrect duration choice is compensated for by the parameter in the link, increasing model flexibility. Two Monte Carlo simulations, based on classical applications of DDMS models, are employed to evaluate the methodology. In addition, an empirical investigation is carried out to forecast the volatility of the S&P500, which showcases the capabilities of the proposed model.},
  archive      = {J_JOAS},
  author       = {Fernando Henrique de Paula e Silva Mendes and Douglas Eduardo Turatti and Guilherme Pumi},
  doi          = {10.1080/02664763.2024.2419505},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1219-1238},
  shortjournal = {J. Appl. Stat.},
  title        = {Mitigating the choice of the duration in DDMS models through a parametric link},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-linear integer-valued autoregressive model with zero-inflated data series. <em>JOAS</em>, <em>52</em>(6), 1195-1218. (<a href='https://doi.org/10.1080/02664763.2024.2419495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new non-linear stationary process for time series of counts is introduced. The process is composed of the survival and innovation component. The survival component is based on the generalized zero-modified geometric thinning operator, where the innovation process figures in the survival component as well. A few probability distributions for the innovation process have been discussed, in order to adjust the model for observed series with the excess number of zeros. The conditional maximum likelihood and the conditional least squares methods are investigated for the estimation of the model parameters. The practical aspect of the model is presented on some real-life data sets, where we observe data with inflation as well as deflation of zeroes so we can notice how the model can be adjusted with the proper parameter selection.},
  archive      = {J_JOAS},
  author       = {Predrag M. Popović and Hassan S. Bakouch and Miroslav M. Ristić},
  doi          = {10.1080/02664763.2024.2419495},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1195-1218},
  shortjournal = {J. Appl. Stat.},
  title        = {A non-linear integer-valued autoregressive model with zero-inflated data series},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semiparametric accelerated failure time-based mixture cure tree. <em>JOAS</em>, <em>52</em>(6), 1177-1194. (<a href='https://doi.org/10.1080/02664763.2024.2418476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mixture cure rate model (MCM) is the most widely used model for the analysis of survival data with a cured subgroup. In this context, the most common strategy to model the cure probability is to assume a generalized linear model with a known link function, such as the logit link function. However, the logit model can only capture simple effects of covariates on the cure probability. In this article, we propose a new MCM where the cure probability is modeled using a decision tree-based classifier and the survival distribution of the uncured is modeled using an accelerated failure time structure. To estimate the model parameters, we develop an expectation maximization algorithm. Our simulation study shows that the proposed model performs better in capturing nonlinear classification boundaries when compared to the logit-based MCM and the spline-based MCM. This results in more accurate and precise estimates of the cured probabilities, which in-turn results in improved predictive accuracy of cure. We further show that capturing nonlinear classification boundary also improves the estimation results corresponding to the survival distribution of the uncured subjects. Finally, we apply our proposed model and the EM algorithm to analyze an existing bone marrow transplant data.},
  archive      = {J_JOAS},
  author       = {Wisdom Aselisewine and Suvra Pal and Helton Saulo},
  doi          = {10.1080/02664763.2024.2418476},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1177-1194},
  shortjournal = {J. Appl. Stat.},
  title        = {A semiparametric accelerated failure time-based mixture cure tree},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the median p-value method for assessing the statistical significance of tests when using multiple imputation. <em>JOAS</em>, <em>52</em>(6), 1161-1176. (<a href='https://doi.org/10.1080/02664763.2024.2418473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rubin’s Rules are commonly used to pool the results of statistical analyses across imputed samples when using multiple imputation. Rubin’s Rules cannot be used when the result of an analysis in an imputed dataset is not a statistic and its associated standard error, but a test statistic (e.g. Student’s t-test). While complex methods have been proposed for pooling test statistics across imputed samples, these methods have not been implemented in many popular statistical software packages. The median p -value method has been proposed for pooling test statistics. The statistical significance level of the pooled test statistic is the median of the associated p -values across the imputed samples. We evaluated the performance of this method with nine statistical tests: Student’s t-test, Wilcoxon Rank Sum test, Analysis of Variance, Kruskal-Wallis test, the test of significance for Pearson’s and Spearman’s correlation coefficient, the Chi-squared test, the test of significance for a regression coefficient from a linear regression and from a logistic regression. For each test, the empirical type I error rate was higher than the advertised rate. The magnitude of inflation increased as the prevalence of missing data increased. The median p -value method should not be used to assess statistical significance across imputed datasets.},
  archive      = {J_JOAS},
  author       = {Peter C. Austin and Iris Eekhout and Stef van Buuren},
  doi          = {10.1080/02664763.2024.2418473},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1161-1176},
  shortjournal = {J. Appl. Stat.},
  title        = {Evaluating the median p-value method for assessing the statistical significance of tests when using multiple imputation},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The PCovR biplot: A graphical tool for principal covariates regression. <em>JOAS</em>, <em>52</em>(5), 1144-1159. (<a href='https://doi.org/10.1080/02664763.2024.2417978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biplots are useful tools because they provide a visual representation of both individuals and variables simultaneously, making it easier to explore relationships and patterns within multidimensional datasets. This paper extends their use to examine the relationship between a set of predictors 𝐗 and a set of response variables Y using Principal Covariates Regression analysis (PCovR). The PCovR biplot provides a simultaneous graphical representation of individuals, predictor variables and response variables. It also provides the ability to examine the relationship between both types of variables in the form of the regression coefficient matrix.},
  archive      = {J_JOAS},
  author       = {Elisa Frutos-Bernal and José Luis Vicente-Villardón},
  doi          = {10.1080/02664763.2024.2417978},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {1144-1159},
  shortjournal = {J. Appl. Stat.},
  title        = {The PCovR biplot: A graphical tool for principal covariates regression},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability analysis based on doubly-truncated and interval-censored data. <em>JOAS</em>, <em>52</em>(5), 1128-1143. (<a href='https://doi.org/10.1080/02664763.2024.2415412'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field data provide important information on product reliability. Interval sampling is widely used for collection of field data, which typically report incident cases during a certain time period. Such sampling scheme induces doubly truncated (DT) data if the exact failure time is known. In many situations, the exact failure date is known only to fall within an interval, leading to doubly truncated and interval censored (DTIC) data. This article considers analysis of DTIC data under parametric failure time models. We consider a conditional likelihood approach and propose interval estimation for parameters and the cumulative distribution functions. Simulation studies show that the proposed method performs well for finite sample size.},
  archive      = {J_JOAS},
  author       = {Pao-Sheng Shen and Huai-Man Li},
  doi          = {10.1080/02664763.2024.2415412},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {1128-1143},
  shortjournal = {J. Appl. Stat.},
  title        = {Reliability analysis based on doubly-truncated and interval-censored data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel ranked k-nearest neighbors algorithm for missing data imputation. <em>JOAS</em>, <em>52</em>(5), 1103-1127. (<a href='https://doi.org/10.1080/02664763.2024.2414357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data is a common problem in many domains that rely on data analysis. The k Nearest Neighbors imputation method has been widely used to address this issue, but it has limitations in accurately imputing missing values, especially for datasets with small pairwise correlations and small values of k . In this study, we proposed a method, Ranked k Nearest Neighbors imputation that uses a similar approach to k Nearest Neighbor, but utilizing the concept of Ranked set sampling to select the most relevant neighbors for imputation. Our results show that the proposed method outperforms the standard k nearest neighbor method in terms of imputation accuracy both in case of Missing Completely at Random and Missing at Random mechanism, as demonstrated by consistently lower MSIE and MAIE values across all datasets. This suggests that the proposed method is a promising alternative for imputing missing values in datasets with small pairwise correlations and small values of k . Thus, the proposed Ranked k Nearest Neighbor method has important implications for data imputation in various domains and can contribute to the development of more efficient and accurate imputation methods without adding any computational complexity to an algorithm.},
  archive      = {J_JOAS},
  author       = {Yasir Khan and Said Farooq Shah and Syed Muhammad Asim},
  doi          = {10.1080/02664763.2024.2414357},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {1103-1127},
  shortjournal = {J. Appl. Stat.},
  title        = {A novel ranked k-nearest neighbors algorithm for missing data imputation},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust multi-outcome regression with correlated covariate blocks using fused LAD-lasso. <em>JOAS</em>, <em>52</em>(5), 1081-1102. (<a href='https://doi.org/10.1080/02664763.2024.2414346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lasso is a popular and efficient approach to simultaneous estimation and variable selection in high-dimensional regression models. In this paper, a robust fused LAD-lasso method for multiple outcomes is presented that addresses the challenges of non-normal outcome distributions and outlying observations. Measured covariate data from space or time, or spectral bands or genomic positions often have natural correlation structure arising from measuring distance between the covariates. The proposed multi-outcome approach includes handling of such covariate blocks by a group fusion penalty, which encourages similarity between neighboring regression coefficient vectors by penalizing their differences, for example, in sequential data situation. Properties of the proposed approach are illustrated by extensive simulations using BIC-type criteria for model selection. The method is also applied to a real-life skewed data on retirement behavior with longitudinal heteroscedastic explanatory variables.},
  archive      = {J_JOAS},
  author       = {Jyrki Möttönen and Tero Lähderanta and Janne Salonen and Mikko J. Sillanpää},
  doi          = {10.1080/02664763.2024.2414346},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {1081-1102},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust multi-outcome regression with correlated covariate blocks using fused LAD-lasso},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COVINet: A deep learning-based and interpretable prediction model for the county-wise trajectories of COVID-19 in the united states. <em>JOAS</em>, <em>52</em>(5), 1063-1080. (<a href='https://doi.org/10.1080/02664763.2024.2412284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The devastating impact of COVID-19 on the United States has been profound since its onset in January 2020. Predicting the trajectory of epidemics accurately and devising strategies to curb their progression are currently formidable challenges. In response to this crisis, we propose COVINet, which combines the architecture of Long Short-Term Memory and Gated Recurrent Unit, incorporating actionable covariates to offer high-accuracy prediction and explainable response. First, we train COVINet models for confirmed cases and total deaths with five input features, and compare Mean Absolute Errors (MAEs) and Mean Relative Errors (MREs) of COVINet against ten competing models from the United States CDC in the last four weeks before April 26, 2021. The results show COVINet outperforms all competing models for MAEs and MREs when predicting total deaths. Then, we focus on prediction for the most severe county in each of the top 10 hot-spot states using COVINet. The MREs are small for all predictions made in the last 7 or 30 days before March 23, 2023. Beyond predictive accuracy, COVINet offers high interpretability, enhancing the understanding of pandemic dynamics. This dual capability positions COVINet as a powerful tool for informing effective strategies in pandemic prevention and governmental decision-making.},
  archive      = {J_JOAS},
  author       = {Yukang Jiang and Ting Tian and Wenting Zhou and Yuting Zhang and Zhongfei Li and Xueqin Wang and Heping Zhang},
  doi          = {10.1080/02664763.2024.2412284},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {1063-1080},
  shortjournal = {J. Appl. Stat.},
  title        = {COVINet: A deep learning-based and interpretable prediction model for the county-wise trajectories of COVID-19 in the united states},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regression-based rectangular tolerance regions as reference regions in laboratory medicine. <em>JOAS</em>, <em>52</em>(5), 1040-1062. (<a href='https://doi.org/10.1080/02664763.2024.2411614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reference ranges are invaluable in laboratory medicine, as these are indispensable tools for the interpretation of laboratory test results. When assessing measurements on a single analyte, univariate reference intervals are required. In many cases, however, measurements on several analytes are needed by medical practitioners to diagnose more complicated conditions such as kidney function or liver function. For such cases, it is recommended to use multivariate reference regions, which account for the cross-correlations among the analytes. Traditionally, multivariate reference regions (MRRs) have been constructed as ellipsoidal regions. The disadvantage of such regions is that they are unable to detect component-wise outlying measurements. Because of this, rectangular reference regions have recently been put forward in the literature. In this study, we develop methodologies to compute rectangular MRRs that incorporate covariate information, which are often necessary in evaluating laboratory test results. We construct the reference region using tolerance-based criteria so that the resulting region possesses the multiple use property. Results show that the proposed regions yield coverage probabilities that are accurate and are robust to the sample size. Finally, we apply the proposed procedures to a real-life example on the computation of an MRR for three components of the insulin-like growth factor system.},
  archive      = {J_JOAS},
  author       = {Iana Michelle L. Garcia and Michael Daniel C. Lucagbo},
  doi          = {10.1080/02664763.2024.2411614},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {1040-1062},
  shortjournal = {J. Appl. Stat.},
  title        = {Regression-based rectangular tolerance regions as reference regions in laboratory medicine},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian poisson regression tensor train decomposition model for learning mortality pattern changes during COVID-19 pandemic. <em>JOAS</em>, <em>52</em>(5), 1017-1039. (<a href='https://doi.org/10.1080/02664763.2024.2411608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 has led to excess deaths around the world. However, the impact on mortality rates from other causes of death during this time remains unclear. To understand the broader impact of COVID-19 on other causes of death, we analyze Italian official data covering monthly mortality counts from January 2015 to December 2020. To handle the high-dimensional nature of the data, we developed a model that combines Poisson regression with tensor train decomposition to explore the lower-dimensional residual structure of the data. Our Bayesian approach incorporates prior information on model parameters and utilizes an efficient Metropolis-Hastings within Gibbs algorithm for posterior inference. Simulation studies were conducted to validate our approach. Our method not only identifies differential effects of interventions on cause-specific mortality rates through Poisson regression but also provides insights into the relationship between COVID-19 and other causes of death. Additionally, it uncovers latent classes related to demographic characteristics, temporal patterns, and causes of death.},
  archive      = {J_JOAS},
  author       = {Wei Zhang and Antonietta Mira and Ernst C. Wit},
  doi          = {10.1080/02664763.2024.2411608},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {1017-1039},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian poisson regression tensor train decomposition model for learning mortality pattern changes during COVID-19 pandemic},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clustering in point processes on linear networks using nearest neighbour volumes. <em>JOAS</em>, <em>52</em>(5), 993-1016. (<a href='https://doi.org/10.1080/02664763.2024.2411214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel method specifically designed to detect clusters of points within linear networks. This method extends a classification approach used for point processes in spatial contexts. Unlike traditional methods that operate on planar spaces, our approach adapts to the unique geometric challenges of linear networks, where classical properties of point processes are altered, and intuitive data visualisation becomes more complex. Our method utilises the distribution of the K th nearest neighbour volumes, extending planar-based clustering techniques to identify regions of increased point density within a network. This approach is particularly effective for distinguishing overlapping Poisson processes within the same linear network. We demonstrate the practical utility of our method through applications to road traffic accident data from two Colombian cities, Bogota and Medellin. Our results reveal distinct clusters of high-density points in road segments where severe traffic accidents (resulting in injuries or fatalities) are most likely to occur, highlighting areas of increased risk. These clusters were primarily located on major arterial roads with high traffic volumes. In contrast, low-density points corresponded to areas with fewer accidents, likely due to lower traffic flow or other mitigating factors. Our findings provide valuable insights for urban planning and road safety management.},
  archive      = {J_JOAS},
  author       = {Juan F. Díaz-Sepúlveda and Nicoletta D'Angelo and Giada Adelfio and Jonatan A. González and Francisco J. Rodríguez-Cortés},
  doi          = {10.1080/02664763.2024.2411214},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {993-1016},
  shortjournal = {J. Appl. Stat.},
  title        = {Clustering in point processes on linear networks using nearest neighbour volumes},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The efficiency of CUSUM schemes for monitoring the multivariate coefficient of variation in short runs process. <em>JOAS</em>, <em>52</em>(4), 966-992. (<a href='https://doi.org/10.1080/02664763.2024.2405111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current monitoring technologies emphasize and address the issue of monitoring high-volume production processes. The high flexibility and diversity of current industrial production processes make monitoring technology for small batch processes even more important. In multivariate process monitoring, a broader applicability exists in multivariate coefficients of variation (MCV) based monitoring schemes due to the lower restriction of the process. In view of the effectiveness of MCV monitoring and with the aim to achieve further performance improvement of current MCV monitoring schemes in a finite horizon production, we additionally introduce two one-sided cumulative sum (CUSUM) MCV schemes. In the case of deterministic and random shifts, the design parameters of the proposed schemes are obtained via an optimization procedure designed by the Markov chain method and the corresponding performance is analysed based on different run length (RL) characteristics, including the mean and the standard deviation. Simulation comparisons with existing exponentially weighted moving average (EWMA) MCV schemes show that the proposed CUSUM MCV schemes are more efficient in monitoring most of the shifts, including the deterministic and random shifts. Finally, to demonstrate the benefits of the new monitoring schemes, a comprehensive case study on monitoring a steel sleeve manufacturing process is conducted.},
  archive      = {J_JOAS},
  author       = {Xuelong Hu and Yixuan Ma and Jiening Zhang and Jiujun Zhang and Ali Yeganeh and Sandile Charles Shongwe},
  doi          = {10.1080/02664763.2024.2405111},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {966-992},
  shortjournal = {J. Appl. Stat.},
  title        = {The efficiency of CUSUM schemes for monitoring the multivariate coefficient of variation in short runs process},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference for depending competing risks from Marshall–Olikin bivariate kies distribution under generalized progressive hybrid censoring. <em>JOAS</em>, <em>52</em>(4), 936-965. (<a href='https://doi.org/10.1080/02664763.2024.2405108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores inferences for a competing risk model with dependent causes of failure. When the lifetimes of competing risks are modelled by a Marshall–Olikin bivariate Kies distribution, classical and Bayesian estimations are studied under generalized progressive hybrid censoring. The existence and uniqueness results for maximum likelihood estimators of unknown parameters are established, whereas approximate confidence intervals are constructed using the observed Fisher information matrix. In addition, Bayes estimates are explored based on a flexible Gamma-Dirichlet prior information. Furthermore, when there is a priori order information on competing risk parameters being available, traditional classical likelihood and Bayesian estimates are also established under restricted parameter case. The behavior of the proposed estimators is evaluated through extensive simulation studies, and a real data study is presented for illustrative purposes.},
  archive      = {J_JOAS},
  author       = {Prakash Chandra and Hemanta Kumar Mandal and Yogesh Mani Tripathi and Liang Wang},
  doi          = {10.1080/02664763.2024.2405108},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {936-965},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference for depending competing risks from Marshall–Olikin bivariate kies distribution under generalized progressive hybrid censoring},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian inference for laplace distribution based on complete and censored samples with illustrations. <em>JOAS</em>, <em>52</em>(4), 914-935. (<a href='https://doi.org/10.1080/02664763.2024.2401470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, Bayesian estimates are derived for the location and scale parameters of the Laplace distribution based on complete, Type-I, and Type-II censored samples under different prior settings. Subsequently, Bayesian point and interval estimates, as well as the associated statistical inference, are discussed in detail. The developed methods are then applied to two real data sets for illustrative purposes. Moreover, a detailed Monte Carlo simulation study is carried out for evaluating the performance of the inferential methods developed here. Finally, we provide a brief discussion of the established results to demonstrate their practical utility and present some associated problems of further interest. Overall, this study fills an existing gap in the development of Bayesian inferential techniques for the parameters of the two-parameter Laplace distribution, making this research innovative and offering more investigative implications. It showcases the potential for broader methodological applications of Bayesian inference for complex real-world data sets, especially in scenarios involving different forms of censoring. This research provides a critical tool for statistical analysis in different fields such as engineering and finance, where the Laplace distribution is frequently adopted as a fundamental model.},
  archive      = {J_JOAS},
  author       = {Wanyue Sun and Xiaojun Zhu and Zhehao Zhang and N. Balakrishnan},
  doi          = {10.1080/02664763.2024.2401470},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {914-935},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian inference for laplace distribution based on complete and censored samples with illustrations},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Framework for constructing an optimal weighted score based on agreement. <em>JOAS</em>, <em>52</em>(4), 894-913. (<a href='https://doi.org/10.1080/02664763.2024.2399586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many medical studies, questionnaires or instruments with item ratings are often used to measure the health outcomes reflecting the disease status. Therefore, combining these item ratings to derive a score that reflects the disease status is an important issue. This paper proposes a new weighted method with the weights determined by maximizing the broad sense agreement Peng et al. [L. Peng, R. Li, Y. Guo, and A. Manatunga, A framework for assessing broad sense agreement between ordinal and continuous measurements, J. Am. Stat. Assoc. 106 (2011), pp. 1592–1601.] between the new score and the disease status measured by an ordinal scale. Theoretical and simulation results demonstrate the validity of our proposed optimal weights. We illustrate our method via an application to a post-traumatic stress disorder (PTSD) study.},
  archive      = {J_JOAS},
  author       = {Zhiping Qiu and Manatunga Amita and Limin Peng and Ying Guo and Tanja Jovanovic},
  doi          = {10.1080/02664763.2024.2399586},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {894-913},
  shortjournal = {J. Appl. Stat.},
  title        = {Framework for constructing an optimal weighted score based on agreement},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A class of infinite number of unbiased estimators using weighted squared distance for two-deck randomized response model. <em>JOAS</em>, <em>52</em>(4), 868-893. (<a href='https://doi.org/10.1080/02664763.2024.2399574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a collection of unbiased estimators for the proportion of a population bearing a sensitive characteristic using a randomized response technique with two decks of cards for any choice of weights. The efficiency of the estimator depends on the weights, and we demonstrate how to find an optimal choice. The coefficients of skewness and kurtosis are introduced. We support our findings with a simulation study that models a real survey dataset. We suggest that a careful choice of such weights can also lead to all estimates of proportion lying between [0, 1]. In addition, we illustrate the use of the estimators in a recent study that estimates the proportion of students, 18 years and over, who had returned to the campus and tested positive for COVID-19.},
  archive      = {J_JOAS},
  author       = {Daryan Naatjes and Stephen A. Sedory and Sarjinder Singh},
  doi          = {10.1080/02664763.2024.2399574},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {868-893},
  shortjournal = {J. Appl. Stat.},
  title        = {A class of infinite number of unbiased estimators using weighted squared distance for two-deck randomized response model},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate and efficient stock market index prediction: An integrated approach based on VMD-SNNs. <em>JOAS</em>, <em>52</em>(4), 841-867. (<a href='https://doi.org/10.1080/02664763.2024.2395961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stock market index typically mirrors the financial market's performance. Hence, accurate prediction of stock market index trends is essential for investors aiming to mitigate financial risk and enhance future investment returns. Traditional statistical approaches often struggle with the non-linear nature of stock market index data, leading to potential inaccuracies in long-term predictions. To address this issue, we introduce the TCN-LSTM-SNN (TLSNN) model, a hybrid framework that integrates Long Short-Term Memory (LSTM) and Temporal Convolutional Network (TCN) for robust feature extraction, within a highly efficient Spiking Neural Network (SNN) architecture. Additionally, we employ the Subtraction-Average-Based Optimizer (SABO) to refine the Variational Mode Decomposition (VMD) technique, thereby separating the periodic and trend components of stock indices, reducing noise interference, and establishing a decomposition ensemble framework to bolster the model's resilience. The experimental results show that the VMD-TLSNN hybrid model suggested in this study surpasses other individual benchmark models and their hybrid models in prediction accuracy. Additionally, it demonstrates notably lower energy consumption compared to other hybrid models.},
  archive      = {J_JOAS},
  author       = {Xuchang Chen and Guoqiang Tang and Yumei Ren and Xin Lin and Tongzhi Li},
  doi          = {10.1080/02664763.2024.2395961},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {841-867},
  shortjournal = {J. Appl. Stat.},
  title        = {Accurate and efficient stock market index prediction: An integrated approach based on VMD-SNNs},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On function-on-function linear quantile regression. <em>JOAS</em>, <em>52</em>(4), 814-840. (<a href='https://doi.org/10.1080/02664763.2024.2395960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present two innovative functional partial quantile regression algorithms designed to accurately and efficiently estimate the regression coefficient function within the function-on-function linear quantile regression model. Our algorithms utilize functional partial quantile regression decomposition to effectively project the infinite-dimensional response and predictor variables onto a finite-dimensional space. Within this framework, the partial quantile regression components are approximated using a basis expansion approach. Consequently, we approximate the infinite-dimensional function-on-function linear quantile regression model using a multivariate quantile regression model constructed from these partial quantile regression components. To evaluate the efficacy of our proposed techniques, we conduct a series of Monte Carlo experiments and analyze an empirical dataset, demonstrating superior performance compared to existing methods in finite-sample scenarios. Our techniques have been implemented in the ffpqr package in .},
  archive      = {J_JOAS},
  author       = {Muge Mutis and Ufuk Beyaztas and Filiz Karaman and Han Lin Shang},
  doi          = {10.1080/02664763.2024.2395960},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {814-840},
  shortjournal = {J. Appl. Stat.},
  title        = {On function-on-function linear quantile regression},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile regression based method for characterizing risk-specific behavioral patterns in relation to longitudinal left-censored biomarker data collected from heterogeneous populations. <em>JOAS</em>, <em>52</em>(4), 779-813. (<a href='https://doi.org/10.1080/02664763.2024.2394784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many studies aimed at promoting positive lifestyle behaviors to reduce lifetime risk of cancer and related diseases. However, assessing these modifiable behaviors through statistical modeling is challenging because of the multidimensionality of interrelated measurements that may dramatically differ between at-risk individuals. Taking into account this heterogeneity while considering the multidimensionality of behavior changes is fundamental to tailoring interventions to their needs. Biomarkers that identify high-risk individuals may help validate proximal measures, but the number of validated methods that link biomarkers to multiple behavioral measurements by determining their dynamic relations with disease risks is limited to just a few, since it requires an advanced statistical methodology to address challenges in analyzing biomarker data, including left-censoring due to limits of detection. To address these challenges, we propose a method that constructs a quantile-specific weighted index of multiple behavioral measurements. Under the quantile regression framework, the proposed method renders a multidimensional view of risk-specific behavioral patterns by connecting them with biomarker levels to provide better insights into heterogeneous behavioral profiles among at-risk populations. We evaluate performances of the proposed method through simulations, and illustrate its applications to the Tu Salud ¡Sí Cuenta! data by examining behavior changes among Mexican-American adults.},
  archive      = {J_JOAS},
  author       = {MinJae Lee and Belinda M. Reininger and Kelley Pettee Gabriel and Nalini Ranjit and Larkin L. Strong},
  doi          = {10.1080/02664763.2024.2394784},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {779-813},
  shortjournal = {J. Appl. Stat.},
  title        = {Quantile regression based method for characterizing risk-specific behavioral patterns in relation to longitudinal left-censored biomarker data collected from heterogeneous populations},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A partitioned weighted moving average control chart. <em>JOAS</em>, <em>52</em>(3), 744-777. (<a href='https://doi.org/10.1080/02664763.2024.2392122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A partitioned weighted moving average (PWMA) chart is developed by partitioning the samples (or observations) into two groups, calculating the groups’ weighted average and adding them. This partitioning gives more control over weight distribution in the most recent j (= 2, 3, …) samples. The PWMA, exponentially weighted moving average (EWMA) and homogenously weighted moving average (HWMA) charts are compared. For zero state, the PWMA chart outperforms the EWMA and HWMA charts for most ( n , λ , δ ) values and the outperformance of the former over the two latter charts increases with the time period ( j ), employed in the partitioning. Here, λ is the charts’ smoothing constant and δ is the shift size (multiples of standard deviation). For steady state, the PWMA chart (regardless of j ) generally outperforms the EWMA chart in detecting a small shift ( δ = 0.25) when the smoothing constant λ ≥ 0.2 for the sample size n = 1; while a larger λ is needed for n = 5. Moreover, for steady state, the PWMA chart outperforms the HWMA chart in detecting small and moderate shifts (0.25 ≤ δ ≤ 1), regardless of ( λ , n , j ). The PWMA chart demonstrates robustness to non-normality and estimated process parameters.},
  archive      = {J_JOAS},
  author       = {Raja Fawad Zafar and Michael B.C. Khoo and Huay Woon You and Sajal Saha and Wai Chung Yeong},
  doi          = {10.1080/02664763.2024.2392122},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {744-777},
  shortjournal = {J. Appl. Stat.},
  title        = {A partitioned weighted moving average control chart},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-sample nonparametric test for one-sided location-scale alternative. <em>JOAS</em>, <em>52</em>(3), 715-743. (<a href='https://doi.org/10.1080/02664763.2024.2392119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increase in location is often accompanied by an increase in variability. Moreover, in randomized studies, the presence of heteroscedasticity can indicate a treatment effect. In these cases a location-scale test is appropriate. A common approach of a location-scale test is to use the sum of a location and a scale test statistic. In this study, we focus on one-sided location-scale tests. We introduce a one-sided Lepage-type test statistic. Since right-skewed data are common in many real-world applications, we design test statistics that are powerful also for right-skewed data. Moreover, this paper introduces both a maximum test and an adaptive test utilizing the one-sided Lepage-type test statistics. The limiting distributions of the maximum test statistics are also derived. We assess the performance of the different test statistics in various scenarios for continuous distributions through Monte Carlo simulations. Our simulation results demonstrate that the proposed new test statistics can significantly increase and stabilize statistical power, making it strong competitors to existing location-scale tests. We present practical illustrations based on example data and conclude with some final remarks.},
  archive      = {J_JOAS},
  author       = {Hidetoshi Murakami and Markus Neuhäuser},
  doi          = {10.1080/02664763.2024.2392119},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {715-743},
  shortjournal = {J. Appl. Stat.},
  title        = {A two-sample nonparametric test for one-sided location-scale alternative},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling the time to dropout under phase-wise variable stress fixed cohort setup. <em>JOAS</em>, <em>52</em>(3), 702-714. (<a href='https://doi.org/10.1080/02664763.2024.2392113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The event of a student dropping out from an academic program depends on several factors namely course content, change in interest, financial problems among many others. These factors vary interdependently with different phases of the academic program. We assume that the factors put different amount of academic stresses on a student in different phases of the program. We formulate and analyze such an accumulated-stress model under the assumption that the attrition time at each phase follows the Kumaraswamy distribution. A hazard-rate based approach is used to model the accumulated stress through different phases. At each phase the stress levels vary. Accordingly, we estimate the model parameters based on the frequentist approach. Extensive simulation experiments indicate satisfactory performance of the estimators. A synthetic dataset related to students' dropout has been analyzed for illustrative purpose.},
  archive      = {J_JOAS},
  author       = {Aniket Biswas and Subrata Chakraborty and Anupama Nandi},
  doi          = {10.1080/02664763.2024.2392113},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {702-714},
  shortjournal = {J. Appl. Stat.},
  title        = {Modeling the time to dropout under phase-wise variable stress fixed cohort setup},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast algorithms of computing admissible intervals for discrete distributions with single parameter. <em>JOAS</em>, <em>52</em>(3), 687-701. (<a href='https://doi.org/10.1080/02664763.2024.2392105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is of great interest to compute optimal exact confidence intervals for the success probability ( p ) in a binomial distribution, the number of subjects with a certain attribute ( M ) or the total number of subjects ( N ) in a hypergeometric distribution, and the mean λ of a Poisson distribution. In this paper, efficient algorithms are proposed to compute an admissible exact interval for each of the four parameters when the sample size ( n ) or the random observation X is large. The algorithms are utilized in four practical examples: evaluating the relationship between two diseases, certifying companies, estimating the proportion of drug users, and analyzing earthquake frequency. The intervals computed by the algorithms are shorter, and the calculations are faster, demonstrating the accuracy of the results and the time efficiency of the proposed algorithms.},
  archive      = {J_JOAS},
  author       = {Weizhen Wang and Chongxiu Yu and Zhongzhan Zhang},
  doi          = {10.1080/02664763.2024.2392105},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {687-701},
  shortjournal = {J. Appl. Stat.},
  title        = {Fast algorithms of computing admissible intervals for discrete distributions with single parameter},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of the mean exponential survival time under a sequential censoring scheme. <em>JOAS</em>, <em>52</em>(3), 669-686. (<a href='https://doi.org/10.1080/02664763.2024.2386609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential distribution can provide a simple and appealing survival-time model in reliability analysis and life tests. Due to certain experimental designs, time limitations, budgetary requirements, and other reasons, however, only censored data can be obtained. In this paper, we propose a novel estimation procedure for the mean survival time of an exponential distribution under a sequential censoring scheme, which can be treated as a combination of type I censoring and type II censoring. The procedure makes a trade-off between estimation error and sampling cost, using the minimum number of observations. An extensive set of Monte Carlo simulations is conducted to further validate its remarkable performance. To demonstrate the practical applicability, we then implement this newly proposed procedure to assess the reliability of various Backblaze's hard disk models.},
  archive      = {J_JOAS},
  author       = {Jun Hu and Hon Yiu So and Yan Zhuang},
  doi          = {10.1080/02664763.2024.2386609},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {669-686},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation of the mean exponential survival time under a sequential censoring scheme},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample size calculation for the sequential parallel comparison design with binary endpoint using exact methods. <em>JOAS</em>, <em>52</em>(3), 656-668. (<a href='https://doi.org/10.1080/02664763.2024.2385997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High placebo responses in clinical trial could reduce the treatment effect leading to the failure of a promising drug. Several strategies have been developed to minimize high placebo responses, including the sequential parallel comparison design (SPCD). For a study with binary outcome, the existing statistical methods to test the drug effectiveness always rely on the asymptotic limiting distribution. When a study's sample size is small to medium, the asymptotic approaches do not have satisfactory performance with regard to type I error rate and statistical power. For that reason, we propose utilizing exact conditional approach to calculate sample size based on the existing test statistics to order the sample space. The proposed method controls the type I error rate under the unconditional framework. We compare the proposed exact sample sizes using different test statistics and the sample size for a randomized parallel study. We would recommend using exact sample sizes for the SPCD with small- to medium sample sizes and the SPCD with extreme response rates.},
  archive      = {J_JOAS},
  author       = {Guogen Shan and Yahui Zhang},
  doi          = {10.1080/02664763.2024.2385997},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {656-668},
  shortjournal = {J. Appl. Stat.},
  title        = {Sample size calculation for the sequential parallel comparison design with binary endpoint using exact methods},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixtures of logistic normal multinomial regression models for microbiome data. <em>JOAS</em>, <em>52</em>(3), 624-655. (<a href='https://doi.org/10.1080/02664763.2024.2383286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of bioinformatics, we frequently encounter discrete data, particularly microbiome taxa count data obtained through 16S rRNA sequencing. These microbiome datasets are commonly characterized by their high dimensionality and the ability to provide insights solely into relative abundance, necessitating their classification as compositional data. Analyzing such data presents challenges due to their confinement within a simplex. Additionally, microbiome taxa counts are subject to influence by various biological and environmental factors like age, gender, and diet. Thus, we have developed a novel approach involving regression-based mixtures of logistic normal multinomial models for clustering microbiome data. These models effectively categorize samples into more homogeneous subpopulations, enabling the exploration of relationships between bacterial abundance and biological or environmental covariates within each identified group. To enhance the accuracy and efficiency of parameter estimation, we employ a robust framework based on variational Gaussian approximations (VGA). Our proposed method's effectiveness is demonstrated through its application to simulated and real datasets.},
  archive      = {J_JOAS},
  author       = {Wenshu Dai and Yuan Fang and Sanjeena Subedi},
  doi          = {10.1080/02664763.2024.2383286},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {624-655},
  shortjournal = {J. Appl. Stat.},
  title        = {Mixtures of logistic normal multinomial regression models for microbiome data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast maximum likelihood estimation for general hierarchical models. <em>JOAS</em>, <em>52</em>(3), 595-623. (<a href='https://doi.org/10.1080/02664763.2024.2383284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical statistical models are important in applied sciences because they capture complex relationships in data, especially when variables are related by space, time, sampling unit, or other shared features. Existing methods for maximum likelihood estimation that rely on Monte Carlo integration over latent variables, such as Monte Carlo Expectation Maximization (MCEM), suffer from drawbacks in efficiency and/or generality. We harness a connection between sampling-stepping iterations for such methods and stochastic gradient descent methods for non-hierarchical models: many noisier steps can do better than few cleaner steps. We call the resulting methods Hierarchical Model Stochastic Gradient Descent (HMSGD) and show that combining efficient, adaptive step-size algorithms with HMSGD yields efficiency gains. We introduce a one-dimensional sampling-based greedy line search for step-size determination. We implement these methods and conduct numerical experiments for a Gamma-Poisson mixture model, a generalized linear mixed models (GLMMs) with single and crossed random effects, and a multi-species ecological occupancy model with over 3000 latent variables. Our experiments show that the accelerated HMSGD methods provide faster convergence than commonly used methods and are robust to reasonable choices of MCMC sample size.},
  archive      = {J_JOAS},
  author       = {Johnny Hong and Sara Stoudt and Perry de Valpine},
  doi          = {10.1080/02664763.2024.2383284},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {595-623},
  shortjournal = {J. Appl. Stat.},
  title        = {Fast maximum likelihood estimation for general hierarchical models},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel phase II single-arm hybrid design to minimize trial duration and enhance subsequent phase III trial success rate. <em>JOAS</em>, <em>52</em>(3), 578-594. (<a href='https://doi.org/10.1080/02664763.2024.2382135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many Phase III trials fail for lack of significant efficacy for the testing agents although their success has been demonstrated in preceding Phase II trials. One of the major reasons for this discordance is the use of different endpoints in Phase II and III trials, respectively. In oncology clinical trials, tumor response (surrogate endpoint) can be determined quickly whereas survival estimation (direct endpoint) requires a long period of follow-up. To better bridge the gap between two endpoints, we propose a novel two-stage single-arm hybrid design for Phase II trials whereby the percent of tumor size change is used as an initial screening to select potentially effective agents within a short time interval followed by a second screening stage where survival is estimated to confirm the efficacy of agents. This design can improve trial efficiency and reduce cost by early stopping the evaluation of an ineffective agent based on the low percent of tumor size change. The second survival endpoint screening will substantially increase the success rate of the follow-up Phase III trial by using the same endpoint. Simulation studies demonstrated that our novel design has improved in terms of trial efficiency, trial length, and the success rate of following Phase III trials.},
  archive      = {J_JOAS},
  author       = {Jun Lu and Yuzi Zhang and Ying Cui and Limin Peng and Zhengjia Chen},
  doi          = {10.1080/02664763.2024.2382135},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {578-594},
  shortjournal = {J. Appl. Stat.},
  title        = {A novel phase II single-arm hybrid design to minimize trial duration and enhance subsequent phase III trial success rate},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance of tests based on the area under the ROC curve for multireader diagnostic data. <em>JOAS</em>, <em>52</em>(3), 555-577. (<a href='https://doi.org/10.1080/02664763.2024.2374931'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main objectives of disease prevention is to lower the healthcare costs and improve the quality of life. To achieve this, reliable diagnostic tools are needed. The diagnostic performance of a tool can be measured by the ROC curve and the AUC. However, some diagnostic tools such as MRI images are not objective, but depend on the interpretation of experts. Therefore, the accuracy of these tools may vary depending on who is interpreting them. To account for possible correlations when multiple readers collect data, Dorfman, Berbaum and Metz (1992) proposed using AUC pseudovalues from the jackknife sampling method and applying them to the mixed model to analyze the diagnostic reagent's accuracy. However, pseudovalues may go beyond the AUC range. Also, the random effect estimate may be negative due to a small number of readers. This paper develops tests based on AUC estimates and gives their asymptotic distribution. Moreover, a two-stage test is suggested to correct for negative random effect estimates. Four tests are created in total and their performance is evaluated by Monte Carlo simulations. The distributional assumption's robustness of these tests is checked, and their applicability is demonstrated by two real data sets.},
  archive      = {J_JOAS},
  author       = {Yi-Ting Hwang and Ya-Ru Hsu and Nan-Cheng Su},
  doi          = {10.1080/02664763.2024.2374931},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {555-577},
  shortjournal = {J. Appl. Stat.},
  title        = {Performance of tests based on the area under the ROC curve for multireader diagnostic data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating changepoints in extremal dependence, applied to aviation stock prices during COVID-19 pandemic. <em>JOAS</em>, <em>52</em>(3), 525-554. (<a href='https://doi.org/10.1080/02664763.2024.2373939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dependence in the tails of the joint distribution of two random variables is generally assessed using χ -measure, the limiting conditional probability of one variable being extremely high given the other variable is also extremely high. This work is motivated by the structural changes in χ -measure between the daily rate of return (RoR) of the two Indian airlines, IndiGo and SpiceJet, during the COVID-19 pandemic. We model the daily maximum and minimum RoR vectors (potentially transformed) using the bivariate Hüsler-Reiss (BHR) distribution. To estimate the changepoint in the χ -measure of the BHR distribution, we explore two changepoint detection procedures based on the Likelihood Ratio Test (LRT) and Modified Information Criterion (MIC). We obtain critical values and power curves of the LRT and MIC test statistics for low through high values of χ -measure. We also explore the consistency of the estimators of the changepoint based on LRT and MIC numerically. In our data application, for RoR maxima and minima, the most prominent changepoints detected by LRT and MIC are close to the announcement of the first phases of lockdown and unlock, respectively, which are realistic; thus, our study would be beneficial for portfolio optimization in the case of future pandemic situations.},
  archive      = {J_JOAS},
  author       = {Arnab Hazra and Shiladitya Bose},
  doi          = {10.1080/02664763.2024.2373939},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {525-554},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating changepoints in extremal dependence, applied to aviation stock prices during COVID-19 pandemic},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network-based topic structure visualization. <em>JOAS</em>, <em>52</em>(2), 509-523. (<a href='https://doi.org/10.1080/02664763.2024.2369953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, many topics are inter-correlated, making it challenging to investigate their structure and relationships. Understanding the interplay between topics and their relevance can provide valuable insights for researchers, guiding their studies and informing the direction of research. In this paper, we utilize the topic-words distribution, obtained from topic models, as item-response data to model the structure of topics using a latent space item response model. By estimating the latent positions of topics based on their distances toward words, we can capture the underlying topic structure and reveal their relationships. Visualizing the latent positions of topics in Euclidean space allows for an intuitive understanding of their proximity and associations. We interpret relationships among topics by characterizing each topic based on representative words selected using a newly proposed scoring scheme. Additionally, we assess the maturity of topics by tracking their latent positions using different word sets, providing insights into the robustness of topics. To demonstrate the effectiveness of our approach, we analyze the topic composition of COVID-19 studies during the early stage of its emergence using biomedical literature in the PubMed database. The software and data used in this paper are publicly available at https://github.com/jeon9677/gViz .},
  archive      = {J_JOAS},
  author       = {Yeseul Jeon and Jina Park and Ick Hoon Jin and Dongjun Chung},
  doi          = {10.1080/02664763.2024.2369953},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {509-523},
  shortjournal = {J. Appl. Stat.},
  title        = {Network-based topic structure visualization},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference and diagnostics for censored linear regression model with skewed generalized t distribution. <em>JOAS</em>, <em>52</em>(2), 477-508. (<a href='https://doi.org/10.1080/02664763.2024.2373933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, some interested data can be recorded only if the values fall within an interval range, and the responses are often subject to censoring. Attempting to perform effective statistical analysis with censored, especially heavy-tailed and asymmetric data, can be difficult. In this paper, we develop a novel linear regression model based on the proposed skewed generalized t distribution for censored data. The likelihood-based inference and diagnostic analysis are established using the Expectation/Conditional Maximization Either algorithm in conjunction with smoothing approximate functions. We derive relevant measures to perform global influence for this novel model and develop local influence analysis based on the conditional expectation of the complete-data log-likelihood function. Some useful perturbation schemes are discussed. We illustrate the finite sample performance and the robustness of the proposed method by simulation studies. The proposed model is compared with other procedures based on a real dataset, and a sensitivity analysis is also conducted.},
  archive      = {J_JOAS},
  author       = {Chengdi Lian and Yaohua Rong and Jinwen Liang and Ruijie Guan and Weihu Cheng},
  doi          = {10.1080/02664763.2024.2373933},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {477-508},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference and diagnostics for censored linear regression model with skewed generalized t distribution},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference on exponentiated rayleigh distribution with constant stress partially accelerated life tests under progressive type-II censoring. <em>JOAS</em>, <em>52</em>(2), 448-476. (<a href='https://doi.org/10.1080/02664763.2024.2373930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to explore the issues of evaluating the parameters and the accelerating factor based on constant stress for partially accelerating life tests when the potential failure times have an exponentiated Rayleigh distribution. Within the framework of progressive Type-II censoring schemes, we employ the Newton-Raphson algorithm as an iterative methodology to gain the maximum likelihood estimates, accompanied by proof of the existence of these point estimators. We also construct asymptotic confidence intervals for interested parameters and acceleration factors by utilizing the asymptotical characteristics of the maximum likelihood estimators. The Bayesian estimations of unknown parameters are derived by using the independent gamma priors and dependent Gamma-Dirichlet prior on the basis of square error and relatively smooth LINEX loss functions, respectively. Furthermore, we adopt the importance sampling method to compute Bayesian point estimates and the credible intervals with the highest posterior density. To validate the effectiveness of the suggested approaches, a series of simulated experiments are carried out. Lastly, we conduct analyzes on two actual datasets to show the applicability of the suggested techniques.},
  archive      = {J_JOAS},
  author       = {Huiying Yao and Wenhao Gui},
  doi          = {10.1080/02664763.2024.2373930},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {448-476},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference on exponentiated rayleigh distribution with constant stress partially accelerated life tests under progressive type-II censoring},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A discrete weibull proportional odds survival model. <em>JOAS</em>, <em>52</em>(2), 429-447. (<a href='https://doi.org/10.1080/02664763.2024.2373929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to present the development of the Proportional Odds Model for discrete time-to-event data in a survival analysis context. In this work, inferences about the model parameters considering the discrete Weibull as baseline distribution are presented, obtaining the point and interval estimates of the model parameters. In addition, procedures for checking the proportional odds assumption were proposed. Simulation studies were performed to evaluate the asymptotic properties of the estimators. The proposed model is illustrated using a dataset on the survival time of patients with Leukemia.},
  archive      = {J_JOAS},
  author       = {Marcílio Ramos Pereira Cardial and Juliana Cobre and Eduardo Yoshio Nakano},
  doi          = {10.1080/02664763.2024.2373929},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {429-447},
  shortjournal = {J. Appl. Stat.},
  title        = {A discrete weibull proportional odds survival model},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Marginalized LASSO in the low-dimensional difference-based partially linear model for variable selection. <em>JOAS</em>, <em>52</em>(2), 400-428. (<a href='https://doi.org/10.1080/02664763.2024.2372676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difference-based partially linear model is an appropriate regression model when both linear and nonlinear predictors are present in the data. However, when we want to optimize the weights using the difference-based method, the problem of variable selection can be difficult since low-variance predictors present a challenge. Therefore, this study aims to establish a novel methodology based on marginal theory to tackle such mixed relationships successfully, emphasizing variable selection in low dimensions. We suggest using a marginalized LASSO estimator with a penalty term that is not as severe and related to the difference order. As part of our numerical analysis of small sample performance, we undertake comprehensive simulation experiments to numerically demonstrate the strength of our proposed technique in estimation and prediction compared to the LASSO under a low-dimensional setup. This is done so that we can numerically demonstrate the strength of our proposed method in estimation and prediction. The bootstrapped method is utilized to evaluate how well our proposed prediction method performs when examining the King House dataset.},
  archive      = {J_JOAS},
  author       = {M. Norouzirad and R. Moura and M. Arashi and F. J. Marques},
  doi          = {10.1080/02664763.2024.2372676},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {400-428},
  shortjournal = {J. Appl. Stat.},
  title        = {Marginalized LASSO in the low-dimensional difference-based partially linear model for variable selection},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time of week intensity estimation from partly interval censored data with applications to police patrol planning. <em>JOAS</em>, <em>52</em>(2), 381-399. (<a href='https://doi.org/10.1080/02664763.2024.2371901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Law enforcement agencies are tasked with crime prevention and crime reduction under limited resources. Having an accurate temporal estimate of the crime rate would be valuable to achieve such a goal. However, estimation is usually complicated by the interval censored nature of crime data. We cast the problem of intensity estimation as a Poisson regression using an EM algorithm to estimate the parameters. Two special penalties are added that provide smoothness over the time of day and day of week. This approach provides accurate intensity estimates and can also uncover day of week clusters that share the same intensity patterns. Both simulated and real crime data gathered from the city of Cincinnati and the city of Dallas are used to demonstrate the effectiveness of the proposed model.},
  archive      = {J_JOAS},
  author       = {Jiahao Tian and Michael D. Porter},
  doi          = {10.1080/02664763.2024.2371901},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {381-399},
  shortjournal = {J. Appl. Stat.},
  title        = {Time of week intensity estimation from partly interval censored data with applications to police patrol planning},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate identification of single-cell types via correntropy-based sparse PCA combining hypergraph and fusion similarity. <em>JOAS</em>, <em>52</em>(2), 356-380. (<a href='https://doi.org/10.1080/02664763.2024.2369955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of single-cell RNA sequencing (scRNA-seq) technology enables researchers to gain deep insights into cellular heterogeneity. However, the high dimensionality and noise of scRNA-seq data pose significant challenges to clustering. Therefore, we propose a new single-cell type identification method, called CHLSPCA, to address these challenges. In this model, we innovatively combine correntropy with PCA to address the noise and outliers inherent in scRNA-seq data. Meanwhile, we integrate the hypergraph into the model to extract more valuable information from the local structure of the original data. Subsequently, to capture crucial similarity information not considered by the PCA model, we employ the Gaussian kernel function and the Euclidean metric to mine the similarity information between cells, and incorporate this information into the model as the similarity constraint. Furthermore, the principal components (PCs) of PCA are very dense. A new sparse constraint is introduced into the model to gain sparse PCs. Finally, based on the principal direction matrix learned from CHLSPCA, we conduct extensive downstream analyses on real scRNA-seq datasets. The experimental results show that CHLSPCA performs better than many popular clustering methods and is expected to promote the understanding of cellular heterogeneity in scRNA-seq data analysis and support biomedical research.},
  archive      = {J_JOAS},
  author       = {Juan Wang and Tai-Ge Wang and Shasha Yuan and Feng Li},
  doi          = {10.1080/02664763.2024.2369955},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {356-380},
  shortjournal = {J. Appl. Stat.},
  title        = {Accurate identification of single-cell types via correntropy-based sparse PCA combining hypergraph and fusion similarity},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Depth-based statistical analysis in the spike train space. <em>JOAS</em>, <em>52</em>(2), 329-355. (<a href='https://doi.org/10.1080/02664763.2024.2369954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric-based summary statistics such as mean and covariance have been introduced in neural spike train space. They can properly describe template and variability in spike train data, but are often sensitive to outliers and expensive to compute. Recent studies also examine outlier detection and classification methods on point processes. These tools provide reasonable result, whereas the accuracy remains at a low level in certain cases. In this study, we propose to adopt a well-established notion of statistical depth to the spike train space. This framework can naturally define the median in a set of spike trains, which provides a robust description of the ‘template’ of the observations. It also provides a principled method to identify ‘outliers’ and classify data from different categories. We systematically compare the new median, outlier detection and classification tools with state-of-the-art competing methods. The result shows the median has superior description for template than the mean. Moreover, the proposed outlier detection and classification perform more accurately than previous methods.},
  archive      = {J_JOAS},
  author       = {Xinyu Zhou and Wei Wu},
  doi          = {10.1080/02664763.2024.2369954},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {329-355},
  shortjournal = {J. Appl. Stat.},
  title        = {Depth-based statistical analysis in the spike train space},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diagnostics for categorical response models based on quantile residuals and distance measures. <em>JOAS</em>, <em>52</em>(2), 306-328. (<a href='https://doi.org/10.1080/02664763.2024.2367150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polytomous categorical data are frequent in studies, that can be obtained with an individual or grouped structure. In both structures, the generalized logit model is commonly used to relate the covariates on the response variable. After fitting a model, one of the challenges is the definition of an appropriate residual and choosing diagnostic techniques. Since the polytomous variable is multivariate, raw, Pearson, or deviance residuals are vectors and their asymptotic distribution is generally unknown, which leads to difficulties in graphical visualization and interpretation. Therefore, the definition of appropriate residuals and the choice of the correct analysis in diagnostic tools is important, especially for nominal data, where a restriction of methods is observed. This paper proposes the use of randomized quantile residuals associated with individual and grouped nominal data, as well as Euclidean and Mahalanobis distance measures, as an alternative to reduce the dimension of the residuals. We developed simulation studies with both data structures associated. The half-normal plots with simulation envelopes were used to assess model performance. These studies demonstrated a good performance of the quantile residuals, and the distance measurements allowed a better interpretation of the graphical techniques. We illustrate the proposed procedures with two applications to real data.},
  archive      = {J_JOAS},
  author       = {Patrícia Peres Araripe and Idemauro Antonio Rodrigues de Lara and Gabriel Rodrigues Palma and Niamh Cahill and Rafael de Andrade Moral},
  doi          = {10.1080/02664763.2024.2367150},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {306-328},
  shortjournal = {J. Appl. Stat.},
  title        = {Diagnostics for categorical response models based on quantile residuals and distance measures},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian high-dimensional mediation analysis for multilevel genome-wide epigenetic data. <em>JOAS</em>, <em>52</em>(2), 287-305. (<a href='https://doi.org/10.1080/02664763.2024.2367148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis has increasingly become a popular practice in various clinical trials and epidemiological applications to evaluate whether an intermediate variable is on the pathway between the exposure of interest and a response. Previous mediation analyses in the literature mainly focused on settings with a single or low-dimensional mediators and single-level data. In this article, we propose a Bayesian causal mediation analysis method that can handle our multilevel intergenerational epigenetic mechanisms study (IEMS) with high-dimensional mediators. Specifically, we develop a Bayesian hierarchical model for data with such complexity, and then employ the Bayesian spike-and-slab priors on the exposure-mediator-outcome effect pathway to identify active mediators involved in mediation. We derive the natural indirect and direct effects based on our hierarchical model and provide statistical inference based on Markov chain Monte Carlo (MCMC) methods. Our simulation study demonstrates that our proposed Bayesian method outperforms other alternative methods in various scenarios. We further illustrate the utility of our method to IEMS to assess the causal mechanisms between maternal exposure to climate extremes and offspring's growth outcomes through DNA methylation.},
  archive      = {J_JOAS},
  author       = {Xi Qiao and Duy Ngo and Bilinda Straight and Belinda L. Needham and Charles E. Hilton and Amy Naugle},
  doi          = {10.1080/02664763.2024.2367148},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {287-305},
  shortjournal = {J. Appl. Stat.},
  title        = {A bayesian high-dimensional mediation analysis for multilevel genome-wide epigenetic data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust convex biclustering with a tuning-free method. <em>JOAS</em>, <em>52</em>(2), 271-286. (<a href='https://doi.org/10.1080/02664763.2024.2367143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering is widely used in different kinds of fields including gene information analysis, text mining, and recommendation system by effectively discovering the local correlation between samples and features. However, many biclustering algorithms will collapse when facing heavy-tailed data. In this paper, we propose a robust version of convex biclustering algorithm with Huber loss. Yet, the newly introduced robustification parameter brings an extra burden to selecting the optimal parameters. Therefore, we propose a tuning-free method for automatically selecting the optimal robustification parameter with high efficiency. The simulation study demonstrates the more fabulous performance of our proposed method than traditional biclustering methods when encountering heavy-tailed noise. A real-life biomedical application is also presented. The R package RcvxBiclustr is available at https://github.com/YifanChen3/RcvxBiclustr .},
  archive      = {J_JOAS},
  author       = {Yifan Chen and Chunyin Lei and Chuanquan Li and Haiqiang Ma and Ningyuan Hu},
  doi          = {10.1080/02664763.2024.2367143},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {271-286},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust convex biclustering with a tuning-free method},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing heart PET scans: An adjustment of kolmogorov-smirnov test under spatial autocorrelation. <em>JOAS</em>, <em>52</em>(1), 253-269. (<a href='https://doi.org/10.1080/02664763.2024.2366300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The principle of independence is a fundamental yet often disregarded assumption in statistical inference. It is observed that the implications of correlations, if not considered, can lead to a conservative estimation of Type I error in the presence of positive linear correlations when utilizing the Kolmogorov-Smirnov (KS) test. Conversely, negative linear correlations may engender a liberal estimation of Type I error. To address the impact of spatial autocorrelation in the analysis of Positron Emission Tomography (PET) images, we have proposed an innovative methodology to reconstruct a grid map of human heart scans using spherical coordinates. We have examined the distribution of the KS test statistic under spatial autocorrelation through Monte Carlo (MC) simulation and have introduced a KS test with a spatial adjustment. The newly proposed KS test with spatial adjustment demonstrates a controlled Type I error and power that is not inferior when compared to the original KS test. This suggests its potential utility in the analysis of spatially autocorrelated data.},
  archive      = {J_JOAS},
  author       = {Wenjun Zheng and Hongjian Zhu and K. Lance Gould and Dejian Lai},
  doi          = {10.1080/02664763.2024.2366300},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {253-269},
  shortjournal = {J. Appl. Stat.},
  title        = {Comparing heart PET scans: An adjustment of kolmogorov-smirnov test under spatial autocorrelation},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new EWMA chart for simultaneously monitoring the parameters of a shifted exponential distribution. <em>JOAS</em>, <em>52</em>(1), 221-252. (<a href='https://doi.org/10.1080/02664763.2024.2363404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various scenarios where products and services are accompanied by warranties to ensure their reliability over a specified time, the two-parameter (shifted) exponential distribution serves as a fundamental model for time-to-event data. In modern production process, the products often come with warranties, and their quality can be manifested by the changes in the scale and origin parameters of a shifted exponential (SE) distribution. This paper introduces the Max-EWMA chart, employing maximum likelihood estimators and exponentially weighted moving average (EWMA) statistics, to jointly monitor SE distribution parameters. Additionally, we extend two additional charts, namely the Max-DEWMA and Max-TEWMA charts to enhance early-stage shift detection. Performance evaluations under zero-state and steady-state conditions compare these charts with the existing Max-CUSUM chart in terms of expected value and standard deviation of the run length (RL) distribution. Our findings reveal that among the Max-EWMA schemes, the Max-EWMA SE chart outperforms the others in terms of steady-state performance, while the Max-TEWMA chart surpasses the Max-EWMA and Max-DEWMA SE charts in respect to zero-state performance. Moreover, the proposed Max-EWMA schemes demonstrate advantages over Max-CUSUM, especially for small to moderate smoothing constants. We also provide an illustrative example to demonstrate the implementation of the proposed schemes.},
  archive      = {J_JOAS},
  author       = {Amita Baranwal and Nirpeksh Kumar and Kashinath Chatterjee and Christos Koukouvinos},
  doi          = {10.1080/02664763.2024.2363404},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {221-252},
  shortjournal = {J. Appl. Stat.},
  title        = {A new EWMA chart for simultaneously monitoring the parameters of a shifted exponential distribution},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint modeling of correlated binary outcomes using multivariate logistic regression: Contraception and HIV knowledge in sri lanka. <em>JOAS</em>, <em>52</em>(1), 208-220. (<a href='https://doi.org/10.1080/02664763.2024.2363399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reproductive health significantly contributes to the overall well-being and social welfare of women. Within the spectrum of modern and traditional contraceptive methods in use, condoms have been strongly advocated by numerous HIV programs as a primary means of preventing HIV infection in Sri Lanka. Given the intrinsic relationship between contraceptive utilization and HIV awareness, our study aims to concurrently analyze the patterns of contraceptive usage and HIV knowledge, while accounting for their potential correlation. In this study, we introduced the application of the Gumbel type II distribution to effectively capture the interdependence of these joint probabilities, accounting for various covariates. The outcome of simulation studies demonstrated the superior performance of the integrated joint model, in comparison to the separate univariate models. Our findings highlighted several noteworthy risk factors associated with both contraceptive usage and HIV prevention knowledge. These included variables such as residence, level of education, wealth quintile, husband's education level, number of children, engagement with the newspapers, television viewership, and mobile phone usage. The Result indicates a positive association between the adoption of contraception and the awareness of HIV prevention measures, suggesting that individuals who actively embrace contraception are more likely to possess knowledge about preventing HIV transmission.},
  archive      = {J_JOAS},
  author       = {N. M. Wijesekara and N. Withanage and N. R. Abeynayake},
  doi          = {10.1080/02664763.2024.2363399},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {208-220},
  shortjournal = {J. Appl. Stat.},
  title        = {Joint modeling of correlated binary outcomes using multivariate logistic regression: Contraception and HIV knowledge in sri lanka},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outcome-guided bayesian clustering for disease subtype discovery using high-dimensional transcriptomic data. <em>JOAS</em>, <em>52</em>(1), 183-207. (<a href='https://doi.org/10.1080/02664763.2024.2362275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the tremendous heterogeneity of disease manifestations, many complex diseases that were once thought to be single diseases are now considered to have disease subtypes. Disease subtyping analysis, that is the identification of subgroups of patients with similar characteristics, is the first step to accomplish precision medicine. With the advancement of high-throughput technologies, omics data offers unprecedented opportunity to reveal disease subtypes. As a result, unsupervised clustering analysis has been widely used for this purpose. Though promising, the subtypes obtained from traditional quantitative approaches may not always be clinically meaningful (i.e. correlate with clinical outcomes). On the other hand, the collection of rich clinical data in modern epidemiology studies has the great potential to facilitate the disease subtyping process via omics data and to discovery clinically meaningful disease subtypes. Thus, we developed an outcome-guided Bayesian clustering (GuidedBayesianClustering) method to fully integrate the clinical data and the high-dimensional omics data. A Gaussian mixed model framework was applied to perform sample clustering; a spike-and-slab prior was utilized to perform gene selection; a mixture model prior was employed to incorporate the guidance from a clinical outcome variable; and a decision framework was adopted to infer the false discovery rate of the selected genes. We deployed conjugate priors to facilitate efficient Gibbs sampling. Our proposed full Bayesian method is capable of simultaneously (i) obtaining sample clustering (disease subtype discovery); (ii) performing feature selection (select genes related to the disease subtype); and (iii) utilizing clinical outcome variable to guide the disease subtype discovery. The superior performance of the GuidedBayesianClustering was demonstrated through simulations and applications of breast cancer expression data and Alzheimer's disease. An R package has been made publicly available on GitHub to improve the applicability of our method.},
  archive      = {J_JOAS},
  author       = {Lingsong Meng and Zhiguang Huo},
  doi          = {10.1080/02664763.2024.2362275},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {183-207},
  shortjournal = {J. Appl. Stat.},
  title        = {Outcome-guided bayesian clustering for disease subtype discovery using high-dimensional transcriptomic data},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse kernel k-means clustering. <em>JOAS</em>, <em>52</em>(1), 158-182. (<a href='https://doi.org/10.1080/02664763.2024.2362266'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is an essential technique that groups similar data points to uncover the underlying structure and features of the data. Although traditional clustering methods such as k -means are widely utilized, they have limitations in identifying nonlinear clusters. Thus, alternative techniques, such as kernel k -means and spectral clustering, have been developed to address this issue. However, another challenge arises when irrelevant variables are present in the data; this can be mitigated by employing variable selection methods such as the filter, wrapper, and embedded approaches. In this study, with a particular focus on kernel k -means clustering, we propose an embedded variable selection method using a tensor product space along with a general analysis of variance kernel for nonlinear clustering. Comprehensive experiments involving simulations and real data analysis demonstrated that the proposed method achieves competitive performance compared to existing approaches. Thus, the proposed method may serve as a reliable tool for accurate cluster identification and variable selection to gain insights into complex datasets.},
  archive      = {J_JOAS},
  author       = {Beomjin Park and Changyi Park and Sungchul Hong and Hosik Choi},
  doi          = {10.1080/02664763.2024.2362266},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {158-182},
  shortjournal = {J. Appl. Stat.},
  title        = {Sparse kernel k-means clustering},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian modeling framework for optimizing pre-hospital stroke triage decisions. <em>JOAS</em>, <em>52</em>(1), 135-157. (<a href='https://doi.org/10.1080/02664763.2024.2360590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ischemic stroke is responsible for significant morbidity and mortality in the United States and worldwide. Stroke treatment optimization requires emergency medical personnel to make rapid triage decisions concerning destination hospitals that may differ in their ability to provide highly time-sensitive pharmaceutical and surgical interventions. These decisions are particularly crucial in rural areas, where transport decisions can have a large impact on treatment times – often involving a trade-off between delay in pharmaceutical therapy or a delay in endovascular thrombectomy. In this work, we explore a Bayesian modeling framework to address this decision-making process, showing how these techniques may be used to fully account for diagnostic and therapeutic uncertainty. We demonstrate how these techniques can contextualize triage decision at a fine-grained spatial scale. We further show the application of this modeling approach in the US State of Iowa, using data from the Virtual International Stroke Trials Archive (VISTA), and describe potential next steps for improved triage. ABBREVIATION LVO: large vessel occlusion; non-LVO, non-large vessel occlusion; IVT: intravenous tissue plasminogen activator; EVT: endovascular thrombectomy; CSC: comprehensive stroke centers; PSC: primary stroke centers; DS: drip and ship; MS, mothership; EMS: Emergency Medical Service; BGLM: Bayesian Generalized Linear Model; BGAM: Bayesian Generalized Additive Model; BART: Bayesian Additive Regression Trees; VISTA: Virtual International Stroke Trials Archive; NIHSS: National Institute of Health Stroke Severity Scale; ASPECTS: Alberta Stroke Programme Early CT Score; mRS, modified Rankin score; ROCAUC: Area under the receiver operating characteristic curve; ELPD: Expected Log pointwise Predictive Density; SE: Standard Error; ICA: Internal Carotid Artery; M1: Middle Cerebral Artery segment 1; M2: Middle Cerebral Artery segment 2; TIA: Transient Ischemic Attack; Cr-I: Credible Intervals; LKW: Last Known Well},
  archive      = {J_JOAS},
  author       = {Uche Nwoke and Mudassir Farooqui and Jacob Oleson and Nicholas Mohr and Santiago Ortega-Gutierrez and Grant D. Brown and on behalf of the VISTA collaborators},
  doi          = {10.1080/02664763.2024.2360590},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {135-157},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian modeling framework for optimizing pre-hospital stroke triage decisions},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal design of variables switch-based sampling scheme for verifying weibull distributed product lifetimes. <em>JOAS</em>, <em>52</em>(1), 119-134. (<a href='https://doi.org/10.1080/02664763.2024.2360589'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring products maintain their normal function during the warranty period is essential for companies to control warranty costs and sustain their brand reputation. A variables acceptance sampling (VAS) plan based on the lifetime performance index (LPI) is a practical technology for verifying product lifetimes because it can provide the number of failures required for life testing and the acceptance criterion for lot disposition. Recently, an LPI-based variables quick-switch sampling (VQSS) system has been developed to improve the sampling efficiency. The VQSS system manipulates the normal and tightened VAS plans for a series of lot dispositions. However, the existing LPI-based VQSS system only alters acceptance criteria to build normal and tightened VAS plans, which may limit its applicability. In this paper, we propose an LPI-based VQSS system with a mechanism that can alter the required number of failures. Compared to the existing LPI-based VQSS system, the proposed method has superior discriminatory power and moderate cost-effectiveness. Finally, we illustrate a lifetime verification of an electric vehicle’s battery as a case study to demonstrate the practicality of the proposed method.},
  archive      = {J_JOAS},
  author       = {Chien-Wei Wu and Ming-Hung Shu and To-Cheng Wang},
  doi          = {10.1080/02664763.2024.2360589},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {119-134},
  shortjournal = {J. Appl. Stat.},
  title        = {Optimal design of variables switch-based sampling scheme for verifying weibull distributed product lifetimes},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian fractional polynomial approach to quantile regression and variable selection with application in the analysis of blood pressure among US adults. <em>JOAS</em>, <em>52</em>(1), 97-118. (<a href='https://doi.org/10.1080/02664763.2024.2359526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the fractional polynomials (FPs) can act as a concise and accurate formula for examining smooth relationships between response and predictors, modelling conditional mean functions observes the partial view of a distribution of response variable, as distributions of many response variables such as blood pressure (BP) measures are typically skew. Conditional quantile functions with FPs provide a comprehensive relationship between the response variable and its predictors, such as median and extremely high-BP measures that may be often required in practical data analysis generally. To the best of our knowledge, this is new in the literature. Therefore, in this article, we develop and employ Bayesian variable selection with quantile-dependent prior for the FP model to propose a Bayesian variable selection with parametric non-linear quantile regression model. The objective is to examine a non-linear relationship between BP measures and their risk factors across median and upper quantile levels using data extracted from the 2007 to 2008 National Health and Nutrition Examination Survey (NHANES). The variable selection in the model analysis identified that the non-linear terms of continuous variables (body mass index, age), and categorical variables (ethnicity, gender, and marital status) were selected as important predictors in the model across all quantile levels.},
  archive      = {J_JOAS},
  author       = {Sanna Soomro and Keming Yu},
  doi          = {10.1080/02664763.2024.2359526},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {97-118},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian fractional polynomial approach to quantile regression and variable selection with application in the analysis of blood pressure among US adults},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monitoring the inverted exponentiated half logistic quantiles under the adaptive progressive type II hybrid censoring scheme. <em>JOAS</em>, <em>52</em>(1), 59-96. (<a href='https://doi.org/10.1080/02664763.2024.2358327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous studies have solved the problem of monitoring statistical processes with complete samples. However, censored or incomplete samples are commonly encountered due to constraints such as time and cost. Adaptive progressive Type II hybrid censoring is a novel method with the advantages of saving time and improving efficiency. On the basis of this scheme, the problem of monitoring a downward shift in the quantiles of the inverted exponentiated half logistic distribution is considered. The conventional Shewhart control chart is insensitive to detect quantile shifts from faulty data processes that exhibit deviations from normality or symmetry. To overcome this limitation, Bootstrap control charts combining with the exponentially weighted moving average method based on the Bayesian estimation and maximum likelihood estimation are proposed, respectively. They are compared with the conventional Shewhart control chart via average run length through Monte-Carlo simulations. Finally, a real dataset related to the tensile strength of carbon fibers is employed to demonstrate the ascendancy of the Bootstrap control charts.},
  archive      = {J_JOAS},
  author       = {Jiao Yu and Chunjie Wu and Ping Luo},
  doi          = {10.1080/02664763.2024.2358327},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {59-96},
  shortjournal = {J. Appl. Stat.},
  title        = {Monitoring the inverted exponentiated half logistic quantiles under the adaptive progressive type II hybrid censoring scheme},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Z-residual diagnostic tool for assessing covariate functional form in shared frailty models. <em>JOAS</em>, <em>52</em>(1), 28-58. (<a href='https://doi.org/10.1080/02664763.2024.2355551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis often involves modeling hazard functions while considering frailty to account for unobserved cluster-level factors in clustered survival data. Shared frailty models have gained popularity for this purpose, but assessing covariate functional form in these models presents unique challenges. Martingale and deviance residuals are commonly used for visually assessing covariate functional form against continuous covariates. Nevertheless, their subjective nature and lack of a reference distribution make it challenging to derive numerical statistical tests from these residuals. To address these limitations, we propose ‘Z-residuals’, a novel diagnostic tool designed for shared frailty models, leveraging the concept of randomized survival probability and introducing both graphical and numerical tests. To implement this approach, we develop an R package to compute Z-residuals for shared frailty models. Through extensive simulation studies, we demonstrate the high power of our derived numerical test for assessing the functional form of covariates. To validate the effectiveness of our method, we apply it to a real data application concerning the modelling of survival time for acute myeloid leukemia patients. Our Z-residual diagnosis results reveal the inadequacy of log-transformation of the covariate, highlighting the limitations of other diagnostic methods for effectively assessing covariate functional form in shared frailty models.},
  archive      = {J_JOAS},
  author       = {Tingxuan Wu and Longhai Li and Cindy Feng},
  doi          = {10.1080/02664763.2024.2355551},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {28-58},
  shortjournal = {J. Appl. Stat.},
  title        = {Z-residual diagnostic tool for assessing covariate functional form in shared frailty models},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-proportional hazards model with a PVF frailty term: Application with a melanoma dataset. <em>JOAS</em>, <em>52</em>(1), 1-27. (<a href='https://doi.org/10.1080/02664763.2024.2354443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival data analysis often uses the Cox proportional hazards (PH) model. This model is widely applied due to its straightforward interpretation of the hazard ratio under the assumption that the hazard rates for two subjects remain constant over time. However, in several randomized clinical trials with long-term survival data comparing two new treatments, it is frequently observed that Kaplan-Meier plots exhibit crossing survival curves. This violation of the PH assumption of the Cox PH model can not be applied to evaluate the treatment's effect on survival. This paper introduces a novel long-term survival model with non-PH that incorporates a frailty term into the hazard function. This model allows us to examine the effect of prognostic factors on survival and quantify the degree of unobservable heterogeneity. The model parameters are estimated using the maximum likelihood estimation procedure, and we evaluate the performance of the proposed models through simulation studies. Additionally, we demonstrate the applicability of our approach by fitting the models to a real skin cancer dataset.},
  archive      = {J_JOAS},
  author       = {Karen C. Rosa and Vinicius F. Calsavara and Francisco Louzada},
  doi          = {10.1080/02664763.2024.2354443},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {1-27},
  shortjournal = {J. Appl. Stat.},
  title        = {Non-proportional hazards model with a PVF frailty term: Application with a melanoma dataset},
  volume       = {52},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
