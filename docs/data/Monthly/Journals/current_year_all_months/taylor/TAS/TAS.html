<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tas">TAS - 44</h2>
<ul>
<li><details>
<summary>
(2025). Learn r: As a language, 2nd ed.. <em>TAS</em>, <em>79</em>(3), 417-419. (<a href='https://doi.org/10.1080/00031305.2025.2490305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Haihan Yu},
  doi          = {10.1080/00031305.2025.2490305},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {417-419},
  shortjournal = {Am. Stat.},
  title        = {Learn r: As a language, 2nd ed.},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric statistical methods using r, 2nd ed.. <em>TAS</em>, <em>79</em>(3), 416. (<a href='https://doi.org/10.1080/00031305.2025.2484865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Bojana Milošević},
  doi          = {10.1080/00031305.2025.2484865},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {416},
  shortjournal = {Am. Stat.},
  title        = {Nonparametric statistical methods using r, 2nd ed.},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data science in practice. <em>TAS</em>, <em>79</em>(3), 416-417. (<a href='https://doi.org/10.1080/00031305.2025.2490304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Xiao Hui Tai},
  doi          = {10.1080/00031305.2025.2490304},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {416-417},
  shortjournal = {Am. Stat.},
  title        = {Data science in practice},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connections between statistics and Mathematics/Probability. <em>TAS</em>, <em>79</em>(3), 410-415. (<a href='https://doi.org/10.1080/00031305.2025.2453230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many connections between probability, other mathematics courses, and statistics. Understanding these connections provides insights that might not be fully appreciated when considering each discipline in isolation. While the typical instruction of statistics courses relies on elucidating its foundational principles from mathematical and probability theory, it is generally less appreciated that statistics can in turn provide a deeper understanding of results in mathematics and probability. We offer several examples for which knowledge of statistics can shed new light on probability and other mathematics results. Examples span both undergraduate and graduate level material. In today’s data driven-world, many students are naturally curious about statistics and are exposed to this field early in their undergraduate curriculum. Leveraging connections between statistics and mathematics and probability makes theoretical concepts more intuitive and relevant, fostering a better understanding.},
  archive      = {J_TAS},
  author       = {Michael A. Proschan and Pamela A. Shaw},
  doi          = {10.1080/00031305.2025.2453230},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {410-415},
  shortjournal = {Am. Stat.},
  title        = {Connections between statistics and Mathematics/Probability},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analytics, have some humility: A statistical view of fourth-down decision making. <em>TAS</em>, <em>79</em>(3), 393-409. (<a href='https://doi.org/10.1080/00031305.2025.2475801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard mathematical approach to fourth-down decision-making in American football is to make the decision that maximizes estimated win probability. Win probability estimates arise from machine learning models fit from historical data. These models attempt to capture a nuanced relationship between a noisy binary outcome variable and game-state variables replete with interactions and non-linearities from a finite dataset of just a few thousand games. Thus, it is imperative to knit uncertainty quantification into the fourth-down decision procedure; we do so using bootstrapping. We find that uncertainty in the estimated optimal fourth-down decision is far greater than that currently expressed by sports analysts in popular sports media.},
  archive      = {J_TAS},
  author       = {Ryan S. Brill and Ronald Yurko and Abraham J. Wyner},
  doi          = {10.1080/00031305.2025.2475801},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {393-409},
  shortjournal = {Am. Stat.},
  title        = {Analytics, have some humility: A statistical view of fourth-down decision making},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An example to illustrate randomized trial estimands and estimators. <em>TAS</em>, <em>79</em>(3), 383-392. (<a href='https://doi.org/10.1080/00031305.2025.2468399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the International Conference on Harmonisation finalized an estimand framework for randomized trials that was adopted by regulatory bodies worldwide. The framework introduced five strategies for handling post-randomization events; namely the treatment policy, composite variable, while on treatment, hypothetical and principal stratum estimands. We describe an illustrative example to elucidate the difference between these five strategies for handling intercurrent events and provide an estimation technique for each. Specifically, we consider the intercurrent event of treatment discontinuation and introduce potential outcome notation to describe five estimands and corresponding estimators: (1) an intention-to-treat estimator of the total effect of a treatment policy; (2) an intention-to-treat estimator of a composite of the outcome and remaining on treatment; (3) a per-protocol estimator of the outcome in individuals observed to remain on treatment; (4) a g-computation estimator of a hypothetical scenario that all individuals remain on treatment; and (5) a principal stratum estimator of the treatment effect in individuals who would remain on treatment under the experimental condition. Additional insight is provided by defining situations where certain estimands are equal, and by studying the while on treatment strategy under repeated outcome measures. We highlight relevant causal inference literature to enable adoption in practice.},
  archive      = {J_TAS},
  author       = {Linda J. Harrison and Sean S. Brummel},
  doi          = {10.1080/00031305.2025.2468399},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {383-392},
  shortjournal = {Am. Stat.},
  title        = {An example to illustrate randomized trial estimands and estimators},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible distributed lag models for count data using mgcv. <em>TAS</em>, <em>79</em>(3), 371-382. (<a href='https://doi.org/10.1080/00031305.2025.2505514'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this tutorial we present the use of R package mgcv to implement Distributed Lag Non-Linear Models (DLNMs) in a flexible way. Interpretation of smoothing splines as random quantities enables approximate Bayesian inference, which in turn allows uncertainty quantification and comprehensive model checking. We illustrate various modeling situations using open-access epidemiological data in conjunction with simulation experiments. We demonstrate the inclusion of temporal structures and the use of mixture distributions to allow for extreme outliers. Moreover, we demonstrate interactions of the temporal lagged structures with other covariates with different lagged periods for different covariates. Spatial structures are also demonstrated, including smooth spatial variability and Markov random fields, in addition to hierarchical formulations to allow for non-structured dependency. Posterior predictive simulation is used to ensure models verify well against the data.},
  archive      = {J_TAS},
  author       = {Theo Economou and Daphne Parliari and Aurelio Tobias and Laura Dawkins and Hamish Steptoe and Christophe Sarran and Oliver Stoner and Rachel Lowe and Jos Lelieveld},
  doi          = {10.1080/00031305.2025.2505514},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {371-382},
  shortjournal = {Am. Stat.},
  title        = {Flexible distributed lag models for count data using mgcv},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing spatial point patterns in digital pathology: Immune cells in high-grade serous ovarian carcinomas. <em>TAS</em>, <em>79</em>(3), 355-370. (<a href='https://doi.org/10.1080/00031305.2025.2459280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplex immunofluorescence (mIF) imaging technology facilitates the study of the tumor microenvironment in cancer patients. Due to the capabilities of this emerging bioimaging technique, it is possible to statistically analyze, for example, the co-varying location and functions of multiple different types of immune cells. Complex spatial relationships between different immune cells have been shown to correlate with patient outcomes and may reveal new pathways for targeted immunotherapy treatments. This tutorial reviews methods and procedures relating to spatial point patterns for complex data analysis. We consider tissue cells as a realization of a spatial point process for each patient. We focus on proper functional descriptors for each observation and techniques that allow us to obtain information about inter-patient variation. Ovarian cancer is the deadliest gynaecological malignancy and can resist chemotherapy treatment effective in cancers. We use a dataset of high-grade serous ovarian cancer samples from 51 patients. We examine the immune cell composition (T cells, B cells, macrophages) within tumors and additional information such as cell classification (tumor or stroma) and other patient clinical characteristics. Our analyses, supported by reproducible software, apply to other digital pathology datasets.},
  archive      = {J_TAS},
  author       = {Jonatan A. González and Julia Wrobel and Simon Vandekar and Paula Moraga},
  doi          = {10.1080/00031305.2025.2459280},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {355-370},
  shortjournal = {Am. Stat.},
  title        = {Analyzing spatial point patterns in digital pathology: Immune cells in high-grade serous ovarian carcinomas},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Play-by-play volleyball win probability model. <em>TAS</em>, <em>79</em>(3), 345-354. (<a href='https://doi.org/10.1080/00031305.2025.2490786'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a volleyball point-by-point win probability model that updates the probability of winning a set after each play in the set. The covariate informed product partition model (PPMx) is well suited to flexibly include in-set team performance information when making predictions. However, making predictions in real time would be too expensive computationally as it would require refitting the PPMx after each play. Instead, we develop a predictive procedure based on a single training of the PPMx that predicts in real-time. We deploy this procedure using data from the 2018 Men’s World Volleyball Championship. The procedure first trains a PPMx model using end-of-set team performance statistics from the round robin stage of the tournament. Then based on the PPMx predictive distribution, we predict the win probability after every play of every match in the knockout stages. Finally, we show how the prediction procedure can be enhanced by including pre-set information toward the beginning of the set and set score toward the end.},
  archive      = {J_TAS},
  author       = {Nathan Hawkins and Gilbert W. Fellingham and Garritt L. Page},
  doi          = {10.1080/00031305.2025.2490786},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {345-354},
  shortjournal = {Am. Stat.},
  title        = {Play-by-play volleyball win probability model},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Closed-form power and sample size calculations for bayes factors. <em>TAS</em>, <em>79</em>(3), 330-344. (<a href='https://doi.org/10.1080/00031305.2025.2467919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining an appropriate sample size is a critical element of study design, and the method used to determine it should be consistent with the planned analysis. When the planned analysis involves Bayes factor hypothesis testing, the sample size is usually desired to ensure a sufficiently high probability of obtaining a Bayes factor indicating compelling evidence for a hypothesis, given that the hypothesis is true. In practice, Bayes factor sample size determination is typically performed using computationally intensive Monte Carlo simulation. Here, we summarize alternative approaches that enable sample size determination without simulation. We show how, under approximate normality assumptions, sample sizes can be determined numerically, and provide the R package bfpwr for this purpose. Additionally, we identify conditions under which sample sizes can even be determined in closed-form, resulting in novel, easy-to-use formulas that also help foster intuition, enable asymptotic analysis, and can also be used for hybrid Bayesian/likelihoodist design. Furthermore, we show how power and sample size can be computed without simulation for more complex analysis priors, such as Jeffreys-Zellner-Siow priors or non-local normal moment priors. Case studies from medicine and psychology illustrate how researchers can use our methods to design informative yet cost-efficient studies.},
  archive      = {J_TAS},
  author       = {Samuel Pawel and Leonhard Held},
  doi          = {10.1080/00031305.2025.2467919},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {330-344},
  shortjournal = {Am. Stat.},
  title        = {Closed-form power and sample size calculations for bayes factors},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A class of regression association measures based on concordance. <em>TAS</em>, <em>79</em>(3), 320-329. (<a href='https://doi.org/10.1080/00031305.2024.2448431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measures of regression association aiming at predictability of a dependent variable Y from an independent variable X have received considerable attention recently. In this article, we provide a unified discussion of some existing measures, including their rationale, properties, and estimation. Motivated by these measures, we consider a general class of regression association measures which views the regression association of Y from X as the association of two independent replications from the conditional distribution of Y given X . We illustrate that the so-called Markov product copulas can be employed as a neat and convenient building block for this class of measures, and the measures so constructed can be expressed as a common form of the proportion of the variance of some function of Y that can be explained by X , rendering the measures a direct interpretation in terms of predictability. Also, the notion of two independent replications from the conditional distribution leads to a simple nonparametric estimation method based on the induced order statistics, hence, no smoothing techniques are required. Under the considered general framework, the performances and utilities of the regression association measures are examined through simulations and real data applications.},
  archive      = {J_TAS},
  author       = {Jia-Han Shih and Yi-Hau Chen},
  doi          = {10.1080/00031305.2024.2448431},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {320-329},
  shortjournal = {Am. Stat.},
  title        = {A class of regression association measures based on concordance},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Laplace’s law of succession estimator and M-statistics. <em>TAS</em>, <em>79</em>(3), 311-319. (<a href='https://doi.org/10.1080/00031305.2024.2448430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classic formula for estimating the binomial probability as the proportion of successes contradicts common sense for extreme probabilities when the event never occurs or occurs every time. Laplace’s law of succession estimator, one of the first applications of Bayesian statistics, has been around for over 250 years and resolves the paradoxes, although rarely discussed in modern statistics texts. This work aims to introduce a new theory for exact optimal statistical inference using Laplace’s law of succession estimator as a motivating example. We prove that this estimator may be viewed from a different theoretical perspective as the limit point of the short confidence interval on the double-log scale when the confidence level approaches zero. This motivating example paves the road to the definition of an estimator as the inflection point on the cumulative distribution function as a function of the parameter given the observed statistic. This estimator has the maximum infinitesimal probability of the coverage of the unknown parameter and, therefore, is called the maximum concentration (MC) estimator as a part of a more general M-statistics theory. The new theory is illustrated with exact optimal confidence intervals for the normal standard deviation and the respective MC estimators.},
  archive      = {J_TAS},
  author       = {Eugene Demidenko},
  doi          = {10.1080/00031305.2024.2448430},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {311-319},
  shortjournal = {Am. Stat.},
  title        = {Laplace’s law of succession estimator and M-statistics},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient computation strategy for generalized single-index models and their variants by integrating with GAM. <em>TAS</em>, <em>79</em>(3), 302-310. (<a href='https://doi.org/10.1080/00031305.2025.2464854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various generalizations of single-index models and associated estimation methods have been developed. However, implementing these developed methods requires much effort to program, case by case, due to the lack of a common and flexible vehicle to cover them. We suggest an efficient computation strategy for easily estimating parameters and nonparametric functions in generalized single-index models and generalized partially linear single-index models by integrating with well-developed algorithms and packages for estimating the generalized additive models (Wood; Hastie and Tibshirani, GAM). Such an integration makes estimation in these index-type models much easier, expedient, and flexible and brings a lot of convenience. We briefly introduce the principle and extensively examine numerical performance for various scenarios. Numerical experiments indicate that the proposed strategy works well with finite sample sizes and is especially flexible to model structures. Finally, we analyze two real-data examples as an illustration.},
  archive      = {J_TAS},
  author       = {Ximin Li and Haozhe Liang and Hua Liang},
  doi          = {10.1080/00031305.2025.2464854},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {302-310},
  shortjournal = {Am. Stat.},
  title        = {An efficient computation strategy for generalized single-index models and their variants by integrating with GAM},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiple imputation approach for the cumulative incidence, with implications for variance estimation. <em>TAS</em>, <em>79</em>(3), 291-301. (<a href='https://doi.org/10.1080/00031305.2025.2453674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an alternative approach to estimating the cumulative incidence function that uses nonparametric multiple imputation to reduce the problem to that of estimating a binomial proportion. In the standard competing risks setting, we show mathematically and empirically that our imputation-based estimator is equivalent to the Aalen-Johansen estimator of the cumulative incidence given a sufficient number of imputations. However, our approach allows for the use of a wider variety of methods for the analysis of binary outcomes, including preferred options for uncertainty estimation. While we focus on the cumulative incidence function, the multiple imputation approach likely extends to more complex problems in competing risks.},
  archive      = {J_TAS},
  author       = {Elizabeth C. Chase and Philip S. Boonstra and Jeremy M. G. Taylor},
  doi          = {10.1080/00031305.2025.2453674},
  journal      = {The American Statistician},
  month        = {7},
  number       = {3},
  pages        = {291-301},
  shortjournal = {Am. Stat.},
  title        = {A multiple imputation approach for the cumulative incidence, with implications for variance estimation},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applied machine learning using mlr3 in r. <em>TAS</em>, <em>79</em>(2), 288-289. (<a href='https://doi.org/10.1080/00031305.2025.2469923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Xueying Tang},
  doi          = {10.1080/00031305.2025.2469923},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {288-289},
  shortjournal = {Am. Stat.},
  title        = {Applied machine learning using mlr3 in r},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modern data visualization with r. <em>TAS</em>, <em>79</em>(2), 287-288. (<a href='https://doi.org/10.1080/00031305.2025.2459438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {John M. Hoenig},
  doi          = {10.1080/00031305.2025.2459438},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {287-288},
  shortjournal = {Am. Stat.},
  title        = {Modern data visualization with r},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foundations of data science with python. <em>TAS</em>, <em>79</em>(2), 286. (<a href='https://doi.org/10.1080/00031305.2025.2458850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Qing Wang},
  doi          = {10.1080/00031305.2025.2458850},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {286},
  shortjournal = {Am. Stat.},
  title        = {Foundations of data science with python},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counternull sets in randomized experiments. <em>TAS</em>, <em>79</em>(2), 275-285. (<a href='https://doi.org/10.1080/00031305.2024.2432884'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a study whose primary results are “not statistically significant”. How often does it lead to the following published conclusion that “there is no effect of the treatment/exposure on the outcome”? We believe too often and that the requirement to report counternull values could help to avoid this! In statistical parlance, the null value of an estimand is a value that is distinguished in some way from other possible values, for example a value that indicates no difference between the general health status of those treated with a new drug versus a traditional drug. A counternull value is a nonnull value of that estimand that is supported by the same amount of evidence that supports the null value. Of course, such a definition depends critically on how “evidence” is defined. Here, we consider the context of a randomized experiment where evidence is summarized by the randomization-based p -value associated with a specified sharp null hypothesis. Consequently, a counternull value has the same p -value from the randomization test as does the null value; the counternull value is rarely unique, but rather comprises a set of values. We explore advantages to reporting a counternull set in addition to the p -value associated with a null value; a first advantage is pedagogical, in that reporting it avoids the mistake of implicitly accepting a not-rejected null hypothesis; a second advantage is that the effort to construct a counternull set can be scientifically helpful by encouraging thought about nonnull values of estimands. Two examples are used to illustrate these ideas.},
  archive      = {J_TAS},
  author       = {M.-A. C. Bind and D. B. Rubin},
  doi          = {10.1080/00031305.2024.2432884},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {275-285},
  shortjournal = {Am. Stat.},
  title        = {Counternull sets in randomized experiments},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of contact time among animals from telemetry data. <em>TAS</em>, <em>79</em>(2), 265-274. (<a href='https://doi.org/10.1080/00031305.2024.2402264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous processes in most applications are measured discretely with error. This complicates the task of detecting intersections and the number of intersections between two continuous processes (i.e., when the processes have the same value). Intersections of continuous processes are scientifically important, but challenging to estimate from data. For example, in the field of animal ecology, intersections of the paths of moving animals tracked with satellite technologies can be used to understand disease transmission. We illustrate how to quantify contact between animals using telemetry data (i.e., the recorded locations of an animal over time). We introduce our method to quantify contact time with accessible concepts from introductory stochastic process literature, such as Brownian motion. Then, we provide two data examples using white-tailed deer ( Odocoileus virginianus ) and mule deer ( Odocoileus hemionus ) telemetry data in a region with high prevalence of chronic wasting disease. Our work provides a needed connection between existing model-based literature for animal movement and rule-based literature for animal interaction. Further, our work illustrates a unique statistical problem receiving minimal attention with broad applicability in human and livestock tracking. Supplementary materials for this article are available online.},
  archive      = {J_TAS},
  author       = {Andrew B. Whetten and Trevor J. Hefley and David A. Haukos},
  doi          = {10.1080/00031305.2024.2402264},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {265-274},
  shortjournal = {Am. Stat.},
  title        = {Estimation of contact time among animals from telemetry data},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance analysis of NSUM estimators in social-network topologies. <em>TAS</em>, <em>79</em>(2), 247-264. (<a href='https://doi.org/10.1080/00031305.2024.2421361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Network Scale-up Methods (NSUM) are methods to estimate unknown populations based on indirect surveys in which the participants provide information about aggregated data of their acquaintances. This preserves the privacy and may lead to higher participation. During the last thirty years, new NSUM estimators have emerged. However, conditions related to the design of the experiments and the robustness of the estimators have not been studied in depth, especially in a realistic simulation environment. This study aims to compare nine NSUM estimators under relevant conditions in the literature through simulation experiments. We have analyzed how the NSUM is affected by the network topology, transmission and recall errors, the distribution of the unknown subpopulation, the number and sizes of subpopulations, and sample size. This article shows that some NSUM estimators barely used are better and more robust to some conditions, especially when the network is scale-free or under barrier effects. In addition, some methods are very sensitive to recall errors. In terms of the subpopulations configuration, we observe that the number of known subpopulations usually employed is quite large and that the most common NSUM is robust to the number and sizes of the subpopulations.},
  archive      = {J_TAS},
  author       = {Sergio Díaz-Aranda and Jose Aguilar and Juan Marcos Ramírez and David Rabanedo and Antonio Fernández Anta and Rosa E. Lillo},
  doi          = {10.1080/00031305.2024.2421361},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {247-264},
  shortjournal = {Am. Stat.},
  title        = {Performance analysis of NSUM estimators in social-network topologies},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective and small sample-size valid confidence interval for isotonic Dose–Response curves by inverting a partial likelihood ratio test. <em>TAS</em>, <em>79</em>(2), 236-246. (<a href='https://doi.org/10.1080/00031305.2024.2407478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dose–response curve is essential for determining the safe dosage of a drug and is widely used in bioassay and in phase 1 clinical trials. It is generally accepted that the probability of death or the probability of dose-limiting toxicity is a nondecreasing function of the dose. This article proposes and develops an effective point-wise confidence interval for an isotonic dose–response curve, a problem without a satisfactory solution so far. We show that one subset of the observations informs the lower limit while the other subset informs the upper limit. A partial likelihood ratio test using these subsets of observations is fully investigated. The resulting confidence interval is compared to three existing methods (Morris, Meyer, and Oron) in an extensive simulation study. The new method produces tight intervals while maintaining coverage. It is therefore preferred when maintaining an actual coverage probability is crucial as often in a regulatory environment. The new method extends to one-way ANOVA of a continuous response variable in a straightforward way. An R function is provided.},
  archive      = {J_TAS},
  author       = {J. G. Liao},
  doi          = {10.1080/00031305.2024.2407478},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {236-246},
  shortjournal = {Am. Stat.},
  title        = {An effective and small sample-size valid confidence interval for isotonic Dose–Response curves by inverting a partial likelihood ratio test},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When heavy tails disrupt statistical inference. <em>TAS</em>, <em>79</em>(2), 221-235. (<a href='https://doi.org/10.1080/00031305.2024.2402898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heavy tails (HT) arise in many applications and their presence can disrupt statistical inference, yet the HT statistical literature requires a theoretical background most practicing statisticians lack. We provide an overview of the influence of HT on the performance of basic statistical methods and useful theorems aimed at the practitioner encountering HT in an applied setting. Higher or even lower product moments (i.e., variance, skewness, etc.) can be infinite for some HT populations, yet all L-moments are always finite, given that the mean exists, thus, the theory of L-moments is uniquely suited to all HT distributions and data. We document how L-kurtosis, (a kurtosis measure based on the fourth L-moment) provides a general and practical heaviness index for contrasting tail heaviness across distributions and datasets and how a single L-moment diagram can document both the prevalence and impact of HT distributions and data across disciplines and datasets. Surprisingly, the theory of L-moments, an extension and evolution of probability weighted moments, has been largely overlooked by the literature on HT distributions that exhibit infinite moments. Experiments reveal L-kurtosis ranges under which various HT distributions result in mild to severe disruption to the bootstrap, the central limit theorem (CLT), and the law of large numbers, even for distributions which exhibit finite product moments.},
  archive      = {J_TAS},
  author       = {Richard M. Vogel and Simon Michael Papalexiou and Jonathan R. Lamontagne and Flannery C. Dolan},
  doi          = {10.1080/00031305.2024.2402898},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {221-235},
  shortjournal = {Am. Stat.},
  title        = {When heavy tails disrupt statistical inference},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selecting the best compositions of a wheelchair basketball team: A data-driven approach. <em>TAS</em>, <em>79</em>(2), 212-220. (<a href='https://doi.org/10.1080/00031305.2024.2402246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wheelchair basketball, regulated by the International Wheelchair Basketball Federation, is a sport designed for individuals with physical disabilities. This article presents a data-driven tool that effectively determines optimal team lineups based on past performance data and metrics for player effectiveness. Our proposed methodology involves combining a Bayesian longitudinal model with an integer linear problem to optimize the lineup of a wheelchair basketball team. To illustrate our approach, we use real data from a team competing in the Rollstuhlbasketball Bundesliga, namely the Doneck Dolphins Trier. We consider three distinct performance metrics for each player and incorporate uncertainty from the posterior predictive distribution of the longitudinal model into the optimization process. The results demonstrate the tool’s ability to select the most suitable team compositions and to calculate posterior probabilities of several players playing together in the best composition.},
  archive      = {J_TAS},
  author       = {Gabriel Calvo and Carmen Armero and Bernd Grimm and Christophe Ley},
  doi          = {10.1080/00031305.2024.2402246},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {212-220},
  shortjournal = {Am. Stat.},
  title        = {Selecting the best compositions of a wheelchair basketball team: A data-driven approach},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-validatory Z-residual for diagnosing shared frailty models. <em>TAS</em>, <em>79</em>(2), 198-211. (<a href='https://doi.org/10.1080/00031305.2024.2421370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate model performance assessment in survival analysis is imperative for robust predictions and informed decision-making. Traditional residual diagnostic tools like martingale and deviance residuals lack a well-characterized reference distribution for censored regression, making numerical statistical tests based on these residuals challenging. Recently, the introduction of Z-residuals for diagnosing survival models addresses this limitation. However, concerns arise from conventional methods that use the entire dataset for both model parameter estimation and residual assessment, which may cause optimistic biases. This article introduces cross-validatory Z-residuals as an innovative approach to address these limitations. Employing a cross-validation (CV) framework, the method systematically partitions the dataset into training and testing sets to reduce the optimistic bias. Our simulation studies demonstrate that, for goodness-of-fit tests and outlier detection, cross-validatory Z-residuals are significantly more powerful (e.g., power increased from 0.2 to 0.6). and more discriminative (e.g., AUC increased from 0.58 to 0.85) than Z-residuals without CV. We also compare the performance of Z-residuals with and without CV in identifying outliers in a real application that models the recurrence time of kidney infection patients. Our findings suggest that cross-validatory Z-residuals can identify outliers, which Z-residuals without CV fail to identify. The CV Z-residual is a more powerful tool than the No-CV Z-residual for checking survival models, particularly in goodness-of-fit tests and outlier detection. We have published a generic function, which is collected in an R package called Zresidual , for computing CV Z-residual for the output of the widely used survival R package.},
  archive      = {J_TAS},
  author       = {Tingxuan Wu and Cindy Feng and Longhai Li},
  doi          = {10.1080/00031305.2024.2421370},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {198-211},
  shortjournal = {Am. Stat.},
  title        = {Cross-validatory Z-residual for diagnosing shared frailty models},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse-group boosting: Unbiased group and variable selection. <em>TAS</em>, <em>79</em>(2), 184-197. (<a href='https://doi.org/10.1080/00031305.2024.2408007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For grouped covariates, we propose a framework for boosting that allows for sparsity within and between groups. By using component-wise and group-wise gradient ridge boosting simultaneously with adjusted degrees of freedom or penalty parameters, a model with similar properties as the sparse-group lasso can be fitted through boosting. We show that within-group and between-group sparsity can be controlled by a mixing parameter, and discuss similarities and differences to the mixing parameter in the sparse-group lasso. Furthermore, we show under which conditions variable selection on a group or individual variable basis happens and provide selection bounds for the regularization parameters depending solely on the singular values of the design matrix in a boosting iteration of linear Ridge penalized boosting. In special cases, we characterize the selection chance of an individual variable versus a group of variables through a generalized beta prime distribution. With simulations as well as two real datasets from ecological and organizational research data, we show the effectiveness and predictive competitiveness of this novel estimator. The results suggest that in the presence of grouped variables, sparse-group boosting is associated with less biased variable selection and higher predictability compared to component-wise or group-component-wise boosting.},
  archive      = {J_TAS},
  author       = {Fabian Obster and Christian Heumann},
  doi          = {10.1080/00031305.2024.2408007},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {184-197},
  shortjournal = {Am. Stat.},
  title        = {Sparse-group boosting: Unbiased group and variable selection},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal inference with complex surveys: A unified perspective on sample selection and exposure selection. <em>TAS</em>, <em>79</em>(2), 173-183. (<a href='https://doi.org/10.1080/00031305.2024.2423814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probability surveys are a major source of population representative data for policy research and program evaluation. However, the data come with the added complications of being observational and selected with unequal probabilities. Propensity score adjustments have become increasingly popular for inferring causal relationships in non-randomized studies, but when using survey data, estimates of the population level causal effect may be biased if the sampling design is not adequately adjusted for. The current practice of using propensity score estimators with complex surveys is somewhat ad-hoc. We propose a potential-outcome super-population framework to streamline the causal analysis. We also develop propensity-score-and-survey weighted estimators and corresponding variance estimators, as well as their asymptotic properties. Our framework clarifies the confusion regarding the use of survey weighted propensity score in practice. The choice actually depends on the available sampling weights. Various estimators are compared in a simulation study, which shows that the proposed estimators perform better than the competing methods in terms of bias and confidence interval coverage when treatment effects are heterogeneous. To address an important public health issue, we evaluate the impact of e-cigarette use on future tobacco use intention in teens, using a large nationally representative survey in the United States.},
  archive      = {J_TAS},
  author       = {Giovanni Nattino and Robert Ashmead and Bo Lu},
  doi          = {10.1080/00031305.2024.2423814},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {173-183},
  shortjournal = {Am. Stat.},
  title        = {Causal inference with complex surveys: A unified perspective on sample selection and exposure selection},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of a generalized treatment effect in a control group versus treatment group design. <em>TAS</em>, <em>79</em>(2), 167-172. (<a href='https://doi.org/10.1080/00031305.2024.2422933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A control group versus treatment group design is considered where the responses in the treatment group are modeled as a two-component mixture model that accounts for the possibility that only a fraction of the patients in the treated group will respond to the treatment. In this setting, the treatment effect is generalized to include both the fraction of treated patients that respond to the treatment and the magnitude of the response. Two alternative correlated and biased estimators are combined to yield an estimator that is preferable to either one of the estimators individually. The combined estimator is demonstrated on an illustrative blood pressure dataset.},
  archive      = {J_TAS},
  author       = {Daniel R. Jeske},
  doi          = {10.1080/00031305.2024.2422933},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {167-172},
  shortjournal = {Am. Stat.},
  title        = {Estimation of a generalized treatment effect in a control group versus treatment group design},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pareto tail plot without moment restrictions. <em>TAS</em>, <em>79</em>(2), 156-166. (<a href='https://doi.org/10.1080/00031305.2024.2413081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a mean functional that exists for arbitrary probability distributions and characterizes the Pareto distribution within the set of distributions with finite left endpoint. This is in sharp contrast to the mean excess plot, which is meaningless for distributions without an existing mean and has nonstandard behavior when the mean is finite, but the second moment does not exist. The construction of the plot is based on the principle of a single huge jump , which differentiates between distributions with moderately heavy and super heavy tails. We present an estimator of the tail function based on U -statistics and study its large sample properties. Several loss datasets illustrate the use of the new plot.},
  archive      = {J_TAS},
  author       = {Bernhard Klar},
  doi          = {10.1080/00031305.2024.2413081},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {156-166},
  shortjournal = {Am. Stat.},
  title        = {A pareto tail plot without moment restrictions},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Additive hazards regression analysis of massive interval-censored data via data splitting. <em>TAS</em>, <em>79</em>(2), 145-155. (<a href='https://doi.org/10.1080/00031305.2024.2407495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of data acquisition and storage space, massive datasets exhibited with large sample size emerge increasingly and make more advanced statistical tools urgently need. To accommodate such big volume in the analysis, a variety of methods have been proposed in the circumstances of complete or right censored survival data. However, existing development of big data methodology has not attended to interval-censored outcomes, which are ubiquitous in cross-sectional or periodical follow-up studies. In this work, we propose an easily implemented divide-and-combine approach for analyzing massive interval-censored survival data under the additive hazards model. We establish the asymptotic properties of the proposed estimator, including the consistency and asymptotic normality. In addition, the divide-and-combine estimator is shown to be asymptotically equivalent to the full-data-based estimator obtained from analyzing all data together. Simulation studies suggest that, relative to the full-data-based approach, the proposed divide-and-combine approach has desirable advantage in terms of computation time, making it more applicable to large-scale data analysis. An application to a set of interval-censored data also demonstrates the practical utility of the proposed method.},
  archive      = {J_TAS},
  author       = {Peiyao Huang and Shuwei Li and Xinyuan Song},
  doi          = {10.1080/00031305.2024.2407495},
  journal      = {The American Statistician},
  month        = {4},
  number       = {2},
  pages        = {145-155},
  shortjournal = {Am. Stat.},
  title        = {Additive hazards regression analysis of massive interval-censored data via data splitting},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A course in the large sample theory of statistical inference,. <em>TAS</em>, <em>79</em>(1), 142-143. (<a href='https://doi.org/10.1080/00031305.2024.2425459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Indranil Sahoo},
  doi          = {10.1080/00031305.2024.2425459},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {142-143},
  shortjournal = {Am. Stat.},
  title        = {A course in the large sample theory of statistical inference,},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial sampling with r. <em>TAS</em>, <em>79</em>(1), 141-142. (<a href='https://doi.org/10.1080/00031305.2024.2406579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Francesco Pantalone and Roberto Benedetti},
  doi          = {10.1080/00031305.2024.2406579},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {141-142},
  shortjournal = {Am. Stat.},
  title        = {Spatial sampling with r},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building regression models with SAS®: A guide for data scientists. <em>TAS</em>, <em>79</em>(1), 140-141. (<a href='https://doi.org/10.1080/00031305.2024.2406047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAS},
  author       = {Juan Sosa},
  doi          = {10.1080/00031305.2024.2406047},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {140-141},
  shortjournal = {Am. Stat.},
  title        = {Building regression models with SAS®: A guide for data scientists},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of design of experiments courses offered to undergraduate students at american universities. <em>TAS</em>, <em>79</em>(1), 129-139. (<a href='https://doi.org/10.1080/00031305.2024.2368803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design of Experiments (DoE) is a relevant class to undergraduate students in the sciences, because it teaches them how to plan, conduct, and analyze experiments. In the literature on DoE, there are several contributions to its pedagogy, such as easy-to-use class experiments, virtual experiments, and software to construct experimental designs. However, there are virtually no systematic evaluations of the actual DoE pedagogy. To address this issue, we build the first database of DoE courses offered to undergraduate students in the United States. The database has records on courses offered from 2019 to 2022 at the best universities in the US News Best National Universities ranking of 2022. Specifically, it has data on 18 general and content-specific features of 206 courses. To study the DoE pedagogy, we analyze the database using descriptive statistics and text mining. Based on our analysis, we provide instructors with recommendations and teaching material to enhance their DoE courses. The database and material are included in the supplement of this article.},
  archive      = {J_TAS},
  author       = {Alan R. Vazquez and Xiaocong Xuan},
  doi          = {10.1080/00031305.2024.2368803},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {129-139},
  shortjournal = {Am. Stat.},
  title        = {A review of design of experiments courses offered to undergraduate students at american universities},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distance covariance, independence, and pairwise differences. <em>TAS</em>, <em>79</em>(1), 122-128. (<a href='https://doi.org/10.1080/00031305.2024.2374966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance covariance (Székely, Rizzo, and Bakirov) is a fascinating recent notion, which is popular as a test for dependence of any type between random variables X and Y . This approach deserves to be touched upon in modern courses on mathematical statistics. It makes use of distances of the type | X − X ′ | and | Y − Y ′ | , where ( X ′ , Y ′ ) is an independent copy of ( X , Y ). This raises natural questions about independence of variables like X − X ′ and Y − Y ′ , about the connection between cov ( | X − X ′ | , | Y − Y ′ | ) and the covariance between doubly centered distances, and about necessary and sufficient conditions for independence. We show some basic results and present a new and nontechnical counterexample to a common fallacy, which provides more insight. We also show some motivating examples involving bivariate distributions and contingency tables, which can be used as didactic material for introducing distance correlation.},
  archive      = {J_TAS},
  author       = {Jakob Raymaekers and Peter J. Rousseeuw},
  doi          = {10.1080/00031305.2024.2374966},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {122-128},
  shortjournal = {Am. Stat.},
  title        = {Distance covariance, independence, and pairwise differences},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment and continuous improvement of an undergraduate data science program. <em>TAS</em>, <em>79</em>(1), 102-121. (<a href='https://doi.org/10.1080/00031305.2024.2365673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been an explosion in the growth of undergraduate statistics and data science programs across the US. Simultaneously, there has been clear guidance written on curriculum development for both data science (De Veaux et al.) and statistics (Carver et al.) programs. While this was occurring, ABET (now simply an acronym, but previously standing for the Accreditation Board for Engineering and Technology), in coordination with organizations such as the American Statistical Association, developed accreditation criteria for Data Science programs. In this article, we discuss our journey through ABET accreditation and discuss how adopting ABET processes for continuous improvement strengthens a program’s assessment process. We share best practices for working across multiple departments to collect data not only on individual courses, but also on the program as a whole. While the framework presented was initially established to support ABET accreditation, we argue that a properly executed program assessment should occur regardless of whether or not an institution is seeking ABET accreditation for their data science program. Throughout this article, we also discuss the extent to which ABET requirements naturally fit within our program’s existing goals, including an assessment of how ABET requirements align with major ideas in the field of data science education.},
  archive      = {J_TAS},
  author       = {Nicholas Clark and Christopher Morrell and Mike Powell},
  doi          = {10.1080/00031305.2024.2365673},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {102-121},
  shortjournal = {Am. Stat.},
  title        = {Assessment and continuous improvement of an undergraduate data science program},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-method data science pipeline for analyzing police service. <em>TAS</em>, <em>79</em>(1), 91-101. (<a href='https://doi.org/10.1080/00031305.2024.2374275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the fact that most police departments in the U.S. serve jurisdictions with fewer than 10,000 residents, policing practices in small towns are understudied. This is due in part to data limitations and technological barriers that exist in the small-town context. In this article we focus on one small town police department in New England with a history of misconduct, and develop a comprehensive data science pipeline that addresses the stages from design and collection to reporting. We present the reader with specific tools in the open-source Python ecosystem for replicating this pipeline. Once these data are processed, we perform two statistical analyses in an attempt to better understand the provisions of service by the small-town police department of focus. First, we perform ecological inference to estimate the rate at which residents are placing calls for service. Second, we model wait times using a negative binomal regression model to account for overdispersion in the data. We discuss data and model limitations arising through the pipeline creation and analysis process.},
  archive      = {J_TAS},
  author       = {Anna Haensch and Daanika Gordon and Karin Knudson and Justina Cheng},
  doi          = {10.1080/00031305.2024.2374275},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {91-101},
  shortjournal = {Am. Stat.},
  title        = {A multi-method data science pipeline for analyzing police service},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional propensity score and its machine learning extensions in residual confounding control. <em>TAS</em>, <em>79</em>(1), 72-90. (<a href='https://doi.org/10.1080/00031305.2024.2368794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“The use of health care claims datasets often encounters criticism due to the pervasive issues of omitted variables and inaccuracies or mis-measurements in available confounders. Ultimately, the treatment effects estimated using such data sources may be subject to residual confounding. Digital electronic administrative records routinely collect a large volume of health-related information; and many of which are usually not considered in conventional pharmacoepidemiological studies. A high-dimensional propensity score (hdPS) algorithm was proposed that uses such information as surrogates or proxies for mismeasured and unobserved confounders in an effort to reduce residual confounding bias. Since then, many machine learning and semi-parametric extensions of this algorithm have been proposed to better exploit the wealth of high-dimensional proxy information. In this tutorial, we will (i) demonstrate logic, steps and implementation guidelines of hdPS using an open data source as an example (using reproducible R codes), (ii) familiarize readers with the key difference between propensity score versus hdPS, as well as the requisite sensitivity analyses, (iii) explain the rationale for using the machine learning and double robust extensions of hdPS, and (iv) discuss advantages, controversies, and hdPS reporting guidelines while writing amanuscript.},
  archive      = {J_TAS},
  author       = {Mohammad Ehsanul Karim},
  doi          = {10.1080/00031305.2024.2368794},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {72-90},
  shortjournal = {Am. Stat.},
  title        = {High-dimensional propensity score and its machine learning extensions in residual confounding control},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrative data analysis where partial covariates have complex nonlinear effects by using summary information from an external data. <em>TAS</em>, <em>79</em>(1), 61-71. (<a href='https://doi.org/10.1080/00031305.2024.2368799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A full parametric and linear specification may be insufficient to capture complicated patterns in studies exploring complex features, such as those investigating age-related changes in brain functional abilities. Alternatively, a partially linear model (PLM) consisting of both parametric and nonparametric elements may have a better fit. This model has been widely applied in economics, environmental science, and biomedical studies. In this article, we introduce a novel statistical inference framework that equips PLM with high estimation efficiency by effectively synthesizing summary information from external data into the main analysis. Such an integrative scheme is versatile in assimilating various types of reduced models from the external study. The proposed method is shown to be theoretically valid and numerically convenient, and it ensures a high-efficiency gain compared to classic methods in PLM. Our method is further validated using two data applications by evaluating the risk factors of brain imaging measures and blood pressure.},
  archive      = {J_TAS},
  author       = {Jia Liang and Shuo Chen and Peter Kochunov and L. Elliot Hong and Chixiang Chen},
  doi          = {10.1080/00031305.2024.2368799},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {61-71},
  shortjournal = {Am. Stat.},
  title        = {Integrative data analysis where partial covariates have complex nonlinear effects by using summary information from an external data},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential monitoring using the second generation P-value with type i error controlled by monitoring frequency. <em>TAS</em>, <em>79</em>(1), 50-60. (<a href='https://doi.org/10.1080/00031305.2024.2356109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Second Generation P-Value (SGPV) measures the overlap between an estimated interval and a composite hypothesis of parameter values. We develop a sequential monitoring scheme of the SGPV (SeqSGPV) to connect study design intentions with end-of-study inference anchored on scientific relevance. We build upon Freedman’s “Region of Equivalence” (ROE) in specifying scientifically meaningful hypotheses called Pre-specified Regions Indicating Scientific Merit (PRISM). We compare PRISM monitoring versus monitoring alternative ROE specifications. Error rates are controlled through the PRISM’s indifference zone around the point null and monitoring frequency strategies. Because the former is fixed due to scientific relevance, the latter is a targettable means for designing studies with desirable operating characters. An affirmation step to stopping rules improves frequency properties including the error rate, the risk of reversing conclusions under delayed outcomes, and bias.},
  archive      = {J_TAS},
  author       = {Jonathan J. Chipman and Robert A. Greevy Jr. and Lindsay S. Mayberry and Jeffrey D. Blume},
  doi          = {10.1080/00031305.2024.2356109},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {50-60},
  shortjournal = {Am. Stat.},
  title        = {Sequential monitoring using the second generation P-value with type i error controlled by monitoring frequency},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The R2D2 prior for generalized linear mixed models. <em>TAS</em>, <em>79</em>(1), 40-49. (<a href='https://doi.org/10.1080/00031305.2024.2352010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Bayesian analysis, the selection of a prior distribution is typically done by considering each parameter in the model. While this can be convenient, in many scenarios it may be desirable to place a prior on a summary measure of the model instead. In this work, we propose a prior on the model fit, as measured by a Bayesian coefficient of determination ( R 2 ) , which then induces a prior on the individual parameters. We achieve this by placing a beta prior on R 2 and then deriving the induced prior on the global variance parameter for generalized linear mixed models. We derive closed-form expressions in many scenarios and present several approximation strategies when an analytic form is not possible and/or to allow for easier computation. In these situations, we suggest approximating the prior by using a generalized beta prime distribution and provide a simple default prior construction scheme. This approach is quite flexible and can be easily implemented in standard Bayesian software. Lastly, we demonstrate the performance of the method on simulated and real-world data, where the method particularly shines in high-dimensional settings, as well as modeling random effects.},
  archive      = {J_TAS},
  author       = {Eric Yanchenko and Howard D. Bondell and Brian J. Reich},
  doi          = {10.1080/00031305.2024.2352010},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {40-49},
  shortjournal = {Am. Stat.},
  title        = {The R2D2 prior for generalized linear mixed models},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The best time to play the lottery. <em>TAS</em>, <em>79</em>(1), 30-39. (<a href='https://doi.org/10.1080/00031305.2024.2351999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The best time to play the lottery is when the jackpot has rolled over several times and grown large, but not so large that you must share the prize if you win. We examine maximizing the expected value of a winning ticket as well as that in a random ticket. The derived optimality criteria depend on the prize elasticity of ticket demand. A regression analysis on data obtained from the Mega Millions® and Powerball ® multi-state lotteries suggests ticket sales grow quadratically in the size of the advertised lump-sum cash jackpot prize. With quadratic growth, the best time to play is when ticket sales are 1.25–2.5 times the jackpot odds, currently about 300 M to one for these two lotteries. Since ticket sales are not known to ticket buyers, we invert the regression function to prescribe the best time to play in terms of the cash prize. It turns out that these lotteries offer a (pretax) fair wager with positive expected value in a surprisingly wide interval of jackpot prizes. That is a good time to play; the best time is in the neighborhood of the nearly 1 $B record cash jackpot awarded in these lotteries in recent years.},
  archive      = {J_TAS},
  author       = {Christopher M. Rump},
  doi          = {10.1080/00031305.2024.2351999},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {30-39},
  shortjournal = {Am. Stat.},
  title        = {The best time to play the lottery},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple and fast algorithm for generating correlation matrices with a known average correlation coefficient. <em>TAS</em>, <em>79</em>(1), 23-29. (<a href='https://doi.org/10.1080/00031305.2024.2350449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes a simple and fast algorithm for generating correlation matrices ( R ) with a known average correlation. The algorithm should be useful for researchers desiring plausible R matrices for substantive domains in which average correlations are known (at least approximately). The method is non-iterative and it can solve relatively large problems (e.g., generate a 500 × 500 R matrix) in less than a second on a personal computer. It also has didactic value for introducing students to the convex set of feasible R matrices of a fixed dimension. This Euclidean body is called an elliptope. The proposed method exploits the geometry of elliptopes to efficiently generate realistic R matrices with a desired average correlation coefficient. R code for implementing the algorithm (and for reproducing all of the results of this article) is reported in an online supplement.},
  archive      = {J_TAS},
  author       = {Niels G. Waller},
  doi          = {10.1080/00031305.2024.2350449},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {23-29},
  shortjournal = {Am. Stat.},
  title        = {A simple and fast algorithm for generating correlation matrices with a known average correlation coefficient},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using exact tests from algebraic statistics in sparse multi-way analyses: An application to analyzing differential item functioning. <em>TAS</em>, <em>79</em>(1), 10-22. (<a href='https://doi.org/10.1080/00031305.2024.2388526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymptotic goodness-of-fit methods in contingency table analysis can struggle with sparse data, especially in multi-way tables where it can be infeasible to meet sample size requirements for a robust application of distributional assumptions. However, algebraic statistics provides exact alternatives to these classical asymptotic methods that remain viable even with sparse data. We apply these methods to a context in psychometrics and education research that leads naturally to multi-way contingency tables: the analysis of differential item functioning (DIF). We explain concretely how to apply the exact methods of algebraic statistics to DIF analysis using the R package algstat , and we compare their performance to that of classical asymptotic methods.},
  archive      = {J_TAS},
  author       = {Shishir Agrawal and Luis David Garcia Puente and Minho Kim and Flavia Sancier-Barbosa},
  doi          = {10.1080/00031305.2024.2388526},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {10-22},
  shortjournal = {Am. Stat.},
  title        = {Using exact tests from algebraic statistics in sparse multi-way analyses: An application to analyzing differential item functioning},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tightening blocks in complementary analyses of observational studies: Optimization algorithm and examples. <em>TAS</em>, <em>79</em>(1), 1-9. (<a href='https://doi.org/10.1080/00031305.2024.2393630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An observational block design has I blocks matched for covariates and J individuals per block, but treatments were not randomly assigned to individuals within blocks, as would have been done in an experiment. Tightening an observational block design means selecting J ′ < J individuals from each block, and possibly I ′ ≤ I blocks, to construct a new observational block design that, in some way, addresses unmeasured biases from nonrandom treatment assignment. Tightening must preserve covariate balance while altering the design to achieve some additional objective. An optimization algorithm is introduced that achieves this while maintaining the block structure by finely balancing covariates across blocks and through optimal subset matching. An example is considered in detail, both to motivate and illustrate the tightening of an observational block design. Two tightened designs are built from a study of light daily alcohol consumption and its possible effects on HDL cholesterol. One tightened design adjusts for an outcome tentatively presuming it was unaffected by the treatment. The second tightened design uses a differential effect to remove bias from an unobserved general disposition that promotes several treatments. An R package tightenBlock implements the method, contains the data, and in that package the help-file for the function tighten reproduces the example.},
  archive      = {J_TAS},
  author       = {Paul R. Rosenbaum},
  doi          = {10.1080/00031305.2024.2393630},
  journal      = {The American Statistician},
  month        = {1},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Am. Stat.},
  title        = {Tightening blocks in complementary analyses of observational studies: Optimization algorithm and examples},
  volume       = {79},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
