<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JASA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jasa">JASA - 126</h2>
<ul>
<li><details>
<summary>
(2025). Objective bayesian inference. <em>JASA</em>, <em>120</em>(550), 1321-1322. (<a href='https://doi.org/10.1080/01621459.2025.2454051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jaeyong Lee},
  doi          = {10.1080/01621459.2025.2454051},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1321-1322},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Objective bayesian inference},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soccer analytics: An introduction using r. <em>JASA</em>, <em>120</em>(550), 1320-1321. (<a href='https://doi.org/10.1080/01621459.2024.2435110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Alexander Aue},
  doi          = {10.1080/01621459.2024.2435110},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1320-1321},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Soccer analytics: An introduction using r},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handbook of bayesian, fiducial, and frequentist inference. <em>JASA</em>, <em>120</em>(550), 1318-1320. (<a href='https://doi.org/10.1080/01621459.2025.2454048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Mengyang Gu},
  doi          = {10.1080/01621459.2025.2454048},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1318-1320},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of bayesian, fiducial, and frequentist inference},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep regression learning with optimal loss function. <em>JASA</em>, <em>120</em>(550), 1305-1317. (<a href='https://doi.org/10.1080/01621459.2024.2412364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a novel efficient and robust nonparametric regression estimator under a framework of a feedforward neural network (FNN). There are several interesting characteristics for the proposed estimator. First, the loss function is built upon an estimated maximum likelihood function, which integrates the information from observed data as well as the information from the data distribution. Consequently, the resulting estimator has desirable optimal properties, such as efficiency. Second, different from the traditional maximum likelihood estimation (MLE), the proposed method avoids the specification of the distribution, making it adaptable to various distributions such as heavy tails and multimodal or heterogeneous distributions. Third, the proposed loss function relies on probabilities rather than direct observations as in least square loss, contributing to the robustness of the proposed estimator. Finally, the proposed loss function involves a nonparametric regression function only. This enables the direct application of the existing packages, simplifying the computational and programming requirements. We establish the large sample property of the proposed estimator in terms of its excess risk and minimax near-optimal rate. The theoretical results demonstrate that the proposed estimator is equivalent to the true MLE where the density function is known in terms of excess risk. Our simulation studies show that the proposed estimator outperforms the existing methods based on prediction accuracy, efficiency and robustness. Particularly, it is comparable to the MLE with the known density and even gets slightly better as the sample size increases. This implies that the adaptive and data-driven loss function from the estimated density may offer an additional avenue for capturing valuable information. We further apply the proposed method to four real data examples, resulting in significantly reduced out-of-sample prediction errors compared to existing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xuancheng Wang and Ling Zhou and Huazhen Lin},
  doi          = {10.1080/01621459.2024.2412364},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1305-1317},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Deep regression learning with optimal loss function},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust permutation tests in linear instrumental variables regression. <em>JASA</em>, <em>120</em>(550), 1294-1304. (<a href='https://doi.org/10.1080/01621459.2024.2412363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops permutation versions of identification-robust tests in linear instrumental variables regression. Unlike the existing randomization and rank-based tests in which independence between the instruments and the error terms is assumed, the permutation Anderson-Rubin (AR), Lagrange Multiplier (LM) and Conditional Likelihood Ratio (CLR) tests are asymptotically similar and robust to conditional heteroscedasticity under standard exclusion restriction, that is, the orthogonality between the instruments and the error terms. Moreover, when the instruments are independent of the structural error term, the permutation AR tests are exact, hence, robust to heavy tails. As such, these tests share the strengths of the rank-based tests and the wild bootstrap AR tests. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Purevdorj Tuvaandorj},
  doi          = {10.1080/01621459.2024.2412363},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1294-1304},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust permutation tests in linear instrumental variables regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sparse beta regression model for network analysis. <em>JASA</em>, <em>120</em>(550), 1281-1293. (<a href='https://doi.org/10.1080/01621459.2024.2411073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For statistical analysis of network data, the ùõΩ -model has emerged as a useful tool, thanks to its flexibility in incorporating nodewise heterogeneity and theoretical tractability. To generalize the Œ≤ -model, this article proposes the Sparse Œ≤ -Regression Model (S Œ≤ RM) that unites two research themes developed recently in modeling homophily and sparsity. In particular, we employ differential heterogeneity that assigns weights only to important nodes and propose penalized likelihood with an l 1 penalty for parameter estimation. While our estimation method is closely related to the LASSO method for logistic regression, we develop a new theory emphasizing the use of our model for dealing with a parameter regime that can handle sparse networks usually seen in practice. More interestingly, the resulting inference on the homophily parameter demands no debiasing normally employed in LASSO type estimation. We provide extensive simulation and data analysis to illustrate the use of the model. As a special case of our model, we extend the Erd≈ës-R√©nyi model by including covariates and develop the associated statistical inference for sparse networks, which may be of independent interest. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Stefan Stein and Rui Feng and Chenlei Leng},
  doi          = {10.1080/01621459.2024.2411073},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1281-1293},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A sparse beta regression model for network analysis},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative methods for vecchia-laplace approximations for latent gaussian process models. <em>JASA</em>, <em>120</em>(550), 1267-1280. (<a href='https://doi.org/10.1080/01621459.2024.2410004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent Gaussian process (GP) models are flexible probabilistic nonparametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations can thus become prohibitively slow precisely when the approximations are usually the most accurate, that is, on large datasets. In this article, we present iterative methods to overcome this drawback. Among other things, we introduce and analyze several preconditioners, derive new convergence results, and propose novel methods for accurately approximating predictive variances. We analyze our proposed methods theoretically and in experiments with simulated and real-world data. In particular, we obtain a speed-up of an order of magnitude compared to Cholesky-based calculations and a 3-fold increase in prediction accuracy in terms of the continuous ranked probability score compared to a state-of-the-art method on a large satellite dataset. All methods are implemented in a free C++ software library with high-level Python and R packages. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Pascal K√ºndig and Fabio Sigrist},
  doi          = {10.1080/01621459.2024.2410004},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1267-1280},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Iterative methods for vecchia-laplace approximations for latent gaussian process models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive learning of the latent space of wasserstein generative adversarial networks. <em>JASA</em>, <em>120</em>(550), 1254-1266. (<a href='https://doi.org/10.1080/01621459.2024.2408778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models based on latent variables, such as generative adversarial networks (GANs) and variational auto-encoders (VAEs), have gained lots of interests due to their impressive performance in many fields. However, many data such as natural images usually do not populate the ambient Euclidean space but instead reside in a lower-dimensional manifold. Thus an inappropriate choice of the latent dimension fails to uncover the structure of the data, possibly resulting in mismatch of latent representations and poor generative qualities. Toward addressing these problems, we propose a novel framework called the latent Wasserstein GAN (LWGAN) that fuses the Wasserstein auto-encoder and the Wasserstein GAN so that the intrinsic dimension of the data manifold can be adaptively learned by a modified informative latent distribution. We prove that there exist an encoder network and a generator network in such a way that the intrinsic dimension of the learned encoding distribution is equal to the dimension of the data manifold. We theoretically establish that our estimated intrinsic dimension is a consistent estimate of the true dimension of the data manifold. Meanwhile, we provide an upper bound on the generalization error of LWGAN, implying that we force the synthetic data distribution to be similar to the real data distribution from a population perspective. Comprehensive empirical experiments verify our framework and show that LWGAN is able to identify the correct intrinsic dimension under several scenarios, and simultaneously generate high-quality synthetic data by sampling from the learned latent distribution. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yixuan Qiu and Qingyi Gao and Xiao Wang},
  doi          = {10.1080/01621459.2024.2408778},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1254-1266},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Adaptive learning of the latent space of wasserstein generative adversarial networks},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring covariance structure from multiple data sources via subspace factor analysis. <em>JASA</em>, <em>120</em>(550), 1239-1253. (<a href='https://doi.org/10.1080/01621459.2024.2408777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor analysis provides a canonical framework for imposing lower-dimensional structure such as sparse covariance in high-dimensional data. High-dimensional data on the same set of variables are often collected under different conditions, for instance in reproducing studies across research groups. In such cases, it is natural to seek to learn the shared versus condition-specific structure. Existing hierarchical extensions of factor analysis have been proposed, but face practical issues including identifiability problems. To address these shortcomings, we propose a class of SUbspace Factor Analysis (SUFA) models, which characterize variation across groups at the level of a lower-dimensional subspace. We prove that the proposed class of SUFA models lead to identifiability of the shared versus group-specific components of the covariance, and study their posterior contraction properties. Taking a Bayesian approach, these contributions are developed alongside efficient posterior computation algorithms. Our sampler fully integrates out latent variables, is easily parallelizable and has complexity that does not depend on sample size. We illustrate the methods through application to integration of multiple gene expression datasets relevant to immunology. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Noirrit Kiran Chandra and David B. Dunson and Jason Xu},
  doi          = {10.1080/01621459.2024.2408777},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1239-1253},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inferring covariance structure from multiple data sources via subspace factor analysis},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model-agnostic graph neural network for integrating local and global information. <em>JASA</em>, <em>120</em>(550), 1225-1238. (<a href='https://doi.org/10.1080/01621459.2024.2404668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, however, existing GNNs suffer from two significant limitations: a lack of interpretability in their results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to effectively integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and demonstrate its power to represent layer-wise neighborhood mixing. We conduct comprehensive numerical studies using simulated data to demonstrate the superior performance of MaGNet in comparison to several state-of-the-art alternatives. Furthermore, we apply MaGNet to a real-world case study aimed at extracting task-critical information from brain activity data, thereby highlighting its effectiveness in advancing scientific research. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Wenzhuo Zhou and Annie Qu and Keiland W. Cooper and Norbert Fortin and Babak Shahbaba},
  doi          = {10.1080/01621459.2024.2404668},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1225-1238},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A model-agnostic graph neural network for integrating local and global information},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating higher-order mixed memberships via the l2,‚àû tensor perturbation bound. <em>JASA</em>, <em>120</em>(550), 1214-1224. (<a href='https://doi.org/10.1080/01621459.2024.2404265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher-order multiway data is ubiquitous in machine learning and statistics and often exhibits community-like structures, where each component (node) along each different mode has a community membership associated with it. In this article we propose the sub-Gaussian) tensor mixed-membership blockmodel , a generalization of the tensor blockmodel positing that memberships need not be discrete, but instead are convex combinations of latent communities. We establish the identifiability of our model and propose a computationally efficient estimation procedure based on the higher-order orthogonal iteration algorithm (HOOI) for tensor SVD composed with a simplex corner-finding algorithm. We then demonstrate the consistency of our estimation procedure by providing a per-node error bound under sub-Gaussian noise, which showcases the effect of higher-order structures on estimation accuracy. To prove our consistency result, we develop the l 2 , ‚àû tensor perturbation bound for HOOI under independent, heteroscedastic, sub-Gaussian noise that may be of independent interest. Our analysis uses a novel leave-one-out construction for the iterates, and our bounds depend only on spectral properties of the underlying low-rank tensor under nearly optimal signal-to-noise ratio conditions such that tensor SVD is computationally feasible. Finally, we apply our methodology to real and simulated data, demonstrating some effects not identifiable from the model with discrete community memberships. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Joshua Agterberg and Anru R. Zhang},
  doi          = {10.1080/01621459.2024.2404265},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1214-1224},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating higher-order mixed memberships via the l2,‚àû tensor perturbation bound},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive transfer learning framework for functional classification. <em>JASA</em>, <em>120</em>(550), 1201-1213. (<a href='https://doi.org/10.1080/01621459.2024.2403788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the transfer learning problem in functional classification, aiming to improve the classification accuracy of the target data by leveraging information from related source datasets. To facilitate transfer learning, we propose a novel transferability function tailored for classification problems, enabling a more accurate evaluation of the similarity between source and target dataset distributions. Interestingly, we find that a source dataset can offer more substantial benefits under certain conditions than another dataset with an identical distribution to the target dataset. This observation renders the commonly-used debiasing step in the parameter-based transfer learning algorithm unnecessary under some circumstances to the classification problem. In particular, we propose two adaptive transfer learning algorithms based on the functional Distance Weighted Discrimination (DWD) classifier for scenarios with and without prior knowledge regarding informative sources. Furthermore, we establish the upper bound on the excess risk of the proposed classifiers, providing the statistical gain via transfer learning mathematically provable. Simulation studies are conducted to thoroughly examine the finite-sample performance of the proposed algorithms. Finally, we implement the proposed method to Beijing air-quality data, and significantly improve the prediction of the PM 2.5 level of a target station by effectively incorporating information from source datasets. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Caihong Qin and Jinhan Xie and Ting Li and Yang Bai},
  doi          = {10.1080/01621459.2024.2403788},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1201-1213},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An adaptive transfer learning framework for functional classification},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale low-rank gaussian process prediction with support points. <em>JASA</em>, <em>120</em>(550), 1189-1200. (<a href='https://doi.org/10.1080/01621459.2024.2403188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank approximation is a popular strategy to tackle the ‚Äúbig n problem‚Äù associated with large-scale Gaussian process regressions. Basis functions for developing low-rank structures are crucial and should be carefully specified. Predictive processes simplify the problem by inducing basis functions with a covariance function and a set of knots. The existing literature suggests certain practical implementations of knot selection and covariance estimation; however, theoretical foundations explaining the influence of these two factors on predictive processes are lacking. In this article, the asymptotic prediction performance of the predictive process and Gaussian process predictions are derived and the impacts of the selected knots and estimated covariance are studied. The use of support points as knots, which best represent data locations, is advocated. Extensive simulation studies demonstrate the superiority of support points and verify our theoretical results. Real data of precipitation and ozone are used as examples, and the efficiency of our method over other widely used low-rank approximation methods is verified. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yan Song and Wenlin Dai and Marc G. Genton},
  doi          = {10.1080/01621459.2024.2403188},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1189-1200},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Large-scale low-rank gaussian process prediction with support points},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based clustering of categorical data based on the hamming distance. <em>JASA</em>, <em>120</em>(550), 1178-1188. (<a href='https://doi.org/10.1080/01621459.2024.2402568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A model-based approach is developed for clustering categorical data with no natural ordering. The proposed method exploits the Hamming distance to define a family of probability mass functions to model the data. The elements of this family are then considered as kernels of a finite mixture model with an unknown number of components. Conjugate Bayesian inference has been derived for the parameters of the Hamming distribution model. The mixture is framed in a Bayesian nonparametric setting, and a transdimensional blocked Gibbs sampler is developed to provide full Bayesian inference on the number of clusters, their structure, and the group-specific parameters, facilitating the computation with respect to customary reversible jump algorithms. The proposed model encompasses a parsimonious latent class model as a special case when the number of components is fixed. Model performances are assessed via a simulation study and reference datasets, showing improvements in clustering recovery over existing approaches. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Raffaele Argiento and Edoardo Filippi-Mazzola and Lucia Paci},
  doi          = {10.1080/01621459.2024.2402568},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1178-1188},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-based clustering of categorical data based on the hamming distance},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neyman-pearson multi-class classification via cost-sensitive learning. <em>JASA</em>, <em>120</em>(550), 1164-1177. (<a href='https://doi.org/10.1080/01621459.2024.2402567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing classification methods aim to minimize the overall misclassification error rate. However, in applications such as loan default prediction, different types of errors can have varying consequences. To address this asymmetry issue, two popular paradigms have been developed: the Neyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous studies on the NP paradigm have primarily focused on the binary case, while the multi-class NP problem poses a greater challenge due to its unknown feasibility. In this work, we tackle the multi-class NP problem by establishing a connection with the CS problem via strong duality and propose two algorithms. We extend the concept of NP oracle inequalities, crucial in binary classifications, to NP oracle properties in the multi-class context. Our algorithms satisfy these NP oracle properties under certain conditions. Furthermore, we develop practical algorithms to assess the feasibility and strong duality in multi-class NP problems, which can offer practitioners the landscape of a multi-class NP problem with various target error levels. Simulations and real data studies validate the effectiveness of our algorithms. To our knowledge, this is the first study to address the multi-class NP problem with theoretical guarantees. The proposed algorithms have been implemented in the R package npcs , which is available on CRAN. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ye Tian and Yang Feng},
  doi          = {10.1080/01621459.2024.2402567},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1164-1177},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Neyman-pearson multi-class classification via cost-sensitive learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On optimality of mallows model averaging. <em>JASA</em>, <em>120</em>(550), 1152-1163. (<a href='https://doi.org/10.1080/01621459.2024.2402566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decades, model averaging (MA) has attracted much attention as it has emerged as an alternative tool to the model selection (MS) statistical approach. Hansen introduced a Mallows model averaging (MMA) method with model weights selected by minimizing a Mallows‚Äô C p criterion. The main theoretical justification for MMA is an asymptotic optimality (AOP), which states that the risk/loss of the resulting MA estimator is asymptotically equivalent to that of the best but infeasible averaged model. MMA‚Äôs AOP is proved in the literature by either constraining weights in a special discrete weight set or limiting the number of candidate models. In this work, it is first shown that under these restrictions, however, the optimal risk of MA becomes an unreachable target, and MMA may converge more slowly than MS. In this background, a foundational issue that has not been addressed is: When a suitably large set of candidate models is considered, and the model weights are not harmfully constrained, can the MMA estimator perform asymptotically as well as the optimal convex combination of the candidate models? We answer this question in both nested and non-nested settings. In the nested setting, we provide finite sample inequalities for the risk of MMA and show that without unnatural restrictions on the candidate models, MMA‚Äôs AOP holds in a general continuous weight set under certain mild conditions. In the non-nested setting, a sufficient condition and a negative result are established for the achievability of the optimal MA risk. Implications on minimax adaptivity are given as well. The results from simulations and real data analysis back up our theoretical findings. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jingfu Peng and Yang Li and Yuhong Yang},
  doi          = {10.1080/01621459.2024.2402566},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1152-1163},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On optimality of mallows model averaging},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust estimation for number of factors in high dimensional factor modeling via spearman correlation matrix. <em>JASA</em>, <em>120</em>(550), 1139-1151. (<a href='https://doi.org/10.1080/01621459.2024.2402565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the number of factors in high-dimensional factor modeling is essential but challenging, especially when the data are heavy-tailed. In this article, we introduce a new estimator based on the spectral properties of Spearman sample correlation matrix under the high-dimensional setting, where both dimension and sample size tend to infinity proportionally. Our estimator is robust against heavy tails in either the common factors or idiosyncratic errors. The consistency of our estimator is established under mild conditions. Numerical experiments demonstrate the superiority of our estimator compared to existing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jiaxin Qiu and Zeng Li and Jianfeng Yao},
  doi          = {10.1080/01621459.2024.2402565},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1139-1151},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust estimation for number of factors in high dimensional factor modeling via spearman correlation matrix},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Valid inference after causal discovery. <em>JASA</em>, <em>120</em>(550), 1127-1138. (<a href='https://doi.org/10.1080/01621459.2024.2402089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery and causal effect estimation are two fundamental tasks in causal inference. While many methods have been developed for each task individually, statistical challenges arise when applying these methods jointly: estimating causal effects after running causal discovery algorithms on the same data leads to ‚Äúdouble dipping,‚Äù invalidating the coverage guarantees of classical confidence intervals. To this end, we develop tools for valid post-causal-discovery inference. Across empirical studies, we show that a naive combination of causal discovery and subsequent inference algorithms leads to highly inflated miscoverage rates; on the other hand, applying our method provides reliable coverage while allowing for a trade-off between causal discovery accuracy and confidence interval width. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Paula Gradu and Tijana Zrnic and Yixin Wang and Michael I. Jordan},
  doi          = {10.1080/01621459.2024.2402089},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1127-1138},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Valid inference after causal discovery},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving tensor regression by optimal model averaging. <em>JASA</em>, <em>120</em>(550), 1115-1126. (<a href='https://doi.org/10.1080/01621459.2024.2398164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensors have broad applications in neuroimaging, data mining, digital marketing, etc. CANDECOMP/PARAFAC (CP) tensor decomposition can effectively reduce the number of parameters to gain dimensionality-reduction and thus plays a key role in tensor regression. However, in CP decomposition, there is uncertainty about which rank to use. In this article, we develop a model averaging method to handle this uncertainty by weighting the estimators from candidate tensor regression models with different ranks. When all candidate models are misspecified, we prove that the model averaging estimator is asymptotically optimal. When correct models are included in the set of candidate models, we prove the consistency of parameters and the convergence of the model averaging weight. Simulations and empirical studies illustrate that the proposed method has superiority over the competition methods and has promising applications. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Qiushi Bu and Hua Liang and Xinyu Zhang and Jiahui Zou},
  doi          = {10.1080/01621459.2024.2398164},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1115-1126},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Improving tensor regression by optimal model averaging},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Off-policy evaluation in doubly inhomogeneous environments. <em>JASA</em>, <em>120</em>(550), 1102-1114. (<a href='https://doi.org/10.1080/01621459.2024.2395593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to study off-policy evaluation (OPE) under scenarios where two key reinforcement learning (RL) assumptions‚Äîtemporal stationarity and individual homogeneity are both violated. To handle the ‚Äúdouble inhomogeneities‚Äù, we propose a class of latent factor models for the reward and transition functions, under which we develop a general OPE framework that consists of both model-based and model-free approaches. To our knowledge, this is the first article that develops statistically sound OPE methods in offline RL with double inhomogeneities. It contributes to a deeper understanding of OPE in environments, where standard RL assumptions are not met, and provides several practical approaches in these settings. We establish the theoretical properties of the proposed value estimators and empirically show that our approach outperforms state-of-the-art methods. Finally, we illustrate our method on a dataset from the Medical Information Mart for Intensive Care. An R implementation of the proposed procedure is available at https://github.com/ZeyuBian/2FEOPE . Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zeyu Bian and Chengchun Shi and Zhengling Qi and Lan Wang},
  doi          = {10.1080/01621459.2024.2395593},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1102-1114},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Off-policy evaluation in doubly inhomogeneous environments},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based causal feature selection for general response types. <em>JASA</em>, <em>120</em>(550), 1090-1101. (<a href='https://doi.org/10.1080/01621459.2024.2395588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering causal relationships from observational data is a fundamental yet challenging task. Invariant causal prediction (ICP, Peters, B√ºhlmann, and Meinshausen) is a method for causal feature selection which requires data from heterogeneous settings and exploits that causal models are invariant. ICP has been extended to general additive noise models and to nonparametric settings using conditional independence tests. However, the latter often suffer from low power (or poor Type I error control) and additive noise models are not suitable for applications in which the response is not measured on a continuous scale, but reflects categories or counts. Here, we develop transformation-model ( tram ) based ICP, allowing for continuous, categorical, count-type, and uninformatively censored responses (these model classes, generally, do not allow for identifiability when there is no exogenous heterogeneity). As an invariance test, we propose tram -GCM based on the expected conditional covariance between environments and score residuals with uniform asymptotic level guarantees. For the special case of linear shift tram s, we also consider tram -Wald, which tests invariance based on the Wald statistic. We provide an open-source R package tramicp and evaluate our approach on simulated data and in a case study investigating causal features of survival in critically ill patients. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Lucas Kook and Sorawit Saengkyongam and Anton Rask Lundborg and Torsten Hothorn and Jonas Peters},
  doi          = {10.1080/01621459.2024.2395588},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1090-1101},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-based causal feature selection for general response types},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zigzag path connects two monte carlo samplers: Hamiltonian counterpart to a piecewise deterministic markov process. <em>JASA</em>, <em>120</em>(550), 1077-1089. (<a href='https://doi.org/10.1080/01621459.2024.2395587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zigzag and other piecewise deterministic Markov process samplers have attracted significant interest for their non-reversibility and other appealing properties for Bayesian posterior computation. Hamiltonian Monte Carlo is another state-of-the-art sampler, exploiting fictitious momentum to guide Markov chains through complex target distributions. We establish an important connection between the zigzag sampler and a variant of Hamiltonian Monte Carlo based on Laplace-distributed momentum. The position and velocity component of the corresponding Hamiltonian dynamics travels along a zigzag path paralleling the Markovian zigzag process; however, the dynamics is non-Markovian in this position-velocity space as the momentum component encodes non-immediate pasts. This information is partially lost during a momentum refreshment step, in which we preserve its direction but resample magnitude. In the limit of increasingly frequent momentum refreshments, we prove that Hamiltonian zigzag converges strongly to its Markovian counterpart. This theoretical insight suggests that, when retaining full momentum information, Hamiltonian zigzag can better explore target distributions with highly correlated parameters by suppressing the diffusive behavior of Markovian zigzag. We corroborate this intuition by comparing performance of the two zigzag cousins on high-dimensional truncated multivariate Gaussians, including a 11,235-dimensional target arising from a Bayesian phylogenetic multivariate probit modeling of HIV virus data. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Akihiko Nishimura and Zhenyu Zhang and Marc A. Suchard},
  doi          = {10.1080/01621459.2024.2395587},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1077-1089},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Zigzag path connects two monte carlo samplers: Hamiltonian counterpart to a piecewise deterministic markov process},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monte carlo inference for semiparametric bayesian regression. <em>JASA</em>, <em>120</em>(550), 1063-1076. (<a href='https://doi.org/10.1080/01621459.2024.2395586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data transformations are essential for broad applicability of parametric regression models. However, for Bayesian analysis, joint inference of the transformation and model parameters typically involves restrictive parametric transformations or nonparametric representations that are computationally inefficient and cumbersome for implementation and theoretical analysis, which limits their usability in practice. This article introduces a simple, general, and efficient strategy for joint posterior inference of an unknown transformation and all regression model parameters. The proposed approach directly targets the posterior distribution of the transformation by linking it with the marginal distributions of the independent and dependent variables, and then deploys a Bayesian nonparametric model via the Bayesian bootstrap. Crucially, this approach delivers (a) joint posterior consistency under general conditions, including multiple model misspecifications, and (b) efficient Monte Carlo (not Markov chain Monte Carlo) inference for the transformation and all parameters for important special cases. These tools apply across a variety of data domains, including real-valued, positive, and compactly-supported data. Simulation studies and an empirical application demonstrate the effectiveness and efficiency of this strategy for semiparametric Bayesian analysis with linear models, quantile regression, and Gaussian processes. The R package SeBR is available on CRAN. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Daniel R. Kowal and Bohan Wu},
  doi          = {10.1080/01621459.2024.2395586},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1063-1076},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Monte carlo inference for semiparametric bayesian regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal network pairwise comparison. <em>JASA</em>, <em>120</em>(550), 1048-1062. (<a href='https://doi.org/10.1080/01621459.2024.2393471'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in the problem of two-sample network hypothesis testing: given two networks with the same set of nodes, we wish to test whether the underlying Bernoulli probability matrices of the two networks are the same or not. We propose Interlacing Balance Measure (IBM) as a new two-sample testing approach. We consider the Degree-Corrected Mixed-Membership (DCMM) model for undirected networks, where we allow severe degree heterogeneity, mixed-memberships, flexible sparsity levels, and weak signals. In such a broad setting, how to find a test that has a tractable limiting null and optimal testing performances is a challenging problem. We show that IBM is such a test: in a broad DCMM setting with only mild regularity conditions, IBM has N ( 0 , 1 ) as the limiting null and achieves the optimal phase transition. While the above is for undirected networks, IBM is a unified approach and is directly implementable for directed networks. For a broad directed-DCMM (extension of DCMM for directed networks) setting, we show that IBM has N ( 0 , 1 / 2 ) as the limiting null and continues to achieve the optimal phase transition. We have also applied IBM to the Enron email network and a gene co-expression network, with interesting results. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jiashun Jin and Zheng Tracy Ke and Shengming Luo and Yucong Ma},
  doi          = {10.1080/01621459.2024.2393471},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1048-1062},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal network pairwise comparison},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised triply robust inductive transfer learning. <em>JASA</em>, <em>120</em>(550), 1037-1047. (<a href='https://doi.org/10.1080/01621459.2024.2393463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a Semi-supervised Triply Robust Inductive transFer LEarning (STRIFLE) approach, which integrates heterogeneous data from a label-rich source population and a label-scarce target population and uses a large amount of unlabeled data simultaneously to improve the learning accuracy in the target population. Specifically, we consider a high dimensional covariate shift setting and employ two nuisance models, a density ratio model and an imputation model, to combine transfer learning and surrogate-assisted semi-supervised learning strategies effectively and achieve triple robustness. While the STRIFLE approach assumes the target and source populations to share the same conditional distribution of outcome Y given both the surrogate features S and predictors X , it allows the true underlying model of Y‚èß X to differ between the two populations due to the potential covariate shift in S and X . Different from double robustness, even if both nuisance models are misspecified or the distribution of Y‚èß S , X is not the same between the two populations when the shifted source population and the target population share enough similarities, the triply robust STRIFLE estimator can still partially use the source population when the shifted source population and the target population share enough similarities. Moreover, it is guaranteed to be no worse than the target-only surrogate-assisted semi-supervised estimator with an additional error term from transferability detection. These desirable properties of our estimator are established theoretically and verified in finite samples via extensive simulation studies. We use the STRIFLE estimator to train a Type II diabetes polygenic risk prediction model for the African American target population by transferring knowledge from electronic health records linked genomic data observed in a larger European source population. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Tianxi Cai and Mengyan Li and Molei Liu},
  doi          = {10.1080/01621459.2024.2393463},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1037-1047},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semi-supervised triply robust inductive transfer learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Euclidean mirrors and dynamics in network time series. <em>JASA</em>, <em>120</em>(550), 1025-1036. (<a href='https://doi.org/10.1080/01621459.2024.2392912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing changes in network evolution is central to statistical network inference. We consider a dynamic network model in which each node has an associated time-varying low-dimensional latent vector of feature data, and connection probabilities are functions of these vectors. Under mild assumptions, the evolution of latent vectors exhibits low-dimensional manifold structure under a suitable distance. This distance can be approximated by a measure of separation between the observed networks themselves, and there exist Euclidean representations for underlying network structure, as characterized by this distance. These Euclidean representations, called Euclidean mirrors, permit the visualization of network dynamics and lead to methods for change point and anomaly detection in networks. We illustrate our methodology with real and synthetic data, and identify change points corresponding to massive shifts in pandemic policies in a communication network of a large organization. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Avanti Athreya and Zachary Lubberts and Youngser Park and Carey Priebe},
  doi          = {10.1080/01621459.2024.2392912},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1025-1036},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Euclidean mirrors and dynamics in network time series},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for networks of high-dimensional point processes. <em>JASA</em>, <em>120</em>(550), 1014-1024. (<a href='https://doi.org/10.1080/01621459.2024.2392907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fueled in part by recent applications in neuroscience, the multivariate Hawkes process has become a popular tool for modeling the network of interactions among high-dimensional point process data. While evaluating the uncertainty of the network estimates is critical in scientific applications, existing methodological and theoretical work has primarily addressed estimation. To bridge this gap, we develop a new statistical inference procedure for high-dimensional Hawkes processes. The key ingredient for the inference procedure is a new concentration inequality on the first- and second-order statistics for integrated stochastic processes, which summarize the entire history of the process. Combining recent martingale central limit theorem with the new concentration inequality, we then characterize the convergence rate of the test statistics in a continuous time domain. Finally, to account for potential non-stationarity of the process in practice, we extend our statistical inference procedure to a flexible class of Hawkes processes with time-varying background intensities and unknown transition functions. The finite sample validity of the inferential tools is illustrated via extensive simulations and further applied to a neuron spike train dataset. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xu Wang and Mladen Kolar and Ali Shojaie},
  doi          = {10.1080/01621459.2024.2392907},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1014-1024},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for networks of high-dimensional point processes},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust regression with covariate filtering: Heavy tails and adversarial contamination. <em>JASA</em>, <em>120</em>(550), 1002-1013. (<a href='https://doi.org/10.1080/01621459.2024.2392906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of linear regression where both covariates and responses are potentially (i) heavy-tailed and (ii) adversarially contaminated. Several computationally efficient estimators have been proposed for the simpler setting where the covariates are sub-Gaussian and uncontaminated; however, these estimators may fail when the covariates are either heavy-tailed or contain outliers. In this work, we show how to modify the Huber regression, least trimmed squares, and least absolute deviation estimators to obtain estimators which are simultaneously computationally and statistically efficient in the stronger contamination model. Our approach is quite simple, and consists of applying a filtering algorithm to the covariates, and then applying the classical robust regression estimators to the remaining data. We show that the Huber regression estimator achieves near-optimal error rates in this setting, whereas the least trimmed squares and least absolute deviation estimators can be made to achieve near-optimal error after applying a postprocessing step. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ankit Pensia and Varun Jog and Po-Ling Loh},
  doi          = {10.1080/01621459.2024.2392906},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1002-1013},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust regression with covariate filtering: Heavy tails and adversarial contamination},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural gradient variational bayes without fisher matrix analytic calculation and its inversion. <em>JASA</em>, <em>120</em>(550), 990-1001. (<a href='https://doi.org/10.1080/01621459.2024.2392904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a method for efficiently approximating the inverse of the Fisher information matrix, a crucial step in achieving effective variational Bayes inference. A notable aspect of our approach is the avoidance of analytically computing the Fisher information matrix and its explicit inversion. Instead, we introduce an iterative procedure for generating a sequence of matrices that converge to the inverse of Fisher information. The natural gradient variational Bayes algorithm without analytic expression of the Fisher matrix and its inversion is provably convergent and achieves a convergence rate of order O ( log s / s ) , with s the number of iterations. We also obtain a central limit theorem for the iterates. Implementation of our method does not require storage of large matrices, and achieves a linear complexity in the number of variational parameters. Our algorithm exhibits versatility, making it applicable across a diverse array of variational Bayes domains, including Gaussian approximation and normalizing flow Variational Bayes. We offer a range of numerical examples to demonstrate the efficiency and reliability of the proposed variational Bayes method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {A. Godichon-Baggioni and D. Nguyen and M.-N. Tran},
  doi          = {10.1080/01621459.2024.2392904},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {990-1001},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Natural gradient variational bayes without fisher matrix analytic calculation and its inversion},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multiple change point detection and localization for high-dimensional quantile regression with heteroscedasticity. <em>JASA</em>, <em>120</em>(550), 976-989. (<a href='https://doi.org/10.1080/01621459.2024.2392903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data heterogeneity is a challenging issue for modern statistical data analysis. There are different types of data heterogeneity in practice. In this article, we consider potential structural changes and complicated tail distributions. There are various existing methods proposed to handle either structural changes or heteroscedasticity. However, it is difficult to handle them simultaneously. To overcome this limitation, we consider statistically and computationally efficient change point detection and localization in high-dimensional quantile regression models. Our proposed framework is general and flexible since the change points and the underlying regression coefficients are allowed to vary across different quantile levels. The model parameters, including the data dimension, the number of change points, and the signal jump size, can be scaled with the sample size. Under this framework, we construct a novel two-step estimation of the number and locations of the change points as well as the underlying regression coefficients. Without any moment constraints on the error term, we present theoretical results, including consistency of the change point number, oracle estimation of change point locations, and estimation for the underlying regression coefficients with the optimal convergence rate. Finally, we present simulation results and an application to the S&P 100 dataset to demonstrate the advantage of the proposed method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xianru Wang and Bin Liu and Xinsheng Zhang and Yufeng Liu},
  doi          = {10.1080/01621459.2024.2392903},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {976-989},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficient multiple change point detection and localization for high-dimensional quantile regression with heteroscedasticity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel sampling of decomposable graphs using markov chains on junction trees. <em>JASA</em>, <em>120</em>(550), 963-975. (<a href='https://doi.org/10.1080/01621459.2024.2388908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference for undirected graphical models is mostly restricted to the class of decomposable graphs, as they enjoy a rich set of properties making them amenable to high-dimensional problems. While parameter inference is straightforward in this setup, inferring the underlying graph is a challenge driven by the computational difficulty in exploring the space of decomposable graphs. This work makes two contributions to address this problem. First, we provide sufficient and necessary conditions for when multi-edge perturbations maintain decomposability of the graph. Using these, we characterize a simple class of partitions that efficiently classify all edge perturbations by whether they maintain decomposability. Second, we propose a novel parallel nonreversible Markov chain Monte Carlo sampler for distributions over junction tree representations of the graph. At every step, the parallel sampler executes simultaneously all edge perturbations within a partition. Through simulations, we demonstrate the efficiency of our new edge perturbation conditions and class of partitions. We find that our parallel sampler yields improved mixing properties in comparison to the single-move variate, and outperforms current state-of-the-art methods in terms of accuracy and computational efficiency. The implementation of our work is available in the Python package parallelDG. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Mohamad Elmasri},
  doi          = {10.1080/01621459.2024.2388908},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {963-975},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Parallel sampling of decomposable graphs using markov chains on junction trees},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal network membership estimation under severe degree heterogeneity. <em>JASA</em>, <em>120</em>(550), 948-962. (<a href='https://doi.org/10.1080/01621459.2024.2388903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real networks often have severe degree heterogeneity, with maximum, average, and minimum node degrees differing significantly. This article examines the impact of degree heterogeneity on statistical limits of network data analysis. Introducing the heterogeneity distribution (HD) under a degree-corrected mixed membership model, we show that the optimal rate of mixed membership estimation is an explicit functional of the HD. This result confirms that severe degree heterogeneity decelerates the error rate, even when the overall sparsity remains unchanged. To obtain a rate-optimal method, we modify an existing spectral algorithm, Mixed-SCORE, by adding a pre-PCA normalization step. This step normalizes the adjacency matrix by a diagonal matrix consisting of the b th power of node degrees, for some b ‚àà R . We discover that b =‚Äâ1/2 is universally favorable. The resulting spectral algorithm is rate-optimal for networks with arbitrary degree heterogeneity. A technical component in our proofs is entry-wise eigenvector analysis of the normalized graph Laplacian. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zheng Tracy Ke and Jingming Wang},
  doi          = {10.1080/01621459.2024.2388903},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {948-962},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal network membership estimation under severe degree heterogeneity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controlling the false split rate in tree-based aggregation. <em>JASA</em>, <em>120</em>(550), 935-947. (<a href='https://doi.org/10.1080/01621459.2024.2376285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many domains, data measurements can naturally be associated with the leaves of a tree, expressing the relationships among these measurements. For example, companies belong to industries, which in turn belong to ever coarser divisions such as sectors; microbes are commonly arranged in a taxonomic hierarchy from species to kingdoms; street blocks belong to neighborhoods, which in turn belong to larger-scale regions. The problem of tree-based aggregation that we consider in this article asks which of these tree-defined subgroups of leaves should really be treated as a single entity and which of these entities should be distinguished from each other. We introduce the false split rate , an error measure that describes the degree to which subgroups have been split when they should not have been. While expressible as the false discovery rate in a special case, we show that these measures can be quite different for the general tree structures common in our setting. We then propose a multiple hypothesis testing algorithm for tree-based aggregation, which we prove controls this error measure. We focus on two main examples of tree-based aggregation, one which involves aggregating means and the other hich involves aggregating regression coefficients. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Simeng Shao and Jacob Bien and Adel Javanmard},
  doi          = {10.1080/01621459.2024.2376285},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {935-947},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Controlling the false split rate in tree-based aggregation},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust matrix completion with heavy-tailed noise. <em>JASA</em>, <em>120</em>(550), 922-934. (<a href='https://doi.org/10.1080/01621459.2024.2375037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies noisy low-rank matrix completion in the presence of heavy-tailed and possibly asymmetric noise, where we aim to estimate an underlying low-rank matrix given a set of highly incomplete noisy entries. Though the matrix completion problem has attracted much attention in the past decade, there is still lack of theoretical understanding when the observations are contaminated by heavy-tailed noises. Prior theory falls short of explaining the empirical results and is unable to capture the optimal dependence of the estimation error on the noise level. In this article, we adopt an adaptive Huber loss to accommodate heavy-tailed noise, which is robust against large and possibly asymmetric errors when the parameter in the Huber loss function is carefully designed to balance the Huberization biases and robustness to outliers. Then, we propose an efficient nonconvex algorithm via a balanced low-rank Burer-Monteiro matrix factorization and gradient descent with robust spectral initialization. We prove that under merely a bounded second-moment condition on the error distributions, rather than the sub-Gaussian assumption, the Euclidean errors of the iterates generated by the proposed algorithm decrease geometrically fast until achieving a minimax-optimal statistical estimation error, which has the same order as that in the sub-Gaussian case. The key technique behind this significant advancement is a powerful leave-one-out analysis framework. The theoretical results are corroborated by our numerical studies. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Bingyan Wang and Jianqing Fan},
  doi          = {10.1080/01621459.2024.2375037},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {922-934},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust matrix completion with heavy-tailed noise},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for H√ºsler‚ÄìReiss graphical models through matrix completions. <em>JASA</em>, <em>120</em>(550), 909-921. (<a href='https://doi.org/10.1080/01621459.2024.2371978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The severity of multivariate extreme events is driven by the dependence between the largest marginal observations. The H√ºsler‚ÄìReiss distribution is a versatile model for this extremal dependence, and it is usually parameterized by a variogram matrix. In order to represent conditional independence relations and obtain sparse parameterizations, we introduce the novel H√ºsler‚ÄìReiss precision matrix. Similarly to the Gaussian case, this matrix appears naturally in density representations of the H√ºsler‚ÄìReiss Pareto distribution and encodes the extremal graphical structure through its zero pattern. For a given, arbitrary graph we prove the existence and uniqueness of the completion of a partially specified H√ºsler‚ÄìReiss variogram matrix so that its precision matrix has zeros on non-edges in the graph. Using suitable estimators for the parameters on the edges, our theory provides the first consistent estimator of graph structured H√ºsler‚ÄìReiss distributions. If the graph is unknown, our method can be combined with recent structure learning algorithms to jointly infer the graph and the corresponding parameter matrix. Based on our methodology, we propose new tools for statistical inference of sparse H√ºsler‚ÄìReiss models and illustrate them on large flight delay data in the United States, as well as Danube river flow data. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Manuel Hentschel and Sebastian Engelke and Johan Segers},
  doi          = {10.1080/01621459.2024.2371978},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {909-921},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for H√ºsler‚ÄìReiss graphical models through matrix completions},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual dynamic pricing with strategic buyers. <em>JASA</em>, <em>120</em>(550), 896-908. (<a href='https://doi.org/10.1080/01621459.2024.2370613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this article, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer‚Äôs true feature, but a manipulated feature according to buyers‚Äô strategic behavior. In addition, the seller does not observe the buyers‚Äô valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers‚Äô strategic behavior into the online learning to maximize the seller‚Äôs cumulative revenue. We first prove that existing nonstrategic pricing policies that neglect the buyers‚Äô strategic behavior result in a linear Œ© ( T ) regret with T the total time horizon, indicating that these policies are not better than a random pricing policy. We then establish an O ( T ) regret upper bound of our proposed policy and an Œ© ( T ) regret lower bound for any pricing policy within our problem setting. This underscores the rate optimality of our policy. Importantly, our policy is not a mere amalgamation of existing dynamic pricing policies and strategic behavior handling algorithms. Our policy can also accommodate the scenario when the marginal cost of manipulation is unknown in advance. To account for it, we simultaneously estimate the valuation parameter and the cost parameter in the online pricing policy, which is shown to also achieve an O ( T ) regret bound. Extensive experiments support our theoretical developments and demonstrate the superior performance of our policy compared to other pricing policies that are unaware of the strategic behaviors. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Pangpang Liu and Zhuoran Yang and Zhaoran Wang and Will Wei Sun},
  doi          = {10.1080/01621459.2024.2370613},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {896-908},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Contextual dynamic pricing with strategic buyers},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic likelihood in misspecified models. <em>JASA</em>, <em>120</em>(550), 884-895. (<a href='https://doi.org/10.1080/01621459.2024.2370594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian synthetic likelihood is a widely used approach for conducting Bayesian analysis in complex models where evaluation of the likelihood is infeasible but simulation from the assumed model is tractable. We analyze the behavior of the Bayesian synthetic likelihood posterior when the assumed model differs from the actual data generating process. We demonstrate that the Bayesian synthetic likelihood posterior can display a wide range of nonstandard behaviors depending on the level of model misspecification, including multimodality and asymptotic non-Gaussianity. Our results suggest that likelihood tempering, a common approach for robust Bayesian inference, fails for synthetic likelihood whilst recently proposed robust synthetic likelihood approaches can ameliorate this behavior and deliver reliable posterior inference under model misspecification. All results are illustrated using a simple running example. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {David T. Frazier and David J. Nott and Christopher Drovandi},
  doi          = {10.1080/01621459.2024.2370594},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {884-895},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Synthetic likelihood in misspecified models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised dynamic PCA: Linear dynamic forecasting with many predictors. <em>JASA</em>, <em>120</em>(550), 869-883. (<a href='https://doi.org/10.1080/01621459.2024.2370592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel dynamic forecasting method using a new supervised Principal Component Analysis (PCA) when a large number of predictors are available. The new supervised PCA provides an effective way to bridge the gap between predictors and the target variable of interest by scaling and combining the predictors and their lagged values, resulting in an effective dynamic forecasting. Unlike the traditional diffusion-index approach, which does not learn the relationships between the predictors and the target variable before conducting PCA, we first rescale each predictor according to their significance in forecasting the targeted variable in a dynamic fashion, and a PCA is then applied to a rescaled and additive panel, which establishes a connection between the predictability of the PCA factors and the target variable. We also propose to use penalized methods such as the LASSO to select the significant factors that have superior predictive power over the others. Theoretically, we show that our estimators are consistent and outperform the traditional methods in prediction under some mild conditions. We conduct extensive simulations to verify that the proposed method produces satisfactory forecasting results and outperforms most of the existing methods using the traditional PCA. An example of predicting U.S. macroeconomic variables using a large number of predictors showcases that our method fares better than most of the existing ones in applications. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zhaoxing Gao and Ruey S. Tsay},
  doi          = {10.1080/01621459.2024.2370592},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {869-883},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Supervised dynamic PCA: Linear dynamic forecasting with many predictors},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced response envelope via envelope regularization. <em>JASA</em>, <em>120</em>(550), 859-868. (<a href='https://doi.org/10.1080/01621459.2024.2368844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The response envelope model provides substantial efficiency gains over the standard multivariate linear regression by identifying the material part of the response to the model and by excluding the immaterial part. In this article, we propose the enhanced response envelope by incorporating a novel envelope regularization term based on a nonconvex manifold formulation. It is shown that the enhanced response envelope can yield better prediction risk than the original envelope estimator. The enhanced response envelope naturally handles high-dimensional data for which the original response envelope is not serviceable without necessary remedies. In an asymptotic high-dimensional regime where the ratio of the number of predictors over the number of samples converges to a nonzero constant, we characterize the risk function and reveal an interesting double descent phenomenon for the envelope model. A simulation study confirms our main theoretical findings. Simulations and real data applications demonstrate that the enhanced response envelope does have significantly improved prediction performance over the original envelope method, especially when the number of predictors is close to or moderately larger than the number of samples. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Oh-Ran Kwon and Hui Zou},
  doi          = {10.1080/01621459.2024.2368844},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {859-868},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Enhanced response envelope via envelope regularization},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tyranny-of-the-minority regression adjustment in randomized experiments. <em>JASA</em>, <em>120</em>(550), 846-858. (<a href='https://doi.org/10.1080/01621459.2024.2366043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression adjustment is widely used in the analysis of randomized experiments to improve the estimation efficiency of the treatment effect. This article reexamines a weighted regression adjustment method termed t yranny- o f-the- m inority (ToM), wherein units in the minority group are given greater weights. We demonstrate that ToM regression adjustment is more robust than Lin ‚Äôs regression adjustment with treatment-covariate interactions, even though these two regression adjustment methods are asymptotically equivalent in completely randomized experiments. Moreover, ToM regression adjustment can be easily extended to stratified randomized experiments and completely randomized survey experiments. We obtain the design-based properties of the ToM regression-adjusted average treatment effect estimator under such designs. In particular, we show that the ToM regression-adjusted estimator improves the asymptotic estimation efficiency compared to the unadjusted estimator, even when the regression model is misspecified, and is optimal in the class of linearly adjusted estimators. We also study the asymptotic properties of various heteroscedasticity-robust standard errors and provide recommendations for practitioners. Simulation studies and real data analysis demonstrate ToM regression adjustment‚Äôs superiority over existing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xin Lu and Hanzhong Liu},
  doi          = {10.1080/01621459.2024.2366043},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {846-858},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Tyranny-of-the-minority regression adjustment in randomized experiments},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test and measure for partial mean dependence based on machine learning methods. <em>JASA</em>, <em>120</em>(550), 833-845. (<a href='https://doi.org/10.1080/01621459.2024.2366030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is of importance to investigate the significance of a subset of covariates W for the response Y given covariates Z in regression modeling. To this end, we propose a significance test for the partial mean independence problem based on machine learning methods and data splitting. The test statistic converges to the standard Chi-squared distribution under the null hypothesis while it converges to a normal distribution under the fixed alternative hypothesis. Power enhancement and algorithm stability are also discussed. If the null hypothesis is rejected, we propose a partial Generalized Measure of Correlation (pGMC) to measure the partial mean dependence of Y given W after controlling for the nonlinear effect of Z . We present the appealing theoretical properties of the pGMC and establish the asymptotic normality of its estimator with the optimal root- N convergence rate. Furthermore, the valid confidence interval for the pGMC is also derived. As an important special case when there are no conditional covariates Z , we introduce a new test of overall significance of covariates for the response in a model-free setting. Numerical studies and real data analysis are also conducted to compare with existing approaches and to demonstrate the validity and flexibility of our proposed procedures. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Leheng Cai and Xu Guo and Wei Zhong},
  doi          = {10.1080/01621459.2024.2366030},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {833-845},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Test and measure for partial mean dependence based on machine learning methods},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric multiple-output center-outward quantile regression. <em>JASA</em>, <em>120</em>(550), 818-832. (<a href='https://doi.org/10.1080/01621459.2024.2366029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building on recent measure-transportation-based concepts of multivariate quantiles, we are considering the problem of nonparametric multiple-output quantile regression. Our approach defines nested conditional center-outward quantile regression contours and regions with given conditional probability content, the graphs of which constitute nested center-outward quantile regression tubes with given unconditional probability content; these (conditional and unconditional) probability contents do not depend on the underlying distribution‚Äîan essential property of quantile concepts. Empirical counterparts of these concepts are constructed, yielding interpretable empirical contours, regions, and tubes which are shown to consistently reconstruct (in the Pompeiu-Hausdorff topology) their population versions. Our method is entirely nonparametric and performs well in simulations‚Äîwith possible heteroscedasticity and nonlinear trends. Its potential as a data-analytic tool is illustrated on some real datasets. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Eustasio del Barrio and Alberto Gonz√°lez Sanz and Marc Hallin},
  doi          = {10.1080/01621459.2024.2366029},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {818-832},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric multiple-output center-outward quantile regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). False discovery rate control for structured multiple testing: Asymmetric rules and conformal Q-values. <em>JASA</em>, <em>120</em>(550), 805-817. (<a href='https://doi.org/10.1080/01621459.2024.2359739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective utilization of structural information in data while ensuring statistical validity poses a significant challenge in false discovery rate (FDR) analyses. Conformal inference provides rigorous theory for grounding complex machine learning methods without relying on strong assumptions or highly idealized models. However, existing conformal methods have limitations in handling structured multiple testing, as their validity often requires the deployment of symmetric decision rules, which assume the exchangeability of data points and permutation-invariance of fitting algorithms. To overcome these limitations, we introduce the pseudo local index of significance (PLIS) procedure, which is capable of accommodating asymmetric rules and requires only pairwise exchangeability between the null conformity scores. We demonstrate that PLIS offers finite-sample guarantees in FDR control and the ability to assign higher weights to relevant data points. Numerical results confirm the effectiveness and robustness of PLIS and demonstrate improvements in power compared to existing model-free methods in various scenarios. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zinan Zhao and Wenguang Sun},
  doi          = {10.1080/01621459.2024.2359739},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {805-817},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {False discovery rate control for structured multiple testing: Asymmetric rules and conformal Q-values},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mediation analysis with the mediator and outcome missing not at random. <em>JASA</em>, <em>120</em>(550), 794-804. (<a href='https://doi.org/10.1080/01621459.2024.2359132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is widely used for investigating direct and indirect causal pathways through which an effect arises. However, many mediation analysis studies are challenged by missingness in the mediator and outcome. In general, when the mediator and outcome are missing not at random, the direct and indirect effects are not identifiable without further assumptions. We study the identifiability of the direct and indirect effects under some interpretable mechanisms that allow for missing not at random in the mediator and outcome. We evaluate the performance of statistical inference under those mechanisms through simulation studies and illustrate the proposed methods via the National Job Corps Study. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shuozhi Zuo and Debashis Ghosh and Peng Ding and Fan Yang},
  doi          = {10.1080/01621459.2024.2359132},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {794-804},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Mediation analysis with the mediator and outcome missing not at random},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed heterogeneity learning for generalized partially linear models with spatially varying coefficients. <em>JASA</em>, <em>120</em>(550), 779-793. (<a href='https://doi.org/10.1080/01621459.2024.2359131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial heterogeneity is of great importance in social, economic, and environmental science studies. The spatially varying coefficient model is a popular and effective spatial regression technique to address spatial heterogeneity. However, accounting for heterogeneity comes at the cost of reducing model parsimony. To balance flexibility and parsimony, this article develops a class of generalized partially linear spatially varying coefficient models which allow the inclusion of both constant and spatially varying effects of covariates. Another significant challenge in many applications comes from the enormous size of the spatial datasets collected from modern technologies. To tackle this challenge, we design a novel distributed heterogeneity learning (DHL) method based on bivariate spline smoothing over a triangulation of the domain. The proposed DHL algorithm has a simple, scalable, and communication-efficient implementation scheme that can almost achieve linear speedup. In addition, this article provides rigorous theoretical support for the DHL framework. We prove that the DHL constant coefficient estimators are asymptotic normal and the DHL spline estimators reach the same convergence rate as the global spline estimators obtained using the entire dataset. The proposed DHL method is evaluated through extensive simulation studies and analyses of U.S. loan application data. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shan Yu and Guannan Wang and Li Wang},
  doi          = {10.1080/01621459.2024.2359131},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {779-793},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distributed heterogeneity learning for generalized partially linear models with spatially varying coefficients},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Node-level community detection within edge exchangeable models for interaction processes. <em>JASA</em>, <em>120</em>(550), 764-778. (<a href='https://doi.org/10.1080/01621459.2024.2358560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientists are increasingly interested in discovering community structure from modern relational data arising on large-scale social networks. While many methods have been proposed for learning community structure, few account for the fact that these modern networks arise from processes of interactions in the population. We introduce block edge exchangeable models (BEEM) for the study of interaction networks with latent node-level community structure. The block vertex components model (B-VCM) is derived as a canonical example. Several theoretical and practical advantages over traditional vertex-centric approaches are highlighted. In particular, BEEMs allow for sparse degree structure and power-law degree distributions within communities. Our theoretical analysis bounds the misspecification rate of block assignments while supporting simulations show the properties of the network can be recovered. A computationally tractable Gibbs algorithm is derived. We demonstrate the proposed model using post-comment interaction data from Talklife, a large-scale online peer-to-peer support network, and contrast the learned communities from those using standard algorithms including degree-corrected stochastic block models, popularity-adjusted block models, and weighted stochastic block models. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yuhua Zhang and Walter Dempsey},
  doi          = {10.1080/01621459.2024.2358560},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {764-778},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Node-level community detection within edge exchangeable models for interaction processes},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Population-level balance in signed networks. <em>JASA</em>, <em>120</em>(550), 751-763. (<a href='https://doi.org/10.1080/01621459.2024.2356894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical network models are useful for understanding the underlying formation mechanism and characteristics of complex networks. However, statistical models for signed networks have been largely unexplored. In signed networks, there exist both positive (e.g., like, trust) and negative (e.g., dislike, distrust) edges, which are commonly seen in real-world scenarios. The positive and negative edges in signed networks lead to unique structural patterns, which pose challenges for statistical modeling. In this article, we introduce a statistically principled latent space approach for modeling signed networks and accommodating the well-known balance theory , that is, ‚Äúthe enemy of my enemy is my friend‚Äù and ‚Äúthe friend of my friend is my friend.‚Äù The proposed approach treats both edges and their signs as random variables, and characterizes the balance theory with a novel and natural notion of population-level balance. This approach guides us towards building a class of balanced inner-product models, and toward developing scalable algorithms via projected gradient descent to estimate the latent variables. We also establish non-asymptotic error rates for the estimates, which are further verified through simulation studies. In addition, we apply the proposed approach to an international relation network, which provides an informative and interpretable model-based visualization of countries during World War II. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Weijing Tang and Ji Zhu},
  doi          = {10.1080/01621459.2024.2356894},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {751-763},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Population-level balance in signed networks},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rate-optimal rank aggregation with private pairwise rankings. <em>JASA</em>, <em>120</em>(550), 737-750. (<a href='https://doi.org/10.1080/01621459.2025.2484843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various real-world scenarios, such as recommender systems and political surveys, pairwise rankings are commonly collected and used for rank aggregation to derive an overall ranking of items. However, preference rankings can reveal individuals‚Äô personal preferences, highlighting the need to protect them from exposure in downstream analysis. In this article, we address the challenge of preserving privacy while ensuring the utility of rank aggregation based on pairwise rankings generated from a general comparison model. A common privacy protection strategy in practice is the use of the randomized response mechanism to perturb raw pairwise rankings. However, a critical challenge arises because the privatized rankings no longer adhere to the original model, resulting in significant bias in downstream rank aggregation tasks. To address this, we propose an adaptive debiasing method for rankings from the randomized response mechanism, ensuring consistent estimation of true preferences and enhancing the utility of downstream rank aggregation. Theoretically, we provide insights into the relationship between overall privacy guarantees and estimation errors in private ranking data, and establish minimax rates for estimation errors. This enables the determination of optimal privacy guarantees that balance consistency in rank aggregation with privacy protection. We also investigate convergence rates of expected ranking errors for partial and full ranking recovery, quantifying how privacy protection affects the specification of top- K item sets and complete rankings. Our findings are validated through extensive simulations and a real-world application. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shirong Xu and Will Wei Sun and Guang Cheng},
  doi          = {10.1080/01621459.2025.2484843},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {737-750},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rate-optimal rank aggregation with private pairwise rankings},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse bayesian group factor model for feature interactions in multiple count tables data. <em>JASA</em>, <em>120</em>(550), 723-736. (<a href='https://doi.org/10.1080/01621459.2025.2449721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group factor models have been developed to infer relationships between multiple co-occurring multivariate continuous responses. Motivated by complex count data from multi-domain microbiome studies using next-generation sequencing, we develop a sparse Bayesian group factor model (Sp-BGFM) for multiple count table data that captures the interaction between microorganisms in different domains. Sp-BGFM uses a rounded kernel mixture model using a Dirichlet process (DP) prior with log-normal mixture kernels for count vectors. A group factor model is used to model the covariance matrix of the mixing kernel that describes microorganism interaction. We construct a Dirichlet-Horseshoe (Dir-HS) shrinkage prior and use it as a joint prior for factor loading vectors. Joint sparsity induced by a Dir-HS prior greatly improves the performance in high-dimensional applications. We further model the effects of covariates on microbial abundances using regression. The semiparametric model flexibly accommodates large variability in observed counts and excess zero counts and provides a basis for robust estimation of the interaction and covariate effects. We evaluate Sp-BGFM using simulation studies and real data analysis, comparing it to popular alternatives. Our results highlight the necessity of joint sparsity induced by the Dir-HS prior, and the benefits of a flexible DP model for baseline abundances. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shuangjie Zhang and Yuning Shen and Irene A. Chen and Juhee Lee},
  doi          = {10.1080/01621459.2025.2449721},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {723-736},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sparse bayesian group factor model for feature interactions in multiple count tables data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GeoWarp: Warped spatial processes for inferring subsea sediment properties. <em>JASA</em>, <em>120</em>(550), 710-722. (<a href='https://doi.org/10.1080/01621459.2024.2445874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For offshore structures like wind turbines, subsea infrastructure, pipelines, and cables, it is crucial to quantify the properties of the seabed sediments at a proposed site. However, data collection offshore is costly, so analysis of the seabed sediments must be made from measurements that are spatially sparse. Adding to this challenge, the structure of the seabed sediments exhibits both nonstationarity and anisotropy. To address these issues, we propose GeoWarp, a hierarchical spatial statistical modeling framework for inferring the 3-D geotechnical properties of subsea sediments. GeoWarp decomposes the seabed properties into a region-wide vertical mean profile (modeled using B-splines), and a nonstationary 3-D spatial Gaussian process. Process nonstationarity and anisotropy are accommodated by warping space in three dimensions and by allowing the process variance to change with depth. We apply GeoWarp to measurements of the seabed made using cone penetrometer tests (CPTs) at six sites on the North West Shelf of Australia. We show that GeoWarp captures the complex spatial distribution of the sediment properties, and produces realistic 3-D simulations suitable for downstream engineering analyses. Through cross-validation, we show that GeoWarp has predictive performance superior to other state-of-the-art methods, demonstrating its value as a tool in offshore geotechnical engineering. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Michael Bertolacci and Andrew Zammit-Mangion and Juan Valderrama Giraldo and Michael O‚ÄôNeill and Fraser Bransby and Phil Watson},
  doi          = {10.1080/01621459.2024.2445874},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {710-722},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {GeoWarp: Warped spatial processes for inferring subsea sediment properties},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining broad and narrow case definitions in matched case-control studies: Firearms in the home and suicide risk. <em>JASA</em>, <em>120</em>(550), 698-709. (<a href='https://doi.org/10.1080/01621459.2024.2441519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Does having firearms in the home increase suicide risk? To test this hypothesis, a matched case-control study can be performed, in which suicide case subjects are compared to living controls who are similar in observed covariates in terms of their retrospective exposure to firearms at home. In this application, cases can be defined using a broad case definition (suicide) or a narrow case definition (suicide occurred at home). The broad case definition offers a larger number of cases, but the narrow case definition may offer a larger effect size, which can reduce sensitivity to bias from unmeasured confounding. However, when the goal is to test whether there is a treatment effect based on the broad case definition, restricting to the narrow case definition may introduce selection bias (i.e., bias due to selecting samples based on characteristics affected by the treatment) because exposure to firearms in the home may affect the location of suicide and thus the type of a case a subject is. We propose a new sensitivity analysis framework for combining broad and narrow case definitions in matched case-control studies, that considers the unmeasured confounding bias and selection bias simultaneously. We develop a valid randomization-based testing procedure using only the narrow case matched sets when the effect of the unmeasured confounder on receiving treatment and the effect of the treatment on case definition among the always-cases are controlled by sensitivity parameters. We then use the Bonferroni method to combine the testing procedures using the broad and narrow case definitions. With the proposed methods, we find robust evidence that having firearms at home increases suicide risk. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ting Ye and Kan Chen and Dylan Small},
  doi          = {10.1080/01621459.2024.2441519},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {698-709},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Combining broad and narrow case definitions in matched case-control studies: Firearms in the home and suicide risk},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring causal effect of a digital communication strategy under a latent sequential ignorability assumption and treatment noncompliance. <em>JASA</em>, <em>120</em>(550), 685-697. (<a href='https://doi.org/10.1080/01621459.2024.2435655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizations are increasingly relying on digital communications, such as targeted e-mails and mobile notifications, to engage with their audiences. Despite the evident advantages like cost-effectiveness and customization, assessing the effectiveness of such communications from observational data poses various statistical challenges. An immediate challenge is to adjust for targeting rules used in these communications. When digital communications involve a sequence of e-mails or notifications, however, further adjustments are required to correct for selection bias arising from previous communications influencing the subsequent ones and to deal with noncompliance issues, for example, not opening the e-mail. This article addresses these challenges in a study of promotional e-mail sequences sent by a U.S. retailer. We use a Bayesian methodology for causal inference from longitudinal data, considering targeting, noncompliance, and sequential confounding with unmeasured variables. The methodology serves three objectives: to evaluate the average treatment effect of any deterministic e-mailing strategy, to compare the effectiveness of these strategies across varying compliance behaviors, and to infer optimal strategies for distinct customer segments. Our analysis finds, among other things, that certain promotional e-mails effectively maintain engagement among individuals who have regularly received such incentives, and individuals who consistently open their e-mails exhibit reduced sensitivity to promotional content. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yuki Ohnishi and Bikram Karmakar and Wreetabrata Kar},
  doi          = {10.1080/01621459.2024.2435655},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {685-697},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inferring causal effect of a digital communication strategy under a latent sequential ignorability assumption and treatment noncompliance},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Immune profiling among colorectal cancer subtypes using dependent mixture models. <em>JASA</em>, <em>120</em>(550), 671-684. (<a href='https://doi.org/10.1080/01621459.2024.2427936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparison of transcriptomic data across different conditions is of interest in many biomedical studies. In this article, we consider comparative immune cell profiling for early-onset (EO) versus late-onset (LO) colorectal cancer (CRC). EOCRC, diagnosed between ages 18‚Äì45, is a rising public health concern that needs to be urgently addressed. However, its etiology remains poorly understood. We work toward filling this gap by identifying homogeneous T cell sub-populations that show significantly distinct characteristics across the two tumor types, and identifying others that are shared between EOCRC and LOCRC. We develop dependent finite mixture models where immune subtypes enriched under a specific condition are characterized by terms in the mixture model with common atoms but distinct weights across conditions, whereas common subtypes are characterized by sharing both atoms and relative weights. The proposed model facilitates the desired comparison across conditions by introducing highly structured multi-layer Dirichlet priors. We illustrate inference with simulation studies and data examples. Results identify EO- and LO-enriched T cells subtypes whose biomarkers are found to be linked to mechanisms of tumor progression, and potentially motivate insights into treatment of CRC. Code implementing the proposed method is available at: https://github.com/YunshanDYS/SASCcode . Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yunshan Duan and Shuai Guo and Wenyi Wang and Peter M√ºller},
  doi          = {10.1080/01621459.2024.2427936},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {671-684},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Immune profiling among colorectal cancer subtypes using dependent mixture models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking retrospective prevalent information in EHRs‚ÄîA revisit to the pairwise pseudolikelihood. <em>JASA</em>, <em>120</em>(550), 658-670. (<a href='https://doi.org/10.1080/01621459.2024.2427431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records offer abundant data on various diseases and health conditions, enabling researchers to explore the relationship between disease onset age and underlying risk factors. Unlike mortality data, the event of interest is nonterminal, hence, individuals can retrospectively report their disease-onset-age upon recruitment to the study. These individuals, diagnosed with the disease before entering the study, are termed ‚Äúprevalent.‚Äù The ascertainment imposes a left truncation condition, also known as a ‚Äúdelayed entry,‚Äù because individuals had to survive a certain period before being eligible for enrollment. The standard method to accommodate delayed entry conditions on the entire history up to recruitment, hence, the retrospective prevalent failure times are conditioned upon and cannot participate in estimating the disease-onset-age distribution. Other methods that condition on less information and allow the incorporation of the prevalent observations either bring about numerical and computational difficulties or require statistical assumptions that are violated by most biobanks. This work presents a novel estimator of the coefficients in a regression model for the age-at-onset, successfully using the prevalent data. Asymptotic results are provided, and simulations are conducted to showcase the substantial efficiency gain. In particular, the method is highly useful in leveraging large-scale repositories for replication analysis of genetic variants. Indeed, analysis of urinary bladder cancer data reveals that the proposed approach yields about twice as many replicated discoveries compared to the popular approach. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Nir Keret and Malka Gorfine},
  doi          = {10.1080/01621459.2024.2427431},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {658-670},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Unlocking retrospective prevalent information in EHRs‚ÄîA revisit to the pairwise pseudolikelihood},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal modeling for record-breaking temperature events in spain. <em>JASA</em>, <em>120</em>(550), 645-657. (<a href='https://doi.org/10.1080/01621459.2024.2427430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Record-breaking temperature events are now very frequently in the news, viewed as evidence of climate change. With this as motivation, we undertake the first substantial spatial modeling investigation of temperature record-breaking across years for any given day within the year. We work with a dataset consisting of over 60 years (1960‚Äì2021) of daily maximum temperatures across peninsular Spain. Formal statistical analysis of record-breaking events is an area that has received attention primarily within the probability community, dominated by results for the stationary record-breaking setting with some additional work addressing trends. Such effort is inadequate for analyzing actual record-breaking data. Resulting from novel and detailed exploratory data analysis, we propose rich hierarchical conditional modeling of the indicator events which define record-breaking sequences. After suitable model selection, we discover explicit trend behavior, necessary autoregression, significance of distance to the coast, useful interactions, helpful spatial random effects, and very strong daily random effects. Illustratively, the model estimates that global warming trends have increased the number of records expected in the past decade almost 2-fold, 1.93 ( 1.89 , 1.98 ) , but also estimates highly differentiated climate warming rates in space and by season. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jorge Castillo-Mateo and Alan E. Gelfand and Zeus Gracia-Tabuenca and Jes√∫s As√≠n and Ana C. Cebri√°n},
  doi          = {10.1080/01621459.2024.2427430},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {645-657},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spatio-temporal modeling for record-breaking temperature events in spain},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ‚Ñì1-based bayesian ideal point model for multidimensional politics. <em>JASA</em>, <em>120</em>(550), 631-644. (<a href='https://doi.org/10.1080/01621459.2024.2425461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ideal point estimation methods in the social sciences lack a principled approach for identifying multidimensional ideal points. We present a novel method for estimating multidimensional ideal points based on l 1 distance. In the Bayesian framework, the use of l 1 distance transforms the invariance problem of infinite rotational turns into the signed perpendicular problem, yielding posterior estimates that contract around a small area. Our simulation shows that the proposed method successfully recovers planted multidimensional ideal points in a variety of settings including non-partisan, two-party, and multi-party systems. The proposed method is applied to the analysis of roll call data from the United States House of Representatives during the late Gilded Age (1891‚Äì1899) when legislative coalitions were distinguished not only by partisan divisions but also by sectional divisions that ran across party lines. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Sooahn Shin and Johan Lim and Jong Hee Park},
  doi          = {10.1080/01621459.2024.2425461},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {631-644},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {‚Ñì1-based bayesian ideal point model for multidimensional politics},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A physics-informed, deep double reservoir network for forecasting boundary layer velocity. <em>JASA</em>, <em>120</em>(550), 618-630. (<a href='https://doi.org/10.1080/01621459.2024.2422131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a fluid flows over a solid surface, it creates a thin boundary layer where the flow velocity is influenced by the surface through viscosity, and can transition from laminar to turbulent at sufficiently high speeds. Understanding and forecasting the fluid dynamics under these conditions is one of the most challenging scientific problems in fluid dynamics. It is therefore of high interest to formulate models able to capture the nonlinear spatio-temporal velocity structure as well as produce forecasts in a computationally efficient manner. Traditional statistical approaches are limited in their ability to produce timely forecasts of complex, nonlinear spatio-temporal structures which are at the same time able to incorporate the underlying flow physics. In this work, we propose a model to accurately forecast boundary layer velocities with a deep double reservoir computing network which is capable of capturing the complex, nonlinear dynamics of the boundary layer while at the same time incorporating physical constraints via a penalty obtained by a Partial Differential Equation (PDE). Simulation studies on a one-dimensional viscous fluid demonstrate how the proposed model is able to produce accurate forecasts while simultaneously accounting for energy loss. The application focuses on boundary layer data in a water tunnel with a PDE penalty derived from an appropriate simplification of the Navier-Stokes equations, showing improved forecasting by the proposed approach in terms of mass conservation and variability of velocity fluctuation against non-physics-informed methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Matthew Bonas and David H. Richter and Stefano Castruccio},
  doi          = {10.1080/01621459.2024.2422131},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {618-630},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A physics-informed, deep double reservoir network for forecasting boundary layer velocity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing a large number of composite null hypotheses using conditionally symmetric multidimensional gaussian mixtures in genome-wide studies. <em>JASA</em>, <em>120</em>(550), 605-617. (<a href='https://doi.org/10.1080/01621459.2024.2422124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation, pleiotropy, and replication analyses are three highly popular genetic study designs. Although these analyses address different scientific questions, the underlying statistical inference problems all involve large-scale testing of composite null hypotheses. The goal is to determine whether all null hypotheses‚Äîas opposed to at least one‚Äîin a set of individual tests should simultaneously be rejected. Recently, various methods have been proposed for each of these situations, including an appealing two-group empirical Bayes approach that calculates local false discovery rates (lfdr). However, lfdr estimation is difficult due to the need for multivariate density estimation. Furthermore, the multiple testing rules for the empirical Bayes lfdr approach can disagree with conventional frequentist z-statistics, which is troubling for a field that ubiquitously uses summary statistics. This work proposes a framework to unify two-group testing in genetic association composite null settings, the conditionally symmetric multidimensional Gaussian mixture model (csmGmm). The csmGmm is shown to demonstrate more robust operating characteristics than recently-proposed alternatives. Crucially, the csmGmm also offers interpretability guarantees by harmonizing lfdr and z-statistic testing rules. We extend the base csmGmm to cover each of the mediation, pleiotropy, and replication settings, and we prove that the lfdr z-statistic agreement holds in each situation. We apply the model to a collection of translational lung cancer genetic association studies that motivated this work. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ryan Sun and Zachary R. McCaw and Xihong Lin},
  doi          = {10.1080/01621459.2024.2422124},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {605-617},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing a large number of composite null hypotheses using conditionally symmetric multidimensional gaussian mixtures in genome-wide studies},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Space-time extremes of severe U.S. thunderstorm environments. <em>JASA</em>, <em>120</em>(550), 591-604. (<a href='https://doi.org/10.1080/01621459.2024.2421582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Severe thunderstorms cause substantial economic and human losses in the United States. Simultaneous high values of convective available potential energy (CAPE) and storm relative helicity (SRH) are favorable to severe weather, and both they and the composite variable PROD = CAPE √ó SRH can be used as indicators of severe thunderstorm activity. Their extremal spatial dependence exhibits temporal non-stationarity due to seasonality and large-scale atmospheric signals such as El Ni√±o-Southern Oscillation (ENSO). In order to investigate this, we introduce a space-time model based on a max-stable, Brown‚ÄìResnick, field whose range depends on ENSO and on time through a tensor product spline. We also propose a max-stability test based on empirical likelihood and the bootstrap. The marginal and dependence parameters must be estimated separately owing to the complexity of the model, and we develop a bootstrap-based model selection criterion that accounts for the marginal uncertainty when choosing the dependence model. In the case study, the out-sample performance of our model is good. We find that extremes of PROD, CAPE, and SRH are generally more localized in summer and, in some regions, less localized during El Ni√±o and La Ni√±a events, and give meteorological interpretations of these phenomena. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jonathan Koh and Erwan Koch and Anthony C. Davison},
  doi          = {10.1080/01621459.2024.2421582},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {591-604},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Space-time extremes of severe U.S. thunderstorm environments},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comment on ‚ÄúData fission: Splitting a single data point‚Äù by james leiner, boyan duan, larry wasserman and aaditya ramdas. <em>JASA</em>, <em>120</em>(549), Views 20. (<a href='https://doi.org/10.1080/01621459.2024.2403728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {V. Roshan Joseph},
  doi          = {10.1080/01621459.2024.2403728},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {Views 20},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on ‚ÄúData fission: Splitting a single data point‚Äù by james leiner, boyan duan, larry wasserman and aaditya ramdas},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional data analysis with r. <em>JASA</em>, <em>120</em>(549), 588-590. (<a href='https://doi.org/10.1080/01621459.2024.2425462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Piotr S. Kokoszka},
  doi          = {10.1080/01621459.2024.2425462},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {588-590},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Functional data analysis with r},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probability modeling and statistical inference in cancer screening. <em>JASA</em>, <em>120</em>(549), 587-588. (<a href='https://doi.org/10.1080/01621459.2024.2425460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Li C. Cheung},
  doi          = {10.1080/01621459.2024.2425460},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {587-588},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Probability modeling and statistical inference in cancer screening},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian nonparametrics for causal inference and missing data. <em>JASA</em>, <em>120</em>(549), 586-587. (<a href='https://doi.org/10.1080/01621459.2024.2423435'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {P. Richard Hahn},
  doi          = {10.1080/01621459.2024.2423435},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {586-587},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian nonparametrics for causal inference and missing data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROC analysis for classification and prediction in practice. <em>JASA</em>, <em>120</em>(549), 585-586. (<a href='https://doi.org/10.1080/01621459.2024.2423434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Mauricio Tec},
  doi          = {10.1080/01621459.2024.2423434},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {585-586},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {ROC analysis for classification and prediction in practice},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based machine learning. <em>JASA</em>, <em>120</em>(549), 584-585. (<a href='https://doi.org/10.1080/01621459.2024.2411074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Emanuela Furfaro},
  doi          = {10.1080/01621459.2024.2411074},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {584-585},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-based machine learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Operationalizing legislative bodies: A methodological and empirical perspective with a bayesian approach. <em>JASA</em>, <em>120</em>(549), 572-583. (<a href='https://doi.org/10.1080/01621459.2024.2413928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article extensively reviews applications, extensions, and models derived from the Bayesian ideal point estimator. We focus our attention on studies conducted in the United States as well as Latin America. First, we provide a detailed description of the Bayesian ideal point estimator. Next, we propose a new taxonomy to synthesize and frame technical developments and applications associated with the estimator in the context of the United States Congress and Latin American governing bodies. The literature available in Latin America allows us to conclude that few legislatures in the region have been analyzed using the methodology under discussion. Also, we highlight those parliaments of Latin America embedded in democratic presidential systems as novel scenarios for operationalizing the electoral behavior of legislative bodies through nominal voting data. Our findings show some alternatives for future research.},
  archive      = {J_JASA},
  author       = {Carolina Luque and Juan Sosa},
  doi          = {10.1080/01621459.2024.2413928},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {572-583},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Operationalizing legislative bodies: A methodological and empirical perspective with a bayesian approach},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution-free prediction intervals under covariate shift, with an application to causal inference. <em>JASA</em>, <em>120</em>(549), 559-571. (<a href='https://doi.org/10.1080/01621459.2024.2356886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to its appealing distribution-free feature, conformal inference has become a popular tool for constructing prediction intervals with a desired coverage rate. In scenarios involving covariate shift, where the shift function needs to be estimated from data, many existing methods resort to data-splitting techniques. However, these approaches often lead to wider intervals and less reliable coverage rates, especially when dealing with finite sample sizes. To address these challenges, we propose methods based on a pivotal quantity derived under a parametric working model and employ a resampling-based framework to approximate its distribution. The resampling-based approach can produce prediction intervals with a desired coverage rate without splitting the data and can be easily applied to causal inference settings where a shift in the covariate distribution can occur between treatment and control arms. Additionally, the proposed approaches enjoy a double robustness property and are adaptable to different prediction tasks. Our extensive numerical experiments demonstrate that, compared to existing methods, the proposed novel approaches can produce substantially shorter conformal prediction intervals with lower variability in the interval lengths while maintaining promising coverage rates and advantages in versatile usage. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jing Qin and Yukun Liu and Moming Li and Chiung-Yu Huang},
  doi          = {10.1080/01621459.2024.2356886},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {559-571},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distribution-free prediction intervals under covariate shift, with an application to causal inference},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rational kriging. <em>JASA</em>, <em>120</em>(549), 548-558. (<a href='https://doi.org/10.1080/01621459.2024.2356296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new kriging that has a rational form. It is shown that the generalized least squares estimator of the mean from rational kriging is much more well behaved than that of ordinary kriging. Parameter estimation and uncertainty quantification for rational kriging are proposed using a Gaussian process framework. A generalized version of rational kriging is also proposed, which includes ordinary and rational kriging as special cases. Extensive simulations carried out over a wide class of functions show that the generalized rational kriging performs on par or better than both ordinary and rational kriging in terms of prediction and uncertainty quantification. The only extra step needed for generalized rational kriging over ordinary kriging is the computation of Perron eigenvector of an augmented correlation matrix which can be computed in near linear time and therefore, its overall computational complexity is no more than that of ordinary kriging. The potential applications of the new kriging methods in the emulation of computationally expensive models and model calibration problems are illustrated with real and simulated examples. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {V. Roshan Joseph},
  doi          = {10.1080/01621459.2024.2356296},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {548-558},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rational kriging},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural networks for geospatial data. <em>JASA</em>, <em>120</em>(549), 535-547. (<a href='https://doi.org/10.1080/01621459.2024.2356293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of geospatial data has traditionally been model-based, with a mean model, customarily specified as a linear regression on the covariates, and a Gaussian process covariance model, encoding the spatial dependence. While nonlinear machine learning algorithms like neural networks are increasingly being used for spatial analysis, current approaches depart from the model-based setup and cannot explicitly incorporate spatial covariance. We propose NN-GLS , embedding neural networks directly within the traditional Gaussian process (GP) geostatistical model to accommodate nonlinear mean functions while retaining all other advantages of GP, like explicit modeling of the spatial covariance and predicting at new locations via kriging. In NN-GLS, estimation of the neural network parameters for the nonlinear mean of the Gaussian Process explicitly accounts for the spatial covariance through use of the generalized least squares (GLS) loss, thus, extending the linear case. We show that NN-GLS admits a representation as a special type of graph neural network (GNN). This connection facilitates the use of standard neural network computational techniques for irregular geospatial data, enabling novel and scalable mini-batching, backpropagation, and kriging schemes. We provide methodology to obtain uncertainty bounds for estimation and predictions from NN-GLS. Theoretically, we show that NN-GLS will be consistent for irregularly observed spatially correlated data processes. We also provide a finite sample concentration rate, which quantifies the need to accurately model the spatial covariance in neural networks for dependent data. To our knowledge, these are the first large-sample results for any neural network algorithm for irregular spatial data. We demonstrate the methodology through numerous simulations and an application to air pollution modeling. We develop a software implementation of NN-GLS in the Python package geospaNN. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Wentao Zhan and Abhirup Datta},
  doi          = {10.1080/01621459.2024.2356293},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {535-547},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Neural networks for geospatial data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust augmented model accuracy transfer inference with high dimensional features. <em>JASA</em>, <em>120</em>(549), 524-534. (<a href='https://doi.org/10.1080/01621459.2024.2356291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is crucial for training models that generalize to unlabeled target populations using labeled source data, especially in real-world studies where label scarcity and covariate shift are common. While most research focuses on model estimation, there is limited literature on transfer inference for model accuracy despite its importance. We introduce a novel Doubly Robust Augmented Model Accuracy Transfer Inferen Ce (DRAMATIC) method for point and interval estimation of commonly used classification performance measures in an unlabeled target population with labeled source data. DRAMATIC derives and evaluates a potentially misspecified risk model for a binary response, leveraging high-dimensional adjustment features from both source and target data. It builds on an imputation model for the response mean and a density ratio model to characterize distributional shifts. The method constructs doubly robust estimators that are valid when either model is correctly specified and certain sparsity assumptions hold. Simulations show negligible bias in point estimation and satisfactory empirical coverage levels in confidence intervals. The utility of DRAMATIC is illustrated by transferring a genetic risk prediction model and its accuracy evaluation for type II diabetes across two patient cohorts in Mass General Brigham (MGB). Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Doudou Zhou and Molei Liu and Mengyan Li and Tianxi Cai},
  doi          = {10.1080/01621459.2024.2356291},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {524-534},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Doubly robust augmented model accuracy transfer inference with high dimensional features},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized data thinning using sufficient statistics. <em>JASA</em>, <em>120</em>(549), 511-523. (<a href='https://doi.org/10.1080/01621459.2024.2353948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our goal is to develop a general strategy to decompose a random variable X into multiple independent random variables, without sacrificing any information about unknown parameters. A recent paper showed that for some well-known natural exponential families, X can be thinned into independent random variables X ( 1 ) , ‚Ä¶ , X ( K ) , such that X = ‚àë k = 1 K X ( k ) . These independent random variables can then be used for various model validation and inference tasks, including in contexts where traditional sample splitting fails. In this article, we generalize their procedure by relaxing this summation requirement and simply asking that some known function of the independent random variables exactly reconstruct X . This generalization of the procedure serves two purposes. First, it greatly expands the families of distributions for which thinning can be performed. Second, it unifies sample splitting and data thinning, which on the surface seem to be very different, as applications of the same principle. This shared principle is sufficiency. We use this insight to perform generalized thinning operations for a diverse set of families. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ameer Dharamshi and Anna Neufeld and Keshav Motwani and Lucy L. Gao and Daniela Witten and Jacob Bien},
  doi          = {10.1080/01621459.2024.2353948},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {511-523},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Generalized data thinning using sufficient statistics},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomness of shapes and statistical inference on shapes via the smooth euler characteristic transform. <em>JASA</em>, <em>120</em>(549), 498-510. (<a href='https://doi.org/10.1080/01621459.2024.2353947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we establish the mathematical foundations for modeling the randomness of shapes and conducting statistical inference on shapes using the smooth Euler characteristic transform. Based on these foundations, we propose two Chi-squared statistic-based algorithms for testing hypotheses on random shapes. Simulation studies are presented to validate our mathematical derivations and to compare our algorithms with state-of-the-art methods to demonstrate the utility of our proposed framework. As real applications, we analyze a dataset of mandibular molars from four genera of primates and show that our algorithms have the power to detect significant shape differences that recapitulate known morphological variation across suborders. Altogether, our discussions bridge the following fields: algebraic and computational topology, probability theory and stochastic processes, Sobolev spaces and functional analysis, analysis of variance for functional data, and geometric morphometrics. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Kun Meng and Jinyu Wang and Lorin Crawford and Ani Eloyan},
  doi          = {10.1080/01621459.2024.2353947},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {498-510},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Randomness of shapes and statistical inference on shapes via the smooth euler characteristic transform},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-aligned random partition model (GARP). <em>JASA</em>, <em>120</em>(549), 486-497. (<a href='https://doi.org/10.1080/01621459.2024.2353943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian nonparametric mixtures and random partition models are powerful tools for probabilistic clustering. However, standard independent mixture models can be restrictive in some applications such as inference on cell lineage due to the biological relations of the clusters. The increasing availability of large genomic data requires new statistical tools to perform model-based clustering and infer the relationship between homogeneous subgroups of units. Motivated by single-cell RNA data we develop a novel dependent mixture model to jointly perform cluster analysis and align the clusters on a graph. Our flexible graph-aligned random partition model (GARP) exploits Gibbs-type priors as building blocks, allowing us to derive analytical results for the probability mass function (pmf) on the graph-aligned random partition. We derive a generalization of the Chinese restaurant process from the pmf and a related efficient and neat MCMC algorithm to implement Bayesian inference. We illustrate posterior inference under the GARP using single-cell RNA-seq data from mice stem cells. We further investigate the performance of the model in recovering the underlying clustering structure as well as the underlying graph by means of simulation studies. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Giovanni Rebaudo and Peter M√ºller},
  doi          = {10.1080/01621459.2024.2353943},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {486-497},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Graph-aligned random partition model (GARP)},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tests for large-dimensional shape matrices via tyler‚Äôs m estimators. <em>JASA</em>, <em>120</em>(549), 472-485. (<a href='https://doi.org/10.1080/01621459.2024.2350573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tyler‚Äôs M estimator, as a robust alternative to the sample covariance matrix, has been widely applied in robust statistics. However, classical theory on Tyler‚Äôs M estimator is mainly developed in the low-dimensional regime for elliptical populations. It remains largely unknown when the parameter of dimension p grows proportionally to the sample size n for general populations. By using the eigenvalues of Tyler‚Äôs M estimator, this article develops tests for the identity and equality of shape matrices in a large-dimensional framework where the dimension-to-sample size ratio p / n has a limit in (0, 1). The proposed tests can be applied to a broad class of multivariate distributions including the family of elliptical distributions (see model (2.1) for details). To analyze both the null and alternative distributions of the proposed tests, we provide a unified theory on the spectrum of a large-dimensional Tyler‚Äôs M estimator when the underlying population is general. Simulation results demonstrate good performance and robustness of our tests. An empirical analysis of the Fama-French 49 industrial portfolios is carried out to demonstrate the shape of the portfolios varying. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Runze Li and Weiming Li and Qinwen Wang},
  doi          = {10.1080/01621459.2024.2350573},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {472-485},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Tests for large-dimensional shape matrices via tyler‚Äôs m estimators},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controlled discovery and localization of signals via bayesian linear programming. <em>JASA</em>, <em>120</em>(549), 460-471. (<a href='https://doi.org/10.1080/01621459.2024.2347667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientists often must simultaneously localize and discover signals. For instance, in genetic fine-mapping, high correlations between nearby genetic variants make it hard to identify the exact locations of causal variants. So the statistical task is to output as many disjoint regions containing a signal as possible, each as small as possible, while controlling false positives. Similar problems arise, for example, when locating stars in astronomical surveys and in changepoint detection. Common Bayesian approaches to these problems involve computing a posterior distribution over signal locations. However, existing procedures to translate these posteriors into credible regions for the signals fail to capture all the information in the posterior, leading to lower power and (sometimes) inflated false discoveries. We introduce Bayesian Linear Programming (BLiP), which can efficiently convert any posterior distribution over signals into credible regions for signals. BLiP overcomes an extremely high-dimensional and nonconvex problem to verifiably nearly maximize expected power while controlling false positives. Applying BLiP to existing state-of-the-art analyses of UK Biobank data (for genetic fine-mapping) and the Sloan Digital Sky Survey (for astronomical point source detection) increased power by 30%‚Äì120% in just a few minutes of additional computation. BLiP is implemented in pyblip (Python) and blipr (R). Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Asher Spector and Lucas Janson},
  doi          = {10.1080/01621459.2024.2347667},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {460-471},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Controlled discovery and localization of signals via bayesian linear programming},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing the number of common factors by bootstrapped sample covariance matrix in high-dimensional factor models. <em>JASA</em>, <em>120</em>(549), 448-459. (<a href='https://doi.org/10.1080/01621459.2024.2346364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the impact of bootstrap procedure on the eigenvalue distributions of the sample covariance matrix under a high-dimensional factor structure. We provide asymptotic distributions for the top eigenvalues of bootstrapped sample covariance matrix under mild conditions. After bootstrap, the spiked eigenvalues which are driven by common factors will converge weakly to Gaussian limits after proper scaling and centralization. However, the largest non-spiked eigenvalue is mainly determined by the order statistics of the bootstrap resampling weights, and follows extreme value distribution. Based on the disparate behavior of the spiked and non-spiked eigenvalues, we propose innovative methods to test the number of common factors. Indicated by extensive numerical and empirical studies, the proposed methods perform reliably and convincingly under the existence of both weak factors and cross-sectionally correlated errors. Our technical details contribute to random matrix theory on spiked covariance model with convexly decaying density and unbounded support, or with general elliptical distributions. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Long Yu and Peng Zhao and Wang Zhou},
  doi          = {10.1080/01621459.2024.2346364},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {448-459},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing the number of common factors by bootstrapped sample covariance matrix in high-dimensional factor models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selection and aggregation of conformal prediction sets. <em>JASA</em>, <em>120</em>(549), 435-447. (<a href='https://doi.org/10.1080/01621459.2024.2344700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction is a generic methodology for finite-sample valid distribution-free prediction. This technique has garnered a lot of attention in the literature partly because it can be applied with any machine learning algorithm that provides point predictions to yield valid prediction regions. Of course, the efficiency (width/volume) of the resulting prediction region depends on the performance of the machine learning algorithm. In the context of point prediction, several techniques (such as cross-validation) exist to select one of many machine learning algorithms for better performance. In contrast, such selection techniques are seldom discussed in the context of set prediction (or prediction regions). In this article, we consider the problem of obtaining the smallest conformal prediction region given a family of machine learning algorithms. We provide two general-purpose selection algorithms and consider coverage as well as width properties of the final prediction region. The first selection method yields the smallest width prediction region among the family of conformal prediction regions for all sample sizes but only has an approximate coverage guarantee. The second selection method has a finite sample coverage guarantee but only attains close to the smallest width. The approximate optimal width property of the second method is quantified via an oracle inequality. As an illustration, we consider the use of aggregation of nonparametric regression estimators in the split conformal method with the absolute residual conformal score. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yachong Yang and Arun Kumar Kuchibhotla},
  doi          = {10.1080/01621459.2024.2344700},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {435-447},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Selection and aggregation of conformal prediction sets},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling and learning on high-dimensional matrix-variate sequences. <em>JASA</em>, <em>120</em>(549), 419-434. (<a href='https://doi.org/10.1080/01621459.2024.2344687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new matrix factor model, named RaDFaM, which is strictly derived from the general rank decomposition and assumes a high-dimensional vector factor model structure for each basis vector. RaDFaM contributes a novel class of low-rank latent structures that trade off between signal intensity and dimension reduction from a tensor subspace perspective. Based on the intrinsic separable covariance structure of RaDFaM, for a collection of matrix-valued observations, we derive a new class of PCA variants for estimating loading matrices, and sequentially the latent factor matrices. The peak signal-to-noise ratio of RaDFaM is proved to be superior in the category of PCA-type estimators. We also establish an asymptotic theory including the consistency, convergence rates, and asymptotic distributions for components in the signal part. Numerically, we demonstrate the performance of RaDFaM in applications such as matrix reconstruction, supervised learning, and clustering, on uncorrelated and correlated data, respectively. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xu Zhang and Catherine C. Liu and Jianhua Guo and K. C. Yuen and A. H. Welsh},
  doi          = {10.1080/01621459.2024.2344687},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {419-434},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling and learning on high-dimensional matrix-variate sequences},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sobolev calibration of imperfect computer models. <em>JASA</em>, <em>120</em>(549), 408-418. (<a href='https://doi.org/10.1080/01621459.2024.2340793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibration refers to the statistical estimation of unknown model parameters in computer experiments, such that computer experiments can match underlying physical systems. This work develops a new calibration method for imperfect computer models, Sobolev calibration, which can rule out calibration parameters that generate overfitting calibrated functions. We prove that the Sobolev calibration enjoys desired theoretical properties including fast convergence rate, asymptotic normality and semiparametric efficiency. We also demonstrate an interesting property that the Sobolev calibration can bridge the gap between two influential methods: L 2 calibration and Kennedy and O‚ÄôHagan‚Äôs calibration. In addition to exploring the deterministic physical experiments, we theoretically justify that our method can transfer to the case when the physical process is indeed a Gaussian process, which follows the original idea of Kennedy and O‚ÄôHagan‚Äôs. Numerical simulations as well as a real-world example illustrate the competitive performance of the proposed method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Qingwen Zhang and Wenjia Wang},
  doi          = {10.1080/01621459.2024.2340793},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {408-418},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sobolev calibration of imperfect computer models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sharp-SSL: Selective high-dimensional axis-aligned random projections for semi-supervised learning. <em>JASA</em>, <em>120</em>(549), 395-407. (<a href='https://doi.org/10.1080/01621459.2024.2340792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for high-dimensional semi-supervised learning problems based on the careful aggregation of the results of a low-dimensional procedure applied to many axis-aligned random projections of the data. Our primary goal is to identify important variables for distinguishing between the classes; existing low-dimensional methods can then be applied for final class assignment. To this end, we score projections according to their class-distinguishing ability; for instance, motivated by a generalized Rayleigh quotient, we can compute the traces of estimated whitened between-class covariance matrices on the projected data. This enables us to assign an importance weight to each variable for a given projection, and to select our signal variables by aggregating these weights over high-scoring projections. Our theory shows that the resulting Sharp-SSL algorithm is able to recover the signal coordinates with high probability when we aggregate over sufficiently many random projections and when the base procedure estimates the diagonal entries of the whitened between-class covariance matrix sufficiently well. For the Gaussian EM base procedure, we provide a new analysis of its performance in semi-supervised settings that controls the parameter estimation error in terms of the proportion of labeled data in the sample. Numerical results on both simulated data and a real colon tumor dataset support the excellent empirical performance of the method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Tengyao Wang and Edgar Dobriban and Milana Gataric and Richard J. Samworth},
  doi          = {10.1080/01621459.2024.2340792},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {395-407},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sharp-SSL: Selective high-dimensional axis-aligned random projections for semi-supervised learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient nonparametric estimation of stochastic policy effects with clustered interference. <em>JASA</em>, <em>120</em>(549), 382-394. (<a href='https://doi.org/10.1080/01621459.2024.2340789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interference occurs when a unit‚Äôs treatment (or exposure) affects another unit‚Äôs outcome. In some settings, units may be grouped into clusters such that it is reasonable to assume that interference, if present, only occurs between individuals in the same cluster, that is, there is clustered interference. Various causal estimands have been proposed to quantify treatment effects under clustered interference from observational data, but these estimands either entail treatment policies lacking real-world relevance or are based on parametric propensity score models. Here, we propose new causal estimands based on modification of the propensity score distribution which may be more relevant in many contexts and are not based on parametric models. Nonparametric sample splitting estimators of the new estimands are constructed, which allow for flexible data-adaptive estimation of nuisance functions and are consistent, asymptotically normal, and efficient, converging at the usual parametric rate. Simulations show the finite sample performance of the proposed estimators. The proposed methods are applied to evaluate the effect of water, sanitation, and hygiene facilities on diarrhea among children in Senegal. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Chanhwa Lee and Donglin Zeng and Michael G. Hudgens},
  doi          = {10.1080/01621459.2024.2340789},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {382-394},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficient nonparametric estimation of stochastic policy effects with clustered interference},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring independent sets of gaussian variables after thresholding correlations. <em>JASA</em>, <em>120</em>(549), 370-381. (<a href='https://doi.org/10.1080/01621459.2024.2337158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider testing whether a set of Gaussian variables, selected from the data, is independent of the remaining variables. This set is selected via a very simple approach: these are the variables for which the correlation with all other variables falls below some threshold. Unlike other settings in selective inference, failure to account for the selection step leads to excessively conservative (as opposed to anti-conservative) results. We propose a new test that conditions on the event that the selection resulted in the set of variables in question, and thus is not overly conservative. To achieve computational tractability, we develop a characterization of the conditioning event in terms of the canonical correlation between groups of random variables. In simulation studies and in the analysis of gene co-expression networks, we show that our approach has much higher power than a ‚Äúnaive‚Äù approach that ignores the effect of selection. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Arkajyoti Saha and Daniela Witten and Jacob Bien},
  doi          = {10.1080/01621459.2024.2337158},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {370-381},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inferring independent sets of gaussian variables after thresholding correlations},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized bayesian additive regression trees models: Beyond conditional conjugacy. <em>JASA</em>, <em>120</em>(549), 356-369. (<a href='https://doi.org/10.1080/01621459.2024.2337156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian additive regression trees have seen increased interest in recent years due to their ability to combine machine learning techniques with principled uncertainty quantification. The Bayesian backfitting algorithm used to fit BART models, however, limits their application to a small class of models for which conditional conjugacy exists. In this article, we greatly expand the domain of applicability of BART to arbitrary generalized BART models by introducing a very simple, tuning-parameter-free, reversible jump Markov chain Monte Carlo algorithm. Our algorithm requires only that the user be able to compute the likelihood and (optionally) its gradient and Fisher information. The potential applications are very broad; we consider examples in survival analysis, structured heteroscedastic regression, and gamma shape regression. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Antonio R. Linero},
  doi          = {10.1080/01621459.2024.2337156},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {356-369},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Generalized bayesian additive regression trees models: Beyond conditional conjugacy},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for noisy matrix completion incorporating auxiliary information. <em>JASA</em>, <em>120</em>(549), 343-355. (<a href='https://doi.org/10.1080/01621459.2024.2335591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates statistical inference for noisy matrix completion in a semi-supervised model when auxiliary covariates are available. The model consists of two parts. One part is a low-rank matrix induced by unobserved latent factors; the other part models the effects of the observed covariates through a coefficient matrix which is composed of high-dimensional column vectors. We model the observational pattern of the responses through a logistic regression of the covariates, and allow its probability to go to zero as the sample size increases. We apply an iterative least squares (LS) estimation approach in our considered context. The iterative LS methods in general enjoy a low computational cost, but deriving the statistical properties of the resulting estimators is a challenging task. We show that our method only needs a few iterations, and the resulting entry-wise estimators of the low-rank matrix and the coefficient matrix are guaranteed to have asymptotic normal distributions. As a result, individual inference can be conducted for each entry of the unknown matrices. We also propose a simultaneous testing procedure with multiplier bootstrap for the high-dimensional coefficient matrix. This simultaneous inferential tool can help us further investigate the effects of covariates for the prediction of missing entries. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shujie Ma and Po-Yao Niu and Yichong Zhang and Yinchu Zhu},
  doi          = {10.1080/01621459.2024.2335591},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {343-355},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for noisy matrix completion incorporating auxiliary information},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly-Valid/Doubly-sharp sensitivity analysis for causal inference with unmeasured confounding. <em>JASA</em>, <em>120</em>(549), 331-342. (<a href='https://doi.org/10.1080/01621459.2024.2335588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of constructing bounds on the average treatment effect (ATE) when unmeasured confounders exist but have bounded influence. Specifically, we assume that omitted confounders could not change the odds of treatment for any unit by more than a fixed factor. We derive the sharp partial identification bounds implied by this assumption by leveraging distributionally robust optimization, and we propose estimators of these bounds with several novel robustness properties. The first is double sharpness : our estimators consistently estimate the sharp ATE bounds when one of two nuisance parameters is misspecified and achieve semiparametric efficiency when all nuisance parameters are suitably consistent. The second and more novel property is double validity : even when most nuisance parameters are misspecified, our estimators still provide valid but possibly conservative bounds for the ATE and our Wald confidence intervals remain valid even when our estimators are not asymptotically normal. As a result, our estimators provide a highly credible method for sensitivity analysis of causal inferences. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jacob Dorn and Kevin Guo and Nathan Kallus},
  doi          = {10.1080/01621459.2024.2335588},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {331-342},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Doubly-Valid/Doubly-sharp sensitivity analysis for causal inference with unmeasured confounding},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic regenerative simulation via non-reversible simulated tempering. <em>JASA</em>, <em>120</em>(549), 318-330. (<a href='https://doi.org/10.1080/01621459.2024.2335587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulated Tempering (ST) is an MCMC algorithm for complex target distributions that operates on a path between the target and a more amenable reference distribution. Crucially, if the reference enables iid sampling, ST is regenerative and can be parallelized across independent tours. However, the difficulty of tuning ST has hindered its widespread adoption. In this work, we develop a simple nonreversible ST (NRST) algorithm, a general theoretical analysis of ST, and an automated tuning procedure for ST. A core contribution that arises from the analysis is a novel performance metric‚Äî Tour Effectiveness (TE) ‚Äîthat controls the asymptotic variance of estimates from ST for bounded test functions. We use the TE to show that NRST dominates its reversible counterpart. We then develop an automated tuning procedure for NRST algorithms that targets the TE while minimizing computational cost. This procedure enables straightforward integration of NRST into existing probabilistic programming languages. We provide extensive experimental evidence that our tuning scheme improves the performance and robustness of NRST algorithms on a diverse set of probabilistic models. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Miguel Biron-Lattes and Trevor Campbell and Alexandre Bouchard-C√¥t√©},
  doi          = {10.1080/01621459.2024.2335587},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {318-330},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Automatic regenerative simulation via non-reversible simulated tempering},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CARE: Large precision matrix estimation for compositional data. <em>JASA</em>, <em>120</em>(549), 305-317. (<a href='https://doi.org/10.1080/01621459.2024.2335586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional compositional data are prevalent in many applications. The simplex constraint poses intrinsic challenges to inferring the conditional dependence relationships among the components forming a composition, as encoded by a large precision matrix. We introduce a precise specification of the compositional precision matrix and relate it to its basis counterpart, which is shown to be asymptotically identifiable under suitable sparsity assumptions. By exploiting this connection, we propose a composition adaptive regularized estimation (CARE) method for estimating the sparse basis precision matrix. We derive rates of convergence for the estimator and provide theoretical guarantees on support recovery and data-driven parameter tuning. Our theory reveals an intriguing tradeoff between identification and estimation, thereby highlighting the blessing of dimensionality in compositional data analysis. In particular, in sufficiently high dimensions, the CARE estimator achieves minimax optimality and performs as well as if the basis were observed. We further discuss how our framework can be extended to handle data containing zeros, including sampling zeros and structural zeros. The advantages of CARE over existing methods are illustrated by simulation studies and an application to inferring microbial ecological networks in the human gut. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shucong Zhang and Huiyuan Wang and Wei Lin},
  doi          = {10.1080/01621459.2024.2335586},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {305-317},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {CARE: Large precision matrix estimation for compositional data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extreme value statistics in semi-supervised models. <em>JASA</em>, <em>120</em>(549), 291-304. (<a href='https://doi.org/10.1080/01621459.2024.2333582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider extreme value analysis in a semi-supervised setting, where we observe, next to the n data on the target variable, n + m data on one or more covariates. This is called the semi-supervised model with n labeled and m unlabeled data. By exploiting the tail dependence between the target variable and the covariates, we derive estimators for the extreme value index and extreme quantiles of the target variable in this setting and establish their asymptotic behavior. Our estimators substantially improve the univariate estimators, based on only the n target variable data, in terms of asymptotic variances whereas the asymptotic biases remain unchanged. A simulation study confirms the substantially improved behavior of both estimators. Finally the estimation method is applied to rainfall data in France. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Hanan Ahmed and John H.J. Einmahl and Chen Zhou},
  doi          = {10.1080/01621459.2024.2333582},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {291-304},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Extreme value statistics in semi-supervised models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly flexible estimation under label shift. <em>JASA</em>, <em>120</em>(549), 278-290. (<a href='https://doi.org/10.1080/01621459.2024.2321653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies ranging from clinical medicine to policy research, complete data are usually available from a population ùí´ , but the quantity of interest is often sought for a related but different population ùí¨ which only has partial data. We consider the setting when both outcome Y and covariate X are available from P but only X is available from Q , under the label shift assumption; that is, the conditional distribution of X given Y is the same in the two populations. To estimate the parameter of interest in Q by leveraging information from P , three ingredients are essential: (a) the common conditional distribution of X given Y , (b) the regression model of Y given X in P , and (c) the density ratio of the outcome Y between the two populations. We propose an estimation procedure that only needs some standard nonparametric technique to approximate the conditional expectations with respect to (a), while by no means needs an estimate or model for (b) or (c); that is, doubly flexible to the model misspecifications of both (b) and (c). This is conceptually different from the well-known doubly robust estimation in that, double robustness allows at most one model to be misspecified whereas our proposal can allow both (b) and (c) to be misspecified. This is of particular interest in label shift because estimating (c) is difficult, if not impossible, by virtue of the absence of the Y -data from Q . While estimating (b) is occasionally off-the-shelf, it may encounter issues related to the curse of dimensionality or computational challenges. We develop the large sample theory for the proposed estimator, and examine its finite-sample performance through simulation studies as well as an application to the MIMIC-III database. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Seong-ho Lee and Yanyuan Ma and Jiwei Zhao},
  doi          = {10.1080/01621459.2024.2321653},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {278-290},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Doubly flexible estimation under label shift},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust personalized federated learning with sparse penalization. <em>JASA</em>, <em>120</em>(549), 266-277. (<a href='https://doi.org/10.1080/01621459.2024.2321652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging topic due to its advantage in collaborative learning with distributed data. Due to the heterogeneity in the local data-generating mechanism, it is important to consider personalization when developing federated learning methods. In this work, we propose a personalized federated learning (PFL) method to address the robust regression problem. Specifically, we aim to learn the regression weight by solving a Huber loss with the sparse fused penalty. Additionally, we designed our personalized federated learning for robust and sparse regression (PerFL-RSR) algorithm to solve the estimation problem in the federated system efficiently. Theoretically, we show that the proposed PerFL-RSR reaches a convergence rate of O ( 1 / T ) , and the proposed estimator is statistically consistent. Thorough experiments and real data analysis are conducted to corroborate the theoretical results of our proposed personalized federated learning method. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Weidong Liu and Xiaojun Mao and Xiaofei Zhang and Xin Zhang},
  doi          = {10.1080/01621459.2024.2321652},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {266-277},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust personalized federated learning with sparse penalization},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling recurrent failures on large directed networks. <em>JASA</em>, <em>120</em>(549), 251-265. (<a href='https://doi.org/10.1080/01621459.2024.2319897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many lifeline infrastructure systems consist of thousands of components configured in a complex directed network. Disruption of the infrastructure constitutes a recurrent failure process over a directed network. Statistical inference for such network recurrence data is challenging because of the large number of nodes with irregular connections among them. Motivated by 16 years of Scottish Water operation records, we propose a network Gamma-Poisson Autoregressive NHPP (GPAN) model for recurrent failure data from large-scale directed physical networks. The model consists of two layers: the temporal layer applies a Non-Homogeneous Poisson Process (NHPP) with node-specific frailties, and the spatial layer uses a well-orchestrated gamma-Poisson autoregressive scheme to establish correlations among the frailties. Under the network-GPAN model, we develop a sum-product algorithm to compute the marginal distribution for each frailty conditional on the recurrence data. The marginal conditional frailty distributions are useful for predicting future failures based on historical data. In addition, the ability to rapidly compute these marginal distributions allows adoption of an EM type algorithm for estimation. Through a Bethe approximation, the output from the sum-product algorithm is used to compute maximum log-likelihood estimates. Applying the methods to the Scottish Water network, we demonstrate utility in aiding operation management and risk assessment of the water utility. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Qingqing Zhai and Zhisheng Ye and Cheng Li and Matthew Revie and David B. Dunson},
  doi          = {10.1080/01621459.2024.2319897},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {251-265},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling recurrent failures on large directed networks},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ranking inferences based on the top choice of multiway comparisons. <em>JASA</em>, <em>120</em>(549), 237-250. (<a href='https://doi.org/10.1080/01621459.2024.2316364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by many applications such as online recommendations and individual choices, this article considers ranking inference of n items based on the observed data on the top choice among M randomly selected items at each trial. This is a useful modification of the Plackett-Luce model for M -way ranking with only the top choice observed and is an extension of the celebrated Bradley-Terry-Luce model that corresponds to M =‚Äâ2. Under a uniform sampling scheme in which any M distinguished items are selected for comparisons with probability p and the selected M items are compared L times with multinomial outcomes, we establish the statistical rates of convergence for underlying n preference scores using both l 2 -norm and l ‚àû -norm, under the minimum sampling complexity (smallest order of p ). In addition, we establish the asymptotic normality of the maximum likelihood estimator that allows us to construct confidence intervals for the underlying scores. Furthermore, we propose a novel inference framework for ranking items through a sophisticated maximum pairwise difference statistic whose distribution is estimated via a valid Gaussian multiplier bootstrap. The estimated distribution is then used to construct simultaneous confidence intervals for the differences in the preference scores and the ranks of individual items. They also enable us to address various inference questions on the ranks of these items. Extensive simulation studies lend further support to our theoretical results. A real data application illustrates the usefulness of the proposed methods. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jianqing Fan and Zhipeng Lou and Weichen Wang and Mengxin Yu},
  doi          = {10.1080/01621459.2024.2316364},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {237-250},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Ranking inferences based on the top choice of multiway comparisons},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence intervals for parameters of unobserved events. <em>JASA</em>, <em>120</em>(549), 226-236. (<a href='https://doi.org/10.1080/01621459.2024.2314318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a finite sample from an unknown distribution over a countable alphabet. Unobserved events are alphabet symbols which do not appear in the sample. Estimating the probabilities of unobserved events is a basic problem in statistics and related fields, which was extensively studied in the context of point estimation. In this work we introduce a novel interval estimation scheme for unobserved events. Our proposed framework applies selective inference, as we construct confidence intervals (CIs) for the desired set of parameters. Interestingly, we show that obtained CIs are dimension-free, as they do not grow with the alphabet size. Further, we show that these CIs are (almost) tight, in the sense that they cannot be further improved without violating the prescribed coverage rate. We demonstrate the performance of our proposed scheme in synthetic and real-world experiments, showing a significant improvement over the alternatives. Finally, we apply our proposed scheme to large alphabet modeling. We introduce a novel simultaneous CI scheme for large alphabet distributions which outperforms currently known methods while maintaining the prescribed coverage rate. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Amichai Painsky},
  doi          = {10.1080/01621459.2024.2314318},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {226-236},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Confidence intervals for parameters of unobserved events},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable and efficient infinite-order vector autoregressive model for high-dimensional time series. <em>JASA</em>, <em>120</em>(549), 212-225. (<a href='https://doi.org/10.1080/01621459.2024.2311365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a special infinite-order vector autoregressive (VAR) model, the vector autoregressive moving average (VARMA) model can capture much richer temporal patterns than the widely used finite-order VAR model. However, its practicality has long been hindered by its non-identifiability, computational intractability, and difficulty of interpretation, especially for high-dimensional time series. This article proposes a novel sparse infinite-order VAR model for high-dimensional time series, which avoids all above drawbacks while inheriting essential temporal patterns of the VARMA model. As another attractive feature, the temporal and cross-sectional structures of the VARMA-type dynamics captured by this model can be interpreted separately, since they are characterized by different sets of parameters. This separation naturally motivates the sparsity assumption on the parameters determining the cross-sectional dependence. As a result, greater statistical efficiency and interpretability can be achieved with little loss of temporal information. We introduce two l 1 -regularized estimation methods for the proposed model, which can be efficiently implemented via block coordinate descent algorithms, and derive the corresponding nonasymptotic error bounds. A consistent model order selection method based on the Bayesian information criteria is also developed. The merit of the proposed approach is supported by simulation studies and a real-world macroeconomic data analysis. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yao Zheng},
  doi          = {10.1080/01621459.2024.2311365},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {212-225},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An interpretable and efficient infinite-order vector autoregressive model for high-dimensional time series},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic low-rank tensor bandits for multi-dimensional online decision making. <em>JASA</em>, <em>120</em>(549), 198-211. (<a href='https://doi.org/10.1080/01621459.2024.2311364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional online decision making plays a crucial role in many real applications such as online recommendation and digital marketing. In these problems, a decision at each time is a combination of choices from different types of entities. To solve it, we introduce stochastic low-rank tensor bandits, a class of bandits whose mean rewards can be represented as a low-rank tensor. We consider two settings, tensor bandits without context and tensor bandits with context. In the first setting, the platform aims to find the optimal decision with the highest expected reward, a.k.a, the largest entry of true reward tensor. In the second setting, some modes of the tensor are contexts and the rest modes are decisions, and the goal is to find the optimal decision given the contextual information. We propose two learning algorithms tensor elimination and tensor epoch-greedy for tensor bandits without context, and derive finite-time regret bounds for them. Comparing with existing competitive methods, tensor elimination has the best overall regret bound and tensor epoch-greedy has a sharper dependency on dimensions of the reward tensor. Furthermore, we develop a practically effective Bayesian algorithm called tensor ensemble sampling for tensor bandits with context. Extensive simulations and real analysis in online advertising data back up our theoretical findings and show that our algorithms outperform various state-of-the-art approaches that ignore the tensor low-rank structure. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jie Zhou and Botao Hao and Zheng Wen and Jingfei Zhang and Will Wei Sun},
  doi          = {10.1080/01621459.2024.2311364},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {198-211},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Stochastic low-rank tensor bandits for multi-dimensional online decision making},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-free statistical inference on high-dimensional data. <em>JASA</em>, <em>120</em>(549), 186-197. (<a href='https://doi.org/10.1080/01621459.2024.2310314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to develop an effective model-free inference procedure for high-dimensional data. We first reformulate the hypothesis testing problem via sufficient dimension reduction framework. With the aid of new reformulation, we propose a new test statistic and show that its asymptotic distribution is œá 2 distribution whose degree of freedom does not depend on the unknown population distribution. We further conduct power analysis under local alternative hypotheses. In addition, we study how to control the false discovery rate of the proposed œá 2 tests, which are correlated, to identify important predictors under a model-free framework. To this end, we propose a multiple testing procedure and establish its theoretical guarantees. Monte Carlo simulation studies are conducted to assess the performance of the proposed tests and an empirical analysis of a real-world dataset is used to illustrate the proposed methodology. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xu Guo and Runze Li and Zhe Zhang and Changliang Zou},
  doi          = {10.1080/01621459.2024.2310314},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {186-197},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-free statistical inference on high-dimensional data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rejoinder. <em>JASA</em>, <em>120</em>(549), 180-185. (<a href='https://doi.org/10.1080/01621459.2025.2459216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {James Leiner and Boyan Duan and Larry Wasserman and Aaditya Ramdas},
  doi          = {10.1080/01621459.2025.2459216},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {180-185},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion on ‚ÄúData fission: Splitting a single data point‚Äù. <em>JASA</em>, <em>120</em>(549), 178-179. (<a href='https://doi.org/10.1080/01621459.2024.2416913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Zhigen Zhao},
  doi          = {10.1080/01621459.2024.2416913},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {178-179},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion on ‚ÄúData fission: Splitting a single data point‚Äù},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comments on ‚ÄúData fission: Splitting a single data point‚Äù by james leiner, boyan duan, larry wasserman, and aaditya ramdas. <em>JASA</em>, <em>120</em>(549), 176-177. (<a href='https://doi.org/10.1080/01621459.2024.2412808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Lijun Wang and Hongyu Zhao},
  doi          = {10.1080/01621459.2024.2412808},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {176-177},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comments on ‚ÄúData fission: Splitting a single data point‚Äù by james leiner, boyan duan, larry wasserman, and aaditya ramdas},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comment on ‚ÄúData fission: Splitting a single data point‚Äù data fission for unsupervised learning: A discussion on post-clustering inference and the challenges of debiasing. <em>JASA</em>, <em>120</em>(549), 174-175. (<a href='https://doi.org/10.1080/01621459.2024.2412191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Changhu Wang and Xinzhou Ge and Dongyuan Song and Jingyi Jessica Li},
  doi          = {10.1080/01621459.2024.2412191},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {174-175},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on ‚ÄúData fission: Splitting a single data point‚Äù data fission for unsupervised learning: A discussion on post-clustering inference and the challenges of debiasing},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comment on ‚ÄúData fission: Splitting a single data point‚Äù. <em>JASA</em>, <em>120</em>(549), 172-173. (<a href='https://doi.org/10.1080/01621459.2024.2402523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Philip Waggoner},
  doi          = {10.1080/01621459.2024.2402523},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {172-173},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on ‚ÄúData fission: Splitting a single data point‚Äù},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comments on ‚ÄúData fission: Splitting a single data point‚Äù. <em>JASA</em>, <em>120</em>(549), 170-171. (<a href='https://doi.org/10.1080/01621459.2024.2414865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Sanat K. Sarkar},
  doi          = {10.1080/01621459.2024.2414865},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {170-171},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comments on ‚ÄúData fission: Splitting a single data point‚Äù},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of ‚ÄúData fission: Splitting a single data point‚Äù by leiner et al.. <em>JASA</em>, <em>120</em>(549), 168-169. (<a href='https://doi.org/10.1080/01621459.2024.2413426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jing Lei and Natalia L. Oliveira and Ryan J. Tibshirani},
  doi          = {10.1080/01621459.2024.2413426},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {168-169},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of ‚ÄúData fission: Splitting a single data point‚Äù by leiner et al.},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical bayes estimation via data fission. <em>JASA</em>, <em>120</em>(549), 165-166. (<a href='https://doi.org/10.1080/01621459.2024.2421994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Nikolaos Ignatiadis and Dennis L. Sun},
  doi          = {10.1080/01621459.2024.2421994},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {165-166},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Empirical bayes estimation via data fission},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comment for ‚ÄúData fission: Splitting a single data point‚Äù. <em>JASA</em>, <em>120</em>(549), 164. (<a href='https://doi.org/10.1080/01621459.2025.2467918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Sangwon Hyun},
  doi          = {10.1080/01621459.2025.2467918},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {164},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment for ‚ÄúData fission: Splitting a single data point‚Äù},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of leiner et al. ‚ÄúData fission: Splitting a single data point‚Äù. <em>JASA</em>, <em>120</em>(549), 162-163. (<a href='https://doi.org/10.1080/01621459.2024.2402539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Daniel Garc√≠a Rasines and Alastair Young},
  doi          = {10.1080/01621459.2024.2402539},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {162-163},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of leiner et al. ‚ÄúData fission: Splitting a single data point‚Äù},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assumption-lean data fission with resampled data. <em>JASA</em>, <em>120</em>(549), 161. (<a href='https://doi.org/10.1080/01621459.2024.2412172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Kevin Fry and Snigdha Panigrahi and Jonathan Taylor},
  doi          = {10.1080/01621459.2024.2412172},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {161},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Assumption-lean data fission with resampled data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data fission and sampling designs: A discussion. <em>JASA</em>, <em>120</em>(549), 159-160. (<a href='https://doi.org/10.1080/01621459.2024.2416530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Kwun Chuen Gary Chan and Ross L. Prentice and Zhenman Yuan},
  doi          = {10.1080/01621459.2024.2416530},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {159-160},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Data fission and sampling designs: A discussion},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A discussion on: ‚ÄúData fission: Splitting a single data point‚Äù by leiner, j., duan, b., wasserman, l. and ramdas, a.. <em>JASA</em>, <em>120</em>(549), 158. (<a href='https://doi.org/10.1080/01621459.2024.2412183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Marta Catalano and Augusto Fasano and Matteo Giordano and Giovanni Rebaudo},
  doi          = {10.1080/01621459.2024.2412183},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {158},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A discussion on: ‚ÄúData fission: Splitting a single data point‚Äù by leiner, j., duan, b., wasserman, l. and ramdas, a.},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of ‚ÄúData fission: Splitting a single data point‚Äù. <em>JASA</em>, <em>120</em>(549), 151-157. (<a href='https://doi.org/10.1080/01621459.2024.2421998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Anna Neufeld and Ameer Dharamshi and Lucy L. Gao and Daniela Witten and Jacob Bien},
  doi          = {10.1080/01621459.2024.2421998},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {151-157},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of ‚ÄúData fission: Splitting a single data point‚Äù},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of ‚ÄúData fission: Splitting a single data point‚Äù ‚Äì Some asymptotic results for data fission. <em>JASA</em>, <em>120</em>(549), 147-150. (<a href='https://doi.org/10.1080/01621459.2024.2441416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Lihua Lei},
  doi          = {10.1080/01621459.2024.2441416},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {147-150},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of ‚ÄúData fission: Splitting a single data point‚Äù ‚Äì Some asymptotic results for data fission},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data fission: Splitting a single data point. <em>JASA</em>, <em>120</em>(549), 135-146. (<a href='https://doi.org/10.1080/01621459.2023.2270748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suppose we observe a random vector X from some distribution in a known family with unknown parameters. We ask the following question: when is it possible to split X into two pieces f ( X ) and g ( X ) such that neither part is sufficient to reconstruct X by itself, but both together can recover X fully, and their joint distribution is tractable? One common solution to this problem when multiple samples of X are observed is data splitting, but Rasines and Young offers an alternative approach that uses additive Gaussian noise‚Äîthis enables post-selection inference in finite samples for Gaussian distributed data and asymptotically when errors are non-Gaussian. In this article, we offer a more general methodology for achieving such a split in finite samples by borrowing ideas from Bayesian inference to yield a (frequentist) solution that can be viewed as a continuous analog of data splitting. We call our method data fission, as an alternative to data splitting, data carving and p -value masking. We exemplify the method on several prototypical applications, such as post-selection inference for trend filtering and other regression problems, and effect size estimation after interactive multiple testing. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {James Leiner and Boyan Duan and Larry Wasserman and Aaditya Ramdas},
  doi          = {10.1080/01621459.2023.2270748},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {135-146},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Data fission: Splitting a single data point},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDNAPlus: A unifying modeling framework for DNA-based biodiversity monitoring. <em>JASA</em>, <em>120</em>(549), 120-134. (<a href='https://doi.org/10.1080/01621459.2024.2412362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA-based biodiversity surveys, which involve collecting physical samples from survey sites and assaying them in the laboratory to detect species via their diagnostic DNA sequences, are increasingly being adopted for biodiversity monitoring and decision-making. The most commonly employed method, metabarcoding, combines PCR with high-throughput DNA sequencing to amplify and read ‚ÄúDNA barcode‚Äù sequences, generating count data indicating the number of times each DNA barcode was read. However, DNA-based data are noisy and error-prone, with several sources of variation, and cannot alone estimate the species-specific amount of DNA present at a surveyed site ( DNA biomass ). In this article, we present a unifying modeling framework for DNA-based survey data that allows estimation of changes in DNA biomass within species, across sites and their links to environmental covariates, while for the first time simultaneously accounting for key sources of variation, error and noise in the data-generating process, and for between-species and between-sites correlation. Bayesian inference is performed using MCMC with Laplace approximations. We describe a re-parameterization scheme for crossed-effects models designed to improve mixing, and an adaptive approach for updating latent variables, which reduces computation time. Theoretical and simulation results are used to guide study design, including the level of replication at different survey stages and the use of quality control methods. Finally, we demonstrate our new framework on a dataset of Malaise-trap samples, quantifying the effects of elevation and distance-to-road on each species, and produce maps identifying areas of high biodiversity and species DNA biomass. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Alex Diana and Eleni Matechou and Jim Griffin and Douglas W. Yu and Mingjie Luo and Marie Tosa and Alex Bush and Richard A. Griffiths},
  doi          = {10.1080/01621459.2024.2412362},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {120-134},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {EDNAPlus: A unifying modeling framework for DNA-based biodiversity monitoring},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Additive covariance matrix models: Modeling regional electricity net-demand in great britain. <em>JASA</em>, <em>120</em>(549), 107-119. (<a href='https://doi.org/10.1080/01621459.2024.2412361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasts of regional electricity net-demand, consumption minus embedded generation, are an essential input for reliable and economic power system operation, and energy trading. While such forecasts are typically performed region by region, operations such as managing power flows require spatially coherent joint forecasts, which account for cross-regional dependencies. Here, we forecast the joint distribution of net-demand across the 14 regions constituting Great Britain‚Äôs electricity network. Joint modeling is complicated by the fact that the net-demand variability within each region, and the dependencies between regions, vary with temporal, socio-economic and weather-related factors. We accommodate for these characteristics by proposing a multivariate Gaussian model based on a modified Cholesky parameterization, which allows us to model each unconstrained parameter via an additive model. Given that the number of model parameters and covariates is large, we adopt a semi-automated approach to model selection, based on gradient boosting. In addition to comparing the forecasting performance of several versions of the proposed model with that of two non-Gaussian copula-based models, we visually explore the model output to interpret how the covariates affect net-demand variability and dependencies. The code for reproducing the results in this article is available at https://doi.org/10.5281/zenodo.7315105. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {V. Gioia and M. Fasiolo and J. Browell and R. Bellio},
  doi          = {10.1080/01621459.2024.2412361},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {107-119},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Additive covariance matrix models: Modeling regional electricity net-demand in great britain},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantification of vaccine waning as a challenge effect. <em>JASA</em>, <em>120</em>(549), 96-106. (<a href='https://doi.org/10.1080/01621459.2024.2408776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowing whether vaccine protection wanes over time is important for health policy and drug development. However, quantifying waning effects is difficult. A simple contrast of vaccine efficacy at two different times compares different populations of individuals: those who were uninfected at the first time versus those who remain uninfected until the second time. Thus, the contrast of vaccine efficacy at early and late times can not be interpreted as a causal effect. We propose to quantify vaccine waning using the challenge effect, which is a contrast of outcomes under controlled exposures to the infectious agent following vaccination. We identify sharp bounds on the challenge effect under nonparametric assumptions that are broadly applicable in vaccine trials using routinely collected data. We demonstrate that the challenge effect can differ substantially from the conventional vaccine efficacy due to depletion of susceptible individuals from the risk set over time. Finally, we apply the methods to derive bounds on the waning of the BNT162b2 COVID-19 vaccine using data from a placebo-controlled randomized trial. Our estimates of the challenge effect suggest waning protection after 2 months beyond administration of the second vaccine dose. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Matias Janvin and Mats J. Stensrud},
  doi          = {10.1080/01621459.2024.2408776},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {96-106},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Quantification of vaccine waning as a challenge effect},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial modeling and future projection of extreme precipitation extents. <em>JASA</em>, <em>120</em>(549), 80-95. (<a href='https://doi.org/10.1080/01621459.2024.2408045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme precipitation events with large spatial extents may have more severe impacts than localized events as they can lead to widespread flooding. It is debated how climate change may affect the spatial extent of precipitation extremes, whose investigation often directly relies on simulations of precipitation from climate models. Here, we use a different strategy to investigate how future changes in spatial extents of precipitation extremes differ across climate zones and seasons in two river basins (Danube and Mississippi). We rely on observed precipitation extremes while exploiting a physics-based average-temperature covariate, enabling us to project future precipitation extents based on projected temperatures. We include the covariate into newly developed time-varying r -Pareto processes using suitably chosen spatial risk functionals r . This model captures temporal non-stationarity in the spatial dependence structure of precipitation extremes by linking it to the temperature covariate, derived from reanalysis data (ERA5-Land) for model calibration and from bias-corrected climate simulations (CMIP6) for projections. Our results show an increasing trend in the margins, with both significantly positive or negative trend coefficients depending on season and river (sub-)basin. During major rainy seasons, the significant trends indicate that future spatial extreme events will become relatively more intense and localized in several sub-basins. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Peng Zhong and Manuela Brunner and Thomas Opitz and Rapha√´l Huser},
  doi          = {10.1080/01621459.2024.2408045},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {80-95},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spatial modeling and future projection of extreme precipitation extents},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using penalized synthetic controls on truncated data: A case study on effect of marijuana legalization on direct payments to physicians by opioid manufacturers. <em>JASA</em>, <em>120</em>(549), 64-79. (<a href='https://doi.org/10.1080/01621459.2024.2406583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amid increasing awareness regarding opioid addiction, medical marijuana has emerged as a substitute to opioids for pain management. Concurrently, opioid manufacturers are putting significant research into making opioids safer yet effective. Interactions between these manufacturers and physicians are critical to advance existing pain management protocols. Direct payments from opioid manufacturers to physicians are established practices that often moderates such interactions. We study the effects of passage of a medical marijuana law (MML) on these direct payments to physicians. To draw causal conclusions, we develop a novel penalized synthetic control (SC) method that accommodates zero-payment related latent structures inherent in these payments. Under a truncated flexible additive mixture model, we show that the SC method has uncontrolled maximal risk without the penalty; by contrast, the proposed penalized method provides efficient estimates. Our analysis finds a significant decrease in direct payments from opioid manufacturers to pain medicine physicians as an effect of MML passage. We provide evidence that this decrease is due to medical marijuana becoming available as a substitute. Finally, our heterogeneity analyses indicate that the decrease in direct payments is comparatively higher for physicians practicing in localities with higher white populations, lower affluence, and a larger proportion of working-age residents. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Bikram Karmakar and Gourab Mukherjee and Wreetabrata Kar},
  doi          = {10.1080/01621459.2024.2406583},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {64-79},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Using penalized synthetic controls on truncated data: A case study on effect of marijuana legalization on direct payments to physicians by opioid manufacturers},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bisection grover‚Äôs search algorithm and its application in analyzing CITE-seq data. <em>JASA</em>, <em>120</em>(549), 52-63. (<a href='https://doi.org/10.1080/01621459.2024.2404259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of quantum computers, researchers have shown quantum advantages in physics-oriented problems. Quantum algorithms tackling computational biology problems are still lacking. In this article, we demonstrate the quantum advantage in analyzing CITE-seq data. CITE-seq, a single-cell technology, enables researchers to simultaneously measure expressions of RNA and surface protein detected by antibody-derived tags (ADTs) in the same cells. CITE-seq data hold tremendous potential for identifying ADTs associated with targeted genes and identifying cell types effectively. However, both tasks are challenging since the best subset of ADTs needs to be identified from enormous candidate subsets. To surmount the challenge, we develop a quantum algorithm named bisection Grover‚Äôs search (BGS) for the best subset selection of ADT markers in CITE-seq data. BGS takes advantage of quantum parallelism by integrating binary search and Grover‚Äôs algorithm to enable fast computation. Theoretical results are provided to show the privilege of BGS in the estimation error and computational complexity. The empirical performance of the BGS algorithm is demonstrated on both the IBM quantum computer and simulator. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ping Ma and Yongkai Chen and Haoran Lu and Wenxuan Zhong},
  doi          = {10.1080/01621459.2024.2404259},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {52-63},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bisection grover‚Äôs search algorithm and its application in analyzing CITE-seq data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating treatment prioritization rules via rank-weighted average treatment effects. <em>JASA</em>, <em>120</em>(549), 38-51. (<a href='https://doi.org/10.1080/01621459.2024.2393466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are a number of available methods for selecting whom to prioritize for treatment, including ones based on treatment effect estimation, risk scoring, and hand-crafted rules. We propose rank-weighted average treatment effect (RATE) metrics as a simple and general family of metrics for comparing and testing the quality of treatment prioritization rules. RATE metrics are agnostic as to how the prioritization rules were derived, and only assess how well they identify individuals that benefit the most from treatment. We define a family of RATE estimators and prove a central limit theorem that enables asymptotically exact inference in a wide variety of randomized and observational study settings. RATE metrics subsume a number of existing metrics, including the Qini coefficient, and our analysis directly yields inference methods for these metrics. We showcase RATE in the context of a number of applications, including optimal targeting of aspirin to stroke patients. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Steve Yadlowsky and Scott Fleming and Nigam Shah and Emma Brunskill and Stefan Wager},
  doi          = {10.1080/01621459.2024.2393466},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {38-51},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Evaluating treatment prioritization rules via rank-weighted average treatment effects},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection. rejoinder. <em>JASA</em>, <em>120</em>(549), 34-37. (<a href='https://doi.org/10.1080/01621459.2024.2443563'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Kes Ward and Giuseppe Dilillo and Idris Eckley and Paul Fearnhead},
  doi          = {10.1080/01621459.2024.2443563},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {34-37},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection. rejoinder},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Saha and ramdas‚Äô discussion of ‚ÄúPoisson-FOCuS‚Äù by ward, dilillo, eckley & fearnhead. <em>JASA</em>, <em>120</em>(549), 31-33. (<a href='https://doi.org/10.1080/01621459.2024.2402957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Aytijhya Saha and Aaditya Ramdas},
  doi          = {10.1080/01621459.2024.2402957},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {31-33},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Saha and ramdas‚Äô discussion of ‚ÄúPoisson-FOCuS‚Äù by ward, dilillo, eckley & fearnhead},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion on ‚ÄúPoisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection‚Äù by ward, dilillo, eckley and fearnhead. <em>JASA</em>, <em>120</em>(549), 26-30. (<a href='https://doi.org/10.1080/01621459.2024.2402961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Yang Chen},
  doi          = {10.1080/01621459.2024.2402961},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {26-30},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion on ‚ÄúPoisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection‚Äù by ward, dilillo, eckley and fearnhead},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A space astrophysics perspective on poisson-FOCuS. <em>JASA</em>, <em>120</em>(549), 22-25. (<a href='https://doi.org/10.1080/01621459.2024.2396960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Carlo Graziani},
  doi          = {10.1080/01621459.2024.2396960},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {22-25},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A space astrophysics perspective on poisson-FOCuS},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of ‚ÄúPoisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection‚Äù. <em>JASA</em>, <em>120</em>(549), 20-21. (<a href='https://doi.org/10.1080/01621459.2024.2402536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Mikael Kuusela},
  doi          = {10.1080/01621459.2024.2402536},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {20-21},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of ‚ÄúPoisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection‚Äù},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection. <em>JASA</em>, <em>120</em>(549), 7-19. (<a href='https://doi.org/10.1080/01621459.2023.2235059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gamma ray bursts are flashes of light from distant, new-born black holes. CubeSats that monitor high-energy photons across different energy bands are used to detect these bursts. There is a need for computationally efficient algorithms, able to run using the limited computational resource onboard a CubeSats, that can detect when gamma ray bursts occur. Current algorithms are based on monitoring photon counts across a grid of different sizes of time window. We propose a new method, which extends the recently proposed FOCuS approach for online change detection to Poisson data. Our method is mathematically equivalent to searching over all possible window sizes, but at half the computational cost of the current grid-based methods. We demonstrate the additional power of our approach using simulations and data drawn from the Fermi gamma ray burst monitor archive. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kes Ward and Giuseppe Dilillo and Idris Eckley and Paul Fearnhead},
  doi          = {10.1080/01621459.2023.2235059},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {7-19},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Our mission in action: Past, present, and future. <em>JASA</em>, <em>120</em>(549), 1-6. (<a href='https://doi.org/10.1080/01621459.2025.2450191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by our mission, we have advanced science for the public good. Our history is rich in theory, methodology, and applications that are instrumental in solving real-world problems. Our reach is boundless, and our impact is undeniable in diverse areas, including engineering, economics, the environment, genetics, public health, and health policy. As the data landscape continues to evolve, our leadership, expertise and knowledge will be essential to meet the global challenges of the future. We must boldly embody our mission of promoting the practice and profession of statistics. In this talk, I will highlight our mission in action as we strive to be a community that uses statistical thinking to drive discovery and inform decisions. This article summarizes the 2023 presidential speech given at the JSM in Toronto.},
  archive      = {J_JASA},
  author       = {Dionne L. Price},
  doi          = {10.1080/01621459.2025.2450191},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {1-6},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Our mission in action: Past, present, and future},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
