<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JASA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jasa">JASA - 184</h2>
<ul>
<li><details>
<summary>
(2025). Correction. <em>JASA</em>, <em>120</em>(551), 2011-2014. (<a href='https://doi.org/10.1080/01621459.2025.2540256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bhattacharya et al. introduce a novel prior, the Dirichlet-Laplace (DL) prior, and propose a Markov chain Monte Carlo (MCMC) method to simulate posterior draws under this prior in a conditionally Gaussian setting. The original algorithm samples from conditional distributions in the wrong order, that is, it does not correctly sample from the joint posterior distribution of all latent variables. This note details the issue and provides two simple solutions: A correction to the original algorithm and a new algorithm based on an alternative, yet equivalent, formulation of the prior. This corrigendum does not affect the theoretical results in Bhattacharya et al.},
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2025.2540256},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {2011-2014},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correction},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Financial data analytics with r: Monte-carlo validation. <em>JASA</em>, <em>120</em>(551), 2009-2010. (<a href='https://doi.org/10.1080/01621459.2025.2526711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Tony Sit},
  doi          = {10.1080/01621459.2025.2526711},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {2009-2010},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Financial data analytics with r: Monte-carlo validation},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical prediction and machine learning. <em>JASA</em>, <em>120</em>(551), 2007-2009. (<a href='https://doi.org/10.1080/01621459.2025.2495318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Michal Pešta},
  doi          = {10.1080/01621459.2025.2495318},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {2007-2009},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical prediction and machine learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ordinal data analysis: Statistical perspective with applications. <em>JASA</em>, <em>120</em>(551), 2005-2007. (<a href='https://doi.org/10.1080/01621459.2024.2443255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Maria Iannario},
  doi          = {10.1080/01621459.2024.2443255},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {2005-2007},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Ordinal data analysis: Statistical perspective with applications},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized linear mixed models: Modern concepts, methods and applications, 2nd ed.. <em>JASA</em>, <em>120</em>(551), 2003-2005. (<a href='https://doi.org/10.1080/01621459.2025.2506201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Xing Liu},
  doi          = {10.1080/01621459.2025.2506201},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {2003-2005},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Generalized linear mixed models: Modern concepts, methods and applications, 2nd ed.},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial linear models for environmental data. <em>JASA</em>, <em>120</em>(551), 2002-2003. (<a href='https://doi.org/10.1080/01621459.2025.2490300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Bruno Sansó},
  doi          = {10.1080/01621459.2025.2490300},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {2002-2003},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spatial linear models for environmental data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep mutual density ratio estimation with bregman divergence and its applications. <em>JASA</em>, <em>120</em>(551), 1990-2001. (<a href='https://doi.org/10.1080/01621459.2025.2507437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a unified approach to estimating the mutual density ratio, defined as the ratio between the joint density function and the product of the individual marginal density functions of two random vectors. It serves as a fundamental measure for quantifying the relationship between two random vectors. Our method uses the Bregman divergence to construct the objective function and leverages deep neural networks to approximate the logarithm of the mutual density ratio. We establish a non-asymptotic error bound for our estimator, achieving the optimal minimax rate of convergence under a bounded support condition. Additionally, our estimator mitigates the curse of dimensionality when the distribution is supported on a lower-dimensional manifold. We extend our results to overparameterized neural networks and the case with unbounded support. Applications of our method include conditional probability density estimation, mutual information estimation, and independence testing. Simulation studies and real data examples demonstrate the effectiveness of our approach. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Dongxiao Han and Siming Zheng and Guohao Shen and Xinyuan Song and Liuquan Sun and Jian Huang},
  doi          = {10.1080/01621459.2025.2507437},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1990-2001},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Deep mutual density ratio estimation with bregman divergence and its applications},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cutting feedback in misspecified copula models. <em>JASA</em>, <em>120</em>(551), 1975-1989. (<a href='https://doi.org/10.1080/01621459.2025.2464270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In copula models the marginal distributions and copula function are specified separately. We treat these as two modules in a modular Bayesian inference framework, and propose conducting modified Bayesian inference by “cutting feedback”. Cutting feedback limits the influence of potentially misspecified modules in posterior inference. We consider two types of cuts. The first limits the influence of a misspecified copula on inference for the marginals, which is a Bayesian analogue of the popular Inference for Margins (IFM) estimator. The second limits the influence of misspecified marginals on inference for the copula parameters by using a pseudo likelihood of the ranks to define the cut model. We establish that if only one of the modules is misspecified, then the appropriate cut posterior gives accurate uncertainty quantification asymptotically for the parameters in the other module. Computation of the cut posteriors is difficult, and new variational inference methods to do so are proposed. The efficacy of the new methodology is demonstrated using both simulated data and a substantive multivariate time series copula application from macroeconomic forecasting. In the latter, cutting feedback from misspecified marginals to a 1096 dimension copula improves posterior inference and predictive accuracy greatly, compared to conventional Bayesian inference. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Michael Stanley Smith and Weichang Yu and David J. Nott and David T. Frazier},
  doi          = {10.1080/01621459.2025.2464270},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1975-1989},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Cutting feedback in misspecified copula models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for high-dimensional spectral density matrix. <em>JASA</em>, <em>120</em>(551), 1960-1974. (<a href='https://doi.org/10.1080/01621459.2025.2468013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spectral density matrix is a fundamental object of interest in time series analysis, and it encodes both contemporary and dynamic linear relationships between component processes of the multivariate system. In this article we develop novel inference procedures for the spectral density matrix in the high-dimensional setting. Specifically, we introduce a new global testing procedure to test the nullity of the cross-spectral density for a given set of frequencies and across pairs of component indices. For the first time, both Gaussian approximation and parametric bootstrap methodologies are employed to conduct inference for a high-dimensional parameter formulated in the frequency domain, and new technical tools are developed to provide asymptotic guarantees of the size accuracy and power for global testing. We further propose a multiple testing procedure for simultaneously testing the nullity of the cross-spectral density at a given set of frequencies. The method is shown to control the false discovery rate. Both numerical simulations and a real data illustration demonstrate the usefulness of the proposed testing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jinyuan Chang and Qing Jiang and Tucker McElroy and Xiaofeng Shao},
  doi          = {10.1080/01621459.2025.2468013},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1960-1974},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for high-dimensional spectral density matrix},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous inference for generalized linear models with unmeasured confounders. <em>JASA</em>, <em>120</em>(551), 1945-1959. (<a href='https://doi.org/10.1080/01621459.2025.2485379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tens of thousands of simultaneous hypothesis tests are routinely performed in genomic studies to identify differentially expressed genes. However, due to unmeasured confounders, many standard statistical approaches may be substantially biased. This article investigates the large-scale hypothesis testing problem for multivariate generalized linear models in the presence of confounding effects. Under arbitrary confounding mechanisms, we propose a unified statistical estimation and inference framework that harnesses orthogonal structures and integrates linear projections into three key stages. It begins by disentangling marginal and uncorrelated confounding effects to recover the latent coefficients. Subsequently, latent factors and primary effects are jointly estimated through lasso-type optimization. Finally, we incorporate projected and weighted bias-correction steps for hypothesis testing. Theoretically, we establish the identification conditions of various effects and non-asymptotic error bounds. We show effective Type-I error control of asymptotic z -tests as sample and response sizes approach infinity. Numerical experiments demonstrate that the proposed method controls the false discovery rate by the Benjamini-Hochberg procedure and is more powerful than alternative methods. By comparing single-cell RNA-seq counts from two groups of samples, we demonstrate the suitability of adjusting confounding effects when significant covariates are absent from the model. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jin-Hong Du and Larry Wasserman and Kathryn Roeder},
  doi          = {10.1080/01621459.2025.2485379},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1945-1959},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Simultaneous inference for generalized linear models with unmeasured confounders},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional variable clustering based on maxima of a weakly dependent random process. <em>JASA</em>, <em>120</em>(551), 1933-1944. (<a href='https://doi.org/10.1080/01621459.2025.2459443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new class of models for variable clustering called Asymptotic Independent block (AI-block) models, which defines population-level clusters based on the independence of the maxima of a multivariate stationary mixing random process among clusters. This class of models is identifiable, meaning that there exists a maximal element with a partial order between partitions, allowing for statistical inference. We also present an algorithm depending on a tuning parameter that recovers the clusters of variables without specifying the number of clusters a priori. Our work provides some theoretical insights into the consistency of our algorithm, demonstrating that under certain conditions it can effectively identify clusters in the data with a computational complexity that is polynomial in the dimension. A data-driven selection method for the tuning parameter is also proposed. To further illustrate the significance of our work, we applied our method to neuroscience and environmental real-datasets. These applications highlight the potential and versatility of the proposed approach. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Alexis Boulin and Elena Di Bernardino and Thomas Laloë and Gwladys Toulemonde},
  doi          = {10.1080/01621459.2025.2459443},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1933-1944},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {High-dimensional variable clustering based on maxima of a weakly dependent random process},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network goodness-of-fit for the block-model family. <em>JASA</em>, <em>120</em>(551), 1919-1932. (<a href='https://doi.org/10.1080/01621459.2025.2479242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The block-model family has four popular network models (SBM, DCBM, MMSBM, and DCMM). A fundamental problem is, how well each of these models fits with real networks. We propose GoF-MSCORE as a new Goodness-of-Fit (GoF) metric for DCMM (the broadest one among the four), with two main ideas. The first is to use cycle count statistics as a general recipe for GoF. The second is a novel network fitting scheme. GoF-MSCORE is a flexible GoF approach, and we further extend it to SBM, DCBM, and MMSBM. This gives rise to a series of GoF metrics covering each of the four models in the block-model family. We show that for each of the four models, if the assumed model is correct, then the corresponding GoF metric converges to N ( 0 , 1 ) as the network sizes diverge. We also analyze the powers and show that these metrics are optimal in many settings. In comparison, many other GoF ideas face challenges: they may lack a parameter-free limiting null, or are nonoptimal in power, or face an analytical hurdle. Note that a parameter-free limiting null is especially desirable as many network models have a large number of unknown parameters. The limiting nulls of our GoF metrics are always N ( 0 , 1 ) , which are parameter-free as desired. For 12 frequently-used real networks, we use the proposed GoF metrics to show that DCMM fits well with almost all of them. We also show that SBM, DCBM, and MMSBM do not fit well with many of these networks, especially when the networks are relatively large. To complement with our study on GoF, we also show that the DCMM is nearly as broad as the rank- K network model. Based on these results, we recommend the DCMM as a promising model for undirected networks. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jiashun Jin and Zheng Tracy Ke and Jiajun Tang and Jingming Wang},
  doi          = {10.1080/01621459.2025.2479242},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1919-1932},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Network goodness-of-fit for the block-model family},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When composite likelihood meets stochastic approximation. <em>JASA</em>, <em>120</em>(551), 1906-1918. (<a href='https://doi.org/10.1080/01621459.2024.2436219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A composite likelihood is an inference function derived by multiplying a set of likelihood components. This approach provides a flexible framework for drawing inferences when the likelihood function of a statistical model is computationally intractable. While composite likelihood has computational advantages, it can still be demanding when dealing with numerous likelihood components and a large sample size. This article tackles this challenge by employing an approximation of the conventional composite likelihood estimator based on a stochastic optimization procedure. This novel estimator is shown to be asymptotically normally distributed around the true parameter. In particular, based on the relative divergent rate of the sample size and the number of iterations of the optimization, the variance of the limiting distribution is shown to compound for two sources of uncertainty: the sampling variability of the data and the optimization noise, with the latter depending on the sampling distribution used to construct the stochastic gradients. The advantages of the proposed framework are illustrated through simulation studies on two working examples: an Ising model for binary data and a gamma frailty model for count data. Finally, a real-data application is presented, showing its effectiveness in a large-scale mental health survey. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Giuseppe Alfonzetti and Ruggero Bellio and Yunxiao Chen and Irini Moustaki},
  doi          = {10.1080/01621459.2024.2436219},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1906-1918},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {When composite likelihood meets stochastic approximation},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive testing for high-dimensional data. <em>JASA</em>, <em>120</em>(551), 1893-1905. (<a href='https://doi.org/10.1080/01621459.2024.2439617'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a class of 𝐿 𝑞 -norm based U-statistics for a family of global testing problems related to high-dimensional data. This includes testing of mean vector and its spatial sign, simultaneous testing of linear model coefficients, and testing of component-wise independence for high-dimensional observations, among others. Under the null hypothesis, we derive asymptotic normality and independence between L q -norm based U-statistics for several qs under mild moment and cumulant conditions. A simple combination of two studentized L q -based test statistics via their p-values is proposed and is shown to attain great power against alternatives of different sparsity. Our work is a substantial extension of He et al., which is mostly focused on mean and covariance testing, and we manage to provide a general treatment of asymptotic independence of L q -norm based U-statistics for a wide class of kernels. To alleviate the computation burden, we introduce a variant of the proposed U-statistics by using the monotone indices in the summation, resulting in a U-statistic with asymmetric kernel. A dynamic programming method is introduced to reduce the computational cost from O ( n qr ) , which is required for the calculation of the full U-statistic, to O ( n r ) where r is the order of the kernel. Numerical results further corroborate the advantage of the proposed adaptive test as compared to some existing competitors. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yangfan Zhang and Runmin Wang and Xiaofeng Shao},
  doi          = {10.1080/01621459.2024.2439617},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1893-1905},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Adaptive testing for high-dimensional data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geodesic mixed effects models for repeatedly Observed/Longitudinal random objects. <em>JASA</em>, <em>120</em>(551), 1879-1892. (<a href='https://doi.org/10.1080/01621459.2025.2474267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed effect modeling for longitudinal data is challenging when the observed data are random objects, which are complex data taking values in a general metric space without either global linear or local linear (Riemannian) structure. In such settings the classical additive error model and distributional assumptions are unattainable. Due to the rapid advancement of technology, longitudinal data containing complex random objects, such as covariance matrices, data on Riemannian manifolds, and probability distributions are becoming more common. Addressing this challenge, we develop a mixed-effects regression for data in geodesic spaces, where the underlying mean response trajectories are geodesics in the metric space and the deviations of the observations from the model are quantified by perturbation maps or transports. A key finding is that the geodesic trajectories assumption for the case of random objects is a natural extension of the linearity assumption in the standard Euclidean scenario to the case of general geodesic metric spaces. Geodesics can be recovered from noisy observations by exploiting a connection between the geodesic path and the path obtained by global Fréchet regression for random objects. The effect of baseline Euclidean covariates on the geodesic paths is modeled by another Fréchet regression step. We study the asymptotic convergence of the proposed estimates and provide illustrations through simulations and real-data applications. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Satarupa Bhattacharjee and Hans-Georg Müller},
  doi          = {10.1080/01621459.2025.2474267},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1879-1892},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Geodesic mixed effects models for repeatedly Observed/Longitudinal random objects},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric ergodicity of trans-dimensional markov chain monte carlo algorithms. <em>JASA</em>, <em>120</em>(551), 1868-1878. (<a href='https://doi.org/10.1080/01621459.2024.2427432'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the convergence properties of trans-dimensional MCMC algorithms when the total number of models is finite. It is shown that, for reversible and some nonreversible trans-dimensional Markov chains, under mild conditions, geometric convergence is guaranteed if the Markov chains associated with the within-model moves are geometrically ergodic. This result is proved in an L 2 framework using the technique of Markov chain decomposition. While the technique was previously developed for reversible chains, this work extends it to the point that it can be applied to some commonly used nonreversible chains. The theory herein is applied to reversible jump algorithms for three Bayesian models: a probit regression with variable selection, a Gaussian mixture model with unknown number of components, and an autoregression with Laplace errors and unknown model order. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Qian Qin},
  doi          = {10.1080/01621459.2024.2427432},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1868-1878},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Geometric ergodicity of trans-dimensional markov chain monte carlo algorithms},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation and inference of quantile spatially varying coefficient models over complicated domains. <em>JASA</em>, <em>120</em>(551), 1853-1867. (<a href='https://doi.org/10.1080/01621459.2025.2480867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a flexible quantile spatially varying coefficient model (QSVCM) for the regression analysis of spatial data. The proposed model enables researchers to assess the dependence of conditional quantiles of the response variable on covariates while accounting for spatial nonstationarity. Our approach facilitates learning and interpreting heterogeneity in spatial data distributed over complex or irregular domains. We introduce a quantile regression method that uses bivariate penalized splines in triangulation to estimate unknown functional coefficients. We establish the L 2 convergence of the proposed estimators, demonstrating their optimal convergence rate under certain regularity conditions. An efficient optimization algorithm is developed using the alternating direction method of multipliers (ADMM). We develop wild residual bootstrap-based pointwise confidence intervals for the QSVCM quantile coefficients. Furthermore, we construct reliable conformal prediction intervals for the response variable using the proposed QSVCM. Simulation studies show the remarkable performance of the proposed methods. Lastly, we illustrate the practical applicability of our methods by analyzing the mortality dataset and the supplementary particulate matter (PM) dataset in the United States. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Myungjin Kim and Li Wang and Huixia Judy Wang},
  doi          = {10.1080/01621459.2025.2480867},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1853-1867},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation and inference of quantile spatially varying coefficient models over complicated domains},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified framework for residual diagnostics in generalized linear models and beyond. <em>JASA</em>, <em>120</em>(551), 1840-1852. (<a href='https://doi.org/10.1080/01621459.2025.2504037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model diagnostics is an indispensable component in regression analysis, yet it has not been well addressed in generalized linear models (GLMs). When outcome data are discrete, classical Pearson and deviance residuals have limited utility in generating diagnostic insights. This article establishes a novel diagnostic framework for GLMs and their extensions. Unlike the convention of using a point statistic as a residual, we propose to use a function as a vehicle to retain residual information. In the presence of data discreteness, we show that such a functional residual is appropriate for summarizing the residual randomness that cannot be captured by the structural part of the model. We establish its theoretical properties, which lead to the innovation of new diagnostic tools including the functional-residual- vs -covariate plot and Function-to-Function plot (similar to a Quantile-Quantile plot). Our numerical studies demonstrate that the use of these tools can reveal a variety of model misspecifications, such as not properly including a higher-order term, an explanatory variable, an interaction effect, a dispersion parameter, or a zero-inflation component. As a general notion, the functional residual considerably broadens the diagnostic scope as it applies to GLMs for binary, ordinal and count data as well as semiparametric models (e.g., generalized additive models), all in a unified framework. Its functional form provides a way to unify point residuals such as Liu-Zhang’s surrogate residual and Li-Shepherd’s probability-scale residual. As its graphical outputs can be interpreted in a similar way to those for linear models, our framework also unifies diagnostic interpretation for discrete data and continuous data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Dungang Liu and Zewei Lin and Heping Zhang},
  doi          = {10.1080/01621459.2025.2504037},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1840-1852},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A unified framework for residual diagnostics in generalized linear models and beyond},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phase-type distributions for sieve estimation. <em>JASA</em>, <em>120</em>(551), 1828-1839. (<a href='https://doi.org/10.1080/01621459.2025.2459442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many semiparametric models, the infinite-dimensional parameter of direct interest is a probability density, but its nonparametric estimation is usually difficult in the presence of incomplete data. To address this issue, this study promotes phase-type distributions as a method of sieve. Phase-type distributions are dense in the space of nonnegative distributions, closed under minimum, maximum, and convolution, and compatible with the accelerated failure time model. This renders them attractive for sieve density estimation for problems with sophisticated missing data. However, the class of phase-type distributions is over-parameterized, and its approximation error rate for a given density as the number of phases increases remains unknown. These handicaps hinder its theoretical development as a sieve method. In this article, we design a sieve class of identifiable phase-type densities and establish its approximation error rate for a given density, which is the first error rate result known for phase type distributions. The proposed sieve is then used for semiparametric M-estimation, where the nonparametric component is a density. Building on the approximation error rate results, we establish general asymptotic properties of the phase-type sieve estimators and apply them to models that have complicated data missingness and cannot be easily handled by existing methods. Our simulation and real data analysis focus on right-censored data with missing indicators, in which we demonstrate that our estimators are more efficient than existing estimators. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Hu Xiangbin and Yudong Wang and Zhisheng Ye and Xingqiu Zhao},
  doi          = {10.1080/01621459.2025.2459442},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1828-1839},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Phase-type distributions for sieve estimation},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially exchangeable stochastic block models for (Node-colored) multilayer networks. <em>JASA</em>, <em>120</em>(551), 1811-1827. (<a href='https://doi.org/10.1080/01621459.2025.2507825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilayer networks generalize single-layered connectivity data in several directions. These generalizations include, among others, settings where multiple types of edges are observed among the same set of nodes (edge-colored networks) or where a single notion of connectivity is measured between nodes belonging to different pre-specified layers (node-colored networks). While progress has been made in statistical modeling of edge-colored networks, principled approaches that flexibly account for both within and across layer block-connectivity structures while incorporating layer information through a rigorous probabilistic construction are still lacking for node-colored multilayer networks. We fill this gap by introducing a novel class of partially exchangeable stochastic block models specified in terms of a hierarchical random partition prior for the allocation of nodes to groups, whose number is learned by the model. This goal is achieved without jeopardizing probabilistic coherence, uncertainty quantification and derivation of closed-form predictive within- and across-layer co-clustering probabilities. Our approach facilitates prior elicitation, the understanding of theoretical properties and the development of yet-unexplored predictive strategies for both the connections and the allocations of future incoming nodes. Posterior inference proceeds via a tractable collapsed Gibbs sampler, while performance is illustrated in simulations and in a real-world criminal network application. The notable gains achieved over competitors clarify the importance of developing general stochastic block models based on suitable node-exchangeability structures coherent with the type of multilayer network being analyzed. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Daniele Durante and Francesco Gaffi and Antonio Lijoi and Igor Prünster},
  doi          = {10.1080/01621459.2025.2507825},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1811-1827},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Partially exchangeable stochastic block models for (Node-colored) multilayer networks},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional expected shortfall regression. <em>JASA</em>, <em>120</em>(551), 1799-1810. (<a href='https://doi.org/10.1080/01621459.2024.2448860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Shushu Zhang and Xuming He and Kean Ming Tan and Wen-Xin Zhou},
  doi          = {10.1080/01621459.2024.2448860},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1799-1810},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {High-dimensional expected shortfall regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustifying likelihoods by optimistically re-weighting data. <em>JASA</em>, <em>120</em>(551), 1787-1798. (<a href='https://doi.org/10.1080/01621459.2025.2468012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Likelihood-based inferences have been remarkably successful in wide-spanning application areas. However, even after due diligence in selecting a good model for the data at hand, there is inevitably some amount of model misspecification: outliers, data contamination or inappropriate parametric assumptions such as Gaussianity mean that most models are at best rough approximations of reality. A significant practical concern is that for certain inferences, even small amounts of model misspecification may have a substantial impact; a problem we refer to as brittleness . This article attempts to address the brittleness problem in likelihood-based inferences by choosing the most model friendly data generating process in a distance-based neighborhood of the empirical measure. This leads to a new Optimistically Weighted Likelihood (OWL), which robustifies the original likelihood by formally accounting for a small amount of model misspecification. Focusing on total variation (TV) neighborhoods, we study theoretical properties, develop estimation algorithms and illustrate the methodology in applications to mixture models and regression. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Miheer Dewaskar and Christopher Tosh and Jeremias Knoblauch and David B. Dunson},
  doi          = {10.1080/01621459.2025.2468012},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1787-1798},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robustifying likelihoods by optimistically re-weighting data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian clustering via fusing of localized densities. <em>JASA</em>, <em>120</em>(551), 1775-1786. (<a href='https://doi.org/10.1080/01621459.2024.2427935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian clustering typically relies on mixture models, with each component interpreted as a different cluster. After defining a prior for the component parameters and weights, Markov chain Monte Carlo (MCMC) algorithms are commonly used to produce samples from the posterior distribution of the component labels. The data are then clustered by minimizing the expectation of a clustering loss function that favors similarity to the component labels. Unfortunately, although these approaches are routinely implemented, clustering results are highly sensitive to kernel misspecification. For example, if Gaussian kernels are used but the true density of data within a cluster is even slightly non-Gaussian, then clusters will be broken into multiple Gaussian components. To address this problem, we develop Fusing of Localized Densities (FOLD), a novel clustering method that melds components together using the posterior of the kernels. FOLD has a fully Bayesian decision theoretic justification, naturally leads to uncertainty quantification, can be easily implemented as an add-on to MCMC algorithms for mixtures, and favors a small number of distinct clusters. We provide theoretical support for FOLD including clustering optimality under kernel misspecification. In simulated experiments and real data, FOLD outperforms competitors by minimizing the number of clusters while inferring meaningful group structure. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Alexander Dombowsky and David B. Dunson},
  doi          = {10.1080/01621459.2024.2427935},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1775-1786},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian clustering via fusing of localized densities},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional knockoffs inference for time series data. <em>JASA</em>, <em>120</em>(551), 1763-1774. (<a href='https://doi.org/10.1080/01621459.2024.2431344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We make some initial attempt to establish the theoretical and methodological foundation for the model-X knockoffs inference for time series data. We suggest the method of time series knockoffs inference (TSKI) by exploiting the ideas of subsampling and e-values to address the difficulty caused by the serial dependence. We also generalize the robust knockoffs inference in Barber, Candès, and Samworth to the time series setting to relax the assumption of known covariate distribution required by model-X knockoffs, since such an assumption is overly stringent for time series data. We establish sufficient conditions under which TSKI achieves the asymptotic false discovery rate (FDR) control. Our technical analysis reveals the effects of serial dependence and unknown covariate distribution on the FDR control. We conduct a power analysis of TSKI using the Lasso coefficient difference knockoff statistic under the generalized linear time series models. The finite-sample performance of TSKI is illustrated with several simulation examples and an economic inflation study. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Chien-Ming Chi and Yingying Fan and Ching-Kang Ing and Jinchi Lv},
  doi          = {10.1080/01621459.2024.2431344},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1763-1774},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {High-dimensional knockoffs inference for time series data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matrix GARCH model: Inference and application. <em>JASA</em>, <em>120</em>(551), 1747-1762. (<a href='https://doi.org/10.1080/01621459.2024.2415719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix-variate time series data are largely available in applications. However, no attempt has been made to study their conditional heteroscedasticity that is often observed in economic and financial data. To address this gap, we propose a novel matrix generalized autoregressive conditional heteroscedasticity (GARCH) model to capture the dynamics of conditional row and column covariance matrices of matrix time series. The key innovation of the matrix GARCH model is the use of a univariate GARCH specification for the trace of conditional row or column covariance matrix, which allows for the model identification. Moreover, we introduce a quasi-maximum likelihood estimator (QMLE) for model estimation and develop a portmanteau test for model diagnostic checking. Simulation studies are conducted to assess the finite-sample performance of the QMLE and portmanteau test. To handle large dimensional matrix time series, we also propose a matrix factor GARCH model, and establish its theoretical properties. Finally, we demonstrate the superiority of the matrix GARCH and matrix factor GARCH models over existing multivariate GARCH-type models in volatility forecasting and portfolio allocations using three applications on credit default swap prices, global stock sector indices, and future prices. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Cheng Yu and Dong Li and Feiyu Jiang and Ke Zhu},
  doi          = {10.1080/01621459.2024.2415719},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1747-1762},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Matrix GARCH model: Inference and application},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication-efficient distributed estimation and inference for cox’s model. <em>JASA</em>, <em>120</em>(551), 1736-1746. (<a href='https://doi.org/10.1080/01621459.2025.2516820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by multi-center biomedical studies that cannot share individual data due to privacy and ownership concerns, we develop communication-efficient iterative distributed algorithms for estimation and inference in the high-dimensional sparse Cox proportional hazards model. We demonstrate that our estimator, even with a relatively small number of iterations, achieves the same convergence rate as the ideal full-sample estimator under very mild conditions. To construct confidence intervals for linear combinations of high-dimensional hazard regression coefficients, we introduce a novel debiased method, establish central limit theorems, and provide consistent variance estimators that yield asymptotically valid distributed confidence intervals. In addition, we provide valid and powerful distributed hypothesis tests for any coordinate element based on a decorrelated score test. We allow time-dependent covariates as well as censored survival times. Extensive numerical experiments on both simulated and real data lend further support to our theory and demonstrate that our communication-efficient distributed estimators, confidence intervals, and hypothesis tests improve upon alternative methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Pierre Bayle and Jianqing Fan and Zhipeng Lou},
  doi          = {10.1080/01621459.2025.2516820},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1736-1746},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Communication-efficient distributed estimation and inference for cox’s model},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial quantile tensor regression. <em>JASA</em>, <em>120</em>(551), 1724-1735. (<a href='https://doi.org/10.1080/01621459.2024.2422129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensors, characterized as multidimensional arrays, are frequently encountered in modern scientific studies. Quantile regression has the unique capacity to explore how a tensor covariate influences different segments of the response distribution. In this work, we propose a partial quantile tensor regression (PQTR) framework, which novelly applies the core principle of the partial least squares technique to achieve effective dimension reduction for quantile regression with a tensor covariate. The proposed PQTR algorithm is computationally efficient and scalable to a large tensor covariate. Moreover, we uncover an appealing latent variable model representation for the PQTR algorithm, justifying a simple population interpretation of the resulting estimator. We further investigate the connection of the PQTR procedure with an envelope quantile tensor regression (EQTR) model, which defines a general set of sparsity conditions tailored to quantile tensor regression. We prove the root-n consistency of the PQTR estimator under the EQTR model, and demonstrate its superior finite-sample performance compared to benchmark methods through simulation studies. We demonstrate the practical utility of the proposed method via an application to a neuroimaging study of post traumatic stress disorder (PTSD). Results derived from the proposed method are more neurobiologically meaningful and interpretable as compared to those from existing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Dayu Sun and Limin Peng and Zhiping Qiu and Ying Guo and Amita Manatunga},
  doi          = {10.1080/01621459.2024.2422129},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1724-1735},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Partial quantile tensor regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deconvolution density estimation with penalized MLE. <em>JASA</em>, <em>120</em>(551), 1711-1723. (<a href='https://doi.org/10.1080/01621459.2024.2436686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deconvolution is the important problem of estimating the distribution of a quantity of interest from a sample with additive measurement error. Nearly all infinite-dimensional deconvolution methods in the literature use Fourier transformations. These methods are mathematically neat, but unstable, and produce bad estimates when signal-noise ratio or sample size are low. A popular alternative is to maximize penalized likelihood for a finite-dimensional basis expansion of the unknown density. We develop a new method to optimize penalized likelihood over the infinite-dimensional space of all functions. This gives the stability of regularized likelihood methods without restricting the space of solutions. Our method compares favorably with state-of-the-art methods on simulated and real data, particularly for small sample size or low signal-noise ratio. We also provide the first results on the consistency and rate of convergence of penalized maximum likelihood estimates for density deconvolution. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yun Cai and Hong Gu and Toby Kenney},
  doi          = {10.1080/01621459.2024.2436686},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1711-1723},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Deconvolution density estimation with penalized MLE},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust inference for federated meta-learning. <em>JASA</em>, <em>120</em>(551), 1695-1710. (<a href='https://doi.org/10.1080/01621459.2024.2443246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing information from multiple data sources is critical to ensure knowledge generalizability. Integrative analysis of multi-source data is challenging due to the heterogeneity across sources and data-sharing constraints. In this article, we consider a general robust inference framework for federated meta-learning of data from multiple sites, enabling statistical inference for the prevailing model, defined as the one matching the majority of the sites. Statistical inference for the prevailing model is challenging since it requires a data-adaptive mechanism to select eligible sites and subsequently account for the selection uncertainty. We propose a novel sampling method to address the additional variation arising from the selection. Our devised confidence interval does not require sites to share individual-level data and is shown to be valid without requiring the selection of eligible sites to be error-free. The proposed robust inference for federated meta-learning (RIFL) methodology is broadly applicable and illustrated with three inference problems: aggregation of parametric models, high-dimensional prediction models, and inference for average treatment effects. We use RIFL to perform federated learning of mortality risk for patients hospitalized with COVID-19 using real-world EHR data from 15 healthcare centers representing 274 hospitals across four countries. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zijian Guo and Xiudi Li and Larry Han and Tianxi Cai},
  doi          = {10.1080/01621459.2024.2443246},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1695-1710},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust inference for federated meta-learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient estimation for longitudinal networks via adaptive merging. <em>JASA</em>, <em>120</em>(551), 1683-1694. (<a href='https://doi.org/10.1080/01621459.2025.2455202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal networks consist of sequences of temporal edges among multiple nodes, where the temporal edges are observed in real-time. They have become ubiquitous with the rise of online social platforms and e-commerce, but largely under-investigated in the literature. In this article, we propose an efficient estimation framework for longitudinal networks, leveraging strengths of adaptive network merging, tensor decomposition, and point processes. It merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. A thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the estimation error and also provides a guideline for network merging under various scenarios. We further demonstrate the advantage of the proposed method through extensive numerical experiments on synthetic datasets and a militarized interstate dispute dataset. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Haoran Zhang and Junhui Wang},
  doi          = {10.1080/01621459.2025.2455202},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1683-1694},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficient estimation for longitudinal networks via adaptive merging},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation-based, finite-sample inference for privatized data. <em>JASA</em>, <em>120</em>(551), 1669-1682. (<a href='https://doi.org/10.1080/01621459.2024.2427436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy protection methods, such as differentially private mechanisms, introduce noise into resulting statistics which often produces complex and intractable sampling distributions. In this article, we propose a simulation-based “repro sample” approach to produce statistically valid confidence intervals and hypothesis tests, which builds on the work of Xie and Wang. We show that this methodology is applicable to a wide variety of private inference problems, appropriately accounts for biases introduced by privacy mechanisms (such as by clamping), and improves over other state-of-the-art inference methods such as the parametric bootstrap in terms of the coverage and Type I error of the private inference. We also develop significant improvements and extensions for the repro sample methodology for general models (not necessarily related to privacy), including (a) modifying the procedure to ensure guaranteed coverage and Type I errors, even accounting for Monte Carlo error, and (b) proposing efficient numerical algorithms to implement the confidence intervals and p -values. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jordan Awan and Zhanyu Wang},
  doi          = {10.1080/01621459.2024.2427436},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1669-1682},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Simulation-based, finite-sample inference for privatized data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analysis for quantiles of hidden biases in matched observational studies. <em>JASA</em>, <em>120</em>(551), 1657-1668. (<a href='https://doi.org/10.1080/01621459.2024.2441527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal conclusions from observational studies may be sensitive to unmeasured confounding. In such cases, a sensitivity analysis is often conducted, which tries to infer the minimum amount of hidden biases or the minimum strength of unmeasured confounding needed in order to explain away the observed association between treatment and outcome. If the needed bias is large, then the treatment is likely to have significant effects. The Rosenbaum sensitivity analysis is a modern approach for conducting sensitivity analysis in matched observational studies. It investigates what magnitude the maximum of hidden biases from all matched sets needs to be in order to explain away the observed association. However, such a sensitivity analysis can be overly conservative and pessimistic, especially when investigators suspect that some matched sets may have exceptionally large hidden biases. In this article, we generalize Rosenbaum’s framework to conduct sensitivity analysis on quantiles of hidden biases from all matched sets, which are more robust than the maximum. Moreover, the proposed sensitivity analysis is simultaneously valid across all quantiles of hidden biases and is thus a free lunch added to the conventional sensitivity analysis. The proposed approach works for general outcomes, general matched studies and general test statistics. In addition, we demonstrate that the proposed sensitivity analysis also works for bounded null hypotheses when the test statistic satisfies certain properties. An R package implementing the proposed approach is available online. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Dongxiao Wu and Xinran Li},
  doi          = {10.1080/01621459.2024.2441527},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1657-1668},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sensitivity analysis for quantiles of hidden biases in matched observational studies},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Higher order accurate symmetric bootstrap confidence intervals in high dimensional penalized regression. <em>JASA</em>, <em>120</em>(551), 1645-1656. (<a href='https://doi.org/10.1080/01621459.2024.2445873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops methodology for higher order accurate two-sided Bootstrap confidence intervals (CIs) in high dimensional penalized regression models using the Bootstrap. We consider a large class of penalized regression methods that satisfy the Oracle property of Fan and Li and a stronger variant of it, called the Strong Oracle property. While second order accuracy of the Bootstrap is known for both classes, it is typically not sufficient to guarantee better accuracy of two-sided Bootstrap CIs over their Oracle limit based counterparts. In this article, we show that for penalization methods with the strong Oracle property (called “Class I” methods here), a variant of the two-sided symmetric Bootstrap CI method, originally proposed by Hall in the traditional fixed dimensional case, can attain an accuracy level of O ( n − 2 ) in sample size n , even if the dimension of the regression model grows at an arbitrary polynomial rate in n . On the other end, for penalized methods with only the Oracle property (called “Class II” methods here), two-sided symmetric Bootstrap CIs can achieve the same O ( n − 2 ) level of accuracy in such high dimensions only after some nontrivial modification. Consequently, the proposed methodology can be used to construct accurate two-sided CIs for the relevant regression parameters in very high dimensional regression models with a much smaller sample size than what has been considered possible in the literature. We also report results from a simulation study and illustrate the methodology with a real data example. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Debraj Das and Arindam Chatterjee and S. N. Lahiri},
  doi          = {10.1080/01621459.2024.2445873},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1645-1656},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Higher order accurate symmetric bootstrap confidence intervals in high dimensional penalized regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal prediction for network-assisted regression. <em>JASA</em>, <em>120</em>(551), 1633-1644. (<a href='https://doi.org/10.1080/01621459.2025.2506198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important problem in network analysis is predicting a node attribute using both network covariates, such as graph embedding coordinates or local subgraph counts, and conventional node covariates, such as demographic characteristics. While standard regression methods that make use of both types of covariates may be used for prediction, statistical inference is complicated by the fact that the nodal summary statistics are often dependent in complex ways. We show that under a mild joint exchangeability assumption, a network analog of conformal prediction achieves finite sample validity for a wide range of network covariates. We also show that a form of asymptotic conditional validity is achievable. The methods are illustrated on both simulated networks and a citation network dataset.},
  archive      = {J_JASA},
  author       = {Robert Lunde and Elizaveta Levina and Ji Zhu},
  doi          = {10.1080/01621459.2025.2506198},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1633-1644},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Conformal prediction for network-assisted regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling preferences: A bayesian mixture of finite mixtures for rankings and ratings. <em>JASA</em>, <em>120</em>(551), 1621-1632. (<a href='https://doi.org/10.1080/01621459.2024.2444700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rankings and ratings are commonly used to express preferences but provide distinct and complementary information. Rankings give ordinal and scale-free comparisons but lack granularity; ratings provide cardinal and granular assessments but may be highly subjective or inconsistent. Collecting and analyzing rankings and ratings jointly has not been performed until recently due to a lack of principled methods. In this work, we propose a flexible, joint statistical model for rankings and ratings—the Bradley-Terry-Luce-Binomial (BTL-Binomial). The model captures rater effects and preference heterogeneity, respectively, with judge-specific random effects and a latent class mixture framework where the number of classes is unknown a priori. We propose computationally-efficient estimation via a Bayesian mixture of finite mixtures (MFM) approach. Finally, we demonstrate statistical inference and decision-making based on rankings and ratings jointly through applications to real and simulated datasets in academic peer review. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Michael Pearce and Elena A. Erosheva},
  doi          = {10.1080/01621459.2024.2444700},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1621-1632},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling preferences: A bayesian mixture of finite mixtures for rankings and ratings},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint spectral clustering in multilayer degree-corrected stochastic blockmodels. <em>JASA</em>, <em>120</em>(551), 1607-1620. (<a href='https://doi.org/10.1080/01621459.2025.2516201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern network datasets are often composed of multiple layers, resulting in collections of networks over the same set of vertices but with potentially different connectivity patterns on each network. These data require models and methods that are flexible enough to capture local and global differences across the networks while at the same time being parsimonious and tractable to yield computationally efficient and theoretically sound solutions that are capable of aggregating information across the networks. This paper considers the multilayer degree-corrected stochastic blockmodel, where a collection of networks shares the same community structure, but degree corrections and block connection probability matrices are permitted to be different. We establish the identifiability of this model and propose a spectral clustering algorithm. Our theoretical results demonstrate that the misclustering error rate of the algorithm improves exponentially with multiple network realizations, even in the presence of significant layer heterogeneity. Simulation studies show that this approach improves on existing multilayer community detection methods in this challenging regime. Furthermore, in a case study of US airport data through January 2016 – September 2021, we find that this methodology identifies meaningful community structure and trends in airport popularity influenced by pandemic impacts on travel. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Joshua Agterberg and Zachary Lubberts and Jesús Arroyo},
  doi          = {10.1080/01621459.2025.2516201},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1607-1620},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Joint spectral clustering in multilayer degree-corrected stochastic blockmodels},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). U-statistic reduction: Higher-order accurate risk control and statistical-computational trade-off. <em>JASA</em>, <em>120</em>(551), 1593-1606. (<a href='https://doi.org/10.1080/01621459.2024.2448029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {U-statistics play central roles in many statistical learning tools but face the haunting issue of scalability. Despite extensive research on accelerating computation by U-statistic reduction, existing results almost exclusively focused on power analysis. Little work addresses risk control accuracy, which requires distinct and much more challenging techniques. In this article, we establish the first statistical inference procedure with provably higher-order accurate risk control for incomplete U-statistics. The sharpness of our new result enables us to reveal how risk control accuracy also trades off with speed, for the first time in literature, which complements the well-known variance-speed tradeoff. Our general framework converts the challenging and case-by-case analysis for many different designs into a surprisingly principled and routine computation. We conducted comprehensive numerical studies and observed results that validate our theory’s sharpness. Our method also demonstrates effectiveness on real-world data applications. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Meijia Shao and Dong Xia and Yuan Zhang},
  doi          = {10.1080/01621459.2024.2448029},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1593-1606},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {U-statistic reduction: Higher-order accurate risk control and statistical-computational trade-off},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency domain statistical inference for high-dimensional time series. <em>JASA</em>, <em>120</em>(551), 1580-1592. (<a href='https://doi.org/10.1080/01621459.2025.2479244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing time series in the frequency domain enables the development of powerful tools for investigating the second-order characteristics of multivariate processes. Parameters like the spectral density matrix and its inverse, the coherence or the partial coherence, encode comprehensively the complex linear relations between the component processes of the multivariate system. In this article, we develop inference procedures for such parameters in a high-dimensional, time series setup. Toward this goal, we first focus on the derivation of consistent estimators of the coherence and, more importantly, of the partial coherence which possess manageable limiting distributions that are suitable for testing purposes. Statistical tests of the hypothesis that the maximum over frequencies of the coherence, respectively, of the partial coherence, do not exceed a prespecified threshold value are developed. Our approach allows for testing hypotheses for individual coherences and/or partial coherences as well as for multiple testing of large sets of such parameters. In the latter case, a consistent procedure to control the false discovery rate is developed. The finite sample performance of the inference procedures introduced is investigated by means of simulations and applications to the construction of graphical interaction models for brain connectivity based on EEG data are presented.Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jonas Krampe and Efstathios Paparoditis},
  doi          = {10.1080/01621459.2025.2479244},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1580-1592},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Frequency domain statistical inference for high-dimensional time series},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic guarantees for bayesian phylogenetic tree reconstruction. <em>JASA</em>, <em>120</em>(551), 1569-1579. (<a href='https://doi.org/10.1080/01621459.2025.2485359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive tractable criteria for the consistency of Bayesian tree reconstruction procedures, which constitute a central class of algorithms for inferring common ancestry among DNA sequence samples in phylogenetics. Our results encompass several Bayesian algorithms in widespread use, such as BEAST , MrBayes , and RevBayes . Unlike essentially all existing asymptotic guarantees for tree reconstruction, we require no discretization or boundedness assumptions on branch lengths. Our results are also very flexible, and easy to adapt to variations of the underlying inference problem. We demonstrate the practicality of our criteria on two examples: a Kingman coalescent prior on rooted, ultrametric trees, and an independence prior on unconstrained binary trees, though we emphasize that our result also applies to nonbinary tree models. In both cases, the convergence rate we obtain matches known, frequentist results obtained using stronger boundedness assumptions, up to logarithmic factors. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Alisa Kirichenko and Luke J. Kelly and Jere Koskela},
  doi          = {10.1080/01621459.2025.2485359},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1569-1579},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Asymptotic guarantees for bayesian phylogenetic tree reconstruction},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of binary classifiers for asymptotically dependent and independent extremes. <em>JASA</em>, <em>120</em>(551), 1558-1568. (<a href='https://doi.org/10.1080/01621459.2025.2529024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning classification methods usually assume that all possible classes are sufficiently present within the training set. Due to their inherent rarities, extreme events are always under-represented and classifiers tailored for predicting extremes need to be carefully designed to handle this under-representation. In this article, we address the question of how to assess and compare classifiers with respect to their capacity to capture extreme occurrences. This is also related to the topic of scoring rules used in forecasting literature. In this context, we propose and study a risk function adapted to extremal classifiers. The inferential properties of our empirical risk estimator are derived under the framework of multivariate regular variation and hidden regular variation. A simulation study compares different classifiers and indicates their performance with respect to our risk function. To conclude, we apply our framework to the analysis of extreme river discharges in the Danube river basin. The application compares different predictive algorithms and test their capacity at forecasting river discharges from other river stations. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Juliette Legrand and Philippe Naveau and Marco Oesting},
  doi          = {10.1080/01621459.2025.2529024},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1558-1568},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Evaluation of binary classifiers for asymptotically dependent and independent extremes},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust bayesian modeling of counts with zero inflation and outliers: Theoretical robustness and efficient computation. <em>JASA</em>, <em>120</em>(551), 1545-1557. (<a href='https://doi.org/10.1080/01621459.2024.2447111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data with zero inflation and large outliers are ubiquitous in many scientific applications. However, posterior analysis under a standard statistical model, such as Poisson or negative binomial distribution, is sensitive to such contamination. This study introduces a novel framework for Bayesian modeling of counts that is robust to both zero inflation and large outliers. In doing so, we introduce rescaled beta distribution and adopt it to absorb undesirable effects from zero and outlying counts. The proposed approach has two appealing features: the efficiency of the posterior computation via a custom Gibbs sampling algorithm and a theoretically guaranteed posterior robustness, where extreme outliers are automatically removed from the posterior distribution. We demonstrate the usefulness of the proposed method by applying it to trend filtering and spatial modeling using predictive Gaussian processes. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yasuyuki Hamura and Kaoru Irie and Shonosuke Sugasawa},
  doi          = {10.1080/01621459.2024.2447111},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1545-1557},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust bayesian modeling of counts with zero inflation and outliers: Theoretical robustness and efficient computation},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When frictions are fractional: Rough noise in high-frequency data. <em>JASA</em>, <em>120</em>(551), 1531-1544. (<a href='https://doi.org/10.1080/01621459.2024.2428466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of high-frequency financial data is often impeded by the presence of noise. This article is motivated by intraday return data in which market microstructure noise appears to be rough , that is, best captured by a continuous-time stochastic process that locally behaves as fractional Brownian motion. Assuming that the underlying efficient price process follows a continuous Itô semimartingale, we derive consistent estimators and asymptotic confidence intervals for the roughness parameter of the noise and the integrated price and noise volatilities, in all cases where these quantities are identifiable. In addition to desirable features such as serial dependence of increments, compatibility between different sampling frequencies and diurnal effects, the rough noise model can further explain divergence rates in volatility signature plots that vary considerably over time and between assets. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Carsten H. Chong and Thomas Delerue and Guoying Li},
  doi          = {10.1080/01621459.2024.2428466},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1531-1544},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {When frictions are fractional: Rough noise in high-frequency data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributional off-policy evaluation in reinforcement learning. <em>JASA</em>, <em>120</em>(551), 1517-1530. (<a href='https://doi.org/10.1080/01621459.2025.2506197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the literature of reinforcement learning (RL), off-policy evaluation is mainly focused on estimating a value of a target policy given the pre-collected data generated by some behavior policy. Motivated by the recent success of distributional RL in many practical applications, we study the distributional off-policy evaluation problem in the batch setting when the reward is multi-variate. We propose an offline Wasserstein-based approach to simultaneously estimate the joint distribution of a multivariate discounted cumulative reward given any initial state-action pair in the setting of an infinite-horizon Markov decision process. Finite sample error bound for the proposed estimator with respect to a modified Wasserstein metric is established in terms of both the number of trajectories and the number of decision points on each trajectory in the batch data. Extensive numerical studies are conducted to demonstrate the superior performance of our proposed method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zhengling Qi and Chenjia Bai and Zhaoran Wang and Lan Wang},
  doi          = {10.1080/01621459.2025.2506197},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1517-1530},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distributional off-policy evaluation in reinforcement learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated adaptive causal estimation (FACE) of target treatment effects. <em>JASA</em>, <em>120</em>(551), 1503-1516. (<a href='https://doi.org/10.1080/01621459.2025.2453249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning of causal estimands may greatly improve estimation efficiency by leveraging data from multiple study sites, but robustness to heterogeneity and model misspecifications is vital for ensuring validity. We develop a Federated Adaptive Causal Estimation (FACE) framework to incorporate heterogeneous data from multiple sites to provide treatment effect estimation and inference for a flexibly specified target population of interest. FACE accounts for site-level heterogeneity in the distribution of covariates through density ratio weighting. To safely incorporate source sites and avoid negative transfer, we introduce an adaptive weighting procedure via a penalized regression, which achieves both consistency and optimal efficiency. Our strategy is communication-efficient and privacy-preserving, allowing participating sites to share summary statistics only once with other sites. We conduct both theoretical and numerical evaluations of FACE and apply it to conduct a comparative effectiveness study of BNT162b2 (Pfizer) and mRNA-1273 (Moderna) vaccines on COVID-19 outcomes in U.S. veterans using electronic health records from five VA regional sites. We show that compared to traditional methods, FACE meaningfully increases the precision of treatment effect estimates, with reductions in standard errors ranging from 26 % to 67 % . Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Larry Han and Jue Hou and Kelly Cho and Rui Duan and Tianxi Cai},
  doi          = {10.1080/01621459.2025.2453249},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1503-1516},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Federated adaptive causal estimation (FACE) of target treatment effects},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling hypergraphs with diversity and heterogeneous popularity. <em>JASA</em>, <em>120</em>(551), 1491-1502. (<a href='https://doi.org/10.1080/01621459.2025.2455200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While relations among individuals make an important part of data with scientific and business interests, existing statistical modeling of relational data has mainly been focusing on dyadic relations, that is, those between two individuals. This article addresses the less studied, though commonly encountered, polyadic relations that can involve more than two individuals. In particular, we propose a new latent space model for hypergraphs using determinantal point processes, which is driven by the diversity within hyperedges and each node’s popularity. This model mechanism is in contrast to existing hypergraph models, which are predominantly driven by similarity rather than diversity. Additionally, the proposed model accommodates broad types of hypergraphs, with no restriction on the cardinality and multiplicity of hyperedges, which previous models often have. Consistency and asymptotic normality of the maximum likelihood estimates of the model parameters have been established. The proof is challenging, owing to the special configuration of the parameter space. Further, we apply the projected accelerated gradient descent algorithm to obtain the parameter estimates, and we show its effectiveness in simulation studies. We also demonstrate an application of the proposed model on the What’s Cooking data and present the embedding of food ingredients learned from cooking recipes using the model. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xianshi Yu and Ji Zhu},
  doi          = {10.1080/01621459.2025.2455200},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1491-1502},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling hypergraphs with diversity and heterogeneous popularity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical and computational efficiency for smooth tensor estimation with unknown permutations. <em>JASA</em>, <em>120</em>(551), 1477-1490. (<a href='https://doi.org/10.1080/01621459.2024.2419114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of structured tensor denoising in the presence of unknown permutations. Such data problems arise commonly in recommendation systems, neuroimaging, community detection, and multiway comparison applications. Here, we develop a general family of smooth tensor models up to arbitrary index permutations; the model incorporates the popular tensor block models and Lipschitz hypergraphon models as special cases. We show that a constrained least-squares estimator in the block-wise polynomial family achieves the minimax error bound. A phase transition phenomenon is revealed with respect to the smoothness threshold needed for optimal recovery. In particular, we find that a polynomial of degree up to ( m − 2 ) ( m + 1 ) / 2 is sufficient for accurate recovery of order- m tensors, whereas higher degrees exhibit no further benefits. This phenomenon reveals the intrinsic distinction for smooth tensor estimation problems with and without unknown permutations. Furthermore, we provide an efficient polynomial-time Borda count algorithm that provably achieves the optimal rate under monotonicity assumptions. The efficacy of our procedure is demonstrated through both simulations and Chicago crime data analysis. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Chanwoo Lee and Miaoyan Wang},
  doi          = {10.1080/01621459.2024.2419114},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1477-1490},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical and computational efficiency for smooth tensor estimation with unknown permutations},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel spectral joint embeddings for high-dimensional noisy datasets using duo-landmark integral operators. <em>JASA</em>, <em>120</em>(551), 1463-1476. (<a href='https://doi.org/10.1080/01621459.2025.2539539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrative analysis of multiple heterogeneous datasets has arised in many research fields. Existing approaches oftentimes suffer from limited power in capturing nonlinear structures, insufficient account of noisiness and effects of high-dimensionality, lack of adaptivity to signals and sample sizes imbalance, and their results are sometimes difficult to interpret. To address these limitations, we propose a kernel spectral method that achieves joint embeddings of two independently observed high-dimensional noisy datasets. The proposed method automatically captures and leverages shared low-dimensional structures across datasets to enhance embedding quality. The obtained low-dimensional embeddings can be used for downstream tasks such as simultaneous clustering, data visualization, and denoising. The proposed method is justified by rigorous theoretical analysis, which guarantees its consistency in capturing the signal structures, and provides a geometric interpretation of the embeddings. Under a joint manifolds model framework, we establish the convergence of the embeddings to the eigenfunctions of some natural integral operators. These operators, referred to as duo-landmark integral operators, are defined by the convolutional kernel maps of some reproducing kernel Hilbert spaces (RKHSs). These RKHSs capture the underlying, shared low-dimensional nonlinear signal structures between the two datasets. Our numerical experiments and analyses of two pairs of single-cell omics datasets demonstrate the empirical advantages of the proposed method over existing methods in both embeddings and several downstream tasks. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xiucai Ding and Rong Ma},
  doi          = {10.1080/01621459.2025.2539539},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1463-1476},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Kernel spectral joint embeddings for high-dimensional noisy datasets using duo-landmark integral operators},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal transport based cross-domain integration for heterogeneous data. <em>JASA</em>, <em>120</em>(551), 1449-1462. (<a href='https://doi.org/10.1080/01621459.2025.2540653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting dynamic patterns shared across heterogeneous datasets is a critical yet challenging task in many scientific domains, particularly within the biomedical sciences. Systematic heterogeneity inherent in diverse data sources can significantly hinder the effectiveness of existing machine learning methods in uncovering shared underlying dynamics. Additionally, practical and technical constraints in real-world experimental designs often limit data collection to only a small number of subjects, even when rich, time-dependent measurements are available for each individual. These limited sample sizes further diminish the power to detect common dynamic patterns across subjects. In this article, we propose a novel heterogeneous data integration framework based on optimal transport to extract shared patterns in the conditional mean dynamics of target responses. The key advantage of the proposed method is its ability to enhance discriminative power by reducing heterogeneity unrelated to the signal. This is achieved through the alignment of extracted domain-shared temporal information across multiple datasets from different domains. Our approach is effective regardless of the number of datasets and does not require auxiliary matching information for alignment. Specifically, the method aligns longitudinal data from heterogeneous datasets within a common latent space, capturing shared dynamic patterns while leveraging temporal dependencies within subjects. Theoretically, we establish generalization error bounds for the proposed data integration approach in supervised learning tasks, highlighting a novel tradeoff between data alignment and pattern learning. Additionally, we derive convergence rates for the barycentric projection under Gromov-Wasserstein and fused Gromov-Wasserstein distances. Numerical studies on both simulated data and neuroscience applications demonstrate that the proposed data integration framework substantially improves prediction accuracy by effectively aggregating information across diverse data sources and subjects. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yubai Yuan and Yijiao Zhang and Babak Shahbaba and Norbert Fortin and Keiland Cooper and Qing Nie and Annie Qu},
  doi          = {10.1080/01621459.2025.2540653},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1449-1462},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal transport based cross-domain integration for heterogeneous data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep fréchet regression. <em>JASA</em>, <em>120</em>(551), 1437-1448. (<a href='https://doi.org/10.1080/01621459.2025.2507982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in modern science have led to the increasing availability of non-Euclidean data in metric spaces. This article addresses the challenge of modeling relationships between non-Euclidean responses and multivariate Euclidean predictors. We propose a flexible regression model capable of handling high-dimensional predictors without imposing parametric assumptions. Two primary challenges are addressed: the curse of dimensionality in nonparametric regression and the absence of linear structure in general metric spaces. The former is tackled using deep neural networks, while for the latter we demonstrate the feasibility of mapping the metric space where responses reside to a low-dimensional Euclidean space using manifold learning. We introduce a reverse mapping approach, employing local Fréchet regression, to map the low-dimensional manifold representations back to objects in the original metric space. We develop a theoretical framework, investigating the convergence rate of deep neural networks under dependent sub-Gaussian noise with bias. The convergence rate of the proposed regression model is then obtained by expanding the scope of local Fréchet regression to accommodate multivariate predictors in the presence of errors in predictors. Simulations and case studies show that the proposed model outperforms existing methods for non-Euclidean responses, focusing on the special cases of probability distributions and networks. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Su I Iao and Yidong Zhou and Hans-Georg Müller},
  doi          = {10.1080/01621459.2025.2507982},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1437-1448},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Deep fréchet regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Debiasing watermarks for large language models via maximal coupling. <em>JASA</em>, <em>120</em>(551), 1424-1436. (<a href='https://doi.org/10.1080/01621459.2025.2520455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Watermarking language models is essential for distinguishing between human and machine-generated text and thus maintaining the integrity and trustworthiness of digital communication. We present a novel green/red list watermarking approach that partitions the token set into “green” and “red” lists, subtly increasing the generation probability for green tokens. To correct token distribution bias, our method employs maximal coupling, using a uniform coin flip to decide whether to apply bias correction, with the result embedded as a pseudorandom watermark signal. Theoretical analysis confirms this approach’s unbiased nature and robust detection capabilities. Experimental results show that it outperforms prior techniques by preserving text quality while maintaining high detectability, and it demonstrates resilience to targeted modifications aimed at improving text quality. This research provides a promising watermarking solution for language models, balancing effective detection with minimal impact on text quality. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yangxinyu Xie and Xiang Li and Tanwi Mallick and Weijie Su and Ruixun Zhang},
  doi          = {10.1080/01621459.2025.2520455},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1424-1436},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Debiasing watermarks for large language models via maximal coupling},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Who are we missing?: A principled approach to characterizing the underrepresented population. <em>JASA</em>, <em>120</em>(551), 1414-1423. (<a href='https://doi.org/10.1080/01621459.2025.2495319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our article addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We apply our methodology to extend inferences from the Starting Treatment with Agonist Replacement Therapies (START) trial—investigating the effectiveness of medication for opioid use disorder—to the real-world population represented by the Treatment Episode Dataset: Admissions (TEDS-A). By refining target populations using ROOT, our framework offers a systematic approach to enhance decision-making accuracy and inform future trials in diverse populations. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Harsh Parikh and Rachael K. Ross and Elizabeth Stuart and Kara E. Rudolph},
  doi          = {10.1080/01621459.2025.2495319},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1414-1423},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Who are we missing?: A principled approach to characterizing the underrepresented population},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating heterogeneous causal mediation effects with bayesian decision tree ensembles. <em>JASA</em>, <em>120</em>(551), 1400-1413. (<a href='https://doi.org/10.1080/01621459.2025.2491155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The causal inference literature has increasingly recognized that targeting treatment effect heterogeneity can lead to improved scientific understanding and policy recommendations. Similarly, studying the causal pathway connecting the treatment to the outcome can be useful. We address these problems in the context of causal mediation analysis . We introduce a varying coefficient model based on Bayesian additive regression trees to estimate and regularize heterogeneous causal mediation effects. Even on large datasets with few covariates, we show LSEMs can produce highly unstable estimates of the conditional average direct and indirect effects, while our Bayesian causal mediation forests model produces stable estimates. We find that our approach is conservative, with effect estimates “shrunk towards homogeneity.” Using data from the Medical Expenditure Panel Survey and empirically-grounded simulated data, we examine the salient properties of our method. Finally, we show how our model can be combined with posterior summarization strategies to identify interesting subgroups and interpret the model fit. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Angela Ting and Antonio R. Linero},
  doi          = {10.1080/01621459.2025.2491155},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1400-1413},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating heterogeneous causal mediation effects with bayesian decision tree ensembles},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safe policy learning through extrapolation: Application to pre-trial risk assessment. <em>JASA</em>, <em>120</em>(551), 1386-1399. (<a href='https://doi.org/10.1080/01621459.2025.2489135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic recommendations and decisions have become ubiquitous in today’s society. Many of these data-driven policies, especially in the realm of public policy, are based on known, deterministic rules to ensure their transparency and interpretability. We examine a particular case of algorithmic pre-trial risk assessments in the US criminal justice system, which provide deterministic classification scores and recommendations to help judges make release decisions. Our goal is to analyze data from a unique field experiment on an algorithmic pre-trial risk assessment to investigate whether the scores and recommendations can be improved. Unfortunately, prior methods for policy learning are not applicable because they require existing policies to be stochastic. We develop a maximin robust optimization approach that partially identifies the expected utility of a policy, and then finds a policy that maximizes the worst-case expected utility. The resulting policy has a statistical safety property, limiting the probability of producing a worse policy than the existing one, under structural assumptions about the outcomes. Our analysis of data from the field experiment shows that we can safely improve certain components of the risk assessment instrument by classifying arrestees as lower risk under a wide range of utility specifications, though the analysis is not informative about several components of the instrument. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Eli Ben-Michael and D. James Greiner and Kosuke Imai and Zhichao Jiang},
  doi          = {10.1080/01621459.2025.2489135},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1386-1399},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Safe policy learning through extrapolation: Application to pre-trial risk assessment},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of cognitive function via brain region volumes with applications to alzheimer’s disease based on space-factor-guided functional principal component analysis. <em>JASA</em>, <em>120</em>(551), 1373-1385. (<a href='https://doi.org/10.1080/01621459.2025.2479220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a prevalent and irreversible brain disorder and early prediction of cognitive function is vital for detecting the onset. The volumes of brain regions can serve as features for predicting cognitive decline, facilitating early detection and intervention. In order to offer a comprehensive representation of brain tissue changes in AD, we employ volume density curves to investigate the relationship between brain regions and cognitive function. However, analyzing these volume curves is complex due to their highly spatial and intrinsic dependence and piecewise structure. To address these challenges, we propose Space-Factor-Guided Functional Principal Component Analysis (SF-FPCA). This method uses factor processes to extract low-dimensional features for intrinsic correlations among regions of interest (ROIs) and applies Functional Principal Component Analysis (FPCA) to these processes to address temporal dependence. Furthermore, by decomposing the loadings into smooth functions of spatial coordinates and a piecewise constant matrix, we identify regions exhibiting smoothness within each region while discontinuities between these regions. We apply SF-FPCA to analyze data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI). Our results demonstrate that SF-FPCA provides the best fit compared to other methods. In addition, features extracted from volume curves using SF-FPCA enable more accurate prediction of cognitive function compared to scalar volumes alone. Leveraging these extracted features, we identify 36 important ROIs influencing cognitive decline. Our investigation into brain atrophy also reveals distinct mechanisms between the left and right hemispheres, shedding light on the nuanced effects of brain region changes on cognitive decline in AD. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shoudao Wen and Yi Li and Dehan Kong and Huazhen Lin},
  doi          = {10.1080/01621459.2025.2479220},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1373-1385},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Prediction of cognitive function via brain region volumes with applications to alzheimer’s disease based on space-factor-guided functional principal component analysis},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast signal region detection with application to whole genome association studies. <em>JASA</em>, <em>120</em>(551), 1360-1372. (<a href='https://doi.org/10.1080/01621459.2025.2464271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on the localization of the genetic basis associated with diseases or traits has been widely conducted in the last few decades. Scan methods have been developed for region-based analysis in whole-genome association studies, helping us better understand how genetics influences human diseases or traits, especially when the aggregated effects of multiple causal variants are present. In this paper, we propose a fast and effective algorithm coupling with high-dimensional test for simultaneously detecting multiple signal regions, which is distinct from existing methods using scan or knockoff statistics. The idea is to conduct binary splitting with re-search and arrangement based on a sequence of dynamic critical values to increase detection accuracy and reduce computation. Theoretical and empirical studies demonstrate that our approach enjoys favorable theoretical guarantees with fewer restrictions and exhibits superior numerical performance with faster computation. Utilizing the UK Biobank data to identify the genetic regions related to breast cancer, we confirm previous findings and meanwhile, identify a number of new regions that suggest strong associations with risk of breast cancer and deserve further investigation. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Wei Zhang and Fan Wang and Fang Yao},
  doi          = {10.1080/01621459.2025.2464271},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1360-1372},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fast signal region detection with application to whole genome association studies},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributional outcome regression via quantile functions and its application to modelling continuously monitored heart rate and physical activity. <em>JASA</em>, <em>120</em>(551), 1347-1359. (<a href='https://doi.org/10.1080/01621459.2025.2460232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern clinical and epidemiological studies widely employ wearables to record parallel streams of real-time data on human physiology and behavior. With recent advances in distributional data analysis, these high-frequency data are now often treated as distributional observations resulting in novel regression settings. Motivated by these modeling setups, we develop a distributional outcome regression via quantile functions (DORQF) that expands existing literature with three key contributions: (i) handling both scalar and distributional predictors, (ii) ensuring jointly monotone regression structure without enforcing monotonicity on individual functional regression coefficients, (iii) providing statistical inference via asymptotic projection-based joint confidence bands and a statistical test of global significance to quantify uncertainty of the estimated functional regression coefficients. The method is motivated by and applied to Actiheart component of Baltimore Longitudinal Study of Aging that collected one week of minute-level heart rate (HR) and physical activity (PA) data on 781 older adults to gain deeper understanding of age-related changes in daily life heart rate reserve, defined as a distribution of daily HR, while accounting for daily distribution of physical activity, age, gender, and body composition. Intriguingly, the results provide novel insights in epidemiology of daily life heart rate reserve. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Rahul Ghosal and Sujit K. Ghosh and Jennifer A. Schrack and Vadim Zipunnikov},
  doi          = {10.1080/01621459.2025.2460232},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1347-1359},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distributional outcome regression via quantile functions and its application to modelling continuously monitored heart rate and physical activity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating heterogeneous exposure effects in the case-crossover design using BART. <em>JASA</em>, <em>120</em>(551), 1335-1346. (<a href='https://doi.org/10.1080/01621459.2025.2460231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiological approaches for examining human health responses to environmental exposures in observational studies often control for confounding by implementing clever matching schemes and using statistical methods based on conditional likelihood. Nonparametric regression models have surged in popularity in recent years as a tool for estimating individual-level heterogeneous effects, which provide a more detailed picture of the exposure–response relationship but can also be aggregated to obtain improved marginal estimates at the population level. In this work we incorporate Bayesian additive regression trees (BART) into the conditional logistic regression model to identify heterogeneous exposure effects in a case-crossover design. Conditional logistic BART (CL-BART) uses reversible jump Markov chain Monte Carlo to bypass the conditional conjugacy requirement of the original BART algorithm. Our work is motivated by the growing interest in identifying subpopulations more vulnerable to environmental exposures. We apply CL-BART to a study of the impact of heat waves on people with Alzheimer’s disease in California and effect modification by other chronic conditions. Through this application, we also describe strategies to examine heterogeneous odds ratios through variable importance, partial dependence, and lower-dimensional summaries. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jacob R. Englert and Stefanie T. Ebelt and Howard H. Chang},
  doi          = {10.1080/01621459.2025.2460231},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1335-1346},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating heterogeneous exposure effects in the case-crossover design using BART},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying genetic variants for brain connectivity using ball covariance ranking and aggregation. <em>JASA</em>, <em>120</em>(551), 1323-1334. (<a href='https://doi.org/10.1080/01621459.2025.2450837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the genetic architecture of brain functions is essential to clarify the biological etiologies of behavioral and psychiatric disorders. Functional connectivity, representing pairwise correlations of neural activities between brain regions, is moderately heritable. Current methods to identify single nucleotide polymorphisms (SNPs) linked to functional connectivity either neglect the complex structure of functional connectivity or fail to control false discoveries. Therefore, we propose a SNP-set hypothesis test, Ball Covariance Ranking and Aggregation (BCRA), to select and test the significance of SNP sets related to functional connectivity, incorporating matrix structure and controlling false discovery rate. Additionally, we present subsample-BCRA, a faster version for large-scale datasets. Simulation studies show both methods effectively detect SNPs with interactive structures, with subsample-BCRA shortening the running time by 700 folds. Applying our method to UK Biobank data from 34,129 individuals, we identify 10 SNP-sets with 29 SNPs significantly impacting functional connectivity. Gene-based analyses reveal three SNPs as eQTLs of gene NBPF15 , known to change functional connectivity. We also detect nine novel genes associated with behavioral and psychiatric disorders, whose connections to brain functions remain unexplored. Our findings improve our understanding of the genetic basis for brain connectivity and showcase our method’s utility for broader applications. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Wei Dai and Heping Zhang},
  doi          = {10.1080/01621459.2025.2450837},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {551},
  pages        = {1323-1334},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Identifying genetic variants for brain connectivity using ball covariance ranking and aggregation},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Objective bayesian inference. <em>JASA</em>, <em>120</em>(550), 1321-1322. (<a href='https://doi.org/10.1080/01621459.2025.2454051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jaeyong Lee},
  doi          = {10.1080/01621459.2025.2454051},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1321-1322},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Objective bayesian inference},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soccer analytics: An introduction using r. <em>JASA</em>, <em>120</em>(550), 1320-1321. (<a href='https://doi.org/10.1080/01621459.2024.2435110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Alexander Aue},
  doi          = {10.1080/01621459.2024.2435110},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1320-1321},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Soccer analytics: An introduction using r},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handbook of bayesian, fiducial, and frequentist inference. <em>JASA</em>, <em>120</em>(550), 1318-1320. (<a href='https://doi.org/10.1080/01621459.2025.2454048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Mengyang Gu},
  doi          = {10.1080/01621459.2025.2454048},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1318-1320},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of bayesian, fiducial, and frequentist inference},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep regression learning with optimal loss function. <em>JASA</em>, <em>120</em>(550), 1305-1317. (<a href='https://doi.org/10.1080/01621459.2024.2412364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a novel efficient and robust nonparametric regression estimator under a framework of a feedforward neural network (FNN). There are several interesting characteristics for the proposed estimator. First, the loss function is built upon an estimated maximum likelihood function, which integrates the information from observed data as well as the information from the data distribution. Consequently, the resulting estimator has desirable optimal properties, such as efficiency. Second, different from the traditional maximum likelihood estimation (MLE), the proposed method avoids the specification of the distribution, making it adaptable to various distributions such as heavy tails and multimodal or heterogeneous distributions. Third, the proposed loss function relies on probabilities rather than direct observations as in least square loss, contributing to the robustness of the proposed estimator. Finally, the proposed loss function involves a nonparametric regression function only. This enables the direct application of the existing packages, simplifying the computational and programming requirements. We establish the large sample property of the proposed estimator in terms of its excess risk and minimax near-optimal rate. The theoretical results demonstrate that the proposed estimator is equivalent to the true MLE where the density function is known in terms of excess risk. Our simulation studies show that the proposed estimator outperforms the existing methods based on prediction accuracy, efficiency and robustness. Particularly, it is comparable to the MLE with the known density and even gets slightly better as the sample size increases. This implies that the adaptive and data-driven loss function from the estimated density may offer an additional avenue for capturing valuable information. We further apply the proposed method to four real data examples, resulting in significantly reduced out-of-sample prediction errors compared to existing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xuancheng Wang and Ling Zhou and Huazhen Lin},
  doi          = {10.1080/01621459.2024.2412364},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1305-1317},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Deep regression learning with optimal loss function},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust permutation tests in linear instrumental variables regression. <em>JASA</em>, <em>120</em>(550), 1294-1304. (<a href='https://doi.org/10.1080/01621459.2024.2412363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops permutation versions of identification-robust tests in linear instrumental variables regression. Unlike the existing randomization and rank-based tests in which independence between the instruments and the error terms is assumed, the permutation Anderson-Rubin (AR), Lagrange Multiplier (LM) and Conditional Likelihood Ratio (CLR) tests are asymptotically similar and robust to conditional heteroscedasticity under standard exclusion restriction, that is, the orthogonality between the instruments and the error terms. Moreover, when the instruments are independent of the structural error term, the permutation AR tests are exact, hence, robust to heavy tails. As such, these tests share the strengths of the rank-based tests and the wild bootstrap AR tests. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Purevdorj Tuvaandorj},
  doi          = {10.1080/01621459.2024.2412363},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1294-1304},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust permutation tests in linear instrumental variables regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sparse beta regression model for network analysis. <em>JASA</em>, <em>120</em>(550), 1281-1293. (<a href='https://doi.org/10.1080/01621459.2024.2411073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For statistical analysis of network data, the 𝛽 -model has emerged as a useful tool, thanks to its flexibility in incorporating nodewise heterogeneity and theoretical tractability. To generalize the β -model, this article proposes the Sparse β -Regression Model (S β RM) that unites two research themes developed recently in modeling homophily and sparsity. In particular, we employ differential heterogeneity that assigns weights only to important nodes and propose penalized likelihood with an l 1 penalty for parameter estimation. While our estimation method is closely related to the LASSO method for logistic regression, we develop a new theory emphasizing the use of our model for dealing with a parameter regime that can handle sparse networks usually seen in practice. More interestingly, the resulting inference on the homophily parameter demands no debiasing normally employed in LASSO type estimation. We provide extensive simulation and data analysis to illustrate the use of the model. As a special case of our model, we extend the Erdős-Rényi model by including covariates and develop the associated statistical inference for sparse networks, which may be of independent interest. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Stefan Stein and Rui Feng and Chenlei Leng},
  doi          = {10.1080/01621459.2024.2411073},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1281-1293},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A sparse beta regression model for network analysis},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative methods for vecchia-laplace approximations for latent gaussian process models. <em>JASA</em>, <em>120</em>(550), 1267-1280. (<a href='https://doi.org/10.1080/01621459.2024.2410004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent Gaussian process (GP) models are flexible probabilistic nonparametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations can thus become prohibitively slow precisely when the approximations are usually the most accurate, that is, on large datasets. In this article, we present iterative methods to overcome this drawback. Among other things, we introduce and analyze several preconditioners, derive new convergence results, and propose novel methods for accurately approximating predictive variances. We analyze our proposed methods theoretically and in experiments with simulated and real-world data. In particular, we obtain a speed-up of an order of magnitude compared to Cholesky-based calculations and a 3-fold increase in prediction accuracy in terms of the continuous ranked probability score compared to a state-of-the-art method on a large satellite dataset. All methods are implemented in a free C++ software library with high-level Python and R packages. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Pascal Kündig and Fabio Sigrist},
  doi          = {10.1080/01621459.2024.2410004},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1267-1280},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Iterative methods for vecchia-laplace approximations for latent gaussian process models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive learning of the latent space of wasserstein generative adversarial networks. <em>JASA</em>, <em>120</em>(550), 1254-1266. (<a href='https://doi.org/10.1080/01621459.2024.2408778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models based on latent variables, such as generative adversarial networks (GANs) and variational auto-encoders (VAEs), have gained lots of interests due to their impressive performance in many fields. However, many data such as natural images usually do not populate the ambient Euclidean space but instead reside in a lower-dimensional manifold. Thus an inappropriate choice of the latent dimension fails to uncover the structure of the data, possibly resulting in mismatch of latent representations and poor generative qualities. Toward addressing these problems, we propose a novel framework called the latent Wasserstein GAN (LWGAN) that fuses the Wasserstein auto-encoder and the Wasserstein GAN so that the intrinsic dimension of the data manifold can be adaptively learned by a modified informative latent distribution. We prove that there exist an encoder network and a generator network in such a way that the intrinsic dimension of the learned encoding distribution is equal to the dimension of the data manifold. We theoretically establish that our estimated intrinsic dimension is a consistent estimate of the true dimension of the data manifold. Meanwhile, we provide an upper bound on the generalization error of LWGAN, implying that we force the synthetic data distribution to be similar to the real data distribution from a population perspective. Comprehensive empirical experiments verify our framework and show that LWGAN is able to identify the correct intrinsic dimension under several scenarios, and simultaneously generate high-quality synthetic data by sampling from the learned latent distribution. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yixuan Qiu and Qingyi Gao and Xiao Wang},
  doi          = {10.1080/01621459.2024.2408778},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1254-1266},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Adaptive learning of the latent space of wasserstein generative adversarial networks},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring covariance structure from multiple data sources via subspace factor analysis. <em>JASA</em>, <em>120</em>(550), 1239-1253. (<a href='https://doi.org/10.1080/01621459.2024.2408777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor analysis provides a canonical framework for imposing lower-dimensional structure such as sparse covariance in high-dimensional data. High-dimensional data on the same set of variables are often collected under different conditions, for instance in reproducing studies across research groups. In such cases, it is natural to seek to learn the shared versus condition-specific structure. Existing hierarchical extensions of factor analysis have been proposed, but face practical issues including identifiability problems. To address these shortcomings, we propose a class of SUbspace Factor Analysis (SUFA) models, which characterize variation across groups at the level of a lower-dimensional subspace. We prove that the proposed class of SUFA models lead to identifiability of the shared versus group-specific components of the covariance, and study their posterior contraction properties. Taking a Bayesian approach, these contributions are developed alongside efficient posterior computation algorithms. Our sampler fully integrates out latent variables, is easily parallelizable and has complexity that does not depend on sample size. We illustrate the methods through application to integration of multiple gene expression datasets relevant to immunology. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Noirrit Kiran Chandra and David B. Dunson and Jason Xu},
  doi          = {10.1080/01621459.2024.2408777},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1239-1253},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inferring covariance structure from multiple data sources via subspace factor analysis},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model-agnostic graph neural network for integrating local and global information. <em>JASA</em>, <em>120</em>(550), 1225-1238. (<a href='https://doi.org/10.1080/01621459.2024.2404668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, however, existing GNNs suffer from two significant limitations: a lack of interpretability in their results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to effectively integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and demonstrate its power to represent layer-wise neighborhood mixing. We conduct comprehensive numerical studies using simulated data to demonstrate the superior performance of MaGNet in comparison to several state-of-the-art alternatives. Furthermore, we apply MaGNet to a real-world case study aimed at extracting task-critical information from brain activity data, thereby highlighting its effectiveness in advancing scientific research. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Wenzhuo Zhou and Annie Qu and Keiland W. Cooper and Norbert Fortin and Babak Shahbaba},
  doi          = {10.1080/01621459.2024.2404668},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1225-1238},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A model-agnostic graph neural network for integrating local and global information},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating higher-order mixed memberships via the l2,∞ tensor perturbation bound. <em>JASA</em>, <em>120</em>(550), 1214-1224. (<a href='https://doi.org/10.1080/01621459.2024.2404265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher-order multiway data is ubiquitous in machine learning and statistics and often exhibits community-like structures, where each component (node) along each different mode has a community membership associated with it. In this article we propose the sub-Gaussian) tensor mixed-membership blockmodel , a generalization of the tensor blockmodel positing that memberships need not be discrete, but instead are convex combinations of latent communities. We establish the identifiability of our model and propose a computationally efficient estimation procedure based on the higher-order orthogonal iteration algorithm (HOOI) for tensor SVD composed with a simplex corner-finding algorithm. We then demonstrate the consistency of our estimation procedure by providing a per-node error bound under sub-Gaussian noise, which showcases the effect of higher-order structures on estimation accuracy. To prove our consistency result, we develop the l 2 , ∞ tensor perturbation bound for HOOI under independent, heteroscedastic, sub-Gaussian noise that may be of independent interest. Our analysis uses a novel leave-one-out construction for the iterates, and our bounds depend only on spectral properties of the underlying low-rank tensor under nearly optimal signal-to-noise ratio conditions such that tensor SVD is computationally feasible. Finally, we apply our methodology to real and simulated data, demonstrating some effects not identifiable from the model with discrete community memberships. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Joshua Agterberg and Anru R. Zhang},
  doi          = {10.1080/01621459.2024.2404265},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1214-1224},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating higher-order mixed memberships via the l2,∞ tensor perturbation bound},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive transfer learning framework for functional classification. <em>JASA</em>, <em>120</em>(550), 1201-1213. (<a href='https://doi.org/10.1080/01621459.2024.2403788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the transfer learning problem in functional classification, aiming to improve the classification accuracy of the target data by leveraging information from related source datasets. To facilitate transfer learning, we propose a novel transferability function tailored for classification problems, enabling a more accurate evaluation of the similarity between source and target dataset distributions. Interestingly, we find that a source dataset can offer more substantial benefits under certain conditions than another dataset with an identical distribution to the target dataset. This observation renders the commonly-used debiasing step in the parameter-based transfer learning algorithm unnecessary under some circumstances to the classification problem. In particular, we propose two adaptive transfer learning algorithms based on the functional Distance Weighted Discrimination (DWD) classifier for scenarios with and without prior knowledge regarding informative sources. Furthermore, we establish the upper bound on the excess risk of the proposed classifiers, providing the statistical gain via transfer learning mathematically provable. Simulation studies are conducted to thoroughly examine the finite-sample performance of the proposed algorithms. Finally, we implement the proposed method to Beijing air-quality data, and significantly improve the prediction of the PM 2.5 level of a target station by effectively incorporating information from source datasets. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Caihong Qin and Jinhan Xie and Ting Li and Yang Bai},
  doi          = {10.1080/01621459.2024.2403788},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1201-1213},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An adaptive transfer learning framework for functional classification},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale low-rank gaussian process prediction with support points. <em>JASA</em>, <em>120</em>(550), 1189-1200. (<a href='https://doi.org/10.1080/01621459.2024.2403188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank approximation is a popular strategy to tackle the “big n problem” associated with large-scale Gaussian process regressions. Basis functions for developing low-rank structures are crucial and should be carefully specified. Predictive processes simplify the problem by inducing basis functions with a covariance function and a set of knots. The existing literature suggests certain practical implementations of knot selection and covariance estimation; however, theoretical foundations explaining the influence of these two factors on predictive processes are lacking. In this article, the asymptotic prediction performance of the predictive process and Gaussian process predictions are derived and the impacts of the selected knots and estimated covariance are studied. The use of support points as knots, which best represent data locations, is advocated. Extensive simulation studies demonstrate the superiority of support points and verify our theoretical results. Real data of precipitation and ozone are used as examples, and the efficiency of our method over other widely used low-rank approximation methods is verified. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yan Song and Wenlin Dai and Marc G. Genton},
  doi          = {10.1080/01621459.2024.2403188},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1189-1200},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Large-scale low-rank gaussian process prediction with support points},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based clustering of categorical data based on the hamming distance. <em>JASA</em>, <em>120</em>(550), 1178-1188. (<a href='https://doi.org/10.1080/01621459.2024.2402568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A model-based approach is developed for clustering categorical data with no natural ordering. The proposed method exploits the Hamming distance to define a family of probability mass functions to model the data. The elements of this family are then considered as kernels of a finite mixture model with an unknown number of components. Conjugate Bayesian inference has been derived for the parameters of the Hamming distribution model. The mixture is framed in a Bayesian nonparametric setting, and a transdimensional blocked Gibbs sampler is developed to provide full Bayesian inference on the number of clusters, their structure, and the group-specific parameters, facilitating the computation with respect to customary reversible jump algorithms. The proposed model encompasses a parsimonious latent class model as a special case when the number of components is fixed. Model performances are assessed via a simulation study and reference datasets, showing improvements in clustering recovery over existing approaches. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Raffaele Argiento and Edoardo Filippi-Mazzola and Lucia Paci},
  doi          = {10.1080/01621459.2024.2402568},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1178-1188},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-based clustering of categorical data based on the hamming distance},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neyman-pearson multi-class classification via cost-sensitive learning. <em>JASA</em>, <em>120</em>(550), 1164-1177. (<a href='https://doi.org/10.1080/01621459.2024.2402567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing classification methods aim to minimize the overall misclassification error rate. However, in applications such as loan default prediction, different types of errors can have varying consequences. To address this asymmetry issue, two popular paradigms have been developed: the Neyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous studies on the NP paradigm have primarily focused on the binary case, while the multi-class NP problem poses a greater challenge due to its unknown feasibility. In this work, we tackle the multi-class NP problem by establishing a connection with the CS problem via strong duality and propose two algorithms. We extend the concept of NP oracle inequalities, crucial in binary classifications, to NP oracle properties in the multi-class context. Our algorithms satisfy these NP oracle properties under certain conditions. Furthermore, we develop practical algorithms to assess the feasibility and strong duality in multi-class NP problems, which can offer practitioners the landscape of a multi-class NP problem with various target error levels. Simulations and real data studies validate the effectiveness of our algorithms. To our knowledge, this is the first study to address the multi-class NP problem with theoretical guarantees. The proposed algorithms have been implemented in the R package npcs , which is available on CRAN. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ye Tian and Yang Feng},
  doi          = {10.1080/01621459.2024.2402567},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1164-1177},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Neyman-pearson multi-class classification via cost-sensitive learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On optimality of mallows model averaging. <em>JASA</em>, <em>120</em>(550), 1152-1163. (<a href='https://doi.org/10.1080/01621459.2024.2402566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decades, model averaging (MA) has attracted much attention as it has emerged as an alternative tool to the model selection (MS) statistical approach. Hansen introduced a Mallows model averaging (MMA) method with model weights selected by minimizing a Mallows’ C p criterion. The main theoretical justification for MMA is an asymptotic optimality (AOP), which states that the risk/loss of the resulting MA estimator is asymptotically equivalent to that of the best but infeasible averaged model. MMA’s AOP is proved in the literature by either constraining weights in a special discrete weight set or limiting the number of candidate models. In this work, it is first shown that under these restrictions, however, the optimal risk of MA becomes an unreachable target, and MMA may converge more slowly than MS. In this background, a foundational issue that has not been addressed is: When a suitably large set of candidate models is considered, and the model weights are not harmfully constrained, can the MMA estimator perform asymptotically as well as the optimal convex combination of the candidate models? We answer this question in both nested and non-nested settings. In the nested setting, we provide finite sample inequalities for the risk of MMA and show that without unnatural restrictions on the candidate models, MMA’s AOP holds in a general continuous weight set under certain mild conditions. In the non-nested setting, a sufficient condition and a negative result are established for the achievability of the optimal MA risk. Implications on minimax adaptivity are given as well. The results from simulations and real data analysis back up our theoretical findings. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jingfu Peng and Yang Li and Yuhong Yang},
  doi          = {10.1080/01621459.2024.2402566},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1152-1163},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On optimality of mallows model averaging},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust estimation for number of factors in high dimensional factor modeling via spearman correlation matrix. <em>JASA</em>, <em>120</em>(550), 1139-1151. (<a href='https://doi.org/10.1080/01621459.2024.2402565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the number of factors in high-dimensional factor modeling is essential but challenging, especially when the data are heavy-tailed. In this article, we introduce a new estimator based on the spectral properties of Spearman sample correlation matrix under the high-dimensional setting, where both dimension and sample size tend to infinity proportionally. Our estimator is robust against heavy tails in either the common factors or idiosyncratic errors. The consistency of our estimator is established under mild conditions. Numerical experiments demonstrate the superiority of our estimator compared to existing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jiaxin Qiu and Zeng Li and Jianfeng Yao},
  doi          = {10.1080/01621459.2024.2402565},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1139-1151},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust estimation for number of factors in high dimensional factor modeling via spearman correlation matrix},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Valid inference after causal discovery. <em>JASA</em>, <em>120</em>(550), 1127-1138. (<a href='https://doi.org/10.1080/01621459.2024.2402089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery and causal effect estimation are two fundamental tasks in causal inference. While many methods have been developed for each task individually, statistical challenges arise when applying these methods jointly: estimating causal effects after running causal discovery algorithms on the same data leads to “double dipping,” invalidating the coverage guarantees of classical confidence intervals. To this end, we develop tools for valid post-causal-discovery inference. Across empirical studies, we show that a naive combination of causal discovery and subsequent inference algorithms leads to highly inflated miscoverage rates; on the other hand, applying our method provides reliable coverage while allowing for a trade-off between causal discovery accuracy and confidence interval width. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Paula Gradu and Tijana Zrnic and Yixin Wang and Michael I. Jordan},
  doi          = {10.1080/01621459.2024.2402089},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1127-1138},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Valid inference after causal discovery},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving tensor regression by optimal model averaging. <em>JASA</em>, <em>120</em>(550), 1115-1126. (<a href='https://doi.org/10.1080/01621459.2024.2398164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensors have broad applications in neuroimaging, data mining, digital marketing, etc. CANDECOMP/PARAFAC (CP) tensor decomposition can effectively reduce the number of parameters to gain dimensionality-reduction and thus plays a key role in tensor regression. However, in CP decomposition, there is uncertainty about which rank to use. In this article, we develop a model averaging method to handle this uncertainty by weighting the estimators from candidate tensor regression models with different ranks. When all candidate models are misspecified, we prove that the model averaging estimator is asymptotically optimal. When correct models are included in the set of candidate models, we prove the consistency of parameters and the convergence of the model averaging weight. Simulations and empirical studies illustrate that the proposed method has superiority over the competition methods and has promising applications. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Qiushi Bu and Hua Liang and Xinyu Zhang and Jiahui Zou},
  doi          = {10.1080/01621459.2024.2398164},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1115-1126},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Improving tensor regression by optimal model averaging},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Off-policy evaluation in doubly inhomogeneous environments. <em>JASA</em>, <em>120</em>(550), 1102-1114. (<a href='https://doi.org/10.1080/01621459.2024.2395593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to study off-policy evaluation (OPE) under scenarios where two key reinforcement learning (RL) assumptions—temporal stationarity and individual homogeneity are both violated. To handle the “double inhomogeneities”, we propose a class of latent factor models for the reward and transition functions, under which we develop a general OPE framework that consists of both model-based and model-free approaches. To our knowledge, this is the first article that develops statistically sound OPE methods in offline RL with double inhomogeneities. It contributes to a deeper understanding of OPE in environments, where standard RL assumptions are not met, and provides several practical approaches in these settings. We establish the theoretical properties of the proposed value estimators and empirically show that our approach outperforms state-of-the-art methods. Finally, we illustrate our method on a dataset from the Medical Information Mart for Intensive Care. An R implementation of the proposed procedure is available at https://github.com/ZeyuBian/2FEOPE . Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zeyu Bian and Chengchun Shi and Zhengling Qi and Lan Wang},
  doi          = {10.1080/01621459.2024.2395593},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1102-1114},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Off-policy evaluation in doubly inhomogeneous environments},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based causal feature selection for general response types. <em>JASA</em>, <em>120</em>(550), 1090-1101. (<a href='https://doi.org/10.1080/01621459.2024.2395588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering causal relationships from observational data is a fundamental yet challenging task. Invariant causal prediction (ICP, Peters, Bühlmann, and Meinshausen) is a method for causal feature selection which requires data from heterogeneous settings and exploits that causal models are invariant. ICP has been extended to general additive noise models and to nonparametric settings using conditional independence tests. However, the latter often suffer from low power (or poor Type I error control) and additive noise models are not suitable for applications in which the response is not measured on a continuous scale, but reflects categories or counts. Here, we develop transformation-model ( tram ) based ICP, allowing for continuous, categorical, count-type, and uninformatively censored responses (these model classes, generally, do not allow for identifiability when there is no exogenous heterogeneity). As an invariance test, we propose tram -GCM based on the expected conditional covariance between environments and score residuals with uniform asymptotic level guarantees. For the special case of linear shift tram s, we also consider tram -Wald, which tests invariance based on the Wald statistic. We provide an open-source R package tramicp and evaluate our approach on simulated data and in a case study investigating causal features of survival in critically ill patients. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Lucas Kook and Sorawit Saengkyongam and Anton Rask Lundborg and Torsten Hothorn and Jonas Peters},
  doi          = {10.1080/01621459.2024.2395588},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1090-1101},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-based causal feature selection for general response types},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zigzag path connects two monte carlo samplers: Hamiltonian counterpart to a piecewise deterministic markov process. <em>JASA</em>, <em>120</em>(550), 1077-1089. (<a href='https://doi.org/10.1080/01621459.2024.2395587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zigzag and other piecewise deterministic Markov process samplers have attracted significant interest for their non-reversibility and other appealing properties for Bayesian posterior computation. Hamiltonian Monte Carlo is another state-of-the-art sampler, exploiting fictitious momentum to guide Markov chains through complex target distributions. We establish an important connection between the zigzag sampler and a variant of Hamiltonian Monte Carlo based on Laplace-distributed momentum. The position and velocity component of the corresponding Hamiltonian dynamics travels along a zigzag path paralleling the Markovian zigzag process; however, the dynamics is non-Markovian in this position-velocity space as the momentum component encodes non-immediate pasts. This information is partially lost during a momentum refreshment step, in which we preserve its direction but resample magnitude. In the limit of increasingly frequent momentum refreshments, we prove that Hamiltonian zigzag converges strongly to its Markovian counterpart. This theoretical insight suggests that, when retaining full momentum information, Hamiltonian zigzag can better explore target distributions with highly correlated parameters by suppressing the diffusive behavior of Markovian zigzag. We corroborate this intuition by comparing performance of the two zigzag cousins on high-dimensional truncated multivariate Gaussians, including a 11,235-dimensional target arising from a Bayesian phylogenetic multivariate probit modeling of HIV virus data. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Akihiko Nishimura and Zhenyu Zhang and Marc A. Suchard},
  doi          = {10.1080/01621459.2024.2395587},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1077-1089},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Zigzag path connects two monte carlo samplers: Hamiltonian counterpart to a piecewise deterministic markov process},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monte carlo inference for semiparametric bayesian regression. <em>JASA</em>, <em>120</em>(550), 1063-1076. (<a href='https://doi.org/10.1080/01621459.2024.2395586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data transformations are essential for broad applicability of parametric regression models. However, for Bayesian analysis, joint inference of the transformation and model parameters typically involves restrictive parametric transformations or nonparametric representations that are computationally inefficient and cumbersome for implementation and theoretical analysis, which limits their usability in practice. This article introduces a simple, general, and efficient strategy for joint posterior inference of an unknown transformation and all regression model parameters. The proposed approach directly targets the posterior distribution of the transformation by linking it with the marginal distributions of the independent and dependent variables, and then deploys a Bayesian nonparametric model via the Bayesian bootstrap. Crucially, this approach delivers (a) joint posterior consistency under general conditions, including multiple model misspecifications, and (b) efficient Monte Carlo (not Markov chain Monte Carlo) inference for the transformation and all parameters for important special cases. These tools apply across a variety of data domains, including real-valued, positive, and compactly-supported data. Simulation studies and an empirical application demonstrate the effectiveness and efficiency of this strategy for semiparametric Bayesian analysis with linear models, quantile regression, and Gaussian processes. The R package SeBR is available on CRAN. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Daniel R. Kowal and Bohan Wu},
  doi          = {10.1080/01621459.2024.2395586},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1063-1076},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Monte carlo inference for semiparametric bayesian regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal network pairwise comparison. <em>JASA</em>, <em>120</em>(550), 1048-1062. (<a href='https://doi.org/10.1080/01621459.2024.2393471'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in the problem of two-sample network hypothesis testing: given two networks with the same set of nodes, we wish to test whether the underlying Bernoulli probability matrices of the two networks are the same or not. We propose Interlacing Balance Measure (IBM) as a new two-sample testing approach. We consider the Degree-Corrected Mixed-Membership (DCMM) model for undirected networks, where we allow severe degree heterogeneity, mixed-memberships, flexible sparsity levels, and weak signals. In such a broad setting, how to find a test that has a tractable limiting null and optimal testing performances is a challenging problem. We show that IBM is such a test: in a broad DCMM setting with only mild regularity conditions, IBM has N ( 0 , 1 ) as the limiting null and achieves the optimal phase transition. While the above is for undirected networks, IBM is a unified approach and is directly implementable for directed networks. For a broad directed-DCMM (extension of DCMM for directed networks) setting, we show that IBM has N ( 0 , 1 / 2 ) as the limiting null and continues to achieve the optimal phase transition. We have also applied IBM to the Enron email network and a gene co-expression network, with interesting results. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jiashun Jin and Zheng Tracy Ke and Shengming Luo and Yucong Ma},
  doi          = {10.1080/01621459.2024.2393471},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1048-1062},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal network pairwise comparison},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised triply robust inductive transfer learning. <em>JASA</em>, <em>120</em>(550), 1037-1047. (<a href='https://doi.org/10.1080/01621459.2024.2393463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a Semi-supervised Triply Robust Inductive transFer LEarning (STRIFLE) approach, which integrates heterogeneous data from a label-rich source population and a label-scarce target population and uses a large amount of unlabeled data simultaneously to improve the learning accuracy in the target population. Specifically, we consider a high dimensional covariate shift setting and employ two nuisance models, a density ratio model and an imputation model, to combine transfer learning and surrogate-assisted semi-supervised learning strategies effectively and achieve triple robustness. While the STRIFLE approach assumes the target and source populations to share the same conditional distribution of outcome Y given both the surrogate features S and predictors X , it allows the true underlying model of Y⏧ X to differ between the two populations due to the potential covariate shift in S and X . Different from double robustness, even if both nuisance models are misspecified or the distribution of Y⏧ S , X is not the same between the two populations when the shifted source population and the target population share enough similarities, the triply robust STRIFLE estimator can still partially use the source population when the shifted source population and the target population share enough similarities. Moreover, it is guaranteed to be no worse than the target-only surrogate-assisted semi-supervised estimator with an additional error term from transferability detection. These desirable properties of our estimator are established theoretically and verified in finite samples via extensive simulation studies. We use the STRIFLE estimator to train a Type II diabetes polygenic risk prediction model for the African American target population by transferring knowledge from electronic health records linked genomic data observed in a larger European source population. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Tianxi Cai and Mengyan Li and Molei Liu},
  doi          = {10.1080/01621459.2024.2393463},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1037-1047},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semi-supervised triply robust inductive transfer learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Euclidean mirrors and dynamics in network time series. <em>JASA</em>, <em>120</em>(550), 1025-1036. (<a href='https://doi.org/10.1080/01621459.2024.2392912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing changes in network evolution is central to statistical network inference. We consider a dynamic network model in which each node has an associated time-varying low-dimensional latent vector of feature data, and connection probabilities are functions of these vectors. Under mild assumptions, the evolution of latent vectors exhibits low-dimensional manifold structure under a suitable distance. This distance can be approximated by a measure of separation between the observed networks themselves, and there exist Euclidean representations for underlying network structure, as characterized by this distance. These Euclidean representations, called Euclidean mirrors, permit the visualization of network dynamics and lead to methods for change point and anomaly detection in networks. We illustrate our methodology with real and synthetic data, and identify change points corresponding to massive shifts in pandemic policies in a communication network of a large organization. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Avanti Athreya and Zachary Lubberts and Youngser Park and Carey Priebe},
  doi          = {10.1080/01621459.2024.2392912},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1025-1036},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Euclidean mirrors and dynamics in network time series},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for networks of high-dimensional point processes. <em>JASA</em>, <em>120</em>(550), 1014-1024. (<a href='https://doi.org/10.1080/01621459.2024.2392907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fueled in part by recent applications in neuroscience, the multivariate Hawkes process has become a popular tool for modeling the network of interactions among high-dimensional point process data. While evaluating the uncertainty of the network estimates is critical in scientific applications, existing methodological and theoretical work has primarily addressed estimation. To bridge this gap, we develop a new statistical inference procedure for high-dimensional Hawkes processes. The key ingredient for the inference procedure is a new concentration inequality on the first- and second-order statistics for integrated stochastic processes, which summarize the entire history of the process. Combining recent martingale central limit theorem with the new concentration inequality, we then characterize the convergence rate of the test statistics in a continuous time domain. Finally, to account for potential non-stationarity of the process in practice, we extend our statistical inference procedure to a flexible class of Hawkes processes with time-varying background intensities and unknown transition functions. The finite sample validity of the inferential tools is illustrated via extensive simulations and further applied to a neuron spike train dataset. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xu Wang and Mladen Kolar and Ali Shojaie},
  doi          = {10.1080/01621459.2024.2392907},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1014-1024},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for networks of high-dimensional point processes},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust regression with covariate filtering: Heavy tails and adversarial contamination. <em>JASA</em>, <em>120</em>(550), 1002-1013. (<a href='https://doi.org/10.1080/01621459.2024.2392906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of linear regression where both covariates and responses are potentially (i) heavy-tailed and (ii) adversarially contaminated. Several computationally efficient estimators have been proposed for the simpler setting where the covariates are sub-Gaussian and uncontaminated; however, these estimators may fail when the covariates are either heavy-tailed or contain outliers. In this work, we show how to modify the Huber regression, least trimmed squares, and least absolute deviation estimators to obtain estimators which are simultaneously computationally and statistically efficient in the stronger contamination model. Our approach is quite simple, and consists of applying a filtering algorithm to the covariates, and then applying the classical robust regression estimators to the remaining data. We show that the Huber regression estimator achieves near-optimal error rates in this setting, whereas the least trimmed squares and least absolute deviation estimators can be made to achieve near-optimal error after applying a postprocessing step. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ankit Pensia and Varun Jog and Po-Ling Loh},
  doi          = {10.1080/01621459.2024.2392906},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {1002-1013},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust regression with covariate filtering: Heavy tails and adversarial contamination},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural gradient variational bayes without fisher matrix analytic calculation and its inversion. <em>JASA</em>, <em>120</em>(550), 990-1001. (<a href='https://doi.org/10.1080/01621459.2024.2392904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a method for efficiently approximating the inverse of the Fisher information matrix, a crucial step in achieving effective variational Bayes inference. A notable aspect of our approach is the avoidance of analytically computing the Fisher information matrix and its explicit inversion. Instead, we introduce an iterative procedure for generating a sequence of matrices that converge to the inverse of Fisher information. The natural gradient variational Bayes algorithm without analytic expression of the Fisher matrix and its inversion is provably convergent and achieves a convergence rate of order O ( log s / s ) , with s the number of iterations. We also obtain a central limit theorem for the iterates. Implementation of our method does not require storage of large matrices, and achieves a linear complexity in the number of variational parameters. Our algorithm exhibits versatility, making it applicable across a diverse array of variational Bayes domains, including Gaussian approximation and normalizing flow Variational Bayes. We offer a range of numerical examples to demonstrate the efficiency and reliability of the proposed variational Bayes method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {A. Godichon-Baggioni and D. Nguyen and M.-N. Tran},
  doi          = {10.1080/01621459.2024.2392904},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {990-1001},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Natural gradient variational bayes without fisher matrix analytic calculation and its inversion},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multiple change point detection and localization for high-dimensional quantile regression with heteroscedasticity. <em>JASA</em>, <em>120</em>(550), 976-989. (<a href='https://doi.org/10.1080/01621459.2024.2392903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data heterogeneity is a challenging issue for modern statistical data analysis. There are different types of data heterogeneity in practice. In this article, we consider potential structural changes and complicated tail distributions. There are various existing methods proposed to handle either structural changes or heteroscedasticity. However, it is difficult to handle them simultaneously. To overcome this limitation, we consider statistically and computationally efficient change point detection and localization in high-dimensional quantile regression models. Our proposed framework is general and flexible since the change points and the underlying regression coefficients are allowed to vary across different quantile levels. The model parameters, including the data dimension, the number of change points, and the signal jump size, can be scaled with the sample size. Under this framework, we construct a novel two-step estimation of the number and locations of the change points as well as the underlying regression coefficients. Without any moment constraints on the error term, we present theoretical results, including consistency of the change point number, oracle estimation of change point locations, and estimation for the underlying regression coefficients with the optimal convergence rate. Finally, we present simulation results and an application to the S&P 100 dataset to demonstrate the advantage of the proposed method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xianru Wang and Bin Liu and Xinsheng Zhang and Yufeng Liu},
  doi          = {10.1080/01621459.2024.2392903},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {976-989},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficient multiple change point detection and localization for high-dimensional quantile regression with heteroscedasticity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel sampling of decomposable graphs using markov chains on junction trees. <em>JASA</em>, <em>120</em>(550), 963-975. (<a href='https://doi.org/10.1080/01621459.2024.2388908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference for undirected graphical models is mostly restricted to the class of decomposable graphs, as they enjoy a rich set of properties making them amenable to high-dimensional problems. While parameter inference is straightforward in this setup, inferring the underlying graph is a challenge driven by the computational difficulty in exploring the space of decomposable graphs. This work makes two contributions to address this problem. First, we provide sufficient and necessary conditions for when multi-edge perturbations maintain decomposability of the graph. Using these, we characterize a simple class of partitions that efficiently classify all edge perturbations by whether they maintain decomposability. Second, we propose a novel parallel nonreversible Markov chain Monte Carlo sampler for distributions over junction tree representations of the graph. At every step, the parallel sampler executes simultaneously all edge perturbations within a partition. Through simulations, we demonstrate the efficiency of our new edge perturbation conditions and class of partitions. We find that our parallel sampler yields improved mixing properties in comparison to the single-move variate, and outperforms current state-of-the-art methods in terms of accuracy and computational efficiency. The implementation of our work is available in the Python package parallelDG. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Mohamad Elmasri},
  doi          = {10.1080/01621459.2024.2388908},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {963-975},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Parallel sampling of decomposable graphs using markov chains on junction trees},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal network membership estimation under severe degree heterogeneity. <em>JASA</em>, <em>120</em>(550), 948-962. (<a href='https://doi.org/10.1080/01621459.2024.2388903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real networks often have severe degree heterogeneity, with maximum, average, and minimum node degrees differing significantly. This article examines the impact of degree heterogeneity on statistical limits of network data analysis. Introducing the heterogeneity distribution (HD) under a degree-corrected mixed membership model, we show that the optimal rate of mixed membership estimation is an explicit functional of the HD. This result confirms that severe degree heterogeneity decelerates the error rate, even when the overall sparsity remains unchanged. To obtain a rate-optimal method, we modify an existing spectral algorithm, Mixed-SCORE, by adding a pre-PCA normalization step. This step normalizes the adjacency matrix by a diagonal matrix consisting of the b th power of node degrees, for some b ∈ R . We discover that b = 1/2 is universally favorable. The resulting spectral algorithm is rate-optimal for networks with arbitrary degree heterogeneity. A technical component in our proofs is entry-wise eigenvector analysis of the normalized graph Laplacian. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zheng Tracy Ke and Jingming Wang},
  doi          = {10.1080/01621459.2024.2388903},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {948-962},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal network membership estimation under severe degree heterogeneity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controlling the false split rate in tree-based aggregation. <em>JASA</em>, <em>120</em>(550), 935-947. (<a href='https://doi.org/10.1080/01621459.2024.2376285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many domains, data measurements can naturally be associated with the leaves of a tree, expressing the relationships among these measurements. For example, companies belong to industries, which in turn belong to ever coarser divisions such as sectors; microbes are commonly arranged in a taxonomic hierarchy from species to kingdoms; street blocks belong to neighborhoods, which in turn belong to larger-scale regions. The problem of tree-based aggregation that we consider in this article asks which of these tree-defined subgroups of leaves should really be treated as a single entity and which of these entities should be distinguished from each other. We introduce the false split rate , an error measure that describes the degree to which subgroups have been split when they should not have been. While expressible as the false discovery rate in a special case, we show that these measures can be quite different for the general tree structures common in our setting. We then propose a multiple hypothesis testing algorithm for tree-based aggregation, which we prove controls this error measure. We focus on two main examples of tree-based aggregation, one which involves aggregating means and the other hich involves aggregating regression coefficients. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Simeng Shao and Jacob Bien and Adel Javanmard},
  doi          = {10.1080/01621459.2024.2376285},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {935-947},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Controlling the false split rate in tree-based aggregation},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust matrix completion with heavy-tailed noise. <em>JASA</em>, <em>120</em>(550), 922-934. (<a href='https://doi.org/10.1080/01621459.2024.2375037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies noisy low-rank matrix completion in the presence of heavy-tailed and possibly asymmetric noise, where we aim to estimate an underlying low-rank matrix given a set of highly incomplete noisy entries. Though the matrix completion problem has attracted much attention in the past decade, there is still lack of theoretical understanding when the observations are contaminated by heavy-tailed noises. Prior theory falls short of explaining the empirical results and is unable to capture the optimal dependence of the estimation error on the noise level. In this article, we adopt an adaptive Huber loss to accommodate heavy-tailed noise, which is robust against large and possibly asymmetric errors when the parameter in the Huber loss function is carefully designed to balance the Huberization biases and robustness to outliers. Then, we propose an efficient nonconvex algorithm via a balanced low-rank Burer-Monteiro matrix factorization and gradient descent with robust spectral initialization. We prove that under merely a bounded second-moment condition on the error distributions, rather than the sub-Gaussian assumption, the Euclidean errors of the iterates generated by the proposed algorithm decrease geometrically fast until achieving a minimax-optimal statistical estimation error, which has the same order as that in the sub-Gaussian case. The key technique behind this significant advancement is a powerful leave-one-out analysis framework. The theoretical results are corroborated by our numerical studies. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Bingyan Wang and Jianqing Fan},
  doi          = {10.1080/01621459.2024.2375037},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {922-934},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust matrix completion with heavy-tailed noise},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for Hüsler–Reiss graphical models through matrix completions. <em>JASA</em>, <em>120</em>(550), 909-921. (<a href='https://doi.org/10.1080/01621459.2024.2371978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The severity of multivariate extreme events is driven by the dependence between the largest marginal observations. The Hüsler–Reiss distribution is a versatile model for this extremal dependence, and it is usually parameterized by a variogram matrix. In order to represent conditional independence relations and obtain sparse parameterizations, we introduce the novel Hüsler–Reiss precision matrix. Similarly to the Gaussian case, this matrix appears naturally in density representations of the Hüsler–Reiss Pareto distribution and encodes the extremal graphical structure through its zero pattern. For a given, arbitrary graph we prove the existence and uniqueness of the completion of a partially specified Hüsler–Reiss variogram matrix so that its precision matrix has zeros on non-edges in the graph. Using suitable estimators for the parameters on the edges, our theory provides the first consistent estimator of graph structured Hüsler–Reiss distributions. If the graph is unknown, our method can be combined with recent structure learning algorithms to jointly infer the graph and the corresponding parameter matrix. Based on our methodology, we propose new tools for statistical inference of sparse Hüsler–Reiss models and illustrate them on large flight delay data in the United States, as well as Danube river flow data. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Manuel Hentschel and Sebastian Engelke and Johan Segers},
  doi          = {10.1080/01621459.2024.2371978},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {909-921},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for Hüsler–Reiss graphical models through matrix completions},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual dynamic pricing with strategic buyers. <em>JASA</em>, <em>120</em>(550), 896-908. (<a href='https://doi.org/10.1080/01621459.2024.2370613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this article, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer’s true feature, but a manipulated feature according to buyers’ strategic behavior. In addition, the seller does not observe the buyers’ valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers’ strategic behavior into the online learning to maximize the seller’s cumulative revenue. We first prove that existing nonstrategic pricing policies that neglect the buyers’ strategic behavior result in a linear Ω ( T ) regret with T the total time horizon, indicating that these policies are not better than a random pricing policy. We then establish an O ( T ) regret upper bound of our proposed policy and an Ω ( T ) regret lower bound for any pricing policy within our problem setting. This underscores the rate optimality of our policy. Importantly, our policy is not a mere amalgamation of existing dynamic pricing policies and strategic behavior handling algorithms. Our policy can also accommodate the scenario when the marginal cost of manipulation is unknown in advance. To account for it, we simultaneously estimate the valuation parameter and the cost parameter in the online pricing policy, which is shown to also achieve an O ( T ) regret bound. Extensive experiments support our theoretical developments and demonstrate the superior performance of our policy compared to other pricing policies that are unaware of the strategic behaviors. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Pangpang Liu and Zhuoran Yang and Zhaoran Wang and Will Wei Sun},
  doi          = {10.1080/01621459.2024.2370613},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {896-908},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Contextual dynamic pricing with strategic buyers},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic likelihood in misspecified models. <em>JASA</em>, <em>120</em>(550), 884-895. (<a href='https://doi.org/10.1080/01621459.2024.2370594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian synthetic likelihood is a widely used approach for conducting Bayesian analysis in complex models where evaluation of the likelihood is infeasible but simulation from the assumed model is tractable. We analyze the behavior of the Bayesian synthetic likelihood posterior when the assumed model differs from the actual data generating process. We demonstrate that the Bayesian synthetic likelihood posterior can display a wide range of nonstandard behaviors depending on the level of model misspecification, including multimodality and asymptotic non-Gaussianity. Our results suggest that likelihood tempering, a common approach for robust Bayesian inference, fails for synthetic likelihood whilst recently proposed robust synthetic likelihood approaches can ameliorate this behavior and deliver reliable posterior inference under model misspecification. All results are illustrated using a simple running example. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {David T. Frazier and David J. Nott and Christopher Drovandi},
  doi          = {10.1080/01621459.2024.2370594},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {884-895},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Synthetic likelihood in misspecified models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised dynamic PCA: Linear dynamic forecasting with many predictors. <em>JASA</em>, <em>120</em>(550), 869-883. (<a href='https://doi.org/10.1080/01621459.2024.2370592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel dynamic forecasting method using a new supervised Principal Component Analysis (PCA) when a large number of predictors are available. The new supervised PCA provides an effective way to bridge the gap between predictors and the target variable of interest by scaling and combining the predictors and their lagged values, resulting in an effective dynamic forecasting. Unlike the traditional diffusion-index approach, which does not learn the relationships between the predictors and the target variable before conducting PCA, we first rescale each predictor according to their significance in forecasting the targeted variable in a dynamic fashion, and a PCA is then applied to a rescaled and additive panel, which establishes a connection between the predictability of the PCA factors and the target variable. We also propose to use penalized methods such as the LASSO to select the significant factors that have superior predictive power over the others. Theoretically, we show that our estimators are consistent and outperform the traditional methods in prediction under some mild conditions. We conduct extensive simulations to verify that the proposed method produces satisfactory forecasting results and outperforms most of the existing methods using the traditional PCA. An example of predicting U.S. macroeconomic variables using a large number of predictors showcases that our method fares better than most of the existing ones in applications. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zhaoxing Gao and Ruey S. Tsay},
  doi          = {10.1080/01621459.2024.2370592},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {869-883},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Supervised dynamic PCA: Linear dynamic forecasting with many predictors},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced response envelope via envelope regularization. <em>JASA</em>, <em>120</em>(550), 859-868. (<a href='https://doi.org/10.1080/01621459.2024.2368844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The response envelope model provides substantial efficiency gains over the standard multivariate linear regression by identifying the material part of the response to the model and by excluding the immaterial part. In this article, we propose the enhanced response envelope by incorporating a novel envelope regularization term based on a nonconvex manifold formulation. It is shown that the enhanced response envelope can yield better prediction risk than the original envelope estimator. The enhanced response envelope naturally handles high-dimensional data for which the original response envelope is not serviceable without necessary remedies. In an asymptotic high-dimensional regime where the ratio of the number of predictors over the number of samples converges to a nonzero constant, we characterize the risk function and reveal an interesting double descent phenomenon for the envelope model. A simulation study confirms our main theoretical findings. Simulations and real data applications demonstrate that the enhanced response envelope does have significantly improved prediction performance over the original envelope method, especially when the number of predictors is close to or moderately larger than the number of samples. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Oh-Ran Kwon and Hui Zou},
  doi          = {10.1080/01621459.2024.2368844},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {859-868},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Enhanced response envelope via envelope regularization},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tyranny-of-the-minority regression adjustment in randomized experiments. <em>JASA</em>, <em>120</em>(550), 846-858. (<a href='https://doi.org/10.1080/01621459.2024.2366043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression adjustment is widely used in the analysis of randomized experiments to improve the estimation efficiency of the treatment effect. This article reexamines a weighted regression adjustment method termed t yranny- o f-the- m inority (ToM), wherein units in the minority group are given greater weights. We demonstrate that ToM regression adjustment is more robust than Lin ’s regression adjustment with treatment-covariate interactions, even though these two regression adjustment methods are asymptotically equivalent in completely randomized experiments. Moreover, ToM regression adjustment can be easily extended to stratified randomized experiments and completely randomized survey experiments. We obtain the design-based properties of the ToM regression-adjusted average treatment effect estimator under such designs. In particular, we show that the ToM regression-adjusted estimator improves the asymptotic estimation efficiency compared to the unadjusted estimator, even when the regression model is misspecified, and is optimal in the class of linearly adjusted estimators. We also study the asymptotic properties of various heteroscedasticity-robust standard errors and provide recommendations for practitioners. Simulation studies and real data analysis demonstrate ToM regression adjustment’s superiority over existing methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xin Lu and Hanzhong Liu},
  doi          = {10.1080/01621459.2024.2366043},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {846-858},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Tyranny-of-the-minority regression adjustment in randomized experiments},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test and measure for partial mean dependence based on machine learning methods. <em>JASA</em>, <em>120</em>(550), 833-845. (<a href='https://doi.org/10.1080/01621459.2024.2366030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is of importance to investigate the significance of a subset of covariates W for the response Y given covariates Z in regression modeling. To this end, we propose a significance test for the partial mean independence problem based on machine learning methods and data splitting. The test statistic converges to the standard Chi-squared distribution under the null hypothesis while it converges to a normal distribution under the fixed alternative hypothesis. Power enhancement and algorithm stability are also discussed. If the null hypothesis is rejected, we propose a partial Generalized Measure of Correlation (pGMC) to measure the partial mean dependence of Y given W after controlling for the nonlinear effect of Z . We present the appealing theoretical properties of the pGMC and establish the asymptotic normality of its estimator with the optimal root- N convergence rate. Furthermore, the valid confidence interval for the pGMC is also derived. As an important special case when there are no conditional covariates Z , we introduce a new test of overall significance of covariates for the response in a model-free setting. Numerical studies and real data analysis are also conducted to compare with existing approaches and to demonstrate the validity and flexibility of our proposed procedures. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Leheng Cai and Xu Guo and Wei Zhong},
  doi          = {10.1080/01621459.2024.2366030},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {833-845},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Test and measure for partial mean dependence based on machine learning methods},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric multiple-output center-outward quantile regression. <em>JASA</em>, <em>120</em>(550), 818-832. (<a href='https://doi.org/10.1080/01621459.2024.2366029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building on recent measure-transportation-based concepts of multivariate quantiles, we are considering the problem of nonparametric multiple-output quantile regression. Our approach defines nested conditional center-outward quantile regression contours and regions with given conditional probability content, the graphs of which constitute nested center-outward quantile regression tubes with given unconditional probability content; these (conditional and unconditional) probability contents do not depend on the underlying distribution—an essential property of quantile concepts. Empirical counterparts of these concepts are constructed, yielding interpretable empirical contours, regions, and tubes which are shown to consistently reconstruct (in the Pompeiu-Hausdorff topology) their population versions. Our method is entirely nonparametric and performs well in simulations—with possible heteroscedasticity and nonlinear trends. Its potential as a data-analytic tool is illustrated on some real datasets. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Eustasio del Barrio and Alberto González Sanz and Marc Hallin},
  doi          = {10.1080/01621459.2024.2366029},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {818-832},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric multiple-output center-outward quantile regression},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). False discovery rate control for structured multiple testing: Asymmetric rules and conformal Q-values. <em>JASA</em>, <em>120</em>(550), 805-817. (<a href='https://doi.org/10.1080/01621459.2024.2359739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective utilization of structural information in data while ensuring statistical validity poses a significant challenge in false discovery rate (FDR) analyses. Conformal inference provides rigorous theory for grounding complex machine learning methods without relying on strong assumptions or highly idealized models. However, existing conformal methods have limitations in handling structured multiple testing, as their validity often requires the deployment of symmetric decision rules, which assume the exchangeability of data points and permutation-invariance of fitting algorithms. To overcome these limitations, we introduce the pseudo local index of significance (PLIS) procedure, which is capable of accommodating asymmetric rules and requires only pairwise exchangeability between the null conformity scores. We demonstrate that PLIS offers finite-sample guarantees in FDR control and the ability to assign higher weights to relevant data points. Numerical results confirm the effectiveness and robustness of PLIS and demonstrate improvements in power compared to existing model-free methods in various scenarios. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zinan Zhao and Wenguang Sun},
  doi          = {10.1080/01621459.2024.2359739},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {805-817},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {False discovery rate control for structured multiple testing: Asymmetric rules and conformal Q-values},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mediation analysis with the mediator and outcome missing not at random. <em>JASA</em>, <em>120</em>(550), 794-804. (<a href='https://doi.org/10.1080/01621459.2024.2359132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is widely used for investigating direct and indirect causal pathways through which an effect arises. However, many mediation analysis studies are challenged by missingness in the mediator and outcome. In general, when the mediator and outcome are missing not at random, the direct and indirect effects are not identifiable without further assumptions. We study the identifiability of the direct and indirect effects under some interpretable mechanisms that allow for missing not at random in the mediator and outcome. We evaluate the performance of statistical inference under those mechanisms through simulation studies and illustrate the proposed methods via the National Job Corps Study. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shuozhi Zuo and Debashis Ghosh and Peng Ding and Fan Yang},
  doi          = {10.1080/01621459.2024.2359132},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {794-804},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Mediation analysis with the mediator and outcome missing not at random},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed heterogeneity learning for generalized partially linear models with spatially varying coefficients. <em>JASA</em>, <em>120</em>(550), 779-793. (<a href='https://doi.org/10.1080/01621459.2024.2359131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial heterogeneity is of great importance in social, economic, and environmental science studies. The spatially varying coefficient model is a popular and effective spatial regression technique to address spatial heterogeneity. However, accounting for heterogeneity comes at the cost of reducing model parsimony. To balance flexibility and parsimony, this article develops a class of generalized partially linear spatially varying coefficient models which allow the inclusion of both constant and spatially varying effects of covariates. Another significant challenge in many applications comes from the enormous size of the spatial datasets collected from modern technologies. To tackle this challenge, we design a novel distributed heterogeneity learning (DHL) method based on bivariate spline smoothing over a triangulation of the domain. The proposed DHL algorithm has a simple, scalable, and communication-efficient implementation scheme that can almost achieve linear speedup. In addition, this article provides rigorous theoretical support for the DHL framework. We prove that the DHL constant coefficient estimators are asymptotic normal and the DHL spline estimators reach the same convergence rate as the global spline estimators obtained using the entire dataset. The proposed DHL method is evaluated through extensive simulation studies and analyses of U.S. loan application data. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shan Yu and Guannan Wang and Li Wang},
  doi          = {10.1080/01621459.2024.2359131},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {779-793},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distributed heterogeneity learning for generalized partially linear models with spatially varying coefficients},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Node-level community detection within edge exchangeable models for interaction processes. <em>JASA</em>, <em>120</em>(550), 764-778. (<a href='https://doi.org/10.1080/01621459.2024.2358560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientists are increasingly interested in discovering community structure from modern relational data arising on large-scale social networks. While many methods have been proposed for learning community structure, few account for the fact that these modern networks arise from processes of interactions in the population. We introduce block edge exchangeable models (BEEM) for the study of interaction networks with latent node-level community structure. The block vertex components model (B-VCM) is derived as a canonical example. Several theoretical and practical advantages over traditional vertex-centric approaches are highlighted. In particular, BEEMs allow for sparse degree structure and power-law degree distributions within communities. Our theoretical analysis bounds the misspecification rate of block assignments while supporting simulations show the properties of the network can be recovered. A computationally tractable Gibbs algorithm is derived. We demonstrate the proposed model using post-comment interaction data from Talklife, a large-scale online peer-to-peer support network, and contrast the learned communities from those using standard algorithms including degree-corrected stochastic block models, popularity-adjusted block models, and weighted stochastic block models. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yuhua Zhang and Walter Dempsey},
  doi          = {10.1080/01621459.2024.2358560},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {764-778},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Node-level community detection within edge exchangeable models for interaction processes},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Population-level balance in signed networks. <em>JASA</em>, <em>120</em>(550), 751-763. (<a href='https://doi.org/10.1080/01621459.2024.2356894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical network models are useful for understanding the underlying formation mechanism and characteristics of complex networks. However, statistical models for signed networks have been largely unexplored. In signed networks, there exist both positive (e.g., like, trust) and negative (e.g., dislike, distrust) edges, which are commonly seen in real-world scenarios. The positive and negative edges in signed networks lead to unique structural patterns, which pose challenges for statistical modeling. In this article, we introduce a statistically principled latent space approach for modeling signed networks and accommodating the well-known balance theory , that is, “the enemy of my enemy is my friend” and “the friend of my friend is my friend.” The proposed approach treats both edges and their signs as random variables, and characterizes the balance theory with a novel and natural notion of population-level balance. This approach guides us towards building a class of balanced inner-product models, and toward developing scalable algorithms via projected gradient descent to estimate the latent variables. We also establish non-asymptotic error rates for the estimates, which are further verified through simulation studies. In addition, we apply the proposed approach to an international relation network, which provides an informative and interpretable model-based visualization of countries during World War II. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Weijing Tang and Ji Zhu},
  doi          = {10.1080/01621459.2024.2356894},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {751-763},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Population-level balance in signed networks},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rate-optimal rank aggregation with private pairwise rankings. <em>JASA</em>, <em>120</em>(550), 737-750. (<a href='https://doi.org/10.1080/01621459.2025.2484843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various real-world scenarios, such as recommender systems and political surveys, pairwise rankings are commonly collected and used for rank aggregation to derive an overall ranking of items. However, preference rankings can reveal individuals’ personal preferences, highlighting the need to protect them from exposure in downstream analysis. In this article, we address the challenge of preserving privacy while ensuring the utility of rank aggregation based on pairwise rankings generated from a general comparison model. A common privacy protection strategy in practice is the use of the randomized response mechanism to perturb raw pairwise rankings. However, a critical challenge arises because the privatized rankings no longer adhere to the original model, resulting in significant bias in downstream rank aggregation tasks. To address this, we propose an adaptive debiasing method for rankings from the randomized response mechanism, ensuring consistent estimation of true preferences and enhancing the utility of downstream rank aggregation. Theoretically, we provide insights into the relationship between overall privacy guarantees and estimation errors in private ranking data, and establish minimax rates for estimation errors. This enables the determination of optimal privacy guarantees that balance consistency in rank aggregation with privacy protection. We also investigate convergence rates of expected ranking errors for partial and full ranking recovery, quantifying how privacy protection affects the specification of top- K item sets and complete rankings. Our findings are validated through extensive simulations and a real-world application. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shirong Xu and Will Wei Sun and Guang Cheng},
  doi          = {10.1080/01621459.2025.2484843},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {737-750},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rate-optimal rank aggregation with private pairwise rankings},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse bayesian group factor model for feature interactions in multiple count tables data. <em>JASA</em>, <em>120</em>(550), 723-736. (<a href='https://doi.org/10.1080/01621459.2025.2449721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group factor models have been developed to infer relationships between multiple co-occurring multivariate continuous responses. Motivated by complex count data from multi-domain microbiome studies using next-generation sequencing, we develop a sparse Bayesian group factor model (Sp-BGFM) for multiple count table data that captures the interaction between microorganisms in different domains. Sp-BGFM uses a rounded kernel mixture model using a Dirichlet process (DP) prior with log-normal mixture kernels for count vectors. A group factor model is used to model the covariance matrix of the mixing kernel that describes microorganism interaction. We construct a Dirichlet-Horseshoe (Dir-HS) shrinkage prior and use it as a joint prior for factor loading vectors. Joint sparsity induced by a Dir-HS prior greatly improves the performance in high-dimensional applications. We further model the effects of covariates on microbial abundances using regression. The semiparametric model flexibly accommodates large variability in observed counts and excess zero counts and provides a basis for robust estimation of the interaction and covariate effects. We evaluate Sp-BGFM using simulation studies and real data analysis, comparing it to popular alternatives. Our results highlight the necessity of joint sparsity induced by the Dir-HS prior, and the benefits of a flexible DP model for baseline abundances. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shuangjie Zhang and Yuning Shen and Irene A. Chen and Juhee Lee},
  doi          = {10.1080/01621459.2025.2449721},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {723-736},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sparse bayesian group factor model for feature interactions in multiple count tables data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GeoWarp: Warped spatial processes for inferring subsea sediment properties. <em>JASA</em>, <em>120</em>(550), 710-722. (<a href='https://doi.org/10.1080/01621459.2024.2445874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For offshore structures like wind turbines, subsea infrastructure, pipelines, and cables, it is crucial to quantify the properties of the seabed sediments at a proposed site. However, data collection offshore is costly, so analysis of the seabed sediments must be made from measurements that are spatially sparse. Adding to this challenge, the structure of the seabed sediments exhibits both nonstationarity and anisotropy. To address these issues, we propose GeoWarp, a hierarchical spatial statistical modeling framework for inferring the 3-D geotechnical properties of subsea sediments. GeoWarp decomposes the seabed properties into a region-wide vertical mean profile (modeled using B-splines), and a nonstationary 3-D spatial Gaussian process. Process nonstationarity and anisotropy are accommodated by warping space in three dimensions and by allowing the process variance to change with depth. We apply GeoWarp to measurements of the seabed made using cone penetrometer tests (CPTs) at six sites on the North West Shelf of Australia. We show that GeoWarp captures the complex spatial distribution of the sediment properties, and produces realistic 3-D simulations suitable for downstream engineering analyses. Through cross-validation, we show that GeoWarp has predictive performance superior to other state-of-the-art methods, demonstrating its value as a tool in offshore geotechnical engineering. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Michael Bertolacci and Andrew Zammit-Mangion and Juan Valderrama Giraldo and Michael O’Neill and Fraser Bransby and Phil Watson},
  doi          = {10.1080/01621459.2024.2445874},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {710-722},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {GeoWarp: Warped spatial processes for inferring subsea sediment properties},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining broad and narrow case definitions in matched case-control studies: Firearms in the home and suicide risk. <em>JASA</em>, <em>120</em>(550), 698-709. (<a href='https://doi.org/10.1080/01621459.2024.2441519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Does having firearms in the home increase suicide risk? To test this hypothesis, a matched case-control study can be performed, in which suicide case subjects are compared to living controls who are similar in observed covariates in terms of their retrospective exposure to firearms at home. In this application, cases can be defined using a broad case definition (suicide) or a narrow case definition (suicide occurred at home). The broad case definition offers a larger number of cases, but the narrow case definition may offer a larger effect size, which can reduce sensitivity to bias from unmeasured confounding. However, when the goal is to test whether there is a treatment effect based on the broad case definition, restricting to the narrow case definition may introduce selection bias (i.e., bias due to selecting samples based on characteristics affected by the treatment) because exposure to firearms in the home may affect the location of suicide and thus the type of a case a subject is. We propose a new sensitivity analysis framework for combining broad and narrow case definitions in matched case-control studies, that considers the unmeasured confounding bias and selection bias simultaneously. We develop a valid randomization-based testing procedure using only the narrow case matched sets when the effect of the unmeasured confounder on receiving treatment and the effect of the treatment on case definition among the always-cases are controlled by sensitivity parameters. We then use the Bonferroni method to combine the testing procedures using the broad and narrow case definitions. With the proposed methods, we find robust evidence that having firearms at home increases suicide risk. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ting Ye and Kan Chen and Dylan Small},
  doi          = {10.1080/01621459.2024.2441519},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {698-709},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Combining broad and narrow case definitions in matched case-control studies: Firearms in the home and suicide risk},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring causal effect of a digital communication strategy under a latent sequential ignorability assumption and treatment noncompliance. <em>JASA</em>, <em>120</em>(550), 685-697. (<a href='https://doi.org/10.1080/01621459.2024.2435655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizations are increasingly relying on digital communications, such as targeted e-mails and mobile notifications, to engage with their audiences. Despite the evident advantages like cost-effectiveness and customization, assessing the effectiveness of such communications from observational data poses various statistical challenges. An immediate challenge is to adjust for targeting rules used in these communications. When digital communications involve a sequence of e-mails or notifications, however, further adjustments are required to correct for selection bias arising from previous communications influencing the subsequent ones and to deal with noncompliance issues, for example, not opening the e-mail. This article addresses these challenges in a study of promotional e-mail sequences sent by a U.S. retailer. We use a Bayesian methodology for causal inference from longitudinal data, considering targeting, noncompliance, and sequential confounding with unmeasured variables. The methodology serves three objectives: to evaluate the average treatment effect of any deterministic e-mailing strategy, to compare the effectiveness of these strategies across varying compliance behaviors, and to infer optimal strategies for distinct customer segments. Our analysis finds, among other things, that certain promotional e-mails effectively maintain engagement among individuals who have regularly received such incentives, and individuals who consistently open their e-mails exhibit reduced sensitivity to promotional content. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yuki Ohnishi and Bikram Karmakar and Wreetabrata Kar},
  doi          = {10.1080/01621459.2024.2435655},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {685-697},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inferring causal effect of a digital communication strategy under a latent sequential ignorability assumption and treatment noncompliance},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Immune profiling among colorectal cancer subtypes using dependent mixture models. <em>JASA</em>, <em>120</em>(550), 671-684. (<a href='https://doi.org/10.1080/01621459.2024.2427936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparison of transcriptomic data across different conditions is of interest in many biomedical studies. In this article, we consider comparative immune cell profiling for early-onset (EO) versus late-onset (LO) colorectal cancer (CRC). EOCRC, diagnosed between ages 18–45, is a rising public health concern that needs to be urgently addressed. However, its etiology remains poorly understood. We work toward filling this gap by identifying homogeneous T cell sub-populations that show significantly distinct characteristics across the two tumor types, and identifying others that are shared between EOCRC and LOCRC. We develop dependent finite mixture models where immune subtypes enriched under a specific condition are characterized by terms in the mixture model with common atoms but distinct weights across conditions, whereas common subtypes are characterized by sharing both atoms and relative weights. The proposed model facilitates the desired comparison across conditions by introducing highly structured multi-layer Dirichlet priors. We illustrate inference with simulation studies and data examples. Results identify EO- and LO-enriched T cells subtypes whose biomarkers are found to be linked to mechanisms of tumor progression, and potentially motivate insights into treatment of CRC. Code implementing the proposed method is available at: https://github.com/YunshanDYS/SASCcode . Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yunshan Duan and Shuai Guo and Wenyi Wang and Peter Müller},
  doi          = {10.1080/01621459.2024.2427936},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {671-684},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Immune profiling among colorectal cancer subtypes using dependent mixture models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking retrospective prevalent information in EHRs—A revisit to the pairwise pseudolikelihood. <em>JASA</em>, <em>120</em>(550), 658-670. (<a href='https://doi.org/10.1080/01621459.2024.2427431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records offer abundant data on various diseases and health conditions, enabling researchers to explore the relationship between disease onset age and underlying risk factors. Unlike mortality data, the event of interest is nonterminal, hence, individuals can retrospectively report their disease-onset-age upon recruitment to the study. These individuals, diagnosed with the disease before entering the study, are termed “prevalent.” The ascertainment imposes a left truncation condition, also known as a “delayed entry,” because individuals had to survive a certain period before being eligible for enrollment. The standard method to accommodate delayed entry conditions on the entire history up to recruitment, hence, the retrospective prevalent failure times are conditioned upon and cannot participate in estimating the disease-onset-age distribution. Other methods that condition on less information and allow the incorporation of the prevalent observations either bring about numerical and computational difficulties or require statistical assumptions that are violated by most biobanks. This work presents a novel estimator of the coefficients in a regression model for the age-at-onset, successfully using the prevalent data. Asymptotic results are provided, and simulations are conducted to showcase the substantial efficiency gain. In particular, the method is highly useful in leveraging large-scale repositories for replication analysis of genetic variants. Indeed, analysis of urinary bladder cancer data reveals that the proposed approach yields about twice as many replicated discoveries compared to the popular approach. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Nir Keret and Malka Gorfine},
  doi          = {10.1080/01621459.2024.2427431},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {658-670},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Unlocking retrospective prevalent information in EHRs—A revisit to the pairwise pseudolikelihood},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal modeling for record-breaking temperature events in spain. <em>JASA</em>, <em>120</em>(550), 645-657. (<a href='https://doi.org/10.1080/01621459.2024.2427430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Record-breaking temperature events are now very frequently in the news, viewed as evidence of climate change. With this as motivation, we undertake the first substantial spatial modeling investigation of temperature record-breaking across years for any given day within the year. We work with a dataset consisting of over 60 years (1960–2021) of daily maximum temperatures across peninsular Spain. Formal statistical analysis of record-breaking events is an area that has received attention primarily within the probability community, dominated by results for the stationary record-breaking setting with some additional work addressing trends. Such effort is inadequate for analyzing actual record-breaking data. Resulting from novel and detailed exploratory data analysis, we propose rich hierarchical conditional modeling of the indicator events which define record-breaking sequences. After suitable model selection, we discover explicit trend behavior, necessary autoregression, significance of distance to the coast, useful interactions, helpful spatial random effects, and very strong daily random effects. Illustratively, the model estimates that global warming trends have increased the number of records expected in the past decade almost 2-fold, 1.93 ( 1.89 , 1.98 ) , but also estimates highly differentiated climate warming rates in space and by season. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jorge Castillo-Mateo and Alan E. Gelfand and Zeus Gracia-Tabuenca and Jesús Asín and Ana C. Cebrián},
  doi          = {10.1080/01621459.2024.2427430},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {645-657},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spatio-temporal modeling for record-breaking temperature events in spain},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ℓ1-based bayesian ideal point model for multidimensional politics. <em>JASA</em>, <em>120</em>(550), 631-644. (<a href='https://doi.org/10.1080/01621459.2024.2425461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ideal point estimation methods in the social sciences lack a principled approach for identifying multidimensional ideal points. We present a novel method for estimating multidimensional ideal points based on l 1 distance. In the Bayesian framework, the use of l 1 distance transforms the invariance problem of infinite rotational turns into the signed perpendicular problem, yielding posterior estimates that contract around a small area. Our simulation shows that the proposed method successfully recovers planted multidimensional ideal points in a variety of settings including non-partisan, two-party, and multi-party systems. The proposed method is applied to the analysis of roll call data from the United States House of Representatives during the late Gilded Age (1891–1899) when legislative coalitions were distinguished not only by partisan divisions but also by sectional divisions that ran across party lines. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Sooahn Shin and Johan Lim and Jong Hee Park},
  doi          = {10.1080/01621459.2024.2425461},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {631-644},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {ℓ1-based bayesian ideal point model for multidimensional politics},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A physics-informed, deep double reservoir network for forecasting boundary layer velocity. <em>JASA</em>, <em>120</em>(550), 618-630. (<a href='https://doi.org/10.1080/01621459.2024.2422131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a fluid flows over a solid surface, it creates a thin boundary layer where the flow velocity is influenced by the surface through viscosity, and can transition from laminar to turbulent at sufficiently high speeds. Understanding and forecasting the fluid dynamics under these conditions is one of the most challenging scientific problems in fluid dynamics. It is therefore of high interest to formulate models able to capture the nonlinear spatio-temporal velocity structure as well as produce forecasts in a computationally efficient manner. Traditional statistical approaches are limited in their ability to produce timely forecasts of complex, nonlinear spatio-temporal structures which are at the same time able to incorporate the underlying flow physics. In this work, we propose a model to accurately forecast boundary layer velocities with a deep double reservoir computing network which is capable of capturing the complex, nonlinear dynamics of the boundary layer while at the same time incorporating physical constraints via a penalty obtained by a Partial Differential Equation (PDE). Simulation studies on a one-dimensional viscous fluid demonstrate how the proposed model is able to produce accurate forecasts while simultaneously accounting for energy loss. The application focuses on boundary layer data in a water tunnel with a PDE penalty derived from an appropriate simplification of the Navier-Stokes equations, showing improved forecasting by the proposed approach in terms of mass conservation and variability of velocity fluctuation against non-physics-informed methods. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Matthew Bonas and David H. Richter and Stefano Castruccio},
  doi          = {10.1080/01621459.2024.2422131},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {618-630},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A physics-informed, deep double reservoir network for forecasting boundary layer velocity},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing a large number of composite null hypotheses using conditionally symmetric multidimensional gaussian mixtures in genome-wide studies. <em>JASA</em>, <em>120</em>(550), 605-617. (<a href='https://doi.org/10.1080/01621459.2024.2422124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation, pleiotropy, and replication analyses are three highly popular genetic study designs. Although these analyses address different scientific questions, the underlying statistical inference problems all involve large-scale testing of composite null hypotheses. The goal is to determine whether all null hypotheses—as opposed to at least one—in a set of individual tests should simultaneously be rejected. Recently, various methods have been proposed for each of these situations, including an appealing two-group empirical Bayes approach that calculates local false discovery rates (lfdr). However, lfdr estimation is difficult due to the need for multivariate density estimation. Furthermore, the multiple testing rules for the empirical Bayes lfdr approach can disagree with conventional frequentist z-statistics, which is troubling for a field that ubiquitously uses summary statistics. This work proposes a framework to unify two-group testing in genetic association composite null settings, the conditionally symmetric multidimensional Gaussian mixture model (csmGmm). The csmGmm is shown to demonstrate more robust operating characteristics than recently-proposed alternatives. Crucially, the csmGmm also offers interpretability guarantees by harmonizing lfdr and z-statistic testing rules. We extend the base csmGmm to cover each of the mediation, pleiotropy, and replication settings, and we prove that the lfdr z-statistic agreement holds in each situation. We apply the model to a collection of translational lung cancer genetic association studies that motivated this work. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ryan Sun and Zachary R. McCaw and Xihong Lin},
  doi          = {10.1080/01621459.2024.2422124},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {605-617},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing a large number of composite null hypotheses using conditionally symmetric multidimensional gaussian mixtures in genome-wide studies},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Space-time extremes of severe U.S. thunderstorm environments. <em>JASA</em>, <em>120</em>(550), 591-604. (<a href='https://doi.org/10.1080/01621459.2024.2421582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Severe thunderstorms cause substantial economic and human losses in the United States. Simultaneous high values of convective available potential energy (CAPE) and storm relative helicity (SRH) are favorable to severe weather, and both they and the composite variable PROD = CAPE × SRH can be used as indicators of severe thunderstorm activity. Their extremal spatial dependence exhibits temporal non-stationarity due to seasonality and large-scale atmospheric signals such as El Niño-Southern Oscillation (ENSO). In order to investigate this, we introduce a space-time model based on a max-stable, Brown–Resnick, field whose range depends on ENSO and on time through a tensor product spline. We also propose a max-stability test based on empirical likelihood and the bootstrap. The marginal and dependence parameters must be estimated separately owing to the complexity of the model, and we develop a bootstrap-based model selection criterion that accounts for the marginal uncertainty when choosing the dependence model. In the case study, the out-sample performance of our model is good. We find that extremes of PROD, CAPE, and SRH are generally more localized in summer and, in some regions, less localized during El Niño and La Niña events, and give meteorological interpretations of these phenomena. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jonathan Koh and Erwan Koch and Anthony C. Davison},
  doi          = {10.1080/01621459.2024.2421582},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {550},
  pages        = {591-604},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Space-time extremes of severe U.S. thunderstorm environments},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comment on “Data fission: Splitting a single data point” by james leiner, boyan duan, larry wasserman and aaditya ramdas. <em>JASA</em>, <em>120</em>(549), Views 20. (<a href='https://doi.org/10.1080/01621459.2024.2403728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {V. Roshan Joseph},
  doi          = {10.1080/01621459.2024.2403728},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {Views 20},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on “Data fission: Splitting a single data point” by james leiner, boyan duan, larry wasserman and aaditya ramdas},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional data analysis with r. <em>JASA</em>, <em>120</em>(549), 588-590. (<a href='https://doi.org/10.1080/01621459.2024.2425462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Piotr S. Kokoszka},
  doi          = {10.1080/01621459.2024.2425462},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {588-590},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Functional data analysis with r},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probability modeling and statistical inference in cancer screening. <em>JASA</em>, <em>120</em>(549), 587-588. (<a href='https://doi.org/10.1080/01621459.2024.2425460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Li C. Cheung},
  doi          = {10.1080/01621459.2024.2425460},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {587-588},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Probability modeling and statistical inference in cancer screening},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian nonparametrics for causal inference and missing data. <em>JASA</em>, <em>120</em>(549), 586-587. (<a href='https://doi.org/10.1080/01621459.2024.2423435'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {P. Richard Hahn},
  doi          = {10.1080/01621459.2024.2423435},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {586-587},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian nonparametrics for causal inference and missing data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROC analysis for classification and prediction in practice. <em>JASA</em>, <em>120</em>(549), 585-586. (<a href='https://doi.org/10.1080/01621459.2024.2423434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Mauricio Tec},
  doi          = {10.1080/01621459.2024.2423434},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {585-586},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {ROC analysis for classification and prediction in practice},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based machine learning. <em>JASA</em>, <em>120</em>(549), 584-585. (<a href='https://doi.org/10.1080/01621459.2024.2411074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Emanuela Furfaro},
  doi          = {10.1080/01621459.2024.2411074},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {584-585},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-based machine learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Operationalizing legislative bodies: A methodological and empirical perspective with a bayesian approach. <em>JASA</em>, <em>120</em>(549), 572-583. (<a href='https://doi.org/10.1080/01621459.2024.2413928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article extensively reviews applications, extensions, and models derived from the Bayesian ideal point estimator. We focus our attention on studies conducted in the United States as well as Latin America. First, we provide a detailed description of the Bayesian ideal point estimator. Next, we propose a new taxonomy to synthesize and frame technical developments and applications associated with the estimator in the context of the United States Congress and Latin American governing bodies. The literature available in Latin America allows us to conclude that few legislatures in the region have been analyzed using the methodology under discussion. Also, we highlight those parliaments of Latin America embedded in democratic presidential systems as novel scenarios for operationalizing the electoral behavior of legislative bodies through nominal voting data. Our findings show some alternatives for future research.},
  archive      = {J_JASA},
  author       = {Carolina Luque and Juan Sosa},
  doi          = {10.1080/01621459.2024.2413928},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {572-583},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Operationalizing legislative bodies: A methodological and empirical perspective with a bayesian approach},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution-free prediction intervals under covariate shift, with an application to causal inference. <em>JASA</em>, <em>120</em>(549), 559-571. (<a href='https://doi.org/10.1080/01621459.2024.2356886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to its appealing distribution-free feature, conformal inference has become a popular tool for constructing prediction intervals with a desired coverage rate. In scenarios involving covariate shift, where the shift function needs to be estimated from data, many existing methods resort to data-splitting techniques. However, these approaches often lead to wider intervals and less reliable coverage rates, especially when dealing with finite sample sizes. To address these challenges, we propose methods based on a pivotal quantity derived under a parametric working model and employ a resampling-based framework to approximate its distribution. The resampling-based approach can produce prediction intervals with a desired coverage rate without splitting the data and can be easily applied to causal inference settings where a shift in the covariate distribution can occur between treatment and control arms. Additionally, the proposed approaches enjoy a double robustness property and are adaptable to different prediction tasks. Our extensive numerical experiments demonstrate that, compared to existing methods, the proposed novel approaches can produce substantially shorter conformal prediction intervals with lower variability in the interval lengths while maintaining promising coverage rates and advantages in versatile usage. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jing Qin and Yukun Liu and Moming Li and Chiung-Yu Huang},
  doi          = {10.1080/01621459.2024.2356886},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {559-571},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distribution-free prediction intervals under covariate shift, with an application to causal inference},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rational kriging. <em>JASA</em>, <em>120</em>(549), 548-558. (<a href='https://doi.org/10.1080/01621459.2024.2356296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new kriging that has a rational form. It is shown that the generalized least squares estimator of the mean from rational kriging is much more well behaved than that of ordinary kriging. Parameter estimation and uncertainty quantification for rational kriging are proposed using a Gaussian process framework. A generalized version of rational kriging is also proposed, which includes ordinary and rational kriging as special cases. Extensive simulations carried out over a wide class of functions show that the generalized rational kriging performs on par or better than both ordinary and rational kriging in terms of prediction and uncertainty quantification. The only extra step needed for generalized rational kriging over ordinary kriging is the computation of Perron eigenvector of an augmented correlation matrix which can be computed in near linear time and therefore, its overall computational complexity is no more than that of ordinary kriging. The potential applications of the new kriging methods in the emulation of computationally expensive models and model calibration problems are illustrated with real and simulated examples. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {V. Roshan Joseph},
  doi          = {10.1080/01621459.2024.2356296},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {548-558},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rational kriging},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural networks for geospatial data. <em>JASA</em>, <em>120</em>(549), 535-547. (<a href='https://doi.org/10.1080/01621459.2024.2356293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of geospatial data has traditionally been model-based, with a mean model, customarily specified as a linear regression on the covariates, and a Gaussian process covariance model, encoding the spatial dependence. While nonlinear machine learning algorithms like neural networks are increasingly being used for spatial analysis, current approaches depart from the model-based setup and cannot explicitly incorporate spatial covariance. We propose NN-GLS , embedding neural networks directly within the traditional Gaussian process (GP) geostatistical model to accommodate nonlinear mean functions while retaining all other advantages of GP, like explicit modeling of the spatial covariance and predicting at new locations via kriging. In NN-GLS, estimation of the neural network parameters for the nonlinear mean of the Gaussian Process explicitly accounts for the spatial covariance through use of the generalized least squares (GLS) loss, thus, extending the linear case. We show that NN-GLS admits a representation as a special type of graph neural network (GNN). This connection facilitates the use of standard neural network computational techniques for irregular geospatial data, enabling novel and scalable mini-batching, backpropagation, and kriging schemes. We provide methodology to obtain uncertainty bounds for estimation and predictions from NN-GLS. Theoretically, we show that NN-GLS will be consistent for irregularly observed spatially correlated data processes. We also provide a finite sample concentration rate, which quantifies the need to accurately model the spatial covariance in neural networks for dependent data. To our knowledge, these are the first large-sample results for any neural network algorithm for irregular spatial data. We demonstrate the methodology through numerous simulations and an application to air pollution modeling. We develop a software implementation of NN-GLS in the Python package geospaNN. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Wentao Zhan and Abhirup Datta},
  doi          = {10.1080/01621459.2024.2356293},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {535-547},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Neural networks for geospatial data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust augmented model accuracy transfer inference with high dimensional features. <em>JASA</em>, <em>120</em>(549), 524-534. (<a href='https://doi.org/10.1080/01621459.2024.2356291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is crucial for training models that generalize to unlabeled target populations using labeled source data, especially in real-world studies where label scarcity and covariate shift are common. While most research focuses on model estimation, there is limited literature on transfer inference for model accuracy despite its importance. We introduce a novel Doubly Robust Augmented Model Accuracy Transfer Inferen Ce (DRAMATIC) method for point and interval estimation of commonly used classification performance measures in an unlabeled target population with labeled source data. DRAMATIC derives and evaluates a potentially misspecified risk model for a binary response, leveraging high-dimensional adjustment features from both source and target data. It builds on an imputation model for the response mean and a density ratio model to characterize distributional shifts. The method constructs doubly robust estimators that are valid when either model is correctly specified and certain sparsity assumptions hold. Simulations show negligible bias in point estimation and satisfactory empirical coverage levels in confidence intervals. The utility of DRAMATIC is illustrated by transferring a genetic risk prediction model and its accuracy evaluation for type II diabetes across two patient cohorts in Mass General Brigham (MGB). Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Doudou Zhou and Molei Liu and Mengyan Li and Tianxi Cai},
  doi          = {10.1080/01621459.2024.2356291},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {524-534},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Doubly robust augmented model accuracy transfer inference with high dimensional features},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized data thinning using sufficient statistics. <em>JASA</em>, <em>120</em>(549), 511-523. (<a href='https://doi.org/10.1080/01621459.2024.2353948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our goal is to develop a general strategy to decompose a random variable X into multiple independent random variables, without sacrificing any information about unknown parameters. A recent paper showed that for some well-known natural exponential families, X can be thinned into independent random variables X ( 1 ) , … , X ( K ) , such that X = ∑ k = 1 K X ( k ) . These independent random variables can then be used for various model validation and inference tasks, including in contexts where traditional sample splitting fails. In this article, we generalize their procedure by relaxing this summation requirement and simply asking that some known function of the independent random variables exactly reconstruct X . This generalization of the procedure serves two purposes. First, it greatly expands the families of distributions for which thinning can be performed. Second, it unifies sample splitting and data thinning, which on the surface seem to be very different, as applications of the same principle. This shared principle is sufficiency. We use this insight to perform generalized thinning operations for a diverse set of families. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ameer Dharamshi and Anna Neufeld and Keshav Motwani and Lucy L. Gao and Daniela Witten and Jacob Bien},
  doi          = {10.1080/01621459.2024.2353948},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {511-523},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Generalized data thinning using sufficient statistics},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomness of shapes and statistical inference on shapes via the smooth euler characteristic transform. <em>JASA</em>, <em>120</em>(549), 498-510. (<a href='https://doi.org/10.1080/01621459.2024.2353947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we establish the mathematical foundations for modeling the randomness of shapes and conducting statistical inference on shapes using the smooth Euler characteristic transform. Based on these foundations, we propose two Chi-squared statistic-based algorithms for testing hypotheses on random shapes. Simulation studies are presented to validate our mathematical derivations and to compare our algorithms with state-of-the-art methods to demonstrate the utility of our proposed framework. As real applications, we analyze a dataset of mandibular molars from four genera of primates and show that our algorithms have the power to detect significant shape differences that recapitulate known morphological variation across suborders. Altogether, our discussions bridge the following fields: algebraic and computational topology, probability theory and stochastic processes, Sobolev spaces and functional analysis, analysis of variance for functional data, and geometric morphometrics. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Kun Meng and Jinyu Wang and Lorin Crawford and Ani Eloyan},
  doi          = {10.1080/01621459.2024.2353947},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {498-510},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Randomness of shapes and statistical inference on shapes via the smooth euler characteristic transform},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-aligned random partition model (GARP). <em>JASA</em>, <em>120</em>(549), 486-497. (<a href='https://doi.org/10.1080/01621459.2024.2353943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian nonparametric mixtures and random partition models are powerful tools for probabilistic clustering. However, standard independent mixture models can be restrictive in some applications such as inference on cell lineage due to the biological relations of the clusters. The increasing availability of large genomic data requires new statistical tools to perform model-based clustering and infer the relationship between homogeneous subgroups of units. Motivated by single-cell RNA data we develop a novel dependent mixture model to jointly perform cluster analysis and align the clusters on a graph. Our flexible graph-aligned random partition model (GARP) exploits Gibbs-type priors as building blocks, allowing us to derive analytical results for the probability mass function (pmf) on the graph-aligned random partition. We derive a generalization of the Chinese restaurant process from the pmf and a related efficient and neat MCMC algorithm to implement Bayesian inference. We illustrate posterior inference under the GARP using single-cell RNA-seq data from mice stem cells. We further investigate the performance of the model in recovering the underlying clustering structure as well as the underlying graph by means of simulation studies. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Giovanni Rebaudo and Peter Müller},
  doi          = {10.1080/01621459.2024.2353943},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {486-497},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Graph-aligned random partition model (GARP)},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tests for large-dimensional shape matrices via tyler’s m estimators. <em>JASA</em>, <em>120</em>(549), 472-485. (<a href='https://doi.org/10.1080/01621459.2024.2350573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tyler’s M estimator, as a robust alternative to the sample covariance matrix, has been widely applied in robust statistics. However, classical theory on Tyler’s M estimator is mainly developed in the low-dimensional regime for elliptical populations. It remains largely unknown when the parameter of dimension p grows proportionally to the sample size n for general populations. By using the eigenvalues of Tyler’s M estimator, this article develops tests for the identity and equality of shape matrices in a large-dimensional framework where the dimension-to-sample size ratio p / n has a limit in (0, 1). The proposed tests can be applied to a broad class of multivariate distributions including the family of elliptical distributions (see model (2.1) for details). To analyze both the null and alternative distributions of the proposed tests, we provide a unified theory on the spectrum of a large-dimensional Tyler’s M estimator when the underlying population is general. Simulation results demonstrate good performance and robustness of our tests. An empirical analysis of the Fama-French 49 industrial portfolios is carried out to demonstrate the shape of the portfolios varying. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Runze Li and Weiming Li and Qinwen Wang},
  doi          = {10.1080/01621459.2024.2350573},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {472-485},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Tests for large-dimensional shape matrices via tyler’s m estimators},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controlled discovery and localization of signals via bayesian linear programming. <em>JASA</em>, <em>120</em>(549), 460-471. (<a href='https://doi.org/10.1080/01621459.2024.2347667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientists often must simultaneously localize and discover signals. For instance, in genetic fine-mapping, high correlations between nearby genetic variants make it hard to identify the exact locations of causal variants. So the statistical task is to output as many disjoint regions containing a signal as possible, each as small as possible, while controlling false positives. Similar problems arise, for example, when locating stars in astronomical surveys and in changepoint detection. Common Bayesian approaches to these problems involve computing a posterior distribution over signal locations. However, existing procedures to translate these posteriors into credible regions for the signals fail to capture all the information in the posterior, leading to lower power and (sometimes) inflated false discoveries. We introduce Bayesian Linear Programming (BLiP), which can efficiently convert any posterior distribution over signals into credible regions for signals. BLiP overcomes an extremely high-dimensional and nonconvex problem to verifiably nearly maximize expected power while controlling false positives. Applying BLiP to existing state-of-the-art analyses of UK Biobank data (for genetic fine-mapping) and the Sloan Digital Sky Survey (for astronomical point source detection) increased power by 30%–120% in just a few minutes of additional computation. BLiP is implemented in pyblip (Python) and blipr (R). Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Asher Spector and Lucas Janson},
  doi          = {10.1080/01621459.2024.2347667},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {460-471},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Controlled discovery and localization of signals via bayesian linear programming},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing the number of common factors by bootstrapped sample covariance matrix in high-dimensional factor models. <em>JASA</em>, <em>120</em>(549), 448-459. (<a href='https://doi.org/10.1080/01621459.2024.2346364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the impact of bootstrap procedure on the eigenvalue distributions of the sample covariance matrix under a high-dimensional factor structure. We provide asymptotic distributions for the top eigenvalues of bootstrapped sample covariance matrix under mild conditions. After bootstrap, the spiked eigenvalues which are driven by common factors will converge weakly to Gaussian limits after proper scaling and centralization. However, the largest non-spiked eigenvalue is mainly determined by the order statistics of the bootstrap resampling weights, and follows extreme value distribution. Based on the disparate behavior of the spiked and non-spiked eigenvalues, we propose innovative methods to test the number of common factors. Indicated by extensive numerical and empirical studies, the proposed methods perform reliably and convincingly under the existence of both weak factors and cross-sectionally correlated errors. Our technical details contribute to random matrix theory on spiked covariance model with convexly decaying density and unbounded support, or with general elliptical distributions. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Long Yu and Peng Zhao and Wang Zhou},
  doi          = {10.1080/01621459.2024.2346364},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {448-459},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing the number of common factors by bootstrapped sample covariance matrix in high-dimensional factor models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selection and aggregation of conformal prediction sets. <em>JASA</em>, <em>120</em>(549), 435-447. (<a href='https://doi.org/10.1080/01621459.2024.2344700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction is a generic methodology for finite-sample valid distribution-free prediction. This technique has garnered a lot of attention in the literature partly because it can be applied with any machine learning algorithm that provides point predictions to yield valid prediction regions. Of course, the efficiency (width/volume) of the resulting prediction region depends on the performance of the machine learning algorithm. In the context of point prediction, several techniques (such as cross-validation) exist to select one of many machine learning algorithms for better performance. In contrast, such selection techniques are seldom discussed in the context of set prediction (or prediction regions). In this article, we consider the problem of obtaining the smallest conformal prediction region given a family of machine learning algorithms. We provide two general-purpose selection algorithms and consider coverage as well as width properties of the final prediction region. The first selection method yields the smallest width prediction region among the family of conformal prediction regions for all sample sizes but only has an approximate coverage guarantee. The second selection method has a finite sample coverage guarantee but only attains close to the smallest width. The approximate optimal width property of the second method is quantified via an oracle inequality. As an illustration, we consider the use of aggregation of nonparametric regression estimators in the split conformal method with the absolute residual conformal score. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yachong Yang and Arun Kumar Kuchibhotla},
  doi          = {10.1080/01621459.2024.2344700},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {435-447},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Selection and aggregation of conformal prediction sets},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling and learning on high-dimensional matrix-variate sequences. <em>JASA</em>, <em>120</em>(549), 419-434. (<a href='https://doi.org/10.1080/01621459.2024.2344687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new matrix factor model, named RaDFaM, which is strictly derived from the general rank decomposition and assumes a high-dimensional vector factor model structure for each basis vector. RaDFaM contributes a novel class of low-rank latent structures that trade off between signal intensity and dimension reduction from a tensor subspace perspective. Based on the intrinsic separable covariance structure of RaDFaM, for a collection of matrix-valued observations, we derive a new class of PCA variants for estimating loading matrices, and sequentially the latent factor matrices. The peak signal-to-noise ratio of RaDFaM is proved to be superior in the category of PCA-type estimators. We also establish an asymptotic theory including the consistency, convergence rates, and asymptotic distributions for components in the signal part. Numerically, we demonstrate the performance of RaDFaM in applications such as matrix reconstruction, supervised learning, and clustering, on uncorrelated and correlated data, respectively. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xu Zhang and Catherine C. Liu and Jianhua Guo and K. C. Yuen and A. H. Welsh},
  doi          = {10.1080/01621459.2024.2344687},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {419-434},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling and learning on high-dimensional matrix-variate sequences},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sobolev calibration of imperfect computer models. <em>JASA</em>, <em>120</em>(549), 408-418. (<a href='https://doi.org/10.1080/01621459.2024.2340793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibration refers to the statistical estimation of unknown model parameters in computer experiments, such that computer experiments can match underlying physical systems. This work develops a new calibration method for imperfect computer models, Sobolev calibration, which can rule out calibration parameters that generate overfitting calibrated functions. We prove that the Sobolev calibration enjoys desired theoretical properties including fast convergence rate, asymptotic normality and semiparametric efficiency. We also demonstrate an interesting property that the Sobolev calibration can bridge the gap between two influential methods: L 2 calibration and Kennedy and O’Hagan’s calibration. In addition to exploring the deterministic physical experiments, we theoretically justify that our method can transfer to the case when the physical process is indeed a Gaussian process, which follows the original idea of Kennedy and O’Hagan’s. Numerical simulations as well as a real-world example illustrate the competitive performance of the proposed method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Qingwen Zhang and Wenjia Wang},
  doi          = {10.1080/01621459.2024.2340793},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {408-418},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sobolev calibration of imperfect computer models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sharp-SSL: Selective high-dimensional axis-aligned random projections for semi-supervised learning. <em>JASA</em>, <em>120</em>(549), 395-407. (<a href='https://doi.org/10.1080/01621459.2024.2340792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for high-dimensional semi-supervised learning problems based on the careful aggregation of the results of a low-dimensional procedure applied to many axis-aligned random projections of the data. Our primary goal is to identify important variables for distinguishing between the classes; existing low-dimensional methods can then be applied for final class assignment. To this end, we score projections according to their class-distinguishing ability; for instance, motivated by a generalized Rayleigh quotient, we can compute the traces of estimated whitened between-class covariance matrices on the projected data. This enables us to assign an importance weight to each variable for a given projection, and to select our signal variables by aggregating these weights over high-scoring projections. Our theory shows that the resulting Sharp-SSL algorithm is able to recover the signal coordinates with high probability when we aggregate over sufficiently many random projections and when the base procedure estimates the diagonal entries of the whitened between-class covariance matrix sufficiently well. For the Gaussian EM base procedure, we provide a new analysis of its performance in semi-supervised settings that controls the parameter estimation error in terms of the proportion of labeled data in the sample. Numerical results on both simulated data and a real colon tumor dataset support the excellent empirical performance of the method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Tengyao Wang and Edgar Dobriban and Milana Gataric and Richard J. Samworth},
  doi          = {10.1080/01621459.2024.2340792},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {395-407},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sharp-SSL: Selective high-dimensional axis-aligned random projections for semi-supervised learning},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient nonparametric estimation of stochastic policy effects with clustered interference. <em>JASA</em>, <em>120</em>(549), 382-394. (<a href='https://doi.org/10.1080/01621459.2024.2340789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interference occurs when a unit’s treatment (or exposure) affects another unit’s outcome. In some settings, units may be grouped into clusters such that it is reasonable to assume that interference, if present, only occurs between individuals in the same cluster, that is, there is clustered interference. Various causal estimands have been proposed to quantify treatment effects under clustered interference from observational data, but these estimands either entail treatment policies lacking real-world relevance or are based on parametric propensity score models. Here, we propose new causal estimands based on modification of the propensity score distribution which may be more relevant in many contexts and are not based on parametric models. Nonparametric sample splitting estimators of the new estimands are constructed, which allow for flexible data-adaptive estimation of nuisance functions and are consistent, asymptotically normal, and efficient, converging at the usual parametric rate. Simulations show the finite sample performance of the proposed estimators. The proposed methods are applied to evaluate the effect of water, sanitation, and hygiene facilities on diarrhea among children in Senegal. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Chanhwa Lee and Donglin Zeng and Michael G. Hudgens},
  doi          = {10.1080/01621459.2024.2340789},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {382-394},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficient nonparametric estimation of stochastic policy effects with clustered interference},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring independent sets of gaussian variables after thresholding correlations. <em>JASA</em>, <em>120</em>(549), 370-381. (<a href='https://doi.org/10.1080/01621459.2024.2337158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider testing whether a set of Gaussian variables, selected from the data, is independent of the remaining variables. This set is selected via a very simple approach: these are the variables for which the correlation with all other variables falls below some threshold. Unlike other settings in selective inference, failure to account for the selection step leads to excessively conservative (as opposed to anti-conservative) results. We propose a new test that conditions on the event that the selection resulted in the set of variables in question, and thus is not overly conservative. To achieve computational tractability, we develop a characterization of the conditioning event in terms of the canonical correlation between groups of random variables. In simulation studies and in the analysis of gene co-expression networks, we show that our approach has much higher power than a “naive” approach that ignores the effect of selection. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Arkajyoti Saha and Daniela Witten and Jacob Bien},
  doi          = {10.1080/01621459.2024.2337158},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {370-381},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inferring independent sets of gaussian variables after thresholding correlations},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized bayesian additive regression trees models: Beyond conditional conjugacy. <em>JASA</em>, <em>120</em>(549), 356-369. (<a href='https://doi.org/10.1080/01621459.2024.2337156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian additive regression trees have seen increased interest in recent years due to their ability to combine machine learning techniques with principled uncertainty quantification. The Bayesian backfitting algorithm used to fit BART models, however, limits their application to a small class of models for which conditional conjugacy exists. In this article, we greatly expand the domain of applicability of BART to arbitrary generalized BART models by introducing a very simple, tuning-parameter-free, reversible jump Markov chain Monte Carlo algorithm. Our algorithm requires only that the user be able to compute the likelihood and (optionally) its gradient and Fisher information. The potential applications are very broad; we consider examples in survival analysis, structured heteroscedastic regression, and gamma shape regression. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Antonio R. Linero},
  doi          = {10.1080/01621459.2024.2337156},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {356-369},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Generalized bayesian additive regression trees models: Beyond conditional conjugacy},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for noisy matrix completion incorporating auxiliary information. <em>JASA</em>, <em>120</em>(549), 343-355. (<a href='https://doi.org/10.1080/01621459.2024.2335591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates statistical inference for noisy matrix completion in a semi-supervised model when auxiliary covariates are available. The model consists of two parts. One part is a low-rank matrix induced by unobserved latent factors; the other part models the effects of the observed covariates through a coefficient matrix which is composed of high-dimensional column vectors. We model the observational pattern of the responses through a logistic regression of the covariates, and allow its probability to go to zero as the sample size increases. We apply an iterative least squares (LS) estimation approach in our considered context. The iterative LS methods in general enjoy a low computational cost, but deriving the statistical properties of the resulting estimators is a challenging task. We show that our method only needs a few iterations, and the resulting entry-wise estimators of the low-rank matrix and the coefficient matrix are guaranteed to have asymptotic normal distributions. As a result, individual inference can be conducted for each entry of the unknown matrices. We also propose a simultaneous testing procedure with multiplier bootstrap for the high-dimensional coefficient matrix. This simultaneous inferential tool can help us further investigate the effects of covariates for the prediction of missing entries. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shujie Ma and Po-Yao Niu and Yichong Zhang and Yinchu Zhu},
  doi          = {10.1080/01621459.2024.2335591},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {343-355},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for noisy matrix completion incorporating auxiliary information},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly-Valid/Doubly-sharp sensitivity analysis for causal inference with unmeasured confounding. <em>JASA</em>, <em>120</em>(549), 331-342. (<a href='https://doi.org/10.1080/01621459.2024.2335588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of constructing bounds on the average treatment effect (ATE) when unmeasured confounders exist but have bounded influence. Specifically, we assume that omitted confounders could not change the odds of treatment for any unit by more than a fixed factor. We derive the sharp partial identification bounds implied by this assumption by leveraging distributionally robust optimization, and we propose estimators of these bounds with several novel robustness properties. The first is double sharpness : our estimators consistently estimate the sharp ATE bounds when one of two nuisance parameters is misspecified and achieve semiparametric efficiency when all nuisance parameters are suitably consistent. The second and more novel property is double validity : even when most nuisance parameters are misspecified, our estimators still provide valid but possibly conservative bounds for the ATE and our Wald confidence intervals remain valid even when our estimators are not asymptotically normal. As a result, our estimators provide a highly credible method for sensitivity analysis of causal inferences. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jacob Dorn and Kevin Guo and Nathan Kallus},
  doi          = {10.1080/01621459.2024.2335588},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {331-342},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Doubly-Valid/Doubly-sharp sensitivity analysis for causal inference with unmeasured confounding},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic regenerative simulation via non-reversible simulated tempering. <em>JASA</em>, <em>120</em>(549), 318-330. (<a href='https://doi.org/10.1080/01621459.2024.2335587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulated Tempering (ST) is an MCMC algorithm for complex target distributions that operates on a path between the target and a more amenable reference distribution. Crucially, if the reference enables iid sampling, ST is regenerative and can be parallelized across independent tours. However, the difficulty of tuning ST has hindered its widespread adoption. In this work, we develop a simple nonreversible ST (NRST) algorithm, a general theoretical analysis of ST, and an automated tuning procedure for ST. A core contribution that arises from the analysis is a novel performance metric— Tour Effectiveness (TE) —that controls the asymptotic variance of estimates from ST for bounded test functions. We use the TE to show that NRST dominates its reversible counterpart. We then develop an automated tuning procedure for NRST algorithms that targets the TE while minimizing computational cost. This procedure enables straightforward integration of NRST into existing probabilistic programming languages. We provide extensive experimental evidence that our tuning scheme improves the performance and robustness of NRST algorithms on a diverse set of probabilistic models. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Miguel Biron-Lattes and Trevor Campbell and Alexandre Bouchard-Côté},
  doi          = {10.1080/01621459.2024.2335587},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {318-330},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Automatic regenerative simulation via non-reversible simulated tempering},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CARE: Large precision matrix estimation for compositional data. <em>JASA</em>, <em>120</em>(549), 305-317. (<a href='https://doi.org/10.1080/01621459.2024.2335586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional compositional data are prevalent in many applications. The simplex constraint poses intrinsic challenges to inferring the conditional dependence relationships among the components forming a composition, as encoded by a large precision matrix. We introduce a precise specification of the compositional precision matrix and relate it to its basis counterpart, which is shown to be asymptotically identifiable under suitable sparsity assumptions. By exploiting this connection, we propose a composition adaptive regularized estimation (CARE) method for estimating the sparse basis precision matrix. We derive rates of convergence for the estimator and provide theoretical guarantees on support recovery and data-driven parameter tuning. Our theory reveals an intriguing tradeoff between identification and estimation, thereby highlighting the blessing of dimensionality in compositional data analysis. In particular, in sufficiently high dimensions, the CARE estimator achieves minimax optimality and performs as well as if the basis were observed. We further discuss how our framework can be extended to handle data containing zeros, including sampling zeros and structural zeros. The advantages of CARE over existing methods are illustrated by simulation studies and an application to inferring microbial ecological networks in the human gut. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shucong Zhang and Huiyuan Wang and Wei Lin},
  doi          = {10.1080/01621459.2024.2335586},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {305-317},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {CARE: Large precision matrix estimation for compositional data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extreme value statistics in semi-supervised models. <em>JASA</em>, <em>120</em>(549), 291-304. (<a href='https://doi.org/10.1080/01621459.2024.2333582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider extreme value analysis in a semi-supervised setting, where we observe, next to the n data on the target variable, n + m data on one or more covariates. This is called the semi-supervised model with n labeled and m unlabeled data. By exploiting the tail dependence between the target variable and the covariates, we derive estimators for the extreme value index and extreme quantiles of the target variable in this setting and establish their asymptotic behavior. Our estimators substantially improve the univariate estimators, based on only the n target variable data, in terms of asymptotic variances whereas the asymptotic biases remain unchanged. A simulation study confirms the substantially improved behavior of both estimators. Finally the estimation method is applied to rainfall data in France. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Hanan Ahmed and John H.J. Einmahl and Chen Zhou},
  doi          = {10.1080/01621459.2024.2333582},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {291-304},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Extreme value statistics in semi-supervised models},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly flexible estimation under label shift. <em>JASA</em>, <em>120</em>(549), 278-290. (<a href='https://doi.org/10.1080/01621459.2024.2321653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies ranging from clinical medicine to policy research, complete data are usually available from a population 𝒫 , but the quantity of interest is often sought for a related but different population 𝒬 which only has partial data. We consider the setting when both outcome Y and covariate X are available from P but only X is available from Q , under the label shift assumption; that is, the conditional distribution of X given Y is the same in the two populations. To estimate the parameter of interest in Q by leveraging information from P , three ingredients are essential: (a) the common conditional distribution of X given Y , (b) the regression model of Y given X in P , and (c) the density ratio of the outcome Y between the two populations. We propose an estimation procedure that only needs some standard nonparametric technique to approximate the conditional expectations with respect to (a), while by no means needs an estimate or model for (b) or (c); that is, doubly flexible to the model misspecifications of both (b) and (c). This is conceptually different from the well-known doubly robust estimation in that, double robustness allows at most one model to be misspecified whereas our proposal can allow both (b) and (c) to be misspecified. This is of particular interest in label shift because estimating (c) is difficult, if not impossible, by virtue of the absence of the Y -data from Q . While estimating (b) is occasionally off-the-shelf, it may encounter issues related to the curse of dimensionality or computational challenges. We develop the large sample theory for the proposed estimator, and examine its finite-sample performance through simulation studies as well as an application to the MIMIC-III database. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Seong-ho Lee and Yanyuan Ma and Jiwei Zhao},
  doi          = {10.1080/01621459.2024.2321653},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {278-290},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Doubly flexible estimation under label shift},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust personalized federated learning with sparse penalization. <em>JASA</em>, <em>120</em>(549), 266-277. (<a href='https://doi.org/10.1080/01621459.2024.2321652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging topic due to its advantage in collaborative learning with distributed data. Due to the heterogeneity in the local data-generating mechanism, it is important to consider personalization when developing federated learning methods. In this work, we propose a personalized federated learning (PFL) method to address the robust regression problem. Specifically, we aim to learn the regression weight by solving a Huber loss with the sparse fused penalty. Additionally, we designed our personalized federated learning for robust and sparse regression (PerFL-RSR) algorithm to solve the estimation problem in the federated system efficiently. Theoretically, we show that the proposed PerFL-RSR reaches a convergence rate of O ( 1 / T ) , and the proposed estimator is statistically consistent. Thorough experiments and real data analysis are conducted to corroborate the theoretical results of our proposed personalized federated learning method. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Weidong Liu and Xiaojun Mao and Xiaofei Zhang and Xin Zhang},
  doi          = {10.1080/01621459.2024.2321652},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {266-277},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust personalized federated learning with sparse penalization},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling recurrent failures on large directed networks. <em>JASA</em>, <em>120</em>(549), 251-265. (<a href='https://doi.org/10.1080/01621459.2024.2319897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many lifeline infrastructure systems consist of thousands of components configured in a complex directed network. Disruption of the infrastructure constitutes a recurrent failure process over a directed network. Statistical inference for such network recurrence data is challenging because of the large number of nodes with irregular connections among them. Motivated by 16 years of Scottish Water operation records, we propose a network Gamma-Poisson Autoregressive NHPP (GPAN) model for recurrent failure data from large-scale directed physical networks. The model consists of two layers: the temporal layer applies a Non-Homogeneous Poisson Process (NHPP) with node-specific frailties, and the spatial layer uses a well-orchestrated gamma-Poisson autoregressive scheme to establish correlations among the frailties. Under the network-GPAN model, we develop a sum-product algorithm to compute the marginal distribution for each frailty conditional on the recurrence data. The marginal conditional frailty distributions are useful for predicting future failures based on historical data. In addition, the ability to rapidly compute these marginal distributions allows adoption of an EM type algorithm for estimation. Through a Bethe approximation, the output from the sum-product algorithm is used to compute maximum log-likelihood estimates. Applying the methods to the Scottish Water network, we demonstrate utility in aiding operation management and risk assessment of the water utility. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Qingqing Zhai and Zhisheng Ye and Cheng Li and Matthew Revie and David B. Dunson},
  doi          = {10.1080/01621459.2024.2319897},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {251-265},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling recurrent failures on large directed networks},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ranking inferences based on the top choice of multiway comparisons. <em>JASA</em>, <em>120</em>(549), 237-250. (<a href='https://doi.org/10.1080/01621459.2024.2316364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by many applications such as online recommendations and individual choices, this article considers ranking inference of n items based on the observed data on the top choice among M randomly selected items at each trial. This is a useful modification of the Plackett-Luce model for M -way ranking with only the top choice observed and is an extension of the celebrated Bradley-Terry-Luce model that corresponds to M = 2. Under a uniform sampling scheme in which any M distinguished items are selected for comparisons with probability p and the selected M items are compared L times with multinomial outcomes, we establish the statistical rates of convergence for underlying n preference scores using both l 2 -norm and l ∞ -norm, under the minimum sampling complexity (smallest order of p ). In addition, we establish the asymptotic normality of the maximum likelihood estimator that allows us to construct confidence intervals for the underlying scores. Furthermore, we propose a novel inference framework for ranking items through a sophisticated maximum pairwise difference statistic whose distribution is estimated via a valid Gaussian multiplier bootstrap. The estimated distribution is then used to construct simultaneous confidence intervals for the differences in the preference scores and the ranks of individual items. They also enable us to address various inference questions on the ranks of these items. Extensive simulation studies lend further support to our theoretical results. A real data application illustrates the usefulness of the proposed methods. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jianqing Fan and Zhipeng Lou and Weichen Wang and Mengxin Yu},
  doi          = {10.1080/01621459.2024.2316364},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {237-250},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Ranking inferences based on the top choice of multiway comparisons},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence intervals for parameters of unobserved events. <em>JASA</em>, <em>120</em>(549), 226-236. (<a href='https://doi.org/10.1080/01621459.2024.2314318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a finite sample from an unknown distribution over a countable alphabet. Unobserved events are alphabet symbols which do not appear in the sample. Estimating the probabilities of unobserved events is a basic problem in statistics and related fields, which was extensively studied in the context of point estimation. In this work we introduce a novel interval estimation scheme for unobserved events. Our proposed framework applies selective inference, as we construct confidence intervals (CIs) for the desired set of parameters. Interestingly, we show that obtained CIs are dimension-free, as they do not grow with the alphabet size. Further, we show that these CIs are (almost) tight, in the sense that they cannot be further improved without violating the prescribed coverage rate. We demonstrate the performance of our proposed scheme in synthetic and real-world experiments, showing a significant improvement over the alternatives. Finally, we apply our proposed scheme to large alphabet modeling. We introduce a novel simultaneous CI scheme for large alphabet distributions which outperforms currently known methods while maintaining the prescribed coverage rate. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Amichai Painsky},
  doi          = {10.1080/01621459.2024.2314318},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {226-236},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Confidence intervals for parameters of unobserved events},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable and efficient infinite-order vector autoregressive model for high-dimensional time series. <em>JASA</em>, <em>120</em>(549), 212-225. (<a href='https://doi.org/10.1080/01621459.2024.2311365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a special infinite-order vector autoregressive (VAR) model, the vector autoregressive moving average (VARMA) model can capture much richer temporal patterns than the widely used finite-order VAR model. However, its practicality has long been hindered by its non-identifiability, computational intractability, and difficulty of interpretation, especially for high-dimensional time series. This article proposes a novel sparse infinite-order VAR model for high-dimensional time series, which avoids all above drawbacks while inheriting essential temporal patterns of the VARMA model. As another attractive feature, the temporal and cross-sectional structures of the VARMA-type dynamics captured by this model can be interpreted separately, since they are characterized by different sets of parameters. This separation naturally motivates the sparsity assumption on the parameters determining the cross-sectional dependence. As a result, greater statistical efficiency and interpretability can be achieved with little loss of temporal information. We introduce two l 1 -regularized estimation methods for the proposed model, which can be efficiently implemented via block coordinate descent algorithms, and derive the corresponding nonasymptotic error bounds. A consistent model order selection method based on the Bayesian information criteria is also developed. The merit of the proposed approach is supported by simulation studies and a real-world macroeconomic data analysis. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yao Zheng},
  doi          = {10.1080/01621459.2024.2311365},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {212-225},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An interpretable and efficient infinite-order vector autoregressive model for high-dimensional time series},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic low-rank tensor bandits for multi-dimensional online decision making. <em>JASA</em>, <em>120</em>(549), 198-211. (<a href='https://doi.org/10.1080/01621459.2024.2311364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional online decision making plays a crucial role in many real applications such as online recommendation and digital marketing. In these problems, a decision at each time is a combination of choices from different types of entities. To solve it, we introduce stochastic low-rank tensor bandits, a class of bandits whose mean rewards can be represented as a low-rank tensor. We consider two settings, tensor bandits without context and tensor bandits with context. In the first setting, the platform aims to find the optimal decision with the highest expected reward, a.k.a, the largest entry of true reward tensor. In the second setting, some modes of the tensor are contexts and the rest modes are decisions, and the goal is to find the optimal decision given the contextual information. We propose two learning algorithms tensor elimination and tensor epoch-greedy for tensor bandits without context, and derive finite-time regret bounds for them. Comparing with existing competitive methods, tensor elimination has the best overall regret bound and tensor epoch-greedy has a sharper dependency on dimensions of the reward tensor. Furthermore, we develop a practically effective Bayesian algorithm called tensor ensemble sampling for tensor bandits with context. Extensive simulations and real analysis in online advertising data back up our theoretical findings and show that our algorithms outperform various state-of-the-art approaches that ignore the tensor low-rank structure. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jie Zhou and Botao Hao and Zheng Wen and Jingfei Zhang and Will Wei Sun},
  doi          = {10.1080/01621459.2024.2311364},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {198-211},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Stochastic low-rank tensor bandits for multi-dimensional online decision making},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-free statistical inference on high-dimensional data. <em>JASA</em>, <em>120</em>(549), 186-197. (<a href='https://doi.org/10.1080/01621459.2024.2310314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to develop an effective model-free inference procedure for high-dimensional data. We first reformulate the hypothesis testing problem via sufficient dimension reduction framework. With the aid of new reformulation, we propose a new test statistic and show that its asymptotic distribution is χ 2 distribution whose degree of freedom does not depend on the unknown population distribution. We further conduct power analysis under local alternative hypotheses. In addition, we study how to control the false discovery rate of the proposed χ 2 tests, which are correlated, to identify important predictors under a model-free framework. To this end, we propose a multiple testing procedure and establish its theoretical guarantees. Monte Carlo simulation studies are conducted to assess the performance of the proposed tests and an empirical analysis of a real-world dataset is used to illustrate the proposed methodology. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Xu Guo and Runze Li and Zhe Zhang and Changliang Zou},
  doi          = {10.1080/01621459.2024.2310314},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {186-197},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-free statistical inference on high-dimensional data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rejoinder. <em>JASA</em>, <em>120</em>(549), 180-185. (<a href='https://doi.org/10.1080/01621459.2025.2459216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {James Leiner and Boyan Duan and Larry Wasserman and Aaditya Ramdas},
  doi          = {10.1080/01621459.2025.2459216},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {180-185},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion on “Data fission: Splitting a single data point”. <em>JASA</em>, <em>120</em>(549), 178-179. (<a href='https://doi.org/10.1080/01621459.2024.2416913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Zhigen Zhao},
  doi          = {10.1080/01621459.2024.2416913},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {178-179},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion on “Data fission: Splitting a single data point”},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comments on “Data fission: Splitting a single data point” by james leiner, boyan duan, larry wasserman, and aaditya ramdas. <em>JASA</em>, <em>120</em>(549), 176-177. (<a href='https://doi.org/10.1080/01621459.2024.2412808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Lijun Wang and Hongyu Zhao},
  doi          = {10.1080/01621459.2024.2412808},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {176-177},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comments on “Data fission: Splitting a single data point” by james leiner, boyan duan, larry wasserman, and aaditya ramdas},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comment on “Data fission: Splitting a single data point” data fission for unsupervised learning: A discussion on post-clustering inference and the challenges of debiasing. <em>JASA</em>, <em>120</em>(549), 174-175. (<a href='https://doi.org/10.1080/01621459.2024.2412191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Changhu Wang and Xinzhou Ge and Dongyuan Song and Jingyi Jessica Li},
  doi          = {10.1080/01621459.2024.2412191},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {174-175},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on “Data fission: Splitting a single data point” data fission for unsupervised learning: A discussion on post-clustering inference and the challenges of debiasing},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comment on “Data fission: Splitting a single data point”. <em>JASA</em>, <em>120</em>(549), 172-173. (<a href='https://doi.org/10.1080/01621459.2024.2402523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Philip Waggoner},
  doi          = {10.1080/01621459.2024.2402523},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {172-173},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on “Data fission: Splitting a single data point”},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comments on “Data fission: Splitting a single data point”. <em>JASA</em>, <em>120</em>(549), 170-171. (<a href='https://doi.org/10.1080/01621459.2024.2414865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Sanat K. Sarkar},
  doi          = {10.1080/01621459.2024.2414865},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {170-171},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comments on “Data fission: Splitting a single data point”},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of “Data fission: Splitting a single data point” by leiner et al.. <em>JASA</em>, <em>120</em>(549), 168-169. (<a href='https://doi.org/10.1080/01621459.2024.2413426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jing Lei and Natalia L. Oliveira and Ryan J. Tibshirani},
  doi          = {10.1080/01621459.2024.2413426},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {168-169},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Data fission: Splitting a single data point” by leiner et al.},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical bayes estimation via data fission. <em>JASA</em>, <em>120</em>(549), 165-166. (<a href='https://doi.org/10.1080/01621459.2024.2421994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Nikolaos Ignatiadis and Dennis L. Sun},
  doi          = {10.1080/01621459.2024.2421994},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {165-166},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Empirical bayes estimation via data fission},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comment for “Data fission: Splitting a single data point”. <em>JASA</em>, <em>120</em>(549), 164. (<a href='https://doi.org/10.1080/01621459.2025.2467918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Sangwon Hyun},
  doi          = {10.1080/01621459.2025.2467918},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {164},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment for “Data fission: Splitting a single data point”},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of leiner et al. “Data fission: Splitting a single data point”. <em>JASA</em>, <em>120</em>(549), 162-163. (<a href='https://doi.org/10.1080/01621459.2024.2402539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Daniel García Rasines and Alastair Young},
  doi          = {10.1080/01621459.2024.2402539},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {162-163},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of leiner et al. “Data fission: Splitting a single data point”},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assumption-lean data fission with resampled data. <em>JASA</em>, <em>120</em>(549), 161. (<a href='https://doi.org/10.1080/01621459.2024.2412172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Kevin Fry and Snigdha Panigrahi and Jonathan Taylor},
  doi          = {10.1080/01621459.2024.2412172},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {161},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Assumption-lean data fission with resampled data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data fission and sampling designs: A discussion. <em>JASA</em>, <em>120</em>(549), 159-160. (<a href='https://doi.org/10.1080/01621459.2024.2416530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Kwun Chuen Gary Chan and Ross L. Prentice and Zhenman Yuan},
  doi          = {10.1080/01621459.2024.2416530},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {159-160},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Data fission and sampling designs: A discussion},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A discussion on: “Data fission: Splitting a single data point” by leiner, j., duan, b., wasserman, l. and ramdas, a.. <em>JASA</em>, <em>120</em>(549), 158. (<a href='https://doi.org/10.1080/01621459.2024.2412183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Marta Catalano and Augusto Fasano and Matteo Giordano and Giovanni Rebaudo},
  doi          = {10.1080/01621459.2024.2412183},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {158},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A discussion on: “Data fission: Splitting a single data point” by leiner, j., duan, b., wasserman, l. and ramdas, a.},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of “Data fission: Splitting a single data point”. <em>JASA</em>, <em>120</em>(549), 151-157. (<a href='https://doi.org/10.1080/01621459.2024.2421998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Anna Neufeld and Ameer Dharamshi and Lucy L. Gao and Daniela Witten and Jacob Bien},
  doi          = {10.1080/01621459.2024.2421998},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {151-157},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Data fission: Splitting a single data point”},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of “Data fission: Splitting a single data point” – Some asymptotic results for data fission. <em>JASA</em>, <em>120</em>(549), 147-150. (<a href='https://doi.org/10.1080/01621459.2024.2441416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Lihua Lei},
  doi          = {10.1080/01621459.2024.2441416},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {147-150},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Data fission: Splitting a single data point” – Some asymptotic results for data fission},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data fission: Splitting a single data point. <em>JASA</em>, <em>120</em>(549), 135-146. (<a href='https://doi.org/10.1080/01621459.2023.2270748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suppose we observe a random vector X from some distribution in a known family with unknown parameters. We ask the following question: when is it possible to split X into two pieces f ( X ) and g ( X ) such that neither part is sufficient to reconstruct X by itself, but both together can recover X fully, and their joint distribution is tractable? One common solution to this problem when multiple samples of X are observed is data splitting, but Rasines and Young offers an alternative approach that uses additive Gaussian noise—this enables post-selection inference in finite samples for Gaussian distributed data and asymptotically when errors are non-Gaussian. In this article, we offer a more general methodology for achieving such a split in finite samples by borrowing ideas from Bayesian inference to yield a (frequentist) solution that can be viewed as a continuous analog of data splitting. We call our method data fission, as an alternative to data splitting, data carving and p -value masking. We exemplify the method on several prototypical applications, such as post-selection inference for trend filtering and other regression problems, and effect size estimation after interactive multiple testing. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {James Leiner and Boyan Duan and Larry Wasserman and Aaditya Ramdas},
  doi          = {10.1080/01621459.2023.2270748},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {135-146},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Data fission: Splitting a single data point},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDNAPlus: A unifying modeling framework for DNA-based biodiversity monitoring. <em>JASA</em>, <em>120</em>(549), 120-134. (<a href='https://doi.org/10.1080/01621459.2024.2412362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA-based biodiversity surveys, which involve collecting physical samples from survey sites and assaying them in the laboratory to detect species via their diagnostic DNA sequences, are increasingly being adopted for biodiversity monitoring and decision-making. The most commonly employed method, metabarcoding, combines PCR with high-throughput DNA sequencing to amplify and read “DNA barcode” sequences, generating count data indicating the number of times each DNA barcode was read. However, DNA-based data are noisy and error-prone, with several sources of variation, and cannot alone estimate the species-specific amount of DNA present at a surveyed site ( DNA biomass ). In this article, we present a unifying modeling framework for DNA-based survey data that allows estimation of changes in DNA biomass within species, across sites and their links to environmental covariates, while for the first time simultaneously accounting for key sources of variation, error and noise in the data-generating process, and for between-species and between-sites correlation. Bayesian inference is performed using MCMC with Laplace approximations. We describe a re-parameterization scheme for crossed-effects models designed to improve mixing, and an adaptive approach for updating latent variables, which reduces computation time. Theoretical and simulation results are used to guide study design, including the level of replication at different survey stages and the use of quality control methods. Finally, we demonstrate our new framework on a dataset of Malaise-trap samples, quantifying the effects of elevation and distance-to-road on each species, and produce maps identifying areas of high biodiversity and species DNA biomass. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Alex Diana and Eleni Matechou and Jim Griffin and Douglas W. Yu and Mingjie Luo and Marie Tosa and Alex Bush and Richard A. Griffiths},
  doi          = {10.1080/01621459.2024.2412362},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {120-134},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {EDNAPlus: A unifying modeling framework for DNA-based biodiversity monitoring},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Additive covariance matrix models: Modeling regional electricity net-demand in great britain. <em>JASA</em>, <em>120</em>(549), 107-119. (<a href='https://doi.org/10.1080/01621459.2024.2412361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasts of regional electricity net-demand, consumption minus embedded generation, are an essential input for reliable and economic power system operation, and energy trading. While such forecasts are typically performed region by region, operations such as managing power flows require spatially coherent joint forecasts, which account for cross-regional dependencies. Here, we forecast the joint distribution of net-demand across the 14 regions constituting Great Britain’s electricity network. Joint modeling is complicated by the fact that the net-demand variability within each region, and the dependencies between regions, vary with temporal, socio-economic and weather-related factors. We accommodate for these characteristics by proposing a multivariate Gaussian model based on a modified Cholesky parameterization, which allows us to model each unconstrained parameter via an additive model. Given that the number of model parameters and covariates is large, we adopt a semi-automated approach to model selection, based on gradient boosting. In addition to comparing the forecasting performance of several versions of the proposed model with that of two non-Gaussian copula-based models, we visually explore the model output to interpret how the covariates affect net-demand variability and dependencies. The code for reproducing the results in this article is available at https://doi.org/10.5281/zenodo.7315105. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {V. Gioia and M. Fasiolo and J. Browell and R. Bellio},
  doi          = {10.1080/01621459.2024.2412361},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {107-119},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Additive covariance matrix models: Modeling regional electricity net-demand in great britain},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantification of vaccine waning as a challenge effect. <em>JASA</em>, <em>120</em>(549), 96-106. (<a href='https://doi.org/10.1080/01621459.2024.2408776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowing whether vaccine protection wanes over time is important for health policy and drug development. However, quantifying waning effects is difficult. A simple contrast of vaccine efficacy at two different times compares different populations of individuals: those who were uninfected at the first time versus those who remain uninfected until the second time. Thus, the contrast of vaccine efficacy at early and late times can not be interpreted as a causal effect. We propose to quantify vaccine waning using the challenge effect, which is a contrast of outcomes under controlled exposures to the infectious agent following vaccination. We identify sharp bounds on the challenge effect under nonparametric assumptions that are broadly applicable in vaccine trials using routinely collected data. We demonstrate that the challenge effect can differ substantially from the conventional vaccine efficacy due to depletion of susceptible individuals from the risk set over time. Finally, we apply the methods to derive bounds on the waning of the BNT162b2 COVID-19 vaccine using data from a placebo-controlled randomized trial. Our estimates of the challenge effect suggest waning protection after 2 months beyond administration of the second vaccine dose. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Matias Janvin and Mats J. Stensrud},
  doi          = {10.1080/01621459.2024.2408776},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {96-106},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Quantification of vaccine waning as a challenge effect},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial modeling and future projection of extreme precipitation extents. <em>JASA</em>, <em>120</em>(549), 80-95. (<a href='https://doi.org/10.1080/01621459.2024.2408045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme precipitation events with large spatial extents may have more severe impacts than localized events as they can lead to widespread flooding. It is debated how climate change may affect the spatial extent of precipitation extremes, whose investigation often directly relies on simulations of precipitation from climate models. Here, we use a different strategy to investigate how future changes in spatial extents of precipitation extremes differ across climate zones and seasons in two river basins (Danube and Mississippi). We rely on observed precipitation extremes while exploiting a physics-based average-temperature covariate, enabling us to project future precipitation extents based on projected temperatures. We include the covariate into newly developed time-varying r -Pareto processes using suitably chosen spatial risk functionals r . This model captures temporal non-stationarity in the spatial dependence structure of precipitation extremes by linking it to the temperature covariate, derived from reanalysis data (ERA5-Land) for model calibration and from bias-corrected climate simulations (CMIP6) for projections. Our results show an increasing trend in the margins, with both significantly positive or negative trend coefficients depending on season and river (sub-)basin. During major rainy seasons, the significant trends indicate that future spatial extreme events will become relatively more intense and localized in several sub-basins. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Peng Zhong and Manuela Brunner and Thomas Opitz and Raphaël Huser},
  doi          = {10.1080/01621459.2024.2408045},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {80-95},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spatial modeling and future projection of extreme precipitation extents},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using penalized synthetic controls on truncated data: A case study on effect of marijuana legalization on direct payments to physicians by opioid manufacturers. <em>JASA</em>, <em>120</em>(549), 64-79. (<a href='https://doi.org/10.1080/01621459.2024.2406583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amid increasing awareness regarding opioid addiction, medical marijuana has emerged as a substitute to opioids for pain management. Concurrently, opioid manufacturers are putting significant research into making opioids safer yet effective. Interactions between these manufacturers and physicians are critical to advance existing pain management protocols. Direct payments from opioid manufacturers to physicians are established practices that often moderates such interactions. We study the effects of passage of a medical marijuana law (MML) on these direct payments to physicians. To draw causal conclusions, we develop a novel penalized synthetic control (SC) method that accommodates zero-payment related latent structures inherent in these payments. Under a truncated flexible additive mixture model, we show that the SC method has uncontrolled maximal risk without the penalty; by contrast, the proposed penalized method provides efficient estimates. Our analysis finds a significant decrease in direct payments from opioid manufacturers to pain medicine physicians as an effect of MML passage. We provide evidence that this decrease is due to medical marijuana becoming available as a substitute. Finally, our heterogeneity analyses indicate that the decrease in direct payments is comparatively higher for physicians practicing in localities with higher white populations, lower affluence, and a larger proportion of working-age residents. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Bikram Karmakar and Gourab Mukherjee and Wreetabrata Kar},
  doi          = {10.1080/01621459.2024.2406583},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {64-79},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Using penalized synthetic controls on truncated data: A case study on effect of marijuana legalization on direct payments to physicians by opioid manufacturers},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bisection grover’s search algorithm and its application in analyzing CITE-seq data. <em>JASA</em>, <em>120</em>(549), 52-63. (<a href='https://doi.org/10.1080/01621459.2024.2404259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of quantum computers, researchers have shown quantum advantages in physics-oriented problems. Quantum algorithms tackling computational biology problems are still lacking. In this article, we demonstrate the quantum advantage in analyzing CITE-seq data. CITE-seq, a single-cell technology, enables researchers to simultaneously measure expressions of RNA and surface protein detected by antibody-derived tags (ADTs) in the same cells. CITE-seq data hold tremendous potential for identifying ADTs associated with targeted genes and identifying cell types effectively. However, both tasks are challenging since the best subset of ADTs needs to be identified from enormous candidate subsets. To surmount the challenge, we develop a quantum algorithm named bisection Grover’s search (BGS) for the best subset selection of ADT markers in CITE-seq data. BGS takes advantage of quantum parallelism by integrating binary search and Grover’s algorithm to enable fast computation. Theoretical results are provided to show the privilege of BGS in the estimation error and computational complexity. The empirical performance of the BGS algorithm is demonstrated on both the IBM quantum computer and simulator. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ping Ma and Yongkai Chen and Haoran Lu and Wenxuan Zhong},
  doi          = {10.1080/01621459.2024.2404259},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {52-63},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bisection grover’s search algorithm and its application in analyzing CITE-seq data},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating treatment prioritization rules via rank-weighted average treatment effects. <em>JASA</em>, <em>120</em>(549), 38-51. (<a href='https://doi.org/10.1080/01621459.2024.2393466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are a number of available methods for selecting whom to prioritize for treatment, including ones based on treatment effect estimation, risk scoring, and hand-crafted rules. We propose rank-weighted average treatment effect (RATE) metrics as a simple and general family of metrics for comparing and testing the quality of treatment prioritization rules. RATE metrics are agnostic as to how the prioritization rules were derived, and only assess how well they identify individuals that benefit the most from treatment. We define a family of RATE estimators and prove a central limit theorem that enables asymptotically exact inference in a wide variety of randomized and observational study settings. RATE metrics subsume a number of existing metrics, including the Qini coefficient, and our analysis directly yields inference methods for these metrics. We showcase RATE in the context of a number of applications, including optimal targeting of aspirin to stroke patients. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Steve Yadlowsky and Scott Fleming and Nigam Shah and Emma Brunskill and Stefan Wager},
  doi          = {10.1080/01621459.2024.2393466},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {38-51},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Evaluating treatment prioritization rules via rank-weighted average treatment effects},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection. rejoinder. <em>JASA</em>, <em>120</em>(549), 34-37. (<a href='https://doi.org/10.1080/01621459.2024.2443563'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Kes Ward and Giuseppe Dilillo and Idris Eckley and Paul Fearnhead},
  doi          = {10.1080/01621459.2024.2443563},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {34-37},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection. rejoinder},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Saha and ramdas’ discussion of “Poisson-FOCuS” by ward, dilillo, eckley & fearnhead. <em>JASA</em>, <em>120</em>(549), 31-33. (<a href='https://doi.org/10.1080/01621459.2024.2402957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Aytijhya Saha and Aaditya Ramdas},
  doi          = {10.1080/01621459.2024.2402957},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {31-33},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Saha and ramdas’ discussion of “Poisson-FOCuS” by ward, dilillo, eckley & fearnhead},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion on “Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection” by ward, dilillo, eckley and fearnhead. <em>JASA</em>, <em>120</em>(549), 26-30. (<a href='https://doi.org/10.1080/01621459.2024.2402961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Yang Chen},
  doi          = {10.1080/01621459.2024.2402961},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {26-30},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion on “Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection” by ward, dilillo, eckley and fearnhead},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A space astrophysics perspective on poisson-FOCuS. <em>JASA</em>, <em>120</em>(549), 22-25. (<a href='https://doi.org/10.1080/01621459.2024.2396960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Carlo Graziani},
  doi          = {10.1080/01621459.2024.2396960},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {22-25},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A space astrophysics perspective on poisson-FOCuS},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of “Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection”. <em>JASA</em>, <em>120</em>(549), 20-21. (<a href='https://doi.org/10.1080/01621459.2024.2402536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Mikael Kuusela},
  doi          = {10.1080/01621459.2024.2402536},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {20-21},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection”},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection. <em>JASA</em>, <em>120</em>(549), 7-19. (<a href='https://doi.org/10.1080/01621459.2023.2235059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gamma ray bursts are flashes of light from distant, new-born black holes. CubeSats that monitor high-energy photons across different energy bands are used to detect these bursts. There is a need for computationally efficient algorithms, able to run using the limited computational resource onboard a CubeSats, that can detect when gamma ray bursts occur. Current algorithms are based on monitoring photon counts across a grid of different sizes of time window. We propose a new method, which extends the recently proposed FOCuS approach for online change detection to Poisson data. Our method is mathematically equivalent to searching over all possible window sizes, but at half the computational cost of the current grid-based methods. We demonstrate the additional power of our approach using simulations and data drawn from the Fermi gamma ray burst monitor archive. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kes Ward and Giuseppe Dilillo and Idris Eckley and Paul Fearnhead},
  doi          = {10.1080/01621459.2023.2235059},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {7-19},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Our mission in action: Past, present, and future. <em>JASA</em>, <em>120</em>(549), 1-6. (<a href='https://doi.org/10.1080/01621459.2025.2450191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by our mission, we have advanced science for the public good. Our history is rich in theory, methodology, and applications that are instrumental in solving real-world problems. Our reach is boundless, and our impact is undeniable in diverse areas, including engineering, economics, the environment, genetics, public health, and health policy. As the data landscape continues to evolve, our leadership, expertise and knowledge will be essential to meet the global challenges of the future. We must boldly embody our mission of promoting the practice and profession of statistics. In this talk, I will highlight our mission in action as we strive to be a community that uses statistical thinking to drive discovery and inform decisions. This article summarizes the 2023 presidential speech given at the JSM in Toronto.},
  archive      = {J_JASA},
  author       = {Dionne L. Price},
  doi          = {10.1080/01621459.2025.2450191},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {549},
  pages        = {1-6},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Our mission in action: Past, present, and future},
  volume       = {120},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
