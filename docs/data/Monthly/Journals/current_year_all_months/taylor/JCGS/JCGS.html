<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JCGS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jcgs">JCGS - 92</h2>
<ul>
<li><details>
<summary>
(2025). Quantile regression and homogeneity identification of a semiparametric panel data model. <em>JCGS</em>, <em>34</em>(3), 1169-1187. (<a href='https://doi.org/10.1080/10618600.2024.2433672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we delve into the quantile regression and homogeneity detection of a varying index coefficient panel data model, which incorporates fixed individual effects and exhibits nonlinear time trends. Using spline approximation, we obtain estimators for the trend functions, link functions, and index parameters, and subsequently establish the corresponding convergence rates and asymptotic normality. Observing that subjects within a group may share identical trend functions, we are motivated to further explore potential homogeneity in these trends. To this end, we propose a homogeneity identification algorithm based on binary segmentation. For the determination of the thresholding parameter in homogeneity identification, we propose a generalized Bayesian information criterion. Furthermore, we introduce a penalized method to discern the constant and linear structures within the nonparametric functions of our model. By leveraging grouped observations, we achieve more efficient estimation and improve the asymptotic properties of the estimators. To demonstrate the finite sample performance of our proposed approach, we conduct simulation studies and apply our methodology to a real-world dataset comprising Air Pollution Data and Integrated Surface Data (APD&ISD). Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Rui Li and Tao Li and Huacheng Su and Jinhong You},
  doi          = {10.1080/10618600.2024.2433672},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1169-1187},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Quantile regression and homogeneity identification of a semiparametric panel data model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural bayes estimators for irregular spatial data using graph neural networks. <em>JCGS</em>, <em>34</em>(3), 1153-1168. (<a href='https://doi.org/10.1080/10618600.2024.2433671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Bayes estimators are neural networks that approximate Bayes estimators in a fast and likelihood-free manner. Although they are appealing to use with spatial models, where estimation is often a computational bottleneck, neural Bayes estimators in spatial applications have, to date, been restricted to data collected over a regular grid. These estimators are also currently dependent on a prescribed set of spatial locations, which means that the neural network needs to be retrained for new datasets; this renders them impractical in many applications and impedes their widespread adoption. In this work, we employ graph neural networks (GNNs) to tackle the important problem of parameter point estimation from data collected over arbitrary spatial locations. In addition to extending neural Bayes estimation to irregular spatial data, the use of GNNs leads to substantial computational benefits, since the estimator can be used with any configuration or number of locations and independent replicates, thus, amortizing the cost of training for a given spatial model. We also facilitate fast uncertainty quantification by training an accompanying neural Bayes estimator that approximates a set of marginal posterior quantiles. We illustrate our methodology on Gaussian and max-stable processes. Finally, we showcase our methodology on a dataset of global sea-surface temperature, where we estimate the parameters of a Gaussian process model in 2161 spatial regions, each containing thousands of irregularly-spaced data points, in just a few minutes with a single graphics processing unit. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Matthew Sainsbury-Dale and Andrew Zammit-Mangion and Jordan Richards and Raphaël Huser},
  doi          = {10.1080/10618600.2024.2433671},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1153-1168},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Neural bayes estimators for irregular spatial data using graph neural networks},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visualization and assessment of copula symmetry. <em>JCGS</em>, <em>34</em>(3), 1140-1152. (<a href='https://doi.org/10.1080/10618600.2024.2432978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization and assessment of copula structures are crucial for accurately understanding and modeling the dependencies in multivariate data analysis. In this article, we introduce an innovative method that employs functional boxplots and rank-based testing procedures to evaluate copula symmetries. This approach is specifically designed to assess key characteristics such as reflection symmetry, radial symmetry, and joint symmetry. We first construct test functions for each specific property and then investigate the asymptotic properties of their empirical estimators. We demonstrate that the functional boxplot of these sample test functions serves as an informative visualization tool of a given copula structure, effectively measuring the departure from zero of the test function. Furthermore, we introduce a nonparametric testing procedure to assess the significance of deviations from symmetry, ensuring the accuracy and reliability of our visualization method. Through extensive simulation studies involving various copula models, we demonstrate the effectiveness of our testing approach. Finally, we apply our visualization and testing techniques to three real-world datasets: a nutritional habits survey with five variables, stock price data for the five top companies in the NASDAQ-100 stock index, and two major stock indices, the US S&P500 and German DAX. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Cristian F. Jiménez-Varón and Hao Lee and Marc G. Genton and Ying Sun},
  doi          = {10.1080/10618600.2024.2432978},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1140-1152},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Visualization and assessment of copula symmetry},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable estimation and two-sample testing for large networks via subsampling. <em>JCGS</em>, <em>34</em>(3), 1127-1139. (<a href='https://doi.org/10.1080/10618600.2024.2432974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large networks are routinely used to represent data from many scientific fields. Statistical analysis of these networks, such as estimation and hypothesis testing, has received considerable attention. However, most of the methods proposed in the literature are computationally expensive for large networks. In this article, we propose a subsampling-based method to reduce the computational cost of estimation and two-sample hypothesis testing. The idea is to divide the network into smaller subgraphs with an overlap region, then draw inference based on each subgraph, and finally combine the results together. We first develop the subsampling method for random dot product graph models, and establish theoretical consistency of the proposed method. Then we extend the subsampling method to a more general setup and establish similar theoretical properties. We demonstrate the performance of our methods through simulation experiments and real data analysis. Supplemental materials for the article are available online. The code is available in the following GitHub repository: https://github.com/kchak19/SubsampleTestingNetwork . Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Kaustav Chakraborty and Srijan Sengupta and Yuguo Chen},
  doi          = {10.1080/10618600.2024.2432974},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1127-1139},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Scalable estimation and two-sample testing for large networks via subsampling},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlexBART: Flexible bayesian regression trees with categorical predictors. <em>JCGS</em>, <em>34</em>(3), 1117-1126. (<a href='https://doi.org/10.1080/10618600.2024.2431072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most implementations of Bayesian additive regression trees (BART) one-hot encode categorical predictors, replacing each one with several binary indicators, one for every level or category. Regression trees built with these indicators partition the discrete set of categorical levels by repeatedly removing one level at a time. Unfortunately, the vast majority of partitions cannot be built with this strategy, severely limiting BART’s ability to partially pool data across groups of levels. Motivated by analyses of baseball data and neighborhood-level crime dynamics, we overcame this limitation by re-implementing BART with regression trees that can assign multiple levels to both branches of a decision tree node. To model spatial data aggregated into small regions, we further proposed a new decision rule prior that creates spatially contiguous regions by deleting a random edge from a random spanning tree of a suitably defined network. Our re-implementation, which is available in the flexBART package, often yields improved out-of-sample predictive performance and scales better to larger datasets than existing implementations of BART. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Sameer K. Deshpande},
  doi          = {10.1080/10618600.2024.2431072},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1117-1126},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {FlexBART: Flexible bayesian regression trees with categorical predictors},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monotone cubic B-splines with a neural-network generator. <em>JCGS</em>, <em>34</em>(3), 1102-1116. (<a href='https://doi.org/10.1080/10618600.2024.2431070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for fitting monotone curves using cubic B-splines with a monotonicity constraint on the coefficients. We explore different ways of enforcing this constraint and analyze their theoretical and empirical properties. We propose two algorithms for solving the spline fitting problem: one that uses standard optimization techniques and one that trains a Multi-Layer Perceptrons (MLP) generator to approximate the solutions under various settings and perturbations. The generator approach can speed up the fitting process when we need to solve the problem repeatedly, such as when constructing confidence bands using bootstrap. We evaluate our method against several existing methods, some of which do not use the monotonicity constraint, on some monotone curves with varying noise levels. We demonstrate that our method outperforms the other methods, especially in high-noise scenarios. We also apply our method to analyze the polarization-hole phenomenon during star formation in astrophysics. The source code is accessible at https://github.com/szcf-weiya/MonotoneSplines.jl . Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Lijun Wang and Xiaodan Fan and Huabai Li and Jun S. Liu},
  doi          = {10.1080/10618600.2024.2431070},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1102-1116},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Monotone cubic B-splines with a neural-network generator},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized learning of quantile regression: A smoothing approach. <em>JCGS</em>, <em>34</em>(3), 1091-1101. (<a href='https://doi.org/10.1080/10618600.2024.2431060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed estimation has attracted a significant amount of attention recently due to its advantages in computational efficiency and data privacy preservation. In this article, we focus on quantile regression over a decentralized network. Without a coordinating central node, a decentralized network improves system stability and increases efficiency by communicating with fewer nodes per round. However, existing related works on decentralized quantile regression have slow (sub-linear) convergence speed. We propose a novel method for decentralized quantile regression which is built upon the smoothed quantile loss. However, we argue that the smoothed loss proposed in the existing literature using a single smoothing bandwidth parameter fails to achieve fast convergence and statistical efficiency simultaneously in the decentralized setting. We propose a novel quadratic approximation of the quantile loss using a big bandwidth for the Hessian and a small bandwidth for the gradient. Our method enjoys a linear convergence rate and has optimal statistical efficiency. Numerical experiments and real data analysis are conducted to demonstrate the effectiveness of our method. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jianwei Shi and Yue Wang and Zhongyi Zhu and Heng Lian},
  doi          = {10.1080/10618600.2024.2431060},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1091-1101},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Decentralized learning of quantile regression: A smoothing approach},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local clustering for functional data. <em>JCGS</em>, <em>34</em>(3), 1075-1090. (<a href='https://doi.org/10.1080/10618600.2024.2431057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In functional data analysis, unsupervised clustering has been extensively conducted and has important implications. In most of the existing functional clustering analyses, it is assumed that there is a single clustering structure across the whole domain of measurement (say, time interval). In some data analyses, for example, the analysis of normalized COVID-19 daily confirmed cases for the U.S. states, it is observed that functions can have different clustering patterns in different time subintervals. To tackle the lack of flexibility of the existing functional clustering techniques, we develop a local clustering approach, which can fully data-dependently identify subintervals, where, in different subintervals, functions have different clustering structures. This approach is built on the basis expansion technique and has a novel penalization form. It simultaneously achieves subinterval identification, clustering, and estimation. Its estimation and clustering consistency properties are rigorously established. In simulation, it significantly outperforms multiple competitors. In the analysis of the COVID-19 case trajectory data, it identifies sensible subintervals and clustering structures. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yuanxing Chen and Qingzhao Zhang and Shuangge Ma},
  doi          = {10.1080/10618600.2024.2431057},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1075-1090},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Local clustering for functional data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). No more, no less than sum of its parts: Groups, monoids, and the algebra of graphics, statistics, and interaction. <em>JCGS</em>, <em>34</em>(3), 1063-1074. (<a href='https://doi.org/10.1080/10618600.2024.2429708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive data visualization has become a staple of modern data presentation. Yet, despite its growing popularity, we still lack a general framework for turning raw data into summary statistics that can be displayed by interactive graphics. This gap may stem from a subtle yet profound issue: while we would often like to treat graphics, statistics, and interaction in our plots as independent, they are in fact deeply connected. This article examines this interdependence in light of two fundamental concepts from category theory: groups and monoids. We argue that the knowledge of these algebraic structures can help us design sensible interactive graphics. Specifically, if we want our graphics to support interactive features which split our data into parts and then combine these parts back together (such as linked selection), then the statistics underlying our plots need to possess certain properties. By grounding our thinking in these algebraic concepts, we may be able to build more flexible and expressive interactive data visualization systems. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Adam Bartonicek and Simon Urbanek and Paul Murrell},
  doi          = {10.1080/10618600.2024.2429708},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1063-1074},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {No more, no less than sum of its parts: Groups, monoids, and the algebra of graphics, statistics, and interaction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional projection K-means. <em>JCGS</em>, <em>34</em>(3), 1051-1062. (<a href='https://doi.org/10.1080/10618600.2024.2429706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new technique for simultaneous clustering and dimensionality reduction of functional data is proposed. The observations are projected into a low-dimensional subspace and clustered by means of a functional K -means. The subspace and the partition are estimated simultaneously by minimizing the within deviance in the reduced space. This allows us to find new dimensions with a very low within deviance, which should correspond to a high level of discriminant power. However, in some cases, the total deviance explained by the new dimensions is so low as to make the subspace, and therefore the partition identified in it, insignificant. To overcome this drawback, we add to the loss a penalty equal to the negative total deviance in the reduced space. In this way, subspaces with a low deviance are avoided. We show how several existing methods are particular cases of our proposal simply by varying the weight of the penalty. The estimation is improved by adding a regularization term to the loss in order to take into account the functional nature of the data by smoothing the centroids. In contrast to existing literature, which largely considers the smoothing as a pre-processing step, in our proposal regularization is integrated with the identification of both subspace and cluster partition. An alternating least squares algorithm is introduced to compute model parameter estimates. The effectiveness of our proposal is demonstrated through its application to both real and simulated data. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Roberto Rocci and Stefano A. Gattone},
  doi          = {10.1080/10618600.2024.2429706},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1051-1062},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Functional projection K-means},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse model-based clustering of three-way data via lasso-type penalties. <em>JCGS</em>, <em>34</em>(3), 1030-1050. (<a href='https://doi.org/10.1080/10618600.2024.2429705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixtures of matrix Gaussian distributions provide a probabilistic framework for clustering continuous matrix-variate data, which are increasingly common in various fields. Despite their widespread use and successful applications, these models suffer from over-parameterization, making them not suitable for even moderately sized matrix-variate data. To address this issue, we introduce a sparse model-based clustering approach for three-way data. Our approach assumes that the matrix mixture parameters are sparse and have different degrees of sparsity across clusters, enabling the induction of parsimony in a flexible manner. Estimation relies on the maximization of a penalized likelihood, with specifically tailored group and graphical lasso penalties. These penalties facilitate the selection of the most informative features for clustering three-way data where variables are recorded over multiple occasions, as well as allowing the identification of cluster-specific association structures. We conduct extensive testing of the proposed methodology on synthetic data and validate its effectiveness through an application to time-dependent crime patterns across multiple U.S. cities. Supplementary files for this article are available online.},
  archive      = {J_JCGS},
  author       = {Andrea Cappozzo and Alessandro Casa and Michael Fop},
  doi          = {10.1080/10618600.2024.2429705},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1030-1050},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Sparse model-based clustering of three-way data via lasso-type penalties},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A majorization-minimization gauss-newton method for 1-bit matrix completion. <em>JCGS</em>, <em>34</em>(3), 1017-1029. (<a href='https://doi.org/10.1080/10618600.2024.2428610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 1-bit matrix completion, the aim is to estimate an underlying low-rank matrix from a partial set of binary observations. We propose a novel method for 1-bit matrix completion called Majorization-Minimization Gauss-Newton ( MMGN ). Our method is based on the majorization-minimization principle, which converts the original optimization problem into a sequence of standard low-rank matrix completion problems. We solve each of these sub-problems by a factorization approach that explicitly enforces the assumed low-rank structure and then apply a Gauss-Newton method. Using simulations and a real data example, we illustrate that in comparison to existing 1-bit matrix completion methods, MMGN outputs comparable if not more accurate estimates. In addition, it is often significantly faster, and less sensitive to the spikiness of the underlying matrix. In comparison with three standard generic optimization approaches that directly minimize the original objective, MMGN also exhibits a clear computational advantage, especially when the fraction of observed entries is small. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Xiaoqian Liu and Xu Han and Eric C. Chi and Boaz Nadler},
  doi          = {10.1080/10618600.2024.2428610},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1017-1029},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {A majorization-minimization gauss-newton method for 1-bit matrix completion},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional block diagonal covariance structure detection using singular vectors. <em>JCGS</em>, <em>34</em>(3), 1005-1016. (<a href='https://doi.org/10.1080/10618600.2024.2422985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assumption of independent subvectors arises in many aspects of multivariate analysis. In most real-world applications, however, we lack prior knowledge about the number of subvectors and the specific variables within each subvector. Yet, testing all these combinations is not feasible. For example, for a data matrix containing 15 variables, there are already 1 , 382 , 958 , 545 possible combinations. Given that zero correlation is a necessary condition for independence, independent subvectors exhibit a block diagonal covariance matrix. This article focuses on the detection of such block diagonal covariance structures in high-dimensional data and therefore also identifies uncorrelated subvectors. Our approach exploits the fact that the structure of the covariance matrix is mirrored by the structure of its eigenvectors. However, the true block diagonal structure is masked by noise in the sample case. To address this problem, we propose to use sparse approximations of the sample eigenvectors to reveal the sparse structure of the population eigenvectors. Notably, the right singular vectors of a data matrix with an overall mean of zero are identical to the sample eigenvectors of its covariance matrix. Using sparse approximations of these singular vectors instead of the eigenvectors makes the estimation of the covariance matrix obsolete. We demonstrate the performance of our method through simulations and provide real data examples. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jan O. Bauer},
  doi          = {10.1080/10618600.2024.2422985},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {1005-1016},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {High-dimensional block diagonal covariance structure detection using singular vectors},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal subsampling for data streams with measurement constrained categorical responses. <em>JCGS</em>, <em>34</em>(3), 994-1004. (<a href='https://doi.org/10.1080/10618600.2024.2421990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-velocity, large-scale data streams have become pervasive. Frequently, the associated labels for such data prove costly to measure and are not always available upfront. Consequently, the analysis of such data poses a significant challenge. In this article, we develop a method that addresses this challenge by employing an online subsampling procedure and a multinomial logistic model for efficient analysis of high-velocity, large-scale data streams. Our algorithm is designed to sequentially update parameter estimation based on the A-optimality criterion. Moreover, it significantly increases computational efficiency while imposing minimal storage requirements. Theoretical properties are rigorously established to quantify the asymptotic behavior of the estimator. The method’s efficacy is further demonstrated through comprehensive numerical studies on both simulated and real-world datasets. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jun Yu and Zhiqiang Ye and Mingyao Ai and Ping Ma},
  doi          = {10.1080/10618600.2024.2421990},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {994-1004},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Optimal subsampling for data streams with measurement constrained categorical responses},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent markov time-interaction processes. <em>JCGS</em>, <em>34</em>(3), 984-993. (<a href='https://doi.org/10.1080/10618600.2024.2421984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present parametric and semiparametric latent Markov time-interaction processes, that are point processes where the occurrence of an event can increase or reduce the probability of future events. We first present time-interaction processes with parametric and nonparametric baselines, then we let model parameters be modulated by a discrete state continuous time latent Markov process. Posterior inference is based on a novel and efficient data augmentation approach in the Markov chain Monte Carlo framework. We illustrate with a simulation study; and an original application to terrorist attacks in Europe in the period 2001–2017, where we find two distinct latent clusters for the hazard of occurrence of terrorist events, negative association with GDP growth, and self-exciting phenomena. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Rosario Barone and Alessio Farcomeni and Maura Mezzetti},
  doi          = {10.1080/10618600.2024.2421984},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {984-993},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Latent markov time-interaction processes},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label random subspace ensemble classification. <em>JCGS</em>, <em>34</em>(3), 971-983. (<a href='https://doi.org/10.1080/10618600.2024.2421248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we develop a new ensemble learning framework, multi-label Random Subspace Ensemble (mRaSE), for multi-label classification. Given a base classifier (e.g., multinomial logistic regression, classification tree, K -nearest neighbors), mRaSE works by first randomly sampling a collection of subspaces, then choosing the best ones that achieve the minimum cross-validation errors and, finally, aggregating the chosen weak learners. In addition to its superior prediction performance, mRaSE also provides a model-free feature ranking depending on the given base classifier. An iterative version of mRaSE is also developed to further improve the performance. A model-free extension is pursued on the iterative version, leading to the so-called Super mRaSE , which accepts a collection of base classifiers as input to the algorithm. We show the proposed algorithms compared favorably with the state-of-the-art classification algorithm including random forest and deep neural network, via extensive simulation studies and two real data applications. The new algorithms are implemented in an updated version of the R package RaSEn .},
  archive      = {J_JCGS},
  author       = {Fan Bi and Jianan Zhu and Yang Feng},
  doi          = {10.1080/10618600.2024.2421248},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {971-983},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Multi-label random subspace ensemble classification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task learning for gaussian graphical regressions with high dimensional covariates. <em>JCGS</em>, <em>34</em>(3), 961-970. (<a href='https://doi.org/10.1080/10618600.2024.2421246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical regression is a powerful approach for regressing the precision matrix of a Gaussian graphical model on covariates, which permits the response variables and covariates to outnumber the sample size. However, traditional approaches of fitting the model via separate node-wise lasso regressions overlook the network-induced structure among these regressions, leading to high error rates, particularly when the number of nodes is large. To address this issue, we propose a multi-task learning estimator for fitting Gaussian graphical regression models, which incorporates a cross-task group sparsity penalty and a within-task element-wise sparsity penalty to govern the sparsity of active covariates and their effects on the graph, respectively. We also develop an efficient augmented Lagrangian algorithm for computation, which solves subproblems with a semi-smooth Newton method. We further prove that our multi-task learning estimator has considerably lower error rates than the separate node-wise regression estimates, as the cross-task penalty enables borrowing information across tasks. We examine the utility of our method through simulations and an application to a gene co-expression network study with brain cancer patients. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jingfei Zhang and Yi Li},
  doi          = {10.1080/10618600.2024.2421246},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {961-970},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Multi-task learning for gaussian graphical regressions with high dimensional covariates},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qini curves for multi-armed treatment rules. <em>JCGS</em>, <em>34</em>(3), 948-960. (<a href='https://doi.org/10.1080/10618600.2024.2418820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Qini curves have emerged as an attractive and popular approach for evaluating the benefit of data-driven targeting rules for treatment allocation. We propose a generalization of the Qini curve to multiple costly treatment arms that quantifies the value of optimally selecting among both units and treatment arms at different budget levels. We develop an efficient algorithm for computing these curves and propose bootstrap-based confidence intervals that are exact in large samples for any point on the curve. These confidence intervals can be used to conduct hypothesis tests comparing the value of treatment targeting using an optimal combination of arms with using just a subset of arms, or with a non-targeting assignment rule ignoring covariates, at different budget levels. We demonstrate the statistical performance in a simulation experiment and an application to treatment targeting for election turnout.},
  archive      = {J_JCGS},
  author       = {Erik Sverdrup and Han Wu and Susan Athey and Stefan Wager},
  doi          = {10.1080/10618600.2024.2418820},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {948-960},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Qini curves for multi-armed treatment rules},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sampling random graphs with specified degree sequences. <em>JCGS</em>, <em>34</em>(3), 934-947. (<a href='https://doi.org/10.1080/10618600.2024.2418817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The configuration model is a standard tool for uniformly generating random graphs with a specified degree sequence, and is often used as a null model to evaluate how much of an observed network’s structure can be explained by its degree structure alone. A Markov chain Monte Carlo (MCMC) algorithm, based on a degree-preserving double-edge swap, provides an asymptotic solution to sample from the configuration model. However, accurately and efficiently detecting when this Markov chain is sufficiently close to its stationary distribution remains an unsolved problem. Here, we provide a solution to sample from the configuration model using this standard MCMC algorithm. We develop an algorithm, based on the assortativity of the sampled graphs, for estimating the gap between effectively independent MCMC states, and a computationally efficient gap-estimation heuristic derived from analyzing a corpus of 509 empirical networks. We provide a convergence detection method based on the Dickey-Fuller Generalized Least Squares test, which we show is more accurate and efficient than three alternative Markov chain convergence tests. Supplementary materials for the proposed methods can be found here.},
  archive      = {J_JCGS},
  author       = {Upasana Dutta and Bailey K. Fosdick and Aaron Clauset},
  doi          = {10.1080/10618600.2024.2418817},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {934-947},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Sampling random graphs with specified degree sequences},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sampling from the watson distribution in arbitrary dimensions. <em>JCGS</em>, <em>34</em>(3), 923-933. (<a href='https://doi.org/10.1080/10618600.2024.2416521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present two efficient methods for sampling from the Watson distribution in arbitrary dimensions. The first method adapts the rejection sampling algorithm from Kent, Ganeiber, and Mardia , originally designed for Bingham distributions, using angular central Gaussian envelopes. For the Watson distribution, we derive a closed-form expression for the parameters that maximize sampling efficiency, which is further investigated and bounded by asymptotic results. This approach avoids the curse of dimensionality through a smart matrix inversion, enabling fast runtimes even in high dimensions. The second method, based on Saw , employs adaptive rejection sampling from a projected distribution. This algorithm is also effective in all dimensions and offers rapid sampling capabilities. Finally, our simulation study compares the two main methods, revealing that each excels under different conditions: the first method is more efficient for small samples or large dimensions, while the second performs better with larger samples and more concentrated distributions. Both algorithms are available in the R package watson on CRAN. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Lukas Sablica and Kurt Hornik and Josef Leydold},
  doi          = {10.1080/10618600.2024.2416521},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {923-933},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Efficient sampling from the watson distribution in arbitrary dimensions},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distortion corrected kernel density estimator on riemannian manifolds. <em>JCGS</em>, <em>34</em>(3), 906-922. (<a href='https://doi.org/10.1080/10618600.2024.2415543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manifold learning obtains a low-dimensional representation of an underlying Riemannian manifold supporting high-dimensional data. Kernel density estimates of the low-dimensional embedding with a fixed bandwidth fail to account for the way manifold learning algorithms distort the geometry of the Riemannian manifold. We propose a novel distortion-corrected kernel density estimator (DC-KDE) for any manifold learning embedding, with a bandwidth that depends on the estimated Riemannian metric at each data point. Exploiting the geometric information of the manifold leads to more accurate density estimation, which subsequently could be used for anomaly detection. To compare our proposed estimator with a fixed-bandwidth kernel density estimator, we run two simulations including one with data lying in a 100 dimensional ambient space. We demonstrate that the proposed DC-KDE improves the density estimates as long as the manifold learning embedding is of sufficient quality, and has higher rank correlations with the true manifold density. Further simulation results are provided via a supplementary R shiny app. The proposed method is applied to density estimation in statistical manifolds of electricity usage with the Irish smart meter data.},
  archive      = {J_JCGS},
  author       = {Fan Cheng and Rob J. Hyndman and Anastasios Panagiotelis},
  doi          = {10.1080/10618600.2024.2415543},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {906-922},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Distortion corrected kernel density estimator on riemannian manifolds},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample efficient nonparametric regression via low-rank regularization. <em>JCGS</em>, <em>34</em>(3), 896-905. (<a href='https://doi.org/10.1080/10618600.2024.2414891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric regression suffers from curse of dimensionality, requiring a relatively large sample size for accurate estimation beyond the univariate case. In this article, we consider a simple method of dimension reduction in nonparametric regression via series estimation, based on the concept of low-rankness which was previously studied in parametric multivariate reduced-rank regression and matrix regression. For d > 2 , the low-rank assumption is realized via tensor regression. We establish a faster convergence rate of the estimator in the (approximate) low-rank case. Limitations of the model are also discussed. Through simulation studies and real data analysis, we compare the estimation accuracy of the proposed method with that of existing approaches. The results demonstrate that the proposed method yields estimates with lower RMSE compared to existing methods. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jiakun Jiang and Jiahao Peng and Heng Lian},
  doi          = {10.1080/10618600.2024.2414891},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {896-905},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Sample efficient nonparametric regression via low-rank regularization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable clustering: Large scale unsupervised learning of gaussian mixture models with outliers. <em>JCGS</em>, <em>34</em>(3), 884-895. (<a href='https://doi.org/10.1080/10618600.2024.2414889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a widely used technique with a long and rich history in a variety of areas. However, most existing algorithms do not scale well to large datasets, or are missing theoretical guarantees of convergence. This article introduces a provably robust clustering algorithm based on loss minimization that performs well on Gaussian mixture models with outliers. It provides theoretical guarantees that the algorithm obtains high accuracy with high probability under certain assumptions. Moreover, it can also be used as an initialization strategy for k -means clustering. Experiments on real-world large-scale datasets demonstrate the effectiveness of the algorithm when clustering a large number of clusters, and a k -means algorithm initialized by the algorithm outperforms many of the classic clustering methods in both speed and accuracy, while scaling well to large datasets such as ImageNet. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yijia Zhou and Kyle A. Gallivan and Adrian Barbu},
  doi          = {10.1080/10618600.2024.2414889},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {884-895},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Scalable clustering: Large scale unsupervised learning of gaussian mixture models with outliers},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous functional regression for subgroup analysis. <em>JCGS</em>, <em>34</em>(3), 872-883. (<a href='https://doi.org/10.1080/10618600.2024.2414113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With ever increasing number of features of modern datasets, data heterogeneity is gradually becoming the norm rather than the exception. Whereas classical regressions usually assume all the samples follow a common model, it becomes imperative to identify the heterogeneous relationship in different subsamples. In this article, we propose a new approach to model heterogeneous functional regression relations. We target at the association between a response and a predictor, whose relationship can vary across underlying subgroups and is modeled as an unknown functional of an auxiliary predictor. We introduce a procedure which performs simultaneous parameter estimation and subgroup identification through a fusion type group-wise penalization. We establish the statistical guarantees in terms of non-asymptotic convergence of the parameter estimation. We also establish the oracle property and asymptotic normality of the estimators. We carry out intensive simulations, and illustrate with a new dataset from an Alzheimer’s disease study. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yeqing Zhou and Fei Jiang},
  doi          = {10.1080/10618600.2024.2414113},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {872-883},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Heterogeneous functional regression for subgroup analysis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AddiVortes: (Bayesian) additive voronoi tessellations. <em>JCGS</em>, <em>34</em>(3), 859-871. (<a href='https://doi.org/10.1080/10618600.2024.2414104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Additive Voronoi Tessellations (AddiVortes) model is a multivariate regression model that uses Voronoi tessellations to partition the covariate space in an additive ensemble model. Unlike other partition methods, such as decision trees, this has the benefit of allowing the boundaries of the partitions to be non-orthogonal and nonparallel to the covariate axes. The AddiVortes model uses a similar sum-of-tessellations approach and a Bayesian backfitting MCMC algorithm to the BART model. We use regularization priors to limit the strength of individual tessellations and accepts new models based on a likelihood. The performance of the AddiVortes model is illustrated through testing on several datasets and comparing the performance to other models along with a simulation study to verify some of the properties of the model. In many cases, the AddiVortes model outperforms random forests, BART and other leading black-box regression models when compared using a range of metrics. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Adam J. Stone and John Paul Gosling},
  doi          = {10.1080/10618600.2024.2414104},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {859-871},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {AddiVortes: (Bayesian) additive voronoi tessellations},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentially private methods for compositional data. <em>JCGS</em>, <em>34</em>(3), 848-858. (<a href='https://doi.org/10.1080/10618600.2024.2412174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidential data, such as electronic health records, activity data from wearable devices, and geolocation data, are becoming increasingly prevalent. Differential privacy provides a framework to conduct statistical analyses while mitigating the risk of leaking private information. Compositional data, which consist of vectors with positive components that add up to a constant, have received little attention in the differential privacy literature. This article proposes differentially private approaches for analyzing compositional data based on the Dirichlet distribution. We explore several methods, including Bayesian and bootstrap procedures. For the Bayesian methods, we consider posterior inference techniques based on Markov chain Monte Carlo, Approximate Bayesian Computation, and asymptotic approximations. We conduct an extensive simulation study to compare these approaches and make evidence-based recommendations. Finally, we apply the methodology to a dataset from the American Time Use Survey.},
  archive      = {J_JCGS},
  author       = {Qi Guo and Andrés F. Barrientos and Víctor Peña},
  doi          = {10.1080/10618600.2024.2412174},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {848-858},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Differentially private methods for compositional data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCMC for bayesian nonparametric mixture modeling under differential privacy. <em>JCGS</em>, <em>34</em>(3), 837-847. (<a href='https://doi.org/10.1080/10618600.2024.2410911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the probability density of a population while preserving the privacy of individuals in that population is an important and challenging problem that has received considerable attention in recent years. While the previous literature focused on frequentist approaches, in this article, we propose a Bayesian nonparametric mixture model under differential privacy (DP) and present two Markov chain Monte Carlo (MCMC) algorithms for posterior inference. One is a marginal approach, resembling Neal’s algorithm 5 with a pseudo-marginal Metropolis-Hastings move, and the other is a conditional approach. Although our focus is primarily on local DP, we show that our MCMC algorithms can be easily extended to deal with global differential privacy mechanisms. Moreover, for some carefully chosen mechanisms and mixture kernels, we show how auxiliary parameters can be analytically marginalized, allowing standard MCMC algorithms (i.e., non-privatized, such as Neal’s Algorithm 2) to be efficiently employed. Our approach is general and applicable to any mixture model and privacy mechanism. In several simulations and a real case study, we discuss the performance of our algorithms and evaluate different privacy mechanisms proposed in the frequentist literature. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Mario Beraha and Stefano Favaro and Vinayak Rao},
  doi          = {10.1080/10618600.2024.2410911},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {837-847},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {MCMC for bayesian nonparametric mixture modeling under differential privacy},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grid point approximation for distributed nonparametric smoothing and prediction. <em>JCGS</em>, <em>34</em>(3), 824-836. (<a href='https://doi.org/10.1080/10618600.2024.2409817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel smoothing is a widely used nonparametric method in modern statistical analysis. The problem of efficiently conducting kernel smoothing for a massive dataset on a distributed system is a problem of great importance. In this work, we find that the popularly used one-shot type estimator is highly inefficient for prediction purposes. To this end, we propose a novel grid point approximation (GPA) method, which has the following advantages. First, the resulting GPA estimator is as statistically efficient as the global estimator under mild conditions. Second, it requires no communication and is extremely efficient in terms of computation for prediction. Third, it is applicable to the case where the data are not randomly distributed across different machines. To select a suitable bandwidth, two novel bandwidth selectors are further developed and theoretically supported. Extensive numerical studies are conducted to corroborate our theoretical findings. Two real data examples are also provided to demonstrate the usefulness of our GPA method. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yuan Gao and Rui Pan and Feng Li and Riquan Zhang and Hansheng Wang},
  doi          = {10.1080/10618600.2024.2409817},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {824-836},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Grid point approximation for distributed nonparametric smoothing and prediction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network embedding-based directed community detection with unknown community number. <em>JCGS</em>, <em>34</em>(3), 812-823. (<a href='https://doi.org/10.1080/10618600.2024.2409789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection of network analysis plays an important role in numerous application areas, in which estimating the number of communities is a fundamental issue. However, many existing methods focus on undirected networks ignoring the directionality of edges or unrealistically assume that the number of communities is known a priori. In this article, we develop a data-dependent community detection method for the directed network to determine the number of communities and recover community structures simultaneously, which absorbs the ideas of network embedding and penalized fusion by embedding the out- and in-nodes into low-dimensional vector space and forcing the embedding vectors toward its center. The asymptotic consistency properties of the proposed method are established in terms of network embedding, directed community detection, and estimation of the number of communities. The proposed method is applied on synthetic networks and real brain functional networks, which demonstrate the superior performance of the proposed method against a number of competitors. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Qingzhao Zhang and Jinlong Zhou and Mingyang Ren},
  doi          = {10.1080/10618600.2024.2409789},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {812-823},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Network embedding-based directed community detection with unknown community number},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient modeling of spatial extremes over large geographical domains. <em>JCGS</em>, <em>34</em>(3), 795-811. (<a href='https://doi.org/10.1080/10618600.2024.2409784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various natural phenomena exhibit spatial extremal dependence at short spatial distances. However, existing models proposed in the spatial extremes literature often assume that extremal dependence persists across the entire domain. This is a strong limitation when modeling extremes over large geographical domains, and yet it has been mostly overlooked in the literature. We here develop a more realistic Bayesian framework based on a novel Gaussian scale mixture model, with the Gaussian process component defined though a stochastic partial differential equation yielding a sparse precision matrix, and the random scale component modeled as a low-rank Pareto-tailed or Weibull-tailed spatial process determined by compactly-supported basis functions. We show that our proposed model is approximately tail-stationary and that it can capture a wide range of extremal dependence structures. Its inherently sparse probabilistic structure allows fast Bayesian computations in high spatial dimensions based on a customized Markov chain Monte Carlo algorithm prioritizing calibration in the tail. We fit our model to analyze heavy monsoon rainfall data in Bangladesh. Our study shows that our model outperforms natural competitors and that it fits precipitation extremes well. We finally use the fitted model to draw inference on long-term return levels for marginal precipitation and spatial aggregates. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Arnab Hazra and Raphaël Huser and David Bolin},
  doi          = {10.1080/10618600.2024.2409784},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {795-811},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Efficient modeling of spatial extremes over large geographical domains},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A latent space model for weighted keyword co-occurrence networks with applications in knowledge discovery in statistics. <em>JCGS</em>, <em>34</em>(3), 779-794. (<a href='https://doi.org/10.1080/10618600.2024.2407465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keywords are widely recognized as pivotal in conveying the central idea of academic articles. In this article, we construct a weighted and dynamic keyword co-occurrence network and propose a latent space model for analyzing it. Our model has two special characteristics. First, it is applicable to weighted networks; however, most previous models were primarily designed for unweighted networks. Simply replacing the frequency of keyword co-occurrence with binary values would result in a significant loss of information. Second, our model can handle the situation where network nodes evolve over time, and assess the effect of new nodes on network connectivity. We use the projected gradient descent algorithm to estimate the latent positions and establish the theoretical properties of the estimators. In the real data application, we study the keyword co-occurrence network within the field of statistics. We identify popular keywords over the whole period as well as within each time period. For keyword pairs, our model provides a new way to assess the association between them. Finally, we observe that the interest of statisticians in emerging research areas has gradually grown in recent years. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yan Zhang and Rui Pan and Xuening Zhu and Kuangnan Fang and Hansheng Wang},
  doi          = {10.1080/10618600.2024.2407465},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {7},
  number       = {3},
  pages        = {779-794},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {A latent space model for weighted keyword co-occurrence networks with applications in knowledge discovery in statistics},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is this normal? a new projection pursuit index to assess a sample against a multivariate null distribution. <em>JCGS</em>, <em>34</em>(2), 771-777. (<a href='https://doi.org/10.1080/10618600.2025.2468785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data problems contain some reference or normal conditions, upon which to compare newly collected data. This scenario occurs in data collected as part of clinical trials to detect adverse events, or for measuring climate change against historical norms. The data is typically multivariate, and often the normal ranges are specified by a multivariate normal distribution. The work presented in this article develops methods to compare the new sample against the reference distribution with high-dimensional visualization. It uses a projection pursuit guided tour to produce a sequence of low-dimensional projections steered toward those where the new sample is most different from the reference. A new projection pursuit index is defined for this purpose. The tour visualization also includes drawing of the projected ellipse, which is computed analytically, corresponding to the reference distribution. The methods are implemented in the R package, tourr . Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Annalisa Calvi and Ursula Laa and Dianne Cook},
  doi          = {10.1080/10618600.2025.2468785},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {771-777},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Is this normal? a new projection pursuit index to assess a sample against a multivariate null distribution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using rejection sampling probability of acceptance as a measure of independence. <em>JCGS</em>, <em>34</em>(2), 759-770. (<a href='https://doi.org/10.1080/10618600.2024.2388544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new association statistic for determining whether random variables are statistically independent. The proposed association statistic can also be used to examine the strength of both linear and nonlinear dependency between variables. This statistic is derived by examining how the conditional probabilities of events differ from their corresponding marginal probabilities. The new statistic is defined in terms of the probability of acceptance commonly associated with the Accept–Reject algorithm and has a very simple formula. The simulated density of the probability of acceptance can be used to measure the degree of uncertainty of the estimated values of the new association statistic. The results from simulations as well as examples that employ real data indicate that this new association statistic is very powerful in detecting linear and nonlinear associations between two random variables. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Markku Kuismin},
  doi          = {10.1080/10618600.2024.2388544},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {759-770},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Using rejection sampling probability of acceptance as a measure of independence},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient nonparametric estimation of 3D point cloud signals through distributed learning. <em>JCGS</em>, <em>34</em>(2), 746-758. (<a href='https://doi.org/10.1080/10618600.2024.2406301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in technology have elevated the prominence of 3D point cloud data, making its analysis increasingly vital across various applications. This need drives the demand for advanced statistical analytic approaches to handle challenges such as size, sparsity, and irregularity in 3D point clouds while ensuring accurate and efficient information extraction. This article introduces a novel nonparametric distributed (NPD) learning framework that uses trivariate spline smoothing over a triangulation of domain. The proposed NPD algorithm features a straightforward, scalable, and communication-efficient implementation scheme that can achieve near-linear speedup. In addition, we provide rigorous theoretical support for the NPD estimation framework and demonstrate that the NPD spline estimators attain the same convergence rate as the global spline estimators obtained using the entire dataset and achieve the optimal nonparametric convergence rate established by Stone under some regularity conditions. To evaluate the efficacy of the proposed NPD method, we conduct simulation studies comparing it with several global nonparametric estimation methods used to smooth the 3D data. The results demonstrate the superior performance of the NPD method in accurately and efficiently processing and learning from 3D point clouds, highlighting its potential to advance large and complex data analysis. Supplementary material, which contains related technical details, proofs of the theoretical results, and additional results in simulation studies, is available online.},
  archive      = {J_JCGS},
  author       = {Guannan Wang and Yuchun Wang and Annie S. Gao and Li Wang and Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1080/10618600.2024.2406301},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {746-758},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Efficient nonparametric estimation of 3D point cloud signals through distributed learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global inference and test for eigensystems of imaging data over complicated domains. <em>JCGS</em>, <em>34</em>(2), 729-745. (<a href='https://doi.org/10.1080/10618600.2024.2374584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nonparametric approach for analyzing eigensystems of image data over a complex domain is novelly developed. The proposed estimators, which are based on bivariate splines, have both oracle efficiency of O p ( n − 1 / 2 ) , meaning they are asymptotically indistinguishable from estimators computed with true trajectories, and computational efficiency with more convenient spectrum decomposition forms. Under mild conditions, we derive consistency and weak convergence of our proposed estimators, which can be used to construct confidence intervals and simultaneous confidence corridors for any individual eigenvalue and eigenfunction, as well as design uniform inference procedures for eigensystems with a diverging number of components. In addition, we extend our method to two-sample test problems. Extensive simulation results provide strong support for the asymptotic theory, and the procedures are applied to two potential seawater temperature data sets to demonstrate its validity.},
  archive      = {J_JCGS},
  author       = {Leheng Cai and Qirui Hu},
  doi          = {10.1080/10618600.2024.2374584},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {729-745},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Global inference and test for eigensystems of imaging data over complicated domains},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian nowcasting with laplacian-P-splines. <em>JCGS</em>, <em>34</em>(2), 718-728. (<a href='https://doi.org/10.1080/10618600.2024.2395414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During an epidemic, the daily number of reported infected cases, deaths or hospitalizations is often lower than the actual number due to reporting delays. Nowcasting aims to estimate the cases that have not yet been reported and combine it with the already reported cases to obtain an estimate of the daily cases. In this article, we present a fast and flexible Bayesian approach for nowcasting by combining P-splines and Laplace approximations. Laplacian-P-splines provide a flexible framework for nowcasting that is computationally less demanding as compared to traditional Markov chain Monte Carlo techniques. The proposed approach also permits to naturally quantify the prediction uncertainty. Model performance is assessed through simulations and the nowcasting method is applied to COVID-19 mortality and incidence cases in Belgium. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Bryan Sumalinab and Oswaldo Gressani and Niel Hens and Christel Faes},
  doi          = {10.1080/10618600.2024.2395414},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {718-728},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Bayesian nowcasting with laplacian-P-splines},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ms.FPOP: A fast exact segmentation algorithm with a multiscale penalty. <em>JCGS</em>, <em>34</em>(2), 707-717. (<a href='https://doi.org/10.1080/10618600.2024.2402895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a time series in R n with a piecewise constant mean and independent noises, we propose an exact dynamic programming algorithm for minimizing a least-squares criterion with a multiscale penalty, favoring well-spread changepoints. This penalty was proposed by Verzelen et al. and achieves optimal rates for changepoint detection and changepoint localization in a non-asymptotic scenario. Our proposed algorithm, Multiscale Functional Pruning Optimal Partitioning (Ms.FPOP), extends functional pruning ideas presented in Rigaill and Maidstone et al. to multiscale penalties. For large signals ( n ≥ 10 5 ) with sparse changepoints, Ms.FPOP is shown empirically to be quasi-linear and faster than the Pruned Exact Linear Time (PELT) method of Killick et al. applied to the multiscale penalty of Verzelen et al. which exhibits quadratic slowdown in these cases. We propose an efficient implementation of Ms.FPOP coded in C++ interfaced with R that can segment profiles of up to n = 10 6 in a matter of seconds. Our algorithm works for slightly more general multiscale penalties. In particular, it allows a minimum segment length to be imposed. Using simple simulations we then show that where profiles are sufficiently large ( n ≥ 10 4 ), Ms.FPOP using the multiscale penalty of Verzelen et al. is typically more powerful than optimizing a least-squares criterion with the BIC penalty of Yao, a criterion that was shown by Fearnhead and Rigaill to perfom well across a wide range of scenarios. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Arnaud Liehrmann and Guillem Rigaill},
  doi          = {10.1080/10618600.2024.2402895},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {707-717},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Ms.FPOP: A fast exact segmentation algorithm with a multiscale penalty},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast bayesian inference for spatial mean-parameterized Conway–Maxwell–Poisson models. <em>JCGS</em>, <em>34</em>(2), 697-706. (<a href='https://doi.org/10.1080/10618600.2024.2394460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data with complex features arise in many disciplines, including ecology, agriculture, criminology, medicine, and public health. Zero inflation, spatial dependence, and non-equidispersion are common features in count data. There are currently two classes of models that allow for these features—the mode-parameterized Conway–Maxwell–Poisson (COMP) distribution and the generalized Poisson model. However both require the use of either constraints on the parameter space or a parameterization that leads to challenges in interpretability. We propose spatial mean-parameterized COMP models that retain the flexibility of these models while resolving the above issues. We use a Bayesian spatial filtering approach in order to efficiently handle high-dimensional spatial data and we use reversible-jump MCMC to automatically choose the basis vectors for spatial filtering. The COMP distribution poses two additional computational challenges—an intractable normalizing function in the likelihood and no closed-form expression for the mean. We propose a fast computational approach that addresses these challenges by, respectively, introducing an efficient auxiliary variable algorithm and pre-computing key approximations for fast likelihood evaluation. We illustrate the application of our methodology to simulated and real datasets, including Texas HPV-cancer data and US vaccine refusal data. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Bokgyeong Kang and John Hughes and Murali Haran},
  doi          = {10.1080/10618600.2024.2394460},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {697-706},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Fast bayesian inference for spatial mean-parameterized Conway–Maxwell–Poisson models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient large-scale nonstationary spatial covariance function estimation using convolutional neural networks. <em>JCGS</em>, <em>34</em>(2), 683-696. (<a href='https://doi.org/10.1080/10618600.2024.2402277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial processes observed in various fields, such as climate and environmental science, often occur at large-scale and demonstrate spatial nonstationarity. However, fitting a Gaussian process with a nonstationary Matérn covariance is challenging, as it requires handling the complexity and computational demands associated with modeling the varying spatial dependencies over large and heterogeneous domains. Previous studies in the literature have tackled this challenge by employing spatial partitioning techniques to estimate the parameters that vary spatially in the covariance function. The selection of partitions is an important consideration, but it is often subjective and lacks a data-driven approach. To address this issue, in this study, we use the power of Convolutional Neural Networks (ConvNets) to derive subregions from the nonstationary data by employing a selection mechanism to identify subregions that exhibit similar behavior to stationary fields. We rely on the ExaGeoStat software for large-scale geospatial modeling to implement the nonstationary Matérn covariance for large scale exact computation of nonstationary Gaussian likelihood. We also assess the performance of the proposed method with synthetic and real datasets at large-scale. The results revealed enhanced accuracy in parameter estimations when relying on ConvNet-based partition compared to traditional user-defined approaches.},
  archive      = {J_JCGS},
  author       = {Pratik Nag and Yiping Hong and Sameh Abdulah and Ghulam A. Qadir and Marc G. Genton and Ying Sun},
  doi          = {10.1080/10618600.2024.2402277},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {683-696},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Efficient large-scale nonstationary spatial covariance function estimation using convolutional neural networks},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond time-homogeneity for continuous-time multistate markov models. <em>JCGS</em>, <em>34</em>(2), 668-682. (<a href='https://doi.org/10.1080/10618600.2024.2388609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Multistate Markov models are a canonical parametric approach for data modeling of observed or latent stochastic processes supported on a finite state space. Continuous-time Markov processes describe data that are observed irregularly over time, as is often the case in longitudinal medical data, for example. Assuming that a continuous-time Markov process is time-homogeneous, a closed-form likelihood function can be derived from the Kolmogorov forward equations—a system of differential equations with a well-known matrix-exponential solution. Unfortunately, however, the forward equations do not admit an analytical solution for continuous-time, time- inhomogeneous Markov processes, and so researchers and practitioners often make the simplifying assumption that the process is piecewise time-homogeneous. In this article, we provide intuitions and illustrations of the potential biases for parameter estimation that may ensue in the more realistic scenario that the piecewise-homogeneous assumption is violated, and we advocate for a solution for likelihood computation in a truly time-inhomogeneous fashion. Particular focus is afforded to the context of multistate Markov models that allow for state label misclassifications, which applies more broadly to hidden Markov models (HMMs), and Bayesian computations bypass the necessity for computationally demanding numerical gradient approximations for obtaining maximum likelihood estimates (MLEs). Supplemental materials are available online.},
  archive      = {J_JCGS},
  author       = {Emmett B. Kendall and Jonathan P. Williams and Gudmund H. Hermansen and Frederic Bois and Vo Hong Thanh},
  doi          = {10.1080/10618600.2024.2388609},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {668-682},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Beyond time-homogeneity for continuous-time multistate markov models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bootstrap inference for linear time-varying coefficient models in locally stationary time series. <em>JCGS</em>, <em>34</em>(2), 654-667. (<a href='https://doi.org/10.1080/10618600.2024.2403705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying coefficient models can capture evolving relationships. However, constructing asymptotic confidence bands for coefficient curves in these models is challenging due to slow convergence rates and the presence of various nuisance parameters. A residual-based sieve bootstrap method has recently been proposed to address these issues. While it successfully produces confidence bands with accurate empirical coverage, its applicability is restricted to strictly stationary processes. We introduce a new bootstrap scheme, the local blockwise wild bootstrap (LBWB), that allows for locally stationary processes. The LBWB can replicate the distribution of the parameter estimates while automatically accounting for nuisance parameters. An extensive simulation study reveals the superior performance of the LBWB compared to various benchmark approaches. It also shows the potential applicability of the LBWB in broader scenarios, including time-varying cointegrating models. We then examine herding effects in the Chinese renewable energy market using the proposed methods. Our findings strongly support the presence of herding behaviors before 2016, aligning with earlier studies. However, contrary to previous research, we find no significant evidence of herding between around 2018 and 2021. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yicong Lin and Mingxuan Song and Bernhard van der Sluis},
  doi          = {10.1080/10618600.2024.2403705},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {654-667},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Bootstrap inference for linear time-varying coefficient models in locally stationary time series},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A tidy framework and infrastructure to systematically assemble spatio-temporal indexes from multivariate data. <em>JCGS</em>, <em>34</em>(2), 642-653. (<a href='https://doi.org/10.1080/10618600.2024.2374960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indexes are useful for summarizing multivariate information into single metrics for monitoring, communicating, and decision-making. While most work has focused on defining new indexes for specific purposes, more attention needs to be directed toward making it possible to understand index behavior in different data conditions, and to determine how their structure affects their values and the variability therein. Here we discuss a modular data pipeline recommendation to assemble indexes. It is universally applicable to index computation and allows investigation of index behavior as part of the development procedure. One can compute indexes with different parameter choices, adjust steps in the index definition by adding, removing, and swapping them to experiment with various index designs, calculate uncertainty measures, and assess indexes’ robustness. The article presents three examples to illustrate the usage of the pipeline framework: comparison of two different indexes designed to monitor the spatio-temporal distribution of drought in Queensland, Australia; the effect of dimension reduction choices on the Global Gender Gap Index (GGGI) on countries’ ranking; and how to calculate bootstrap confidence intervals for the Standardized Precipitation Index (SPI). The methods are supported by a new R package, called tidyindex . Supplemental materials for the article are available online.},
  archive      = {J_JCGS},
  author       = {H. Sherry Zhang and Dianne Cook and Ursula Laa and Nicolas Langrené and Patricia Menéndez},
  doi          = {10.1080/10618600.2024.2374960},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {642-653},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {A tidy framework and infrastructure to systematically assemble spatio-temporal indexes from multivariate data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Degrees of freedom: Search cost and self-consistency. <em>JCGS</em>, <em>34</em>(2), 630-641. (<a href='https://doi.org/10.1080/10618600.2024.2388545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model degrees of freedom ( df ) is a fundamental concept in statistics because it quantifies the flexibility of a fitting procedure and is indispensable in model selection. To investigate the gap between df and the number of independent variables in the fitting procedure, Tibshirani introduced the search degrees of freedom ( sdf ) concept to account for the search cost during model selection. However, this definition has two limitations: it does not consider fitting procedures in augmented spaces and does not use the same fitting procedure for sdf and df . We propose a modified search degrees of freedom ( msdf ) to directly account for the cost of searching in either original or augmented spaces. We check this definition for various fitting procedures, including classical linear regressions, spline methods, adaptive regressions (the best subset and the lasso), regression trees, and multivariate adaptive regression splines (MARS). In many scenarios when sdf is applicable, msdf reduces to sdf . However, for certain procedures like the lasso, msdf offers a fresh perspective on search costs. For some complex procedures like MARS, the df has been pre-determined during model fitting, but the df of the final fitted procedure might differ from the pre-determined one. To investigate this discrepancy, we introduce the concepts of nominal df and actual df , and define the property of self-consistency , which occurs when there is no gap between these two df ’s. We propose a correction procedure for MARS to align these two df ’s, demonstrating improved fitting performance through extensive simulations and two real data applications. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Lijun Wang and Hongyu Zhao and Xiaodan Fan},
  doi          = {10.1080/10618600.2024.2388545},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {630-641},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Degrees of freedom: Search cost and self-consistency},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous coefficient clustering and sparsity for multivariate mixed models. <em>JCGS</em>, <em>34</em>(2), 618-629. (<a href='https://doi.org/10.1080/10618600.2024.2402904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications of multivariate longitudinal mixed models, it is reasonable to assume that each response is informed by only a subset of covariates. Moreover, one or more responses may exhibit the same relationship to a particular covariate for example, if they are capturing the same underlying aspect of an individual physical, mental, and emotional health. To address the above challenges, we propose a method for simultaneous clustering and variable selection of fixed effect coefficients in multivariate mixed models. We achieve this in a computationally scalable manner via a composite likelihood approach: separate mixed models are first fitted to each response, after which the model estimates are combined into a single quadratic form resembling a multivariate Wald statistic. We then augment this with fusion- and sparsity-inducing penalties based on broken adaptive ridge regression. Simulation studies demonstrate that the proposed composite quadratic estimator is similar to or better than several existing techniques for fixed effects selection in (univariate) mixed models while being computationally much more efficient. We apply the proposed method to longitudinal panel data from Australia to quantify how an individual’s overall health, assessed via a set of eight composite scores, evolves as a function of various demographic and lifestyle variables.},
  archive      = {J_JCGS},
  author       = {Francis K. C. Hui and Khue-Dung Dang and Luca Maestrini},
  doi          = {10.1080/10618600.2024.2402904},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {618-629},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Simultaneous coefficient clustering and sparsity for multivariate mixed models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable estimation for structured additive distributional regression. <em>JCGS</em>, <em>34</em>(2), 601-617. (<a href='https://doi.org/10.1080/10618600.2024.2388604'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining probabilistic models is of high relevance in many recent applications. However, estimation of such distributional models with very large datasets remains a difficult task. In particular, the use of rather complex models can easily lead to memory-related efficiency problems and thereby make estimation infeasible even on high-performance computers. We address these challenges and propose a novel backfitting algorithm, which is based on the ideas of stochastic gradient descent and can deal virtually with any amount of data on a conventional laptop. The algorithm performs automatic selection of variables and determination of smoothing parameters. Its performance is superior or at least equivalent to other implementations for structured additive distributional regression, such as, gradient boosting, while maintaining lower computation time. Performance is evaluated using an extensive simulation study and an exceptionally challenging example of lightning count prediction across Austria with over 9 million observations and 80 covariates. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Nikolaus Umlauf and Johannes Seiler and Mattias Wetscher and Thorsten Simon and Stefan Lang and Nadja Klein},
  doi          = {10.1080/10618600.2024.2388604},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {601-617},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Scalable estimation for structured additive distributional regression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariance assisted multivariate penalized additive regression (CoMPAdRe). <em>JCGS</em>, <em>34</em>(2), 591-600. (<a href='https://doi.org/10.1080/10618600.2024.2407453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for the simultaneous selection and estimation of multivariate sparse additive models with correlated errors. Our method called Covariance Assisted Multivariate Penalized Additive Regression (CoMPAdRe) simultaneously selects among null, linear, and smooth nonlinear effects for each predictor while incorporating joint estimation of the sparse residual structure among responses, with the motivation that accounting for inter-response correlation structure can lead to improved accuracy in variable selection and estimation efficiency. CoMPAdRe is constructed in a computationally efficient way that allows the selection and estimation of linear and nonlinear covariates to be conducted in parallel across responses. Compared to single-response approaches that marginally select linear and nonlinear covariate effects, we demonstrate in simulation studies that the joint multivariate modeling leads to gains in both estimation efficiency and selection accuracy, of greater magnitude in settings where signal is moderate relative to the level of noise. We apply our approach to protein-mRNA expression levels from multiple breast cancer pathways obtained from The Cancer Proteome Atlas and characterize both mRNA-protein associations and protein–protein subnetworks for each pathway. We find nonlinear mRNA-protein associations for the Core Reactive, EMT, PIK-AKT, and RTK pathways. Supplementary Materials are available online.},
  archive      = {J_JCGS},
  author       = {Neel Desai and Veerabhadran Baladandayuthapani and Russell T. Shinohara and Jeffrey S. Morris},
  doi          = {10.1080/10618600.2024.2407453},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {591-600},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Covariance assisted multivariate penalized additive regression (CoMPAdRe)},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wasserstein-kaplan-meier survival regression. <em>JCGS</em>, <em>34</em>(2), 580-590. (<a href='https://doi.org/10.1080/10618600.2024.2404708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis plays a pivotal role in medical research, offering valuable insights into the timing of events such as survival time. One common challenge in survival analysis is the necessity to adjust the survival function to account for additional factors, such as age, gender, and ethnicity. We propose an innovative regression model for right-censored survival data across heterogeneous populations, leveraging the Wasserstein space of probability measures. Our approach models the probability measure of survival time and the corresponding nonparametric Kaplan-Meier estimator for each subgroup as elements of the Wasserstein space. The Wasserstein space provides a flexible framework for modeling heterogeneous populations, allowing us to capture complex relationships between covariates and survival times. We address an underexplored aspect by deriving the non-asymptotic convergence rate of the Kaplan-Meier estimator to the underlying probability measure in terms of the Wasserstein metric. The proposed model is supported with a solid theoretical foundation including pointwise and uniform convergence rates, along with an efficient algorithm for model fitting. The proposed model effectively accommodates random variation that may exist in the probability measures across different subgroups, demonstrating superior performance in both simulations and two case studies compared to the Cox proportional hazards model and other alternative models. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yidong Zhou and Hans-Georg Müller},
  doi          = {10.1080/10618600.2024.2404708},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {580-590},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Wasserstein-kaplan-meier survival regression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FAStEN: An efficient adaptive method for feature selection and estimation in high-dimensional functional regressions. <em>JCGS</em>, <em>34</em>(2), 567-579. (<a href='https://doi.org/10.1080/10618600.2024.2407464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional regression analysis is an established tool for many contemporary scientific applications. Regression problems involving large and complex datasets are ubiquitous, and feature selection is crucial for avoiding overfitting and achieving accurate predictions. We propose a new, flexible and ultra-efficient approach to perform feature selection in a sparse high dimensional function-on-function regression problem, and we show how to extend it to the scalar-on-function framework. Our method, called FAStEN , combines functional data, optimization, and machine learning techniques to perform feature selection and parameter estimation simultaneously. We exploit the properties of Functional Principal Components and the sparsity inherent to the Dual Augmented Lagrangian problem to significantly reduce computational cost, and we introduce an adaptive scheme to improve selection accuracy. In addition, we derive asymptotic oracle properties, which guarantee estimation and selection consistency for the proposed FAStEN estimator. Through an extensive simulation study, we benchmark our approach to the best existing competitors and demonstrate a massive gain in terms of CPU time and selection performance, without sacrificing the quality of the coefficients’ estimation. The theoretical derivations and the simulation study provide a strong motivation for our approach. Finally, we present an application to brain fMRI data from the AOMIC PIOP1 study. Complete FAStEN code is provided at https://github.com/IBM/funGCN . Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Tobia Boschi and Lorenzo Testa and Francesca Chiaromonte and Matthew Reimherr},
  doi          = {10.1080/10618600.2024.2407464},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {567-579},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {FAStEN: An efficient adaptive method for feature selection and estimation in high-dimensional functional regressions},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal subsampling for functional quasi-mode regression with big data. <em>JCGS</em>, <em>34</em>(2), 552-566. (<a href='https://doi.org/10.1080/10618600.2024.2402279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose investigating optimal subsampling for functional regression with massive datasets based on the mode value, which is referred to as functional quasi-mode regression, to reduce data volume and alleviate computational burden. Using data-adaptive weights derived from regression residuals, the suggested regression offers enhanced robustness against nonnormal errors compared to traditional least squares or maximum likelihood estimation methods. To estimate the model, we employ B-spline basis functions to approximate the functional coefficient and include a penalty term in the objective function for enforcing smoothness in the resulting estimator. We adopt a computationally efficient mode-expectation-maximization algorithm, augmented by a Gaussian kernel, for numerical estimation. Under mild regularity conditions, we derive the asymptotic distributions of both full data and subsample quasi-mode estimators. The optimal subsampling probabilities by minimizing the asymptotic variance-covariance matrix under A- and L-optimality criteria are identified. These optimal probabilities rely on the full data estimate, prompting the development of a two-step algorithm to approximate the optimal subsampling procedure. The resultant algorithm is processing-efficient and can significantly reduce computational time compared to the full data approach. We also establish the asymptotic normality of the quasi-mode estimator obtained through this two-step algorithm. To assess finite sample performance, we conduct Monte Carlo simulations and analyze air quality data, showcasing the effectiveness of the developed estimator. Supplemental materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Tao Wang},
  doi          = {10.1080/10618600.2024.2402279},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {552-566},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Optimal subsampling for functional quasi-mode regression with big data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient convex PCA with applications to wasserstein GPCA and ranked data. <em>JCGS</em>, <em>34</em>(2), 540-551. (<a href='https://doi.org/10.1080/10618600.2024.2402280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convex PCA, which was introduced by Bigot et al. modifies Euclidean PCA by restricting the data and the principal components to lie in a given convex subset of a Hilbert space. This setting arises naturally in many applications, including distributional data in the Wasserstein space of an interval, and ranked compositional data under the Aitchison geometry. Our contribution in this article is 3-fold. First, we present several new theoretical results including consistency as well as continuity and differentiability of the objective function in the finite dimensional case. Second, we develop a numerical implementation of finite dimensional convex PCA when the convex set is polyhedral, and show that this provides a natural approximation of Wasserstein GPCA. Third, we illustrate our results with two financial applications, namely distributions of stock returns ranked by size and the capital distribution curve, both of which are of independent interest in stochastic portfolio theory. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Steven Campbell and Ting-Kam Leonard Wong},
  doi          = {10.1080/10618600.2024.2402280},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {540-551},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Efficient convex PCA with applications to wasserstein GPCA and ranked data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal decorrelated score subsampling for high-dimensional generalized linear models under measurement constraints. <em>JCGS</em>, <em>34</em>(2), 530-539. (<a href='https://doi.org/10.1080/10618600.2024.2402896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When responses of massive data are hard to obtain due to some reasons such as privacy and security, high cost and administrative management, response-free subsampling is considered. In this article, we propose a response-free decorrelated score subsampling approach to estimate and make statistical inference for a preconceived low-dimensional parameter in high-dimensional generalized linear models. The unconditional consistency and asymptotic normality of the resulting weighted subsample estimator are established using martingale techniques since the subsamples are no longer independent. The optimal response-free subsampling probabilities are derived based on A- and L-optimality criteria. Based on the optimal subsample, we further propose a more efficient and stable unweighted decorrelated score subsample estimator. The satisfactory performance of our proposed subsample estimators are demonstrated by simulation results and two real data applications. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yujing Shao and Lei Wang and Heng Lian},
  doi          = {10.1080/10618600.2024.2402896},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {530-539},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Optimal decorrelated score subsampling for high-dimensional generalized linear models under measurement constraints},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blocked gibbs sampler for hierarchical dirichlet processes. <em>JCGS</em>, <em>34</em>(2), 519-529. (<a href='https://doi.org/10.1080/10618600.2024.2388543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Posterior computation in hierarchical Dirichlet process (HDP) mixture models is an active area of research in nonparametric Bayes inference of grouped data. Existing literature almost exclusively focuses on the Chinese restaurant franchise (CRF) analogy of the marginal distribution of the parameters, which can mix poorly and has a quadratic complexity with the sample size. A recently developed slice sampler allows for efficient blocked updates of the parameters, but is shown to be statistically unstable in our article. We develop a blocked Gibbs sampler that employs a truncated approximation of the underlying random measures to sample from the posterior distribution of HDP, which produces statistically stable results, is highly scalable with respect to sample size, and is shown to have good mixing. The heart of the construction is to endow the shared concentration parameter with an appropriately chosen gamma prior that allows us to break the dependence of the shared mixing proportions and permits independent updates of certain log-concave random variables in a block. En route , we develop an efficient rejection sampler for these random variables leveraging piece-wise tangent-line approximations. Supplementary materials, which include substantive additional details and code, are available online.},
  archive      = {J_JCGS},
  author       = {Snigdha Das and Yabo Niu and Yang Ni and Bani K. Mallick and Debdeep Pati},
  doi          = {10.1080/10618600.2024.2388543},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {519-529},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Blocked gibbs sampler for hierarchical dirichlet processes},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian federated learning with hamiltonian monte carlo: Algorithm and theory. <em>JCGS</em>, <em>34</em>(2), 509-518. (<a href='https://doi.org/10.1080/10618600.2024.2380051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel and efficient Bayesian federated learning algorithm, namely, the Federated Averaging stochastic Hamiltonian Monte Carlo (FA-HMC), for parameter estimation and uncertainty quantification. We establish rigorous convergence guarantees of FA-HMC on non-iid distributed datasets, under the strong convexity and Hessian smoothness assumptions. Our analysis investigates the effects of parameter space dimension, noise on gradients and momentum, and the frequency of communication (between the central node and local nodes) on the convergence and communication costs of FA-HMC. Beyond that, we establish the tightness of our analysis by showing that the convergence rate cannot be improved even for continuous FA-HMC process. Moreover, extensive empirical studies demonstrate that FA-HMC outperforms the existing Federated Averaging-Langevin Monte Carlo (FA-LD) algorithm. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jiajun Liang and Qian Zhang and Wei Deng and Qifan Song and Guang Lin},
  doi          = {10.1080/10618600.2024.2380051},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {509-518},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Bayesian federated learning with hamiltonian monte carlo: Algorithm and theory},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmentation samplers for multinomial probit bayesian additive regression trees. <em>JCGS</em>, <em>34</em>(2), 498-508. (<a href='https://doi.org/10.1080/10618600.2024.2388605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multinomial probit (MNP) framework is based on a multivariate Gaussian latent structure, allowing for natural extensions to multilevel modeling. Unlike multinomial logistic models, MNP does not assume independent alternatives. Kindo, Wang, and Peña proposed multinomial probit BART (MPBART) to accommodate Bayesian additive regression trees (BART) formulation in MNP. The posterior sampling algorithms for MNP and MPBART are collapsed Gibbs samplers. Because the collapsing augmentation strategy yields a geometric rate of convergence no greater than that of a standard Gibbs sampling step, it is recommended whenever computationally feasible (Liu; Imai and van Dyk). While this strategy necessitates simple sampling steps and a reasonably fast converging Markov chain, the complexity of the stochastic search for posterior trees may undermine its benefit. We address this problem by sampling posterior trees conditional on the constrained parameter space and compare our proposals to that of Kindo, Wang, and Peña, who sample posterior trees based on an augmented parameter space. In terms of MCMC convergence and posterior predictive accuracy, our proposals outperform the augmented tree sampling approach. We also show that the theoretical mixing rates of our proposals are guaranteed to be no greater than the augmented tree sampling approach. Appendices and codes for simulations and demonstrations are available online.},
  archive      = {J_JCGS},
  author       = {Yizhen Xu and Joseph Hogan and Michael Daniels and Rami Kantor and Ann Mwangi},
  doi          = {10.1080/10618600.2024.2388605},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {498-508},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Augmentation samplers for multinomial probit bayesian additive regression trees},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate cross-validated mean estimates for bayesian hierarchical regression models. <em>JCGS</em>, <em>34</em>(2), 488-497. (<a href='https://doi.org/10.1080/10618600.2024.2404711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel procedure for obtaining cross-validated predictive estimates for Bayesian hierarchical regression models (BHRMs). BHRMs are popular for modeling complex dependence structures (e.g., Gaussian processes and Gaussian Markov random fields) but can be computationally expensive to run. Cross-validation (CV) is, therefore, not a common practice to evaluate the predictive performance of BHRMs. Our method circumvents the need to rerun computationally costly estimation methods for each cross-validation fold and makes CV more feasible for large BHRMs. We shift the CV problem from probability-based sampling to a familiar and straightforward optimization problem by conditioning on the variance-covariance parameters. Our approximation applies to leave-one-out CV and leave-one-cluster-out CV, the latter of which is more appropriate for models with complex dependencies. In many cases, this produces estimates equivalent to full CV. We provide theoretical results, demonstrate the efficacy of our method on publicly available data and in simulations, and compare the model performance with several competing methods for CV approximation. Code and other supplementary materials available online.},
  archive      = {J_JCGS},
  author       = {Amy Zhang and Michael J. Daniels and Changcheng Li and Le Bao},
  doi          = {10.1080/10618600.2024.2404711},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {488-497},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Approximate cross-validated mean estimates for bayesian hierarchical regression models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast computer model calibration using annealed and transformed variational inference. <em>JCGS</em>, <em>34</em>(2), 474-487. (<a href='https://doi.org/10.1080/10618600.2024.2374962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer models play a crucial role in numerous scientific and engineering domains. To ensure the accuracy of simulations, it is essential to properly calibrate the input parameters of these models through statistical inference. While Bayesian inference is the standard approach for this task, employing Markov chain Monte Carlo methods often encounters computational hurdles due to the costly evaluation of likelihood functions and slow mixing rates. Although variational inference (VI) can be a fast alternative to traditional Bayesian approaches, VI has limited applicability due to boundary issues and local optima problems. To address these challenges, we propose flexible VI methods based on deep generative models that do not require parametric assumptions on the variational distribution. We embed a surjective transformation in our framework to avoid posterior truncation at the boundary. Additionally, we provide theoretical conditions that guarantee the success of the algorithm. Furthermore, our temperature annealing scheme and fine-tuning can prevent being trapped in local optima through a series of intermediate posteriors and weight adjustment. We apply our method to infectious disease models and a geophysical model, illustrating that the proposed method can provide fast and accurate inference compared to its competitors.},
  archive      = {J_JCGS},
  author       = {Dongkyu Derek Cho and Won Chang and Jaewoo Park},
  doi          = {10.1080/10618600.2024.2374962},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {474-487},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Fast computer model calibration using annealed and transformed variational inference},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational methods for fast bayesian model assessment via calibrated posterior p-values. <em>JCGS</em>, <em>34</em>(2), 462-473. (<a href='https://doi.org/10.1080/10618600.2024.2374585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Posterior predictive p -values (ppps) have become popular tools for Bayesian model assessment, being general-purpose and easy to use. However, interpretation can be difficult because their distribution is not uniform under the hypothesis that the model did generate the data. Calibrated ppps (cppps) can be obtained via a bootstrap-like procedure, yet remain unavailable in practice due to high computational cost. This article introduces methods to enable efficient approximation of cppps and their uncertainty for fast model assessment. We first investigate the computational tradeoff between the number of calibration replicates and the number of MCMC samples per replicate. Provided that the MCMC chain from the real data has converged, using short MCMC chains per calibration replicate can save significant computation time compared to naive implementations, without significant loss in accuracy. We propose different variance estimators for the cppp approximation, which can be used to confirm quickly the lack of evidence against model misspecification. As variance estimation uses effective sample sizes of many short MCMC chains, we show these can be approximated well from the real-data MCMC chain. The procedure for cppp is implemented in NIMBLE, a flexible framework for hierarchical modeling that supports many models and discrepancy measures. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Sally Paganin and Perry de Valpine},
  doi          = {10.1080/10618600.2024.2374585},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {462-473},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Computational methods for fast bayesian model assessment via calibrated posterior p-values},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-factor analysis of citation networks. <em>JCGS</em>, <em>34</em>(2), 448-461. (<a href='https://doi.org/10.1080/10618600.2024.2394464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One compelling use of citation networks is to characterize papers by their relationships to the surrounding literature. We propose a method to characterize papers by embedding them into two distinct “co-factor” spaces: one describing how papers send citations, and the other describing how papers receive citations. This approach presents several challenges. First, older documents cannot cite newer documents, and thus it is not clear that co-factors are even identifiable. We resolve this challenge by developing a co-factor model for asymmetric adjacency matrices with missing lower triangles and showing that identification is possible. We then frame estimation as a matrix completion problem and develop a specialized implementation of matrix completion because prior implementations are memory bound in our setting. Simulations show that our estimator has promising finite sample properties, and that naive approaches fail to recover latent co-factor structure. We leverage our estimator to investigate 255,780 papers published in statistics journals from 1898 to 2024, resulting in the most comprehensive topic model of the statistics literature to date. We find interpretable co-factors corresponding to many statistical subfields, including time series, variable selection, spatial methods, graphical models, GLM(M)s, causal inference, multiple testing, quantile regression, semiparametrics, dimension reduction, and several more. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Alex Hayes and Karl Rohe},
  doi          = {10.1080/10618600.2024.2394464},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {448-461},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Co-factor analysis of citation networks},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSLiNGAM: DirectLiNGAM under heavy tails. <em>JCGS</em>, <em>34</em>(2), 437-447. (<a href='https://doi.org/10.1080/10618600.2024.2394462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the established approaches to causal discovery consists of combining directed acyclic graphs (DAGs) with structural causal models (SCMs) to describe the functional dependencies of effects on their causes. Possible identifiability of SCMs given data depends on assumptions made on the noise variables and the functional classes in the SCM. For instance, in the LiNGAM model, the functional class is restricted to linear functions and the disturbances have to be non-Gaussian. In this work, we propose TSLiNGAM, a new method for identifying the DAG of a causal model based on observational data. TSLiNGAM builds on DirectLiNGAM, a popular algorithm which uses simple OLS regression for identifying causal directions between variables. TSLiNGAM leverages the non-Gaussianity assumption of the error terms in the LiNGAM model to obtain more efficient and robust estimation of the causal structure. TSLiNGAM is justified theoretically and is studied empirically in an extensive simulation study. It performs significantly better on heavy-tailed and skewed data and demonstrates a high small-sample efficiency. In addition, TSLiNGAM also shows better robustness properties as it is more resilient to contamination. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Sarah Leyder and Jakob Raymaekers and Tim Verdonck},
  doi          = {10.1080/10618600.2024.2394462},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {437-447},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {TSLiNGAM: DirectLiNGAM under heavy tails},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational inference based on a subclass of closed skew normals. <em>JCGS</em>, <em>34</em>(2), 422-436. (<a href='https://doi.org/10.1080/10618600.2024.2402278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian distributions are widely used in Bayesian variational inference to approximate intractable posterior densities, but the ability to accommodate skewness can improve approximation accuracy significantly, when data or prior information is scarce. We study the properties of a subclass of closed skew normals constructed using affine transformation of independent standardized univariate skew normals as the variational density, and illustrate how it provides increased flexibility and accuracy in approximating the joint posterior in various applications, by overcoming limitations in existing skew normal variational approximations. The evidence lower bound is optimized using stochastic gradient ascent, where analytic natural gradient updates are derived. We also demonstrate how problems in maximum likelihood estimation of skew normal parameters occur similarly in stochastic variational inference, and can be resolved using the centered parameterization. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Linda S. L. Tan and Aoxiang Chen},
  doi          = {10.1080/10618600.2024.2402278},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {422-436},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Variational inference based on a subclass of closed skew normals},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate moment least-squares variance estimators for reversible markov chains. <em>JCGS</em>, <em>34</em>(2), 409-421. (<a href='https://doi.org/10.1080/10618600.2024.2407458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov chain Monte Carlo (MCMC) is a commonly used method for approximating expectations with respect to probability distributions. Uncertainty assessment for MCMC estimators is essential in practical applications. Moreover, for multivariate functions of a Markov chain, it is important to estimate not only the auto-correlation for each component but also to estimate cross-correlations, in order to better assess sample quality, improve estimates of effective sample size, and use more effective stopping rules. Berg and Song introduced the moment least squares (momentLS) estimator, a shape-constrained estimator for the autocovariance sequence from a reversible Markov chain, for univariate functions of the Markov chain. Based on this sequence estimator, they proposed an estimator of the asymptotic variance of the sample mean from MCMC samples. In this study, we propose novel autocovariance sequence and asymptotic variance estimators for Markov chain functions with multiple components, based on the univariate momentLS estimators from Berg and Song . We demonstrate strong consistency of the proposed auto(cross)-covariance sequence and asymptotic variance matrix estimators. We conduct empirical comparisons of our method with other state-of-the-art approaches on simulated and real-data examples, using popular samplers including the random-walk Metropolis sampler and the No-U-Turn sampler from STAN. Supplemental materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Hyebin Song and Stephen Berg},
  doi          = {10.1080/10618600.2024.2407458},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {409-421},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Multivariate moment least-squares variance estimators for reversible markov chains},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using early rejection markov chain monte carlo and gaussian processes to accelerate ABC methods. <em>JCGS</em>, <em>34</em>(2), 395-408. (<a href='https://doi.org/10.1080/10618600.2024.2379349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate Bayesian computation (ABC) is a class of Bayesian inference algorithms that targets problems with intractable or unavailable likelihood functions. It uses synthetic data drawn from the simulation model to approximate the posterior distribution. However, ABC is computationally intensive for complex models in which simulating synthetic data is very expensive. In this article, we propose an early rejection Markov chain Monte Carlo (ejMCMC) sampler based on Gaussian processes to accelerate inference speed. We early reject samples in the first stage of the kernel using a discrepancy model, in which the discrepancy between the simulated and observed data is modeled by Gaussian process (GP). Hence, synthetic data is generated only if the parameter space is worth exploring. We demonstrate through theory, simulation experiments, and real data analysis that the new algorithm significantly improves inference efficiency compared to existing early-rejection MCMC algorithms. In addition, we employ our proposed method within an ABC sequential Monte Carlo (SMC) sampler. In our numerical experiments, we use examples of ordinary differential equations, stochastic differential equations, and delay differential equations to demonstrate the effectiveness of the proposed algorithm. We develop an R package that is available at https://github.com/caofff/ejMCMC .},
  archive      = {J_JCGS},
  author       = {Xuefei Cao and Shijia Wang and Yongdao Zhou},
  doi          = {10.1080/10618600.2024.2379349},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {4},
  number       = {2},
  pages        = {395-408},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Using early rejection markov chain monte carlo and gaussian processes to accelerate ABC methods},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous-time multivariate analysis. <em>JCGS</em>, <em>34</em>(1), 384-394. (<a href='https://doi.org/10.1080/10618600.2024.2374570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The starting point for much of multivariate analysis (MVA) is an n × p data matrix whose n rows represent observations and whose p columns represent variables. Some multivariate datasets, however, may be best conceptualized not as n discrete p -variate observations, but as p curves or functions defined on a common time interval. Here we introduce a framework for extending techniques of multivariate analysis to such settings. The proposed continuous-time multivariate analysis (CTMVA) framework rests on the assumption that the curves can be represented as linear combinations of basis functions such as B -splines, as in the Ramsay-Silverman representation of functional data; but whereas functional data analysis extends MVA to the case of observations that are curves rather than vectors—heuristically, n × p data with p infinite—we are instead concerned with what happens when n is infinite. We present continuous-time extensions of the classical MVA methods of covariance and correlation estimation, principal component analysis, Fisher’s linear discriminant analysis, and k -means clustering. We show that CTMVA can improve on the performance of classical MVA, in particular for correlation estimation and clustering, and can be applied in some settings where classical MVA cannot, including variables observed at disparate time points. CTMVA is illustrated with a novel perspective on a well-known Canadian weather dataset, and with applications to datasets involving international development, brain signals, and air quality. The proposed methods are implemented in the publicly available R package ctmva . Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Biplab Paul and Philip T. Reiss and Erjia Cui and Noemi Foà},
  doi          = {10.1080/10618600.2024.2374570},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {384-394},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Continuous-time multivariate analysis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Principal variables analysis for non-gaussian data. <em>JCGS</em>, <em>34</em>(1), 374-383. (<a href='https://doi.org/10.1080/10618600.2024.2367098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal variables analysis (PVA) is a technique for selecting a subset of variables that capture as much of the information in a dataset as possible. Existing approaches for PVA are based on the Pearson correlation matrix, which is not well-suited to describing the relationships between non-Gaussian variables. We propose a generalized approach to PVA enabling the use of different types of correlation, and we explore using Spearman, Gaussian copula, and polychoric correlations as alternatives to Pearson correlation. We compare performance in simulation studies varying the form of the true multivariate distribution over a range of possibilities. Our results show that on continuous non-Gaussian data, using generalized PVA with Gaussian copula or Spearman correlations provides a major improvement in performance compared to Pearson. On ordinal data, generalized PVA with polychoric correlations outperforms the rest by a wide margin. We apply generalized PVA to a dataset of 102 clinical variables measured on individuals with X-linked dystonia parkinsonism (XDP), a neurodegenerative disorder involving symptoms of both dystonia and parkinsonism. We find that using different types of correlation yields substantively different sets of principal variables; for example, parkinsonism-related metrics appear more explanatory than dystonia-related metrics on the observed data. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Dylan Clark-Boucher and Jeffrey W. Miller},
  doi          = {10.1080/10618600.2024.2367098},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {374-383},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Principal variables analysis for non-gaussian data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate singular spectrum analysis by robust diagonalwise low-rank approximation. <em>JCGS</em>, <em>34</em>(1), 360-373. (<a href='https://doi.org/10.1080/10618600.2024.2362222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate Singular Spectrum Analysis (MSSA) is a powerful and widely used nonparametric method for multivariate time series, which allows the analysis of complex temporal data from diverse fields such as finance, healthcare, ecology, and engineering. However, MSSA lacks robustness against outliers because it relies on the singular value decomposition, which is very sensitive to the presence of anomalous values. MSSA can then give biased results and lead to erroneous conclusions. In this article a new MSSA method is proposed, named RObust Diagonalwise Estimation of SSA (RODESSA), which is robust against the presence of cellwise and casewise outliers. In particular, the decomposition step of MSSA is replaced by a new robust low-rank approximation of the trajectory matrix that takes its special structure into account. A fast algorithm is constructed, and it is proved that each iteration step decreases the objective function. In order to visualize different types of outliers, a new graphical display is introduced, called an enhanced time series plot. An extensive Monte Carlo simulation study is performed to compare RODESSA with competing approaches in the literature. A real data example about temperature analysis in passenger railway vehicles demonstrates the practical utility of the proposed approach.},
  archive      = {J_JCGS},
  author       = {Fabio Centofanti and Mia Hubert and Biagio Palumbo and Peter J. Rousseeuw},
  doi          = {10.1080/10618600.2024.2362222},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {360-373},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Multivariate singular spectrum analysis by robust diagonalwise low-rank approximation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parsimonious tensor dimension reduction. <em>JCGS</em>, <em>34</em>(1), 343-359. (<a href='https://doi.org/10.1080/10618600.2024.2362220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor data is emerging in many scientific applications, such as multi-tissue transcriptomics. In such cases, the covariates for each individual are no longer a vector. To apply traditional vector-based methods to this type of data, we need to either do the vectorization or analyze data marginally, which suffers a significant information loss. We propose a novel parsimonious tensor dimension reduction (pTDR) approach to directly link the response and tensor covariate through an unknown function g . In pTDR, the response variable, continuous or discrete, depends on K rank-one projections of the covariates, with the projections estimated via a sequential iterative dimension reduction algorithm. We further propose an asymptotic sequential statistical test to select the correct number of rank-one tensors. In contrast to the classic low-rank tensor regression, pTDR model is not restricted to the linear relationship between response and covariates. We apply pTDR to two modern genomic studies. We find that the gene expression of multiple tissues has a stronger association with aging and obesity than was apparent using previous approaches. Numerical results demonstrate the advantages of pTDR over competitors in terms of prediction accuracy and computing efficiency. Our software is publicly available on GitHub ( https://github.com/BioAlgs/pTDR ).},
  archive      = {J_JCGS},
  author       = {Xin Xing and Peng Zeng and Youhui Ye and Wenxuan Zhong},
  doi          = {10.1080/10618600.2024.2362220},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {343-359},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Parsimonious tensor dimension reduction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data nuggets: A method for reducing big data while preserving data structure. <em>JCGS</em>, <em>34</em>(1), 330-342. (<a href='https://doi.org/10.1080/10618600.2024.2341896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data, with N × P dimension where N is extremely large, has created new challenges for data analysis, particularly in the realm of creating meaningful clusters of data. Clustering techniques, such as K-means or hierarchical clustering are popular methods for performing exploratory analysis on large datasets. Unfortunately, these methods are not always possible to apply to big data due to memory or time constraints generated by calculations of order P * N ( N − 1 ) 2 . To circumvent this problem, typically the clustering technique is applied to a random sample drawn from the dataset; however, a weakness is that the structure of the dataset, particularly at the edges, is not necessarily maintained. We propose a new solution through the concept of “data nuggets”, which reduces a large dataset into a small collection of nuggets of data, each containing a center, weight, and scale parameter. The data nuggets are then input into algorithms that compute methods such as principal components analysis and clustering in a more computationally efficient manner. We show the consistency of the data nuggets based covariance estimator and apply the methodology of data nuggets to perform exploratory analysis of a flow cytometry dataset containing over one million observations using PCA and K-means clustering for weighted observations. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Traymon E. Beavers and Ge Cheng and Yajie Duan and Javier Cabrera and Mariusz Lubomirski and Dhammika Amaratunga and Jeffrey E. Teigler},
  doi          = {10.1080/10618600.2024.2341896},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {330-342},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Data nuggets: A method for reducing big data while preserving data structure},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed learning for principal eigenspaces without moment constraints. <em>JCGS</em>, <em>34</em>(1), 318-329. (<a href='https://doi.org/10.1080/10618600.2024.2341889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed Principal Component Analysis (PCA) has been studied to deal with the case when data are stored across multiple machines and communication cost or privacy concerns prohibit the computation of PCA in a central location. However, the sub-Gaussian assumption in the related literature is restrictive in real application where outliers or heavy-tailed data are common in areas such as finance and macroeconomics. In this article, we propose a distributed algorithm for estimating the principal eigenspaces without any moment constraints on the underlying distribution. We study the problem under the elliptical family framework and adopt the sample multivariate Kendall’s tau matrix to extract eigenspace estimators from all submachines, which can be viewed as points in the Grassmann manifold. We then find the “center” of these points as the final distributed estimator of the principal eigenspace. We investigate the bias and variance for the distributed estimator and derive its convergence rate which depends on the effective rank, eigengap of the scatter matrix and the number of submachines. We show that the distributed estimator performs as if we have full access to the whole data. Simulation studies show that the distributed algorithm performs comparably with the existing one for light-tailed data, while showing great advantages for heavy-tailed data. We also extend the distributed algorithm to cases with limited communication constraints and with elliptical factor structure. Thorough simulation studies and a real application to a macroeconomic dataset verify the advantages of the proposed distributed algorithms. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yong He and Zichen Liu and Yalin Wang},
  doi          = {10.1080/10618600.2024.2341889},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {318-329},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Distributed learning for principal eigenspaces without moment constraints},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bootstrapped edge count tests for nonparametric two-sample inference under heterogeneity. <em>JCGS</em>, <em>34</em>(1), 306-317. (<a href='https://doi.org/10.1080/10618600.2024.2374583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric two-sample testing is a classical problem in inferential statistics. While modern two-sample tests, such as the edge count test and its variants, can handle multivariate and non-Euclidean data, contemporary gargantuan datasets often exhibit heterogeneity due to the presence of latent subpopulations. Direct application of these tests, without regulating for such heterogeneity, may lead to incorrect statistical decisions. We develop a new nonparametric testing procedure that accurately detects differences between the two samples in the presence of unknown heterogeneity in the data generation process. Our framework handles this latent heterogeneity through a composite null that entertains the possibility that the two samples arise from a mixture distribution with identical component distributions but with possibly different mixing weights. In this regime, we study the asymptotic behavior of weighted edge count test statistic and show that it can be effectively recalibrated to detect arbitrary deviations from the composite null. For practical implementation we propose a Bootstrapped Weighted Edge Count test which involves a bootstrap-based calibration procedure that can be easily implemented across a wide range of heterogeneous regimes. A comprehensive simulation study and an application to detecting aberrant user behaviors in online games demonstrates the excellent non-asymptotic performance of the proposed test. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Trambak Banerjee and Bhaswar B. Bhattacharya and Gourab Mukherjee},
  doi          = {10.1080/10618600.2024.2374583},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {306-317},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Bootstrapped edge count tests for nonparametric two-sample inference under heterogeneity},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distribution-free method for change point detection in non-sparse high dimensional data. <em>JCGS</em>, <em>34</em>(1), 290-305. (<a href='https://doi.org/10.1080/10618600.2024.2365733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a distribution-free distance-based method for high dimensional change points that can address challenging situations when the sample size is very small compared to the dimension as in the so-called HDLSS data or when non-sparse changes may occur due to change in many variables but with small significant magnitudes. Our method can detect changes in mean or variance of high dimensional observations as well as other distributional changes. We present efficient algorithms that can detect single and multiple high dimensional change points. We use nonparametric metrics, including a new dissimilarity measure and some new distance and difference distance matrices, to develop a procedure to estimate change point locations. We also introduce a nonparametric test to determine the significance of estimated change points. We provide theoretical guaranties for our method and demonstrate its empirical performance in comparison with some of the recent methods for high dimensional change points. An R package called HDDchangepoint is developed to implement the proposed method.},
  archive      = {J_JCGS},
  author       = {Reza Drikvandi and Reza Modarres},
  doi          = {10.1080/10618600.2024.2365733},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {290-305},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {A distribution-free method for change point detection in non-sparse high dimensional data},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel angle dependence measures in metric spaces. <em>JCGS</em>, <em>34</em>(1), 280-289. (<a href='https://doi.org/10.1080/10618600.2024.2357620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring and testing dependence between data in separable metric spaces is of great importance in modern statistics. Most existing work relied on the distance between random variables, which inevitably required the moment conditions to guarantee the distance is well-defined. Based on the geometry element “angle”, we develop a novel class of nonlinear dependence measures for data in metric space that can avoid such conditions. Specifically, by making use of the reproducing kernel Hilbert space equipped with Gaussian measure, we introduce kernel angle covariances that can be applied to various types of data, including low dimensional vector, high dimensional vectors, non-Euclidean data like symmetric positive definite matrices, and compositional data. We estimate kernel angle covariances based on U -statistic and establish the corresponding independence tests via gamma approximation. Our kernel angle independence tests, imposing no-moment conditions on kernels, are robust with heavy-tailed random variables. We conduct comprehensive simulation studies and apply our proposed methods to a facial recognition task. Our kernel angle covariances-based tests show remarkable performances in dealing with image data. All the codes and proofs are included in the supplementary materials.},
  archive      = {J_JCGS},
  author       = {Yilin Zhang and Songshan Yang},
  doi          = {10.1080/10618600.2024.2357620},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {280-289},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Kernel angle dependence measures in metric spaces},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Universal inference meets random projections: A scalable test for log-concavity. <em>JCGS</em>, <em>34</em>(1), 267-279. (<a href='https://doi.org/10.1080/10618600.2024.2347338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape constraints yield flexible middle grounds between fully nonparametric and fully parametric approaches to modeling distributions of data. The specific assumption of log-concavity is motivated by applications across economics, survival modeling, and reliability theory. However, there do not currently exist valid tests for whether the underlying density of given data is log-concave. The recent universal inference methodology provides a valid test. The universal test relies on maximum likelihood estimation (MLE), and efficient methods already exist for finding the log-concave MLE. This yields the first test of log-concavity that is provably valid in finite samples in any dimension, for which we also establish asymptotic consistency results. Empirically, we find that a random projections approach that converts the d -dimensional testing problem into many one-dimensional problems can yield high power, leading to a simple procedure that is statistically and computationally efficient.},
  archive      = {J_JCGS},
  author       = {Robin Dunn and Aditya Gangrade and Larry Wasserman and Aaditya Ramdas},
  doi          = {10.1080/10618600.2024.2347338},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {267-279},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Universal inference meets random projections: A scalable test for log-concavity},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the wasserstein median of probability measures. <em>JCGS</em>, <em>34</em>(1), 253-266. (<a href='https://doi.org/10.1080/10618600.2024.2374580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary choice to summarize a finite collection of random objects is by using measures of central tendency, such as mean and median. In the field of optimal transport, the Wasserstein barycenter corresponds to the Fréchet or geometric mean of a set of probability measures, which is defined as a minimizer of the sum of squared distances to each element in a given set with respect to the Wasserstein distance of order 2. We introduce the Wasserstein median as a robust alternative to the Wasserstein barycenter. The Wasserstein median corresponds to the Fréchet median under the 2-Wasserstein metric. The existence and consistency of the Wasserstein median are first established, along with its robustness property. In addition, we present a general computational pipeline that employs any recognized algorithms for the Wasserstein barycenter in an iterative fashion and demonstrate its convergence. The utility of the Wasserstein median as a robust measure of central tendency is demonstrated using real and simulated data.},
  archive      = {J_JCGS},
  author       = {Kisung You and Dennis Shung and Mauro Giuffrè},
  doi          = {10.1080/10618600.2024.2374580},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {253-266},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {On the wasserstein median of probability measures},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Versatile descent algorithms for group regularization and variable selection in generalized linear models. <em>JCGS</em>, <em>34</em>(1), 239-252. (<a href='https://doi.org/10.1080/10618600.2024.2362232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an adaptively bounded gradient descent (ABGD) algorithm for group elastic net penalized regression. Unlike previously proposed algorithms, the proposed algorithm adaptively bounds the Fisher information matrix, which results in a flexible and stable computational framework. In particular, the proposed algorithm (i) does not require orthogonalization of the predictors, and (ii) can be easily applied to any combination of exponential family response distribution and link function. The proposed algorithm is implemented in the grpnet R package (available from CRAN), which implements the approach for common response distributions (Gaussian, binomial, and Poisson), as well as several response distributions not previously considered in the group penalization literature (i.e., multinomial, negative binomial, gamma, and inverse Gaussian). Simulated and real data examples demonstrate that the proposed algorithm is as or more efficient than existing methods for Gaussian, binomial, and Poisson distributions. Furthermore, using two genomic examples, I demonstrate how the proposed algorithm can be applied to high-dimensional multinomial regression problems with grouped predictors. R code to reproduce the results is included as supplementary materials.},
  archive      = {J_JCGS},
  author       = {Nathaniel E. Helwig},
  doi          = {10.1080/10618600.2024.2362232},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {239-252},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Versatile descent algorithms for group regularization and variable selection in generalized linear models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variance-reduced stochastic optimization for efficient inference of hidden markov models. <em>JCGS</em>, <em>34</em>(1), 222-238. (<a href='https://doi.org/10.1080/10618600.2024.2350476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov models (HMMs) are popular models to identify a finite number of latent states from sequential data. However, fitting them to large datasets can be computationally demanding because most likelihood maximization techniques require iterating through the entire underlying dataset for every parameter update. We propose a novel optimization algorithm that updates the parameters of an HMM without iterating through the entire dataset. Namely, we combine a partial E step with variance-reduced stochastic optimization within the M step. We prove the algorithm converges under certain regularity conditions. We test our algorithm empirically using a simulation study as well as a case study of kinematic data collected using suction-cup attached biologgers from eight northern resident killer whales ( Orcinus orca ) off the western coast of Canada. In both, our algorithm converges in fewer epochs, with less computation time, and to regions of higher likelihood compared to standard numerical optimization techniques. Our algorithm allows practitioners to fit complicated HMMs to large time-series datasets more efficiently than existing baselines. Supplemental materials are available online.},
  archive      = {J_JCGS},
  author       = {Evan Sidrow and Nancy Heckman and Alexandre Bouchard-Côté and Sarah M. E. Fortune and Andrew W. Trites and Marie Auger-Méthé},
  doi          = {10.1080/10618600.2024.2350476},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {222-238},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Variance-reduced stochastic optimization for efficient inference of hidden markov models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relative entropy gradient sampler for unnormalized distribution. <em>JCGS</em>, <em>34</em>(1), 211-221. (<a href='https://doi.org/10.1080/10618600.2024.2340523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a relative entropy gradient sampler (REGS) for sampling from unnormalized distributions. REGS is a particle method that seeks a sequence of simple nonlinear transforms iteratively pushing the initial samples from a reference distribution into the samples from an unnormalized target distribution. To determine the nonlinear transforms at each iteration, we consider the Wasserstein gradient flow of relative entropy. This gradient flow determines a path of probability distributions that interpolates the reference distribution and the target distribution. It is characterized by an ordinary differential equation (ODE) system with velocity fields depending on the density ratios of the density of evolving particles and the unnormalized target density. To sample with REGS, we need to estimate the density ratios and simulate the ODE system with particle evolution. We propose a novel nonparametric approach to estimating the logarithmic density ratio using neural networks. Extensive simulation studies on challenging multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets demonstrate that REGS has reasonable performance compared with popular samplers based on Wasserstein gradient flows. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Xingdong Feng and Yuan Gao and Jian Huang and Yuling Jiao and Xu Liu},
  doi          = {10.1080/10618600.2024.2340523},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {211-221},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Relative entropy gradient sampler for unnormalized distribution},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian l1/2 regression. <em>JCGS</em>, <em>34</em>(1), 199-210. (<a href='https://doi.org/10.1080/10618600.2024.2374579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that Bridge regression enjoys superior theoretical properties when compared to traditional LASSO. However, the current latent variable representation of its Bayesian counterpart, based on the exponential power prior, is computationally expensive in higher dimensions. In this article, we show that the exponential power prior has a closed form scale mixture of normal decomposition for α = ( 1 2 ) γ , γ ∈ { 1 , 2 , … } . We call these types of priors L 1 2 prior for short. We develop an efficient partially collapsed Gibbs sampling scheme for computation using the L 1 2 prior and study theoretical properties when p > n . In addition, we introduce a non-separable Bridge penalty function inspired by the fully Bayesian formulation and a novel, efficient coordinate descent algorithm. We prove the algorithm’s convergence and show that the local minimizer from our optimization algorithm has an oracle property. Finally, simulation studies were carried out to illustrate the performance of the new algorithms. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Xiongwen Ke and Yanan Fan},
  doi          = {10.1080/10618600.2024.2374579},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {199-210},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Bayesian l1/2 regression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-censored linear quantile regression. <em>JCGS</em>, <em>34</em>(1), 187-198. (<a href='https://doi.org/10.1080/10618600.2024.2365740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Censored quantile regression has emerged as a prominent alternative to classical Cox’s proportional hazards model or accelerated failure time model in both theoretical and applied statistics. While quantile regression has been extensively studied for right-censored survival data, methodologies for analyzing interval-censored data remain limited in the survival analysis literature. This article introduces a novel local weighting approach for estimating linear censored quantile regression, specifically tailored to handle diverse forms of interval-censored survival data. The estimation equation and the corresponding convex objective function for the regression parameter can be constructed as a weighted average of quantile loss contributions at two interval endpoints. The weighting components are nonparametrically estimated using local kernel smoothing or ensemble machine learning techniques. To estimate the nonparametric distribution mass for interval-censored data, a modified EM algorithm for nonparametric maximum likelihood estimation is employed by introducing subject-specific latent Poisson variables. The proposed method’s empirical performance is demonstrated through extensive simulation studies and real data analyses of two HIV/AIDS datasets. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Taehwa Choi and Seohyeon Park and Hunyong Cho and Sangbum Choi},
  doi          = {10.1080/10618600.2024.2365740},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {187-198},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Interval-censored linear quantile regression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class-distributed learning for multinomial logistic regression with high dimensional features and a large number of classes. <em>JCGS</em>, <em>34</em>(1), 175-186. (<a href='https://doi.org/10.1080/10618600.2024.2362230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating a high-dimensional multinomial logistic regression model with a larger number of categories is of fundamental importance but it presents two challenges. Computationally, it leads to heavy computation cost. Statistically, it suffers unsatisfactory statistical efficiency. Therefore, how to solve this problem in a computationally and statistically efficient way is of great interest. To tackle these challenges, we have developed a new class-distributed learning algorithm with a rank-reducible coefficient structure. The key innovation here is piecing together two important techniques for distributed computing and improved statistical efficiency. The two techniques are, respectively, dimension reduction and a circular-structured working model. Dimension reduction effectively alleviates the curse of dimensionality due to high dimensional features. A circular-structured working model allows the use of a class-distributed algorithm for distributed computing. To support our new methodology, we develop rigorous asymptotic theory and present extensive numerical experiments. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Shuyuan Wu and Jing Zhou and Ke Xu and Hansheng Wang},
  doi          = {10.1080/10618600.2024.2362230},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {175-186},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Class-distributed learning for multinomial logistic regression with high dimensional features and a large number of classes},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterated data sharpening. <em>JCGS</em>, <em>34</em>(1), 162-174. (<a href='https://doi.org/10.1080/10618600.2024.2362219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sharpening in kernel regression has been shown to be an effective method of reducing bias while having minimal effects on variance. Earlier efforts to iterate the data sharpening procedure have been less effective, due to the employment of an inappropriate sharpening transformation. In this article, an iterated data sharpening algorithm is proposed which reduces the asymptotic bias at each iteration, while having modest effects on the variance. The efficacy of the iterative approach is demonstrated theoretically and via a simulation study. Boundary effects persist and the affected region successively grows when the iteration is applied to local constant regression. By contrast, boundary bias successively decreases for each iteration step when applied to local linear regression. This study also shows that after iteration, the resulting estimates are less sensitive to bandwidth choice, and a further simulation study demonstrates that iterated data sharpening with data-driven bandwidth selection via cross-validation can lead to more accurate regression function estimation. Examples with real data are used to illustrate the scope of change made possible by using iterated data sharpening and to also identify its limitations. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Hanxiao Chen and W. John Braun and Xiaoping Shi},
  doi          = {10.1080/10618600.2024.2362219},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {162-174},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Iterated data sharpening},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple-use calibration for all future values and exact two-sided simultaneous tolerance intervals in linear regression. <em>JCGS</em>, <em>34</em>(1), 155-161. (<a href='https://doi.org/10.1080/10618600.2024.2359507'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-use calibration using regression is an important statistical tool. Confidence sets for the x -values associated with all future y -values should guarantee a key property , which can be satisfied by simultaneous tolerance intervals (STI’s), and so multiple-use calibration requires the construction of STI’s. In this article, exact two-sided STI’s have been constructed for polynomial regression over any given covariate interval. There is a misconception that two-sided pointwise tolerance intervals (PTI’s) can be employed for multiple-use calibration. This article shows that the confidence sets based on the two-sided PTI’s do not satisfy the key property and so should not be used. Real-world data examples are given in this article for illustration. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Yang Han and Lingjiao Wang and Wei Liu and Frank Bretz},
  doi          = {10.1080/10618600.2024.2359507},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {155-161},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Multiple-use calibration for all future values and exact two-sided simultaneous tolerance intervals in linear regression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic block smooth graphon model. <em>JCGS</em>, <em>34</em>(1), 140-154. (<a href='https://doi.org/10.1080/10618600.2024.2374571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose combining the stochastic blockmodel and the smooth graphon model, two of the most prominent modeling approaches in statistical network analysis. Stochastic blockmodels are generally used for clustering networks into groups of actors with homogeneous connectivity behavior. Smooth graphon models, on the other hand, assume that all nodes can be arranged on a one-dimensional scale such that closeness implies a similar behavior in connectivity. Both frameworks belong to the class of node-specific latent variable models, entailing a natural relationship. While these two modeling concepts have developed independently, this article proposes their generalization toward stochastic block smooth graphon models . This combined approach enables to exploit the advantages of both worlds. Employing concepts of the EM-type algorithm allows to develop an appropriate estimation routine, where MCMC techniques are used to accomplish the E-step. Simulations and real-world applications support the practicability of our novel method and demonstrate its advantages. The article is accompanied by supplementary material covering details about computation and implementation.},
  archive      = {J_JCGS},
  author       = {Benjamin Sischka and Göran Kauermann},
  doi          = {10.1080/10618600.2024.2374571},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {140-154},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Stochastic block smooth graphon model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating independent replicates directly from the posterior distribution for a class of spatial hierarchical models. <em>JCGS</em>, <em>34</em>(1), 123-139. (<a href='https://doi.org/10.1080/10618600.2024.2365728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov chain Monte Carlo (MCMC) allows one to generate dependent replicates from a posterior distribution for effectively any Bayesian hierarchical model. However, MCMC can produce a significant computational burden. This motivates us to consider finding expressions of the posterior distribution that are computationally straightforward to obtain independent replicates from directly. We focus on a broad class of Bayesian hierarchical models for spatially dependent data, which are often modeled via a latent Gaussian process (LGP). First, we derive a new class of distributions referred to as the generalized conjugate multivariate (GCM) distribution. The GCM distribution’s theoretical development follows that of the conjugate multivariate (CM) distribution with two main differences: the GCM allows for latent Gaussian process assumptions, and the GCM explicitly accounts for hyperparameters through marginalization. The development of GCM is needed to obtain independent replicates directly from the exact posterior distribution, which has an efficient regression form. Hence, we refer to our method as Exact Posterior Regression (EPR). Simulation studies with weakly stationary spatial processes and spatial basis function expansions are provided. We provide an analysis of poverty incidence from the U.S. Census Bureau, and an analysis of high-dimensional remote sensing data. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Jonathan R. Bradley and Madelyn Clinch},
  doi          = {10.1080/10618600.2024.2365728},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {123-139},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Generating independent replicates directly from the posterior distribution for a class of spatial hierarchical models},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A projection approach to local regression with variable-dimension covariates. <em>JCGS</em>, <em>34</em>(1), 109-122. (<a href='https://doi.org/10.1080/10618600.2024.2357636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete covariate vectors are known to be problematic for estimation and inferences on model parameters, but their impact on prediction performance is less understood. We develop an imputation-free method that builds on a random partition model admitting variable-dimension covariates. Cluster-specific response models further incorporate covariates via linear predictors, facilitating estimation of smooth prediction surfaces with relatively few clusters. We exploit marginalization techniques of Gaussian kernels to analytically project response distributions according to any pattern of missing covariates, yielding a local regression with internally consistent uncertainty propagation that uses only one set of coefficients per cluster. Aggressive shrinkage of these coefficients regulates uncertainty due to missing covariates. The method allows in- and out-of-sample prediction for any missingness pattern, even if the pattern in a new subject’s incomplete covariate vector was not seen in the training data. We develop an MCMC algorithm for posterior sampling that improves a computationally expensive update for latent cluster allocation. Finally, we demonstrate the model’s effectiveness for nonlinear point and density prediction under various circumstances by comparing with other recent methods for regression of variable dimensions on synthetic and real data. Supplemental materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Matthew J. Heiner and Garritt L. Page and Fernando Andrés Quintana},
  doi          = {10.1080/10618600.2024.2357636},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {109-122},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {A projection approach to local regression with variable-dimension covariates},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast variational inference for bayesian factor analysis in single and multi-study settings. <em>JCGS</em>, <em>34</em>(1), 96-108. (<a href='https://doi.org/10.1080/10618600.2024.2356173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factors models are commonly used to analyze high-dimensional data in both single-study and multi-study settings. Bayesian inference for such models relies on Markov chain Monte Carlo (MCMC) methods, which scale poorly as the number of studies, observations, or measured variables increase. To address this issue, we propose new variational inference algorithms to approximate the posterior distribution of Bayesian latent factor models using the multiplicative gamma process shrinkage prior. The proposed algorithms provide fast approximate inference at a fraction of the time and memory of MCMC-based implementations while maintaining comparable accuracy in characterizing the data covariance matrix. We conduct extensive simulations to evaluate our proposed algorithms and show their utility in estimating the model for high-dimensional multi-study gene expression data in ovarian cancers. Overall, our proposed approaches enable more efficient and scalable inference for factor models, facilitating their use in high-dimensional settings. An R package VIMSFA implementing our methods is available on GitHub ( github.com/blhansen/VI-MSFA ). Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Blake Hansen and Alejandra Avalos-Pacheco and Massimiliano Russo and Roberta De Vito},
  doi          = {10.1080/10618600.2024.2356173},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {96-108},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Fast variational inference for bayesian factor analysis in single and multi-study settings},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Loss-based variational bayes prediction. <em>JCGS</em>, <em>34</em>(1), 84-95. (<a href='https://doi.org/10.1080/10618600.2024.2341899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach to Bayesian prediction that caters for models with a large number of parameters and is robust to model misspecification. Given a class of high-dimensional (but parametric) predictive models, this new approach constructs a posterior predictive using a variational approximation to a generalized posterior that is directly focused on predictive accuracy. The theoretical behavior of the new prediction approach is analyzed and a form of optimality demonstrated. Applications to both simulated and empirical data using high-dimensional Bayesian neural network and autoregressive mixture models demonstrate that the approach provides more accurate results than various alternatives, including misspecified likelihood-based predictions. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {David T. Frazier and Rubén Loaiza-Maya and Gael M. Martin and Bonsoo Koo},
  doi          = {10.1080/10618600.2024.2341899},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {84-95},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Loss-based variational bayes prediction},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional time series analysis and visualization based on records. <em>JCGS</em>, <em>34</em>(1), 72-83. (<a href='https://doi.org/10.1080/10618600.2024.2374578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many phenomena, data are collected on a large scale and at different frequencies. In this context, functional data analysis (FDA) has become an important statistical methodology for analyzing and modeling such data. The approach of FDA is to assume that data are continuous functions and that each continuous function is considered as a single observation. Thus, FDA deals with large-scale and complex data. However, visualization and exploratory data analysis, which are very important in practice, can be challenging due to the complexity of the continuous functions. Here we introduce a type of record concept for functional data, and we propose some nonparametric tools based on the record concept for functional data observed over time (functional time series). We study the properties of the trajectory of the number of record curves under different scenarios. Also, we propose a unit root test based on the number of records. The trajectory of the number of records over time and the unit root test can be used for visualization and exploratory data analysis. We illustrate the advantages of our proposal through a Monte Carlo simulation study. We also illustrate our method on two different datasets: Daily wind speed curves at Yanbu, Saudi Arabia and annual mortality rates in France. Overall, we can identify the type of functional time series being studied based on the number of record curves observed. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Israel Martínez-Hernández and Marc G. Genton},
  doi          = {10.1080/10618600.2024.2374578},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {72-83},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Functional time series analysis and visualization based on records},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic prediction using landmark historical functional cox regression. <em>JCGS</em>, <em>34</em>(1), 59-71. (<a href='https://doi.org/10.1080/10618600.2024.2374576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic prediction of survival data in the presence of time-varying covariates is an area of active research. Two common analytic approaches for this type of data are joint modeling of the longitudinal and survival processes and landmarking. However, there has been little work dedicated to densely measured time-varying covariates using either approach. Moreover, the software for joint modeling is slow, especially for large datasets, and rather limited for landmarking. We propose a landmark approach for dynamic prediction of survival outcomes using densely measured longitudinal predictors, which treats the past of the time-varying covariate at each landmark point as a functional predictor. This approach is orders of magnitude faster than existing software for simpler joint models. Our extensive comparative simulation study required 8.4 computation-years, over 99% of which was devoted to fitting and predicting from two joint models. Our landmark approach performs similarly to joint modeling when the joint model is correctly specified and substantially out-performs it when it is not. Methods are motivated by an application predicting time to recovery of Multiple Sclerosis lesions in a large neuroimaging dataset. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Andrew Leroux and Ciprian Crainiceanu},
  doi          = {10.1080/10618600.2024.2374576},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {59-71},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Dynamic prediction using landmark historical functional cox regression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distance-based clustering of functional data with derivative principal component analysis. <em>JCGS</em>, <em>34</em>(1), 47-58. (<a href='https://doi.org/10.1080/10618600.2024.2366499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis (FDA) is an important modern paradigm for handling infinite-dimensional data. An important task in FDA is clustering, which identifies subgroups based on the shapes of measured curves. Considering that derivatives can provide additional useful information about the shapes of functionals, we propose a novel L 2 distance between two random functions by incorporating the functions and their derivative information to determine the dissimilarity of curves under a unified scheme for dense observations. The Karhunen–Loève expansion is used to approximate the curves and their derivatives. Cluster membership prediction for each curve intends to minimize the new distances between the observed and predicted curves through subspace projection among all possible clusters. We provide consistent estimators for the curves, curve derivatives, and the proposed distance. Identifiability issues of the clustering procedure are also discussed. The utility of the proposed method is illustrated via simulation studies and applications to two real datasets. The proposed method can considerably improve cluster performance compared with existing functional clustering methods. Supplementary materials for the article are available online.},
  archive      = {J_JCGS},
  author       = {Ping Yu and Gongming Shi and Chunjie Wang and Xinyuan Song},
  doi          = {10.1080/10618600.2024.2366499},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {47-58},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Distance-based clustering of functional data with derivative principal component analysis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ultra-efficient MCMC for bayesian longitudinal functional data analysis. <em>JCGS</em>, <em>34</em>(1), 34-46. (<a href='https://doi.org/10.1080/10618600.2024.2362227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional mixed models are widely useful for regression analysis with dependent functional data, including longitudinal functional data with scalar predictors. However, existing algorithms for Bayesian inference with these models only provide either scalable computing or accurate approximations to the posterior distribution, but not both. We introduce a new MCMC sampling strategy for highly efficient and fully Bayesian regression with longitudinal functional data. Using a novel blocking structure paired with an orthogonalized basis reparameterization, our algorithm jointly samples the fixed effects regression functions together with all subject- and replicate-specific random effects functions. Crucially, the joint sampler optimizes sampling efficiency for these key parameters while preserving computational scalability. Perhaps surprisingly, our new MCMC sampling algorithm even surpasses state-of-the-art algorithms for frequentist estimation and variational Bayes approximations for functional mixed models—while also providing accurate posterior uncertainty quantification—and is orders of magnitude faster than existing Gibbs samplers. Simulation studies show improved point estimation and interval coverage in nearly all simulation settings over competing approaches. We apply our method to a large physical activity dataset to study how various demographic and health factors associate with intraday activity. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Thomas Y. Sun and Daniel R. Kowal},
  doi          = {10.1080/10618600.2024.2362227},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {34-46},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Ultra-efficient MCMC for bayesian longitudinal functional data analysis},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SmashGP: Large-scale spatial modeling via matrix-free gaussian processes. <em>JCGS</em>, <em>34</em>(1), 15-33. (<a href='https://doi.org/10.1080/10618600.2024.2353653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes are essential for spatial data analysis. Not only do they allow the prediction of unknown values, but they also allow for uncertainty quantification. However, in the era of big data, directly using Gaussian processes has become computationally infeasible as cubic run times are required for dense matrix decomposition and inversion. Various alternatives have been proposed to reduce the computational burden of directly fitting Gaussian processes. These alternatives rely on assumptions on the underlying structure of the covariance or precision matrices, such as sparsity or low-rank. In contrast, this article uses hierarchical matrices and matrix-free methods to enable the computation of Gaussian processes for large spatial datasets by exploiting the underlying kernel properties. The proposed framework, smashGP, represents the covariance matrix as an H 2 matrix in O ( n ) time and is able to estimate the unknown parameters of the model and predict the values of spatial observations at unobserved locations in O ( n log n ) time thanks to fast matrix-vector products. Additionally, it can be parallelized to take full advantage of shared-memory computing environments. With simulations and case studies, we illustrate the advantage of smashGP to model large-scale spatial datasets. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {Lucas Erlandson and Ana María Estrada Gómez and Edmond Chow and Kamran Paynabar},
  doi          = {10.1080/10618600.2024.2353653},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {15-33},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {SmashGP: Large-scale spatial modeling via matrix-free gaussian processes},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast calculation of gaussian process multiple-fold cross-validation residuals and their covariances. <em>JCGS</em>, <em>34</em>(1), 1-14. (<a href='https://doi.org/10.1080/10618600.2024.2353633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We generalize fast Gaussian process leave-one-out formulas to multiple-fold cross-validation, highlighting in turn the covariance structure of cross-validation residuals in simple and universal kriging frameworks. We illustrate how resulting covariances affect model diagnostics. We further establish in the case of noiseless observations that correcting for covariances between residuals in cross-validation-based estimation of the scale parameter leads back to maximum likelihood estimation. Also, we highlight in broader settings how differences between pseudo-likelihood and likelihood methods boil down to accounting or not for residual covariances. The proposed fast calculation of cross-validation residuals is implemented and benchmarked against a naive implementation, all in R. Numerical experiments highlight the substantial speed-ups that our approach enables. However, as supported by a discussion on main drivers of computational costs and by a numerical benchmark, speed-ups steeply decline as the number of folds (say, all sharing the same size) decreases. An application to a contaminant localization test case illustrates that the way of grouping observations in folds may affect model assessment and parameter fitting compared to leave-one-out. Overall, our results enable fast multiple-fold cross-validation, have consequences in model diagnostics, and pave the way to future work on hyperparameter fitting as well as on goal-oriented fold design. Supplementary materials for this article are available online.},
  archive      = {J_JCGS},
  author       = {David Ginsbourger and Cédric Schärer},
  doi          = {10.1080/10618600.2024.2353633},
  journal      = {Journal of Computational and Graphical Statistics},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Comput. Graph. Stat.},
  title        = {Fast calculation of gaussian process multiple-fold cross-validation residuals and their covariances},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
