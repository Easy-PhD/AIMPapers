<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JBES</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jbes">JBES - 56</h2>
<ul>
<li><details>
<summary>
(2025). Another look at dependence: The most predictable aspects of time series. <em>JBES</em>, <em>43</em>(3), 723-736. (<a href='https://doi.org/10.1080/07350015.2024.2424345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serial dependence and predictability are two sides of the same coin. The literature has considered alternative measures of these two fundamental concepts. In this article, we aim to distill the most predictable aspect of a univariate time series, that is, the one for which predictability is optimized. Our target measure is the mutual information between the past and future of a random process, a broad measure of predictability that takes into account all future forecast horizons, rather than focusing on the one-step-ahead prediction error mean square error. The most predictable aspect is defined as the measurable transformation of the series that maximizes the mutual information between past and future. This transformation arises from the linear combination of a set of basis functions localized at the quantiles of the unconditional distribution of the process. The mutual information is estimated as a function of the sample partial autocorrelations, using a semiparametric method that estimates an infinite sum by a regularized finite sum. The second most predictable aspect can also be defined, subject to suitable orthogonality restrictions. Finally, we illustrate the use of the most predictable aspect for testing the null hypothesis of no predictability and for point and interval prediction of the original time series.},
  archive      = {J_JBES},
  author       = {Tommaso Proietti},
  doi          = {10.1080/07350015.2024.2424345},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {723-736},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Another look at dependence: The most predictable aspects of time series},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calibrated equilibrium estimation and double selection for high-dimensional partially linear measurement error models. <em>JBES</em>, <em>43</em>(3), 710-722. (<a href='https://doi.org/10.1080/07350015.2024.2422982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, measurement error data is frequently encountered and needs to be handled appropriately. As a result of additional bias induced by measurement error, many existing estimation methods fail to achieve satisfactory performances. This article studies high-dimensional partially linear measurement error models. It proposes a calibrated equilibrium (CARE) estimation method to calibrate the bias caused by measurement error and overcomes the technical difficulty of the objective function unbounded from below in high-dimensional cases due to non-convexity. To facilitate the applications of the CARE estimation method, a bootstrap approach for approximating the covariance matrix of measurement errors is introduced. For the high-dimensional or ultra-high dimensional partially linear measurement error models, a novel multiple testing method, the calibrated equilibrium multiple double selection (CARE–MUSE) algorithm, is proposed to control the false discovery rate (FDR). Under certain regularity conditions, we derive the oracle inequalities for estimation error and prediction risk, along with an upper bound on the number of falsely discovered signs for the CARE estimator. We further establish the convergence rate of the nonparametric function estimator. In addition, FDR and power guarantee for the CARE–MUSE algorithm are investigated under a weaker minimum signal condition, which is insufficient for the CARE estimator to achieve sign consistency. Extensive simulation studies and a real data application demonstrate the satisfactory finite sample performance of the proposed methods.},
  archive      = {J_JBES},
  author       = {Jingxuan Luo and Gaorong Li and Heng Peng and Lili Yue},
  doi          = {10.1080/07350015.2024.2422982},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {710-722},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Calibrated equilibrium estimation and double selection for high-dimensional partially linear measurement error models},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive kernel-based structural change test for copulas. <em>JBES</em>, <em>43</em>(3), 696-709. (<a href='https://doi.org/10.1080/07350015.2024.2422980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a structural change test for copula models based on the kernel smoothing method. The proposed approach enables adaptable estimation of the dynamic marginal distributions, either parametrically or semi-parametrically. The test statistic is formulated via the weighted quadratic distance between the local smoothing copula and the empirical copula function, using pseudo-observations of marginal distributions. The test statistic is pivotal with an asymptotic standard Normal distribution, irrespective of the marginal distributions, parameters, and estimations, and is consistent against a wide range of smoothly transitioning structural changes as well as abrupt structural breaks for copula models. Monte Carlo simulations show that the test performs well in finite samples and outperforms existing tests in the case of periodic changes.},
  archive      = {J_JBES},
  author       = {Xiaohui Lu and Yahong Zhou},
  doi          = {10.1080/07350015.2024.2422980},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {696-709},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {An adaptive kernel-based structural change test for copulas},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining instrumental variable estimators for a panel data model with factors. <em>JBES</em>, <em>43</em>(3), 684-695. (<a href='https://doi.org/10.1080/07350015.2024.2421991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the estimation of factor-augmented panel data models using observed measurements to proxy for unobserved factors or loadings and explore the use of internal instruments to address the resulting endogeneity. The main challenge consists in that economic theory rarely provides insights into which measurements to choose as proxies when several are available. To overcome this problem, we propose a new class of estimators that are linear combinations of instrumental variable estimators and establish large sample results. We also show that an optimal weighting scheme exists, leading to efficiency gains relative to an instrumental variable estimator. Simulations show that the proposed approach performs better than existing methods. We illustrate the new method using data on test scores across U.S. school districts.},
  archive      = {J_JBES},
  author       = {Matthew Harding and Carlos Lamarche and Chris Muris},
  doi          = {10.1080/07350015.2024.2421991},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {684-695},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Combining instrumental variable estimators for a panel data model with factors},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural phillips curve and a deep output gap. <em>JBES</em>, <em>43</em>(3), 669-683. (<a href='https://doi.org/10.1080/07350015.2024.2421279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many problems plague empirical Phillips curves (PCs). Among them is the hurdle that the two key components, inflation expectations and the output gap, are both unobserved. Traditional remedies include proxying for the absentees or extracting them via assumptions-heavy filtering procedures. I propose an alternative route: a Hemisphere Neural Network (HNN) whose architecture yields a final layer where components can be interpreted as latent states within a Neural PC. First, HNN conducts the supervised estimation of nonlinearities that arise when translating a high-dimensional set of observed regressors into latent states. Second, forecasts are economically interpretable. Among other findings, the contribution of real activity to inflation appears understated in traditional PCs. In contrast, HNN captures the 2021 upswing in inflation and attributes it to a large positive output gap starting from late 2020. The unique path of HNN’s gap comes from dispensing with unemployment and GDP in favor of an amalgam of nonlinearly processed alternative tightness indicators.},
  archive      = {J_JBES},
  author       = {Philippe Goulet Coulombe},
  doi          = {10.1080/07350015.2024.2421279},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {669-683},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A neural phillips curve and a deep output gap},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group sparse β-model for network. <em>JBES</em>, <em>43</em>(3), 657-668. (<a href='https://doi.org/10.1080/07350015.2024.2418849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparsity, homogeneity, and heterogeneity are three important characteristics of many real-life networks. The recently proposed Sparse β -Model divides nodes into core ones and peripheral ones to accommodate sparsity, but the parameters of core nodes are assumed to be of similar magnitude, which may not be in line with applications. In this article, we propose the Group Sparse β -Model that splits the core nodes into groups and assumes different orders of magnitude of parameters in different groups, accounting for the heterogeneity among core nodes. When the groups are known, we provide consistent and asymptotically normal moment estimators of the parameters that control the global and local density. Based on that, consistency and asymptotic normality of the maximum likelihood estimators of the remaining parameters are derived. We also establish finite-sample error bounds results. When the groups are unknown, a ratio method is proposed to detect groups, which is computationally efficient. Simulations show competitive results and the analysis of a corporate inter-relationships network illustrates the usefulness of the proposed model.},
  archive      = {J_JBES},
  author       = {Zhonghan Wang and Junlong Zhao},
  doi          = {10.1080/07350015.2024.2418849},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {657-668},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Group sparse β-model for network},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for equal average forecast accuracy in possibly unstable environments. <em>JBES</em>, <em>43</em>(3), 643-656. (<a href='https://doi.org/10.1080/07350015.2024.2418835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the issue of testing the null of equal average forecast accuracy in a model where the forecast error loss differential series has a potentially nonconstant mean function over time. We show that when time variation is present in the loss differential mean, the standard Diebold and Mariano test, which was proposed for evaluating forecasts in a stable environment, has an asymptotic size of zero, and, whilst consistent, can have reduced local power. This arises due to inconsistent estimation of the implicit long run variance estimator, which diverges under a time varying mean. We suggest a modified statistic that replaces the standard long run variance estimator based on full-sample demeaning of the loss differential series with one based on nonparametric local demeaning. The new long run variance estimator is consistent under both the null and alternative when the mean function is time varying or constant, and in both cases, the modified test recovers the asymptotic size and power properties associated with the original test in the constant mean case. The modified test therefore provides a robust method for testing the equal average forecast accuracy null, allowing for instability in the loss differential mean. The benefits of our test are demonstrated via Monte Carlo simulation and two empirical applications.},
  archive      = {J_JBES},
  author       = {David I. Harvey and Stephen J. Leybourne and Yang Zu},
  doi          = {10.1080/07350015.2024.2418835},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {643-656},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Testing for equal average forecast accuracy in possibly unstable environments},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference in experiments with matched pairs and imperfect compliance. <em>JBES</em>, <em>43</em>(3), 627-642. (<a href='https://doi.org/10.1080/07350015.2024.2416972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies inference for the local average treatment effect in randomized controlled trials with imperfect compliance where treatment status is determined according to “matched pairs.” By “matched pairs,” we mean that units are sampled iid from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment. Under weak assumptions governing the quality of the pairings, we first derive the limit distribution of the usual Wald (i.e., two-stage least squares) estimator of the local average treatment effect. We show further that conventional heteroscedasticity-robust estimators of the Wald estimator’s limiting variance are generally conservative, in that their probability limits are (typically strictly) larger than the limiting variance. We therefore provide an alternative estimator of the limiting variance that is consistent. Finally, we consider the use of additional observed, baseline covariates not used in pairing units to increase the precision with which we can estimate the local average treatment effect. To this end, we derive the limiting behavior of a two-stage least squares estimator of the local average treatment effect which includes both the additional covariates in addition to pair fixed effects, and show that its limiting variance is always less than or equal to that of the Wald estimator. To complete our analysis, we provide a consistent estimator of this limiting variance. A simulation study confirms the practical relevance of our theoretical results. Finally, we apply our results to revisit a prominent experiment studying the effect of macroinsurance on microenterprise in Egypt.},
  archive      = {J_JBES},
  author       = {Yuehao Bai and Hongchang Guo and Azeem M. Shaikh and Max Tabord-Meehan},
  doi          = {10.1080/07350015.2024.2416972},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {627-642},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Inference in experiments with matched pairs and imperfect compliance},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized sparse covariance regression with high dimensional covariates. <em>JBES</em>, <em>43</em>(3), 615-626. (<a href='https://doi.org/10.1080/07350015.2024.2415109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariance regression offers an effective way to model the large covariance matrix with the auxiliary similarity matrices. In this work, we propose a sparse covariance regression (SCR) approach to handle the potentially high-dimensional predictors (i.e., similarity matrices). Specifically, we use the penalization method to identify the informative predictors and estimate their associated coefficients simultaneously. We first investigate the Lasso estimator and subsequently consider the folded concave penalized estimation methods (e.g., SCAD and MCP). However, the theoretical analysis of the existing penalization methods is primarily based on iid data, which is not directly applicable to our scenario. To address this difficulty, we establish the non-asymptotic error bounds by exploiting the spectral properties of the covariance matrix and similarity matrices. Then, we derive the estimation error bound for the Lasso estimator and establish the desirable oracle property of the folded concave penalized estimator. Extensive simulation studies are conducted to corroborate our theoretical results. We also illustrate the usefulness of the proposed method by applying it to a Chinese stock market dataset.},
  archive      = {J_JBES},
  author       = {Yuan Gao and Zhiyuan Zhang and Zhanrui Cai and Xuening Zhu and Tao Zou and Hansheng Wang},
  doi          = {10.1080/07350015.2024.2415109},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {615-626},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Penalized sparse covariance regression with high dimensional covariates},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An oracle inequality for multivariate dynamic quantile forecasting. <em>JBES</em>, <em>43</em>(3), 603-614. (<a href='https://doi.org/10.1080/07350015.2024.2415107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I derive an oracle inequality for a family of possibly misspecified multivariate conditional autoregressive quantile models. The family includes standard specifications for (nonlinear) quantile prediction proposed in the literature. This inequality is used to establish that the predictor that minimizes the in-sample average check loss achieves the best out-of-sample performance within its class at a near optimal rate, even when the model is fully misspecified. An empirical application to backtesting global Growth-at-Risk shows that a combination of the generalized autoregressive conditionally heteroscedastic model and the vector autoregression for Value-at-Risk performs best out-of-sample in terms of the check loss.},
  archive      = {J_JBES},
  author       = {Jordi Llorens-Terrazas},
  doi          = {10.1080/07350015.2024.2415107},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {603-614},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {An oracle inequality for multivariate dynamic quantile forecasting},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correcting for misclassified binary regressors using instrumental variables. <em>JBES</em>, <em>43</em>(3), 592-602. (<a href='https://doi.org/10.1080/07350015.2024.2415102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimators that exploit an instrumental variable to correct for misclassification in a binary regressor typically assume that the misclassification rates are invariant across all values of the instrument. We show this assumption is invalid in routine empirical settings. We derive a new estimator which allows misclassification rates to vary across values of the instrumental variable. Our key identifying assumption, that the sum of misclassification rates remains constant across instrument values, follows from the empirical examples we present. We also show this assumption can be relaxed using moment inequalities that arise from our model. We demonstrate the usefulness of our estimator through Monte Carlo simulations and a reanalysis of the extent to which Medicaid eligibility crowds out other forms of health insurance. Correcting for measurement error substantially reduces estimates of crowd out and the extent to which Medicaid eligibility lowers the share of the uninsured.},
  archive      = {J_JBES},
  author       = {Steven J. Haider and Melvin Stephens Jr.},
  doi          = {10.1080/07350015.2024.2415102},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {592-602},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Correcting for misclassified binary regressors using instrumental variables},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust estimation for threshold autoregressive moving-average models. <em>JBES</em>, <em>43</em>(3), 579-591. (<a href='https://doi.org/10.1080/07350015.2024.2412011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Threshold autoregressive moving-average (TARMA) models extend the popular TAR model and are among the few parametric time series specifications to include a moving average in a nonlinear setting. The state dependent reactions to shocks is particularly appealing in Economics and Finance. However, no theory is currently available when the data present heavy tails or anomalous observations. Here we provide the first theoretical framework for robust M -estimation for TARMA models and study its practical relevance. Under mild conditions, we show that the robust estimator for the threshold parameter is super-consistent, while the estimators for autoregressive and moving-average parameters are strongly consistent and asymptotically normal. The Monte Carlo study shows that the M -estimator is superior, in terms of both bias and variance, to the least squares estimator, which can be heavily affected by outliers. The findings suggest that robust M -estimation should be generally preferred to the least squares method. We apply our methodology to a set of commodity price time series; the robust TARMA fit presents smaller standard errors and superior forecasting accuracy. The results support the hypothesis of a two-regime non-linearity characterized by slow expansions and fast contractions.},
  archive      = {J_JBES},
  author       = {Greta Goracci and Davide Ferrari and Simone Giannerini and Francesco Ravazzolo},
  doi          = {10.1080/07350015.2024.2412011},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {579-591},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Robust estimation for threshold autoregressive moving-average models},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting panel data binary choice models with lagged dependent variables. <em>JBES</em>, <em>43</em>(3), 568-578. (<a href='https://doi.org/10.1080/07350015.2024.2412006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article revisits the identification and estimation of a class of semiparametric (distribution-free) panel data binary choice models with lagged dependent variables, exogenous covariates, and entity fixed effects. We provide a novel identification strategy, using an “identification at infinity” argument. In contrast with the celebrated work by Honoré and Kyriazidou published in 2000, our method permits time trends of any form and does not suffer from the “curse of dimensionality”. We propose an easily implementable conditional maximum score estimator. The asymptotic properties of the proposed estimator are fully characterized. A small-scale Monte Carlo study demonstrates that our approach performs satisfactorily in finite samples. We illustrate the usefulness of our method by presenting an empirical application to enrollment in private hospital insurance using the Household, Income and Labor Dynamics in Australia (HILDA) Survey data.},
  archive      = {J_JBES},
  author       = {Christopher R. Dobronyi and Fu Ouyang and Thomas Tao Yang},
  doi          = {10.1080/07350015.2024.2412006},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {568-578},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Revisiting panel data binary choice models with lagged dependent variables},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Endogenous kink threshold regression. <em>JBES</em>, <em>43</em>(3), 556-567. (<a href='https://doi.org/10.1080/07350015.2024.2407634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers an endogenous kink threshold regression model with an unknown threshold value in a time series as well as a panel data framework, where both the threshold variable and regressors are allowed to be endogenous. We construct our estimators from a nonparametric control function approach and derive the consistency and asymptotic distribution of our proposed estimators. Monte Carlo simulations are used to assess the finite sample performance of our proposed estimators. Finally, we apply our model to analyze the impact of COVID-19 cases on labor markets in the United States and Canada.},
  archive      = {J_JBES},
  author       = {Jianhan Zhang and Chaoyi Chen and Yiguo Sun and Thanasis Stengos},
  doi          = {10.1080/07350015.2024.2407634},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {556-567},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Endogenous kink threshold regression},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design-based theory for lasso adjustment in randomized block experiments and rerandomized experiments. <em>JBES</em>, <em>43</em>(3), 544-555. (<a href='https://doi.org/10.1080/07350015.2024.2403381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blocking, a special case of rerandomization, is routinely implemented in the design stage of randomized experiments to balance the baseline covariates. This study proposes a regression adjustment method based on the least absolute shrinkage and selection operator (Lasso) to efficiently estimate the average treatment effect in randomized block experiments with high-dimensional covariates. We derive the asymptotic properties of the proposed estimator and outline the conditions under which this estimator is more efficient than the unadjusted one. We provide a conservative variance estimator to facilitate valid inferences. Our framework allows one treated or control unit in some blocks and heterogeneous propensity scores across blocks, thus including paired experiments and finely stratified experiments as special cases. We further accommodate rerandomized experiments and a combination of blocking and rerandomization. Moreover, our analysis allows both the number of blocks and block sizes to tend to infinity, as well as heterogeneous treatment effects across blocks without assuming a true outcome data-generating model. Simulation studies and two real-data analyses demonstrate the advantages of the proposed method.},
  archive      = {J_JBES},
  author       = {Ke Zhu and Hanzhong Liu and Yuehan Yang},
  doi          = {10.1080/07350015.2024.2403381},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {544-555},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Design-based theory for lasso adjustment in randomized block experiments and rerandomized experiments},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic quantile factor analysis. <em>JBES</em>, <em>43</em>(3), 530-543. (<a href='https://doi.org/10.1080/07350015.2024.2396956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article extends quantile factor analysis to a probabilistic variant that incorporates regularization and computationally efficient variational approximations. We establish through synthetic and real data experiments that the proposed estimator can, in many cases, achieve better accuracy than a recently proposed loss-based estimator. We contribute to the factor analysis literature by extracting new indexes of low , medium , and high economic policy uncertainty, as well as loose , median , and tight financial conditions. We show that the high uncertainty and tight financial conditions indexes have superior predictive ability for various measures of economic activity. In a high-dimensional exercise involving about 1000 daily financial series, we find that quantile factors also provide superior out-of-sample information compared to mean or median factors.},
  archive      = {J_JBES},
  author       = {Dimitris Korobilis and Maximilian Schröder},
  doi          = {10.1080/07350015.2024.2396956},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {530-543},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Probabilistic quantile factor analysis},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved divide-and-conquer approach to estimating mean functional, with application to average treatment effect estimation. <em>JBES</em>, <em>43</em>(3), 520-529. (<a href='https://doi.org/10.1080/07350015.2024.2395429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mean estimation is an important issue in statistical inference and machine learning. We are concerned with estimating mean functional that is a function of several nonparametric functions when there is a large amount of observations. Directly estimating such mean functional through nonparametric smoothing has the complexity of at least a quadratic order of the sample size, which is computationally prohibitive for massive data. The divide-and-conquer approach are thus readily used to alleviate the computational complexity issue, which however imposes a stringent condition on the sample size in each local machine if a locally optimal bandwidth is used. To address this issue, we suggest to use a globally optimal bandwidth in each local machine, which alleviates the restriction on the local sample sizes substantially. We show that the divide-and-conquer approach with a globally optimal bandwidth achieves the estimation efficiency bound as if all observations were pooled together. In terms of computational efficiency, our proposal outperforms the pooled algorithm dramatically. We demonstrate these properties through average treatment effect estimation from both the asymptotic and the non-asymptotic perspectives.},
  archive      = {J_JBES},
  author       = {Zhengtian Zhu and Liping Zhu},
  doi          = {10.1080/07350015.2024.2395429},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {520-529},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {An improved divide-and-conquer approach to estimating mean functional, with application to average treatment effect estimation},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distinguishing time-varying factor models. <em>JBES</em>, <em>43</em>(3), 508-519. (<a href='https://doi.org/10.1080/07350015.2024.2395424'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying factor models have been widely used to model changing relationships among economic and financial variables. The existing literature usually specifies the time-varying factor loadings as deterministic functions of time or unit root processes. This article proposes two consistent tests to distinguish between these two specifications based on a randomization approach. By setting the null hypothesis as either specification, we show that the proposed test statistics follow an asymptotic Chi-squared distribution under the respective null hypotheses and diverge to infinity in probability under the respective alternatives. Simulation studies reveal that both test statistics perform reasonably well in finite samples. We apply the proposed tests to the U.S. macroeconomic and global macroeconomic and financial datasets. The results suggest that the time-varying factor loadings as deterministic functions of time should be adopted for these two applications.},
  archive      = {J_JBES},
  author       = {Zhonghao Fu and Liangjun Su and Xia Wang},
  doi          = {10.1080/07350015.2024.2395424},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {508-519},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Distinguishing time-varying factor models},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based co-clustering in customer targeting utilizing large-scale online product rating networks. <em>JBES</em>, <em>43</em>(3), 495-507. (<a href='https://doi.org/10.1080/07350015.2024.2395423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the widely available online customer ratings on products, the individual-level rating prediction and clustering of customers and products are increasingly important for sellers to create targeting strategies for expanding the customer base and improving product ratings. However, the massive missing data problem is a significant challenge for modeling online product ratings. To address this issue, we propose a new co-clustering methodology based on a bipartite network modeling of large-scale ordinal product ratings. Our method extends existing co-clustering methods by incorporating covariates and ordinal ratings in the model-based co-clustering of a weighted bipartite network. We devise an efficient variational EM algorithm for model estimation. A simulation study demonstrates that our methodology is scalable for modeling large datasets and provides accurate estimation and clustering results. We further show that our model can successfully identify different groups of customers and products with meaningful interpretations and achieve promising predictive performance in a real application for customer targeting.},
  archive      = {J_JBES},
  author       = {Qian Chen and Amal Agarwal and Duncan K. H. Fong and Wayne S. DeSarbo and Lingzhou Xue},
  doi          = {10.1080/07350015.2024.2395423},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {495-507},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Model-based co-clustering in customer targeting utilizing large-scale online product rating networks},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constrained polynomial likelihood. <em>JBES</em>, <em>43</em>(2), 482-493. (<a href='https://doi.org/10.1080/07350015.2024.2394587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a nonnegative polynomial minimum-norm likelihood ratio (PLR) of two distributions of which only moments are known. The sample PLR converges to the unknown population PLR under mild conditions. The methodology allows for additional shape restrictions, as we illustrate with two empirical applications. The first develops a PLR for the unknown transition density of a jump-diffusion process, while the second extracts a positive density directly from option prices. In both cases, we show the importance of implementing the non-negativity restriction.},
  archive      = {J_JBES},
  author       = {Caio Almeida and Ricardo Masini and Paul Schneider},
  doi          = {10.1080/07350015.2024.2394587},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {482-493},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Constrained polynomial likelihood},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systemic contagion. <em>JBES</em>, <em>43</em>(2), 468-481. (<a href='https://doi.org/10.1080/07350015.2024.2394580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article unifies existing systemic risk measures to present a novel intersecting notion of systemic contagion which captures institutional risk transmission amplified by a distressed financial system. The article constructs a test statistic for systemic contagion which demonstrates its systemic capacity in testing contagion under a series of numerical experiments. Using the new test the article constructs a forward-looking directed network to empirically measure the scale of systemic contagion between leading U.S. financial institutions. The proposed systemic contagion measure stands out as it provides timely warning signals of market crashes including that driven by the recent health-induced financial crisis.},
  archive      = {J_JBES},
  author       = {Soon Heng Leong},
  doi          = {10.1080/07350015.2024.2394580},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {468-481},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Systemic contagion},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and efficient estimation of potential outcome means under random assignment. <em>JBES</em>, <em>43</em>(2), 454-467. (<a href='https://doi.org/10.1080/07350015.2024.2394576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study efficiency improvements in randomized experiments for estimating a vector of potential outcome means using regression adjustment (RA) when there are more than two treatment levels. We show that linear RA which estimates separate slopes for each assignment level is never worse, asymptotically, than using the subsample averages. We also show that separate RA improves over pooled RA except in the obvious case where slope parameters in the linear projections are identical across the different assignment levels. We further characterize the class of nonlinear RA methods that preserve consistency of the potential outcome means despite arbitrary misspecification of the conditional mean functions. Finally, we apply these regression adjustment techniques to efficiently estimate the lower bound mean willingness to pay for an oil spill prevention program in California.},
  archive      = {J_JBES},
  author       = {Akanksha Negi and Jeffrey M. Wooldridge},
  doi          = {10.1080/07350015.2024.2394576},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {454-467},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Robust and efficient estimation of potential outcome means under random assignment},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quasi maximum likelihood estimation for large-dimensional matrix factor models. <em>JBES</em>, <em>43</em>(2), 439-453. (<a href='https://doi.org/10.1080/07350015.2024.2393724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce a novel approach, called the quasi maximum likelihood estimation (Q-MLE), for estimating large-dimensional matrix factor models. In contrast to the principal component analysis based approach, Q-MLE considers the heteroscedasticity of the idiosyncratic error term, the heteroscedasticity of which is simultaneously estimated with other parameters. Interestingly, under the homoscedasticity assumption of the idiosyncratic error, the Q-MLE estimator encompassed the projected estimator (PE) as a special case. We provide the convergence rates and asymptotic distributions of the Q-MLE estimators under mild conditions. Extensive numerical experiments demonstrate that the Q-MLE method performs better, especially when heteroscedasticity exists. Furthermore, two real examples in finance and macroeconomics reveal factor patterns across rows and columns, which coincide with financial, economic, or geographical interpretations.},
  archive      = {J_JBES},
  author       = {Sainan Xu and Chaofeng Yuan and Jianhua Guo},
  doi          = {10.1080/07350015.2024.2393724},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {439-453},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Quasi maximum likelihood estimation for large-dimensional matrix factor models},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical identification of independent shocks with kernel-based maximum likelihood estimation and an application to the global crude oil market. <em>JBES</em>, <em>43</em>(2), 423-438. (<a href='https://doi.org/10.1080/07350015.2024.2388657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Independent component analysis has emerged as a promising approach for revealing structural relationships in multivariate dynamic systems, particularly in scenarios with limited knowledge of causal patterns. This article introduces a robust kernel-based maximum likelihood (KML) estimation method that accommodates the distributional characteristics of the structural sources of data variation. Our Monte Carlo study demonstrates the superior performance of the KML estimator compared to existing approaches for independence-based identification. Moreover, the proposed method enables partial identification and dimension reduction even in the presence of dependent shocks. We illustrate the benefits of our approach by applying it to the global oil market model of Kilian, highlighting its ability to capture unmodeled higher-order dependence between oil supply and speculative oil demand shocks.},
  archive      = {J_JBES},
  author       = {Christian M. Hafner and Helmut Herwartz and Shu Wang},
  doi          = {10.1080/07350015.2024.2388657},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {423-438},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Statistical identification of independent shocks with kernel-based maximum likelihood estimation and an application to the global crude oil market},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A heteroscedasticity-robust overidentifying restriction test with high-dimensional covariates. <em>JBES</em>, <em>43</em>(2), 413-422. (<a href='https://doi.org/10.1080/07350015.2024.2388654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an overidentifying restriction test for high-dimensional linear instrumental variable models. The novelty of the proposed test is that it allows the number of covariates and instruments to be larger than the sample size. The test is scale-invariant and robust to heteroscedastic errors. To construct the final test statistic, we first introduce a test based on the maximum norm of multiple parameters that could be high-dimensional. The theoretical power based on the maximum norm is higher than that in the modified Cragg-Donald test, the only existing test allowing for large-dimensional covariates. Second, following the principle of power enhancement, we introduce the power-enhanced test, with an asymptotically zero component used to enhance the power to detect some extreme alternatives with many locally invalid instruments. Finally, an empirical example of the trade and economic growth nexus demonstrates the usefulness of the proposed test.},
  archive      = {J_JBES},
  author       = {Qingliang Fan and Zijian Guo and Ziwei Mei},
  doi          = {10.1080/07350015.2024.2388654},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {413-422},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A heteroscedasticity-robust overidentifying restriction test with high-dimensional covariates},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Difference-in-differences estimator of quantile treatment effect on the treated. <em>JBES</em>, <em>43</em>(2), 401-412. (<a href='https://doi.org/10.1080/07350015.2024.2388643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new difference-in-differences (DID) estimator of the quantile treatment effect on the treated (QTT). The model assumes a common time effect on the cumulative distribution functions of untreated potential outcomes, allowing for covariates. This condition holds if and only if the net change in the untreated outcome densities is common across treated and control groups. Unlike the Changes-in-Changes model our model is compatible with the usual DID assumption for means, and it provides a computationally simple and straightforward way to control for covariates. We establish uniform consistency and weak convergence of the proposed estimator of QTT and the related functions. The estimators and the simultaneous confidence bands remain valid even for discrete outcome variables. As an empirical application, the distributional impact of the earned income tax credit on birth weight is investigated. We provide a STATA ado file package.},
  archive      = {J_JBES},
  author       = {Doosoo Kim and Jeffrey M. Wooldridge},
  doi          = {10.1080/07350015.2024.2388643},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {401-412},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Difference-in-differences estimator of quantile treatment effect on the treated},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probability of causation with sample selection: A reanalysis of the impacts of jóvenes en acción on formality. <em>JBES</em>, <em>43</em>(2), 391-400. (<a href='https://doi.org/10.1080/07350015.2024.2388639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article identifies the probability of causation when there is sample selection. We show that the probability of causation is partially identified for individuals who are always observed regardless of treatment status and derive sharp bounds under three increasingly restrictive sets of assumptions. The first set imposes an exogenous treatment and a monotone sample selection mechanism. To tighten these bounds, the second set also imposes the monotone treatment response assumption, while the third set additionally imposes a stochastic dominance assumption. Finally, we use experimental data from the Colombian job training program Jóvenes en Acción to empirically illustrate our approach’s usefulness. We find that, among always-employed women, at least 10.2% and at most 13.4% transitioned to the formal labor market because of the program. However, our 90%-confidence region does not reject the null hypothesis that the lower bound is equal to zero.},
  archive      = {J_JBES},
  author       = {Vitor Possebom and Flavio Riva},
  doi          = {10.1080/07350015.2024.2388639},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {391-400},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Probability of causation with sample selection: A reanalysis of the impacts of jóvenes en acción on formality},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decomposition of differences in distribution under sample selection and the gender wage gap. <em>JBES</em>, <em>43</em>(2), 378-390. (<a href='https://doi.org/10.1080/07350015.2024.2385823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I address the decomposition of the differences between the distribution of outcomes of two groups when individuals self-select themselves into participation. I differentiate between the decomposition for participants and the entire population, highlighting how the primitive components of the model affect each of the distributions of outcomes. Additionally, I introduce two ancillary decompositions that help uncover the sources of differences in the distribution of unobservables and participation between the two groups. The estimation is done using existing quantile regression methods, for which I show how to perform uniformly valid inference. I illustrate these methods by revisiting the gender wage gap, finding that changes in female participation and self-selection have been the main drivers for reducing the gap.},
  archive      = {J_JBES},
  author       = {Santiago Pereda-Fernández},
  doi          = {10.1080/07350015.2024.2385823},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {378-390},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Decomposition of differences in distribution under sample selection and the gender wage gap},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating different sources of information for bayesian optimal portfolio selection. <em>JBES</em>, <em>43</em>(2), 365-377. (<a href='https://doi.org/10.1080/07350015.2024.2379361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces Bayesian inference procedures for tangency portfolios, with a primary focus on deriving a new conjugate prior for portfolio weights. This approach not only enables direct inference about the weights but also seamlessly integrates additional information into the prior specification. Specifically, it automatically incorporates high-frequency returns and a market condition metric (MCM), exemplified by the CBOE Volatility Index (VIX) and Economic Policy Uncertainty Index (EPU), significantly enhancing the decision-making process for optimal portfolio construction. While the Jeffreys’ prior is also acknowledged, emphasis is placed on the advantages and practical applications of the conjugate prior. An extensive empirical study reveals that our method, leveraging this conjugate prior, consistently outperforms existing trading strategies in the majority of examined cases.},
  archive      = {J_JBES},
  author       = {Olha Bodnar and Taras Bodnar and Vilhelm Niklasson},
  doi          = {10.1080/07350015.2024.2379361},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {365-377},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Incorporating different sources of information for bayesian optimal portfolio selection},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiperiod dynamic portfolio choice: When high dimensionality meets return predictability. <em>JBES</em>, <em>43</em>(2), 351-364. (<a href='https://doi.org/10.1080/07350015.2024.2374971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiperiod portfolio choice is the central problem in active asset management. Multiperiod dynamic portfolios are notoriously difficult to solve, especially when there are hundreds of tradable assets as well as a large number of state variables. In this article, we develop a novel two-step methodology to solve the multiperiod dynamic portfolio choice problem with high dimensional assets in the presence of return predictability conditional on a large number of state predictors. Specifically, in the first step, we propose the new Risk-Premium Projected-PCA (RP-PPCA) method to reduce the dimension of tradable assets. This method achieves Dimension Reduction (DR) by estimating latent factors with explanatory power in both time series variation and expected return in high-dimension-low-sample-size data. In the second step, we use dynamic programming to solve the multiperiod portfolio choice problem, and in each recursive step, we adopt an Adjusted semiparametric Model Averaging (AMA) method to avoid the curse of dimensionality associated with a large set of state variables while remaining computationally efficient. Thus, we name this two-step approach DRAMA, which stands for a combination of a new dimension reduction method and an adjusted semiparametric model averaging method. Analytically, we show that the portfolios constructed by the DRAMA are approximately optimal under mild assumptions. Moreover, our numerical results based on empirical data from US stock markets show that the proposed portfolios have excellent out-of-sample performances.},
  archive      = {J_JBES},
  author       = {Wenfeng He and Xiaoling Mei and Wei Zhong and Huanjun Zhu},
  doi          = {10.1080/07350015.2024.2374971},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {351-364},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Multiperiod dynamic portfolio choice: When high dimensionality meets return predictability},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tests for almost stochastic dominance. <em>JBES</em>, <em>43</em>(2), 338-350. (<a href='https://doi.org/10.1080/07350015.2024.2374274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a two-dimensional stochastic dominance (2DSD) index to characterize both strict and almost stochastic dominance. Based on this index, we derive an estimator for the minimum violation ratio (MVR), also known as the critical parameter , of the almost stochastic ordering condition between two variables. We determine the asymptotic properties of the empirical 2DSD index and MVR for the most frequently used stochastic orders. We also provide conditions under which the bootstrap estimators of these quantities are strongly consistent. As an application, we develop consistent bootstrap testing procedures for almost stochastic dominance. The performance of the tests is checked via simulations and the analysis of real data.},
  archive      = {J_JBES},
  author       = {Amparo Baíllo and Javier Cárcamo and Carlos Mora-Corral},
  doi          = {10.1080/07350015.2024.2374274},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {338-350},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Tests for almost stochastic dominance},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating latent-variable panel data models using parameter-expanded SEM methods. <em>JBES</em>, <em>43</em>(2), 324-337. (<a href='https://doi.org/10.1080/07350015.2024.2365783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents new estimation algorithms for three types of dynamic panel data models with latent variables: factor models, discrete choice models, and persistent-transitory quantile processes. The new methods combine the parameter expansion (PX) ideas of Liu, Rubin, and Wu with the stochastic expectation-maximization (SEM) algorithm in likelihood and moment-based contexts. The goal is to facilitate convergence in models with a large space of latent variables by improving algorithmic efficiency. This is achieved by specifying expanded models within the M step. Effectively, we are proposing new estimators for the pseudo-data within iterations that take into account the fact that the model of interest is misspecified for draws based on parameter values far from the truth. We establish the asymptotic equivalence of the likelihood-based PX-SEM to an alternative SEM algorithm with a smaller expected fraction of missing information compared to the standard SEM based on the original model, implying a faster global convergence rate. Finally, in simulations we show that the new algorithms significantly improve the convergence speed relative to standard SEM algorithms, sometimes dramatically so, by reducing the total computing time from hours to a few minutes.},
  archive      = {J_JBES},
  author       = {Siqi Wei},
  doi          = {10.1080/07350015.2024.2365783},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {324-337},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Estimating latent-variable panel data models using parameter-expanded SEM methods},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shapley curves: A smoothing perspective. <em>JBES</em>, <em>43</em>(2), 312-323. (<a href='https://doi.org/10.1080/07350015.2024.2365781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article fills the limited statistical understanding of Shapley values as a variable importance measure from a nonparametric (or smoothing) perspective. We introduce population-level Shapley curves to measure the true variable importance, determined by the conditional expectation function and the distribution of covariates. Having defined the estimand, we derive minimax convergence rates and asymptotic normality under general conditions for the two leading estimation strategies. For finite sample inference, we propose a novel version of the wild bootstrap procedure tailored for capturing lower-order terms in the estimation of Shapley curves. Numerical studies confirm our theoretical findings, and an empirical application analyzes the determining factors of vehicle prices.},
  archive      = {J_JBES},
  author       = {Ratmir Miftachov and Georg Keilbar and Wolfgang Karl Härdle},
  doi          = {10.1080/07350015.2024.2365781},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {312-323},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Shapley curves: A smoothing perspective},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A smooth shadow-rate dynamic nelson-siegel model for yields at the zero lower bound. <em>JBES</em>, <em>43</em>(2), 298-311. (<a href='https://doi.org/10.1080/07350015.2024.2365779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a smooth shadow-rate version of the dynamic Nelson-Siegel (DNS) model to analyze the term structure of interest rates during a zero lower bound (ZLB) period. By relaxing the no-arbitrage restriction, our shadow-rate model becomes highly tractable with a closed-form yield curve expression. This permits the implementation of readily available DNS extensions such as allowing for time-varying parameters and the integration of macroeconomic variables. Using U.S. Treasury data, we provide clear evidence of a smooth transition of the yields entering and leaving the ZLB state. Moreover, we show that the smooth shadow-rate DNS model dominates the baseline DNS model and (shadow-rate) affine term structure models in terms of fitting and forecasting the yield curve, while it also produces plausible policy insights at the ZLB.},
  archive      = {J_JBES},
  author       = {Daan Opschoor and Michel van der Wel},
  doi          = {10.1080/07350015.2024.2365779},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {298-311},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A smooth shadow-rate dynamic nelson-siegel model for yields at the zero lower bound},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trend and variance adaptive bayesian changepoint analysis and local outlier scoring. <em>JBES</em>, <em>43</em>(2), 286-297. (<a href='https://doi.org/10.1080/07350015.2024.2362269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We adaptively estimate both changepoints and local outlier processes in a Bayesian dynamic linear model with global-local shrinkage priors in a novel model we call Adaptive Bayesian Changepoints with Outliers (ABCO). We use a state-space approach to identify a dynamic signal in the presence of outliers and measurement error with stochastic volatility. We find that global state equation parameters are inadequate for most real applications and we include local parameters to track noise at each time-step. This setup provides a flexible framework to detect unspecified changepoints in complex series, such as those with large interruptions in local trends, with robustness to outliers and heteroscedastic noise. Finally, we compare our algorithm against several alternatives to demonstrate its efficacy in diverse simulation scenarios and two empirical examples on the U.S. economy.},
  archive      = {J_JBES},
  author       = {Haoxuan Wu and Toryn L. J. Schafer and David S. Matteson},
  doi          = {10.1080/07350015.2024.2362269},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {286-297},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Trend and variance adaptive bayesian changepoint analysis and local outlier scoring},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large skew-t copula models and asymmetric dependence in intraday equity returns. <em>JBES</em>, <em>43</em>(2), 269-285. (<a href='https://doi.org/10.1080/07350015.2024.2360592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skew-t copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A stochastic gradient ascent algorithm is used to solve the variational optimization. The methodology is used to estimate skew-t factor copula models with up to 15 factors for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise correlations. In a moving window study we show that the asymmetric dependencies also vary over time, and that intraday predictive densities from the skew-t copula are more accurate than those from benchmark copula models. Portfolio selection strategies based on the estimated pairwise asymmetric dependencies improve performance relative to the index.},
  archive      = {J_JBES},
  author       = {Lin Deng and Michael Stanley Smith and Worapree Maneesoonthorn},
  doi          = {10.1080/07350015.2024.2360592},
  journal      = {Journal of Business & Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {269-285},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Large skew-t copula models and asymmetric dependence in intraday equity returns},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for heterogeneous treatment effects discovered by generic machine learning in randomized experiments. <em>JBES</em>, <em>43</em>(1), 256-268. (<a href='https://doi.org/10.1080/07350015.2024.2358909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers are increasingly turning to machine learning (ML) algorithms to investigate causal heterogeneity in randomized experiments. Despite their promise, ML algorithms may fail to accurately ascertain heterogeneous treatment effects under practical settings with many covariates and small sample size. In addition, the quantification of estimation uncertainty remains a challenge. We develop a general approach to statistical inference for heterogeneous treatment effects discovered by a generic ML algorithm. We apply the Neyman’s repeated sampling framework to a common setting, in which researchers use an ML algorithm to estimate the conditional average treatment effect and then divide the sample into several groups based on the magnitude of the estimated effects. We show how to estimate the average treatment effect within each of these groups, and construct a valid confidence interval. In addition, we develop nonparametric tests of treatment effect homogeneity across groups, and rank-consistency of within-group average treatment effects. The validity of our methodology does not rely on the properties of ML algorithms because it is solely based on the randomization of treatment assignment and random sampling of units. Finally, we generalize our methodology to the cross-fitting procedure by accounting for the additional uncertainty induced by the random splitting of data.},
  archive      = {J_JBES},
  author       = {Kosuke Imai and Michael Lingzhi Li},
  doi          = {10.1080/07350015.2024.2358909},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {256-268},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Statistical inference for heterogeneous treatment effects discovered by generic machine learning in randomized experiments},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of the local conditional tail average treatment effect. <em>JBES</em>, <em>43</em>(1), 241-255. (<a href='https://doi.org/10.1080/07350015.2024.2356731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conditional tail average treatment effect (CTATE) is defined as a difference between the conditional tail expectations of potential outcomes, which can capture heterogeneity and deliver aggregated local information on treatment effects over different quantile levels and is closely related to the notion of second-order stochastic dominance and the Lorenz curve. These properties render it a valuable tool for policy evaluation. In this article, we study estimation of the CTATE locally for a group of compliers (local CTATE or LCTATE) under the two-sided noncompliance framework. We consider a semiparametric treatment effect framework under endogeneity for the LCTATE estimation using a newly introduced class of consistent loss functions jointly for the CTE and quantile. We establish the asymptotic theory of our proposed LCTATE estimator and provide an efficient algorithm for its implementation. We then apply the method to evaluate the effects of participating in programs under the Job Training Partnership Act in the United States.},
  archive      = {J_JBES},
  author       = {Le-Yu Chen and Yu-Min Yen},
  doi          = {10.1080/07350015.2024.2356731},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {241-255},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Estimation of the local conditional tail average treatment effect},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous confidence intervals for partially identified parameters. <em>JBES</em>, <em>43</em>(1), 232-240. (<a href='https://doi.org/10.1080/07350015.2024.2356083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article extends the Imbens and Manski and Stoye confidence interval for a partially identified scalar parameter to a vector-valued parameter. The proposed method produces uniformly valid simultaneous confidence intervals for each dimension, or, equivalently, a rectangular confidence region that covers points in the identified set with a specified probability. The method applies when asymptotically normal estimates of upper and lower bounds for each dimension are available. The intervals are computationally simple and fast relative to methods based on test inversion or bootstrapped calibration, and do not suffer from the conservativity of projection-based approaches.},
  archive      = {J_JBES},
  author       = {Brigham R. Frandsen and Zachari A. Pond},
  doi          = {10.1080/07350015.2024.2356083},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {232-240},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Simultaneous confidence intervals for partially identified parameters},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting inflation using economic narratives. <em>JBES</em>, <em>43</em>(1), 216-231. (<a href='https://doi.org/10.1080/07350015.2024.2347619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use economic narratives to forecast inflation with a large news corpus and machine learning algorithms. The economic narratives from the full text content of over 880,000 Wall Street Journal articles are decomposed into multiple time series representing interpretable news topics, which are then used to predict inflation. The results indicate that narrative-based forecasts are more accurate than the benchmarks, especially during recession periods. Narrative-based forecasts perform better in long-run forecasting and provide incremental predictive information even after controlling macroeconomic big data. In particular, information about inflation expectations and prices of specific goods embedded in narratives contributes to their predictive power. Overall, we provide a novel representation of economic narratives and document the important role of economic narratives in inflation forecasting.},
  archive      = {J_JBES},
  author       = {Yongmiao Hong and Fuwei Jiang and Lingchao Meng and Bowen Xue},
  doi          = {10.1080/07350015.2024.2347619},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {216-231},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Forecasting inflation using economic narratives},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification and estimation of discrete choice models with unobserved choice sets. <em>JBES</em>, <em>43</em>(1), 204-215. (<a href='https://doi.org/10.1080/07350015.2024.2342731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework for nonparametric identification and estimation of discrete choice models with unobserved choice sets. We recover the joint distribution of choice sets and preferences from a cross-section of repeated choices. We assume that either the latent choice sets are sparse or that the number of repeated choices is sufficiently large. Sparsity requires the number of possible choice sets to be relatively small. It is satisfied, for instance, when the choice sets are nested or when they form a partition. Our estimation procedure is computationally fast and uses mixed-integer programming to recover the sparse support of choice sets. Analyzing the ready-to-eat cereal industry using a household scanner dataset, we find that ignoring the unobservability of choice sets can lead to incorrect estimates of preferences.},
  archive      = {J_JBES},
  author       = {Victor H. Aguiar and Nail Kashaev},
  doi          = {10.1080/07350015.2024.2342731},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {204-215},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Identification and estimation of discrete choice models with unobserved choice sets},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trending time-varying coefficient spatial panel data models. <em>JBES</em>, <em>43</em>(1), 191-203. (<a href='https://doi.org/10.1080/07350015.2024.2340516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the estimation and inference of spatial panel data models in which the regression coefficient vector is a trending function. We use time differences to eliminate the individual effects and employ various GMM estimations for regression coefficients with both linear and quadratic moments. Time trend estimator based on these GMM estimations is also proposed. Monte Carlo experiments show that the finite sample performance of the estimators is satisfactory. As an empirical illustration, we investigate the trending pattern of the spillover effect of air pollution among Chinese cities from 2015 to 2021.},
  archive      = {J_JBES},
  author       = {Hsuan-Yu Chang and Xiaojun Song and Jihai Yu},
  doi          = {10.1080/07350015.2024.2340516},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {191-203},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Trending time-varying coefficient spatial panel data models},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting weak distribution shifts via displacement interpolation. <em>JBES</em>, <em>43</em>(1), 178-190. (<a href='https://doi.org/10.1080/07350015.2024.2335957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting weak, systematic distribution shifts and quantitatively modeling individual, heterogeneous responses to policies or incentives have found increasing empirical applications in social and economic sciences. Given two probability distributions P (null) and Q (alternative), we study the problem of detecting weak distribution shift deviating from the null P toward the alternative Q , where the level of deviation vanishes as a function of n , the sample size. We propose a model for weak distribution shifts via displacement interpolation between P and Q , drawing from the optimal transport theory. We study a hypothesis testing procedure based on the Wasserstein distance, derive sharp conditions under which detection is possible, and provide the exact characterization of the asymptotic Type I and Type II errors at the detection boundary using empirical processes. We demonstrate how the proposed testing procedure works in modeling and detecting weak distribution shifts in real datasets using two empirical examples: distribution shifts in consumer spending after COVID-19, and heterogeneity in the published p -values of statistical tests in journals across different disciplines.},
  archive      = {J_JBES},
  author       = {YoonHaeng Hur and Tengyuan Liang},
  doi          = {10.1080/07350015.2024.2335957},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {178-190},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Detecting weak distribution shifts via displacement interpolation},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Abadie’s kappa and weighting estimators of the local average treatment effect. <em>JBES</em>, <em>43</em>(1), 164-177. (<a href='https://doi.org/10.1080/07350015.2024.2332763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has demonstrated the importance of flexibly controlling for covariates in instrumental variables estimation. In this article we study the finite sample and asymptotic properties of various weighting estimators of the local average treatment effect (LATE), motivated by Abadie’s kappa theorem and offering the requisite flexibility relative to standard practice. We argue that two of the estimators under consideration, which are weight normalized, are generally preferable. Several other estimators, which are unnormalized, do not satisfy the properties of scale invariance with respect to the natural logarithm and translation invariance, thereby exhibiting sensitivity to the units of measurement when estimating the LATE in logs and the centering of the outcome variable more generally. We also demonstrate that, when noncompliance is one sided, certain weighting estimators have the advantage of being based on a denominator that is strictly greater than zero by construction. This is the case for only one of the two normalized estimators, and we recommend this estimator for wider use. We illustrate our findings with a simulation study and three empirical applications, which clearly document the sensitivity of unnormalized estimators to how the outcome variable is coded. We implement the proposed estimators in the Stata package kappalate .},
  archive      = {J_JBES},
  author       = {Tymon Słoczyński and S. Derya Uysal and Jeffrey M. Wooldridge},
  doi          = {10.1080/07350015.2024.2332763},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {164-177},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Abadie’s kappa and weighting estimators of the local average treatment effect},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear spatial dynamic panel data models with endogenous dominant units: An application to share data. <em>JBES</em>, <em>43</em>(1), 150-163. (<a href='https://doi.org/10.1080/07350015.2024.2329645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a nonlinear spatial dynamic panel data model with one particularly interesting application to a structural interaction model for share data. To account for effects from dominant (popular) units, the spatial weights matrix in our model can allow for unbounded column sums. To account for heterogeneity, our model includes two-way fixed effects and heteroscedastic errors. We further consider the potential endogeneity of the spatial weight matrix constructed from socioeconomic distance. We investigate the quasi-maximum likelihood estimator (QMLE), generalized methods of moments estimator (GMME), and root estimator (RTE), and establish their consistency and asymptotic normality based on the near epoch dependence (NED) framework. The RTE can derive a relatively computationally simple and closed-form solution without evaluating the QMLE’s Jacobian matrix as well as the iterations by GMME. We consider both n , T → ∞ , and the strength of the dominant units is equal to 1 when T → ∞ . For the purpose of empirical analysis, we derive the marginal effects and their limiting distributions based on the proposed estimators. In an empirical application, we apply our model to China’s prefecture city-level data, revealing significant spillover effects of the tertiary industry share. These findings suggest that the development of the tertiary sector in large cities can foster its growth in small cities.},
  archive      = {J_JBES},
  author       = {Jiajun Zhang and Chuanmin Zhao and Xi Qu},
  doi          = {10.1080/07350015.2024.2329645},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {150-163},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Nonlinear spatial dynamic panel data models with endogenous dominant units: An application to share data},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating posterior sensitivities with application to structural analysis of bayesian vector autoregressions. <em>JBES</em>, <em>43</em>(1), 134-149. (<a href='https://doi.org/10.1080/07350015.2024.2329639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent feature of Bayesian empirical analysis is the dependence of posterior inference on prior parameters, which researchers typically specify. However, quantifying the magnitude of this dependence remains difficult. This article extends Infinitesimal Perturbation Analysis, widely used in classical simulation, to compute asymptotically unbiased and consistent sensitivities of posterior statistics with respect to prior parameters from Markov chain Monte Carlo inference via Gibbs sampling. The method demonstrates the possibility of efficiently computing the complete set of prior sensitivities for a wide range of posterior statistics, alongside the estimation algorithm using Automatic Differentiation. The method’s application is exemplified in Bayesian Vector Autoregression analysis of fiscal policy in U.S. macroeconomic time series data. The analysis assesses the sensitivities of posterior estimates, including the Impulse response functions and Forecast error variance decompositions, to prior parameters under common Minnesota shrinkage priors. The findings illuminate the significant and intricate influence of prior specification on the posterior distribution. This effect is particularly notable in crucial posterior statistics, such as the substantial absolute eigenvalue of the companion matrix, ultimately shaping the structural analysis.},
  archive      = {J_JBES},
  author       = {Liana Jacobi and Dan Zhu and Mark Joshi},
  doi          = {10.1080/07350015.2024.2329639},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {134-149},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Estimating posterior sensitivities with application to structural analysis of bayesian vector autoregressions},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Panel data cointegration testing with structural instabilities. <em>JBES</em>, <em>43</em>(1), 122-133. (<a href='https://doi.org/10.1080/07350015.2024.2327844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spurious regression analysis in panel data when the time series are cross-section dependent is analyzed in the article. The set-up includes (possibly unknown) multiple structural breaks that can affect both the deterministic and the common factor components. We show that consistent estimation of the long-run average parameter is possible once cross-section dependence is controlled using cross-section averages in the spirit of Pesaran’s common correlated effects approach. This result is used to design individual and panel cointegration test statistics that accommodate the presence of structural breaks that can induce parameter instabilities in the deterministic component, the cointegration vector and the common factor loadings.},
  archive      = {J_JBES},
  author       = {Anindya Banerjee and Josep Lluís Carrion-i-Silvestre},
  doi          = {10.1080/07350015.2024.2327844},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {122-133},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Panel data cointegration testing with structural instabilities},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully data-driven normalized and exponentiated kernel density estimator with hyvärinen score. <em>JBES</em>, <em>43</em>(1), 110-121. (<a href='https://doi.org/10.1080/07350015.2024.2326149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Jewson and Rossell proposed a new approach for kernel density estimation using an exponentiated form of kernel density estimators. The density estimator contained two hyperparameters that flexibly controls the smoothness of the resulting density. We tune them in a data-driven manner by minimizing an objective function based on the Hyvärinen score to avoid the optimization involving the intractable normalizing constant caused by the exponentiation. We show the asymptotic properties of the proposed estimator and emphasize the importance of including the two hyperparameters for flexible density estimation. Our simulation studies and application to income data show that the proposed density estimator is promising when the underlying density is multi-modal or when observations contain outliers.},
  archive      = {J_JBES},
  author       = {Shunsuke Imai and Takuya Koriyama and Shouto Yonekura and Shonosuke Sugasawa and Yoshihiko Nishiyama},
  doi          = {10.1080/07350015.2024.2326149},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {110-121},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Fully data-driven normalized and exponentiated kernel density estimator with hyvärinen score},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reduced rank spatio-temporal models. <em>JBES</em>, <em>43</em>(1), 98-109. (<a href='https://doi.org/10.1080/07350015.2024.2326142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To simultaneously model the cross-sectional dependency and dynamic time dependency among n units, most research in spatial econometrics parameterizes the coefficient matrices among the n units as functions of known weights matrices. This modeling framework is over-simplified and faces the risk of misspecification when constructing the weights matrices. In this article, we propose a novel reduced-rank spatio-temporal model by assuming the coefficient matrices follow a reduced-rank structure. This specification avoids construction of the weights matrices and provides a good interpretation, especially for financial data. To estimate the unknown parameters, a quasi-maximum likelihood estimator (QMLE) is proposed and obtained via the Gradient descent algorithm with Armijo line search. We establish the asymptotic properties of QMLE when the number of units and the number of time periods both diverge to infinity. To determine the rank, we propose a ridge-type ratio estimator and demonstrate its rank selection consistency. The proposed methodology is illustrated via extensive simulation studies. Finally, a Chinese stock dataset is analyzed to investigate the cross-sectional and temporal spillover effects among stock returns.},
  archive      = {J_JBES},
  author       = {Dan Pu and Kuangnan Fang and Wei Lan and Jihai Yu and Qingzhao Zhang},
  doi          = {10.1080/07350015.2024.2326142},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {98-109},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Reduced rank spatio-temporal models},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile policy effects: An application to U.S. macroprudential policy. <em>JBES</em>, <em>43</em>(1), 81-97. (<a href='https://doi.org/10.1080/07350015.2024.2326140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To assess the dynamic distributional impacts of macroeconomic policy, we propose quantile policy effects to quantify disparities between the quantiles of potential outcomes under different policies. We first identify quantile policy effects under the unconfoundedness assumption and propose an inverse probability weighting estimator. We then examine the asymptotic behavior of the proposed estimator in a time series framework and suggest a blockwise bootstrap method for inference. Applying this method, we investigate the effectiveness of U.S. macroprudential actions on bank credit growth from 1948 to 2019. Empirically, we find that the effects of macroprudential policy on credit growth are asymmetric and depend on the quantiles of credit growth. The tightening of macroprudential actions fails to rein in high credit growth, whereas easing policies do not effectively stimulate bank credit growth during low-growth periods. These findings suggest that U.S. macroprudential policies might not sufficiently address the challenges of soaring bank credit or ensure overarching financial stability.},
  archive      = {J_JBES},
  author       = {Hsin-Yi Lin and Yu-Hsiang Hsiao and Yu-Chin Hsu},
  doi          = {10.1080/07350015.2024.2326140},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {81-97},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Quantile policy effects: An application to U.S. macroprudential policy},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grouped heterogeneity in linear panel data models with heterogeneous error variances. <em>JBES</em>, <em>43</em>(1), 68-80. (<a href='https://doi.org/10.1080/07350015.2024.2325440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a procedure to identify latent group structures in linear panel data models that exploits a grouping in the error variances of cross-sectional units. To accommodate such grouping, we introduce an objective function that avoids a singularity that arises in a pseudolikelihood approach. We provide theoretical and numerical evidence showing when allowing for variance groups improves classification. The developed procedure provides new evidence on the relation between firm-level research and development (R&D) investments and the business cycle. We find a well-defined group structure in the variances that ex-post can be related to firm size. Our estimates indicate stronger procyclical investment patterns at medium-size firms compared to large firms.},
  archive      = {J_JBES},
  author       = {Jhordano Aguilar Loyo and Tom Boot},
  doi          = {10.1080/07350015.2024.2325440},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {68-80},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Grouped heterogeneity in linear panel data models with heterogeneous error variances},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing many zero restrictions in a high dimensional linear regression setting. <em>JBES</em>, <em>43</em>(1), 55-67. (<a href='https://doi.org/10.1080/07350015.2024.2325436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a test of many zero parameter restrictions in a high dimensional linear iid regression model with k ≫ n regressors. The test statistic is formed by estimating key parameters one at a time based on many low dimension regression models with nuisance terms. The parsimoniously parameterized models identify whether the original parameter of interest is or is not zero. Estimating fixed low dimension sub-parameters ensures greater estimator accuracy, it does not require a sparsity assumption nor therefore a regularized estimator, it is computationally fast compared to, for example, de-biased Lasso, and using only the largest in a sequence of weighted estimators reduces test statistic complexity and therefore estimation error. We provide a parametric wild bootstrap for p -value computation, and prove the test is consistent and has nontrivial n / { ln ( n ) M n } -local-to-null power where M n is the l ∞ covariate fourth moment.},
  archive      = {J_JBES},
  author       = {Jonathan B. Hill},
  doi          = {10.1080/07350015.2024.2325436},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {55-67},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Testing many zero restrictions in a high dimensional linear regression setting},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A statistically identified structural vector autoregression with endogenously switching volatility regime. <em>JBES</em>, <em>43</em>(1), 44-54. (<a href='https://doi.org/10.1080/07350015.2024.2322090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a structural vector autoregressive model with endogenously switching conditional covariance matrix. The structural shocks are identified by simultaneously diagonalizing the reduced form error covariance matrices. It is not, however, always clear whether the condition for the full statistical identification is satisfied, and its validity is difficult to justify formally. Therefore, we provide general sets of conditions, that allow to combine sign and zero restrictions on the impact matrix, for identifying a subset of the shocks when the condition for statistical identification of the model fails. In an empirical application to the effects of the U.S. monetary policy shock, we find that a contractionary monetary policy shock significantly decreases output in a persistent hump-shaped pattern. Prices decrease permanently, but there is short-run inertia in their response. The accompanying R package gmvarkit provides a comprehensive set of tools for numerical analysis of the model.},
  archive      = {J_JBES},
  author       = {Savi Virolainen},
  doi          = {10.1080/07350015.2024.2322090},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {44-54},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A statistically identified structural vector autoregression with endogenously switching volatility regime},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian process vector autoregressions and macroeconomic uncertainty. <em>JBES</em>, <em>43</em>(1), 27-43. (<a href='https://doi.org/10.1080/07350015.2024.2322089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a nonparametric multivariate time series model that remains agnostic on the precise relationship between a (possibly) large set of macroeconomic time series and their lagged values. The main building block of our model is a Gaussian process prior on the functional relationship that determines the conditional mean of the model, hence, the name of Gaussian process vector autoregression (GP-VAR). A flexible stochastic volatility specification is used to provide additional flexibility and control for heteroscedasticity. Markov chain Monte Carlo (MCMC) estimation is carried out through an efficient and scalable algorithm which can handle large models. The GP-VAR is used to analyze the effects of macroeconomic uncertainty, with a particular emphasis on time variation and asymmetries in the transmission mechanisms.},
  archive      = {J_JBES},
  author       = {Niko Hauzenberger and Florian Huber and Massimiliano Marcellino and Nico Petz},
  doi          = {10.1080/07350015.2024.2322089},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {27-43},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Gaussian process vector autoregressions and macroeconomic uncertainty},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gamma-driven markov processes and extensions with application to realized volatility. <em>JBES</em>, <em>43</em>(1), 14-26. (<a href='https://doi.org/10.1080/07350015.2024.2321375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel class of Markov processes for dealing with continuous positive time series data, which is constructed based on a latent gamma effect and named gamma-driven (GD) models. The GD processes possess desirable properties and features: (i) it can produce any desirable invariant distribution with support on R + , (ii) it is time-reversible, and (iii) it has the transition density function given in an explicit form. Estimation of parameters is performed through the maximum likelihood method combined with a Gauss Laguerre quadrature to approximate the likelihood function. The evaluation of the estimators and also confidence intervals of parameters are explored via Monte Carlo simulation studies. Two generalizations of the GD processes are also proposed to handle nonstationary and long-memory time series. We apply the proposed methodologies to analyze the daily realized volatility of the FTSE 100 equity index.},
  archive      = {J_JBES},
  author       = {Fernanda G. B. Mendes and Wagner Barreto-Souza and Sokol Ndreca},
  doi          = {10.1080/07350015.2024.2321375},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {14-26},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Gamma-driven markov processes and extensions with application to realized volatility},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Max share identification of multiple shocks: An application to uncertainty and financial conditions. <em>JBES</em>, <em>43</em>(1), 1-13. (<a href='https://doi.org/10.1080/07350015.2024.2316829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We generalize the Max Share approach to allow for simultaneous identification of a multiplicity of shocks in a Structural Vector Autoregression. Our machinery therefore overcomes the well-known drawbacks that individually identified shocks (i) tend to be correlated to each other or (ii) can be separated under orthogonalizations with weak economic ground. We show that identification corresponds to solving a nontrivial optimization problem. We provide conditions for non-emptiness of solutions and point-identification, and Bayesian algorithms for estimation and inference. We use the approach to study the effects of uncertainty and financial shocks, allowing for the possibility that the former responds contemporaneously to other shocks, distinguishing macroeconomic from financial uncertainty and credit supply shocks. Using U.S. data we find that financial uncertainty mimics a demand shock, while the interpretation of macro uncertainty is more mixed. Furthermore, variation in uncertainty partially represents the endogenous response of uncertainty to other shocks.},
  archive      = {J_JBES},
  author       = {Andrea Carriero and Alessio Volpicella},
  doi          = {10.1080/07350015.2024.2316829},
  journal      = {Journal of Business & Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Max share identification of multiple shocks: An application to uncertainty and financial conditions},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
