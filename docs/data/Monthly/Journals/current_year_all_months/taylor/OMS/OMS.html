<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OMS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="oms">OMS - 52</h2>
<ul>
<li><details>
<summary>
(2025). Gradient-type methods for decentralized optimization problems with polyak–Łojasiewicz condition over time-varying networks. <em>OMS</em>, <em>40</em>(5), 1267-1294. (<a href='https://doi.org/10.1080/10556788.2025.2475404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the decentralized optimization (minimization and saddle point) problems with objective functions that satisfy Polyak–Łojasiewicz condition (PL-condition). The first part of the paper is devoted to the minimization problem of the sum-type cost functions. In order to solve a such class of problems, we propose a gradient descent type method with a consensus projection procedure and the inexact gradient of the objectives. Next, in the second part, we study the saddle-point problem (SPP) with a structure of the sum, with objectives satisfying the two-sided PL-condition. To solve such SPP, we propose a generalization of the Multi-step Gradient Descent Ascent method with a consensus procedure, and inexact gradients of the objective function with respect to both variables. Finally, we present some of the numerical experiments, to show the efficiency of the proposed algorithm for the robust least squares problem.},
  archive      = {J_OMS},
  author       = {Ilya Kuruzov and Mohammad Alkousa and Fedor Stonyakin and Alexander Gasnikov},
  doi          = {10.1080/10556788.2025.2475404},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1267-1294},
  shortjournal = {Optim. Methods Softw.},
  title        = {Gradient-type methods for decentralized optimization problems with polyak–Łojasiewicz condition over time-varying networks},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed saddle point problems: Lower bounds, near-optimal and robust algorithms. <em>OMS</em>, <em>40</em>(5), 1249-1266. (<a href='https://doi.org/10.1080/10556788.2025.2463986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the distributed optimization of stochastic saddle point problems. The first part of the paper is devoted to lower bounds for the centralized and decentralized distributed methods for smooth (strongly) convex-(strongly) concave saddle point problems, as well as the near-optimal algorithms by which these bounds are achieved. Next, we present a new federated algorithm for centralized distributed saddle-point problems – Extra Step Local SGD. The theoretical analysis of the new method is carried out for strongly convex-strongly concave and non-convex-non-concave problems. In the experimental part of the paper, we show the effectiveness of our method in practice. In particular, we train GANs in a distributed manner.},
  archive      = {J_OMS},
  author       = {Aleksandr Beznosikov and Valentin Samokhin and Alexander Gasnikov},
  doi          = {10.1080/10556788.2025.2463986},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1249-1266},
  shortjournal = {Optim. Methods Softw.},
  title        = {Distributed saddle point problems: Lower bounds, near-optimal and robust algorithms},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLECS: A federated learning second-order framework via compression and sketching. <em>OMS</em>, <em>40</em>(5), 1222-1248. (<a href='https://doi.org/10.1080/10556788.2025.2456118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the recent work FedNL [Safaryan et al. FedNL: making newton-type methods applicable to federated learning], we propose a new communication efficient second-order framework for Federated learning, namely FLECS. The proposed method reduces the high-memory requirements of FedNL by the usage of an L-SR1 type update for the Hessian approximation which is stored on the central server. A low dimensional ‘sketch’ of the Hessian is all that is needed by each device to generate an update, so that memory costs as well as number of Hessian-vector products for the agent are low. Biased and unbiased compressions are utilized to make communication costs also low. Convergence guarantees for FLECS are provided in both the strongly convex, and non-convex cases, and local linear convergence is also established under strong convexity. Numerical experiments confirm the practical benefits of this new FLECS algorithm.},
  archive      = {J_OMS},
  author       = {A. Agafonov and D. Kamzolov and R. Tappenden and A. Gasnikov and M. Takáč},
  doi          = {10.1080/10556788.2025.2456118},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1222-1248},
  shortjournal = {Optim. Methods Softw.},
  title        = {FLECS: A federated learning second-order framework via compression and sketching},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The mirror-prox sliding method for non-smooth decentralized saddle-point problems. <em>OMS</em>, <em>40</em>(5), 1197-1221. (<a href='https://doi.org/10.1080/10556788.2024.2396295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The saddle-point optimization problems have a lot of practical applications. This paper focuses on such non-smooth convex-concave problems in a decentralized case. This work is based on recently proposed sliding for centralized variational inequalities. Through a specific penalization method and this sliding, we obtain an algorithm for non-smooth decentralized saddle-point problems. The proposed method approaches lower bounds for several communication rounds and calls of (sub-)gradient per node. As a result, we obtain the convergence of sliding for some non-smooth variational inequalities.},
  archive      = {J_OMS},
  author       = {Ilya Kuruzov and Alexander Rogozin and Demyan Yarmoshik and Alexander Gasnikov},
  doi          = {10.1080/10556788.2024.2396295},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1197-1221},
  shortjournal = {Optim. Methods Softw.},
  title        = {The mirror-prox sliding method for non-smooth decentralized saddle-point problems},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed learning with compressed gradient differences. <em>OMS</em>, <em>40</em>(5), 1181-1196. (<a href='https://doi.org/10.1080/10556788.2024.2358790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training large machine learning models requires a distributed computing approach, with communication of the model updates being the bottleneck. For this reason, several methods based on the compression of updates were recently proposed using sparsification or quantization. However, none of the prior methods are able to learn the gradients, which renders them incapable of converging to the true optimum in the batch mode. In this work we propose a new method – DIANA – which resolves this issue via compression of gradient differences . We provide theory in the strongly convex and nonconvex settings that shows improved convergence rates, and use it to obtain the first convergence rate for the previously proposed method TernGrad . Finally, we provide theory to support non-smooth regularizers.},
  archive      = {J_OMS},
  author       = {K. Mishchenko and E. Gorbunov and M. Takáč and P. Richtárik},
  doi          = {10.1080/10556788.2024.2358790},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1181-1196},
  shortjournal = {Optim. Methods Softw.},
  title        = {Distributed learning with compressed gradient differences},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized gradient tracking with local steps. <em>OMS</em>, <em>40</em>(5), 1153-1180. (<a href='https://doi.org/10.1080/10556788.2024.2322095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient tracking (GT) is an algorithm designed for solving decentralized optimization problems over a network (such as training a machine learning model). A key feature of GT is a tracking mechanism that allows us to overcome data heterogeneity between nodes. We develop a novel decentralized tracking mechanism, K -GT, which enables communication-efficient local updates in GT while inheriting the data-independence property of GT. We prove a convergence rate for K -GT on smooth non-convex functions and prove that it reduces the communication overhead asymptotically by a linear factor K , where K denotes the number of local steps. We illustrate the robustness and effectiveness of this heterogeneity correction on convex and non-convex benchmark problems and a non-convex neural network training task with the MNIST dataset.},
  archive      = {J_OMS},
  author       = {Yue Liu and Tao Lin and Anastasia Koloskova and Sebastian U. Stich},
  doi          = {10.1080/10556788.2024.2322095},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1153-1180},
  shortjournal = {Optim. Methods Softw.},
  title        = {Decentralized gradient tracking with local steps},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized saddle point problems via non-euclidean mirror prox. <em>OMS</em>, <em>40</em>(5), 1127-1152. (<a href='https://doi.org/10.1080/10556788.2023.2280062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider smooth convex-concave saddle point problems in the decentralized distributed setting, where a finite-sum objective is distributed among the nodes of a computational network. At each node, the local objective depends on the groups of local and global variables. For such problems, we propose a decentralized distributed algorithm with O ( ϵ − 1 ) communication and oracle calls complexities to achieve accuracy ε in terms of the duality gap and in terms of consensus between nodes. Further, we prove lower bounds for the communication and oracle calls complexities and show that our algorithm matches these bounds, i.e. it is optimal. In contrast to existing decentralized algorithms, our algorithm admits non-euclidean proximal setup, including, e.g. entropic. We illustrate the work of the proposed algorithm on the prominent problem of computing Wasserstein barycenters (WB), where a non-euclidean proximal setup arises naturally in a bilinear saddle point reformulation of the WB problem.},
  archive      = {J_OMS},
  author       = {Alexander Rogozin and Aleksandr Beznosikov and Darina Dvinskikh and Dmitry Kovalev and Pavel Dvurechensky and Alexander Gasnikov},
  doi          = {10.1080/10556788.2023.2280062},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1127-1152},
  shortjournal = {Optim. Methods Softw.},
  title        = {Decentralized saddle point problems via non-euclidean mirror prox},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PersA-FL: Personalized asynchronous federated learning. <em>OMS</em>, <em>40</em>(5), 1089-1126. (<a href='https://doi.org/10.1080/10556788.2023.2280056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the personalized federated learning problem under asynchronous updates. In this problem, each client seeks to obtain a personalized model that simultaneously outperforms local and global models. We consider two optimization-based frameworks for personalization: (i) Model-Agnostic Meta-Learning ( MAML ) and (ii) Moreau Envelope ( ME ). MAML involves learning a joint model adapted for each client through fine-tuning, whereas ME requires a bi-level optimization problem with implicit gradients to enforce personalization via regularized losses. We focus on improving the scalability of personalized federated learning by removing the synchronous communication assumption. Moreover, we extend the studied function class by removing boundedness assumptions on the gradient norm. Our main technical contribution is a unified proof for asynchronous federated learning with bounded staleness that we apply to MAML and ME personalization frameworks. For the smooth and non-convex functions class, we show the convergence of our method to a first-order stationary point. We illustrate the performance of our method and its tolerance to staleness through experiments for classification tasks over heterogeneous datasets.},
  archive      = {J_OMS},
  author       = {Mohammad Taha Toghani and Soomin Lee and César A. Uribe},
  doi          = {10.1080/10556788.2023.2280056},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1089-1126},
  shortjournal = {Optim. Methods Softw.},
  title        = {PersA-FL: Personalized asynchronous federated learning},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay-tolerant distributed bregman proximal algorithms. <em>OMS</em>, <em>40</em>(5), 1072-1088. (<a href='https://doi.org/10.1080/10556788.2023.2278089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many problems in machine learning write as the minimization of a sum of individual loss functions over the training examples. These functions are usually differentiable but, in some cases, their gradients are not Lipschitz continuous, which compromises the use of (proximal) gradient algorithms. Fortunately, changing the geometry and using Bregman divergences can alleviate this issue in several applications, such as for Poisson linear inverse problems. However, the Bregman operation makes the aggregation of several points and gradients more involved, hindering the distribution of computations for such problems. In this paper, we propose an asynchronous variant of the Bregman proximal-gradient method, able to adapt to any centralized computing system. In particular, we prove that the algorithm copes with arbitrarily long delays and we illustrate its behaviour on distributed Poisson inverse problems.},
  archive      = {J_OMS},
  author       = {S. Chraibi and F. Iutzeler and J. Malick and A. Rogozin},
  doi          = {10.1080/10556788.2023.2278089},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1072-1088},
  shortjournal = {Optim. Methods Softw.},
  title        = {Delay-tolerant distributed bregman proximal algorithms},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AB/Push-pull method for distributed optimization in time-varying directed networks. <em>OMS</em>, <em>40</em>(5), 1044-1071. (<a href='https://doi.org/10.1080/10556788.2023.2261602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the distributed optimization problem for a system of agents embedded in time-varying directed communication networks. Each agent has its own cost function and agents cooperate to determine the global decision that minimizes the summation of all individual cost functions. We consider the so-called push-pull gradient-based algorithm (termed as AB /Push-Pull) which employs both row- and column-stochastic weights simultaneously to track the optimal decision and the gradient of the global cost while ensuring consensus and optimality. We show that the algorithm converges linearly to the optimal solution over a time-varying directed network for a constant stepsize when the agent's cost function is smooth and strongly convex. The linear convergence of the method has been shown in [F. Saadatniaki, R. Xin, and U.A. Khan, Decentralized optimization over time-varying directed graphs with row and column-stochastic matrices , IEEE Trans. Autom. Control 65(11) (2020), pp. 4769–4780], where the multi-step consensus contraction parameters for row- and column-stochastic mixing matrices are not directly related to the underlying graph structure, and the explicit range for the stepsize value is not provided. With respect to [F. Saadatniaki, R. Xin, and U.A. Khan, Decentralized optimization over time-varying directed graphs with row and column-stochastic matrices , IEEE Trans. Autom. Control 65(11) (2020), pp. 4769–4780], the novelty of this work is twofold: (1) we establish the one-step consensus contraction for both row- and column-stochastic mixing matrices with the contraction parameters given explicitly in terms of the graph diameter and other graph properties; and (2) we provide explicit upper bounds for the stepsize value in terms of the properties of the cost functions, the mixing matrices, and the graph connectivity structure.},
  archive      = {J_OMS},
  author       = {Angelia Nedić and Duong Thuy Anh Nguyen and Duong Tung Nguyen},
  doi          = {10.1080/10556788.2023.2261602},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1044-1071},
  shortjournal = {Optim. Methods Softw.},
  title        = {AB/Push-pull method for distributed optimization in time-varying directed networks},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of local steps in local SGD. <em>OMS</em>, <em>40</em>(5), 1017-1043. (<a href='https://doi.org/10.1080/10556788.2023.2241151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the distributed stochastic optimization problem where n agents want to minimize a global function given by the sum of agents' local functions and focus on the heterogeneous setting when agents' local functions are defined over non-i.i.d. datasets. We study the Local SGD method, where agents perform a number of local stochastic gradient steps and occasionally communicate with a central node to improve their local optimization tasks. We analyze the effect of local steps on the convergence rate and the communication complexity of Local SGD. In particular, instead of assuming a fixed number of local steps across all communication rounds, we allow the number of local steps during the j th communication round, H j , to be different and arbitrary numbers. Our main contribution is to characterize the convergence rate of Local SGD as a function of { H j } j = 1 R under various settings of strongly convex, convex, and nonconvex local functions, where R is the total number of communication rounds. Based on this characterization, we provide sufficient conditions on the sequence { H j } j = 1 R such that Local SGD can achieve linear speedup with respect to the number of workers. Furthermore, we propose a new communication strategy with increasing local steps that is superior to constant local steps for strongly convex local functions. On the other hand, for convex and nonconvex local functions, we argue that fixed local steps are the best communication strategy for Local SGD and recover state-of-the-art convergence rate results. Finally, we justify our theoretical results through extensive numerical experiments.},
  archive      = {J_OMS},
  author       = {Tiancheng Qin and S. Rasoul Etesami and César A. Uribe},
  doi          = {10.1080/10556788.2023.2241151},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1017-1043},
  shortjournal = {Optim. Methods Softw.},
  title        = {The role of local steps in local SGD},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preface for the special issue advances in distributed optimization. <em>OMS</em>, <em>40</em>(5), 1015-1016. (<a href='https://doi.org/10.1080/10556788.2025.2555028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OMS},
  author       = {Alexander Gasnikov and Angelia Nedich},
  doi          = {10.1080/10556788.2025.2555028},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1015-1016},
  shortjournal = {Optim. Methods Softw.},
  title        = {Preface for the special issue advances in distributed optimization},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction. <em>OMS</em>, <em>40</em>(4), 1014. (<a href='https://doi.org/10.1080/10556788.2025.2516933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OMS},
  doi          = {10.1080/10556788.2025.2516933},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {1014},
  shortjournal = {Optim. Methods Softw.},
  title        = {Correction},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exponential convergence rates of a second-order dynamic system and algorithm for a linear equality constrained optimization problem. <em>OMS</em>, <em>40</em>(4), 977-1013. (<a href='https://doi.org/10.1080/10556788.2025.2517174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The O ( 1 / t 2 ) convergence rate of second-order dynamical systems with asymptotic vanishing viscous damping is faster than the O ( 1 / t ) rate of systems with fixed viscous damping in unconstrained and linear equality constrained optimization problems. We explore whether the performance of systems with vanishing viscous damping remains superior when both use time scaling. We compare the best polynomial convergence rates of vanishing damping systems with the best exponential convergence rates of a fixed damping system with time scaling for linear equality constrained problems. We prove that the primal-dual trajectory weakly converges to an optimal solution. Additionally, we present an inertial algorithm derived from the implicit discretization of the dynamical system, establishing exponential convergence rates for the primal-dual gap, feasibility measure, and objective value without assuming strong convexity. The sequence of iterates generated by the inertial algorithm weakly converges to an optimal solution when the objective function is proper, convex, and lower semicontinuous. These results align with those in the continuous setting.},
  archive      = {J_OMS},
  author       = {Ke-wei Ding and Lingling Liu and Phan Tu Vuong},
  doi          = {10.1080/10556788.2025.2517174},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {977-1013},
  shortjournal = {Optim. Methods Softw.},
  title        = {Exponential convergence rates of a second-order dynamic system and algorithm for a linear equality constrained optimization problem},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A proximal-gradient inertial algorithm with tikhonov regularization: Strong convergence to the minimal norm solution. <em>OMS</em>, <em>40</em>(4), 947-976. (<a href='https://doi.org/10.1080/10556788.2025.2517172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the strong convergence properties of a proximal-gradient inertial algorithm with two Tikhonov regularization terms in connection with the minimization problem of the sum of a convex lower semi-continuous function f and a smooth convex function g . For the appropriate setting of the parameters, we provide the strong convergence of the generated sequence ( x k ) k ≥ 0 to the minimum norm minimizer of our objective function f + g . Further, we obtain fast convergence to zero of the objective function values in a generated sequence but also for the discrete velocity and the sub-gradient of the objective function. We also show that for another setting of the parameters the optimal rate of order O ( k − 2 ) for the potential energy ( f + g ) ( x k ) − min ( f + g ) can be obtained.},
  archive      = {J_OMS},
  author       = {Szilárd Csaba László},
  doi          = {10.1080/10556788.2025.2517172},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {947-976},
  shortjournal = {Optim. Methods Softw.},
  title        = {A proximal-gradient inertial algorithm with tikhonov regularization: Strong convergence to the minimal norm solution},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach for solving a class of diffusion identification problems. <em>OMS</em>, <em>40</em>(4), 920-946. (<a href='https://doi.org/10.1080/10556788.2025.2506176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach to the formulation and solution of a class of elliptic diffusion identification problems in the framework of the Pontryagin maximum principle (PMP) is investigated. The proposed approach considers twice continuously differentiable diffusion coefficients defined as the convolution with a square-integrable optimization function, which allows to prove the PMP by spike variation and to construct and analyze an efficient PMP-based iterative algorithm that efficiently solves diffusion identification problems approximated by finite elements.},
  archive      = {J_OMS},
  author       = {Ştefana-Lucia Aniţa and Alfio Borzì},
  doi          = {10.1080/10556788.2025.2506176},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {920-946},
  shortjournal = {Optim. Methods Softw.},
  title        = {A novel approach for solving a class of diffusion identification problems},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-point feedback for composite optimization with applications to distributed and federated learning. <em>OMS</em>, <em>40</em>(4), 904-919. (<a href='https://doi.org/10.1080/10556788.2025.2502829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is devoted to solving the composite optimization problem with the mixture oracle: for the smooth part of the problem, we have access to the gradient, and for the non-smooth part, only the one-point zero-order oracle is available. For such a setup, we present a new method based on the sliding algorithm. Our method allows to separate the oracle complexities and to compute the gradient for one of the functions as rarely as possible. This paper also presents the applicability of our new method to the problems of distributed optimization and federated learning. Experimental results confirm the theory.},
  archive      = {J_OMS},
  author       = {Aleksandr Beznosikov and Ivan Stepanov and Artyom Voronov and Alexander Gasnikov},
  doi          = {10.1080/10556788.2025.2502829},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {904-919},
  shortjournal = {Optim. Methods Softw.},
  title        = {One-point feedback for composite optimization with applications to distributed and federated learning},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S2MPJ and CUTEst optimization problems for matlab, python and julia. <em>OMS</em>, <em>40</em>(4), 871-903. (<a href='https://doi.org/10.1080/10556788.2025.2490640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new suite of test problems for optimization is presented, which contains a large fraction of the problems in the CUTEst collection. The problems are supplied in the form of Matlab, Python and Julia files allowing the computation of values and derivatives of the objective function and constraints directly within ‘native’ Matlab, Python or Julia, without any additional installation or interfacing with MEX files or Fortran programs. These files are produced by a new decoder (written in Matlab) for the original SIF descriptions in the CUTEst collection. When used within Matlab, the new problem files optionally support reduced-precision computations.},
  archive      = {J_OMS},
  author       = {S. Gratton and Ph. L. Toint},
  doi          = {10.1080/10556788.2025.2490640},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {871-903},
  shortjournal = {Optim. Methods Softw.},
  title        = {S2MPJ and CUTEst optimization problems for matlab, python and julia},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum computing and the stable set problem. <em>OMS</em>, <em>40</em>(4), 837-870. (<a href='https://doi.org/10.1080/10556788.2025.2490639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an undirected graph, the stable set problem asks to determine the cardinality of the largest subset of pairwise non-adjacent vertices. This value is called the stability number of the graph, and its computation is an NP-hard problem. In this paper, we solve the stable set problem using the D-Wave quantum annealer. By formulating the problem as a quadratic unconstrained binary optimization problem with the penalty method, we show its optimal value equals the graph's stability number for specific penalty values. However, D-Wave's quantum annealer is a heuristic, so the solutions may be far from the optimum and may not represent stable sets. To address these, we introduce a post-processing procedure that identifies samples that could lead to improved solutions. Additionally, we propose a partitioning method to handle larger instances that cannot be embedded on D-Wave's quantum processing unit. Finally, we investigate how different penalty parameter values affect the solutions' quality. Extensive computational results show that the post-processing procedure significantly improves the solution quality, while the partitioning method successfully extends our approach to medium-size instances.},
  archive      = {J_OMS},
  author       = {Aljaž Krpan and Janez Povh and Dunja Pucher},
  doi          = {10.1080/10556788.2025.2490639},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {837-870},
  shortjournal = {Optim. Methods Softw.},
  title        = {Quantum computing and the stable set problem},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bundle trust-region algorithm for nonsmooth nonconvex constrained optimization. <em>OMS</em>, <em>40</em>(4), 813-836. (<a href='https://doi.org/10.1080/10556788.2025.2475518'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an algorithm based on the idea of the bundle trust-region method to solve nonsmooth nonconvex constrained optimization problems. The resulting algorithm inherits some attractive features from both bundle and trust-region methods. Moreover, it allows effective control of the size of trust-region subproblems via the compression and aggregation techniques of bundle methods. On the other hand, the trust-region strategy is used to manage the search region and accept a candidate point as a new successful iterate. Global convergence of the developed algorithm is studied under some mild assumptions and its encouraging preliminary computational results are reported.},
  archive      = {J_OMS},
  author       = {N. Hoseini Monjezi and S. Nobakhtian},
  doi          = {10.1080/10556788.2025.2475518},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {813-836},
  shortjournal = {Optim. Methods Softw.},
  title        = {A bundle trust-region algorithm for nonsmooth nonconvex constrained optimization},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soft quasi-newton: Guaranteed positive definiteness by relaxing the secant constraint. <em>OMS</em>, <em>40</em>(4), 783-812. (<a href='https://doi.org/10.1080/10556788.2025.2475406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel algorithm, termed soft quasi-Newton (soft QN), for optimization in the presence of bounded noise. Traditional quasi-Newton algorithms are vulnerable to such noise-induced perturbations. To develop a more robust quasi-Newton method, we replace the secant condition in the matrix optimization problem for the Hessian update with a penalty term in its objective and derive a closed-form update formula. A key feature of our approach is its ability to maintain positive definiteness of the Hessian inverse approximation throughout the iterations. Furthermore, we establish the following properties of soft QN: it recovers the BFGS method under specific limits, it treats positive and negative curvature equally, and it is scale invariant. Collectively, these features enhance the efficacy of soft QN in noisy environments. For strongly convex objective functions and Hessian approximations obtained using soft QN, we develop an algorithm that exhibits linear convergence toward a neighborhood of the optimal solution even when gradient and function evaluations are subject to bounded perturbations. Through numerical experiments, we demonstrate that soft QN consistently outperforms state-of-the-art methods across a range of scenarios.},
  archive      = {J_OMS},
  author       = {Erik Berglund and Jiaojiao Zhang and Mikael Johansson},
  doi          = {10.1080/10556788.2025.2475406},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {783-812},
  shortjournal = {Optim. Methods Softw.},
  title        = {Soft quasi-newton: Guaranteed positive definiteness by relaxing the secant constraint},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proximal subgradient method for non-lipschitz objective functions. <em>OMS</em>, <em>40</em>(4), 755-782. (<a href='https://doi.org/10.1080/10556788.2025.2475405'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a convergence analysis framework of the proximal subgradient method for optimization problems involving non-Lipschitz continuous objective functions. In the conventional analysis of the various subgradient methods, including the proximal subgradient method, the Lipschitz continuity assumption has been placed to guarantee the boundedness of the subgradient and derive the convergence rate. However, the Lipschitz continuity does not hold in practical problems, including the sum-of- ℓ 2 -norms (SO ℓ 2 N) optimal control problem, which is examined in the numerical experiments of this paper. Without the Lipschitz continuity assumption, this paper provides the convergence analysis for strongly convex and non-strongly convex objective functions under mild assumptions. Suitable stepsize rules and resulting convergence rates are established; non-strongly convex cases result in a rate close to the rate of the existing subgradient method, and strongly convex cases achieve the same rate as the existing convergence analysis.},
  archive      = {J_OMS},
  author       = {Mitsuru Toyoda and Mirai Tanaka},
  doi          = {10.1080/10556788.2025.2475405},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {755-782},
  shortjournal = {Optim. Methods Softw.},
  title        = {Proximal subgradient method for non-lipschitz objective functions},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A modified polak-ribière-polyak type conjugate gradient method for vector optimization. <em>OMS</em>, <em>40</em>(4), 725-754. (<a href='https://doi.org/10.1080/10556788.2025.2475402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a modified Polak-Ribière-Polyak conjugate gradient method for solving vector optimization problems. Unlike the existing methods, it is not necessary to use the inner loop with sufficient accurate line search to generate the descent direction at each iteration. Moreover, it does not ignore the fact that the proposed algorithm may still generate the descent direction when the conjugate parameter is negative. We prove that the generated direction is sufficiently descent independent of any line search or convexity. Under the standard Wolfe line search, we also prove the global convergence of the modified Polak-Ribière-Polyak conjugate gradient method without restart or convexity assumption. Finally, through numerical experiments and comparative analysis with the existing methods, we validate the numerical performance of the proposed algorithm.},
  archive      = {J_OMS},
  author       = {Qingjie Hu and Yanyan Zhang and Ruyun Li and Zhibin Zhu},
  doi          = {10.1080/10556788.2025.2475402},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {725-754},
  shortjournal = {Optim. Methods Softw.},
  title        = {A modified polak-ribière-polyak type conjugate gradient method for vector optimization},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction. <em>OMS</em>, <em>40</em>(3), 723. (<a href='https://doi.org/10.1080/10556788.2025.2460329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OMS},
  doi          = {10.1080/10556788.2025.2460329},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {723},
  shortjournal = {Optim. Methods Softw.},
  title        = {Correction},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scaled alternating multiplier method for solving the matrix equation AXB+CXD=E. <em>OMS</em>, <em>40</em>(3), 696-722. (<a href='https://doi.org/10.1080/10556788.2025.2463980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we use the scaled alternating direction multiplier method (SADMM) and its variant form to obtain the least squares solution of the generalized Sylvester matrix equation. Unlike currently ADMM algorithms for solving matrix equations, we consider the effect of the penalty coefficients of the augmented Lagrangian function on the convergence of the SADMM algorithm and give a way to adjust the values of the penalty coefficients. Optimality conditions and termination criteria for the SADMM algorithm are given. Two types of convergence analyses under different assumptions are also given and it is shown that the SADMM algorithm converges for any initial matrix under satisfying the assumptions. The simplicity and effectiveness of the new method are shown by the reported numerical results.},
  archive      = {J_OMS},
  author       = {Changfeng Ma and Shihai Li},
  doi          = {10.1080/10556788.2025.2463980},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {696-722},
  shortjournal = {Optim. Methods Softw.},
  title        = {Scaled alternating multiplier method for solving the matrix equation AXB+CXD=E},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cone-compactness of a set and related topological properties: Stability issues and applications. <em>OMS</em>, <em>40</em>(3), 675-695. (<a href='https://doi.org/10.1080/10556788.2025.2459841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study several stability issues concerning the property of cone-compactness of a set and other related topological properties. We develop this investigation over four types of questions: stability of cone-compactness through mappings, a generalized form of Ky Fan lemma in a fully conic setting, stability of cone-Lipschitzness under subtraction, the behaviour of certain types of minimality in set optimization in the presence of conic topological properties.},
  archive      = {J_OMS},
  author       = {Marius Durea and Elena-Andreea Florea},
  doi          = {10.1080/10556788.2025.2459841},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {675-695},
  shortjournal = {Optim. Methods Softw.},
  title        = {Cone-compactness of a set and related topological properties: Stability issues and applications},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A first-order regularized approach to the order-value optimization problem. <em>OMS</em>, <em>40</em>(3), 650-674. (<a href='https://doi.org/10.1080/10556788.2025.2453111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimization of the order-value function is part of a large family of problems involving functions whose value is calculated by sorting values from a set or subset of other functions. The order-value function has as particular cases the minimum and maximum functions of a set of functions and is well suited for applications involving robust estimation. In this paper, a first-order method with quadratic regularization to solve the problem of minimizing the order-value function is proposed. An optimality condition for the problem and theoretical results of iteration complexity and evaluation complexity for the proposed method are presented. The applicability of the problem and method to parameter estimation problems with outliers is illustrated.},
  archive      = {J_OMS},
  author       = {G. Q. Álvarez and E. G. Birgin},
  doi          = {10.1080/10556788.2025.2453111},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {650-674},
  shortjournal = {Optim. Methods Softw.},
  title        = {A first-order regularized approach to the order-value optimization problem},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interior point method for nonlinear constrained derivative-free optimization. <em>OMS</em>, <em>40</em>(3), 611-649. (<a href='https://doi.org/10.1080/10556788.2025.2453110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider constrained optimization problems where both the objective and constraint functions are of the black-box type. Furthermore, we assume that the nonlinear inequality constraints are non-relaxable, i.e. the values of the objective function and constraints cannot be computed outside of the feasible region. This situation happens frequently in practice especially in the black-box setting where function values are typically computed by means of complex simulation programs which may fail to execute if the considered point is outside of the feasible region. For such problems, we propose a new derivative-free optimization method which is based on the use of a merit function that handles inequality constraints by means of a log-barrier approach and equality constraints by means of an exterior penalty approach. We prove the convergence of the proposed method to KKT stationary points of the problem under standard assumptions (we do not require any convexity assumption). Furthermore, we also carry out a preliminary numerical experience on standard test problems and comparison with state-of-the-art solvers showing the efficiency of the proposed method.},
  archive      = {J_OMS},
  author       = {A. Brilli and G. Liuzzi and S. Lucidi},
  doi          = {10.1080/10556788.2025.2453110},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {611-649},
  shortjournal = {Optim. Methods Softw.},
  title        = {An interior point method for nonlinear constrained derivative-free optimization},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Line-search based optimization using function approximations with tunable accuracy. <em>OMS</em>, <em>40</em>(3), 579-610. (<a href='https://doi.org/10.1080/10556788.2024.2436192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a line-search algorithm that uses objective function models with tunable accuracy to solve smooth optimization problems with convex constraints. The evaluation of objective function and its gradient is potentially computationally expensive, but it is assumed that one can construct effective, computationally inexpensive models. This paper specifies how these models can be used to generate new iterates. At each iteration, the model has to satisfy function error and relative gradient error tolerances determined by the algorithm based on its progress. Moreover, a bound for the model error is used to explore regions where the model is sufficiently accurate. The algorithm has the same first-order global convergence properties as standard line-search methods, but only uses the models and the model error bounds. The algorithm is applied to problems where the evaluation of the objective requires the solution of a large-scale system of nonlinear equations. The models are constructed from reduced order models of this system. Numerical results for partial differential equation constrained optimization problems show the benefits of the proposed algorithm.},
  archive      = {J_OMS},
  author       = {Dane S. Grundvig and Matthias Heinkenschloss},
  doi          = {10.1080/10556788.2024.2436192},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {579-610},
  shortjournal = {Optim. Methods Softw.},
  title        = {Line-search based optimization using function approximations with tunable accuracy},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Some upgrading problems on networks related to eccentricity concept. <em>OMS</em>, <em>40</em>(3), 557-578. (<a href='https://doi.org/10.1080/10556788.2024.2436188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses optimization problems related to the concept of eccentricity in a network, specifically focussing on minimizing radius, diameter, and average eccentricity within a given budget. We refer to these problems as the upgrading radius problem, upgrading diameter problem, and upgrading average eccentricity problem, respectively. We prove that all of these problems are NP -hard in general networks. Additionally, we propose an O ( n 3 ) algorithm to solve the upgrading average eccentricity problem when the underlying graph is a tree. Furthermore, we demonstrate that the upgrading radius and upgrading diameter problems can also be solved in polynomial time for trees. Keywords: Eccentric vertex - Average eccentricity–Complexity–Trees-radius-diameter.},
  archive      = {J_OMS},
  author       = {Ali Reza Sepasian and Kien Trung Nguyen},
  doi          = {10.1080/10556788.2024.2436188},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {557-578},
  shortjournal = {Optim. Methods Softw.},
  title        = {Some upgrading problems on networks related to eccentricity concept},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NPPro: A newton projection with proportioning solver for quadratic programming with affine constraints. <em>OMS</em>, <em>40</em>(3), 525-556. (<a href='https://doi.org/10.1080/10556788.2024.2436180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The second-order method for the solution of quadratic programming (QP) with affine constraints is introduced in this paper. It belongs to an active-set method family. However, it uses a projection and a proportionality test to speed up active set identification. Matrix factor updates accompany active set changes to decrease the computational load. The development of the algorithm is motivated by problems arising in embedded applications of model predictive control in automotive, particularly in autonomous driving, hybridization, electrification, and system optimization. These applications typically lead to an ill-conditioned problem with heavy changes in an active set of solutions at each sample time. Further, underlying quadratic programming needs to be solved in a short time with limited computational and memory resources. Resistance against ill-conditioning, fast convergence, and low memory footprint are vital attributes of the proposed method. Practical aspects, namely preprocessing or Hessian factor updates, are described together with the proposed numerical method. Benchmark results accentuate the efficiency of the algorithm.},
  archive      = {J_OMS},
  author       = {Pavel Otta and Ondřej Šantin and Vladimír Havlena},
  doi          = {10.1080/10556788.2024.2436180},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {525-556},
  shortjournal = {Optim. Methods Softw.},
  title        = {NPPro: A newton projection with proportioning solver for quadratic programming with affine constraints},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the convergence of newton-type proximal gradient method for multiobjective optimization problems. <em>OMS</em>, <em>40</em>(3), 509-524. (<a href='https://doi.org/10.1080/10556788.2024.2436179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Ansary ( Optim. Methods Softw. 38(2023), pp. 570–590) proposed a Newton-type proximal gradient method for nonlinear multiobjective optimization problems (NPGMO). However, the favourable convergence properties typically associated with Newton-type methods for NPGMO were not established in Ansary's work. In response to this gap, we develop a new analytic framework, rooted in an auxiliary inequality, to analyse the convergence behaviour of NPGMO. Specifically, we establish the quadratic termination of NPGMO for problems with strongly convex quadratic smooth terms. Under the assumption of strong convexity, we demonstrate that the NPGMO enjoys superlinear convergence, and quadratic convergence for problems that involve twice continuously differentiable, and twice Lipschitz continuously differentiable smooth terms, respectively. The novel study on the analytic framework simplifies the existing convergence analysis of second-order methods for multiobjective optimization problems.},
  archive      = {J_OMS},
  author       = {J. Chen and X. X. Jiang and L. P. Tang and X. M. Yang},
  doi          = {10.1080/10556788.2024.2436179},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {509-524},
  shortjournal = {Optim. Methods Softw.},
  title        = {On the convergence of newton-type proximal gradient method for multiobjective optimization problems},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An implementable descent method for nonsmooth multiobjective optimization on riemannian manifolds. <em>OMS</em>, <em>40</em>(3), 487-508. (<a href='https://doi.org/10.1080/10556788.2024.2436173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an implementable descent method for nonsmooth multiobjective optimization problems on complete Riemannian manifolds is proposed. The objective functions are only assumed to be locally Lipschitz continuous instead of convexity used in the existing subgradient method for Riemannian multiobjective optimization. And the constraint manifold is a general manifold rather than some specific manifolds used in the proximal point method. The retraction mapping is introduced to avoid the use of computationally difficult geodesic. A Riemannian version of the necessary condition for Pareto optimality is proposed, which generalized the classical one in Euclidean space to the manifold setting. At every iteration, an acceptable descent direction is obtained by constructing a convex hull of some Riemannian ε -subgradients. And then a Riemannian Armijo-type line search is executed to produce the next iterate. The convergence result is established in the sense that a point satisfying the necessary condition for Pareto optimality can be generated by the algorithm in a finite number of iterations. Finally, some preliminary numerical results are reported, which show that the proposed method is efficient.},
  archive      = {J_OMS},
  author       = {Chunming Tang and Hao He and Jinbao Jian and Miantao Chao},
  doi          = {10.1080/10556788.2024.2436173},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {487-508},
  shortjournal = {Optim. Methods Softw.},
  title        = {An implementable descent method for nonsmooth multiobjective optimization on riemannian manifolds},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two RMIL-type schemes with compressed sensing applications. <em>OMS</em>, <em>40</em>(3), 455-486. (<a href='https://doi.org/10.1080/10556788.2024.2425001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been proven that the conjugate gradient (CG) method by Rivaie, Mustafa, Ismail and Leong (RMIL) (Appl. Math. Comput. 218 (2012) 11323–11332), does not satisfy the sufficient descent condition, and its global convergence is attained only by employing the exact line search (ELS) procedure. In this study, two RMIL-type algorithms are proposed for system of monotone nonlinear equations, where both properties hold regardless of the line search strategy used. Two approximations of the nonnegative parameter that is embedded in search directions of the algorithms are determined by analysing eigenvalues and singular values of their iteration matrices. The new solvers are also ideal for nonsmooth problems. Four recent solvers are employed to show effectiveness of the new methods. Furthermore, the schemes are used to solve some signal and image recovery problems.},
  archive      = {J_OMS},
  author       = {Kabiru Ahmed and Mohammed Yusuf Waziri and Abubakar Sani Halilu and Salisu Murtala},
  doi          = {10.1080/10556788.2024.2425001},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {455-486},
  shortjournal = {Optim. Methods Softw.},
  title        = {Two RMIL-type schemes with compressed sensing applications},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative methods for projection onto the ℓp quasi-norm ball. <em>OMS</em>, <em>40</em>(2), 433-454. (<a href='https://doi.org/10.1080/10556788.2024.2424539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on computing the projection onto the ℓ 𝑝 quasi-norm ball. Motivated by the existing IRBP algorithm, which tackles ℓ 𝑝 quasi-norm ball projection through projections onto a sequence of simple and tractable weighted ℓ 1 norm balls, we propose an innovative and effective iteratively reweighting approach for solving the projection problem. Specifically, our method centres on an ϵ -approximation that yields better approximation accuracy. We propose an innovative non-smooth, yet continuously differentiable approximation to the ℓ p quasi-norm function. Leveraging the concavity of our approximate model, we developed a novel variant of the iterative reweighted ℓ 1 norm ball projection algorithm. Through rigorous analysis, we demonstrated the global convergence properties of our proposed numerical approach. Numerical studies demonstrates the superior computational efficiency of our proposed algorithm. Moreover, our solution method can be further leveraged within current iterative algorithms designed to solve ℓ p quasi-norm ball constrained optimization problems, especially in scenarios where rapid convergence is of utmost importance.},
  archive      = {J_OMS},
  author       = {Qi An and Nana Zhang and Shan Jiang},
  doi          = {10.1080/10556788.2024.2424539},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {433-454},
  shortjournal = {Optim. Methods Softw.},
  title        = {Iterative methods for projection onto the ℓp quasi-norm ball},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An inexact restoration direct multisearch filter approach to multiobjective constrained derivative-free optimization. <em>OMS</em>, <em>40</em>(2), 406-432. (<a href='https://doi.org/10.1080/10556788.2024.2412646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct Multisearch (DMS) is a well-established class of methods for multiobjective derivative-free optimization, where constraints are addressed by an extreme barrier approach, only evaluating feasible points. In this work, we propose the replacement of this extreme barrier approach by a filter strategy, combined with an inexact feasibility restoration step, to address constraints in the DMS framework. The filter approach treats feasibility as an additional component of the objective function that needs to be minimized. The inexact restoration step attempts to generate new feasible points, contributing to prioritize this feasibility, a requirement for the good performance of any filter approach. Theoretical results are provided, analysing the different types of sequences of points generated by the new algorithm, and numerical experiments on a set of nonlinearly constrained biobjective problems are reported, stating the good algorithmic performance of the proposed approach.},
  archive      = {J_OMS},
  author       = {Everton J. Silva and Ana Luísa Custódio},
  doi          = {10.1080/10556788.2024.2412646},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {406-432},
  shortjournal = {Optim. Methods Softw.},
  title        = {An inexact restoration direct multisearch filter approach to multiobjective constrained derivative-free optimization},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobjective optimization by using cutting angle methods and hypervolume criterion. <em>OMS</em>, <em>40</em>(2), 388-405. (<a href='https://doi.org/10.1080/10556788.2024.2405984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We translate a multiobjective optimization problem into a single objective Lipschitz problem by using the hypervolume criterion of the Pareto set. Deterministic global optimization methods allow one to track the whole Pareto front rather than converging to a single non-dominated solution. We augmented an efficient hypervolume computation technique with hypervolume increment strategy, and established Lipschitzianity of the resulting objective function. We used two deterministic Lipschitz optimization methods together with the hypervolume objective and benchmarked them against some state-of-the-art alternative multiobjective optimization methods, establishing the competitiveness of the proposed approach.},
  archive      = {J_OMS},
  author       = {Gleb Beliakov and Longxiang Gao and Yong Xiang and Wanlei Zhou},
  doi          = {10.1080/10556788.2024.2405984},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {388-405},
  shortjournal = {Optim. Methods Softw.},
  title        = {Multiobjective optimization by using cutting angle methods and hypervolume criterion},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor decompositions for count data that leverage stochastic and deterministic optimization. <em>OMS</em>, <em>40</em>(2), 352-387. (<a href='https://doi.org/10.1080/10556788.2024.2401981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest to extend low-rank matrix decompositions to multi-way arrays, or tensors . One fundamental low-rank tensor decomposition is the canonical polyadic decomposition (CPD) . The challenge of fitting a low-rank, nonnegative CPD model to Poisson-distributed count data is of particular interest. Several popular algorithms use local search methods to approximate the maximum likelihood estimator (MLE) of the Poisson CPD model. This work presents two new algorithms that extend state-of-the-art local methods for Poisson CPD. Hybrid GCP-CPAPR combines Generalized Canonical Decomposition (GCP) with stochastic optimization and CP Alternating Poisson Regression (CPAPR), a deterministic algorithm, to increase the probability of converging to the MLE over either method used alone. Restarted CPAPR with SVDrop uses a heuristic based on the singular values of the CPD model unfoldings to identify convergence toward optimizers that are not the MLE and restarts within the feasible domain of the optimization problem, thus reducing overall computational cost when using a multi-start strategy. We provide empirical evidence that indicates our approaches outperform existing methods with respect to converging to the Poisson CPD MLE.},
  archive      = {J_OMS},
  author       = {Jeremy M. Myers and Daniel M. Dunlavy},
  doi          = {10.1080/10556788.2024.2401981},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {352-387},
  shortjournal = {Optim. Methods Softw.},
  title        = {Tensor decompositions for count data that leverage stochastic and deterministic optimization},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discretization and quantification for distributionally robust optimization with decision-dependent ambiguity sets. <em>OMS</em>, <em>40</em>(2), 322-351. (<a href='https://doi.org/10.1080/10556788.2024.2401975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the discrete approximation and the quantitative analysis for a class of the distributionally robust optimization problems with decision-dependent ambiguity sets. We establish the local Lipschitz continuity of the decision-dependent ambiguity set, measured by the Hausdorff distance, under a broad class of metrics known as ζ -structure and the Slater condition. Furthermore, we employ Lagrange duality and first-order growth conditions to derive quantitative analysis for the optimal value and optimal solution. We also examine the application of a classical and widely-used ambiguity set within the theoretical framework of this paper. Finally, we conduct experiments to demonstrate the computational times and variations in the optimal value.},
  archive      = {J_OMS},
  author       = {Manlan Li and Xiaojiao Tong and Hailin Sun},
  doi          = {10.1080/10556788.2024.2401975},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {322-351},
  shortjournal = {Optim. Methods Softw.},
  title        = {Discretization and quantification for distributionally robust optimization with decision-dependent ambiguity sets},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two modified conjugate gradient methods for unconstrained optimization. <em>OMS</em>, <em>40</em>(2), 308-321. (<a href='https://doi.org/10.1080/10556788.2024.2400710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conjugate gradient (CG) methods are a popular class of iterative methods for solving linear systems of equations and nonlinear optimization problems. Motivated by the construction of some modern CG methods, we propose two modified CG methods, named DHS* and DPRP*. Under the strong Wolfe line search (SWLS), the two presented methods are proven to be sufficient descent and globally convergent. Preliminary numerical results show that the DHS* and DPRP* methods are effective for the given test problems.},
  archive      = {J_OMS},
  author       = {Mehamdia Abd Elhamid and Chaib Yacine},
  doi          = {10.1080/10556788.2024.2400710},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {308-321},
  shortjournal = {Optim. Methods Softw.},
  title        = {Two modified conjugate gradient methods for unconstrained optimization},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Superlinear convergence of an interior point algorithm on linear semi-definite feasibility problems. <em>OMS</em>, <em>40</em>(2), 287-307. (<a href='https://doi.org/10.1080/10556788.2024.2400705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the literature, besides the assumption of strict complementarity, superlinear convergence of implementable polynomial-time interior point algorithms using known search directions, namely, the HKM direction, its dual or the NT direction, to solve semi-definite programs (SDPs) is shown by (i) assuming that the given SDP is nondegenerate and making modifications to these algorithms [Kojima et al. Local convergence of predictor–corrector infeasible-interior-point algorithms for SDPs and SDLCPs , Math. Program. Ser. A 80 (1998), pp. 129–160], or (ii) considering special classes of SDPs, such as the class of linear semi-definite feasibility problems (LSDFPs) and requiring the initial iterate to the algorithm to satisfy certain conditions [Sim, Superlinear convergence of an infeasible predictor–corrector path-following interior point algorithm for a semidefinite linear complementarity problem using the Helmberg–Kojima–Monteiro direction , SIAM J. Optim. 21 (2011), pp. 102–126], [Sim, Interior point method on semi-definite linear complementarity problems using the Nesterov-Todd (NT) search direction: Polynomial complexity and local convergence , Comput. Optim. Appl. 74 (2019), pp. 583–621]. Otherwise, these algorithms are not easy to implement even though they are shown to have polynomial iteration complexities and superlinear convergence [Luo et al. Superlinear convergence of a symmetric primal–dual path following algorithm for semidefinite programming , SIAM J. Optim. 8 (1998), pp. 59–81]. The conditions in [Sim, Superlinear convergence of an infeasible predictor–corrector path-following interior point algorithm for a semidefinite linear complementarity problem using the Helmberg–Kojima–Monteiro direction , SIAM J. Optim. 21 (2011), pp. 102–126], [Sim, Interior point method on semi-definite linear complementarity problems using the Nesterov-Todd (NT) search direction: Polynomial complexity and local convergence , Comput. Optim. Appl. 74 (2019), pp. 583–621] that the initial iterate to the algorithm is required to satisfy to have superlinear convergence when solving LSDFPs however are not practical. In this paper, we propose a practical initial iterate to an implementable infeasible interior point algorithm that guarantees superlinear convergence when the algorithm is used to solve the homogeneous feasibility model of an LSDFP.},
  archive      = {J_OMS},
  author       = {Chee-Khian Sim},
  doi          = {10.1080/10556788.2024.2400705},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {287-307},
  shortjournal = {Optim. Methods Softw.},
  title        = {Superlinear convergence of an interior point algorithm on linear semi-definite feasibility problems},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A derivative free projection method for the singularities of vector fields with convex constraints on hadamard manifolds. <em>OMS</em>, <em>40</em>(2), 266-286. (<a href='https://doi.org/10.1080/10556788.2024.2400700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this paper is to introduce a derivative free projection method designed to find the singularities of pseudomonotone vector fields with convex constraints on Hadamard manifolds. This innovative approach combines the hyperplane projection method with a novel search direction. The global convergence of the proposed method is established under certain conditions. Our method improves some existing results in the literature on Hadamard manifolds. Additionally, illustrative numerical examples are provided to demonstrate the practical efficacy of our method.},
  archive      = {J_OMS},
  author       = {D. R. Sahu and Shikher Sharma},
  doi          = {10.1080/10556788.2024.2400700},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {266-286},
  shortjournal = {Optim. Methods Softw.},
  title        = {A derivative free projection method for the singularities of vector fields with convex constraints on hadamard manifolds},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Globalization of convergence of the constrained piecewise Levenberg–Marquardt method. <em>OMS</em>, <em>40</em>(2), 243-265. (<a href='https://doi.org/10.1080/10556788.2024.2400468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop linesearch algorithms intended for globalization of convergence of the piecewise Levenberg–Marquardt method for constrained piecewise smooth equation. Conditions ensuring global convergence properties and asymptotic superlinear convergence rate are proposed. The peculiarities of the global convergence results in the piecewise smooth case are discussed and illustrated by examples. We also provide numerical results for unconstrained and constrained reformulations of nonlinear complementarity problems, comparing the performance of the globalized piecewise Levenberg–Marquardt algorithm with some relevant alternatives.},
  archive      = {J_OMS},
  author       = {Alexey F. Izmailov and Evgeniy I. Uskov and Yan Zhibai},
  doi          = {10.1080/10556788.2024.2400468},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {243-265},
  shortjournal = {Optim. Methods Softw.},
  title        = {Globalization of convergence of the constrained piecewise Levenberg–Marquardt method},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Observability inequality of backward stochastic heat equations with lévy process for measurable sets and its applications. <em>OMS</em>, <em>40</em>(1), 224-242. (<a href='https://doi.org/10.1080/10556788.2024.2381674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper endeavours to directly establish the observability inequality of backward stochastic heat equations involving a Lévy process for measurable sets. As an immediate application, we attain the approximate controllability of forward stochastic heat equations featuring a Lévy process. Subsequently, we embark upon the formulation of a nuanced optimization problem, encompassing both the determination of an optimal actuator location and its associated minimum norm control. This formulation is elegantly cast as a two-person zero-sum game problem. We then proceed to articulate a set of conditions–both sufficient and necessary–required for optimizing the solution through the prism of a Nash equilibrium. At last, we prove that the relaxed optimal solution is an optimal actuator location for the classical problem.},
  archive      = {J_OMS},
  author       = {Yuanhang Liu},
  doi          = {10.1080/10556788.2024.2381674},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {224-242},
  shortjournal = {Optim. Methods Softw.},
  title        = {Observability inequality of backward stochastic heat equations with lévy process for measurable sets and its applications},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical methods for distributed stochastic compositional optimization problems with aggregative structure. <em>OMS</em>, <em>40</em>(1), 192-223. (<a href='https://doi.org/10.1080/10556788.2024.2381214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper studies the distributed stochastic compositional optimization problems over networks, where all the agents' inner-level function is the sum of each agent's private expectation function. Focusing on the aggregative structure of the inner-level function, we employ the hybrid variance reduction method to obtain the information on each agent's private expectation function, and apply the dynamic consensus mechanism to track the information on each agent's inner-level function. Then by combining with the standard distributed stochastic gradient descent method, we propose a distributed aggregative stochastic compositional gradient descent method. When the objective function is smooth, the proposed method achieves the convergence rate O ( K − 1 / 2 ) . We further combine the proposed method with the communication compression and propose the communication compressed variant distributed aggregative stochastic compositional gradient descent method. The compressed variant of the proposed method maintains the convergence rate O ( K − 1 / 2 ) . Simulated experiments on decentralized reinforcement learning verify the effectiveness of the proposed methods.},
  archive      = {J_OMS},
  author       = {Shengchao Zhao and Yongchao Liu},
  doi          = {10.1080/10556788.2024.2381214},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {192-223},
  shortjournal = {Optim. Methods Softw.},
  title        = {Numerical methods for distributed stochastic compositional optimization problems with aggregative structure},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The Dai–Liao-type conjugate gradient methods for solving vector optimization problems. <em>OMS</em>, <em>40</em>(1), 157-191. (<a href='https://doi.org/10.1080/10556788.2024.2380697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper attempts to propose Dai–Liao (DL)-type nonlinear conjugate gradient (CG) methods for solving vector optimization problems. Four variants of the DL method are extended and analysed from the scalar case to the vector setting. We first give a direct vector extended version of the modified Dai–Liao (DL+) method. Then the second algorithm is presented by combining a new extension form of the well-known Hager–Zhang (HZ) parameter. This new scheme equips a good descent property and its parameter also reduces to the classical one in the scalar case. The last two algorithms extend two sufficient descent DL-type methods. Particularly, two different vector forms of their parameters are considered and compared. As a result, we reveal some differences between scalar and vector cases to a certain extent. Without any convex assumption, the sufficient descent property and global convergence of all algorithms are established under inexact line searches. Finally, preliminary numerical experiments illustrate the practical behaviour of our algorithms by using Dolan and Moré performance profile.},
  archive      = {J_OMS},
  author       = {Bo-Ya Zhang and Qing-Rui He and Chun-Rong Chen and Sheng-Jie Li and Ming-Hua Li},
  doi          = {10.1080/10556788.2024.2380697},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {157-191},
  shortjournal = {Optim. Methods Softw.},
  title        = {The Dai–Liao-type conjugate gradient methods for solving vector optimization problems},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neurodynamic approach for a class of pseudoconvex semivectorial bilevel optimization problems. <em>OMS</em>, <em>40</em>(1), 129-156. (<a href='https://doi.org/10.1080/10556788.2024.2380688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article proposes an exact approach to finding the global solution of a nonconvex semivectorial bilevel optimization problem, where the objective functions at each level are pseudoconvex, and the constraints are quasiconvex. Due to its non-convexity, this problem is challenging, but it attracts more and more interest because of its practical applications. The algorithm is developed based on monotonic optimization combined with a recent neurodynamic approach, where the solution set of the lower-level problem is inner approximated by copolyblocks in outcome space. From that, the upper-level problem is solved using the branch-and-bound method. Finding the bounds is converted to pseudoconvex programming problems, which are solved using the neurodynamic method. The algorithm's convergence is proved, and computational experiments are implemented to demonstrate the accuracy of the proposed approach.},
  archive      = {J_OMS},
  author       = {Tran Ngoc Thang and Dao Minh Hoang and Nguyen Viet Dung},
  doi          = {10.1080/10556788.2024.2380688},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {129-156},
  shortjournal = {Optim. Methods Softw.},
  title        = {A neurodynamic approach for a class of pseudoconvex semivectorial bilevel optimization problems},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis and comparison of two-level KFAC methods for training deep neural networks. <em>OMS</em>, <em>40</em>(1), 96-128. (<a href='https://doi.org/10.1080/10556788.2024.2380684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a second-order method, the Natural Gradient Descent (NGD) has the ability to accelerate training of neural networks. However, due to the prohibitive computational and memory costs of computing and inverting the Fisher Information Matrix (FIM), efficient approximations are necessary to make NGD scalable to Deep Neural Networks (DNNs). Many such approximations have been attempted. The most sophisticated of these is KFAC, which approximates the FIM as a block-diagonal matrix, where each block corresponds to a layer of the neural network. By doing so, KFAC ignores the interactions between different layers. In this work, we investigate the interest of restoring some low-frequency interactions between the layers by means of two-level methods. Inspired from domain decomposition, several two-level corrections to KFAC using different coarse spaces are proposed and assessed. The obtained results show that incorporating the layer interactions in this fashion does not really improve the performance of KFAC. This suggests that it is safe to discard the off-diagonal blocks of the FIM, since the block-diagonal approach is sufficiently robust, accurate and economical in computation time.},
  archive      = {J_OMS},
  author       = {Abdoulaye Koroko and Ani Anciaux-Sedrakian and Ibtihel Ben Gharbia and Valérie Garès and Mounir Haddou and Quang Huy Tran},
  doi          = {10.1080/10556788.2024.2380684},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {96-128},
  shortjournal = {Optim. Methods Softw.},
  title        = {Analysis and comparison of two-level KFAC methods for training deep neural networks},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new spectral conjugate subgradient method with application in computed tomography image reconstruction. <em>OMS</em>, <em>40</em>(1), 72-95. (<a href='https://doi.org/10.1080/10556788.2024.2372668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new spectral conjugate subgradient method is presented to solve nonsmooth unconstrained optimization problems. The method combines the spectral conjugate gradient method for smooth problems with the spectral subgradient method for nonsmooth problems. We study the effect of two different choices of line search, as well as three formulas for determining the conjugate directions. In addition to numerical experiments with standard nonsmooth test problems, we also apply the method to several image reconstruction problems in computed tomography, using total variation regularization. Performance profiles are used to compare the performance of the algorithm using different line search strategies and conjugate directions to that of the original spectral subgradient method. Our results show that the spectral conjugate subgradient algorithm outperforms the original spectral subgradient method, and that the use of the Polak–Ribière formula for conjugate directions provides the best and most robust performance.},
  archive      = {J_OMS},
  author       = {M. Loreto and T. Humphries and C. Raghavan and K. Wu and S. Kwak},
  doi          = {10.1080/10556788.2024.2372668},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {72-95},
  shortjournal = {Optim. Methods Softw.},
  title        = {A new spectral conjugate subgradient method with application in computed tomography image reconstruction},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A note on the generalized hessian of the least squares associated with systems of linear inequalities. <em>OMS</em>, <em>40</em>(1), 65-71. (<a href='https://doi.org/10.1080/10556788.2024.2372660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this note is to point out an erroneous formula for the generalized Hessian of the least squares associated with a system of linear inequalities, that was given in the paper ‘A finite Newton method for classification’ by Mangasarian (Optim. Methods Softw. 17 (2002), pp. 913–929) and reproduced multiple times in other publications. We also provide sufficient contiditions for the validity of Mangasarian's formula and show that Slater's condition guarantees that some particular elements from the set defined by Mangasarian belong to the generalized Hessian of the corresponding function.},
  archive      = {J_OMS},
  author       = {M.V. Dolgopolik},
  doi          = {10.1080/10556788.2024.2372660},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {65-71},
  shortjournal = {Optim. Methods Softw.},
  title        = {A note on the generalized hessian of the least squares associated with systems of linear inequalities},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A trust-region scheme for constrained multi-objective optimization problems with superlinear convergence property. <em>OMS</em>, <em>40</em>(1), 24-64. (<a href='https://doi.org/10.1080/10556788.2024.2372303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a numerical approximation method is developed to find approximate solutions to a class of constrained multi-objective optimization problems. All the functions of the problem are not necessarily convex functions. At each iteration of the method, a particular type of subproblem is solved using the trust region technique, and the step is evaluated using the notions of actual reduction and predicted reduction. A non-differentiable l ∞ penalty function restricts the constraint violations. An adaptive BFGS update formula is introduced. Global convergence of the proposed algorithm is established under the Mangasarian-Fromovitz constraint qualification and some mild assumptions. Furthermore, it is justified that the proposed algorithm displays a super-linear convergence rate. Numerical results are provided to show the efficiency of the algorithm in the quality of the approximated Pareto front.},
  archive      = {J_OMS},
  author       = {Nantu Kumar Bisui and Geetanjali Panda},
  doi          = {10.1080/10556788.2024.2372303},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {24-64},
  shortjournal = {Optim. Methods Softw.},
  title        = {A trust-region scheme for constrained multi-objective optimization problems with superlinear convergence property},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the complexity of a quadratic regularization algorithm for minimizing nonsmooth and nonconvex functions. <em>OMS</em>, <em>40</em>(1), 1-23. (<a href='https://doi.org/10.1080/10556788.2024.2368578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of minimizing the function 𝑓 ⁡ ( 𝑥 ) = 𝑔 1 ⁡ ( 𝑥 ) + 𝑔 2 ⁡ ( 𝑥 ) − ℎ ⁡ ( 𝑥 ) over ℝ 𝑛 , where 𝑔 1 is a proper and lower semicontinuous function, g 2 is continuously differentiable with a Hölder continuous gradient and h is a convex function that may be nondifferentiable. This problem has important practical applications but is challenging to solve due to the presence of nonconvexities and nonsmoothness. To address this issue, we propose an algorithm based on a proximal gradient method that uses a quadratic approximation of the function g 2 and a nonconvex regularization term. We show that the number of iterations required to reach our stopping criterion is O ( max { ϵ − β + 1 β , η 2 β ϵ − 2 ( β + 1 ) β } ) . Our approach offers a promising strategy for solving this challenging optimization problem and has potential applications in various fields. Numerical examples are provided to illustrate the theoretical results.},
  archive      = {J_OMS},
  author       = {V. S. Amaral and J. O. Lopes and P. S. M. Santos and G. N. Silva},
  doi          = {10.1080/10556788.2024.2368578},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Optim. Methods Softw.},
  title        = {On the complexity of a quadratic regularization algorithm for minimizing nonsmooth and nonconvex functions},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
