<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MICRO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="micro">MICRO - 45</h2>
<ul>
<li><details>
<summary>
(2025). The scramble after breakthrough. <em>MICRO</em>, <em>45</em>(3), 108-110. (<a href='https://doi.org/10.1109/MM.2025.3567048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2025.3567048},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {108-110},
  shortjournal = {IEEE Micro},
  title        = {The scramble after breakthrough},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sipping matcha of security: A fireside chat with mengjia yan. <em>MICRO</em>, <em>45</em>(3), 103-107. (<a href='https://doi.org/10.1109/MM.2025.3572585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Jianming Tong and Zishen Wan},
  doi          = {10.1109/MM.2025.3572585},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {103-107},
  shortjournal = {IEEE Micro},
  title        = {Sipping matcha of security: A fireside chat with mengjia yan},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of wisconsin alumni research foundation v. Apple—Part IV. <em>MICRO</em>, <em>45</em>(3), 97-102. (<a href='https://doi.org/10.1109/MM.2025.3573578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is the fourth part in a series that reviews the decisions that the district judge and appellate panel made in Wisconsin Alumni Research Foundation v. Apple.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2025.3573578},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {97-102},
  shortjournal = {IEEE Micro},
  title        = {A review of wisconsin alumni research foundation v. Apple—Part IV},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three SoCs in three years: How to get agile. <em>MICRO</em>, <em>45</em>(3), 86-94. (<a href='https://doi.org/10.1109/MM.2025.3534917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We taped out three large system on chips in three years on 22-nm CMOS technology, featuring multiple RISC-V cores, and subsystems for machine learning, Ethernet, SerDes, Low-Power SDRAM, and input–output. We have covered all steps in the flow from specification to sample chips. Ballast, Tackle, and Headsail include 130 M, 12 M, and 340 M transistors and took 12, 10, and nine calendar months. Several persons from seven companies and university contributed to the three chips, and staff ranged from experts to novice master students. This article provides insight into modern fast-paced system-on-chip hardware (HW) development which is important when intellectual properties such as RISC-V processors and security accelerators are evolving rapidly. We achieved Agile development with the following guidelines: intellectual properties elaborated on the go, staff moves along the design flow, interface over instance, and schedule over features.},
  archive      = {J_MICRO},
  author       = {Antti Rautakoura and Timo Hämäläinen and Ari Kulmala},
  doi          = {10.1109/MM.2025.3534917},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {86-94},
  shortjournal = {IEEE Micro},
  title        = {Three SoCs in three years: How to get agile},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FuriosaAI RNGD: A tensor contraction processor for sustainable AI computing. <em>MICRO</em>, <em>45</em>(3), 76-85. (<a href='https://doi.org/10.1109/MM.2025.3551880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern artificial intelligence (AI) workloads require architectures capable of efficiently managing diverse tensor contraction patterns. Traditional approaches based on fixed-size matrix multiplications often fall short in scalability and flexibility. RNGD (pronounced “Renegade”), a second-generation tensor contraction processor, introduces an innovative architecture designed to exploit the parallelism and data locality inherent in tensor computations. Its coarse-grained processing elements (PEs) can operate as a unified large-scale unit or as multiple independent units, providing flexibility for various tensor shapes. Key innovations, such as a circuit switch-based fetch network, input broadcasting, and buffer-based reuse mechanisms, further enhance computational efficiency. RNGD represents a significant advancement in processor architecture, delivering optimized performance and energy efficiency for sustainable computation of next-generation AI workloads.},
  archive      = {J_MICRO},
  author       = {Younggeun Choi and Junyoung Park and Sang Min Lee and Jeseung Yeon and Minho Kim and Changjae Park and Byeongwook Bae and Hyunmin Jeong and Hanjoon Kim and June Paik and Nuno P. Lopes and Sungjoo Yoo},
  doi          = {10.1109/MM.2025.3551880},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {76-85},
  shortjournal = {IEEE Micro},
  title        = {FuriosaAI RNGD: A tensor contraction processor for sustainable AI computing},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The IBM telum II processor. <em>MICRO</em>, <em>45</em>(3), 66-75. (<a href='https://doi.org/10.1109/MM.2025.3563803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IBM Telum II is the latest processor designed specifically for IBM Z’s next-generation mainframe. Designed-for-purpose, Telum II is focused on mission-critical enterprise workloads where performance and sustainability are of the utmost importance and the demand for artificial intelligence acceleration is increasing dramatically. Innovations discussed in this article are the new on-die data processing unit for input/output acceleration, the updated cache, enhancements to the on-chip artificial intelligence accelerator, core improvements, and changes to the off-chip input/output interfaces.},
  archive      = {J_MICRO},
  author       = {Christopher Berry and Michael Becht and Tim Bubb and Howard Haynie and Robert Sonnelitter and Katie Seggerman and Jonathan Hsieh and Edward Malley and Mike Cadigan and Susan M. Eickhoff and Matthias Klein and Craig Walters and Christian G. Zoellin and Cedric Lichtenau},
  doi          = {10.1109/MM.2025.3563803},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {66-75},
  shortjournal = {IEEE Micro},
  title        = {The IBM telum II processor},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing programmable accelerators for sparse tensor algebra. <em>MICRO</em>, <em>45</em>(3), 58-65. (<a href='https://doi.org/10.1109/MM.2025.3556611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has focused on leveraging sparsity in hardware accelerators to improve the efficiency of applications spanning scientific computing to machine learning. Most such prior accelerators are fixed-function, which is insufficient for two reasons. First, applications typically include both dense and sparse components, and second, the algorithms that comprise these applications are constantly evolving. To address these challenges, we designed a programmable accelerator called Onyx for both sparse tensor algebra and dense workloads. Onyx extends a coarse-grained reconfigurable array (CGRA) optimized for dense applications with composable hardware primitives to support arbitrary sparse tensor algebra kernels. In this article, we show that we can further optimize Onyx by adding a small set of hardware features for parallelization that significantly increase both temporal and spatial utilization of the CGRA, reducing runtime by up to 6.2×.},
  archive      = {J_MICRO},
  author       = {Kalhan Koul and Zhouhua Xie and Maxwell Strange and Sai Gautham Ravipati and Bo Wun Cheng and Olivia Hsu and Po-Han Chen and Mark Horowitz and Fredrik Kjolstad and Priyanka Raina},
  doi          = {10.1109/MM.2025.3556611},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {58-65},
  shortjournal = {IEEE Micro},
  title        = {Designing programmable accelerators for sparse tensor algebra},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XiangShan: An open source project for high-performance RISC-V processors meeting industrial-grade standards. <em>MICRO</em>, <em>45</em>(3), 49-57. (<a href='https://doi.org/10.1109/MM.2025.3565285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is widely believed that an open source hardware ecosystem can reduce development costs and lower barriers to innovation. However, developing an open source industrial-grade high-performance processor is a challenging undertaking. For mass adoption, such an IP needs to have an advanced out-of-order microarchitecture for high performance, a robust verification infrastructure for reliable quality, and high configurability to accommodate the myriad use cases. With a best-in-class performance, XiangShan is an open source project for RISC-V processors that fully meets these requirements. To maximize overall efficiency, XiangShan adopted a collaborative hardware development model, partnering with industry on processor design, implementation, and verification. With innovations in Agile development processes and tools, the design of the processors can be evolved, optimized, and verified quickly, ensuring high quality and enabling architectural innovation and rapid commercialization.},
  archive      = {J_MICRO},
  author       = {Kaifan Wang and Jian Chen and Yinan Xu and Zihao Yu and Wei He and Dan Tang and Ninghui Sun and Yungang Bao},
  doi          = {10.1109/MM.2025.3565285},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {49-57},
  shortjournal = {IEEE Micro},
  title        = {XiangShan: An open source project for high-performance RISC-V processors meeting industrial-grade standards},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMD instinct MI300X: A generative AI accelerator and platform architecture. <em>MICRO</em>, <em>45</em>(3), 41-48. (<a href='https://doi.org/10.1109/MM.2025.3552324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AMD Instinct MI300X sets a new benchmark in generative artificial intelligence (AI) acceleration, combining architectural innovation with advanced system integration to tackle the ever-growing demands of modern AI workloads. Featuring a chiplet-based architecture, the MI300X employs the fourth-generation Infinity Fabric, eight-stack HBM3 memory, and CDNA 3 compute cores to deliver unparalleled performance for both inference and training tasks. Additionally, the MI300X is central to the AMD Infinity platform, which offers industry-standard scalability through universal baseboard designs, high-bandwidth interconnectivity, and robust system management features. This article provides a detailed exploration of the MI300X architecture, its Infinity platform integration, and its impact on generative AI applications.},
  archive      = {J_MICRO},
  author       = {Alan Smith and Vamsi Krishna Alla},
  doi          = {10.1109/MM.2025.3552324},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {41-48},
  shortjournal = {IEEE Micro},
  title        = {AMD instinct MI300X: A generative AI accelerator and platform architecture},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intel xeon 6 product family. <em>MICRO</em>, <em>45</em>(3), 31-40. (<a href='https://doi.org/10.1109/MM.2025.3553756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Intel® Xeon 6 product family delivers new degrees of performance and scalability to address a wide variety of deployments across data center, enterprise, networking, and edge. The diversity of workloads, power, performance, and form factor requirements led to Intel’s most advanced modular system on chip (SoC) processor architecture. This modular construction allows the flexibility to optimize each die and build multiple SoCs using the same building blocks. For ultimate versatility, Intel Xeon 6 processors allow for the choice of two different CPU microarchitectures: performance cores and efficient cores. Both core types use a compatible x86 instruction set architecture and a common hardware platform.},
  archive      = {J_MICRO},
  author       = {Michael D. Powell and Patrick Fleming and Venkidesh Iyer Krishna and Naveen Lakkakula and Subhiksha Ravisundar and Praveen Mosur and Arijit Biswas and Pradeep Dubey and Kapil Sood and Andrew Cunningham and Smita Kumar},
  doi          = {10.1109/MM.2025.3553756},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {31-40},
  shortjournal = {IEEE Micro},
  title        = {Intel xeon 6 product family},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMD versal AI edge series gen 2. <em>MICRO</em>, <em>45</em>(3), 22-30. (<a href='https://doi.org/10.1109/MM.2025.3551319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AMD’s Next-Gen Adaptive System on Chip, Versal AI Edge Series Gen 2, is a high-performance, scalable, and customizable platform for a wide array of markets, including automotive (advanced driver assistance systems and autonomous driving), robotics, audio–video broadcast, aerospace and defense, and industrial. The platform is designed to support ISO-26262 ASIL-D and IEC-61508 SIL3 for safety critical applications and was architected considering the needs of embedded vision applications, where the heterogenous adaptive architecture integrates field programmable gate array programmable logic with high-performance multicluster processors, imaging and video processing engines, and a next-generation artificial (AI) engine array with advanced data types. The advanced MX data types of the AI engine enable embedded vision applications to achieve accuracies comparable to FP32 with reduced AI engine array and memory footprint costs.},
  archive      = {J_MICRO},
  author       = {Tomai Knopp and Jeffrey Chu and Sagheer Ahmad},
  doi          = {10.1109/MM.2025.3551319},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {22-30},
  shortjournal = {IEEE Micro},
  title        = {AMD versal AI edge series gen 2},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lunar lake an intel mobile processor: SoC architecture overview (2024). <em>MICRO</em>, <em>45</em>(3), 15-21. (<a href='https://doi.org/10.1109/MM.2025.3558407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lunar Lake (LNL) is the codename for the Core Ultra Series mobile processors designed by Intel, released in September 2024. LNL took ambitious targets to cope with core and graphics performance, performance/watt, battery life, and artificial intelligence compute for an outstanding user experience. To address that, a ground-up architecture was defined. LNL enhanced the partition of cores to performance and efficient clusters with the ability to contain software load to the desired hardware at runtime, it optimized performance cluster with single threaded core, revised the idle state management, reduced frequent CPU wakes, enhanced the memory subsystem power states, and added fine-grain power delivery. LNL added premium capabilities such as memory-on-package, a power management integrated circuit, a powerful neural processing unit, memory-side cache, and total storage encryption for NVMe. LNL architecture scales from 8 W to 30 W+ and supports LPDDR5 frequencies up to 8533 Mhz.},
  archive      = {J_MICRO},
  author       = {Nadav Bonen and Arik Gihon and Leon Polishuk and Yoni Aizik and Yulia Okunev and Tsvika Kurts and Nithiyanandan Bashyam},
  doi          = {10.1109/MM.2025.3558407},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {15-21},
  shortjournal = {IEEE Micro},
  title        = {Lunar lake an intel mobile processor: SoC architecture overview (2024)},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qualcomm oryon CPU in snapdragon x elite: Micro-architecture and design. <em>MICRO</em>, <em>45</em>(3), 8-14. (<a href='https://doi.org/10.1109/MM.2025.3568807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the micro-architecture and design of the Qualcomm custom CPU, named Qualcomm Oryon CPU, that was introduced in 2024 in the Qualcomm Snapdragon X Elite system on a chip for the client computing market. It describes the micro-architecture of the CPU core and its cache and memory subsystem and is illustrative of a modern high-performance CPU with best-in-class energy efficiencies that is designed to be scalable across different product categories and price points.},
  archive      = {J_MICRO},
  author       = {Gerard Williams and Pradeep Kanapathipillai},
  doi          = {10.1109/MM.2025.3568807},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {8-14},
  shortjournal = {IEEE Micro},
  title        = {Qualcomm oryon CPU in snapdragon x elite: Micro-architecture and design},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on hot chips 2024. <em>MICRO</em>, <em>45</em>(3), 6-7. (<a href='https://doi.org/10.1109/MM.2025.3572594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Rob Aitken and Larry Yang},
  doi          = {10.1109/MM.2025.3572594},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot chips 2024},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward disaggregated and heterogenous AI systems. <em>MICRO</em>, <em>45</em>(3), 4-5. (<a href='https://doi.org/10.1109/MM.2025.3575180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2025.3575180},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Toward disaggregated and heterogenous AI systems},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence and the jevons paradox. <em>MICRO</em>, <em>45</em>(2), 118-120. (<a href='https://doi.org/10.1109/MM.2025.3548921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2025.3548921},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {118-120},
  shortjournal = {IEEE Micro},
  title        = {Artificial intelligence and the jevons paradox},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of wisconsin alumni research foundation v. Apple—Part III. <em>MICRO</em>, <em>45</em>(2), 114-117. (<a href='https://doi.org/10.1109/MM.2025.3557487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2025.3557487},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {114-117},
  shortjournal = {IEEE Micro},
  title        = {A review of wisconsin alumni research foundation v. Apple—Part III},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving key-value cache performance with heterogeneous memory tiering: A case study of compute-express-link-based memory expansion. <em>MICRO</em>, <em>45</em>(2), 102-113. (<a href='https://doi.org/10.1109/MM.2024.3358861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute Express Link (CXL) memory brings extra bandwidth and capacity via Peripheral-Component-Interconnect-Express-based memory expansion beyond double-data-rate-based dynamic random-access memory. This article introduces the CXL 2.0 memory expansion solution, which incorporates two parts: 1) a CXL memory expander prototype and 2) the heterogeneous memory software development kit. We demonstrate the feasibility of our CXL memory solution by implementing it on CacheLib, Meta’s general-purpose key-value caching engine. We highlight how our application design and guidelines for CXL memory enable resolving the shortcomings of conventional memory system architectures. Our proposals enable 1) expanding memory bandwidth and capacity or 2) considerable DRAM savings. Evaluation results show that we can achieve a 25% increase in memory bandwidth, up to 15% throughput gain, and a 9% latency reduction. Furthermore, in hybrid cache using nonvolatile memory (NVM), expanding the RAM cache area with CXL memory, which is relatively cheaper than DRAM, enhances the throughput and hit ratio due to reduced NVM input–output.},
  archive      = {J_MICRO},
  author       = {KyungSoo Lee and Sohyun Kim and Joohee Lee and Donguk Moon and Rakie Kim and Honggyu Kim and Hyeongtak Ji and Yunjeong Mun and Youngpyo Joo},
  doi          = {10.1109/MM.2024.3358861},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {102-113},
  shortjournal = {IEEE Micro},
  title        = {Improving key-value cache performance with heterogeneous memory tiering: A case study of compute-express-link-based memory expansion},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible approximate computing for mitigating branch divergence in GPUs. <em>MICRO</em>, <em>45</em>(2), 90-100. (<a href='https://doi.org/10.1109/MM.2024.3504261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphics processing units (GPUs) have become prevalent across various domains due to their high throughput and energy efficiency. A major challenge in GPU computing is branch divergence, which occurs when the control flow paths of concurrently executing threads diverge. Branch divergence significantly degrades performance, as diverged threads are executed in a serialized, time-sliced manner. To mitigate this issue, we propose a novel approximate computing technique for GPUs. Our method approximates the results of diverged threads by selectively terminating their execution. Furthermore, it incorporates a dynamic mechanism that adjusts both the probability of termination and the number of instructions executed before termination, based on user-defined parameters. This technique allows users to balance execution speed and result accuracy, offering greater flexibility in controlling computational quality. Additionally, by changing the approximation aggressiveness at runtime, we can accommodate applications such as virtual reality/augmented reality, where the demand for accuracy changes in real-time.},
  archive      = {J_MICRO},
  author       = {Reoma Matsuo and Yuya Degawa and Hidetsugu Irie and Shuichi Sakai and Ryota Shioya},
  doi          = {10.1109/MM.2024.3504261},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {90-100},
  shortjournal = {IEEE Micro},
  title        = {Flexible approximate computing for mitigating branch divergence in GPUs},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MRCA 2.0: An area-optimized multigrained reconfigurable cryptographic accelerator for securing blockchain-based internet of things systems. <em>MICRO</em>, <em>45</em>(2), 78-89. (<a href='https://doi.org/10.1109/MM.2024.3501313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a multigrained reconfigurable cryptographic accelerator 2.0 (MRCA 2.0), upgrading from the first 8/32/64-bit coarse-grained reconfigurable array, named MRCA, to offer high performance with a smaller area and lower power consumption for blockchain-based Internet of Things systems. To achieve these goals, we propose four innovative ideas: a dual-processing element array (D-PEA) with reduced memories, a synchronous row connection and processing element (PE) architecture, a concatenable crypto arithmetic logic unit, and heterogeneous PEs. Our MRCA 2.0 has been successfully implemented on the TySOM-3A field-programmable gate array platform across 19 different algorithms. Our application-specific integrated circuit experiment on 45-nm CMOS technology shows that MRCA 2.0 reduces area and power consumption by 1.6 and 1.4 times, respectively, compared to the previous MRCA. Compared to related works, MRCA 2.0 outperforms in throughput by 1.63–28.3 times and in energy efficiency by 1.6–3.4 times, while also offering better flexibility in supporting more 64-bit algorithms.},
  archive      = {J_MICRO},
  author       = {Hoai Luan Pham and Vu Trung Duong Le and Tuan Hai Vu and Van Duy Tran and Van Tinh Nguyen and Thi Diem Tran and Yasuhiko Nakashima},
  doi          = {10.1109/MM.2024.3501313},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {78-89},
  shortjournal = {IEEE Micro},
  title        = {MRCA 2.0: An area-optimized multigrained reconfigurable cryptographic accelerator for securing blockchain-based internet of things systems},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A mobile semantic lidar SLAM processor with artificial-intelligence-based 3-D perception and spatiotemporal-aware computing. <em>MICRO</em>, <em>45</em>(2), 67-77. (<a href='https://doi.org/10.1109/MM.2024.3503414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A low-power artificial intelligence (AI)-based semantic lidar simultaneous localization and mapping (SLAM) processor is proposed to expand autonomous driving into emerging mobile robots. It combines point neural network (PNN)-based 3-D segmentation with lidar SLAM to minimize pose errors due to the lack of perception in previous SLAM. The proposed processor is designed with a heterogeneous multicore architecture that utilizes single instruction, multiple data and reconfigurable processing elements to fully support its three main operations: k-nearest neighbor (KNN), PNN, and nonlinear optimization. It accelerates KNN operations with spherical bin partitioning optimized for the distribution of lidar data to eliminate unnecessary search spaces. In addition, the proposed spatiotemporal-aware computing minimizes excessive memory overhead and workload imbalance in KNN and PNN operations. Consequently, fabricated with 28-nm CMOS technology, the processor achieves 8.245 mJ/frame of energy consumption and a maximum performance of 20.7-ms latency, successfully demonstrating real-time semantic lidar SLAM system with 99.86% lower power consumption compared to modern CPU+GPU platforms.},
  archive      = {J_MICRO},
  author       = {Jueun Jung and Seungbin Kim and Bokyoung Seo and Wuyoung Jang and Sangho Lee and Jeongmin Shin and Donghyeon Han and Kyuho Jason Lee},
  doi          = {10.1109/MM.2024.3503414},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {67-77},
  shortjournal = {IEEE Micro},
  title        = {A mobile semantic lidar SLAM processor with artificial-intelligence-based 3-D perception and spatiotemporal-aware computing},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on COOL chips. <em>MICRO</em>, <em>45</em>(2), 65-66. (<a href='https://doi.org/10.1109/MM.2025.3555464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This introduction to the special section on COOL Chips discusses the state-of-the-art low-power, high-speed chips and the challenges facing researchers. It introduces three articles that explore different solutions to reducing power consumption and enhancing chip performance.},
  archive      = {J_MICRO},
  author       = {Ryusuke Egawa and Yasutaka Wada},
  doi          = {10.1109/MM.2025.3555464},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {65-66},
  shortjournal = {IEEE Micro},
  title        = {Special issue on COOL chips},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECP: Improving the accuracy of congesting-packets identification in high-performance interconnection networks. <em>MICRO</em>, <em>45</em>(2), 56-64. (<a href='https://doi.org/10.1109/MM.2025.3527722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interconnection networks are crucial in data centers and supercomputers, ensuring high communication bandwidth and low latency under demanding traffic patterns from data-intensive applications. These patterns can cause congestion, affecting system performance if not addressed efficiently. Current congestion control techniques, like data center quantized congestion notification (DCQCN), struggle to precisely identify which packets cause congestion, leading to false positives. We propose the enhanced congestion point (ECP) mechanism, which accurately identifies congesting packets. ECP monitors packets at the head of switch ingress queues, flagging them as congesting when queue occupancy exceeds a threshold and packet requests are rejected. Additionally, ECP introduces a reevaluation mechanism to cancel the identification of congesting packets if they no longer contribute to congestion after rerouting. We evaluated ECP using a network simulator modeling various configurations and realistic traffic patterns. Results show that ECP significantly improves congestion detection accuracy with a low error margin, enhancing DCQCN performance.},
  archive      = {J_MICRO},
  author       = {Cristina Olmedilla and Jesus Escudero-Sahuquillo and Pedro Javier García and Francisco J. Quiles and Wenhao Sun and Long Yan and Yunping Lyu and Jose Duato},
  doi          = {10.1109/MM.2025.3527722},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {56-64},
  shortjournal = {IEEE Micro},
  title        = {ECP: Improving the accuracy of congesting-packets identification in high-performance interconnection networks},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward a standardized representation for deep learning collective algorithms. <em>MICRO</em>, <em>45</em>(2), 46-55. (<a href='https://doi.org/10.1109/MM.2025.3547363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosion of machine learning (ML) model size has led to its execution on distributed clusters at a large scale. Many works have tried to optimize the process of producing collective algorithms and running collective communications, which act as a bottleneck to distributed ML. However, different works use their own collective algorithm representation, resulting in a fragmented optimization environment. We propose a standardized workflow leveraging a common representation format based on Chakra Execution Trace, a widely used graph-based representation of distributed ML workloads. This common representation enables us to view collective communications at the same level as workload operations and decouple producer and consumer tools, enhancing interoperability and reducing engineering effort. We provide a proof of concept of this standardized workflow by simulating the collective algorithms generated by MSCCLang domain-specific language and the topology-aware collective algorithm synthesizer (TACOS) through the accelerator scaling for training simulator (ASTRA-sim) distributed ML simulator using multiple network configurations.},
  archive      = {J_MICRO},
  author       = {Jinsun Yoo and William Won and Meghan Cowan and Nan Jiang and Benjamin Klenk and Srinivas Sridharan and Tushar Krishna},
  doi          = {10.1109/MM.2025.3547363},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {46-55},
  shortjournal = {IEEE Micro},
  title        = {Toward a standardized representation for deep learning collective algorithms},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OHIO: Enhancing RDMA scalability in alltoall with optimized communication overlap. <em>MICRO</em>, <em>45</em>(2), 36-45. (<a href='https://doi.org/10.1109/MM.2024.3524891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of exascale computers has pushed a new boundary in computing capability, posing performance challenges in parallel programming models on how to exploit such systems efficiently. The message passing interface (MPI) is a dominant model for parallel programming. Among its primitives, MPI_Alltoall is a communication-intensive operation widely employed in numerous applications, yet it remains challenging to optimize. Alltoall algorithms are mainly classified into flat and hierarchical. The hierarchical designs avoid the slowdown of intra/inter-node communication by decoupling them. Hierarchical designs also reduce network congestion by limiting concurrently injected messages. This work demonstrates hierarchical designs also improve connection scalability in remote direct memory access networks. This improvement is attributed to the cache thrashing happening inside network adapters. All of these advantages of hierarchical schemes collectively contribute to the network scalability of Alltoall. We propose and evaluate a network-agnostic design on InfiniBand and Omni-Path clusters, showing benefits at both micro-benchmark and application levels over other MPI libraries.},
  archive      = {J_MICRO},
  author       = {Tu Tran and Goutham Kalikrishna Reddy Kuncham and Bharath Ramesh and Shulei Xu and Hari Subramoni and Dhabaleswar K. DK Panda},
  doi          = {10.1109/MM.2024.3524891},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {36-45},
  shortjournal = {IEEE Micro},
  title        = {OHIO: Enhancing RDMA scalability in alltoall with optimized communication overlap},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified collective communication: A unified library for CPU, GPU, and DPU collectives. <em>MICRO</em>, <em>45</em>(2), 26-35. (<a href='https://doi.org/10.1109/MM.2025.3534638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unified collective communication (UCC) is an application programming interface (API) and library implementation of collective communication operations. The goal of UCC is to provide a unified API and library serving the collective communication needs of various workloads running on a wide variety of system architectures. Particularly, we aim to unify the collective communication interfaces and semantics and provide a common implementation framework for: 1) parallel programming models, deep learning, and I/O; 2) collectives moving data in CPU main memory and device memory (GPU and data processing unit, DPU); and 3) collective operations using software point-to-point and hardware transports. In this article, we present an overview of UCC’s design, interfaces, semantics, and an implementation. We demonstrate UCC’s capabilities through evaluations with microbenchmarks representing diverse workloads and applications across various programming models, including MPI, Partitioned Global Address Space (OpenSHMEM), and PyTorch, on multiple hardware architectures (CPU, GPU, and DPU).},
  archive      = {J_MICRO},
  author       = {Manjunath Gorentla Venkata and Valentine Petrov and Sergey Lebedev and Devendar Bureddy and Ferrol Aderholdt and Joshua Ladd and Gil Bloch and Mike Dubman and Gilad Shainer},
  doi          = {10.1109/MM.2025.3534638},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {26-35},
  shortjournal = {IEEE Micro},
  title        = {Unified collective communication: A unified library for CPU, GPU, and DPU collectives},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spine-free networks for large language model training. <em>MICRO</em>, <em>45</em>(2), 18-25. (<a href='https://doi.org/10.1109/MM.2025.3540663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the optimal parallelization strategy of large language models (LLMs) and demonstrate that LLM training workloads generate sparse communication patterns in the network. Consequently, we argue that LLM training clusters do not require any-to-any full-bisection networks. We then propose Rail-only, a novel datacenter network architecture tailored to LLMs’ unique communication patterns. Rail-only networks eliminate the spine layer of conventional fabrics, resulting in lower cost and energy consumption. We demonstrate that Rail-only networks achieve the same training performance while reducing network cost by 38% to 77% and network power consumption by 37% to 75%, compared to traditional GPU clusters with full-bisection bandwidth.},
  archive      = {J_MICRO},
  author       = {Weiyang Wang and Manya Ghobadi},
  doi          = {10.1109/MM.2025.3540663},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {18-25},
  shortjournal = {IEEE Micro},
  title        = {Spine-free networks for large language model training},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding and characterizing communication characteristics for distributed transformer models. <em>MICRO</em>, <em>45</em>(2), 8-17. (<a href='https://doi.org/10.1109/MM.2025.3531323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transformer architecture has revolutionized many applications, such as large language models. This progress has been largely enabled by distributed training, yet communication remains a significant bottleneck. This article examines the communication behavior of transformer models, focusing on how different parallelism schemes in multinode/multi-GPU training communicate data. We use Generative Pre-trained Transformer-based language models as a case study due to their prevalence. We validate our empirical results using analytical models. Our analysis reveals practical insights and potential areas for further optimization in framework and high-performance computing middleware design.},
  archive      = {J_MICRO},
  author       = {Quentin Anthony and Benjamin Michalowicz and Jacob Hatef and Lang Xu and Mustafa Abduljabbar and Aamir Shafi and Hari Subramoni and Dhabaleswar K. DK Panda},
  doi          = {10.1109/MM.2025.3531323},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {8-17},
  shortjournal = {IEEE Micro},
  title        = {Understanding and characterizing communication characteristics for distributed transformer models},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on hot interconnects 31. <em>MICRO</em>, <em>45</em>(2), 6-7. (<a href='https://doi.org/10.1109/MM.2025.3555461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Whit Schonbein and Joseph Schuchart},
  doi          = {10.1109/MM.2025.3555461},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot interconnects 31},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taiwan semiconductor manufacturing company’s $165 billion bet. <em>MICRO</em>, <em>45</em>(2), 4-5. (<a href='https://doi.org/10.1109/MM.2025.3557486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2025.3557486},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Taiwan semiconductor manufacturing company’s $165 billion bet},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring what matters: A fireside chat with joel emer. <em>MICRO</em>, <em>45</em>(1), 104-112. (<a href='https://doi.org/10.1109/MM.2025.3536716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Mariam Elgamal and Yueying Lisa Li},
  doi          = {10.1109/MM.2025.3536716},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {104-112},
  shortjournal = {IEEE Micro},
  title        = {Measuring what matters: A fireside chat with joel emer},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spillovers, bottlenecks, and more invention after invention. <em>MICRO</em>, <em>45</em>(1), 101-103. (<a href='https://doi.org/10.1109/MM.2025.3527797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2025.3527797},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {101-103},
  shortjournal = {IEEE Micro},
  title        = {Spillovers, bottlenecks, and more invention after invention},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of wisconsin alumni research foundation v. Apple—Part II. <em>MICRO</em>, <em>45</em>(1), 95-100. (<a href='https://doi.org/10.1109/MM.2025.3536656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2025.3536656},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {95-100},
  shortjournal = {IEEE Micro},
  title        = {A review of wisconsin alumni research foundation v. Apple—Part II},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Management of TinyML-enabled internet of things devices. <em>MICRO</em>, <em>45</em>(1), 87-94. (<a href='https://doi.org/10.1109/MM.2024.3382488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) systems are used in many aspects of our lives. Thanks to TinyML algorithms, they provide several new and smart functionalities that were impossible before. However, implementing such a solution on a large scale and maintaining it over time is a big challenge. This is due to the need to not only update the device firmware to remove errors and extend the functionality of the devices but also to update the ML models. This imposes several requirements for device monitoring and management mechanisms. In this article, we discuss not only the required aspects that an IoT system should meet, but we also show how they fit into the ML model management process.},
  archive      = {J_MICRO},
  author       = {Tomasz Szydło and Marcin Nagy},
  doi          = {10.1109/MM.2024.3382488},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {87-94},
  shortjournal = {IEEE Micro},
  title        = {Management of TinyML-enabled internet of things devices},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing tiny machine learning operations: Robust model updates in the internet of intelligent vehicles. <em>MICRO</em>, <em>45</em>(1), 76-86. (<a href='https://doi.org/10.1109/MM.2024.3354323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Intelligent Vehicles is becoming increasingly important, and embedded machine learning is gaining popularity due to new development paradigms. However, the demand for machine learning model updates on embedded systems has become relevant in multiple scenarios. This article proposes a methodology for tiny machine learning operations within the context of the Internet of Intelligent Vehicles, utilizing affordable microcontrollers based on the ESP32 platform. The solution presented in the article consists of two ESP32 devices: one functioning as a radio station (RS) and the other as the microcontroller of an onboard diagnostic (OBD-II) scanner. The RS hosts the updated model and transmits it to the OBD-II scanner using the Espressif Systems Peer-to-Peer Over Wi-Fi communication protocol over 802.11 Wi-Fi. Experimental results demonstrate significant improvement in model performance postupdate, but the article also identifies critical challenges to model robustness because of the use of the interpreter method on microcontrollers.},
  archive      = {J_MICRO},
  author       = {Thommas K. S. Flores and Ivanovitch Silva and Mariana B. Azevedo and Thaís de A. de Medeiros and Morsinaldo de A. Medeiros and Daniel G. Costa and Paolo Ferrari and Emiliano Sisinni},
  doi          = {10.1109/MM.2024.3354323},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {76-86},
  shortjournal = {IEEE Micro},
  title        = {Advancing tiny machine learning operations: Robust model updates in the internet of intelligent vehicles},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A test, debug, and silicon lifecycle management architecture for a UCIe-based open chiplet ecosystem. <em>MICRO</em>, <em>45</em>(1), 67-74. (<a href='https://doi.org/10.1109/MM.2024.3435702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, chiplets have become foundational to continuing the pace of innovation. The chiplet market is projected to significantly ramp up in the next few years, and we need to be able to support chiplets from multiple vendors in the same package. These realities create new test and debug challenges through all stages of chiplet productization from sort to in field. There is a scaling challenge to test and debug efficiently without growing package pin count; there is a challenge of test time reduction and interoperability when testing and debugging with chiplets from multiple vendors. Additionally, silicon lifecycle management (SLM) of chiplets in a system in package (SIP) for manufacturing quality monitoring and in-field predictive analysis is a significant challenge. This article presents a comprehensive test, debug, and SLM architecture for addressing these challenges in a Universal Chiplet Interconnect Express-based SIP, paving the way for an open chiplet ecosystem.},
  archive      = {J_MICRO},
  author       = {Sridhar Muthrasanallur and Yervant Zorian},
  doi          = {10.1109/MM.2024.3435702},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {67-74},
  shortjournal = {IEEE Micro},
  title        = {A test, debug, and silicon lifecycle management architecture for a UCIe-based open chiplet ecosystem},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interconnect design for heterogeneous integration of chiplets in the AMD instinct MI300X accelerator. <em>MICRO</em>, <em>45</em>(1), 57-66. (<a href='https://doi.org/10.1109/MM.2024.3462351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semiconductor industry has deployed chiplet-based system-on-chip architectures for several years. Central to a successful chiplet-based product is the die-to-die interconnect technology between the chiplets. Based on product requirements, some chiplet designs can utilize a single interconnect technology such as 2-D signals over an organic substrate or higher-density 2.5-D integration technologies. With increasing demands on compute and memory capabilities, high-performance products are now moving to heterogeneous integration, which combines multiple advanced packaging technologies all within a single system on chip. To address the market demands for high-performance artificial intelligence solutions, AMD has introduced the AMD Instinct MI300X accelerator. This article details the chiplet interconnect design required to support a sophisticated package that takes high-volume heterogeneous integration to a new level.},
  archive      = {J_MICRO},
  author       = {Alan Smith and Gabriel H. Loh and Samuel Naffziger and John Wuu and Nathan Kalyanasundharam and Eric Chapman and Raja Swaminathan and Tyrone Huang and Wonjun Jung and Alexander Kaganov and Hugh McIntyre and Ramon Mangaser},
  doi          = {10.1109/MM.2024.3462351},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {57-66},
  shortjournal = {IEEE Micro},
  title        = {Interconnect design for heterogeneous integration of chiplets in the AMD instinct MI300X accelerator},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison of single-ended, NRZ unidirectional signaling and single-ended, NRZ simultaneous-bidirectional signaling for die-to-die links. <em>MICRO</em>, <em>45</em>(1), 48-56. (<a href='https://doi.org/10.1109/MM.2024.3436008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article compares single-ended, NRZ unidirectional (UD) signaling to single-ended, NRZ simultaneous bidirectional signaling for ultrashort-reach (USR) die-to-die (D2D) links in terms of power and bit error rate (BER). We show that for the same transceiver architecture, aggregate bandwidth, and BER, a simultaneous bidirectional link is able to achieve better power efficiency and a wider eye opening than a UD link. Furthermore, the signal-integrity constraints for each signaling approach is examined in terms of the insertion-loss-to-crosstalk ratio and main-cursor-to-crosstalk ratio for three different arrangements of USR D2D links in organic substrates.},
  archive      = {J_MICRO},
  author       = {Durand Jarrett-Amor and Tony Chan Carusone},
  doi          = {10.1109/MM.2024.3436008},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {48-56},
  shortjournal = {IEEE Micro},
  title        = {A comparison of single-ended, NRZ unidirectional signaling and single-ended, NRZ simultaneous-bidirectional signaling for die-to-die links},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient parallel interconnects for chiplet integration. <em>MICRO</em>, <em>45</em>(1), 41-47. (<a href='https://doi.org/10.1109/MM.2024.3450841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating multiple chiplets in advanced packaging requires high-bandwidth and energy-efficient chiplet-to-chiplet interconnects. Parallel interconnects have emerged as the preferred interface for chiplet integration. This article provides an overview of the physical layer design of the Advanced Interface Bus (AIB), highlighting its simplicity and high energy efficiency. Additionally, we present a concrete example of successful chiplet integration employing AIB.},
  archive      = {J_MICRO},
  author       = {Wei Tang and Chester Liu and Zhengya Zhang},
  doi          = {10.1109/MM.2024.3450841},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {41-47},
  shortjournal = {IEEE Micro},
  title        = {Energy-efficient parallel interconnects for chiplet integration},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-design of interchiplet, package, and system interconnect protocols. <em>MICRO</em>, <em>45</em>(1), 35-40. (<a href='https://doi.org/10.1109/MM.2024.3447711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) applications are adopting modular tile design paradigms for compute and networking chiplets. The integration of UCIe as a die-to-die interconnect for low latency and power savings allows a disaggregation of large monolithic dies. This paper covers interchiplet architectures that can be built toward a chiplet portfolio for escalating bandwidth needs in networking and AI/machine learning. The electrical layer of UCIe design verification is illustrated. In addition, the topic of package design and challenges associated with the integration of the physical layer, in both standard and advanced packaging, are discussed.},
  archive      = {J_MICRO},
  author       = {Tony Chan Carusone and Dustin Dunwell and Sundeep Gupta and Letizia Giuliano and Adrien Auge and Michael Klempa and Sue Hung Fung},
  doi          = {10.1109/MM.2024.3447711},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {35-40},
  shortjournal = {IEEE Micro},
  title        = {Co-design of interchiplet, package, and system interconnect protocols},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCIe standard: Enhancing die-to-die connectivity in modern packaging. <em>MICRO</em>, <em>45</em>(1), 26-34. (<a href='https://doi.org/10.1109/MM.2025.3526048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, packaging plays a crucial role in determining chip performance in systems. As transistor sizes shrink and chip sizes grow to meet increasing processing demands, wafer yield drops. Connecting smaller chips within a package or using multidie designs has gained attention as it improves yield, enables intellectual property reuse, enhances performance, and lowers costs. The Universal Chiplet Interconnect Express standard, developed by industry leaders, supports these efforts by enabling different dies, or chiplets, to work together smoothly. Various structures based on this standard, such as standard 2-D organic substrate, 2.5-D through-silicon via interposer, Fanout redistribution layer organic interposer, and 3-D hybrid bonding, have been suggested. Despite the benefits, die integration faces challenges due to dense and intricate connections. This article focuses on achieving effective die-to-die connectivity using organic and advanced packaging. These patterns have been validated through simulations and the fabrication of the test chip.},
  archive      = {J_MICRO},
  author       = {Au Huynh and Kent Stahn and Manuel Mota and Christian de Verteuil and Jennifer Pyon and Reza Movahedinia},
  doi          = {10.1109/MM.2025.3526048},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {26-34},
  shortjournal = {IEEE Micro},
  title        = {UCIe standard: Enhancing die-to-die connectivity in modern packaging},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCIe: Standard for an open chiplet ecosystem. <em>MICRO</em>, <em>45</em>(1), 16-25. (<a href='https://doi.org/10.1109/MM.2024.3451532'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universal chiplet interconnect express (UCIe) is an open industry standard die-to-die physical layer, link layer, and protocol layer for chiplets. It has industry-leading key performance indicators (KPIs) and has successfully coalesced the industry around a common die-to-die specification. The ultimate goal of UCIe is to enable the rapid creation of system-in-package (SiP) solutions from an open ecosystem where chiplets are designed and manufactured by different vendors and are offered as standard products. This paper provides an overview of existing UCIe technology and highlights the work being done to create the next layer of standards required for an open chiplet ecosystem.},
  archive      = {J_MICRO},
  author       = {Peter Onufryk and Swadesh Choudhary},
  doi          = {10.1109/MM.2024.3451532},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {16-25},
  shortjournal = {IEEE Micro},
  title        = {UCIe: Standard for an open chiplet ecosystem},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disaggregated designs: Technology challenges and enablers. <em>MICRO</em>, <em>45</em>(1), 9-15. (<a href='https://doi.org/10.1109/MM.2024.3524130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unsustainable pace of die-size growth has come up against the reticle limit. The rise in cost per transistor (CPT) is outweighing scaling benefits from advances in generational process technology. These diminishing silicon economies of scale have pushed foundries, EDA companies, and the manufacturing ecosystem to enable chiplet designs. Developments in packaging technology and standardization of die-to-die interfaces provide technology gains offsetting challenges of die sizes and CPT. Standards bodies and IP implementers have risen to the challenge, providing solutions to interface bottlenecks. We examine standards for chiplet and die-to-die interfaces, such as UCIe and the work done in the industry to enable these. Architectural partitions for chiplet implementations and development of reusable components that enable a future chiplet marketplace are considered. The juggernaut for differentiation and disaggregation in the more-than-Moore era continues to build momentum. We examine these trends and recent technology advances, looking at future directions for implementations.},
  archive      = {J_MICRO},
  author       = {Boyd Phelps and Arif Khan},
  doi          = {10.1109/MM.2024.3524130},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {9-15},
  shortjournal = {IEEE Micro},
  title        = {Disaggregated designs: Technology challenges and enablers},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on interconnects for chiplet integration technologies. <em>MICRO</em>, <em>45</em>(1), 6-8. (<a href='https://doi.org/10.1109/MM.2025.3534457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Debendra Das Sharma and Nam Sung Kim},
  doi          = {10.1109/MM.2025.3534457},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {6-8},
  shortjournal = {IEEE Micro},
  title        = {Special issue on interconnects for chiplet integration technologies},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rise of the agentic AI workforce. <em>MICRO</em>, <em>45</em>(1), 4-5. (<a href='https://doi.org/10.1109/MM.2025.3535912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2025.3535912},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Rise of the agentic AI workforce},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
