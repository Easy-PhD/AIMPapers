<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MICRO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="micro">MICRO - 60</h2>
<ul>
<li><details>
<summary>
(2025). Prototype competition and breakthroughs. <em>MICRO</em>, <em>45</em>(4), 110--112. (<a href='https://doi.org/10.1109/MM.2025.3580075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commercial prototypes are not just products; they are the stepping stones of technological progress. They embed new features, acting as a focal point for expanding the application of cutting-edge technologies in new directions. Prototypes are not just about addressing issues that need resolution; they are about pushing the boundaries of what is possible.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2025.3580075},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {110--112},
  shortjournal = {IEEE Micro},
  title        = {Prototype competition and breakthroughs},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of wisconsin alumni research foundation v. Apple—Part v. <em>MICRO</em>, <em>45</em>(4), 104--109. (<a href='https://doi.org/10.1109/MM.2025.3599331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Part I of this series introduced the Wisconsin Alumni Research Foundation v. Apple cases and described the asserted patent (U.S. Patent Number 5,781,752). That article also summarized some recent large verdicts for patents asserted by academic institutions and provided several reasons why this series may be of interest to the readership of IEEE Micro, most notably because the inventors are well known and several well-known computer architects worked as experts on this case. Part II described the complaints, namely, it described the plaintiff, Wisconsin Alumni Research Foundation (“WARF”), the inventors, and WARF’s allegations as to how Apple’s products infringed WARF’s patent. Part III described Apple’s answer to the allegations in WARF’s complaint, Apple’s counterclaims, and WARF’s response to those counterclaims. Part IV examined Apple’s allegation of inequitable conduct by the inventors, a technical analysis of that allegation, and Judge Conley’s legal analysis of the sufficiency of Apple’s allegations.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2025.3599331},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {104--109},
  shortjournal = {IEEE Micro},
  title        = {A review of wisconsin alumni research foundation v. Apple—Part v},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From colocation to exfiltration: Practical cache side-channel attacks in the modern public cloud. <em>MICRO</em>, <em>45</em>(4), 95--102. (<a href='https://doi.org/10.1109/MM.2025.3574715'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharing resources among tenants is fundamental to public clouds, enhancing efficiency but also creating opportunities for microarchitectural side-channel attacks. However, cloud vendors remain skeptical about the practicality of these attacks, particularly regarding the ability to colocate attacker and victim, and to overcome system noise. In this work, we develop a series of techniques for each step of the attack and, for the first time, demonstrate cross-tenant information leakage on the public Google Cloud Run, refuting the belief that such attacks are impractical. Our findings highlight the need to secure public clouds against side-channel attacks.},
  archive      = {J_MICRO},
  author       = {Zirui Neil Zhao and Adam Morrison and Christopher W. Fletcher and Josep Torrellas},
  doi          = {10.1109/MM.2025.3574715},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {95--102},
  shortjournal = {IEEE Micro},
  title        = {From colocation to exfiltration: Practical cache side-channel attacks in the modern public cloud},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay-space arithmetic and architecture. <em>MICRO</em>, <em>45</em>(4), 87--94. (<a href='https://doi.org/10.1109/MM.2025.3588787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What operations can you perform efficiently when you use the “time of arrival” of a signal’s edge to represent a number? Past work has shown how linear representations can be effectively used to optimize problems expressed in max-plus algebras, but efficient general-purpose arithmetic operations have remained elusive. We present negative-logarithmic delay-space arithmetic as a completely new approach to temporal coding. Under this approach, general-purpose arithmetic is transformed to a “soft” version of the standard temporal operations in such a way that preserves all of the algebraic identities. We further show that these soft operations can be approximated by composing the original “sharp” temporal operators, resulting in simple, energy-efficient implementations. We demonstrate the effectiveness of this novel arithmetic with a near-sensor architecture for energy-efficient convolutions. Cycle-over-cycle operation is supported through temporal recurrence, dramatically limiting the need for expensive domain conversions or noise-prone temporal memories.},
  archive      = {J_MICRO},
  author       = {Rhys Gretsch and Peiyang Song and Advait Madhavan and Jeremy Lau and Timothy Sherwood},
  doi          = {10.1109/MM.2025.3588787},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {87--94},
  shortjournal = {IEEE Micro},
  title        = {Delay-space arithmetic and architecture},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BlitzCoin: A decentralized hardware solution for power management of highly heterogeneous systems on chip. <em>MICRO</em>, <em>45</em>(4), 79--86. (<a href='https://doi.org/10.1109/MM.2025.3574281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in both the number and the types of accelerators in modern systems on chip (SoCs) necessitates a rethinking of power management (PM) strategies. To overcome the scalability shortcomings of current methods, we propose BlitzCoin, a fully decentralized hardware-based PM coupled with optimized unified voltage and frequency regulation. We evaluated BlitzCoin through register transfer-level simulations of multiple SoCs targeted toward different application domains. The results are further validated through silicon measurements of a fabricated 12-nm many-accelerator SoC that includes BlitzCoin. Our evaluations show that BlitzCoin is markedly faster than state-of-the-art centralized PM strategies, with 8 × to 12 × lower response times. This results in 25%–34% throughput improvement and allows for scaling to 7 × to 13 × larger SoCs, all with a small area overhead of <1%. BlitzCoin is an addition to the open source ESP SoC platform, offering a foundation for further exploration of PM strategies.},
  archive      = {J_MICRO},
  author       = {Martin Cochet and Karthik Swaminathan and Erik Loscalzo and Joseph Zuckerman and Maico Cassel dos Santos and Davide Giri and Alper Buyuktosunoglu and Tianyu Jia and David Brooks and Gu-Yeon Wei and Kenneth Shepard and Luca P. Carloni and Pradip Bose},
  doi          = {10.1109/MM.2025.3574281},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {79--86},
  shortjournal = {IEEE Micro},
  title        = {BlitzCoin: A decentralized hardware solution for power management of highly heterogeneous systems on chip},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalar vector runahead: Removing the shackles of indirect memory chains on in-order cores. <em>MICRO</em>, <em>45</em>(4), 72--78. (<a href='https://doi.org/10.1109/MM.2025.3577524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern processors often face the memory wall as a bottleneck, an exacerbated problem for stall-on-use in-order cores. Despite this limitation, there is growing demand for energy-efficient in-order cores due to privacy and sustainability concerns. Scalar vector runahead (SVR) provides an elegant solution by extracting high memory-level parallelism through piggybacking on existing instructions executed on the processor that lead to future irregular memory accesses. SVR speculatively executes multiple transient, independent, parallel instances of memory accesses and their instruction chains, by initiating memory accesses from many different values of a predicted induction variable. This approach moves mutually independent memory accesses next to each other to hide dependent stalls. With a hardware overhead of only 2 KiB and without the need for hardware vector extensions, SVR delivers 3.2× higher performance than a baseline three-wide in-order core inspired by an Arm Cortex A510, and 1.3× higher performance than an out-of-order core, while halving energy consumption.},
  archive      = {J_MICRO},
  author       = {Jaime Roelandts and Ajeya Naithani and Sam Ainsworth and Timothy M. Jones and Lieven Eeckhout},
  doi          = {10.1109/MM.2025.3577524},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {72--78},
  shortjournal = {IEEE Micro},
  title        = {Scalar vector runahead: Removing the shackles of indirect memory chains on in-order cores},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated high-level code optimization for warehouse performance. <em>MICRO</em>, <em>45</em>(4), 60--71. (<a href='https://doi.org/10.1109/MM.2025.3590033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the twilight of Moore’s law, optimizing program performance has emerged as a central focus in computer architecture research. Yet, high-level source optimization remains challenging due to the intricate nature of understanding code semantics. Our approach unifies machine learning techniques with established insights and tools from computer architecture to tackle the inherent challenges of high-level optimization. In this work, we introduce a framework that harnesses large language models (LLMs) for high-level program optimization. We curate a dataset of competitive C++ submissions, each accompanied by extensive unit tests to capture performance-improving patterns. To mitigate the variability of performance measurements, we develop an evaluation harness using the gem5 full-system simulator. Our results show a mean speedup of 6.86, outperforming the average human optimization of 3.66×. We also give an overview of subsequent work in this space, describing how LLM-driven optimization enables autonomously applying performance-improving edits across billions of lines of code in Google data centers.},
  archive      = {J_MICRO},
  author       = {Alexander Shypula and Aman Madaan and Yimeng Zeng and Uri Alon and Jacob Gardner and Milad Hashemi and Graham Neubig and Parthasarathy Ranganathan and Osbert Bastani and Amir Yazdanbakhsh},
  doi          = {10.1109/MM.2025.3590033},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {60--71},
  shortjournal = {IEEE Micro},
  title        = {Automated high-level code optimization for warehouse performance},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Splitwise: Efficient generative LLM inference using phase splitting. <em>MICRO</em>, <em>45</em>(4), 54--59. (<a href='https://doi.org/10.1109/MM.2025.3575361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative large language model (LLM) applications are rapidly growing, leading to widespread deployment of expensive, power-hungry GPUs. Growing power demands of artificial intelligence (AI) in the cloud industry has become a global problem.5 Our analysis shows that LLM inference involves two distinct phases: a compute-intensive prefill phase and a memory-intensive decode phase, each with different resource needs. Running them together introduces inefficient scheduling. Furthermore, unlike the prefill phase, the decode phase can run on lower-cost and lower-power hardware. Building on these insights, we propose Splitwise, a scheduling technique that splits prefill and decode phases across different machines to achieve better throughput. Additionally, Splitwise allows phase-specific hardware optimization. By efficiently transferring request state between machines, Splitwise achieves up to 2.35× more throughput within the same power and cost budgets, or 1.4× higher throughput at 20% lower cost and same power.},
  archive      = {J_MICRO},
  author       = {Esha Choukse and Pratyush Patel and Chaojie Zhang and Aashaka Shah and Íñigo Goiri and Saeed Maleki and Rodrigo Fonseca and Ricardo Bianchini},
  doi          = {10.1109/MM.2025.3575361},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {54--59},
  shortjournal = {IEEE Micro},
  title        = {Splitwise: Efficient generative LLM inference using phase splitting},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From TeAAL to FuseMax: Separation of concerns for attention accelerator design. <em>MICRO</em>, <em>45</em>(4), 44--53. (<a href='https://doi.org/10.1109/MM.2025.3589955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention for transformers has recently received significant “attention” as a target for custom acceleration. Our prior work, TeAAL, proposes a new accelerator design methodology that allows architects to reason about and optimize their designs iteratively. With a focus on attention, this work makes contributions to both the theory and practice of TeAAL’s methodology. On the theory side, we propose a set of analyses that can be applied using only the algorithm specification—the cascade of Einsums summations. On the practice side, we use the new analyses to understand and taxonomize the space of attention algorithms and to iteratively build up an efficient, high-utilization accelerator. Our resulting design, FuseMax, achieves an average 6.7× speedup on attention and 5.3× speedup on end-to-end transformer inference over the prior state of the art, FLAT, while using 79% and 83% of the energy, respectively.},
  archive      = {J_MICRO},
  author       = {Nandeeka Nayak and Toluwanimi O. Odemuyiwa and Xinrui Wu and Michael Pellauer and Joel S. Emer and Christopher W. Fletcher},
  doi          = {10.1109/MM.2025.3589955},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {44--53},
  shortjournal = {IEEE Micro},
  title        = {From TeAAL to FuseMax: Separation of concerns for attention accelerator design},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Waferscale network switches. <em>MICRO</em>, <em>45</em>(4), 37--43. (<a href='https://doi.org/10.1109/MM.2025.3589927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spite of being a key determinant of latency, cost, power, space, and capability of modern computer systems, network switch radix has not seen much growth over the years due to poor scaling of off-chip input/output pitches and switch die sizes. In this work, we show that one could use waferscale integration as a way to dramatically increase the size of the switch substrate and build a network switch with 32× higher radix than state-of-the-art network switches. We identified and addressed the limitations of internal bandwidth, external bandwidth, and power density, as the vanilla design would make the benefits of waferscale network switch minimal. We show that the waferscale network switch can be used to enable new computing systems, such as single-switch data centers and massive-scale singular GPUs. It can also lead to a dramatic reduction in data center network costs.},
  archive      = {J_MICRO},
  author       = {Shuangliang David Chen and Saptadeep Pal and Rakesh Kumar},
  doi          = {10.1109/MM.2025.3589927},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {37--43},
  shortjournal = {IEEE Micro},
  title        = {Waferscale network switches},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware-assisted virtualization of neural processing units for cloud platforms. <em>MICRO</em>, <em>45</em>(4), 29--36. (<a href='https://doi.org/10.1109/MM.2025.3574630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud platforms have deployed hardware accelerators like neural processing units (NPUs) for machine learning (ML) inference services. To maximize resource utilization while ensuring quality of service, a natural approach is to virtualize NPUs for efficient resource sharing for multitenant ML services. However, virtualizing NPUs is challenging. This is not only due to the lack of system abstraction support for NPU hardware, but also due to the lack of architectural and instruction set architecture (ISA) support for fine-grained operator scheduling. This article presents Neu10, an NPU virtualization framework consisting of 1) an abstraction called vNPU for fine-grained virtualization of the heterogeneous compute units in a physical NPU (pNPU), 2) a vNPU allocator that enables pay-as-you-go pricing and flexible vNPU-to-pNPU mappings, and 3) an ISA extension of modern NPU architecture for fine-grained tensor operator scheduling for multiple vNPUs. We evaluate Neu10 with a production-level simulator to demonstrate its benefits over state-of-the-art NPU sharing approaches.},
  archive      = {J_MICRO},
  author       = {Yuqi Xue and Yiqi Liu and Lifeng Nai and Jian Huang},
  doi          = {10.1109/MM.2025.3574630},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {29--36},
  shortjournal = {IEEE Micro},
  title        = {Hardware-assisted virtualization of neural processing units for cloud platforms},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling sustainable cloud computing with low-carbon server design. <em>MICRO</em>, <em>45</em>(4), 19--28. (<a href='https://doi.org/10.1109/MM.2025.3572955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To combat climate change, we must reduce carbon emissions from hyperscale cloud computing. Compute servers cause the majority of a general-purpose cloud’s emissions. Thus, we are motivated to design carbon-efficient compute server stock keeping units (SKUs), or GreenSKUs, using recently available low-carbon components. We built three GreenSKU prototypes, integrating energy-efficient CPUs, reusing old dynamic RAM via compute express link, and reusing old solid-state drives. We reveal challenges that limit GreenSKUs’ carbon savings at scale and may prevent their adoption by cloud providers. To address these challenges, we developed a novel framework, GSF (GreenSKU Framework), that enables cloud providers to systematically evaluate GreenSKUs’ carbon savings at scale. By implementing GSF within Microsoft Azure’s production constraints, we demonstrate that GreenSKUs reduce net cloud emissions by 8%, which is globally significant. This work is the first to demonstrate and quantify how carbon-efficient server designs translate to measurable cloud-scale emissions reductions, enabling meaningful contributions to cloud sustainability goals.},
  archive      = {J_MICRO},
  author       = {Jaylen Wang and Daniel S. Berger and Fiodar Kazhamiaka and Celine Irvene and Chaojie Zhang and Esha Choukse and Kali Frost and Rodrigo Fonseca and Brijesh Warrier and Chetan Bansal and Jonathan Stern and Ricardo Bianchini and Akshitha Sriraman},
  doi          = {10.1109/MM.2025.3572955},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {19--28},
  shortjournal = {IEEE Micro},
  title        = {Enabling sustainable cloud computing with low-carbon server design},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing processor sustainability using the first-order FOCAL carbon model. <em>MICRO</em>, <em>45</em>(4), 11--18. (<a href='https://doi.org/10.1109/MM.2025.3576714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the inherent data uncertainty regarding the sustainability of computing devices in general and processors in particular, this article proposes the parameterized First-Order analytical CArbon modeL (FOCAL) to assess processor sustainability from first principles. FOCAL’s normalized carbon footprint metric guides computer architects to holistically optimize chip area, energy, and power consumption to reduce a processor’s environmental footprint. We use FOCAL to analyze and categorize a broad set of archetypal processor mechanisms into strongly, weakly, or less sustainable design choices, providing insight and intuition into how to reduce a processor’s environmental footprint with implications to both hardware and software. A case study illustrates a pathway for designing strongly sustainable multicore processors delivering high performance while at the same time reducing their environmental footprint.},
  archive      = {J_MICRO},
  author       = {Lieven Eeckhout},
  doi          = {10.1109/MM.2025.3576714},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {11--18},
  shortjournal = {IEEE Micro},
  title        = {Assessing processor sustainability using the first-order FOCAL carbon model},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on top picks from the 2024 computer architecture conferences. <em>MICRO</em>, <em>45</em>(4), 6--10. (<a href='https://doi.org/10.1109/MM.2025.3599323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is our great pleasure to present the IEEE Micro Special Issue on Top Picks From the 2024 Computer Architecture Conferences. This special issue upholds a long-standing tradition within the computer architecture community of recognizing outstanding research contributions. The articles featured herein were selected by the Top Picks Selection Committee for their exceptional novelty and strong potential for long-term impact on the field.},
  archive      = {J_MICRO},
  author       = {Jun Yang and Xulong Tang},
  doi          = {10.1109/MM.2025.3599323},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {6--10},
  shortjournal = {IEEE Micro},
  title        = {Special issue on top picks from the 2024 computer architecture conferences},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligence for sale. <em>MICRO</em>, <em>45</em>(4), 4--5. (<a href='https://doi.org/10.1109/MM.2025.3601305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2025.3601305},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {4--5},
  shortjournal = {IEEE Micro},
  title        = {Intelligence for sale},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The scramble after breakthrough. <em>MICRO</em>, <em>45</em>(3), 108--110. (<a href='https://doi.org/10.1109/MM.2025.3567048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2025.3567048},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {108--110},
  shortjournal = {IEEE Micro},
  title        = {The scramble after breakthrough},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sipping matcha of security: A fireside chat with mengjia yan. <em>MICRO</em>, <em>45</em>(3), 103--107. (<a href='https://doi.org/10.1109/MM.2025.3572585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Jianming Tong and Zishen Wan},
  doi          = {10.1109/MM.2025.3572585},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {103--107},
  shortjournal = {IEEE Micro},
  title        = {Sipping matcha of security: A fireside chat with mengjia yan},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of wisconsin alumni research foundation v. Apple—Part IV. <em>MICRO</em>, <em>45</em>(3), 97--102. (<a href='https://doi.org/10.1109/MM.2025.3573578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is the fourth part in a series that reviews the decisions that the district judge and appellate panel made in Wisconsin Alumni Research Foundation v. Apple.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2025.3573578},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {97--102},
  shortjournal = {IEEE Micro},
  title        = {A review of wisconsin alumni research foundation v. Apple—Part IV},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three SoCs in three years: How to get agile. <em>MICRO</em>, <em>45</em>(3), 86--94. (<a href='https://doi.org/10.1109/MM.2025.3534917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We taped out three large system on chips in three years on 22-nm CMOS technology, featuring multiple RISC-V cores, and subsystems for machine learning, Ethernet, SerDes, Low-Power SDRAM, and input–output. We have covered all steps in the flow from specification to sample chips. Ballast, Tackle, and Headsail include 130 M, 12 M, and 340 M transistors and took 12, 10, and nine calendar months. Several persons from seven companies and university contributed to the three chips, and staff ranged from experts to novice master students. This article provides insight into modern fast-paced system-on-chip hardware (HW) development which is important when intellectual properties such as RISC-V processors and security accelerators are evolving rapidly. We achieved Agile development with the following guidelines: intellectual properties elaborated on the go, staff moves along the design flow, interface over instance, and schedule over features.},
  archive      = {J_MICRO},
  author       = {Antti Rautakoura and Timo Hämäläinen and Ari Kulmala},
  doi          = {10.1109/MM.2025.3534917},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {86--94},
  shortjournal = {IEEE Micro},
  title        = {Three SoCs in three years: How to get agile},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FuriosaAI RNGD: A tensor contraction processor for sustainable AI computing. <em>MICRO</em>, <em>45</em>(3), 76--85. (<a href='https://doi.org/10.1109/MM.2025.3551880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern artificial intelligence (AI) workloads require architectures capable of efficiently managing diverse tensor contraction patterns. Traditional approaches based on fixed-size matrix multiplications often fall short in scalability and flexibility. RNGD (pronounced “Renegade”), a second-generation tensor contraction processor, introduces an innovative architecture designed to exploit the parallelism and data locality inherent in tensor computations. Its coarse-grained processing elements (PEs) can operate as a unified large-scale unit or as multiple independent units, providing flexibility for various tensor shapes. Key innovations, such as a circuit switch-based fetch network, input broadcasting, and buffer-based reuse mechanisms, further enhance computational efficiency. RNGD represents a significant advancement in processor architecture, delivering optimized performance and energy efficiency for sustainable computation of next-generation AI workloads.},
  archive      = {J_MICRO},
  author       = {Younggeun Choi and Junyoung Park and Sang Min Lee and Jeseung Yeon and Minho Kim and Changjae Park and Byeongwook Bae and Hyunmin Jeong and Hanjoon Kim and June Paik and Nuno P. Lopes and Sungjoo Yoo},
  doi          = {10.1109/MM.2025.3551880},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {76--85},
  shortjournal = {IEEE Micro},
  title        = {FuriosaAI RNGD: A tensor contraction processor for sustainable AI computing},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The IBM telum II processor. <em>MICRO</em>, <em>45</em>(3), 66--75. (<a href='https://doi.org/10.1109/MM.2025.3563803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IBM Telum II is the latest processor designed specifically for IBM Z’s next-generation mainframe. Designed-for-purpose, Telum II is focused on mission-critical enterprise workloads where performance and sustainability are of the utmost importance and the demand for artificial intelligence acceleration is increasing dramatically. Innovations discussed in this article are the new on-die data processing unit for input/output acceleration, the updated cache, enhancements to the on-chip artificial intelligence accelerator, core improvements, and changes to the off-chip input/output interfaces.},
  archive      = {J_MICRO},
  author       = {Christopher Berry and Michael Becht and Tim Bubb and Howard Haynie and Robert Sonnelitter and Katie Seggerman and Jonathan Hsieh and Edward Malley and Mike Cadigan and Susan M. Eickhoff and Matthias Klein and Craig Walters and Christian G. Zoellin and Cedric Lichtenau},
  doi          = {10.1109/MM.2025.3563803},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {66--75},
  shortjournal = {IEEE Micro},
  title        = {The IBM telum II processor},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing programmable accelerators for sparse tensor algebra. <em>MICRO</em>, <em>45</em>(3), 58--65. (<a href='https://doi.org/10.1109/MM.2025.3556611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has focused on leveraging sparsity in hardware accelerators to improve the efficiency of applications spanning scientific computing to machine learning. Most such prior accelerators are fixed-function, which is insufficient for two reasons. First, applications typically include both dense and sparse components, and second, the algorithms that comprise these applications are constantly evolving. To address these challenges, we designed a programmable accelerator called Onyx for both sparse tensor algebra and dense workloads. Onyx extends a coarse-grained reconfigurable array (CGRA) optimized for dense applications with composable hardware primitives to support arbitrary sparse tensor algebra kernels. In this article, we show that we can further optimize Onyx by adding a small set of hardware features for parallelization that significantly increase both temporal and spatial utilization of the CGRA, reducing runtime by up to 6.2×.},
  archive      = {J_MICRO},
  author       = {Kalhan Koul and Zhouhua Xie and Maxwell Strange and Sai Gautham Ravipati and Bo Wun Cheng and Olivia Hsu and Po-Han Chen and Mark Horowitz and Fredrik Kjolstad and Priyanka Raina},
  doi          = {10.1109/MM.2025.3556611},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {58--65},
  shortjournal = {IEEE Micro},
  title        = {Designing programmable accelerators for sparse tensor algebra},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XiangShan: An open source project for high-performance RISC-V processors meeting industrial-grade standards. <em>MICRO</em>, <em>45</em>(3), 49--57. (<a href='https://doi.org/10.1109/MM.2025.3565285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is widely believed that an open source hardware ecosystem can reduce development costs and lower barriers to innovation. However, developing an open source industrial-grade high-performance processor is a challenging undertaking. For mass adoption, such an IP needs to have an advanced out-of-order microarchitecture for high performance, a robust verification infrastructure for reliable quality, and high configurability to accommodate the myriad use cases. With a best-in-class performance, XiangShan is an open source project for RISC-V processors that fully meets these requirements. To maximize overall efficiency, XiangShan adopted a collaborative hardware development model, partnering with industry on processor design, implementation, and verification. With innovations in Agile development processes and tools, the design of the processors can be evolved, optimized, and verified quickly, ensuring high quality and enabling architectural innovation and rapid commercialization.},
  archive      = {J_MICRO},
  author       = {Kaifan Wang and Jian Chen and Yinan Xu and Zihao Yu and Wei He and Dan Tang and Ninghui Sun and Yungang Bao},
  doi          = {10.1109/MM.2025.3565285},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {49--57},
  shortjournal = {IEEE Micro},
  title        = {XiangShan: An open source project for high-performance RISC-V processors meeting industrial-grade standards},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMD instinct MI300X: A generative AI accelerator and platform architecture. <em>MICRO</em>, <em>45</em>(3), 41--48. (<a href='https://doi.org/10.1109/MM.2025.3552324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AMD Instinct MI300X sets a new benchmark in generative artificial intelligence (AI) acceleration, combining architectural innovation with advanced system integration to tackle the ever-growing demands of modern AI workloads. Featuring a chiplet-based architecture, the MI300X employs the fourth-generation Infinity Fabric, eight-stack HBM3 memory, and CDNA 3 compute cores to deliver unparalleled performance for both inference and training tasks. Additionally, the MI300X is central to the AMD Infinity platform, which offers industry-standard scalability through universal baseboard designs, high-bandwidth interconnectivity, and robust system management features. This article provides a detailed exploration of the MI300X architecture, its Infinity platform integration, and its impact on generative AI applications.},
  archive      = {J_MICRO},
  author       = {Alan Smith and Vamsi Krishna Alla},
  doi          = {10.1109/MM.2025.3552324},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {41--48},
  shortjournal = {IEEE Micro},
  title        = {AMD instinct MI300X: A generative AI accelerator and platform architecture},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intel xeon 6 product family. <em>MICRO</em>, <em>45</em>(3), 31--40. (<a href='https://doi.org/10.1109/MM.2025.3553756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Intel® Xeon 6 product family delivers new degrees of performance and scalability to address a wide variety of deployments across data center, enterprise, networking, and edge. The diversity of workloads, power, performance, and form factor requirements led to Intel’s most advanced modular system on chip (SoC) processor architecture. This modular construction allows the flexibility to optimize each die and build multiple SoCs using the same building blocks. For ultimate versatility, Intel Xeon 6 processors allow for the choice of two different CPU microarchitectures: performance cores and efficient cores. Both core types use a compatible x86 instruction set architecture and a common hardware platform.},
  archive      = {J_MICRO},
  author       = {Michael D. Powell and Patrick Fleming and Venkidesh Iyer Krishna and Naveen Lakkakula and Subhiksha Ravisundar and Praveen Mosur and Arijit Biswas and Pradeep Dubey and Kapil Sood and Andrew Cunningham and Smita Kumar},
  doi          = {10.1109/MM.2025.3553756},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {31--40},
  shortjournal = {IEEE Micro},
  title        = {Intel xeon 6 product family},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMD versal AI edge series gen 2. <em>MICRO</em>, <em>45</em>(3), 22--30. (<a href='https://doi.org/10.1109/MM.2025.3551319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AMD’s Next-Gen Adaptive System on Chip, Versal AI Edge Series Gen 2, is a high-performance, scalable, and customizable platform for a wide array of markets, including automotive (advanced driver assistance systems and autonomous driving), robotics, audio–video broadcast, aerospace and defense, and industrial. The platform is designed to support ISO-26262 ASIL-D and IEC-61508 SIL3 for safety critical applications and was architected considering the needs of embedded vision applications, where the heterogenous adaptive architecture integrates field programmable gate array programmable logic with high-performance multicluster processors, imaging and video processing engines, and a next-generation artificial (AI) engine array with advanced data types. The advanced MX data types of the AI engine enable embedded vision applications to achieve accuracies comparable to FP32 with reduced AI engine array and memory footprint costs.},
  archive      = {J_MICRO},
  author       = {Tomai Knopp and Jeffrey Chu and Sagheer Ahmad},
  doi          = {10.1109/MM.2025.3551319},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {22--30},
  shortjournal = {IEEE Micro},
  title        = {AMD versal AI edge series gen 2},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lunar lake an intel mobile processor: SoC architecture overview (2024). <em>MICRO</em>, <em>45</em>(3), 15--21. (<a href='https://doi.org/10.1109/MM.2025.3558407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lunar Lake (LNL) is the codename for the Core Ultra Series mobile processors designed by Intel, released in September 2024. LNL took ambitious targets to cope with core and graphics performance, performance/watt, battery life, and artificial intelligence compute for an outstanding user experience. To address that, a ground-up architecture was defined. LNL enhanced the partition of cores to performance and efficient clusters with the ability to contain software load to the desired hardware at runtime, it optimized performance cluster with single threaded core, revised the idle state management, reduced frequent CPU wakes, enhanced the memory subsystem power states, and added fine-grain power delivery. LNL added premium capabilities such as memory-on-package, a power management integrated circuit, a powerful neural processing unit, memory-side cache, and total storage encryption for NVMe. LNL architecture scales from 8 W to 30 W+ and supports LPDDR5 frequencies up to 8533 Mhz.},
  archive      = {J_MICRO},
  author       = {Nadav Bonen and Arik Gihon and Leon Polishuk and Yoni Aizik and Yulia Okunev and Tsvika Kurts and Nithiyanandan Bashyam},
  doi          = {10.1109/MM.2025.3558407},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {15--21},
  shortjournal = {IEEE Micro},
  title        = {Lunar lake an intel mobile processor: SoC architecture overview (2024)},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qualcomm oryon CPU in snapdragon x elite: Micro-architecture and design. <em>MICRO</em>, <em>45</em>(3), 8--14. (<a href='https://doi.org/10.1109/MM.2025.3568807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the micro-architecture and design of the Qualcomm custom CPU, named Qualcomm Oryon CPU, that was introduced in 2024 in the Qualcomm Snapdragon X Elite system on a chip for the client computing market. It describes the micro-architecture of the CPU core and its cache and memory subsystem and is illustrative of a modern high-performance CPU with best-in-class energy efficiencies that is designed to be scalable across different product categories and price points.},
  archive      = {J_MICRO},
  author       = {Gerard Williams and Pradeep Kanapathipillai},
  doi          = {10.1109/MM.2025.3568807},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {8--14},
  shortjournal = {IEEE Micro},
  title        = {Qualcomm oryon CPU in snapdragon x elite: Micro-architecture and design},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on hot chips 2024. <em>MICRO</em>, <em>45</em>(3), 6--7. (<a href='https://doi.org/10.1109/MM.2025.3572594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Rob Aitken and Larry Yang},
  doi          = {10.1109/MM.2025.3572594},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {6--7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot chips 2024},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward disaggregated and heterogenous AI systems. <em>MICRO</em>, <em>45</em>(3), 4--5. (<a href='https://doi.org/10.1109/MM.2025.3575180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2025.3575180},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {4--5},
  shortjournal = {IEEE Micro},
  title        = {Toward disaggregated and heterogenous AI systems},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence and the jevons paradox. <em>MICRO</em>, <em>45</em>(2), 118--120. (<a href='https://doi.org/10.1109/MM.2025.3548921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2025.3548921},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {118--120},
  shortjournal = {IEEE Micro},
  title        = {Artificial intelligence and the jevons paradox},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of wisconsin alumni research foundation v. Apple—Part III. <em>MICRO</em>, <em>45</em>(2), 114--117. (<a href='https://doi.org/10.1109/MM.2025.3557487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2025.3557487},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {114--117},
  shortjournal = {IEEE Micro},
  title        = {A review of wisconsin alumni research foundation v. Apple—Part III},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving key-value cache performance with heterogeneous memory tiering: A case study of compute-express-link-based memory expansion. <em>MICRO</em>, <em>45</em>(2), 102--113. (<a href='https://doi.org/10.1109/MM.2024.3358861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute Express Link (CXL) memory brings extra bandwidth and capacity via Peripheral-Component-Interconnect-Express-based memory expansion beyond double-data-rate-based dynamic random-access memory. This article introduces the CXL 2.0 memory expansion solution, which incorporates two parts: 1) a CXL memory expander prototype and 2) the heterogeneous memory software development kit. We demonstrate the feasibility of our CXL memory solution by implementing it on CacheLib, Meta’s general-purpose key-value caching engine. We highlight how our application design and guidelines for CXL memory enable resolving the shortcomings of conventional memory system architectures. Our proposals enable 1) expanding memory bandwidth and capacity or 2) considerable DRAM savings. Evaluation results show that we can achieve a 25% increase in memory bandwidth, up to 15% throughput gain, and a 9% latency reduction. Furthermore, in hybrid cache using nonvolatile memory (NVM), expanding the RAM cache area with CXL memory, which is relatively cheaper than DRAM, enhances the throughput and hit ratio due to reduced NVM input–output.},
  archive      = {J_MICRO},
  author       = {KyungSoo Lee and Sohyun Kim and Joohee Lee and Donguk Moon and Rakie Kim and Honggyu Kim and Hyeongtak Ji and Yunjeong Mun and Youngpyo Joo},
  doi          = {10.1109/MM.2024.3358861},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {102--113},
  shortjournal = {IEEE Micro},
  title        = {Improving key-value cache performance with heterogeneous memory tiering: A case study of compute-express-link-based memory expansion},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible approximate computing for mitigating branch divergence in GPUs. <em>MICRO</em>, <em>45</em>(2), 90--100. (<a href='https://doi.org/10.1109/MM.2024.3504261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphics processing units (GPUs) have become prevalent across various domains due to their high throughput and energy efficiency. A major challenge in GPU computing is branch divergence, which occurs when the control flow paths of concurrently executing threads diverge. Branch divergence significantly degrades performance, as diverged threads are executed in a serialized, time-sliced manner. To mitigate this issue, we propose a novel approximate computing technique for GPUs. Our method approximates the results of diverged threads by selectively terminating their execution. Furthermore, it incorporates a dynamic mechanism that adjusts both the probability of termination and the number of instructions executed before termination, based on user-defined parameters. This technique allows users to balance execution speed and result accuracy, offering greater flexibility in controlling computational quality. Additionally, by changing the approximation aggressiveness at runtime, we can accommodate applications such as virtual reality/augmented reality, where the demand for accuracy changes in real-time.},
  archive      = {J_MICRO},
  author       = {Reoma Matsuo and Yuya Degawa and Hidetsugu Irie and Shuichi Sakai and Ryota Shioya},
  doi          = {10.1109/MM.2024.3504261},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {90--100},
  shortjournal = {IEEE Micro},
  title        = {Flexible approximate computing for mitigating branch divergence in GPUs},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MRCA 2.0: An area-optimized multigrained reconfigurable cryptographic accelerator for securing blockchain-based internet of things systems. <em>MICRO</em>, <em>45</em>(2), 78--89. (<a href='https://doi.org/10.1109/MM.2024.3501313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a multigrained reconfigurable cryptographic accelerator 2.0 (MRCA 2.0), upgrading from the first 8/32/64-bit coarse-grained reconfigurable array, named MRCA, to offer high performance with a smaller area and lower power consumption for blockchain-based Internet of Things systems. To achieve these goals, we propose four innovative ideas: a dual-processing element array (D-PEA) with reduced memories, a synchronous row connection and processing element (PE) architecture, a concatenable crypto arithmetic logic unit, and heterogeneous PEs. Our MRCA 2.0 has been successfully implemented on the TySOM-3A field-programmable gate array platform across 19 different algorithms. Our application-specific integrated circuit experiment on 45-nm CMOS technology shows that MRCA 2.0 reduces area and power consumption by 1.6 and 1.4 times, respectively, compared to the previous MRCA. Compared to related works, MRCA 2.0 outperforms in throughput by 1.63–28.3 times and in energy efficiency by 1.6–3.4 times, while also offering better flexibility in supporting more 64-bit algorithms.},
  archive      = {J_MICRO},
  author       = {Hoai Luan Pham and Vu Trung Duong Le and Tuan Hai Vu and Van Duy Tran and Van Tinh Nguyen and Thi Diem Tran and Yasuhiko Nakashima},
  doi          = {10.1109/MM.2024.3501313},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {78--89},
  shortjournal = {IEEE Micro},
  title        = {MRCA 2.0: An area-optimized multigrained reconfigurable cryptographic accelerator for securing blockchain-based internet of things systems},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A mobile semantic lidar SLAM processor with artificial-intelligence-based 3-D perception and spatiotemporal-aware computing. <em>MICRO</em>, <em>45</em>(2), 67--77. (<a href='https://doi.org/10.1109/MM.2024.3503414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A low-power artificial intelligence (AI)-based semantic lidar simultaneous localization and mapping (SLAM) processor is proposed to expand autonomous driving into emerging mobile robots. It combines point neural network (PNN)-based 3-D segmentation with lidar SLAM to minimize pose errors due to the lack of perception in previous SLAM. The proposed processor is designed with a heterogeneous multicore architecture that utilizes single instruction, multiple data and reconfigurable processing elements to fully support its three main operations: k-nearest neighbor (KNN), PNN, and nonlinear optimization. It accelerates KNN operations with spherical bin partitioning optimized for the distribution of lidar data to eliminate unnecessary search spaces. In addition, the proposed spatiotemporal-aware computing minimizes excessive memory overhead and workload imbalance in KNN and PNN operations. Consequently, fabricated with 28-nm CMOS technology, the processor achieves 8.245 mJ/frame of energy consumption and a maximum performance of 20.7-ms latency, successfully demonstrating real-time semantic lidar SLAM system with 99.86% lower power consumption compared to modern CPU+GPU platforms.},
  archive      = {J_MICRO},
  author       = {Jueun Jung and Seungbin Kim and Bokyoung Seo and Wuyoung Jang and Sangho Lee and Jeongmin Shin and Donghyeon Han and Kyuho Jason Lee},
  doi          = {10.1109/MM.2024.3503414},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {67--77},
  shortjournal = {IEEE Micro},
  title        = {A mobile semantic lidar SLAM processor with artificial-intelligence-based 3-D perception and spatiotemporal-aware computing},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on COOL chips. <em>MICRO</em>, <em>45</em>(2), 65--66. (<a href='https://doi.org/10.1109/MM.2025.3555464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This introduction to the special section on COOL Chips discusses the state-of-the-art low-power, high-speed chips and the challenges facing researchers. It introduces three articles that explore different solutions to reducing power consumption and enhancing chip performance.},
  archive      = {J_MICRO},
  author       = {Ryusuke Egawa and Yasutaka Wada},
  doi          = {10.1109/MM.2025.3555464},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {65--66},
  shortjournal = {IEEE Micro},
  title        = {Special issue on COOL chips},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECP: Improving the accuracy of congesting-packets identification in high-performance interconnection networks. <em>MICRO</em>, <em>45</em>(2), 56--64. (<a href='https://doi.org/10.1109/MM.2025.3527722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interconnection networks are crucial in data centers and supercomputers, ensuring high communication bandwidth and low latency under demanding traffic patterns from data-intensive applications. These patterns can cause congestion, affecting system performance if not addressed efficiently. Current congestion control techniques, like data center quantized congestion notification (DCQCN), struggle to precisely identify which packets cause congestion, leading to false positives. We propose the enhanced congestion point (ECP) mechanism, which accurately identifies congesting packets. ECP monitors packets at the head of switch ingress queues, flagging them as congesting when queue occupancy exceeds a threshold and packet requests are rejected. Additionally, ECP introduces a reevaluation mechanism to cancel the identification of congesting packets if they no longer contribute to congestion after rerouting. We evaluated ECP using a network simulator modeling various configurations and realistic traffic patterns. Results show that ECP significantly improves congestion detection accuracy with a low error margin, enhancing DCQCN performance.},
  archive      = {J_MICRO},
  author       = {Cristina Olmedilla and Jesus Escudero-Sahuquillo and Pedro Javier García and Francisco J. Quiles and Wenhao Sun and Long Yan and Yunping Lyu and Jose Duato},
  doi          = {10.1109/MM.2025.3527722},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {56--64},
  shortjournal = {IEEE Micro},
  title        = {ECP: Improving the accuracy of congesting-packets identification in high-performance interconnection networks},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward a standardized representation for deep learning collective algorithms. <em>MICRO</em>, <em>45</em>(2), 46--55. (<a href='https://doi.org/10.1109/MM.2025.3547363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosion of machine learning (ML) model size has led to its execution on distributed clusters at a large scale. Many works have tried to optimize the process of producing collective algorithms and running collective communications, which act as a bottleneck to distributed ML. However, different works use their own collective algorithm representation, resulting in a fragmented optimization environment. We propose a standardized workflow leveraging a common representation format based on Chakra Execution Trace, a widely used graph-based representation of distributed ML workloads. This common representation enables us to view collective communications at the same level as workload operations and decouple producer and consumer tools, enhancing interoperability and reducing engineering effort. We provide a proof of concept of this standardized workflow by simulating the collective algorithms generated by MSCCLang domain-specific language and the topology-aware collective algorithm synthesizer (TACOS) through the accelerator scaling for training simulator (ASTRA-sim) distributed ML simulator using multiple network configurations.},
  archive      = {J_MICRO},
  author       = {Jinsun Yoo and William Won and Meghan Cowan and Nan Jiang and Benjamin Klenk and Srinivas Sridharan and Tushar Krishna},
  doi          = {10.1109/MM.2025.3547363},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {46--55},
  shortjournal = {IEEE Micro},
  title        = {Toward a standardized representation for deep learning collective algorithms},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OHIO: Enhancing RDMA scalability in alltoall with optimized communication overlap. <em>MICRO</em>, <em>45</em>(2), 36--45. (<a href='https://doi.org/10.1109/MM.2024.3524891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of exascale computers has pushed a new boundary in computing capability, posing performance challenges in parallel programming models on how to exploit such systems efficiently. The message passing interface (MPI) is a dominant model for parallel programming. Among its primitives, MPI_Alltoall is a communication-intensive operation widely employed in numerous applications, yet it remains challenging to optimize. Alltoall algorithms are mainly classified into flat and hierarchical. The hierarchical designs avoid the slowdown of intra/inter-node communication by decoupling them. Hierarchical designs also reduce network congestion by limiting concurrently injected messages. This work demonstrates hierarchical designs also improve connection scalability in remote direct memory access networks. This improvement is attributed to the cache thrashing happening inside network adapters. All of these advantages of hierarchical schemes collectively contribute to the network scalability of Alltoall. We propose and evaluate a network-agnostic design on InfiniBand and Omni-Path clusters, showing benefits at both micro-benchmark and application levels over other MPI libraries.},
  archive      = {J_MICRO},
  author       = {Tu Tran and Goutham Kalikrishna Reddy Kuncham and Bharath Ramesh and Shulei Xu and Hari Subramoni and Dhabaleswar K. DK Panda},
  doi          = {10.1109/MM.2024.3524891},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {36--45},
  shortjournal = {IEEE Micro},
  title        = {OHIO: Enhancing RDMA scalability in alltoall with optimized communication overlap},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified collective communication: A unified library for CPU, GPU, and DPU collectives. <em>MICRO</em>, <em>45</em>(2), 26--35. (<a href='https://doi.org/10.1109/MM.2025.3534638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unified collective communication (UCC) is an application programming interface (API) and library implementation of collective communication operations. The goal of UCC is to provide a unified API and library serving the collective communication needs of various workloads running on a wide variety of system architectures. Particularly, we aim to unify the collective communication interfaces and semantics and provide a common implementation framework for: 1) parallel programming models, deep learning, and I/O; 2) collectives moving data in CPU main memory and device memory (GPU and data processing unit, DPU); and 3) collective operations using software point-to-point and hardware transports. In this article, we present an overview of UCC’s design, interfaces, semantics, and an implementation. We demonstrate UCC’s capabilities through evaluations with microbenchmarks representing diverse workloads and applications across various programming models, including MPI, Partitioned Global Address Space (OpenSHMEM), and PyTorch, on multiple hardware architectures (CPU, GPU, and DPU).},
  archive      = {J_MICRO},
  author       = {Manjunath Gorentla Venkata and Valentine Petrov and Sergey Lebedev and Devendar Bureddy and Ferrol Aderholdt and Joshua Ladd and Gil Bloch and Mike Dubman and Gilad Shainer},
  doi          = {10.1109/MM.2025.3534638},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {26--35},
  shortjournal = {IEEE Micro},
  title        = {Unified collective communication: A unified library for CPU, GPU, and DPU collectives},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spine-free networks for large language model training. <em>MICRO</em>, <em>45</em>(2), 18--25. (<a href='https://doi.org/10.1109/MM.2025.3540663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the optimal parallelization strategy of large language models (LLMs) and demonstrate that LLM training workloads generate sparse communication patterns in the network. Consequently, we argue that LLM training clusters do not require any-to-any full-bisection networks. We then propose Rail-only, a novel datacenter network architecture tailored to LLMs’ unique communication patterns. Rail-only networks eliminate the spine layer of conventional fabrics, resulting in lower cost and energy consumption. We demonstrate that Rail-only networks achieve the same training performance while reducing network cost by 38% to 77% and network power consumption by 37% to 75%, compared to traditional GPU clusters with full-bisection bandwidth.},
  archive      = {J_MICRO},
  author       = {Weiyang Wang and Manya Ghobadi},
  doi          = {10.1109/MM.2025.3540663},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {18--25},
  shortjournal = {IEEE Micro},
  title        = {Spine-free networks for large language model training},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding and characterizing communication characteristics for distributed transformer models. <em>MICRO</em>, <em>45</em>(2), 8--17. (<a href='https://doi.org/10.1109/MM.2025.3531323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transformer architecture has revolutionized many applications, such as large language models. This progress has been largely enabled by distributed training, yet communication remains a significant bottleneck. This article examines the communication behavior of transformer models, focusing on how different parallelism schemes in multinode/multi-GPU training communicate data. We use Generative Pre-trained Transformer-based language models as a case study due to their prevalence. We validate our empirical results using analytical models. Our analysis reveals practical insights and potential areas for further optimization in framework and high-performance computing middleware design.},
  archive      = {J_MICRO},
  author       = {Quentin Anthony and Benjamin Michalowicz and Jacob Hatef and Lang Xu and Mustafa Abduljabbar and Aamir Shafi and Hari Subramoni and Dhabaleswar K. DK Panda},
  doi          = {10.1109/MM.2025.3531323},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {8--17},
  shortjournal = {IEEE Micro},
  title        = {Understanding and characterizing communication characteristics for distributed transformer models},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on hot interconnects 31. <em>MICRO</em>, <em>45</em>(2), 6--7. (<a href='https://doi.org/10.1109/MM.2025.3555461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Whit Schonbein and Joseph Schuchart},
  doi          = {10.1109/MM.2025.3555461},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {6--7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot interconnects 31},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taiwan semiconductor manufacturing company’s $165 billion bet. <em>MICRO</em>, <em>45</em>(2), 4--5. (<a href='https://doi.org/10.1109/MM.2025.3557486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2025.3557486},
  journal      = {IEEE Micro},
  month        = {3-4},
  number       = {2},
  pages        = {4--5},
  shortjournal = {IEEE Micro},
  title        = {Taiwan semiconductor manufacturing company’s $165 billion bet},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring what matters: A fireside chat with joel emer. <em>MICRO</em>, <em>45</em>(1), 104--112. (<a href='https://doi.org/10.1109/MM.2025.3536716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Mariam Elgamal and Yueying Lisa Li},
  doi          = {10.1109/MM.2025.3536716},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {104--112},
  shortjournal = {IEEE Micro},
  title        = {Measuring what matters: A fireside chat with joel emer},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spillovers, bottlenecks, and more invention after invention. <em>MICRO</em>, <em>45</em>(1), 101--103. (<a href='https://doi.org/10.1109/MM.2025.3527797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2025.3527797},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {101--103},
  shortjournal = {IEEE Micro},
  title        = {Spillovers, bottlenecks, and more invention after invention},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of wisconsin alumni research foundation v. Apple—Part II. <em>MICRO</em>, <em>45</em>(1), 95--100. (<a href='https://doi.org/10.1109/MM.2025.3536656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2025.3536656},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {95--100},
  shortjournal = {IEEE Micro},
  title        = {A review of wisconsin alumni research foundation v. Apple—Part II},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Management of TinyML-enabled internet of things devices. <em>MICRO</em>, <em>45</em>(1), 87--94. (<a href='https://doi.org/10.1109/MM.2024.3382488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) systems are used in many aspects of our lives. Thanks to TinyML algorithms, they provide several new and smart functionalities that were impossible before. However, implementing such a solution on a large scale and maintaining it over time is a big challenge. This is due to the need to not only update the device firmware to remove errors and extend the functionality of the devices but also to update the ML models. This imposes several requirements for device monitoring and management mechanisms. In this article, we discuss not only the required aspects that an IoT system should meet, but we also show how they fit into the ML model management process.},
  archive      = {J_MICRO},
  author       = {Tomasz Szydło and Marcin Nagy},
  doi          = {10.1109/MM.2024.3382488},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {87--94},
  shortjournal = {IEEE Micro},
  title        = {Management of TinyML-enabled internet of things devices},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing tiny machine learning operations: Robust model updates in the internet of intelligent vehicles. <em>MICRO</em>, <em>45</em>(1), 76--86. (<a href='https://doi.org/10.1109/MM.2024.3354323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Intelligent Vehicles is becoming increasingly important, and embedded machine learning is gaining popularity due to new development paradigms. However, the demand for machine learning model updates on embedded systems has become relevant in multiple scenarios. This article proposes a methodology for tiny machine learning operations within the context of the Internet of Intelligent Vehicles, utilizing affordable microcontrollers based on the ESP32 platform. The solution presented in the article consists of two ESP32 devices: one functioning as a radio station (RS) and the other as the microcontroller of an onboard diagnostic (OBD-II) scanner. The RS hosts the updated model and transmits it to the OBD-II scanner using the Espressif Systems Peer-to-Peer Over Wi-Fi communication protocol over 802.11 Wi-Fi. Experimental results demonstrate significant improvement in model performance postupdate, but the article also identifies critical challenges to model robustness because of the use of the interpreter method on microcontrollers.},
  archive      = {J_MICRO},
  author       = {Thommas K. S. Flores and Ivanovitch Silva and Mariana B. Azevedo and Thaís de A. de Medeiros and Morsinaldo de A. Medeiros and Daniel G. Costa and Paolo Ferrari and Emiliano Sisinni},
  doi          = {10.1109/MM.2024.3354323},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {76--86},
  shortjournal = {IEEE Micro},
  title        = {Advancing tiny machine learning operations: Robust model updates in the internet of intelligent vehicles},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A test, debug, and silicon lifecycle management architecture for a UCIe-based open chiplet ecosystem. <em>MICRO</em>, <em>45</em>(1), 67--74. (<a href='https://doi.org/10.1109/MM.2024.3435702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, chiplets have become foundational to continuing the pace of innovation. The chiplet market is projected to significantly ramp up in the next few years, and we need to be able to support chiplets from multiple vendors in the same package. These realities create new test and debug challenges through all stages of chiplet productization from sort to in field. There is a scaling challenge to test and debug efficiently without growing package pin count; there is a challenge of test time reduction and interoperability when testing and debugging with chiplets from multiple vendors. Additionally, silicon lifecycle management (SLM) of chiplets in a system in package (SIP) for manufacturing quality monitoring and in-field predictive analysis is a significant challenge. This article presents a comprehensive test, debug, and SLM architecture for addressing these challenges in a Universal Chiplet Interconnect Express-based SIP, paving the way for an open chiplet ecosystem.},
  archive      = {J_MICRO},
  author       = {Sridhar Muthrasanallur and Yervant Zorian},
  doi          = {10.1109/MM.2024.3435702},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {67--74},
  shortjournal = {IEEE Micro},
  title        = {A test, debug, and silicon lifecycle management architecture for a UCIe-based open chiplet ecosystem},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interconnect design for heterogeneous integration of chiplets in the AMD instinct MI300X accelerator. <em>MICRO</em>, <em>45</em>(1), 57--66. (<a href='https://doi.org/10.1109/MM.2024.3462351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semiconductor industry has deployed chiplet-based system-on-chip architectures for several years. Central to a successful chiplet-based product is the die-to-die interconnect technology between the chiplets. Based on product requirements, some chiplet designs can utilize a single interconnect technology such as 2-D signals over an organic substrate or higher-density 2.5-D integration technologies. With increasing demands on compute and memory capabilities, high-performance products are now moving to heterogeneous integration, which combines multiple advanced packaging technologies all within a single system on chip. To address the market demands for high-performance artificial intelligence solutions, AMD has introduced the AMD Instinct MI300X accelerator. This article details the chiplet interconnect design required to support a sophisticated package that takes high-volume heterogeneous integration to a new level.},
  archive      = {J_MICRO},
  author       = {Alan Smith and Gabriel H. Loh and Samuel Naffziger and John Wuu and Nathan Kalyanasundharam and Eric Chapman and Raja Swaminathan and Tyrone Huang and Wonjun Jung and Alexander Kaganov and Hugh McIntyre and Ramon Mangaser},
  doi          = {10.1109/MM.2024.3462351},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {57--66},
  shortjournal = {IEEE Micro},
  title        = {Interconnect design for heterogeneous integration of chiplets in the AMD instinct MI300X accelerator},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison of single-ended, NRZ unidirectional signaling and single-ended, NRZ simultaneous-bidirectional signaling for die-to-die links. <em>MICRO</em>, <em>45</em>(1), 48--56. (<a href='https://doi.org/10.1109/MM.2024.3436008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article compares single-ended, NRZ unidirectional (UD) signaling to single-ended, NRZ simultaneous bidirectional signaling for ultrashort-reach (USR) die-to-die (D2D) links in terms of power and bit error rate (BER). We show that for the same transceiver architecture, aggregate bandwidth, and BER, a simultaneous bidirectional link is able to achieve better power efficiency and a wider eye opening than a UD link. Furthermore, the signal-integrity constraints for each signaling approach is examined in terms of the insertion-loss-to-crosstalk ratio and main-cursor-to-crosstalk ratio for three different arrangements of USR D2D links in organic substrates.},
  archive      = {J_MICRO},
  author       = {Durand Jarrett-Amor and Tony Chan Carusone},
  doi          = {10.1109/MM.2024.3436008},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {48--56},
  shortjournal = {IEEE Micro},
  title        = {A comparison of single-ended, NRZ unidirectional signaling and single-ended, NRZ simultaneous-bidirectional signaling for die-to-die links},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient parallel interconnects for chiplet integration. <em>MICRO</em>, <em>45</em>(1), 41--47. (<a href='https://doi.org/10.1109/MM.2024.3450841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating multiple chiplets in advanced packaging requires high-bandwidth and energy-efficient chiplet-to-chiplet interconnects. Parallel interconnects have emerged as the preferred interface for chiplet integration. This article provides an overview of the physical layer design of the Advanced Interface Bus (AIB), highlighting its simplicity and high energy efficiency. Additionally, we present a concrete example of successful chiplet integration employing AIB.},
  archive      = {J_MICRO},
  author       = {Wei Tang and Chester Liu and Zhengya Zhang},
  doi          = {10.1109/MM.2024.3450841},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {41--47},
  shortjournal = {IEEE Micro},
  title        = {Energy-efficient parallel interconnects for chiplet integration},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-design of interchiplet, package, and system interconnect protocols. <em>MICRO</em>, <em>45</em>(1), 35--40. (<a href='https://doi.org/10.1109/MM.2024.3447711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) applications are adopting modular tile design paradigms for compute and networking chiplets. The integration of UCIe as a die-to-die interconnect for low latency and power savings allows a disaggregation of large monolithic dies. This paper covers interchiplet architectures that can be built toward a chiplet portfolio for escalating bandwidth needs in networking and AI/machine learning. The electrical layer of UCIe design verification is illustrated. In addition, the topic of package design and challenges associated with the integration of the physical layer, in both standard and advanced packaging, are discussed.},
  archive      = {J_MICRO},
  author       = {Tony Chan Carusone and Dustin Dunwell and Sundeep Gupta and Letizia Giuliano and Adrien Auge and Michael Klempa and Sue Hung Fung},
  doi          = {10.1109/MM.2024.3447711},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {35--40},
  shortjournal = {IEEE Micro},
  title        = {Co-design of interchiplet, package, and system interconnect protocols},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCIe standard: Enhancing die-to-die connectivity in modern packaging. <em>MICRO</em>, <em>45</em>(1), 26--34. (<a href='https://doi.org/10.1109/MM.2025.3526048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, packaging plays a crucial role in determining chip performance in systems. As transistor sizes shrink and chip sizes grow to meet increasing processing demands, wafer yield drops. Connecting smaller chips within a package or using multidie designs has gained attention as it improves yield, enables intellectual property reuse, enhances performance, and lowers costs. The Universal Chiplet Interconnect Express standard, developed by industry leaders, supports these efforts by enabling different dies, or chiplets, to work together smoothly. Various structures based on this standard, such as standard 2-D organic substrate, 2.5-D through-silicon via interposer, Fanout redistribution layer organic interposer, and 3-D hybrid bonding, have been suggested. Despite the benefits, die integration faces challenges due to dense and intricate connections. This article focuses on achieving effective die-to-die connectivity using organic and advanced packaging. These patterns have been validated through simulations and the fabrication of the test chip.},
  archive      = {J_MICRO},
  author       = {Au Huynh and Kent Stahn and Manuel Mota and Christian de Verteuil and Jennifer Pyon and Reza Movahedinia},
  doi          = {10.1109/MM.2025.3526048},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {26--34},
  shortjournal = {IEEE Micro},
  title        = {UCIe standard: Enhancing die-to-die connectivity in modern packaging},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCIe: Standard for an open chiplet ecosystem. <em>MICRO</em>, <em>45</em>(1), 16--25. (<a href='https://doi.org/10.1109/MM.2024.3451532'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universal chiplet interconnect express (UCIe) is an open industry standard die-to-die physical layer, link layer, and protocol layer for chiplets. It has industry-leading key performance indicators (KPIs) and has successfully coalesced the industry around a common die-to-die specification. The ultimate goal of UCIe is to enable the rapid creation of system-in-package (SiP) solutions from an open ecosystem where chiplets are designed and manufactured by different vendors and are offered as standard products. This paper provides an overview of existing UCIe technology and highlights the work being done to create the next layer of standards required for an open chiplet ecosystem.},
  archive      = {J_MICRO},
  author       = {Peter Onufryk and Swadesh Choudhary},
  doi          = {10.1109/MM.2024.3451532},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {16--25},
  shortjournal = {IEEE Micro},
  title        = {UCIe: Standard for an open chiplet ecosystem},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disaggregated designs: Technology challenges and enablers. <em>MICRO</em>, <em>45</em>(1), 9--15. (<a href='https://doi.org/10.1109/MM.2024.3524130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unsustainable pace of die-size growth has come up against the reticle limit. The rise in cost per transistor (CPT) is outweighing scaling benefits from advances in generational process technology. These diminishing silicon economies of scale have pushed foundries, EDA companies, and the manufacturing ecosystem to enable chiplet designs. Developments in packaging technology and standardization of die-to-die interfaces provide technology gains offsetting challenges of die sizes and CPT. Standards bodies and IP implementers have risen to the challenge, providing solutions to interface bottlenecks. We examine standards for chiplet and die-to-die interfaces, such as UCIe and the work done in the industry to enable these. Architectural partitions for chiplet implementations and development of reusable components that enable a future chiplet marketplace are considered. The juggernaut for differentiation and disaggregation in the more-than-Moore era continues to build momentum. We examine these trends and recent technology advances, looking at future directions for implementations.},
  archive      = {J_MICRO},
  author       = {Boyd Phelps and Arif Khan},
  doi          = {10.1109/MM.2024.3524130},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {9--15},
  shortjournal = {IEEE Micro},
  title        = {Disaggregated designs: Technology challenges and enablers},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on interconnects for chiplet integration technologies. <em>MICRO</em>, <em>45</em>(1), 6--8. (<a href='https://doi.org/10.1109/MM.2025.3534457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Debendra Das Sharma and Nam Sung Kim},
  doi          = {10.1109/MM.2025.3534457},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {6--8},
  shortjournal = {IEEE Micro},
  title        = {Special issue on interconnects for chiplet integration technologies},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rise of the agentic AI workforce. <em>MICRO</em>, <em>45</em>(1), 4--5. (<a href='https://doi.org/10.1109/MM.2025.3535912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2025.3535912},
  journal      = {IEEE Micro},
  month        = {1-2},
  number       = {1},
  pages        = {4--5},
  shortjournal = {IEEE Micro},
  title        = {Rise of the agentic AI workforce},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
