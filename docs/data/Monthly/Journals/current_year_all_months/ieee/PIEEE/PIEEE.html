<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PIEEE</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pieee">PIEEE - 18</h2>
<ul>
<li><details>
<summary>
(2025). Protecting the mixed-signal domain: Secure ADCs for internet of things devices. <em>PIEEE</em>, <em>113</em>(6), 586-604. (<a href='https://doi.org/10.1109/JPROC.2025.3605535'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analog-to-digital converters (ADCs) are a standard building block of most Internet of Things (IoT) devices, used to convert between analog signals of the physical world and digital values for efficient storage and computation. While decades of research have explored methods to optimize the traditional power, performance, and area parameters of these circuits, a new requirement has emerged in the last five years for ADCs—security. Due to the deployment of these devices at the edge outside of a trusted computing base, there is a potential for various eavesdropping and tampering attacks. This can have a severe impact on the privacy and integrity of sensor data, which cannot be corrected for through the secure design of other blocks that follow the front end. In this article, we explore the recent work in ADC security and analyze what has been accomplished as well as what remains to be done for the successful deployment of secure ADCs in commercial systems.},
  archive      = {J_PIEEE},
  author       = {Maitreyi Ashok and Ruicong Chen and Taehoon Jeong and Anantha P. Chandrakasan and Hae-Seung Lee},
  doi          = {10.1109/JPROC.2025.3605535},
  journal      = {Proceedings of the IEEE},
  month        = {6},
  number       = {6},
  pages        = {586-604},
  shortjournal = {Proc. IEEE},
  title        = {Protecting the mixed-signal domain: Secure ADCs for internet of things devices},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for distribution system operations: A tutorial and survey. <em>PIEEE</em>, <em>113</em>(6), 557-585. (<a href='https://doi.org/10.1109/JPROC.2025.3599840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of modern electric power distribution systems into complex networks of interconnected active devices, distributed generation (DG), and storage poses increasing difficulties for system operators. The large-scale integration of distributed energy resources (DERs) and the rapid exchange of measurement data via communication networks present major opportunities for advancing grid operations but also introduce greater uncertainty, higher data dimensionality, more complex network and device models, and challenging control and optimization problems. Deep reinforcement learning (DRL) algorithms are promising in addressing these challenges. However, they have not been effectively adapted for power systems applications, requiring extensive customization for implementation and evaluation. This has resulted in reproducibility challenges and a steep learning curve for researchers new to applying DRL algorithms to the power systems domain. To bridge these gaps, this tutorial aims to serve as a valuable resource for researchers interested in exploring learning-based algorithms to operate active power distribution networks. Specifically, this work presents a generalized process for translating sequential decision-making problems in power distribution systems into Markov decision process (MDP) formulations, illustrated through concrete grid service examples. Additionally, we introduce a simple environment design strategy to develop and evaluate example DRL algorithms for distribution system applications, complete with an included code repository to guide users through environment construction.},
  archive      = {J_PIEEE},
  author       = {Daniel Glover and Gayathri Krishnamoorthy and Hongda Ren and Anamika Dubey and Assefaw Gebremedhin},
  doi          = {10.1109/JPROC.2025.3599840},
  journal      = {Proceedings of the IEEE},
  month        = {6},
  number       = {6},
  pages        = {557-585},
  shortjournal = {Proc. IEEE},
  title        = {Deep reinforcement learning for distribution system operations: A tutorial and survey},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep-learning-empowered super resolution: A comprehensive survey and future prospects. <em>PIEEE</em>, <em>113</em>(6), 516-556. (<a href='https://doi.org/10.1109/JPROC.2025.3613233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super resolution (SR) has garnered significant attention within the computer vision community, driven by advances in deep learning (DL) techniques and the growing demand for high-quality visual applications. With the expansion of this field, numerous surveys have emerged. Most existing surveys focus on specific domains, lacking a comprehensive overview of this field. Here, we present an in-depth review of diverse SR methods, encompassing single-image SR (SISR), video SR (VSR), stereo SR (SSR), and light field SR (LFSR). We extensively cover over 150 SISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR and LFSR. We analyze methodologies, datasets, evaluation protocols, empirical results, and complexity. In addition, we conducted a taxonomy based on each backbone structure according to the diverse purposes. We also explore valuable yet understudied open issues in the field. We believe that this work will serve as a valuable resource and offer guidance to researchers in this domain. To facilitate access to related work, we created a dedicated repository available at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review},
  archive      = {J_PIEEE},
  author       = {Le Zhang and Ao Li and Qibin Hou and Ce Zhu and Yonina C. Eldar},
  doi          = {10.1109/JPROC.2025.3613233},
  journal      = {Proceedings of the IEEE},
  month        = {6},
  number       = {6},
  pages        = {516-556},
  shortjournal = {Proc. IEEE},
  title        = {Deep-learning-empowered super resolution: A comprehensive survey and future prospects},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain–Computer Interface—A brain-in-the-loop communication system. <em>PIEEE</em>, <em>113</em>(5), 478-511. (<a href='https://doi.org/10.1109/JPROC.2025.3600389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain–computer interface (BCI) establishes a direct communication system between the brain and a computer or other external devices. Since the inception of BCI technology half a century ago, it has advanced rapidly and developed into an active area of frontier research in modern applied science and technology. This article provides a comprehensive survey on BCI with respect to a brain-in-the-loop communication system. In the present work, we first introduce the underlying architecture of the BCI system from the theoretical and methodological perspectives of communication systems. The key technologies are then detailed, including the construction of BCI system, brain-to-computer (B2C) communication, computer-to-brain (C2B) communication, and multiuser BCI systems. Additionally, this article discusses the various applications of BCI and the challenges they face. Finally, this article discusses BCI’s future development, with an emphasis on the convergence of human intelligence (HI) and artificial intelligence (AI), and the interaction of BCI with wireless communication and the metaverse.},
  archive      = {J_PIEEE},
  author       = {Xiaorong Gao and Yijun Wang and Xiaogang Chen and Bingchuan Liu and Shangkai Gao},
  doi          = {10.1109/JPROC.2025.3600389},
  journal      = {Proceedings of the IEEE},
  month        = {5},
  number       = {5},
  pages        = {478-511},
  shortjournal = {Proc. IEEE},
  title        = {Brain–Computer Interface—A brain-in-the-loop communication system},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicle-to-everything cooperative perception for autonomous driving. <em>PIEEE</em>, <em>113</em>(5), 443-477. (<a href='https://doi.org/10.1109/JPROC.2025.3600903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving fully autonomous driving with enhanced safety and efficiency relies on vehicle-to-everything (V2X) cooperative perception (CP), which enables vehicles to share perception data, thereby enhancing situational awareness and overcoming the limitations of the sensing ability of individual vehicles. V2X CP plays a crucial role in extending the perception range, increasing detection accuracy, and supporting more robust decision-making and control in complex environments. This article provides a comprehensive survey of recent developments in V2X CP, introducing mathematical models that characterize the perception process under different collaboration strategies. Key techniques for enabling reliable perception sharing, such as agent selection, data alignment, and feature fusion, are examined in detail. In addition, major challenges are discussed, including differences in agents and models, uncertainty in perception outputs, and the impact of communication constraints such as transmission delay and data loss. This article concludes by outlining promising research directions, including privacy-preserving artificial intelligence methods, collaborative intelligence, and integrated sensing frameworks to support future advancements in V2X CP.},
  archive      = {J_PIEEE},
  author       = {Tao Huang and Jianan Liu and Xi Zhou and Dinh C. Nguyen and Mostafa Rahimi Azghadi and Yuxuan Xia and Qing-Long Han and Sumei Sun},
  doi          = {10.1109/JPROC.2025.3600903},
  journal      = {Proceedings of the IEEE},
  month        = {5},
  number       = {5},
  pages        = {443-477},
  shortjournal = {Proc. IEEE},
  title        = {Vehicle-to-everything cooperative perception for autonomous driving},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drone-as-a-service: Research challenges and directions. <em>PIEEE</em>, <em>113</em>(5), 416-442. (<a href='https://doi.org/10.1109/JPROC.2025.3599126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct a survey on drones used as a service, denoted as drone-as-a-service (DaaS). We develop a novel taxonomy based on DaaS functions, research tasks, and application domains. We provide a discussion on drones and their associated capabilities based on their type of use. We propose a three-layered DaaS system architecture that vertically integrates cloud computing, drones, and services as a reference framework to compare existing drone service implementations. Additionally, we propose a representative uncertainty-aware DaaS model for delivery scenarios, illustrating how service definitions can incorporate both functional and nonfunctional attributes under dynamic environmental conditions. Finally, we identify and discuss future research directions and open problems related to the use of drones for service delivery.},
  archive      = {J_PIEEE},
  author       = {Ali Hamdi and Balsam Alkouz and Babar Shahzaad and Athman Bouguettaya and Azadeh Ghari Neiat and Flora Salim and Du Yong Kim},
  doi          = {10.1109/JPROC.2025.3599126},
  journal      = {Proceedings of the IEEE},
  month        = {5},
  number       = {5},
  pages        = {416-442},
  shortjournal = {Proc. IEEE},
  title        = {Drone-as-a-service: Research challenges and directions},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated domain generalization: A survey. <em>PIEEE</em>, <em>113</em>(4), 370-410. (<a href='https://doi.org/10.1109/JPROC.2025.3596173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) typically relies on the assumption that training and testing distributions are identical and that data are centrally stored for training and testing. However, in real-world scenarios, distributions may differ significantly, and data are often distributed across different devices, organizations, or edge nodes. Consequently, it is to develop models capable of effectively generalizing across unseen distributions in data spanning various domains. In response to this challenge, there has been a surge of interest in federated domain generalization (FDG) in recent years. FDG synergizes federated learning (FL) and domain generalization (DG) techniques, facilitating collaborative model development across diverse source domains for effective generalization to unseen domains, all while maintaining data privacy. However, generalizing the federated model under domain shifts remains a complex, underexplored issue. This article provides a comprehensive survey of the latest advancements in this field. Initially, we discuss the development process from traditional ML to domain adaptation (DA) and DG, leading to FDG, as well as provide the corresponding formal definition. Subsequently, we classify recent methodologies into four distinct categories: federated domain alignment (FDAL), data manipulation (DM), learning strategies (LSs), and aggregation optimization (AO), detailing appropriate algorithms for each. We then overview commonly utilized datasets, applications, evaluations, and benchmarks. Conclusively, this survey outlines potential future research directions.},
  archive      = {J_PIEEE},
  author       = {Ying Li and Xingwei Wang and Rongfei Zeng and Praveen Kumar Donta and Ilir Murturi and Min Huang and Schahram Dustdar},
  doi          = {10.1109/JPROC.2025.3596173},
  journal      = {Proceedings of the IEEE},
  month        = {4},
  number       = {4},
  pages        = {370-410},
  shortjournal = {Proc. IEEE},
  title        = {Federated domain generalization: A survey},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Talkative power conversion: A tutorial. <em>PIEEE</em>, <em>113</em>(4), 344-369. (<a href='https://doi.org/10.1109/JPROC.2025.3577229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides a systematic overview of the basics of talkative power conversion (TPC). TPC is an emerging technique for simultaneous information and power transmission, in which data modulation is integrated into a switched-mode power converter. The data sequence is embedded in the ripple voltage, which is superimposing the output voltage of the converter. In contrast to conventional power line communication (PLC), TPC can be used universally, not only in grid applications. Aspects of power electronics (PE) and digital communication are presented in a structured form, including new perspectives such as multiple-input multiple-output (MIMO) techniques applied to TPC, adaptive modulation and channel coding, and advanced receiver design with adaptive channel and load estimation. The new aspects aim to mitigate the inherent shortcomings of TPC.},
  archive      = {J_PIEEE},
  author       = {Peter Adam Hoeher and Yang Leng and Rongwu Zhu and Marco Liserre},
  doi          = {10.1109/JPROC.2025.3577229},
  journal      = {Proceedings of the IEEE},
  month        = {4},
  number       = {4},
  pages        = {344-369},
  shortjournal = {Proc. IEEE},
  title        = {Talkative power conversion: A tutorial},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer audition: From task-specific machine learning to foundation models. <em>PIEEE</em>, <em>113</em>(4), 317-343. (<a href='https://doi.org/10.1109/JPROC.2025.3593952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models (FMs) are increasingly spearheading recent advances on a variety of tasks that fall under the purview of computer audition—i.e., the use of machines to understand sounds. They feature several advantages over traditional pipelines: among others, the ability to consolidate multiple tasks in a single model, the option to leverage knowledge from other modalities, and the readily available interaction with human users. Naturally, these promises have created substantial excitement in the audio community and have led to a wave of early attempts to build new, generalpurpose FMs for audio. In the present contribution, we give an overview of computational audio analysis as it transitions from traditional pipelines toward auditory FMs. Our work highlights the key operating principles that underpin those models and showcases how they can accommodate multiple tasks that the audio community previously tackled separately.},
  archive      = {J_PIEEE},
  author       = {Andreas Triantafyllopoulos and Iosif Tsangko and Alexander Gebhard and Annamaria Mesaros and Tuomas Virtanen and Björn W. Schuller},
  doi          = {10.1109/JPROC.2025.3593952},
  journal      = {Proceedings of the IEEE},
  month        = {4},
  number       = {4},
  pages        = {317-343},
  shortjournal = {Proc. IEEE},
  title        = {Computer audition: From task-specific machine learning to foundation models},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fighting malicious media data: A survey on tampering detection and deepfake detection. <em>PIEEE</em>, <em>113</em>(3), 287-311. (<a href='https://doi.org/10.1109/JPROC.2025.3576367'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online media data, in the form of images and videos, are becoming mainstream communication channels. However, recent advances in deep learning (DL), particularly deep generative models, open the doors for producing perceptually convincing images and videos at a low cost, which not only poses a serious threat to the trustworthiness of digital information but also has severe societal implications. This motivates a growing interest in research in media tampering detection (TD), i.e., using DL techniques to examine whether media data have been maliciously manipulated. Depending on the content of the targeted images, media forgery could be divided into image tampering and Deepfake techniques. The former typically moves or erases the visual elements in ordinary images, while the latter manipulates the expressions and even the identity of human faces. Accordingly, the means of defense include image TD and Deepfake detection (DFD), which share a wide variety of properties. In this article, we provide a comprehensive review of the current media TD approaches and discuss the challenges and trends in this field for future research.},
  archive      = {J_PIEEE},
  author       = {Junke Wang and Zhenxin Li and Chao Zhang and Jingjing Chen and Zuxuan Wu and Larry S. Davis and Yu-Gang Jiang},
  doi          = {10.1109/JPROC.2025.3576367},
  journal      = {Proceedings of the IEEE},
  month        = {3},
  number       = {3},
  pages        = {287-311},
  shortjournal = {Proc. IEEE},
  title        = {Fighting malicious media data: A survey on tampering detection and deepfake detection},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMG acquisition and processing for hand movement decoding on embedded systems: State of the art and challenges. <em>PIEEE</em>, <em>113</em>(3), 256-286. (<a href='https://doi.org/10.1109/JPROC.2025.3581995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electromyography (EMG) signal is particularly useful in monitoring muscle activity, and it can be acquired noninvasively on the skin surface. Thanks to these key characteristics, EMG-based human–machine interfaces (HMIs) for prosthetic myocontrol, as well as gesture recognition, are becoming widespread. A key challenge in this context is to design embedded systems to process EMG signals and generate motor commands with miniaturized, unobtrusive, and low-power devices, reliably and in real time, at a relatively low cost to provide continuous monitoring without causing stigma or discomfort. This article presents an in-depth review of the current status and future research challenges in systems and circuits for EMG acquisition and processing. We start by illustrating the sensor interfaces and acquisition systems required for signal analysis to provide efficient and effective ways of understanding the signal and its nature. We, then, focus on conventional state-of-the-art (SoA) EMG gesture recognition algorithms as well as novel architectures that tackle EMG processing challenges, i.e., hyperdimensional computing (HDC), blind source separation (BSS), and spiking neural networks (SNNs). Finally, we discuss open challenges, such as EMG variability, natural control, and efficient computation, to bring the myocontrol completely out of the laboratory, filling the gap between research prototypes and real-world applications.},
  archive      = {J_PIEEE},
  author       = {Simone Benatti and Elisa Donati and Ali Moin and Marcello Zanghieri and Mattia Orlandi and Alessio Burrello and Fiorenzo Artoni and Silvestro Micera and Luca Benini and Jan M. Rabaey},
  doi          = {10.1109/JPROC.2025.3581995},
  journal      = {Proceedings of the IEEE},
  month        = {3},
  number       = {3},
  pages        = {256-286},
  shortjournal = {Proc. IEEE},
  title        = {EMG acquisition and processing for hand movement decoding on embedded systems: State of the art and challenges},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of safe reinforcement learning methods for modern power systems. <em>PIEEE</em>, <em>113</em>(3), 213-255. (<a href='https://doi.org/10.1109/JPROC.2025.3584656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the availability of more comprehensive measurement data in modern power systems, reinforcement learning (RL) has gained significant interest in operation and control. Conventional RL relies on trial-and-error interactions with the environment and reward feedback, which often leads to exploring unsafe operating regions and executing unsafe actions, especially when deployed in real-world power systems. To address these challenges, safe RL has been proposed to optimize operational objectives while ensuring safety constraints are met, keeping actions and states within safe regions throughout both training and deployment. Rather than relying solely on manually designed penalty terms for unsafe actions, as is common in conventional RL, safe RL methods reviewed here primarily leverage advanced and proactive mechanisms. These include techniques such as Lagrangian relaxation, safety layers, and theoretical guarantees like Lyapunov functions to rigorously enforce safety boundaries. This article provides a comprehensive review of safe RL methods and their applications across various power system operations and control domains, including security control, real-time operation, operational planning, and emerging areas. It summarizes existing safe RL techniques, evaluates their performance, analyzes suitable deployment scenarios, and examines algorithm benchmarks and application environments. This article also highlights real-world implementation cases and identifies critical challenges such as scalability in large-scale systems and robustness under uncertainty, providing potential solutions and outlining future directions to advance the reliable integration and deployment of safe RL in modern power systems.},
  archive      = {J_PIEEE},
  author       = {Tong Su and Tong Wu and Junbo Zhao and Anna Scaglione and Le Xie},
  doi          = {10.1109/JPROC.2025.3584656},
  journal      = {Proceedings of the IEEE},
  month        = {3},
  number       = {3},
  pages        = {213-255},
  shortjournal = {Proc. IEEE},
  title        = {A review of safe reinforcement learning methods for modern power systems},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey and comparative analysis of number systems for deep neural networks. <em>PIEEE</em>, <em>113</em>(2), 172-207. (<a href='https://doi.org/10.1109/JPROC.2025.3578756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are indispensable in various artificial intelligence (AI) applications. However, their inherent complexity presents significant challenges, particularly when deploying them on resource-constrained devices. To overcome these hurdles, academia and industry are actively seeking ways to accelerate and optimize DNN implementations. A significant area of research revolves around discovering more effective methods to represent the enormous data volumes processed by DNNs. Traditional number systems (NSs) have proven nonoptimal for this task, prompting extensive exploration into alternative and bespoke systems for DNNs. This survey aims to comprehensively discuss various NSs utilized to efficiently represent DNN data. These systems are categorized mainly based on their impact on DNN performance and hardware implementation. This survey offers an overview of these categorized NSs and delves into different subsystems within each, outlining their effect on DNN performance and hardware design. Furthermore, these systems are compared quantitatively and qualitatively concerning their expected quantization error, memory utilization, and computational requirements. This survey also emphasizes the challenges linked with each system and the diverse proposed solutions to address them. Insights into the utilization of these NSs for sophisticated DNNs are also presented in this survey. Readers will acquire a deeper understanding of the importance of efficient NSs for DNNs, explore commonly used systems, comprehend the tradeoffs between these systems, delve into design considerations influencing their impact on DNN performance, and discover recent trends and potential research avenues in this field.},
  archive      = {J_PIEEE},
  author       = {Ghada Alsuhli and Vasilis Sakellariou and Hani Saleh and Mahmoud Al-Qutayri and Baker Mohammad and Thanos Stouraitis},
  doi          = {10.1109/JPROC.2025.3578756},
  journal      = {Proceedings of the IEEE},
  month        = {2},
  number       = {2},
  pages        = {172-207},
  shortjournal = {Proc. IEEE},
  title        = {A survey and comparative analysis of number systems for deep neural networks},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of artificial intelligence techniques for talent analytics. <em>PIEEE</em>, <em>113</em>(2), 125-171. (<a href='https://doi.org/10.1109/JPROC.2025.3572744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s competitive and fast-evolving business environment, it is critical for organizations to rethink how to make talent-related decisions in a quantitative manner. Indeed, the recent development of big data and artificial intelligence (AI) techniques has revolutionized human resource management (HRM). The availability of large-scale talent and management-related data provides unparalleled opportunities for business leaders to comprehend organizational behaviors and gain tangible knowledge from a data science perspective, which, in turn, delivers intelligence for real-time decision-making and effective talent management for their organizations. In the last decade, talent analytics has emerged as a promising field in applied data science for HRM, garnering significant attention from AI communities and inspiring numerous research efforts. To this end, we present an up-to-date and comprehensive survey on AI technologies used for talent analytics in the field of HRM. Specifically, we first provide the background knowledge of talent analytics and categorize various pertinent data. Subsequently, we offer a comprehensive taxonomy of relevant research efforts, categorized based on three distinct application-driven scenarios at different levels: talent management, organization management, and labor market analysis. In conclusion, we summarize the open challenges and potential prospects for future research directions in the domain of AI-driven talent analytics.},
  archive      = {J_PIEEE},
  author       = {Chuan Qin and Le Zhang and Yihang Cheng and Rui Zha and Dazhong Shen and Qi Zhang and Xi Chen and Ying Sun and Chen Zhu and Hengshu Zhu and Hui Xiong},
  doi          = {10.1109/JPROC.2025.3572744},
  journal      = {Proceedings of the IEEE},
  month        = {2},
  number       = {2},
  pages        = {125-171},
  shortjournal = {Proc. IEEE},
  title        = {A comprehensive survey of artificial intelligence techniques for talent analytics},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The quantum tortoise and the classical hare: When will quantum computers outpace classical ones and when will they be left behind?. <em>PIEEE</em>, <em>113</em>(2), 113-124. (<a href='https://doi.org/10.1109/JPROC.2025.3574102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the children’s story of the Tortoise and the Hare, the speedier Hare is outpaced by a Tortoise with other advantages (diligence). An analogous contest is happening in computing, between a Quantum Tortoise and a Classical Hare. Here, the Classical Hare’s speed advantage is literal—classical computers run faster than quantum ones. Like his namesake, the Quantum Tortoise is slower, but also has an advantage—in this case, the ability to run algorithms that are unavailable to classical computers. When this algorithmic advantage is substantial enough, the Quantum Tortoise can beat the Classical Hare and solve a problem faster. This article analyzes when the Quantum Tortoise will beat the Classical Hare—and when it will not.},
  archive      = {J_PIEEE},
  author       = {Sukwoong Choi and William S. Moses and Neil Thompson},
  doi          = {10.1109/JPROC.2025.3574102},
  journal      = {Proceedings of the IEEE},
  month        = {2},
  number       = {2},
  pages        = {113-124},
  shortjournal = {Proc. IEEE},
  title        = {The quantum tortoise and the classical hare: When will quantum computers outpace classical ones and when will they be left behind?},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reactive power implications of penetrating inverter-based renewable and storage resources in future grids toward energy Transition—A review. <em>PIEEE</em>, <em>113</em>(1), 66-104. (<a href='https://doi.org/10.1109/JPROC.2025.3555509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transitioning to net-zero emission energy systems is currently on the agenda in various countries to tackle climate change, a global challenge that threatens the lives of future generations. To fully decarbonize energy systems, a radical paradigm shift through deep integration of renewable resources supported by storage technologies is envisaged in multisector energy systems, especially in the electric power sector. As a result, inverter-based resources (IBRs), mainly wind, photovoltaics (PVs), and batteries, will dominate the electric power grids. This transition involves phasing out conventional fossil fuel-based plants and decommissioning associated synchronous machines, the grid’s primary reactive power sources. The ongoing removal of these primary reactive power sources introduces critical operational challenges that could compromise the reliability and stability of the grid. The inverters used for integrating IBRs can deliver diverse crucial ancillary services, particularly reactive power support. However, the potential of IBRs to address reactive power requirements in future decarbonized grids still needs to be fully addressed. The existing literature lacks a comprehensive approach to coordinating and harmonizing the efforts of various stakeholders and drivers to leverage the reactive power capability of IBRs. To bridge this gap, this article thoroughly reviews the reactive power implications for future grids with a considerable share of primary IBRs, comprising distributed and large-scale wind, PV and battery storage plants. This article starts with a summary of the concept, measurement methods, and importance of reactive power for voltage control and how it is managed today utilizing conventional sources. The reactive power transition from current to future grids within the context of the greater energy transition is then discussed by shedding light on its diverse aspects. Afterward, the reactive capability curve of each IBR is derived from the equivalent circuits and equations. Various grid codes and integration requirements of IBRs are then analyzed from a reactive power support viewpoint. Also, the concepts related to reactive power and voltage control comprising control extents, modes, and techniques are elaborated. Finally, recommendations are provided to set the stage for leveraging the capabilities of IBRs to address the reactive power requirements of future grids. The presented material sheds light on the pivotal role of reactive power in future grids and provides a roadmap for policymakers, utilities, and grid operators to manage a seamless transition to a decarbonized grid.},
  archive      = {J_PIEEE},
  author       = {Hedayat Saboori and Hesam Pishbahar and Shahab Dehghan and Goran Strbac and Nima Amjady and Damir Novosel and Vladimir Terzija},
  doi          = {10.1109/JPROC.2025.3555509},
  journal      = {Proceedings of the IEEE},
  month        = {1},
  number       = {1},
  pages        = {66-104},
  shortjournal = {Proc. IEEE},
  title        = {Reactive power implications of penetrating inverter-based renewable and storage resources in future grids toward energy Transition—A review},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A tutorial on distributed optimization for cooperative robotics: From setups and algorithms to toolboxes and research directions. <em>PIEEE</em>, <em>113</em>(1), 40-65. (<a href='https://doi.org/10.1109/JPROC.2025.3557698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several interesting problems in multirobot systems can be cast in the framework of distributed optimization. Examples include multirobot task allocation, vehicle routing, target protection, and surveillance. While the theoretical analysis of distributed optimization algorithms has received significant attention, its application to cooperative robotics has not been investigated in detail. In this article, we show how notable scenarios in cooperative robotics can be addressed by suitable distributed optimization setups. Specifically, after a brief introduction on the widely investigated consensus optimization (most suited for data analytics) and on the partition-based setup (matching the graph structure in the optimization), we focus on two distributed settings modeling several scenarios in cooperative robotics, i.e., the so-called constraint-coupled and aggregative optimization frameworks. For each one, we consider use-case applications, and we discuss tailored distributed algorithms with their convergence properties. Then, we revise state-of-the-art toolboxes allowing for the implementation of distributed schemes on real networks of robots without central coordinators. For each use case, we discuss its implementation in these toolboxes and provide simulations and real experiments on networks of heterogeneous robots.},
  archive      = {J_PIEEE},
  author       = {Andrea Testa and Guido Carnevale and Giuseppe Notarstefano},
  doi          = {10.1109/JPROC.2025.3557698},
  journal      = {Proceedings of the IEEE},
  month        = {1},
  number       = {1},
  pages        = {40-65},
  shortjournal = {Proc. IEEE},
  title        = {A tutorial on distributed optimization for cooperative robotics: From setups and algorithms to toolboxes and research directions},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is trust misplaced? a zero-trust survey. <em>PIEEE</em>, <em>113</em>(1), 5-39. (<a href='https://doi.org/10.1109/JPROC.2025.3555131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information technology (IT) security has been, and largely is, based on compartmentalization. To implement compartmentalization, system access privileges are granted depending on the topological location of systems, grouped into perimeters, with network mechanisms (firewalls, VLANs, ...) enforcing isolation between perimeters, thus implicitly trusting systems based on their location. However, history has shown that such trust is misplaced. This has led to the emergence of an alternative paradigm, called zero trust. After contextualizing the history of IT and the emergence of zero trust for securing networks, this article presents a taxonomy of zero trust models and architectures, summarizing the goals and core principles of zero trust. Furthermore, an in-depth description of state-of-the-art technologies and methods, for transforming perimeter-based architectures to mature zero-trust architectures, is provided. This article presents a formalization of zero trust and of optimal zero-trust architectures, to which traditional architectures migrate, as well as a method for positioning migrating architectures relative to this ideal of zero trust, with as purpose of enabling a clearer understanding of the benefits and risks induced by a migration to zero trust. Finally, this article analyses the benefits, and drawbacks, of zero trust, focusing on the security properties granted by zero trust, as well as the vulnerabilities introduced.},
  archive      = {J_PIEEE},
  author       = {Alexandre Poirrier and Laurent Cailleux and Thomas Heide Clausen},
  doi          = {10.1109/JPROC.2025.3555131},
  journal      = {Proceedings of the IEEE},
  month        = {1},
  number       = {1},
  pages        = {5-39},
  shortjournal = {Proc. IEEE},
  title        = {Is trust misplaced? a zero-trust survey},
  volume       = {113},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
