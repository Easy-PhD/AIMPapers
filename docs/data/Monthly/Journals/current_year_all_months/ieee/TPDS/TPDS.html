<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPDS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpds">TPDS - 148</h2>
<ul>
<li><details>
<summary>
(2025). ELICA: Efficient and load balanced I/O cache architecture for hyperconverged infrastructures. <em>TPDS</em>, <em>36</em>(10), 2152-2168. (<a href='https://doi.org/10.1109/TPDS.2025.3592275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperconverged Infrastructures (HCIs) combine processing and storage elements to meet the requirements of data-intensive applications in performance, scalability, and quality of service. As an emerging paradigm, HCI should couple with a variety of traditional performance improvement approaches such as I/O caching in virtualized platforms. Contemporary I/O caching schemes are optimized for traditional single-node storage architectures and suffer from two major shortcomings for multi-node architectures: a) imbalanced cache space requirement and b) imbalanced I/O traffic and load. This makes existing schemes inefficient in distributing cache resources over an array of separate physical nodes. In this paper, we propose an Efficient and Load Balanced I/O Cache Architecture (ELICA), managing the solid-state drive (SSD) cache resources across HCI nodes to enhance I/O performance. ELICA dynamically reconfigures and distributes the SSD cache resources throughout the array of HCI nodes and also balances the network traffic and I/O cache load by dynamic reallocation of cache resources. To maximize the performance, we further present an optimization problem defined by Integer Linear Programming to efficiently distribute cache resources and balance the network traffic and I/O cache relocations. Our experimental results on a real platform show that ELICA improves quality of service in terms of average and worst-case latency in HCIs by 3.1× and 23%, respectively, compared to the state-of-the-art.},
  archive      = {J_TPDS},
  author       = {Mostafa Kishani and Sina Ahmadi and Saba Ahmadian and Reza Salkhordeh and Zdenek Becvar and Onur Mutlu and André Brinkmann and Hossein Asadi},
  doi          = {10.1109/TPDS.2025.3592275},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {2152-2168},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ELICA: Efficient and load balanced I/O cache architecture for hyperconverged infrastructures},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mariana: Exploring native SkipList index design for disaggregated memory. <em>TPDS</em>, <em>36</em>(10), 2137-2151. (<a href='https://doi.org/10.1109/TPDS.2025.3596988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory disaggregation has emerged as a promising architecture for improving resource efficiency by decoupling the computing and memory resources. But building efficient range indices in such an architecture faces three critical challenges: (1) coarse-grained concurrency control schemes for coordinating concurrent read/write operations with node splitting incur high contention under the skewed and write-intensive workloads; (2) existing data layouts fail to balance consistency verification and hardware acceleration via SIMD (Single Instruction Multiple Data); and (3) naive caching schemes struggle to adapt to rapidly changing access patterns. To address these challenges, we propose Mariana, a memory-disaggregated skiplist index that integrates three key innovations. First, it uses a fine-grained (i.e., entry-level) latch mechanism combined with dynamic node resizing to minimize the contention and splitting frequency. Second, it employs a tailored data layout for leaf node, which separates keys and values to enable SIMD acceleration while maintaining consistency checks with minimal write overhead. Third, it implements an adaptive caching strategy that tracks node popularity in real-time to optimize network bandwidth utilization during the index traversal. Experimental results show that Mariana achieves $1.7\times$ higher throughput under write-intensive workloads and reduces the P90 latency by 23% under the read-intensive workloads, when comparing to the state-of-the-art indices on disaggregated memory.},
  archive      = {J_TPDS},
  author       = {Xing Wei and Ke Wang and Yinjun Han and Hao Jin and Yaofeng Tu and Huiqi Hu and Xuan Zhou and Minghao Zhao},
  doi          = {10.1109/TPDS.2025.3596988},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {2137-2151},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Mariana: Exploring native SkipList index design for disaggregated memory},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized QoS-aware model inference using federated split learning for cloud-edge medical detection. <em>TPDS</em>, <em>36</em>(10), 2119-2136. (<a href='https://doi.org/10.1109/TPDS.2025.3594694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of federated learning (FL) has been widely extended to medical domains, including medical image analysis and health monitoring. With the increasing computation power demand on edge devices, split federated learning has emerged as a promising FL architecture. In this work, a home healthcare monitoring scenario is explored. Unlike existing split federated learning studies that primarily focus on model-level optimization, this study considers a system-level optimization involving latency, packet error rate, and federated training time. Specifically, a k-means algorithm is presented to select inference nodes, participating training clients, and aggregation servers referring to network conditions and data quality. Furthermore, a reinforcement learning method is utilized to allocate the computation and bandwidth resources during inference, training, and aggregation, thereby further improving the quality of service (QoS) and training efficiency. Simulation results demonstrate that the proposed architecture can achieve the target accuracy while offering the enhanced QoS and reduced the FL training time.},
  archive      = {J_TPDS},
  author       = {Yishan Chen and Xiangwei Zeng and Huashuai Cai and Qing Xu and Zhiquan Liu},
  doi          = {10.1109/TPDS.2025.3594694},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {2119-2136},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Decentralized QoS-aware model inference using federated split learning for cloud-edge medical detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RL-based hybrid CPU scaling for soft deadline constrained tasks in container clouds. <em>TPDS</em>, <em>36</em>(10), 2104-2118. (<a href='https://doi.org/10.1109/TPDS.2025.3597195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing CPU scaling approaches have limitations that can lead to inefficient resource allocation and increased penalty costs for tasks with soft deadlines running in container clouds. First, quota allocation based approaches overlook the gap between the obtainable CPU time and allocated quota, causing inefficient CPU utilization and unexpected task behaviors. Second, core allocation based approaches ignore workload dynamics within decision intervals, potentially increasing contention for CPU time among tasks on the same core. Third, existing approaches lack strategies to allocate more resources to critical tasks that incur higher penalty costs when the node’s capacity is insufficient. This article proposes a reinforcement learning based hybrid CPU scaling approach that allocates quota and cores jointly, aiming to minimize penalty costs for timeouts. Based on the embedding generated from a fine-grained CPU demand series, we allocate CPU quotas and determine a dynamic workload-aware core sharing scheme using an attention mechanism that combines respective demands and global criticality regarding penalty costs. Additionally, we integrate the resource gap, CPU time contention, and penalty costs into the reward function to update our model online. The experimental results show the proposed approach achieves state-of-the-art performance.},
  archive      = {J_TPDS},
  author       = {Yepeng Zhang and Haitao Zhang and Huadong Ma},
  doi          = {10.1109/TPDS.2025.3597195},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {2104-2118},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RL-based hybrid CPU scaling for soft deadline constrained tasks in container clouds},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic multiresource fair allocation with time discount utility. <em>TPDS</em>, <em>36</em>(10), 2089-2103. (<a href='https://doi.org/10.1109/TPDS.2025.3594741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiresource allocation mechanisms have been studied in many scenarios. A new dynamic multiresource fair allocation model with time discount utility is proposed in this article, where users can arrive and depart at different time slots. We propose a new any price share time discount (APS-TD) mechanism for this model, which accounts for the users’ time discount utility while maintaining desirable properties. We prove that the APS-TD mechanism satisfies cumulative incentive sharing (CSI), i.e., that the cumulative utility of each user is not lower than the cumulative utility generated by evenly allocating the available resources in each time slot; cumulative strategyproofness (CSP), where users cannot increase their cumulative utility by falsely reporting their demands in any time slot; cumulative Pareto optimality (CPO), i.e., where no allocation can increase the cumulative utility of one user without reducing the cumulative utility of another user in any time slot; cumulative envy-freeness (CEF), where users who arrive later should not prefer allocations from other users who arrive first in any time slot; time discount share fairness (TDSF), where users with higher time discount values occupy larger resource shares in each time slot unless the utility levels of both users are generated by evenly allocating resources; and bottleneck fairness (BF), where the allocation should satisfy max-min fairness with respect to the bottleneck resources contained in each time slot. We run the APS-TD mechanism on Alibaba trace-driven data to demonstrate the performance enhancement achieved by our proposed mechanism over the existing mechanism extensions. The results show that the APS-TD mechanism is superior to hybrid multiresource fairness (H-MRF) and stateful dominant resource fairness (SDRF) in many ways.},
  archive      = {J_TPDS},
  author       = {Bin Deng and Weidong Li},
  doi          = {10.1109/TPDS.2025.3594741},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {2089-2103},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Dynamic multiresource fair allocation with time discount utility},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task scheduling in geo-distributed computing: A survey. <em>TPDS</em>, <em>36</em>(10), 2073-2088. (<a href='https://doi.org/10.1109/TPDS.2025.3591010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geo-distributed computing, a paradigm that assigns computational tasks to globally distributed nodes, has emerged as a promising approach in cloud computing, edge computing, cloud-edge computing, and supercomputer computing (SC). It enables low-latency services, ensures data locality, and handles large-scale applications. As global computing capacity and task demands increase rapidly, scheduling tasks for efficient execution in geo-distributed computing systems has become an increasingly critical research challenge. It arises from the inherent characteristics of geographic distribution, including heterogeneous network conditions, region-specific resource pricing, and varying computational capabilities across locations. Researchers have developed diverse task scheduling methods tailored to geo-distributed scenarios, aiming to achieve objectives such as performance enhancement, fairness assurance, and fault-tolerance improvement. This survey provides a comprehensive and systematic review of task scheduling techniques across four major distributed computing environments, with an in-depth analysis of these approaches based on their core scheduling objectives. Through our analysis, we identify key research challenges and outline promising directions for advancing task scheduling in geo-distributed computing.},
  archive      = {J_TPDS},
  author       = {Yujian Wu and Shanjiang Tang and Ce Yu and Bin Yang and Chao Sun and Jian Xiao and Hutong Wu and Jinghua Feng},
  doi          = {10.1109/TPDS.2025.3591010},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {2073-2088},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Task scheduling in geo-distributed computing: A survey},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MUCVR: Edge computing-enabled high-quality multi-user collaboration for interactive MVR. <em>TPDS</em>, <em>36</em>(10), 2058-2072. (<a href='https://doi.org/10.1109/TPDS.2025.3595801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Virtual Reality (MVR), which aims to provide high-quality VR services to mobile devices of end users, has become the latest trend in virtual reality developments. The current MVR solution is to remotely render frame data from a cloud server, while the potential of edge computing in MVR is underexploited. In this paper, we propose a new approach named MUCVR to achieve high-quality interactive MVR collaboration for multiple users by exploiting edge computing. First, we design “vertical” edge–cloud collaboration for VR task rendering, in which foreground interaction is offloaded to an edge server for rendering, while the background environment is rendered by the cloud server. Correspondingly, the VR device of a user is only responsible for decoding and displaying. Second, we propose the “horizontal” multi-user collaboration based on edge–edge cooperation, which synchronizes the data among edge servers. Finally, we implement the proposed MUCVR on an MVR device and the Unity VR application engine. The results show that MUCVR can effectively reduce the MVR service latency, improve the rendering performance, reduce the computing load on the VR device, and, ultimately, improve users’ quality of experience.},
  archive      = {J_TPDS},
  author       = {Weimin Li and Qin Li and Weihong Tian and Jie Gao and Fan Wu and Jianxun Liu and Ju Ren},
  doi          = {10.1109/TPDS.2025.3595801},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {2058-2072},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MUCVR: Edge computing-enabled high-quality multi-user collaboration for interactive MVR},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance portability assessment in gaia. <em>TPDS</em>, <em>36</em>(10), 2045-2057. (<a href='https://doi.org/10.1109/TPDS.2025.3591452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern scientific experiments produce ever-increasing amounts of data, soon requiring ExaFLOPs computing capacities for analysis. Reaching such performance requires purpose-built supercomputers with $O(10^{3})$ nodes, each hosting multicore CPUs and multiple GPUs, and applications designed to exploit this hardware optimally. Given that each supercomputer is generally a one-off project, the need for computing frameworks portable across diverse CPU and GPU architectures without performance losses is increasingly compelling. We investigate the performance portability () of a real-world application: the solver module of the AVU–GSR pipeline for the ESA Gaia mission. This code finds the astrometric parameters of ${\sim} 10^{8}$ stars in the Milky Way using the LSQR iterative algorithm. LSQR is widely used to solve linear systems of equations across a wide range of high-performance computing applications, elevating the study beyond its astrophysical relevance. The code is memory-bound, with six main compute kernels implementing sparse matrix-by-vector products. We optimize the previous CUDA implementation and port the code to further six GPU-acceleration frameworks: C++ PSTL, SYCL, OpenMP, HIP, KOKKOS, and OpenACC. We evaluate each framework’s performance portability across multiple GPUs (NVIDIA and AMD) and problem sizes in terms of application and architectural efficiency. Architectural efficiency is estimated through the roofline model of the six most computationally expensive GPU kernels. Our results show that C++ library-based (C++ PSTL and KOKKOS), pragma-based (OpenMP and OpenACC), and language-specific (CUDA, HIP, and SYCL) frameworks achieve increasingly better performance portability across the supported platforms with larger problem sizes providing better scores due to higher GPU occupancies.},
  archive      = {J_TPDS},
  author       = {Giulio Malenza and Valentina Cesare and Marco Edoardo Santimaria and Robert Birke and Alberto Vecchiato and Ugo Becciani and Marco Aldinucci},
  doi          = {10.1109/TPDS.2025.3591452},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {2045-2057},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Performance portability assessment in gaia},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallelization of network dynamics computations in heterogeneous distributed environment. <em>TPDS</em>, <em>36</em>(10), 2030-2044. (<a href='https://doi.org/10.1109/TPDS.2025.3593154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of parallelizing computations to study nonlinear dynamics in large networks of non-locally coupled oscillators using heterogeneous computing resources. The proposed approach can be applied to a variety of nonlinear dynamics models with runtime specification of parameters and network topologies. Parallelizing the solution of equations for different network elements is performed transparently and, in contrast to available tools, does not require parallel programming from end-users. The runtime scheduler takes into account the performance of computing and communication resources to reduce downtime and to achieve a quasi-optimal parallelizing speed-up. The proposed approach was implemented, and its efficiency is proven by numerous applications for simulating large dynamical networks with 103-108 elements described by Hodgkin–Huxley, FitzHugh–Nagumo, and Kuramoto models, for investigating pathological synchronization during Parkinson’s disease, analyzing multi-stability, for studying chimera and solitary states in 3D networks, etc. All the above computations may be performed using symmetrical multiprocessors, graphic processing units, and a network of workstations within the same run and it was demonstrated that near-linear speed-up can be achieved for large networks. The proposed approach is promising for extension to new hardware like edge-computing devices.},
  archive      = {J_TPDS},
  author       = {Oleksandr Sudakov and Volodymyr Maistrenko},
  doi          = {10.1109/TPDS.2025.3593154},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {2030-2044},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallelization of network dynamics computations in heterogeneous distributed environment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M$^{2}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>LLM: A multi-dimensional optimization framework for LLM inference on mobile devices. <em>TPDS</em>, <em>36</em>(10), 2014-2029. (<a href='https://doi.org/10.1109/TPDS.2025.3587445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are reshaping mobile AI. Directly deploying LLMs on mobile devices is an emerging paradigm that can widely support different mobile applications while preserving data privacy. However, intensive memory footprint, long inference latency and high energy consumption severely bottlenecks on-device inference of LLM in real-world scenarios. In response to these challenges, this work introduces m$^{2}$LLM, an innovative framework that performs joint optimization from multiple dimensions for on-device LLM inference in order to strike a balance among performance, realtimeliness and energy efficiency. Specifically, m$^{2}$LLM features the following four core components including : 1) Hardware-aware Model Customization, 2) Elastic Chunk-wise Pipeline, 3) Latency-guided Prompt Compression and 4) Layer-wise Resource Scheduling. These four components interact with each other in order to guide the inference process from the following three dimensions. At the model level, m$^{2}$LLM designs an elastic chunk-wise pipeline to expand device memory and customize the model according to the hardware configuration, maximizing performance within the memory budget. At the prompt level, facing the stochastic input, m$^{2}$LLM judiciously compresses the prompts in order to guarantee the first token can be generated in time while maintaining the semantic information. Additionally, at the system level, the layer-wise resource scheduler is employed in order to complete the token generation process with minimized energy consumption while guaranteeing the realtimeness in the highly dynamic mobile environment. m$^{2}$LLM is evaluated on off-the-shelf smartphone with represented models and datasets. Compared to baseline methods, m$^{2}$LLM delivers 2.99–13.5× TTFT acceleration and 2.28–24.3× energy savings, with only a minimal model performance loss of 2% –7% .},
  archive      = {J_TPDS},
  author       = {Kaiyuan Liu and Xiaobo Zhou and Li Li},
  doi          = {10.1109/TPDS.2025.3587445},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {2014-2029},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {M$^{2}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>LLM: A multi-dimensional optimization framework for LLM inference on mobile devices},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating half-precision seismic simulation on neural processing unit. <em>TPDS</em>, <em>36</em>(10), 1998-2013. (<a href='https://doi.org/10.1109/TPDS.2025.3584773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the superiority of handling irregular regions of interest, the curvilinear grid finite difference method (CGFDM) has become wildely used in seismic simulation for earthquake hazard evaluation and understanding of earthquake physics. This paper proposes a novel approach that optimizes a CGFDM solver on the Ascend, a cutting-edge Neural Processing Unit (NPU) using half-precision storage and mixed-precision arithmetic. The approach increases the data throughput and computing efficiency, enabling more effective seismic modeling. Furthermore, we propose an efficient matrix unit enabled 3D difference algorithm that employs matrix unit on NPU to accelerate the computation. By fully exploiting the capability of matrix unit and wide SIMD lane, our solver on Ascend achieves a speedup of 4.19 × over the performance of parallel solver on two AMD CPUs and has successfully simulated real-world Wenchuan earthquake. To the best of our knowledge, we are the first to conduct seismic simulations on NPU.},
  archive      = {J_TPDS},
  author       = {Yinuo Wang and Zeyu Song and Wubing Wan and Xinpeng Zhao and Lin Gan and Ping Gao and Wenqiang Wang and Zhenguo Zhang and Haohuan Fu and Wei Xue and Guangwen Yang},
  doi          = {10.1109/TPDS.2025.3584773},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1998-2013},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating half-precision seismic simulation on neural processing unit},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High performance OpenCL-based GEMM kernel auto-tuned by bayesian optimization. <em>TPDS</em>, <em>36</em>(9), 1985-1997. (<a href='https://doi.org/10.1109/TPDS.2025.3587673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OpenCL has become the favored framework for emerging heterogeneous devices and FPGAs, owing to its versatility and portability. However, OpenCL-based math libraries still face challenges in fully leveraging device performance. When deploying high-performance arithmetic applications on these devices, the most important hot function is General Matrix-matrix Multiplication (GEMM). This study presents a meticulously optimized OpenCL GEMM kernel. Our enhanced GEMM kernel emphasizes two key improvements: 1) a three-level double buffer pipeline that efficiently overlaps data fetching with floating-point computations; 2) a fine-grained prefetching strategy of private memory to increase device occupancy by optimizing register unit utilization. Furthermore, this work presents a Bayesian Optimization (BO) tuner for kernel auto-tuning. Experimental results demonstrate considerable optimization improvement and performance advantages achieved on diverse OpenCL devices. Additionally, the BO tuner demonstrates superior efficiency and robustness, outperforming contemporary tuning methods.},
  archive      = {J_TPDS},
  author       = {Shengle Lin and Guoqing Xiao and Haotian Wang and Wangdong Yang and Kenli Li and Keqin Li},
  doi          = {10.1109/TPDS.2025.3587673},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1985-1997},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High performance OpenCL-based GEMM kernel auto-tuned by bayesian optimization},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cannikin: No lagger of SLO in concurrent multiple LoRA LLM serving. <em>TPDS</em>, <em>36</em>(9), 1972-1984. (<a href='https://doi.org/10.1109/TPDS.2025.3590014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank adaptation (LoRA) is widely used to efficiently fine-tune large language models (LLMs), leading to multiple models fine-tuned from the same pre-trained LLM. State-of-the-art LLM serving systems colocate these LoRA models on the same GPU instances for concurrent serving, which decreases memory usage and boosts efficiency. However, the unawareness of the SLO requirements of each LoRA service and the interference between requests from different LoRA services can cause significant SLO violations. This paper presents Cannikin, a multi-LoRA inference serving system that optimizes the minimum of the SLO attainments of all LoRA services in the serving system, denoted as lagger-SLO attainment. We obtain insights from the characterization of a real-world multi-LoRA serving trace, which reveals the stable input/output lengths of the most popular LoRA services. This motivates Cannikin to propose an SLO-aware scheduling algorithm that prioritizes requests based on efficient deadline estimation. Cannikin further detects the influence of interference between different LoRA services on SLO violations and eliminates the bias between these services. The evaluation using real-world traces demonstrates that compared to the state-of-the-art multi-LoRA serving systems, Cannikin can handle up to 3.6× higher rates or 2.8× more burstiness while maintaining the SLO attainment of each LoRA service $&gt; $ 90% .},
  archive      = {J_TPDS},
  author       = {Ruidong Zhu and Ziyue Jiang and Zhi Zhang and Xin Liu and Xuanzhe Liu and Xin Jin},
  doi          = {10.1109/TPDS.2025.3590014},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1972-1984},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cannikin: No lagger of SLO in concurrent multiple LoRA LLM serving},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NDP: Network division positioning for irregular multi-hop networks. <em>TPDS</em>, <em>36</em>(9), 1955-1971. (<a href='https://doi.org/10.1109/TPDS.2025.3583502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate geographical information of nodes is crucial for network applications. However, many existing positioning algorithms face challenges in achieving efficient, accurate, and robust performance when applied to irregular networks with holes or obstacles. Therefore, we introduce a new algorithm, named Network Division Positioning (NDP), to tackle this issue. In NDP, we use a similarity function to derive the distance between neighboring nodes and explore the routing concurrently, facilitating efficient distance measurement. Next, we analyze measurement errors between landmark nodes to define a threshold that filters out incorrect distances, ensuring measuring and positioning accuracy. We initially identify collinearity issues to enhance positioning robustness by examining the positional relationship between unpositioned nodes and their nearest landmark node. Subsequently, we addressed the poor positioning results and built the subnetwork utilizing the nearest landmark node and its associated measurement distance, seeking the most accurate and robust estimated position within this subnetwork. The simulation results demonstrate that NDP outperforms state-of-the-art algorithms in terms of efficiency, accuracy, and robustness when dealing with various irregular networks. Specifically, NDP enhances positioning accuracy by at least 40.82% in terms of the average median.},
  archive      = {J_TPDS},
  author       = {Xiaoyong Yan and Fu Xiao and Jian Zhou and Xiulong Liu and Chuntao Ding and Jiannong Cao and Aiguo Song and Alex X. Liu},
  doi          = {10.1109/TPDS.2025.3583502},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1955-1971},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {NDP: Network division positioning for irregular multi-hop networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compositional coordinated resource provisioning in workflows with stochastic durations. <em>TPDS</em>, <em>36</em>(9), 1937-1954. (<a href='https://doi.org/10.1109/TPDS.2025.3585821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In performance engineering of composed services, coordinated provisioning can reduce the amount of resources required to meet end-to-end response time objectives. To this aim, various intertwined aspects of the application architecture need to be taken into account, notably including precedence constraints in the composition of elementary services, along with their durations and sensitivity to the scaling of provisioned resources. We address coordinated provisioning of resources for elementary services with stochastic durations with general distributions (i.e., including non-exponential distributions). We compose services in a workflow where precedence constraints define a Directed Acyclic Graph (DAG) and the distribution of the end-to-end (E2E) response time is subject to a Service Level Objective (SLO). We leverage a surrogate model of service performance, assuming a low workload of workflow requests (i.e., a single-request scenario) and service durations inversely proportional to provisioned resources. Given the total amount of resources, our approach derives the service provisioning that optimizes the workflow E2E response time distribution, by exploiting a compositional approach and by using stochastically ordered approximations to manage dependencies in non-well-nested precedence DAGs. Then, the approach scales provisioned resources up or down to determine the minimum amount of resources needed to satisfy the SLO, while leaving the remaining resources for horizontal scaling in order to manage multiple workflow requests at high workloads. Experiments consider low-workload and high-workload scenarios, different relations between elementary service durations and provisioned resources, and workflow topologies taken from benchmarks or randomly generated with controlled statistics, using elementary service durations from a dataset of the literature. Results show that the technique is feasible also for workflows with a thousand of services and that it outperforms other provisioning methods in fitting the SLO using the same resource amount and in minimizing the resource amount needed to fit the SLO.},
  archive      = {J_TPDS},
  author       = {Laura Carnevali and Marco Paolieri and Riccardo Reali and Leonardo Scommegna and Enrico Vicario},
  doi          = {10.1109/TPDS.2025.3585821},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1937-1954},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Compositional coordinated resource provisioning in workflows with stochastic durations},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPComp: Using GPU and SSD-GPU peer to peer DMA to accelerate LSM-tree compaction for key-value store. <em>TPDS</em>, <em>36</em>(9), 1920-1936. (<a href='https://doi.org/10.1109/TPDS.2025.3586616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LSM-tree-based Key-value systems are widely used in many internet applications, known for their superior write performance. Compaction operations, responsible for maintaining the pyramidal storage structure of the LSM-tree to ensure acceptable read performance, pose significant performance bottlenecks. The application of high-performance SSDs and lightweight user-space file systems in LSM storage alleviates IO bandwidth bottlenecks, but it amplifies the computational resource consumption of compaction when KV is small and medium, shifting the bottleneck from IO to computation. To mitigate the computational bottleneck of compaction, we propose GPComp, a GPU-accelerated compaction strategy for high-performance SSDs with lightweight user-space file systems. GPComp features efficient GPU compaction units and a CPU-GPU cooperative compaction acceleration strategy. We introduce a user-space file system specifically designed for LSM storage, TopFS-GPU. It implements an SPDK-based SSD-GPU P2P IO stack to enhance data transfer throughput in GPU-accelerated Compaction. It features an asynchronous write-back cache strategy, facilitating mixed read-write workloads in LSM-tree-based key-value systems. Additionally, our pipeline mechanism overlaps GPU computations with SSD-GPU IO, increasing system throughput. Implemented based on LevelDB, GPComp shows up to a 2.65x increase in average write throughput and a 2.32x improvement in mixed read-write throughput, with a P99 tail latency reduction of up to 169.65% compared to state-of-the-art methods.},
  archive      = {J_TPDS},
  author       = {Hao Zhou and Yuanhui Chen and Wu Zeng and Lixiao Cui and Gang Wang and Xiaoguang Liu},
  doi          = {10.1109/TPDS.2025.3586616},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1920-1936},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GPComp: Using GPU and SSD-GPU peer to peer DMA to accelerate LSM-tree compaction for key-value store},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A learned performance model with transfer learning across GPUs on tensorized instructions. <em>TPDS</em>, <em>36</em>(9), 1904-1919. (<a href='https://doi.org/10.1109/TPDS.2025.3578630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training and inference efficiency of ever-larger deep neural networks highly rely on the performance of tensor operators on specific hardware accelerators. Therefore, a performance tuning framework with tensorized instruction compilation for automatic tensor generation is necessary for efficient deployment. These novel tensorized instruction, along with the emerging machine learning models, bring tremendous engineering challenges in compilation-based methods. They suffer from a large design space exploration with rough measurement accuracy and poor transferability among specialized instructions with certain hardware constraints. This paper presents a novel performance model for automatic code optimization with tensorized instruction. Central to the performance model is the assignment feature that not only clearly specifies the behaviour of instruction with computation and data movement abstraction, but also formally defines the matching problem from algorithm to tensorized instructions. Meanwhile, a simple yet efficient design with attention-inspired modules to accurately predict the performance of optimized tensor program by capturing global and long-range dependencies within a complete scheduling space. Compared with state-of-the-arts, our performance model can predict the optimal implementation of code configurations with tensorized instruction to reduce inference latency and search time by up to 1.21× and 3.41× on modern DNN benchmarks. Furthermore, with pre-trained parameters, our performance can quickly adapt to different workloads and platforms on tensorized instruction via transfer learning.},
  archive      = {J_TPDS},
  author       = {Yang Bai and Mingjun Li and Wendong Xu and Bei Yu},
  doi          = {10.1109/TPDS.2025.3578630},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1904-1919},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A learned performance model with transfer learning across GPUs on tensorized instructions},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $\text {GPUSCAN}^{++}$: Efficient structural graph clustering on GPUs. <em>TPDS</em>, <em>36</em>(9), 1890-1903. (<a href='https://doi.org/10.1109/TPDS.2025.3582996'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural clustering is one of the most popular graph clustering methods, which has achieved great performance improvement by utilizing GPUs. Even though, the state-of-the-art GPU-based structural clustering algorithm, $\mathsf {GPUSCAN}$, still suffers from efficiency issues since lots of extra costs are introduced for parallelization. Moreover, $\mathsf {GPUSCAN}$ assumes that the graph is resident in the GPU memory. However, the GPU memory capacity is limited currently while many real-world graphs are big and cannot fit in the GPU memory, which makes $\mathsf {GPUSCAN}$ unable to handle large graphs. Motivated by this, we present a new GPU-based structural clustering algorithm, ${\mathsf {GPUSCAN^{++}}}$, in this paper. To address the efficiency issue, we propose a new progressive clustering method tailored for GPUs that not only avoid high parallelization costs but also fully exploits the computing resources of GPUs. To address the GPU memory limitation issue, we propose a partition-based algorithm for structural clustering that can process large graphs with limited GPU memory. We conduct experiments on real graphs, and the experimental results demonstrate that our algorithm can achieve up to 168 times speedup compared with the state-of-the-art GPU-based algorithm when the graph can be resident in the GPU memory. Moreover, our algorithm is scalable to handle large graphs. As an example, our algorithm can finish the structural clustering on a graph with 1.8 billion edges using less than 2 GB GPU memory.},
  archive      = {J_TPDS},
  author       = {Long Yuan and Zeyu Zhou and Zi Chen and Xuemin Lin and Xiang Zhao and Fan Zhang},
  doi          = {10.1109/TPDS.2025.3582996},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1890-1903},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {$\text {GPUSCAN}^{++}$: Efficient structural graph clustering on GPUs},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PipeMesh: Achieving memory-efficient computation-communication overlap for training large language models. <em>TPDS</em>, <em>36</em>(9), 1872-1889. (<a href='https://doi.org/10.1109/TPDS.2025.3583983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently training large language models (LLMs) on commodity cloud resources remains challenging due to limitations in network bandwidth and accelerator memory capacity. Existing training systems can be categorized based on their pipeline schedules. Depth-first scheduling, employed by systems like Megatron, prioritizes memory efficiency but restricts the overlap between communication and computation, causing accelerators to remain idle for over 20% of the training time. Conversely, breadth-first scheduling maximizes communication overlap but generates excessive intermediate activations, exceeding memory capacity and slowing computation by more than 34%. To address these limitations, we propose a novel elastic pipeline schedule that enables fine-grained control over the trade-off between communication overlap and memory consumption. Our approach determines the number of micro-batches scheduled together according to the communication time and the memory available. Furthermore, we introduce a mixed sharding strategy and a pipeline-aware selective recomputation technique to reduce memory usage. Experimental results demonstrate that our system eliminates most of the 28% all-accelerator idle time caused by communication, with recomputation accounting for less than 1.9% of the training time. Compared to existing baselines, PipeMesh improves training throughput on commodity clouds by 20.1% to 33.8%.},
  archive      = {J_TPDS},
  author       = {Fanxin Li and Shixiong Zhao and Yuhao Qing and Jianyu Jiang and Xusheng Chen and Heming Cui},
  doi          = {10.1109/TPDS.2025.3583983},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1872-1889},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PipeMesh: Achieving memory-efficient computation-communication overlap for training large language models},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved methods of task assignment and resource allocation with preemption in edge computing systems. <em>TPDS</em>, <em>36</em>(9), 1857-1871. (<a href='https://doi.org/10.1109/TPDS.2025.3583966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing has become a very popular service that enables mobile devices to run complex tasks with the help of network-based computing resources. However, edge clouds are often resource-constrained, which makes resource allocation a challenging issue. In addition, edge cloud servers must make allocation decisions with only limited information available, since the arrival of future client tasks might be impossible to predict, and the states and behavior of neighboring servers might be obscured. We focus on a distributed resource allocation method in which servers operate independently and do not communicate with each other, but interact with clients (tasks) to make allocation decisions. We follow a two-round bidding approach to assign tasks to edge cloud servers, and servers are allowed to preempt previous tasks to allocate more useful ones. We evaluate the performance of our system using realistic simulations and real-world trace data from a high-performance computing cluster. Results show that our heuristic improves system-wide performance by 20-25% over previous work when accounting for the time taken by each approach. In this way, an ideal trade-off between performance and speed is achieved.},
  archive      = {J_TPDS},
  author       = {Adrian C. Rublein and Fidan Mehmeti and Mark Mahon and Thomas F. La Porta},
  doi          = {10.1109/TPDS.2025.3583966},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1857-1871},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improved methods of task assignment and resource allocation with preemption in edge computing systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Everything distributed and asynchronous: A practical system for key management service. <em>TPDS</em>, <em>36</em>(9), 1841-1856. (<a href='https://doi.org/10.1109/TPDS.2025.3577038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key management service (KMS) is vital to modern mission-critical systems. At the core of KMS are the key generation process and the key refresh process. In this paper, we design and implement a purely asynchronous system for completely distributed KMS supporting traditional applications such as threshold cryptosystems and multiparty computation (MPC) as well as emerging blockchains and Web3 applications. In this system, we have built a number of new asynchronous distributed key generation (ADKG) protocols and their corresponding asynchronous distributed key refresh (ADKR) protocols. We have demonstrated that our ADKG and ADKR protocols in the standard model outperform existing ones of the same kind, while our protocols in the random oracle model (ROM) are more efficient than other protocols with small and medium-sized networks.},
  archive      = {J_TPDS},
  author       = {Zhaoyang Xie and Haibin Zhang and Sisi Duan and Chao Liu and Shengli Liu and Xuanji Meng and Yong Yu and Fangguo Zhang and Boxin Zhao and Liehuang Zhu and Tianqing Zhu},
  doi          = {10.1109/TPDS.2025.3577038},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1841-1856},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Everything distributed and asynchronous: A practical system for key management service},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oases: Efficient large-scale model training on commodity servers via overlapped and automated tensor model parallelism. <em>TPDS</em>, <em>36</em>(9), 1828-1840. (<a href='https://doi.org/10.1109/TPDS.2025.3583165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is experiencing a rise in large-scale models. Training large-scale models is costly, prompting researchers to train large-scale models on commodity servers that more researchers can access. The massive number of parameters necessitates the use of model parallelism training methods. Existing studies focus on training with pipeline model parallelism. However, the tensor model parallelism (TMP) is inevitable when the model size keeps increasing, where frequent data-dependent communication and computation operations significantly reduce the training efficiency. In this article, we present Oases, an automated TMP method with overlapped communication to accelerate large-scale model training on commodity servers. Oases proposes a fine-grained training operation schedule to maximize overlapping communication and computation that have data dependence. Additionally, we design the Oases planner that searches for the best model parameter partition strategy of TMP to achieve further accelerations. Unlike existing methods, Oases planner is tailored to model the cost of overlapped communication-computation operations. We evaluate Oases on various model settings and two commodity clusters, and compare Oases to four state-of-the-art implementations. Experimental results show that Oases achieves speedups of 1.01–1.48× over the fastest baseline, and speedups of up to 1.95× over Megatron.},
  archive      = {J_TPDS},
  author       = {Shengwei Li and Zhiquan Lai and Dongsheng Li and Yanqi Hao and Weijie Liu and Keshi Ge and Xiaoge Deng and Kai Lu},
  doi          = {10.1109/TPDS.2025.3583165},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1828-1840},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Oases: Efficient large-scale model training on commodity servers via overlapped and automated tensor model parallelism},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SSS-DIMM: Removing redundant data movement in trusted DIMM-based near-memory-processing kernel offloading via secure space sharing. <em>TPDS</em>, <em>36</em>(8), 1810-1827. (<a href='https://doi.org/10.1109/TPDS.2025.3576438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DIMM-based Near-Memory-Processing (NMP) kernel offloading enables a program to execute in computation-enabled DIMM buffer chips, bypassing the bandwidth-constrained CPU main memory bus for high performance. Yet, it also enables programs to access memory without restrictions and protection from CPU, resulting in potential security hazards. To protect general NMP kernel offloading even with malicious privileged software, a heterogeneous TEE is required. However, for architectural design simplification, the conventional heterogeneous TEE design isolates host CPU process from NMP kernel’s memory and vice versa, such that CPU TEE and trusted NMP driver can protect CPU processes and NMP kernels in complete separation. Such isolation results in redundant input/output data movement between the two isolated memory spaces, with half of the movement performed by host CPU. Worsened by limited CPU memory bandwidth, we identify that such redundancy severely bottlenecks the performance of many potential NMP applications. To overcome this bottleneck, we propose to abandon isolation and share the NMP kernel memory with its host CPU process. Based on this idea, we design SSS-DIMM, an efficient TEE for DIMM-based NMP kernel offloading that removes the redundant data movement via Secure Space Sharing. SSS-DIMM resolves the two security challenges faced by memory sharing: to provide consistent security guarantees on CPU processes and NMP kernels with CPU TEE and the NMP driver for both memory ownership (allocation) and views (mapping), and to ensure that cryptography metadata be securely shared and synchronized between CPU and NMP unit. Our evaluation shows that SSS-DIMM maintains both security and high performance.},
  archive      = {J_TPDS},
  author       = {Weiyi Sun and Jianfeng Zhu and Mingyu Gao and Zhaoshi Li and Shaojun Wei and Leibo Liu},
  doi          = {10.1109/TPDS.2025.3576438},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1810-1827},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SSS-DIMM: Removing redundant data movement in trusted DIMM-based near-memory-processing kernel offloading via secure space sharing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel acceleration of genome variation detection on multi-zone heterogeneous system. <em>TPDS</em>, <em>36</em>(8), 1797-1809. (<a href='https://doi.org/10.1109/TPDS.2025.3581972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genomic variation is critical for understanding the genetic basis of disease. Pindel, a widely used structural variant caller, leverages short-read sequencing data to detect variation at single-base resolution; however, its hotspot module imposes substantial computational demands, limiting efficiency in large-scale whole-genome analyses. Heterogeneous architectures offer a promising solution, yet disparities in hardware design and programming models preclude direct porting of the original algorithm. To address this, we introduce MTPindel, a novel heterogeneous parallel optimization framework tailored to the MT-3000 processor. Focusing on Pindel’s most compute-intensive modules, we design multi-core and task-level parallel algorithms that exploit the MT-3000’s accelerator domains to balance and accelerate workload distribution. On 128 MT-3000–equipped nodes of the Tianhe next-generation supercomputer, MTPindel achieves an impressive 122.549 times of speedup and 95.74% parallel efficiency, with only a 0.74% error margin relative to the original implementation. This work represents a pioneering effort in heterogeneous parallelization for variant detection, paving the way for rapid, large-scale genomic analyses in research and clinical settings.},
  archive      = {J_TPDS},
  author       = {Yaning Yang and Xiaoqi Wang and Chengqing Li and Shaoliang Peng},
  doi          = {10.1109/TPDS.2025.3581972},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1797-1809},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel acceleration of genome variation detection on multi-zone heterogeneous system},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building accurate and interpretable online classifiers on edge devices. <em>TPDS</em>, <em>36</em>(8), 1779-1796. (<a href='https://doi.org/10.1109/TPDS.2025.3579121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By integrating machine learning with edge devices, we can augment the capabilities of edge devices, such as IoT devices, household appliances, and wearable technologies. These edge devices generally operate on microcontrollers with inherently limited resources, such as constrained RAM capacity and limited computational power. Nonetheless, they often process data in a high-velocity stream fashion, exemplified by sequences of activities and statuses monitored by advanced industrial sensors. In practical scenarios, models must be interpretable to facilitate troubleshooting and behavior understanding. Implementing machine learning models on edge devices is valuable and challenging, striking a balance between model efficacy and resource constraint. To address this challenge, we introduce our novel Onfesk, which combines online learning algorithms with an innovative interpretable kernel. Specifically, our Onfesk trains an online classifier over the kernel’s feature sketches. Benefiting from our specially designed modules, the kernel’s feature sketches can be efficiently produced, and the memory requirements of the classifier can be significantly reduced. As a result, Onfesk delivers effective and efficient performance in environments with limited resources without compromising on model interpretability. Extensive experiments with diverse real-world datasets have shown that Onfesk outperforms state-of-the-art methods, achieving up to a 7.4% improvement in accuracy within identical memory constraints.},
  archive      = {J_TPDS},
  author       = {Yuanming Zhang and Pinghui Wang and Kuankuan Cheng and Junzhou Zhao and Jing Tao and Jingxin Hai and Junlan Feng and Chao Deng and Xidian Wang},
  doi          = {10.1109/TPDS.2025.3579121},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1779-1796},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Building accurate and interpretable online classifiers on edge devices},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficiency and decentralization: A blockchain assisted distributed fuzzy-rough feature selection. <em>TPDS</em>, <em>36</em>(8), 1762-1778. (<a href='https://doi.org/10.1109/TPDS.2025.3578032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy-rough sets-based feature selection (FRFS), as an effective data pre-processing technique, has drawn significant attention with the growing prevalence of large-scale datasets. However, centralized FRFS approaches suffer from the following shortcomings: 1) low computational efficiency, 2) bottlenecks in memory and computational resources, and 3) strict limitation of collaborative implementation using non-shared datasets owned by different data providers. These limitations highlight the growing necessity of integrating FRFS into a distributed FS framework. Nevertheless, most existing distributed FS schemes are reliant on a designated central server to collect and merge the local results from all slave nodes, which may result in several challenges including single point of failure risk, lack of trust and reliability, and lack of transparency and traceability. To relieve the above issues, this paper proposes a blockchain assisted distributed FS framework, successfully implementing a distributed solution for FRFS (BDFRFS). First, this framework introduces blockchain to merge, reach consensus and publish the global results generated during each iteration of FRFS, including the currently selected feature subset with its corresponding similarity matrix and dependency degree. This not only eliminates the reliance of central server and alleviates the burden on the central server, but also enhances the credibility and traceability of the results. Additionally, the implementation of FRFS is designed within this framework, utilizing three strategies to improve the efficiency of centralized FRFS: 1) eliminating the irrelevant and redundant features prior to the executing FRFS; 2) removing redundant and unnecessary computations involved in generating the similarity matrices; and 3) enabling parallel computation of dependency degrees. Finally, the experimental results conducted on eight large-scale datasets demonstrate that the proposed framework can significantly reduce the runtime cost and improve the classification accuracy compared to centralized FRFS and several distributed FS approaches.},
  archive      = {J_TPDS},
  author       = {Lin Qiu and Xingwei Wang and Bo Yi and Kaimin Zhang and Fei Gao and Min Huang and Yanpeng Qu},
  doi          = {10.1109/TPDS.2025.3578032},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1762-1778},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards efficiency and decentralization: A blockchain assisted distributed fuzzy-rough feature selection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safe multi-agent deep reinforcement learning for the management of autonomous connected vehicles at future intersections. <em>TPDS</em>, <em>36</em>(8), 1744-1761. (<a href='https://doi.org/10.1109/TPDS.2025.3580092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Connected and Autonomous Vehicles (vehicle) evolve, Autonomous Intersection Management (AIM) systems are emerging to enable safe, efficient traffic flow at urban intersections without traffic signals. However, existing AIM systems, whether based on traditional optimization control methods or machine learning, suffer from low computational efficiency and a lack of robustness in ensuring safety, respectively. To overcome these limitations, we propose an innovative AIM scheme rooted in Safe Multi-Agent Deep Reinforcement Learning (MADRL). We initially model the safe MADRL problem as a constrained Markov game (CMG) and tackle it with our multi-agent projective constrained policy optimization (MAPCPO). This method first optimizes policy updates within the Kullback-Leibler divergence trust region to maximize performance, and then projects these optimized policies onto the bounds of risk constraints, thus ensuring safety. Building on this, we introduce a Risk-Bounded RL for Autonomous Intersection Management (RbRL-AIM) algorithm. This algorithm adopts an architecture that consists of an LSTM based policy neural network, a reward value network, and a risk neural network. These components, through the MAPCPO policy, enable continuous learning from complex and random intersection traffic environments, thereby facilitating the safe, efficient, and smooth control of vehicles at intersections. Our method is validated in a CARLA simulation, showing significant gains in computational and traffic efficiency over baseline optimization control methods. Compared to non-safety-aware MADRL methods, our approach achieves zero collisions and improved ride comfort.},
  archive      = {J_TPDS},
  author       = {Rui Zhao and Kui Wang and Yun Li and Yuze Fan and Fei Gao and Zhenhai Gao},
  doi          = {10.1109/TPDS.2025.3580092},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1744-1761},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Safe multi-agent deep reinforcement learning for the management of autonomous connected vehicles at future intersections},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient speculative federated tree learning system with a lightweight NN-based predictor. <em>TPDS</em>, <em>36</em>(8), 1728-1743. (<a href='https://doi.org/10.1109/TPDS.2025.3581295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated tree-based models are popular in many real-world applications owing to their high accuracy and good interpretability. However, the classical synchronous method causes inefficient federated tree-based model training due to tree node dependencies. Inspired by speculative execution techniques in modern high-performance processors, this paper proposes FTSeir, a novel and efficient speculative federated learning system. Instead of simply waiting, FTSeir optimistically predicts the outcome of the prior tree node. By resolving tree node dependencies with a neural network-based split point predictor, the training tasks of child tree nodes can be executed speculatively in advance via separate threads. This speculation enables cross-layer concurrent training, thus significantly reducing the waiting time. Furthermore, we propose an eager verification mechanism to promptly identify mispredictions, thereby reducing wasted computing resources. On a misprediction, an incomplete rollback is triggered for quick recovery by reusing the output of the mis-speculative training, which reduces computational requirements. We implement FTSeir and evaluate its efficiency in a real-world federated learning setting with six public datasets. Evaluation results demonstrate that FTSeir achieves up to 3.45× and 3.60× speedup over the state-of-the-art gradient boosted decision trees and random forests implementations, respectively.},
  archive      = {J_TPDS},
  author       = {Yuhui Zhang and Hong Liao and Lutan Zhao and Yuncong Shao and Zhihong Tian and XiaoFeng Wang and Dan Meng and Rui Hou},
  doi          = {10.1109/TPDS.2025.3581295},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1728-1743},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An efficient speculative federated tree learning system with a lightweight NN-based predictor},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VAHRM: Variation-aware resource management in heterogeneous supercomputing systems. <em>TPDS</em>, <em>36</em>(8), 1713-1727. (<a href='https://doi.org/10.1109/TPDS.2025.3577252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel resource management technique for heterogeneous supercomputing systems affected by manufacturing variability. Our proposed technique called VAHRM (Variation-Aware Heterogeneous Resource Management) takes a holistic approach to job scheduling on highly heterogeneous computing resources. VAHRM preferentially allocates energy-efficient computing resources to an energy-consuming job in a job queue, considering the impact on both the job turnaround time and the power consumption of individual resources. Furthermore, we have developed a novel approach to modeling the power consumption of computing resources that have manufacturing variability. Our approach called TSMVA (Two-Stage Modeling with Variation Awareness) enables us to generate the first variation-aware GPU power models, which can correctly estimate the power consumption of each GPU for a given job. Our experimental results show that, compared to conventional first-come-first-serve (FCFS) and state-of-the-art variation-aware scheduling algorithms, VAHRM can achieve respective improvements in system energy efficiency of up to 5.8% and 5.4% (4.5% and 4.2% on average) while reducing the average turnaround time of 21.2% and 11.9%, respectively, for various workloads obtained from a production system.},
  archive      = {J_TPDS},
  author       = {Kohei Yoshida and Ryuichi Sakamoto and Kento Sato and Abhinav Bhatele and Hayato Yamaki and Hiroki Honda and Shinobu Miwa},
  doi          = {10.1109/TPDS.2025.3577252},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1713-1727},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {VAHRM: Variation-aware resource management in heterogeneous supercomputing systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generic, high-performance, compression-aware framework for data parallel DNN training. <em>TPDS</em>, <em>36</em>(8), 1695-1712. (<a href='https://doi.org/10.1109/TPDS.2023.3266246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient compression is a promising approach to alleviating the communication bottleneck in data parallel deep neural network (DNN) training by significantly reducing the data volume of gradients for synchronization. While gradient compression is being actively adopted by the industry (e.g., Facebook and AWS), our study reveals that there are two critical but often overlooked challenges: 1) inefficient coordination between compression and communication during gradient synchronization incurs substantial overheads, and 2) developing, optimizing, and integrating gradient compression algorithms into DNN systems imposes heavy burdens on DNN practitioners, and ad-hoc compression implementations often yield surprisingly poor system performance. In this paper, we propose a compression-aware gradient synchronization architecture, CaSync, which relies on flexible composition of basic computing and communication primitives. It is general and compatible with any gradient compression algorithms and gradient synchronization strategies and enables high-performance computation-communication pipelining. We further introduce a gradient compression toolkit, CompLL, to enable efficient development and automated integration of on-GPU compression algorithms into DNN systems with little programming burden. Lastly, we build a compression-aware DNN training framework HiPress with CaSync and CompLL. HiPress is open-sourced and runs on mainstream DNN systems such as MXNet, TensorFlow, and PyTorch. Evaluation via a 16-node cluster with 128 NVIDIA V100 GPUs and a 100 Gbps network shows that HiPress improves the training speed over current compression-enabled systems (e.g., BytePS-onebit, Ring-DGC and PyTorch-PowerSGD) by 9.8%–69.5% across six popular DNN models.},
  archive      = {J_TPDS},
  author       = {Hao Wu and Shiyi Wang and Youhui Bai and Cheng Li and Quan Zhou and Jun Yi and Feng Yan and Ruichuan Chen and Yinlong Xu},
  doi          = {10.1109/TPDS.2023.3266246},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1695-1712},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A generic, high-performance, compression-aware framework for data parallel DNN training},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-search with improved multi-dimensional dichotomy-based joint optimization for distributed parallel training of DNN. <em>TPDS</em>, <em>36</em>(8), 1680-1694. (<a href='https://doi.org/10.1109/TPDS.2025.3580098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed parallel training of large-scale deep neural networks (DNN) has attracted the attentions of both artificial intelligence and high-performance distributed computing. One of efficient approaches is the micro-batch-based pipeline parallelism (MBPP), e.g., GPipe and Terapipe. Based on the MBPP, we establish a time-cost model with the basic time function of layers, which considers computing time and communication time simultaneously as well as considers they are nonlinear with the amount of input data. Focusing on the jointly optimal solutions of network division and data partition, we propose a Cross-Search algorithm with Improved Multi-dimensional Dichotomy (CSIMD). Through theoretical derivation, we prove improved multi-dimensional dichotomy (IMD) has appreciable theoretical optimality and linear computational complexity significantly faster than the state-of-the-art methods including dynamic programming and recursive algorithm. Extensive experiments on both CNN-based and transformer-based neural networks demonstrate our proposed CSIMD can obtain optimal network division and data partition schemes under MBPP. On average, the training speeds of CSIMD in CNN- and transformer-based DNNs are respectively $(2.0, 2.5)\times$ and $(2.66, 5.48)\times$ of (MBPP-R, MBPP-E).},
  archive      = {J_TPDS},
  author       = {Guangyao Zhou and Yiqin Fu and Haocheng Lan and Yuanlun Xie and Wenhong Tian and Rajkumar Buyya and Jianhong Qian and Teng Su},
  doi          = {10.1109/TPDS.2025.3580098},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1680-1694},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cross-search with improved multi-dimensional dichotomy-based joint optimization for distributed parallel training of DNN},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting resource-constrained federated learning systems with guessed updates. <em>TPDS</em>, <em>36</em>(8), 1666-1679. (<a href='https://doi.org/10.1109/TPDS.2025.3578522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables a set of client devices to collaboratively train a model without sharing raw data. This process, though, operates under the constrained computation and communication resources of edge devices. These constraints combined with systems heterogeneity force some participating clients to perform fewer local updates than expected by the server, thus slowing down convergence. Exhaustive tuning of hyperparameters in FL, furthermore, can be resource-intensive, without which the convergence is adversely affected. In this work, we propose GeL, the guess and learn algorithm. GeL enables constrained edge devices to perform additional learning through guessed updates on top of gradient-based steps. These guesses are gradientless, i.e., participating clients leverage them for free. Our generic guessing algorithm (i) can be flexibly combined with several state-of-the-art algorithms including FedProx, FedNova, FedYogi or ScaleFL; and (ii) achieves significantly improved performance when the learning rates are not best tuned. We conduct extensive experiments and show that GeL can boost empirical convergence by up to 40% in resource-constrained networks while relieving the need for exhaustive learning rate tuning.},
  archive      = {J_TPDS},
  author       = {Mohamed Yassine Boukhari and Akash Dhasade and Anne-Marie Kermarrec and Rafael Pires and Othmane Safsafi and Rishi Sharma},
  doi          = {10.1109/TPDS.2025.3578522},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1666-1679},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Boosting resource-constrained federated learning systems with guessed updates},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Featherlight stateful WebAssembly for serverless inference workflows. <em>TPDS</em>, <em>36</em>(8), 1651-1665. (<a href='https://doi.org/10.1109/TPDS.2025.3575753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In serverless inference, complex prediction tasks are executed as workflows, relying on efficient state transfer across multiple functions. Serverless platforms typically deploy each function in a separate stateless container, depending on external processes for state management, which often results in suboptimal system utilization and increased latency. We introduce WasmFlow, a novel framework designed for serverless inference that ensures low latency and high throughput. This is achieved through process-level virtualization using WebAssembly. WasmFlow operates functions on a per-thread basis within compact WebAssembly modules, significantly reducing startup times and memory usage. The framework has two key features. (1) Efficient Memory Sharing: WasmFlow facilitates direct and rapid state transfer between functions using threads within the WebAssembly runtime. This is enabled through lightweight, lock-free, zero-copy intra-process communication, complemented by effective inter-process RPC. (2) System Optimizations: We further optimize WasmFlow with an advanced synchronization technique between functions, an affinity-aware workflow scheduler, and adaptive request batching. Implemented and integrated within the Kubernetes ecosystem, WasmFlow’s performance was evaluated using synthetic workloads and real-world Azure traces, including typical serverless workflows and ML models. Our results demonstrate that WasmFlow dramatically outperforms existing serverless frameworks. It reduces P90 end-to-end latency by 74x and 78x, increases function density by 1.7x and 223x compared to Faasm and SPRIGHT, and improves system throughput by 12.3x and 8.8x over Knative and WasmEdge, respectively.},
  archive      = {J_TPDS},
  author       = {Xingguo Pang and Liu Liu and Yanze Zhang and Zhuofu Chen and Zhijun Ding and Dazhao Cheng and Xiaobo Zhou},
  doi          = {10.1109/TPDS.2025.3575753},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1651-1665},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Featherlight stateful WebAssembly for serverless inference workflows},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ABSE: Adaptive baseline score-based election for leader-based BFT systems. <em>TPDS</em>, <em>36</em>(8), 1634-1650. (<a href='https://doi.org/10.1109/TPDS.2025.3572553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leader-based BFT systems face potential disruption and performance degradation from malicious leaders, with current solutions often lacking scalability or greatly increasing complexity. In this paper, we introduce ABSE, an Adaptive Baseline Score-based Election approach to mitigate the negative impact of malicious leaders on leader-based BFT systems. ABSE is fully localized and proposes to accumulate scores for processes based on their contribution to consensus advancement, aiming to bypass less reliable participants when electing leaders. We present a formal treatment of ABSE, addressing the primary design and implementation challenges, defining its generic components and rules for adherence to ensure global consistency. We also apply ABSE to two different BFT protocols, demonstrating its scalability and negligible impact on protocol complexity. Finally, by building a system prototype and conducting experiments on it, we demonstrate that ABSE-enhanced protocols can effectively minimize the disruptions caused by malicious leaders, whilst incurring minimal additional resource overhead and maintaining base performance.},
  archive      = {J_TPDS},
  author       = {Xuyang Liu and Zijian Zhang and Zhen Li and Hao Yin and Meng Li and Jiamou Liu and Mauro Conti and Liehuang Zhu},
  doi          = {10.1109/TPDS.2025.3572553},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1634-1650},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ABSE: Adaptive baseline score-based election for leader-based BFT systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A highly-parallel and scalable hardware accelerator for the NTest othello game engine. <em>TPDS</em>, <em>36</em>(8), 1620-1633. (<a href='https://doi.org/10.1109/TPDS.2025.3570596'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Othello is a two-player combinatorial game with 1E+28 legal positions and 1E+58 game tree complexity. We propose a HIghly PArallel, Scalable and configurable hardware accelerator for evaluating the middle and endgame Othello positions. We base HIPAS on NTest - a leading software Othello engine that uses the minimax algorithm with a quality pattern-based evaluation function, alpha-beta pruning, and heuristic mobility sorting. We describe its architecture and Field Programmable Gate Array implementation, measure its performance, and compare it with prior solutions. HIPAS achieves the highest quality evaluation, the highest performance with speed-ups up to several hundreds, and the best energy efficiency. The main novelty is the algorithm implementation as a circular pipeline and a Finite State Machine with pseudo-parallel processing. Although Othello was recently claimed to be weakly solved, the game remains unsolved in a stronger sense. A weak solution only shows how to force a draw. It does not guarantee a win if the opponent makes a mistake. HIPAS can validate the weak solution faster and more efficiently. A multi-threaded NTest software component evaluating the beginning and part of the middle game, combined with one or more instances of HIPAS for handling the remainder can provide a stronger solution.},
  archive      = {J_TPDS},
  author       = {Stefan Popa and Vlad Petric and Mihai Ivanovici},
  doi          = {10.1109/TPDS.2025.3570596},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1620-1633},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A highly-parallel and scalable hardware accelerator for the NTest othello game engine},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DisPLOY: Target-constrained distributed deployment for network measurement tasks on data plane. <em>TPDS</em>, <em>36</em>(8), 1608-1619. (<a href='https://doi.org/10.1109/TPDS.2025.3572246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In programmable networks, measurement tasks are placed on programmable switches to monitor network traffic at line rate. These tasks typically require substantial resources (e.g., significant SRAM), while programmable switches are constrained by limited resources due to their hardware design (e.g., Tofino ASIC), making distributed deployment essentially. Measurement tasks must monitor specific network locations or traffic flows, introducing significant complexity in deployment optimization. This target-constrained nature makes task optimization on switches (e.g., task merging) become device-dependent and order-dependent, which can lead to deployment failures or performance degradation if ignored. In this paper, we introduce DisPLOY, a novel target-constrained distributed deployment framework specifically designed for network measurement tasks on the data plane. DisPLOY enables operators to specify monitoring targets—network traffic or device/link—across multiple switches. Given the monitoring targets, DisPLOY effectively minimizes redundant operations and optimizes deployment to achieve both resource efficiency (e.g., minimizing stage consumption) and high-performance monitoring (e.g., high accuracy). We implement and evaluate DisPLOY through deployment on both P4 hardware switches (Intel Tofino ASIC) and BMv2. Experimental results show that DisPLOY significantly reduces stage consumption by up to 66% and improves ARE by up to 78.4% in flow size estimation while maintaining end-to-end performance.},
  archive      = {J_TPDS},
  author       = {Mimi Qian and Lin Cui and Xiaoquan Zhang and Fung Po Tso and Yuhui Deng and Zhetao Li and Weijia Jia},
  doi          = {10.1109/TPDS.2025.3572246},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1608-1619},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DisPLOY: Target-constrained distributed deployment for network measurement tasks on data plane},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISACPP: Interference-aware scheduling approach for deep learning training workloads based on co-location performance prediction. <em>TPDS</em>, <em>36</em>(8), 1591-1607. (<a href='https://doi.org/10.1109/TPDS.2025.3577796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional exclusive cloud resource allocation for deep learning training (DLT) workloads is unsuitable for advanced GPU infrastructure, leading to resource under-utilization. Fortunately, DLT workload co-location provides a promising way to improve resource utilization. However, existing workload co-location methods fail to accurately quantify interference among DLT workloads, resulting in performance degradation. To address this problem, this article proposes an interference-aware scheduling approach for DLT workloads based on co-location performance prediction, dubbed ‘ISACPP’. ISACPP first builds an edge-fusion gated graph attention network (E-GGAT) that incorporates DL model structures, underlying GPU types, and hyper-parameter settings to predict co-location performance. Since the co-location state changes as each workload is completed, ISACPP proposes a multi-stage co-location interference quantification model derived from the predicted co-location performance to identify the GPU device with the minimum overall interference. Experimental results demonstrate that ISACPP can accurately estimate the co-location performance of DLT workloads with a maximum prediction error of 8.72%, 1.9%, and 4.4% for execution time, GPU memory consumption, and GPU utilization, respectively. Meanwhile, ISACPP can significantly shorten workload makespan by up to 34.9% compared to state-of-the-art interference-aware scheduling methods.},
  archive      = {J_TPDS},
  author       = {Zijie Liu and Yi Cheng and Can Chen and Jun Hu and Rongguo Fu and Dengyin Zhang},
  doi          = {10.1109/TPDS.2025.3577796},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1591-1607},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ISACPP: Interference-aware scheduling approach for deep learning training workloads based on co-location performance prediction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OpenSN: An open source library for emulating LEO satellite networks. <em>TPDS</em>, <em>36</em>(8), 1574-1590. (<a href='https://doi.org/10.1109/TPDS.2025.3575920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-earth-orbit (LEO) satellite constellations (e.g., Starlink) are becoming a necessary component of future Internet. There have been increasing studies on LEO satellite networking. It is a crucial problem how to evaluate these studies in a systematic and reproducible manner. In this paper, we present OpenSN, i.e., an open source library for emulating large-scale satellite network (SN). Different from Mininet-based SN emulators (e.g., LeoEM), OpenSN adopts container-based virtualization, thus allows for running distributed routing software on each node, and can achieve horizontal scalability via flexible multi-machine extension. Compared to other container-based SN emulators (e.g., StarryNet), OpenSN streamlines the interaction with Docker command line interface and significantly reduces unnecessary operations of creating virtual links. These modifications improve emulation efficiency and vertical scalability on a single machine. Furthermore, OpenSN separates user-defined configuration from container network management via a Key-Value Database that records the necessary information for SN emulation. Such a separation architecture enhances the function extensibility. To sum up, OpenSN exhibits advantages in efficiency, scalability, and extensibility, thus is a valuable open source library that empowers research on LEO satellite networking. Experiment results show that OpenSN constructs mega-constellations 5X-10X faster than StarryNet, and updates link state 2X-4X faster than LeoEM. We also verify the scalability of OpenSN by successfully emulating the five-shell Starlink constellation with a total of 4408 satellites.},
  archive      = {J_TPDS},
  author       = {Wenhao Lu and Zhiyuan Wang and Hefan Zhang and Shan Zhang and Hongbin Luo},
  doi          = {10.1109/TPDS.2025.3575920},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1574-1590},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {OpenSN: An open source library for emulating LEO satellite networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RHINO: An efficient serverless container system for small-scale HPC applications. <em>TPDS</em>, <em>36</em>(8), 1560-1573. (<a href='https://doi.org/10.1109/TPDS.2025.3576584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing, characterized by its pay-as-you-go and auto-scaling features, offers a promising alternative for High Performance Computing (HPC) applications, as traditional HPC clusters often face long waiting times and resources over/under-provisioning. However, current serverless platforms struggle to support HPC applications due to restricted inter-function communication and high coupling runtime. To address these issues, we introduce RHINO, which offers end-to-end support for the development and deployment of serverless HPC. Using the Two-Step Adaptive Build strategy, the HPC code is packaged into lightweight, scalable functions. The Rhino Function Execution Model decouples HPC applications from the underlying infrastructures. The Auto-scaling Engine dynamically scales cloud resources and schedules tasks based on performance and cost requirements. We deploy RHINO on AWS Fargate and evaluate it on both benchmarks and real-world workloads. Experimental results show that, when compared to the traditional VM clusters, RHINO can achieve a performance improvement of 10% –30% for small-scale applications and more than 40% cost reduction.},
  archive      = {J_TPDS},
  author       = {He Zhu and Mingyu Li and Haihang You},
  doi          = {10.1109/TPDS.2025.3576584},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1560-1573},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RHINO: An efficient serverless container system for small-scale HPC applications},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GraFetch: Accelerating graph applications through domain specific hierarchical hybrid prefetching. <em>TPDS</em>, <em>36</em>(8), 1542-1559. (<a href='https://doi.org/10.1109/TPDS.2025.3575106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory performance bottlenecks the execution of graph applications, from traditional graph analytics (GA) to rapidly evolving graph neural networks (GNNs), due to the large size and complexity of graphs. While machine learning (ML) algorithms have shown potential in data prefetching to hide memory access latency, existing approaches face challenges with phase transitions and irregular memory access patterns in graph applications. To address these challenges, we introduce GraFetch, a specialized prefetching system for accelerating graph applications. GraFetch comprises of 1) a novel Hierarchical Hybrid Prefetching (HHP) framework that supports the cooperation of phase-specific ML predictors for high-complexity pattern prefetching and rule-based prefetchers for low-complexity pattern prefetching; and 2) Domain Specific Machine Learning (DSML) models integrated in the framework, which incorporate domain knowledge of graph applications to detect phases, recognize patterns, and predict memory accesses. We evaluate our approach using popular GA frameworks GPOP and X-Stream, and state-of-the-art GNN frameworks PyG and DGL. Our domain specific attention-based memory access predictors achieve 7.4% higher F1-score for delta (consecutive address jump) prediction and 15.35% higher accuracy@10 for page prediction compared with basic attention models. GraFetch achieves an average IPC improvement of 12.47% for GA and 4.18% for GNNs over a system with no prefetcher. This outperforms state-of-the-art rule-based prefetchers BO (7.12% for GA, 1.10% for GNNs), ISB (3.82% for GA, 1.60% for GNNs), and IMP (8.47% for GA, 2.20% for GNNs), as well as ML-based prefetchers Voyager (9.61% for GA, 3.14% for GNNs) and TransFetch (10.98% for GA, 2.48% for GNNs).},
  archive      = {J_TPDS},
  author       = {Pengmiao Zhang and Rajgopal Kannan and Viktor K. Prasanna},
  doi          = {10.1109/TPDS.2025.3575106},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1542-1559},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GraFetch: Accelerating graph applications through domain specific hierarchical hybrid prefetching},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GIF-FHE: A comprehensive implementation and evaluation of GPU-accelerated FHE with integer and floating-point computing power. <em>TPDS</em>, <em>36</em>(8), 1524-1541. (<a href='https://doi.org/10.1109/TPDS.2025.3574481'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully Homomorphic Encryption (FHE) allows computations on encrypted data without revealing the plaintext, garnering significant interest from both academic and industrial communities. However, its broader adoption has been hindered by performance limitations. Consequently, researchers have turned to GPUs for efficient FHE implementation. Nevertheless, most have predominantly favored integer units due to their ease of use, overlooking the considerable computational potential of floating-point units in GPUs. Recognizing this untapped floating-point computational power, our article introduces GIF-FHE, an extensive exploration and implementation of FHE, leveraging GPUs’ integer and floating-point instructions for FHE acceleration. We develop a comprehensive suite of low-level and middle-level FHE primitives, offering multiple implementation variants with support for three word size configurations ($64/52/32$.},
  archive      = {J_TPDS},
  author       = {Fangyu Zheng and Guang Fan and Wenxu Tang and Yixuan Song and Tian Zhou and Yuan Zhao and Jiankuo Dong and Jingqiang Lin and Shoumeng Yan and Jiwu Jing},
  doi          = {10.1109/TPDS.2025.3574481},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1524-1541},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GIF-FHE: A comprehensive implementation and evaluation of GPU-accelerated FHE with integer and floating-point computing power},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinating computational capacity for adaptive federated learning in heterogeneous edge computing systems. <em>TPDS</em>, <em>36</em>(8), 1509-1523. (<a href='https://doi.org/10.1109/TPDS.2025.3574718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of IoT technology and the rise of smart devices, edge computing, particularly federated learning (FL), has gained importance for preserving user data privacy. However, FL faces challenges like non-independent identically distributed data and device heterogeneity, leading to model disparities and reduced precision. Our research proposes a novel adaptive FL framework specifically engineered to synchronize computational capacities within heterogeneous edge computing landscapes. Building upon the proof of convergence boundaries for local aggregation model, this algorithm adapts the number of iterations for local updates by considering the resource consumption relationship between local aggregation model and the local updated model by various clients. This method exhibit adaptability within an environment where disparities in edge device computational capacities exist, effectively balancing computational prowess among diverse devices and enhancing the output performance of federated learning. Experiments on MNIST and PlantVillage datasets show that in heterogeneous environments, our algorithm outperforms existing methods, improving the loss function by at least 16.87% and the convergence speed by at least 2 times, in various environments (MobileNet, AlexNet).},
  archive      = {J_TPDS},
  author       = {Kechang Yang and Biao Hu and Mingguo Zhao},
  doi          = {10.1109/TPDS.2025.3574718},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1509-1523},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Coordinating computational capacity for adaptive federated learning in heterogeneous edge computing systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PWDFT-SW: Extending the limit of plane-wave DFT calculations to 16K atoms on the new sunway supercomputer. <em>TPDS</em>, <em>36</em>(7), 1495-1508. (<a href='https://doi.org/10.1109/TPDS.2025.3557621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {First-principles density functional theory (DFT) with plane wave (PW) basis set is the most widely used method in quantum mechanical material simulations due to its advantages in accuracy and universality. However, a perceived drawback of PW-based DFT calculations is their substantial computational cost and memory usage, which currently limits their ability to simulate large-scale complex systems containing thousands of atoms. This situation is exacerbated in the new Sunway supercomputer, where each process is limited to a mere 16 GB of memory. Herein, we present a novel parallel implementation of plane wave density functional theory on the new Sunway supercomputer (PWDFT-SW). PWDFT-SW fully extracts the benefits of Sunway supercomputer by extensively refactoring and calibrating our algorithms to align with the system characteristics of the Sunway system. Through extensive numerical experiments, we demonstrate that our methods can substantially decrease both computational costs and memory usage. Our optimizations translate to a speedup of 64.8x for a physical system containing 4,096 silicon atoms, enabling us to push the limit of PW-based DFT calculations to large-scale systems containing 16,384 carbon atoms.},
  archive      = {J_TPDS},
  author       = {Qingcai Jiang and Zhenwei Cao and Junshi Chen and Xinming Qin and Wei Hu and Hong An and Jinlong Yang},
  doi          = {10.1109/TPDS.2025.3557621},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1495-1508},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PWDFT-SW: Extending the limit of plane-wave DFT calculations to 16K atoms on the new sunway supercomputer},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ZeroTracer: In-band eBPF-based trace generator with zero instrumentation for microservice systems. <em>TPDS</em>, <em>36</em>(7), 1478-1494. (<a href='https://doi.org/10.1109/TPDS.2025.3571934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microservice enables agility in modern cloud-native applications but introduces challenges in fault troubleshooting due to its complex service coordination and cooperation. To tackle these challenges, distributed tracing has emerged for end-to-end request tracing and system understanding. However, existing tracing solutions often suffer from code instrumentation, trace loss and inaccuracy. To overcome these limitations, we introduce ZeroTracer, an in-kernel online distributed tracing system equipped with an eBPF-based (extended Berkeley Packet Filter) trace generator. ZeroTracer tailors for tracking HTTP requests due to its popularity in microservice systems. In our evaluations, ZeroTracer achieves remarkable trace accuracy (i.e., over 91% ) and maintains stable performance under different workload concurrency. Moreover, ZeroTracer outperforms other non-invasive approaches which fail to reconcile accurate request causality. Notably, ZeroTracer effectively tracks end-to-end requests in multi-threaded microservice applications, which is absent in existing invasive distributed tracing systems with third-party library instrumentation. Moreover, ZeroTracer introduces a negligible overhead, with latency increasing by only 0.5% –1.2% and a modest 3% –5.8% increase in CPU and memory consumption.},
  archive      = {J_TPDS},
  author       = {Wanqi Yang and Pengfei Chen and Kai Liu and Huxing Zhang},
  doi          = {10.1109/TPDS.2025.3571934},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1478-1494},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ZeroTracer: In-band eBPF-based trace generator with zero instrumentation for microservice systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight and fine-grained ciphertext search scheme for big data assisted by proxy servers. <em>TPDS</em>, <em>36</em>(7), 1460-1477. (<a href='https://doi.org/10.1109/TPDS.2025.3560694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Big Data scenarios, the data volume is enormous. Data computation and storage in distributed manner with more efficient algorithms is promising. However, most current ciphertext search schemes are designed for the centralized cloud computing platforms and they are inefficient and inapplicable in Big Data scenarios. A proxy server based system is a cloud computing extension. This new pattern moves some of the data storage and computation burden from end users to the edge servers and it greatly decrease the resource costs of data users. In this paper, we propose a searchable encryption scheme assisted by cloud computing and proxy servers for Big Data, which can accomplish Lightweight Fine-grained access control and Efficient multi-keyword top-k ciphertext Search synchronously (LFES). To cope with all types of data, we design an innovative fine-grained access control mechanism based on attribute-based encryption and key distribution protocol. Thus, the scheme only allows users with licensed attributes to access data efficiently. Then, a public key searchable encryption scheme is proposed based on privacy Protection Set Intersection (PSI) and the proxy server model. Our scheme greatly reduces the computation burden on end-users and improves retrieval efficiency. Meanwhile, to prevent tampering with stored ciphertexts, a practical data integrity audit mechanism is also designed. Security analysis illustrates that the LFES can resist Chosen Keyword Attack (CKA) and Keyword Guessing Attack (KGA). Finally, the simulation shows that the LFES is efficient and feasible in practice.},
  archive      = {J_TPDS},
  author       = {Na Wang and Kaifa Zheng and Wen Zhou and Jianwei Liu and Lunzhi Deng and Junsong Fu},
  doi          = {10.1109/TPDS.2025.3560694},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1460-1477},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A lightweight and fine-grained ciphertext search scheme for big data assisted by proxy servers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AlignMalloc: Warp-aware memory rearrangement aligned with UVM prefetching for large-scale GPU dynamic allocations. <em>TPDS</em>, <em>36</em>(7), 1444-1459. (<a href='https://doi.org/10.1109/TPDS.2025.3568688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As parallel computing tasks rapidly expand in both complexity and scale, the need for efficient GPU dynamic memory allocation becomes increasingly important. While progress has been made in developing dynamic allocators for substantial applications, their real-world applicability is still limited due to inefficient memory access behaviors. This paper introduces AlignMalloc, a novel memory management system that aligns with the Unified Virtual Memory (UVM) prefetching strategy, significantly enhancing both memory allocation and access performance in large-scale dynamic allocation scenarios. We analyze the fundamental inefficiencies in UVM access and first reveal the mismatch between memory access and UVM prefetching methods. To resolve this issue, AlignMalloc implements a warp-aware memory rearrangement strategy that exploits the regularity of warps to align with the UVM’s static prefetching setup. Additionally, AlignMalloc introduces an OR tree-based structure within a host-co-managed framework to further optimize dynamic allocation. Comprehensive experiments demonstrate that AlignMalloc substantially outperforms current state-of-the-art systems, achieving up to $2.7 \times$ improvement in dynamic allocation and $2.3 \times$ in memory access. Additionally, eight real-world applications with diverse memory access patterns exhibit consistent performance enhancements, with average speedups $1.5 \times$.},
  archive      = {J_TPDS},
  author       = {Jiajian Zhang and Fangyu Wu and Hai Jiang and Qiufeng Wang and Genlang Chen and Guangliang Cheng and Eng Gee Lim and Keqin Li},
  doi          = {10.1109/TPDS.2025.3568688},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1444-1459},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AlignMalloc: Warp-aware memory rearrangement aligned with UVM prefetching for large-scale GPU dynamic allocations},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast and scalable neural network quantum states method for molecular potential energy surfaces. <em>TPDS</em>, <em>36</em>(7), 1431-1443. (<a href='https://doi.org/10.1109/TPDS.2025.3568360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Neural Network Quantum States (NNQS) method is highly promising for accurately solving the Schrödinger equation, yet it encounters challenges such as computational demands and slow rates of convergence. To address the high computational requirements, we introduce optimizations including a cross-sample KV cache sharing technique to enhance sampling efficiency, Quantum Bitwise and BloomHash methods for more efficient local energy computation, and mixed-precision training strategies to boost computational efficiency. To overcome the issue of slow convergence, we propose a parallel training algorithm for NNQS under second quantization to accelerate the training of base models for molecular potential surfaces. Our approach achieves up to 27-fold acceleration specifically in local energy calculations in systems with 154 spin orbitals and demonstrates strong and weak scaling efficiencies of 98% and 97%, respectively, on the H$_{2}$O$_{2}$ potential surface training set. The parallelized implementation of transformer-based NNQS is highly portable on various high-performance computing architectures, offering new perspectives on quantum chemistry simulations.},
  archive      = {J_TPDS},
  author       = {Yangjun Wu and Wanlu Cao and Jiacheng Zhao and Honghui Shang},
  doi          = {10.1109/TPDS.2025.3568360},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1431-1443},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fast and scalable neural network quantum states method for molecular potential energy surfaces},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Publicly verifiable distributed computation for MEC setting. <em>TPDS</em>, <em>36</em>(7), 1416-1430. (<a href='https://doi.org/10.1109/TPDS.2025.3566080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of the Internet of Things (IoT), the shift from cloud computing to Mobile Edge Computing (MEC) has become necessary to address the low-latency requirements of real-time applications. Verifiable computation (VC) enables resource-limited clients to outsource their computation-intensive tasks to a powerful cloud while ensuring the correctness of the computation result. However, traditional VC schemes, originally designed for cloud computing, face challenges when applied to MEC environments, such as scalability issues, robustness, and efficiency concerns. To this end, we propose a verifiable distributed computation scheme for MEC, where computation tasks are distributed between a cloud server cluster (consisting of $n$ servers) and an edge server. The cloud handles most of the computation through parallel sub-tasks, while the edge server verifies intermediate results and performs minimal computation to recover the final outcome. Our scheme guarantees that the result can be recovered if at least $t$ servers, out of a total of $n$ servers in the cloud server cluster, perform their computations honestly. By leveraging batch verification and matrix-optimized polynomial evaluations, our scheme significantly enhances scalability, fault tolerance, and efficiency. The extensive analysis and simulations demonstrate that our proposed scheme is more feasible than existing solutions.},
  archive      = {J_TPDS},
  author       = {Qiang Wang and Zhicheng Li and Fucai Zhou and Jian Xu and Changsheng Zhang},
  doi          = {10.1109/TPDS.2025.3566080},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1416-1430},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Publicly verifiable distributed computation for MEC setting},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An acceleration framework for deep reinforcement learning using heterogeneous systems. <em>TPDS</em>, <em>36</em>(7), 1401-1415. (<a href='https://doi.org/10.1109/TPDS.2025.3566766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning (DRL) is vital in various AI applications. DRL algorithms comprise diverse compute primitives, which may not be simultaneously optimized using a homogeneous architecture. However, even with available heterogeneous architectures, optimizing DRL performance remains a challenge due to the complexity of design space in parallelizing DRL primitives and the variety of hardware employed in modern data centers. To address this, we introduce a framework for composing parallel DRL systems on heterogeneous platforms consisting of general-purpose processors (CPUs) and accelerators (GPUs, FPGAs). Our innovations include: 1. A general training protocol agnostic of the underlying hardware, enabling portable implementations across various processors and accelerators. 2. Efficient design exploration and automatic task placement enabling parallelization of tasks within each DRL primitive over one or multiple heterogeneous devices. 3. Incorporation of DRL-specific optimizations on runtime scheduling and resource allocation, facilitating parallelized training and enhancing the overall system performance. 4. High-level API for productive development using the framework. We showcase our framework through experimentation with three widely used DRL algorithms, DQN, DDPG, and SAC, on three heterogeneous platforms with diverse hardware characteristics and interconnections. The generated implementations outperform state-of-the-art libraries for CPU-GPU platforms by throughput improvements of up to 2×, and $1.7\times$ higher performance portability across platforms.},
  archive      = {J_TPDS},
  author       = {Yuan Meng and Mahesh A. Iyer and Viktor K. Prasanna},
  doi          = {10.1109/TPDS.2025.3566766},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1401-1415},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An acceleration framework for deep reinforcement learning using heterogeneous systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying performance inefficiencies of parallel program with spatial and temporal trace analysis. <em>TPDS</em>, <em>36</em>(7), 1387-1400. (<a href='https://doi.org/10.1109/TPDS.2025.3566735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance inefficiencies can lead to performance anomalies in parallel programs. Existing performance analysis tools either have a limited detection scope or require significant domain knowledge to use, which constrains their practical adoption to identify performance inefficiencies. In this paper, we propose STAD, a performance analysis tool for parallel programs that considers both spatial and temporal patterns within trace data. STAD captures the spatial communication patterns between processes using a spatial communication pattern graph. It then adopts a dynamic graph neural network-based unsupervised model to learn the evolving temporal patterns along the timeline. Additionally, STAD diagnoses the root causes of performance anomalies by exploiting the aggregated feature of anomalies along the call tree. Our evaluation results demonstrate that STAD can effectively detect performance anomalies with acceptable overhead and diagnose the root causes attributed to both the program itself and the running environment.},
  archive      = {J_TPDS},
  author       = {Zhibo Xuan and Xin Sun and Xin You and Hailong Yang and Zhongzhi Luan and Yi Liu and Depei Qian},
  doi          = {10.1109/TPDS.2025.3566735},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1387-1400},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Identifying performance inefficiencies of parallel program with spatial and temporal trace analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedCSpc: A cross-silo federated learning system with error-bounded lossy parameter compression. <em>TPDS</em>, <em>36</em>(7), 1372-1386. (<a href='https://doi.org/10.1109/TPDS.2025.3564736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Silo federated learning is widely used for scaling deep neural network (DNN) training over data silos from different locations worldwide while guaranteeing data privacy. Communication has been identified as the main bottleneck when training large-scale models due to large-volume model parameters and gradient transmission across public networks with limited bandwidth. Most previous works focus on gradient compression, while limited work tries to compress parameters that can not be ignored and extremely affect communication performance during the training. To bridge this gap, we propose FedCSpc: an efficient cross-silo federated learning system with an XAI-driven adaptive parameter compression strategy for large-scale model training. Our work substantially differs from existing gradient compression techniques due to the distinct data features of gradient and parameter. The key contributions of this paper are fourfold. (1) Our designed FedCSpc proposes to compress the parameter during the training using the state-of-the-art error-bounded lossy compressor – SZ3. (2) We develop an adaptive compression error bound adjustment algorithm to guarantee the model accuracy effectively. (3) We exploit an efficient approach to utilize the idle CPU resources of clients to compress the parameters. (4) We perform a comprehensive evaluation with a wide range of models and benchmarks on a GPU cluster with 65 GPUs. Results show that FedCSpc can achieve the same model accuracy as FedAvg while reducing the data volume of parameters and gradients in communication by up to 7.39× and 288×, respectively. With 32 clients on a 4 Gb size model, FedCSpc significantly outperforms FedAvg in wall-clock time in the emulated WAN environment (at the bandwidth of 1 Gbps or lower without loss of generality).},
  archive      = {J_TPDS},
  author       = {Zhaorui Zhang and Sheng Di and Kai Zhao and Sian Jin and Dingwen Tao and Zhuoran Ji and Benben Liu and Khalid Ayed Alharthi and Jiannong Cao and Franck Cappello},
  doi          = {10.1109/TPDS.2025.3564736},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1372-1386},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedCSpc: A cross-silo federated learning system with error-bounded lossy parameter compression},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CausalConf: Datasize-aware configuration auto-tuning for recurring big data processing jobs via adaptive causal structure learning. <em>TPDS</em>, <em>36</em>(7), 1354-1371. (<a href='https://doi.org/10.1109/TPDS.2025.3560304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure high-performance processing capabilities across diverse application scenarios, Big Data frameworks such as Spark and Flink usually provide a number of performance-related parameters to configure. Considering the computation scale and the characteristic of repeated executions of typical recurring Big Data processing jobs, how to automatically tune parameters for performance optimization has emerged as a hot research topic in both academic and industry. With the advantages in interpretability and generalization ability, causal inference-based methods recently prove their advancement over conventional search-based and machine learning-based methods. However, the complexity of Big Data frameworks, the time-varying input dataset size of a recurring job and the limitation of a single causal structure learning algorithm together prevent these methods from practical application. Therefore, in this paper, we design and implement CausalConf, a datasize-aware configuration auto-tuning approach for recurring Big Data processing jobs via adaptive causal structure learning. Specifically, the offline training phase is responsible for training multiple datasize-aware causal structure models with different causal structure learning algorithms, while the online tuning phase is responsible for recommending the next promising configuration in an iterative manner via the Multi-Armed Bandit-based optimal intervention set selection as well as the novel datasize-aware causal Bayesian optimization. To evaluate the performance of CausalConf, a series of experiments are conducted on our local Spark cluster with 9 different previously unknown target applications from HiBench. Experimental results show that the performance speed ratio achieved by CausalConf compared to the four recent and representative baselines can respectively reach 1.45×, 1.31×, 1.26× and 1.54× on average and up to 2.53×, 1.55×, 1.57×, 2.18×. Besides, the average total online tuning cost of CausalConf is reduced by 8.85%, 14.26%, 18.58%, and 14.29%, respectively.},
  archive      = {J_TPDS},
  author       = {Hui Dou and Mingjie He and Lei Zhang and Yiwen Zhang and Zibin Zheng},
  doi          = {10.1109/TPDS.2025.3560304},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1354-1371},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CausalConf: Datasize-aware configuration auto-tuning for recurring big data processing jobs via adaptive causal structure learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generic specification framework for weakly consistent replicated data types. <em>TPDS</em>, <em>36</em>(6), 1338-1353. (<a href='https://doi.org/10.1109/TPDS.2025.3533546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Burckhardt et al. proposed a formal specification framework for eventually consistent replicated data types, denoted $(vis, ar)$, based on the notions of visibility and arbitration relations. However, being specific to eventually consistent systems, this framework has two limitations. First, it does not cover non-convergent consistency models since arbitration $ar$ is a total order over events. Second, it does not cover the consistency models in which each event is required to be aware of the return values of some events that are visible to it when justifying its return value. These limitations make the $(vis, ar)$ framework not generic enough to specify and reason about important weak consistency models such as Causal Memory and PRAM. In this article, we extend this framework to a more generic one called $(vis, ar, V)$ for weakly consistent replicated data types. To specify non-convergent consistency models as well, we relax the arbitration relation $ar$ to be a partial order. To overcome the second limitation, we allow to specify for each event $e$, a subset $V(e)$ of its visible set whose return values cannot be ignored when justifying the return value of $e$. To make it practically feasible, we provide candidates for the visibility and arbitration relations and the $V$ function. By combining candidates for these three components, we are able to specify not only existing consistency models but also new ones that are reasonable and promising for practical usefulness. We then show how to specify consistency models in our framework, and provide three case studies.},
  archive      = {J_TPDS},
  author       = {Xue Jiang and Hengfeng Wei and Yu Huang and Yuxing Chen and Anqun Pan},
  doi          = {10.1109/TPDS.2025.3533546},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1338-1353},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A generic specification framework for weakly consistent replicated data types},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beehive: Decentralised high-frequency small tasks scheduling in large clusters. <em>TPDS</em>, <em>36</em>(6), 1326-1337. (<a href='https://doi.org/10.1109/TPDS.2025.3563457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data centers struggle with growing cluster sizes and rising submissions of short-lived, high-frequency tasks that cause performance bottlenecks in task scheduling. Existing centralized and distributed scheduling systems fall short in meeting performance requirements due to computational overload on the scheduler, cluster state management overhead, and scheduling conflicts. To address these challenges, this article introduces Beehive, a novel lightweight decentralized scheduling framework. In Beehive, each cluster node can schedule tasks within its local neighborhood, effectively reducing resource management overhead and scheduling conflicts. Moreover, all nodes are interconnected in a small-world network, an efficient structure that allows tasks to access resources across the entire cluster through global routing. This lightweight design enables Beehive to scale efficiently, supporting over 10,000 nodes and up to 80,000 task submissions per second without causing single-node scheduling bottlenecks. Experimental results demonstrate that Beehive significantly reduces scheduling latency. Specifically, 99% of tasks are scheduled within 100 milliseconds, and scheduling throughput can increase linearly with the number of nodes. Compared to existing centralized and distributed scheduling frameworks, Beehive substantially alleviates scheduling bottlenecks, particularly for high-frequency, short-lived tasks.},
  archive      = {J_TPDS},
  author       = {Yuxia Cheng and Linfeng Xu and Tongkai Yang and Wei Wu and Zhiqiang Lin and Antong Yu and Wenzhi Chen},
  doi          = {10.1109/TPDS.2025.3563457},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1326-1337},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Beehive: Decentralised high-frequency small tasks scheduling in large clusters},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel greedy algorithms for steiner forest. <em>TPDS</em>, <em>36</em>(6), 1311-1325. (<a href='https://doi.org/10.1109/TPDS.2025.3563849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Steiner Forest Problem is a fundamental combinatorial optimization problem in operations research and computer science. Given an undirected graph with non-negative weights for edges and a set of pairs of vertices called terminals, the Steiner Forest Problem is to find the minimum cost subgraph that connects each of the terminal pairs together. We design a family of parallel greedy algorithms based on a sequential heuristic greedy algorithm called Paired Greedy, which iteratively connects the terminal pairs that have the minimum distance. The family of parallel algorithms consists of a set of algorithms exhibiting various degrees of parallelism determined by the number of pairs that are connected in parallel in each iteration of the algorithms. We implement and run the algorithms on a multi-core system and perform an extensive experimental analysis. We analyzed the performance of the algorithms on a rich library of Steiner Forest instances with various underlying graph types. The results show that our proposed parallel algorithms achieve significant speedup with respect to the sequential Paired Greedy algorithm and provide solutions with costs that are very close to those of the solutions obtained by the sequential Paired Greedy algorithm. We provide recommendation on selecting the type of parallel algorithm and its parameters in order to achieve the most efficient results for each class of instances.},
  archive      = {J_TPDS},
  author       = {Laleh Ghalami and Daniel Grosu},
  doi          = {10.1109/TPDS.2025.3563849},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1311-1325},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel greedy algorithms for steiner forest},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Raccoon: Lightweight support for comprehensive control flows in reconfigurable spatial architectures. <em>TPDS</em>, <em>36</em>(6), 1294-1310. (<a href='https://doi.org/10.1109/TPDS.2025.3561145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coarse-grained reconfigurable arrays (CGRAs) have emerged as promising candidates for digital signal processing, biomedical, and automotive applications, where energy efficiency and flexibility are paramount. Yet existing CGRAs suffer from the Amdahl bottleneck caused by constrained control handling via either off-device communication or expensive tag-matching mechanisms. More importantly, mapping control flow onto CGRAs is extremely arduous and time-consuming due to intricate instruction structures and hardware mechanisms. To counteract these limitations, we propose Raccoon, a portable and lightweight framework for CGRAs targeting vast control flows. Raccoon comprises a comprehensive approach that spans microarchitecture, HW/SW interface, and compiler aspects. Regarding microarchitecture, Raccoon incorporates specialized infrastructure for branch- and loop-level control patterns with concise execution mechanisms. The HW/SW interface of Raccoon includes well-characterized abstractions and instruction sets tailored for easy compilation, featuring custom operators and architectural models for control-oriented units. On the compiler front, Raccoon integrates advanced control handling techniques and employs a portable mapper leveraging reinforcement learning and Monte Carlo tree search. This enables agile mapping and optimization of the entire program, ensuring efficient execution and high-quality results. Through the cohesive co-design, Raccoon can empower various CGRAs with robust control-flow handling capabilities, surpassing conventional tagged mechanisms in terms of hardware efficiency and compiler adaptability. Evaluation results show that Raccoon achieves up to a 5.78× improvement in energy efficiency and a 2.24× reduction in cycle count over state-of-the-art CGRAs. Raccoon stands out for its versatility in managing intricate control flows and showcases remarkable portability across diverse CGRA architectures.},
  archive      = {J_TPDS},
  author       = {Xiangyu Kong and Yi Huang and Longlong Chen and Jianfeng Zhu and Liangwei Li and Xingchen Man and Mingyu Gao and Shaojun Wei and Leibo Liu},
  doi          = {10.1109/TPDS.2025.3561145},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1294-1310},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Raccoon: Lightweight support for comprehensive control flows in reconfigurable spatial architectures},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetric properties and two variants of shuffle-cubes. <em>TPDS</em>, <em>36</em>(6), 1282-1293. (<a href='https://doi.org/10.1109/TPDS.2025.3558885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Li et al. in [Inf. Process. Lett. 77 (2001) 35–41] proposed the shuffle-cube $SQ_{n}$, a hypercube variant, as an attractive interconnection network topology for massive parallel and distributed systems. Diameter and symmetry are two desirable measures of network performance in terms of transmission delay and routing algorithms. Almost all $n$-regular hypercube variants of dimension $n$ have diameter not less than $n/2$. The diameter of the shuffle-cube is approximately a quarter of the diameter of the hypercube of the same dimension, making it a competitive candidate network topology. By far, symmetric properties of the shuffle-cube remain unknown. In this paper, we show that $SQ_{n}$ is not vertex-transitive for $n&gt; 2$, which is not an appealing property in interconnection networks. This shortcoming limits the practical application of the shuffle-cube. To overcome this limitation, two novel variants of the shuffle-cube, namely simplified shuffle-cube $SSQ_{n}$ and balanced shuffle-cube $BSQ_{n}$ are introduced, and their vertex-transitivity are proved simultaneously. By proposing the shuffle-cube-like graph, we obtain that both $SSQ_{n}$ and $BSQ_{n}$ are maximally connected, implying high connectivity similar to the hypercube. Additionally, super-connectivity, a refined parameter of connectivity, of $SSQ_{n}$ and $BSQ_{n}$ are also determined. Then, by vertex-transitivity of $SSQ_{n}$ and $BSQ_{n}$, routing algorithms of $SSQ_{n}$ and $BSQ_{n}$ are given for all $n&gt; 2$ respectively. We show that both $SSQ_{n}$ and $BSQ_{n}$ possess Hamiltonian cycle embedding for all $n&gt; 2$, and we also show that $SSQ_{n}$ is Hamiltonian-connected. It is noticeable that each vertex of $SSQ_{n}$ is contained in exactly one clique of size four, making it also a viable interconnection topology for data center networking since each clique of size four can be viewed as an efficient local data processing cluster of the network. Finally, as a by-product of proving vertex-transitivity of $BSQ_{n}$, we mend a flaw in the Property 3 in [IEEE Trans. Comput. 46 (1997) 484–490].},
  archive      = {J_TPDS},
  author       = {Huazhong Lü and Kai Deng and Xiaomei Yang},
  doi          = {10.1109/TPDS.2025.3558885},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1282-1293},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Symmetric properties and two variants of shuffle-cubes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $AWB^+$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>A</mml:mi><mml:mi>W</mml:mi><mml:msup><mml:mi>B</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math>-$Tree$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math>: A novel width-based index structure supporting hybrid matching for large-scale content-based Pub/Sub systems. <em>TPDS</em>, <em>36</em>(6), 1268-1281. (<a href='https://doi.org/10.1109/TPDS.2025.3561714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event matching is a key component in a large-scale content-based publish/subscribe system. The performance of most existing algorithms is easily affected by the subscription matching probability. In this article, we propose a new data structure, named $AWB^+$-$Tree$, which is based on the width of the predicates, to efficiently index the subscriptions. The most notable feature of $AWB^+$-$Tree$ is its ability to combine the advantages of different matching methods, thus achieving high and robust performance in dynamic environments. First, we implement both a forward matching method (AFM) and a backward matching method (ABM) based on $AWB^+$-$Tree$. Then, we introduce a hybrid matching method (AHM) that combines AFM and ABM. Moreover, we extend $AWB^+$-$Tree$ in three aspects: approximate matching, string type matching, and fine-grained parallelization. We conducted extensive experiments to evaluate the performance of the proposed matching algorithms on synthetic and real-world datasets. The experiment results reveal that AHM achieves a reduction in matching time by up to 53.8% compared to the state-of-the-art method. Additionally, AHM exhibits improved performance robustness, with up to a 76.9% reduction in terms of the standard deviation of matching time. Particularly in dynamic scenarios, AHM is at least 2.3 times faster and 41.3% more stable than its counterparts. Furthermore, by implementing parallelization, the matching speed of 8 threads can be accelerated by 4.16 times compared to the single-thread matching speed.},
  archive      = {J_TPDS},
  author       = {Zhengyu Liao and Shiyou Qian and Zhonglong Zheng and Jian Cao and Guangtao Xue and Minglu Li},
  doi          = {10.1109/TPDS.2025.3561714},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1268-1281},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {$AWB^+$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>A</mml:mi><mml:mi>W</mml:mi><mml:msup><mml:mi>B</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math>-$Tree$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math>: A novel width-based index structure supporting hybrid matching for large-scale content-based Pub/Sub systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OmniLearn: A framework for distributed deep learning over heterogeneous clusters. <em>TPDS</em>, <em>36</em>(6), 1253-1267. (<a href='https://doi.org/10.1109/TPDS.2025.3553066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning systems are optimized for clusters with homogeneous resources. However, heterogeneity is prevalent in computing infrastructure across edge, cloud and HPC. When training neural networks using stochastic gradient descent techniques on heterogeneous resources, performance degrades due to stragglers and stale updates. In this work, we develop an adaptive batch-scaling framework called OmniLearn to mitigate the effects of heterogeneity in distributed training. Our approach is inspired by proportional controllers to balance computation across heterogeneous servers, and works under varying resource availability. By dynamically adjusting worker mini-batches at runtime, OmniLearn reduces training time by 14-85%. We also investigate asynchronous training, where our techniques improve accuracy by up to 6.9%.},
  archive      = {J_TPDS},
  author       = {Sahil Tyagi and Prateek Sharma},
  doi          = {10.1109/TPDS.2025.3553066},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1253-1267},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {OmniLearn: A framework for distributed deep learning over heterogeneous clusters},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChunkFunc: Dynamic SLO-aware configuration of serverless functions. <em>TPDS</em>, <em>36</em>(6), 1237-1252. (<a href='https://doi.org/10.1109/TPDS.2025.3559021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing promises to be a cost effective form of on demand computing. To fully utilize its cost saving potential, workflows must be configured with the appropriate amount of resources to meet their response time Service Level Objective (SLO), while keeping costs at a minimum. Since determining and updating these configuration models manually is a nontrivial and error prone task, researchers have developed solutions for automatically finding configurations that meet the aforementioned requirements. However, our initial experiments show that even when following best practices and using state-of-the-art configuration tools, resources may still be considerably over- or underprovisioned, depending on the size of functions’ input payload. In this paper we present ChunkFunc, an SLO- and input data-aware framework for tuning serverless workflows. Our main contributions include: i) an SLO- and input size-aware function performance model for optimized configurations in serverless workflows, ii) ChunkFunc Profiler, an auto-tuned, Bayesian Optimization-guided profiling mechanism for profiling serverless functions with typical input data sizes to build a performance model, and iii) ChunkFunc Workflow Optimizer, which uses these models to determine an input size dependent configuration for each serverless function in a workflow to meet the SLO, while keeping costs to a minimum. We evaluate ChunkFunc on real-life serverless workflows and compare it to two state-of-the-art solutions, showing that it increases SLO adherence by a factor of 1.04 to 2.78, depending on the workflow, and reduces costs by up to 61% .},
  archive      = {J_TPDS},
  author       = {Thomas Pusztai and Stefan Nastic},
  doi          = {10.1109/TPDS.2025.3559021},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1237-1252},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ChunkFunc: Dynamic SLO-aware configuration of serverless functions},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IRHunter: Universal detection of instruction reordering vulnerabilities for enhanced concurrency in distributed and parallel systems. <em>TPDS</em>, <em>36</em>(6), 1220-1236. (<a href='https://doi.org/10.1109/TPDS.2025.3556861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instruction reordering is an essential optimization technique used in both compilers and multi-core processors to enhance parallelism and resource utilization. Although the original intent of this technique is to benefit the program, some improper reordering can significantly impact the program correctness, which we call instruction reordering vulnerability (IRV). However, existing methods detect IRV by defining CPU instruction reordering rules to schedule execution paths while neglecting compiler reordering, and thus generate false positives that require manual filtering and resulting in inefficiency. To bridge this gap, in this paper, we propose the IRV detection method, IRHunter, which analyzes IRV characteristics and extracts vulnerability patterns, integrating program dependency analysis for compiler reordering and memory model constraints for CPU reordering. Specifically, we use static analysis based on specific patterns to narrow the analysis scope, and adopt log-based dynamic analysis to confirm vulnerability by checking the log constraints. We built the IRV benchmark to compare IRHunter with five state-of-the-art tools (i.e., GENMC, Nidhugg, CBMC, SHB, BiRD). IRHunter detected all 19 errors, doubling the best model checking tools’ performance, with half the false positive rate of leading data race detectors. It was 10× faster on small programs and outperformed data race detectors on large programs.},
  archive      = {J_TPDS},
  author       = {GuoHua Xin and Guangquan Xu and Yao Zhang and Cheng Wen and Cen Zhang and Xiaofei Xie and Neal N. Xiong and Shaoying Liu and Pan Gao},
  doi          = {10.1109/TPDS.2025.3556861},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1220-1236},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IRHunter: Universal detection of instruction reordering vulnerabilities for enhanced concurrency in distributed and parallel systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient parallel sparse tensor contraction. <em>TPDS</em>, <em>36</em>(6), 1206-1219. (<a href='https://doi.org/10.1109/TPDS.2025.3557750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the performance of algorithms for sparse tensor-sparse tensor multiplication (SpGETT). This operation, also called sparse tensor contraction, is a higher order analogue of the sparse matrix-sparse matrix multiplication (SpGEMM) operation. Therefore, SpGETT can be performed by first converting the input tensors into matrices, then invoking high performance variants of SpGEMM, and finally reconverting the resultant matrix into a tensor. Alternatively, one can carry out the scalar operations underlying SpGETT in the realm of tensors without matrix formulation. We discuss the building blocks in both approaches and formulate a hashing-based method to avoid costly search or redirection operations. We present performance results with the current state-of-the-art SpGEMM-based approaches, existing SpGETT approaches, and a carefully implemented SpGETT approach with a new fine-tuned hashing method, proposed in this article. We evaluate the methods on real world tensors by contracting a tensor with itself along varying dimensions. Our proposed hashing-based method for SpGETT consistently outperforms the state-of-the-art method, achieving a 25% reduction in sequential execution time on average and a 21% reduction in parallel execution time on average across a variety of input instances.},
  archive      = {J_TPDS},
  author       = {Somesh Singh and Bora Uçar},
  doi          = {10.1109/TPDS.2025.3557750},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1206-1219},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient parallel sparse tensor contraction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taming offload overheads in a massively parallel open-source RISC-V MPSoC: Analysis and optimization. <em>TPDS</em>, <em>36</em>(6), 1193-1205. (<a href='https://doi.org/10.1109/TPDS.2025.3555718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous multi-core architectures combine on a single chip a few large, general-purpose host cores, optimized for single-thread performance, with (many) clusters of small, specialized, energy-efficient accelerator cores for data-parallel processing. Offloading a computation to the many-core acceleration fabric implies synchronization and communication overheads which can hamper overall performance and efficiency, particularly for small and fine-grained parallel tasks. In this work, we present a detailed, cycle-accurate quantitative analysis of the offload overheads on Occamy, an open-source massively parallel RISC-V based heterogeneous MPSoC. We study how the overheads scale with the number of accelerator cores. We explore an approach to drastically reduce these overheads by co-designing the hardware and the offload routines. Notably, we demonstrate that by incorporating multicast capabilities into the Network-on-Chip of a large (200+ cores) accelerator fabric we can improve offloaded application runtimes by as much as 2.3x, restoring more than 70% of the ideally attainable speedups. Finally, we propose a quantitative model to estimate the runtime of selected applications accounting for the offload overheads, with an error consistently below 15%.},
  archive      = {J_TPDS},
  author       = {Luca Colagrande and Luca Benini},
  doi          = {10.1109/TPDS.2025.3555718},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1193-1205},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Taming offload overheads in a massively parallel open-source RISC-V MPSoC: Analysis and optimization},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OneOS: Distributed operating system for the edge-to-cloud continuum. <em>TPDS</em>, <em>36</em>(6), 1175-1192. (<a href='https://doi.org/10.1109/TPDS.2025.3557747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application developers often need to employ a combination of software such as communication middleware and cloud-based services to deal with the challenges of heterogeneity and network dynamism in the edge-to-cloud continuum. Consequently, developers write extra glue code peripheral to the application’s core business logic, to provide interoperability between interacting software frameworks. Each software framework comes with its own framework-specific API, and as technology evolves, the developer must keep up with the changing APIs by updating the glue code in their application. Thus, framework-specific APIs hinder interoperability and cause technology fragmentation. We propose a design of a middleware-based distributed operating system (OS) called OneOS to realize a computing paradigm that alleviates such interoperability challenges. OneOS provides a single system image of the distributed computing platform, and transparently provides interoperability between software components through the standard POSIX API. Using OneOS’s domain-specific language, users can compose complex distributed applications from legacy POSIX programs. OneOS tolerates failures by adopting a distributed checkpoint-restore algorithm. We evaluate the performance of OneOS against an open-source IoT Platform, ThingsJS, using an IoT stream processing benchmark suite, and a video processing application. OneOS executes the programs about 3x faster than ThingsJS, reduces the code size by about 22%, and recovers the state of failed applications within 1 s upon detecting their failure.},
  archive      = {J_TPDS},
  author       = {Kumseok Jung and Julien Gascon-Samson and Sathish Gopalakrishnan and Karthik Pattabiraman},
  doi          = {10.1109/TPDS.2025.3557747},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1175-1192},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {OneOS: Distributed operating system for the edge-to-cloud continuum},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed and adaptive partitioning for large graphs in geo-distributed data centers. <em>TPDS</em>, <em>36</em>(6), 1161-1174. (<a href='https://doi.org/10.1109/TPDS.2025.3557610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph partitioning is of great importance to optimizing the performance and cost of geo-distributed graph analytics applications. However, it is non-trivial to obtain efficient and effective partitioning due to the challenges brought by the large graph scales, dynamic graph changes and the network heterogeneity in geo-distributed data centers (DCs). Existing studies usually adopt heuristic-based methods to achieve fast and balanced partitioning for large graphs, which are not powerful enough to address the complexity in our problem. Further, graph structures of many applications can change at various frequencies. Dynamic partitioning methods usually focus on achieving low latency to quickly adapt to changes, which unfortunately sacrifices partitioning effectiveness. Also, such methods are not aware of the dynamicity of graphs and can over sacrifice effectiveness for unnecessarily low latency. To address the limitations of existing studies, we propose DistRLCut, a novel graph partitioner which leverages Multi-Agent Reinforcement Learning (MARL) to solve the complexity of the partitioning problem. To achieve fast partitioning for large graphs, DistRLCut adapts MARL to a distributed implementation which significantly accelerates the learning process. Further, DistRLCut incorporates two techniques to trade-off between partitioning effectiveness and efficiency, including local training and agent sampling. By adaptively tuning the number of local training iterations and the agent sampling rate, DistRLCut is able to achieve good partitioning results within an overhead constraint required by graph dynamicity. Experiments using real cloud DCs and real-world graphs show that, compared to state-of-the-art static partitioning methods, DistRLCut improves the performance of geo-distributed graph analytics by 11%-95%. DistRLCut can partition over 28 million edges per second, showcasing its scalability for large graphs. With varying graph changing frequencies, DistRLCut can improve the performance by up to 71% compared to state-of-the-art dynamic partitioning.},
  archive      = {J_TPDS},
  author       = {Haobin Tan and Yao Xiao and Amelie Chi Zhou and Kezhong Lu and Xuan Yang},
  doi          = {10.1109/TPDS.2025.3557610},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1161-1174},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed and adaptive partitioning for large graphs in geo-distributed data centers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WCET estimation for CNN inference on FPGA SoC with multi-DPU engines. <em>TPDS</em>, <em>36</em>(6), 1146-1160. (<a href='https://doi.org/10.1109/TPDS.2025.3555968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Deep Learning Processor Unit (DPU) released in the official Xilinx Vitis AI toolchain stands as a commercial off-the-shelf solution tailored for accelerating convolutional neural network (CNN) inference on Xilinx FPGA devices. While most FPGA accelerator focus on high performance and energy-efficiency, analyzing the worst-case execution time (WCET) bound is essential for using CNN accelerations in real-time embedded systems design. In this work, we show that in a multi-DPU environment, the observed worst-case inference time for a CNN inference task could become 3X larger w.r.t. the best case inference time, which prompts the prominent importance of a static timing analysis for FPGA-based CNN inference. We propose, to the best of the authors’ knowledge, the first static timing analysis framework for CNN inference in a multi-DPU environment. The proposed framework introduces a generalized timing behavior model for shared bus arbitration and memory access contention between parallel running DPU engines. Additionally, it incorporates a fine-grained memory access contention analysis that takes into account the characteristics of deep learning applications. For a single-DPU environment, the analysis result is 27% tighter in average compared with the state-of-the-art results. Furthermore, our proposed method produces relatively tight estimated results in the multi-DPU environment.},
  archive      = {J_TPDS},
  author       = {Wei Zhang and Yunlong Yu and Xiao Jiang and Nan Guan and Naijun Zhan and Lei Ju},
  doi          = {10.1109/TPDS.2025.3555968},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1146-1160},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {WCET estimation for CNN inference on FPGA SoC with multi-DPU engines},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CiMBA: Accelerating genome sequencing through on-device basecalling via compute-in-memory. <em>TPDS</em>, <em>36</em>(6), 1130-1145. (<a href='https://doi.org/10.1109/TPDS.2025.3550811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As genome sequencing is finding utility in a wide variety of domains beyond the confines of traditional medical settings, its computational pipeline faces two significant challenges. First, the creation of up to 0.5 GB of data per minute imposes substantial communication and storage overheads. Second, the sequencing pipeline is bottlenecked at the basecalling step, consuming >40% of genome analysis time. A range of proposals have attempted to address these challenges, with limited success. We propose to address these challenges with a Compute-in-Memory Basecalling Accelerator (CiMBA), the first embedded ($\sim 25$ mm$^{2}$) accelerator capable of real-time, on-device basecalling, coupled with AnaLog (AL)-Dorado, a new family of analog focused basecalling DNNs. Our resulting hardware/software co-design greatly reduces data communication overhead, is capable of a throughput of 4.77 million bases per second, 24× that required for real-time operation, and achieves 17 × /27× power/area efficiency over the best prior basecalling embedded accelerator while maintaining a high accuracy comparable to state-of-the-art software basecallers.},
  archive      = {J_TPDS},
  author       = {William Andrew Simon and Irem Boybat and Riselda Kodra and Elena Ferro and Gagandeep Singh and Mohammed Alser and Shubham Jain and Hsinyu Tsai and Geoffrey W. Burr and Onur Mutlu and Abu Sebastian},
  doi          = {10.1109/TPDS.2025.3550811},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1130-1145},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CiMBA: Accelerating genome sequencing through on-device basecalling via compute-in-memory},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cube-fx: Mapping taylor expansion onto matrix multiplier-accumulators of huawei ascend AI processors. <em>TPDS</em>, <em>36</em>(6), 1115-1129. (<a href='https://doi.org/10.1109/TPDS.2025.3557444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taylor expansion, a mature method for function evaluations used in Artificial Intelligence (AI) applications, approximates functions with polynomials. In addition to the function evaluations, AI applications require massive matrix multiplications, inspiring manufacturers to propose AI processors with matrix multiplier-accumulators (MACs). However, compared with the powerful Matrix MACs, the vectorized units of the AI processors cannot efficiently carry the existing Taylor expansion implementation of Single Instruction Multiple Data (SIMD) parallelism. Leveraging the Matrix MACs for Taylor expansion becomes an ideal direction. In previous studies, migrating optimized algorithms to the Matrix MACs requires matrix generation during the runtime. The generation is expensive and even cancels the accelerations brought by the Matrix MACs on the AI processors, which Taylor expansion also suffers. This article presents Cube-fx, a mapping algorithm of Taylor expansion for multiple functions onto Matrix MACs. Cube-fx expresses the building and computation in matrix multiplications without inefficient dynamic matrix generation. On Huawei Ascend processors, Cube-fx averagely achieves 1.64× speedups compared with vectorized Horner’s Method with 56.38$\%$ vectorized operations reduced.},
  archive      = {J_TPDS},
  author       = {Yifeng Tang and Huaman Zhou and Zhuoran Ji and Cho-Li Wang},
  doi          = {10.1109/TPDS.2025.3557444},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1115-1129},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cube-fx: Mapping taylor expansion onto matrix multiplier-accumulators of huawei ascend AI processors},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFU-E: A dataflow architecture for edge DSP and AI applications. <em>TPDS</em>, <em>36</em>(6), 1100-1114. (<a href='https://doi.org/10.1109/TPDS.2025.3555329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing aims to enable swift, real-time data processing, analysis, and storage close to the data source. However, edge computing platforms are often constrained by limited processing power and efficiency. This paper presents DFU-E, a dataflow-based accelerator specifically designed to meet the demands of edge digital signal processing (DSP) and artificial intelligence (AI) applications. Our design addresses real-world requirements with three main innovations. First, to accommodate the diverse algorithms utilized at the edge, we propose a multi-layer dataflow mechanism capable of exploiting task-level, instruction block-level, instruction-level, and data-level parallelism. Second, we develop an edge dataflow architecture that includes a customized processing element (PE) array, memory, and on-chip network microarchitecture optimized for the multi-layer dataflow mechanism. Third, we design an edge dataflow software stack that enables automatic optimizations through operator fusion, dataflow graph mapping, and task scheduling. We utilize representative real-world DSP and AI applications for evaluation. Comparing with Nvidia's state-of-the-art edge computing processor, DFU-E achieves up to 1.42× geometric mean performance improvement and 1.27× energy efficiency improvement.},
  archive      = {J_TPDS},
  author       = {Wenming Li and Zhihua Fan and Tianyu Liu and Zhen Wang and Haibin Wu and Meng Wu and Kunming Zhang and Yanhuan Liu and Ninghui Sun and Xiaochun Ye and Dongrui Fan},
  doi          = {10.1109/TPDS.2025.3555329},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1100-1114},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DFU-E: A dataflow architecture for edge DSP and AI applications},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy efficient and multi-resource optimization for virtual machine placement by improving MOEA/D. <em>TPDS</em>, <em>36</em>(6), 1087-1099. (<a href='https://doi.org/10.1109/TPDS.2025.3538525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosive growth of cloud services has led to the widespread construction of large-scale data centers to meet diverse and multifaceted cloud computing demands. However, this expansion has resulted in substantial energy consumption. Virtual machine placement (VMP) has been extensively studied as a means to provide flexible and scalable cloud services while optimizing energy efficiency. Yet, the increasing complexity and diversity of applications have posted VMP suffering from waste of resources and bottlenecks due to unbalanced utilization of multi-dimensional resources. To address these issues, this article proposes a bi-objective optimization model for VMP that jointly optimizes power consumption and multi-dimensional resource utilization. Solving this large-scale bi-objective model presents a significant challenge in balancing performance and computational complexity. To tackle this, an enhanced decomposition-based multi-objective evolutionary algorithm (MOEA/D) based on $\varepsilon$-domination, termed $\varepsilon$-IMOEA/D-M2M is designed to provide solutions for the proposed optimization. Compared with both heuristics and evolutionary algorithms, performance evaluations demonstrate that our proposed VMP algorithm effectively reduces power consumption and balances multidimensional resource utilization while significantly decreasing running time compared to both heuristic and traditional evolutionary algorithms.},
  archive      = {J_TPDS},
  author       = {Wenting Wei and Huaxi Gu and Zhe Xiao and Yi Chen},
  doi          = {10.1109/TPDS.2025.3538525},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1087-1099},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy efficient and multi-resource optimization for virtual machine placement by improving MOEA/D},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IceFrog: A layer-elastic scheduling system for deep learning training in GPU clusters. <em>TPDS</em>, <em>36</em>(6), 1071-1086. (<a href='https://doi.org/10.1109/TPDS.2025.3553137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high resource demand of deep learning training (DLT) workloads necessitates the design of efficient schedulers. While most existing schedulers expedite DLT workloads by considering GPU sharing and elastic training, they neglect layer elasticity, which dynamically freezes certain layers of a network. This technique has been shown to significantly speed up individual workloads. In this paper, we explore how to incorporate layer elasticity into DLT scheduler designs to achieve higher cluster-wide efficiency. A key factor that hinders the application of layer elasticity in GPU clusters is the potential loss in model accuracy, making users reluctant to enable layer elasticity for their workloads. It is necessary to have an efficient layer-elastic system, which can well balance training accuracy and speed for layer elasticity. We introduce IceFrog, the first scheduling system that utilizes layer elasticity to improve the efficiency of DLT workloads in GPU clusters. It achieves this goal with superior algorithmic designs and intelligent resource management. In particular, (1) we model the frozen penalty and layer-aware throughput to measure the effective progress metric of layer-elastic workloads. (2) We design a novel scheduler to further improve the efficiency of layer elasticity. We implement and deploy IceFrog in a physical cluster of 48 GPUs. Extensive evaluations and large-scale simulations show that IceFrog reduces average job completion times by 36-48% relative to state-of-the-art DL schedulers.},
  archive      = {J_TPDS},
  author       = {Wei Gao and Zhuoyuan Ouyang and Peng Sun and Tianwei Zhang and Yonggang Wen},
  doi          = {10.1109/TPDS.2025.3553137},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1071-1086},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IceFrog: A layer-elastic scheduling system for deep learning training in GPU clusters},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Workload-aware performance model based soft preemptive real-time scheduling for neural processing units. <em>TPDS</em>, <em>36</em>(6), 1058-1070. (<a href='https://doi.org/10.1109/TPDS.2025.3553922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A neural processing unit (NPU) is a microprocessor which is specially designed for various types of neural network applications. Because of its high acceleration efficiency and lower power consumption, the airborne embedded system has widely deployed NPU to replace GPU as the new accelerator. Unfortunately, the inherent scheduler of NPU does not consider real-time scheduling. Therefore, it cannot meet real-time requirements of airborne embedded systems. At present, there is less research on the multi-task real-time scheduling of the NPU device. In this article, we first design an NPU resource management framework based on Kubernetes. Then, we propose WAMSPRES, a workload-aware NPU performance model based soft preemptive real-time scheduling method. The proposed workload-aware NPU performance model can accurately predict the remaining execution time of the task when it runs with other tasks concurrently. The soft preemptive real-time scheduling algorithm can provide approximate preemption capability by dynamically adjusting the NPU computing resources of tasks. Finally, we implement a prototype NPU scheduler of the airborne embedded system for the fixed-wing UAV. The proposed models and algorithms are validated on both the simulated and realistic task sets. Experimental results illustrate that WAMSPRES can achieve low prediction error and high scheduling success rate.},
  archive      = {J_TPDS},
  author       = {Yuan Yao and Yujiao Hu and Yi Dang and Wei Tao and Kai Hu and Qiming Huang and Zhe Peng and Gang Yang and Xingshe Zhou},
  doi          = {10.1109/TPDS.2025.3553922},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1058-1070},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Workload-aware performance model based soft preemptive real-time scheduling for neural processing units},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PimBeam: Efficient regular path queries over graph database using processing-in-memory. <em>TPDS</em>, <em>36</em>(5), 1042-1057. (<a href='https://doi.org/10.1109/TPDS.2025.3547365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular path queries (RPQs) in graph databases are bottlenecked by the memory wall. Emerging processing-in-memory (PIM) technologies offer a promising solution to dispatch and execute path matching tasks in parallel within PIM modules. We present an efficient PIM-based data management system tailored for RPQs and graph updates. Our solution, called PimBeam, facilitates efficient batch RPQs and graph updates by implementing a PIM-friendly dynamic graph partitioning algorithm. This algorithm effectively addresses graph skewness issues while maintaining graph locality with low overhead for handling RPQs. PimBeam streamlines label filtering queries by adding a filtering module on the PIM side and leveraging the parallelism of PIM. For the graph updates, PimBeam enhances processing efficiency by amortizing the host CPU's update overhead to PIM modules. Evaluation results of PimBeam indicate 3.59x speedup for RPQs and 29.33x speedup for graph update on average over the state-of-the-art traditional graph database.},
  archive      = {J_TPDS},
  author       = {Weihan Kong and Shengan Zheng and Yifan Hua and Ruoyan Ma and Yuheng Wen and Guifeng Wang and Cong Zhou and Linpeng Huang},
  doi          = {10.1109/TPDS.2025.3547365},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {1042-1057},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PimBeam: Efficient regular path queries over graph database using processing-in-memory},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating sparse tensor decomposition using adaptive linearized representation. <em>TPDS</em>, <em>36</em>(5), 1025-1041. (<a href='https://doi.org/10.1109/TPDS.2025.3553092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional sparse data emerge in many critical application domains such as healthcare and cybersecurity. To extract meaningful insights from massive volumes of these multi-dimensional data, scientists employ unsupervised analysis tools based on tensor decomposition (TD) methods. However, real-world sparse tensors exhibit highly irregular shapes and data distributions, which pose significant challenges for making efficient use of modern parallel processors. This study breaks the prevailing assumption that compressing sparse tensors into coarse-grained structures (i.e., tensor slices or blocks) or along a particular dimension/mode (i.e., mode-specific) is more efficient than keeping them in a fine-grained, mode-agnostic form. Our novel sparse tensor representation, Adaptive Linearized Tensor Order (${\sf ALTO}$), encodes tensors in a compact format that can be easily streamed from memory and is amenable to both caching and parallel execution. In contrast to existing compressed tensor formats, ${\sf ALTO}$ constructs one tensor copy that is agnostic to both the mode orientation and the irregular distribution of nonzero elements. To demonstrate the efficacy of ${\sf ALTO}$, we accelerate popular TD methods that compute the Canonical Polyadic Decomposition (CPD) model across different types of sparse tensors. We propose a set of parallel TD algorithms that exploit the inherent data reuse of tensor computations to substantially reduce synchronization overhead, decrease memory footprint, and improve parallel performance. Additionally, we characterize the major execution bottlenecks of TD methods on multiple generations of the latest Intel Xeon Scalable processors, including Sapphire Rapids CPUs, and introduce dynamic adaptation heuristics to automatically select the best algorithm based on the sparse tensor characteristics. Across a diverse set of real-world data sets, ${\sf ALTO}$ outperforms the state-of-the-art approaches, achieving more than an order-of-magnitude speedup over the best mode-agnostic formats. Compared to the best mode-specific formats, which require multiple tensor copies, ${\sf ALTO}$achieves $5.1\times$ geometric mean speedup at a fraction (25% ) of their storage costs. Moreover, ${\sf ALTO}$ obtains $8.4\times$ geometric mean speedup over the state-of-the-art memoization approach, which reduces computations by using extra memory, while requiring 14% of its memory consumption.},
  archive      = {J_TPDS},
  author       = {Jan Laukemann and Ahmed E. Helal and S. Isaac Geronimo Anderson and Fabio Checconi and Yongseok Soh and Jesmin Jahan Tithi and Teresa Ranadive and Brian J. Gravelle and Fabrizio Petrini and Jee Choi},
  doi          = {10.1109/TPDS.2025.3553092},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {1025-1041},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating sparse tensor decomposition using adaptive linearized representation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GEREM: Fast and precise error resilience assessment for GPU microarchitectures. <em>TPDS</em>, <em>36</em>(5), 1011-1024. (<a href='https://doi.org/10.1109/TPDS.2025.3552679'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPUs are widely used hardware acceleration platforms in many areas due to their great computational throughput. In the meanwhile, GPUs are vulnerable to transient hardware faults in the post-Moore era. Analyzing the error resilience of GPUs are critical for both hardware and software. Statistical fault injection approaches are commonly used for error resilience analysis, which are highly accurate but very time consuming. In this work, we propose GEREM, a first framework to speed up fault injection process so as to estimate the error resilience of GPU microarchitectures swiftly and precisely. We find early fault behaviors can be used to accurately predict the final outcomes of program execution. Based on this observation, we categorize the early behaviors of hardware faults into GPU Early Fault Manifestation models (EFMs). For data structures, EFMs are early propagation characteristics of faults, while for pipeline instructions, EFMs are heuristic properties of several instruction contexts. We further observe that EFMs are determined by static microarchitecture states, so we can capture them without actually simulating the program execution process under fault injections. Leveraging these observations, our GEREM framework first profiles the microarchitectural states related for EFMs at one time. It then injects faults into the profiled traces to immediately generate EFMs. For data storage structures, EFMs are directly used to predict final fault outcomes, while for pipeline instructions, machine learning is used for prediction. Evaluation results show GEREM precisely assesses the error resilience of GPU microarchitecture structures with $237\times$ speedup on average comparing with traditional fault injections.},
  archive      = {J_TPDS},
  author       = {Jingweijia Tan and Xurui Li and An Zhong and Kaige Yan and Xiaohui Wei and Guanpeng Li},
  doi          = {10.1109/TPDS.2025.3552679},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {1011-1024},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GEREM: Fast and precise error resilience assessment for GPU microarchitectures},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedLoRE: Communication-efficient and personalized edge intelligence framework via federated low-rank estimation. <em>TPDS</em>, <em>36</em>(5), 994-1010. (<a href='https://doi.org/10.1109/TPDS.2025.3548444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has recently garnered significant attention in edge intelligence. However, FL faces two major challenges: First, statistical heterogeneity can adversely impact the performance of the global model on each client. Second, the model transmission between server and clients leads to substantial communication overhead. Previous works often suffer from the trade-off issue between these seemingly competing goals, yet we show that it is possible to address both challenges simultaneously. We propose a novel communication-efficient personalized FL framework for edge intelligence that estimates the low-rank component of the training model gradient and stores the residual component at each client. The low-rank components obtained across communication rounds have high similarity, and sharing these components with the server can significantly reduce communication overhead. Specifically, we highlight the importance of previously neglected residual components in tackling statistical heterogeneity, and retaining them locally for training model updates can effectively improve the personalization performance. Moreover, we provide a theoretical analysis of the convergence guarantee of our framework. Extensive experimental results demonstrate that our framework outperforms state-of-the-art approaches, achieving up to 89.18% reduction in communication overhead and 91.00% reduction in computation overhead while maintaining comparable personalization accuracy compared to previous works.},
  archive      = {J_TPDS},
  author       = {Zerui Shao and Beibei Li and Peiran Wang and Yi Zhang and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TPDS.2025.3548444},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {994-1010},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedLoRE: Communication-efficient and personalized edge intelligence framework via federated low-rank estimation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning-driven adaptive prefetch aggressiveness control for enhanced performance in parallel system architectures. <em>TPDS</em>, <em>36</em>(5), 977-993. (<a href='https://doi.org/10.1109/TPDS.2025.3550531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern parallel system architectures, prefetchers are essential to mitigating the performance challenges posed by long memory access latencies. These architectures rely heavily on efficient memory access patterns to maximize system throughput and resource utilization. Prefetch aggressiveness is a central parameter in managing these access patterns; although increased prefetch aggressiveness can enhance performance for certain applications, it often risks causing cache pollution and bandwidth contention, leading to significant performance degradation in other workloads. While many existing prefetchers rely on static or simple built-in aggressiveness controllers, a more flexible, adaptive approach based on system-level feedback is essential to achieving optimal performance across parallel computing environments. In this paper, we introduce an Adaptive Prefetch Aggressiveness Control (APAC) framework that leverages Reinforcement Learning (RL) to dynamically manage prefetch aggressiveness in parallel system architectures. The APAC controller operates as an RL agent, which optimizes prefetch aggressiveness by dynamically responding to system feedback on prefetch accuracy, timeliness, and cache pollution. The agent receives a reward signal that reflects the impact of each adjustment on both performance and memory bandwidth, learning to adapt its control strategy based on workload characteristics. This data-driven adaptability makes APAC particularly well-suited for parallel architectures, where efficient resource management across cores is essential to scaling system performance. Our evaluation with the ChampSim simulator demonstrates that APAC effectively adapts to diverse workloads and system configurations, achieving performance gains of 6.73$\%$ in multi-core systems compared to traditional Feedback Directed Prefetching (FDP). By improving memory bandwidth utilization, reducing cache pollution, and minimizing inter-core interference, APAC significantly enhances prefetching performance in multi-core processors. These results underscore APAC’s potential as a robust solution for performance optimization in parallel system architectures, where efficient resource management is paramount for scaling modern processing environments.},
  archive      = {J_TPDS},
  author       = {Huijing Yang and Juan Fang and Yumin Hou and Xing Su and Neal N. Xiong},
  doi          = {10.1109/TPDS.2025.3550531},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {977-993},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reinforcement learning-driven adaptive prefetch aggressiveness control for enhanced performance in parallel system architectures},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards communication-efficient out-of-core graph processing on the GPU. <em>TPDS</em>, <em>36</em>(5), 961-976. (<a href='https://doi.org/10.1109/TPDS.2025.3547356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key performance bottleneck of large-scale graph processing on memory-limited GPUs is the host-GPU graph data transfer. Existing GPU-accelerated graph processing frameworks address this issue by managing the active subgraph transfer at runtime. Some frameworks adopt explicit transfer management approaches based on explicit memory copy with filter or compaction. In contrast, others adopt implicit transfer management approaches based on on-demand accesses with the zero-copy mechanism or unified virtual memory. Having made intensive analysis, we find that as the active vertices evolve, the performance of the two approaches varies in different workloads. Due to heavy redundant data transfers, high CPU compaction overhead, or low bandwidth utilization, adopting a single approach often results in suboptimal performance. Moreover, these methods lack effective cache management methods to address the irregular and sparse memory access pattern of graph processing. In this work, we propose a hybrid transfer management approach that takes the merits of both two transfer approaches at runtime. Moreover, we present an efficient vertex-centric graph caching framework that minimizes CPU-GPU communication by caching frequently accessed graph data at runtime. Based on these techniques, we present HytGraph, a GPU-accelerated graph processing framework, which is empowered by a set of effective task-scheduling optimizations to improve performance. Experiments on real-world and synthetic graphs show that HytGraph achieves average speedups of 2.5 ×, 5.0 ×, and 2.0 × compared to the state-of-the-art GPU-accelerated graph processing systems, Grus, Subway, and EMOGI, respectively.},
  archive      = {J_TPDS},
  author       = {Qiange Wang and Xin Ai and Yongze Yan and Shufeng Gong and Yanfeng Zhang and Jing Chen and Ge Yu},
  doi          = {10.1109/TPDS.2025.3547356},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {961-976},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards communication-efficient out-of-core graph processing on the GPU},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The design of a high-performance fine-grained deduplication framework for backup storage. <em>TPDS</em>, <em>36</em>(5), 945-960. (<a href='https://doi.org/10.1109/TPDS.2025.3551306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained deduplication (also known as delta compression) can achieve a better deduplication ratio compared to chunk-level deduplication. This technique removes not only identical chunks but also reduces redundancies between similar but non-identical chunks. Nevertheless, it introduces considerable I/O overhead in deduplication and restore processes, hindering the performance of these two processes and rendering fine-grained deduplication less popular than chunk-level deduplication to date. In this paper, we explore various issues that lead to additional I/O overhead and tackle them using several techniques. Moreover, we introduce MeGA, which attains fine-grained deduplication/restore speed nearly equivalent to chunk-level deduplication while maintaining the significant deduplication ratio benefit of fine-grained deduplication. Specifically, MeGA employs (1) a backup-workflow-oriented delta selector and cache-centric resemblance detection to mitigate poor spatial/temporal locality in the deduplication process, and (2) a delta-friendly data layout and “Always-Forward-Reference” traversal to address poor spatial/temporal locality in the restore workflow. Evaluations on four datasets show that MeGA achieves a better performance than other fine-grained deduplication approaches. Specifically, MeGA significantly outperforms the traditional greedy approach, providing 10–46 times better backup speed and 30–105 times more efficient restore speed, all while preserving a high deduplication ratio.},
  archive      = {J_TPDS},
  author       = {Xiangyu Zou and Wen Xia and Philip Shilane and Haijun Zhang and Xuan Wang},
  doi          = {10.1109/TPDS.2025.3551306},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {945-960},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The design of a high-performance fine-grained deduplication framework for backup storage},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel multi objective shortest path update algorithm in large dynamic networks. <em>TPDS</em>, <em>36</em>(5), 932-944. (<a href='https://doi.org/10.1109/TPDS.2025.3536357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi objective shortest path (MOSP) problem, crucial in various practical domains, seeks paths that optimize multiple objectives. Due to its high computational complexity, numerous parallel heuristics have been developed for static networks. However, real-world networks are often dynamic where the network topology changes with time. Efficiently updating the shortest path in such networks is challenging, and existing algorithms for static graphs are inadequate for these dynamic conditions, necessitating novel approaches. Here, we first develop a parallel algorithm to efficiently update a single objective shortest path (SOSP) in fully dynamic networks, capable of accommodating both edge insertions and deletions. Building on this, we propose DynaMOSP, a parallel heuristic for Dynamic Multi Objective Shortest Path searches in large, fully dynamic networks. We provide a theoretical analysis of the conditions to achieve Pareto optimality. Furthermore, we devise a dedicated shared memory CPU implementation along with a version for heterogeneous computing environments. Empirical analysis on eight real-world graphs demonstrates that our method scales effectively. The shared memory CPU implementation achieves an average speedup of 12.74× and a maximum of 57.22×, while on an Nvidia GPU, it attains an average speedup of 69.19×, reaching up to 105.39× when compared to state-of-the-art techniques.},
  archive      = {J_TPDS},
  author       = {S. M. Shovan and Arindam Khanda and Sajal K. Das},
  doi          = {10.1109/TPDS.2025.3536357},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {932-944},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel multi objective shortest path update algorithm in large dynamic networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graphite: Hardware-aware GNN reshaping for acceleration with GPU tensor cores. <em>TPDS</em>, <em>36</em>(5), 918-931. (<a href='https://doi.org/10.1109/TPDS.2025.3549180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have emerged as powerful tools for addressing non-euclidean problems. GNNs operate through two key execution phases: i) aggregation and ii) combination. In the aggregation phase, the feature data of neighboring graph nodes are gathered, which is expressed as sparse-dense matrix multiplication (SpMM) between an adjacency matrix and a feature embedding table. The combination phase takes the aggregated feature embedding as input to a neural network model with learnable weights. Typically, the adjacency matrix is extremely sparse due to inherent graph structures, making the aggregation phase a significant bottleneck in GNN computations. This paper introduces Graphite, a GNN acceleration framework to overcome the challenge of SpMM operations and enable graphics processing units (GPUs) to exploit massive thread-level parallelism more efficiently via existing dense acceleration units (i.e., tensor cores). To that end, Graphite employs three techniques for GNN acceleration. First, hardware-aware sparse graph reshaping (HAS) rearranges graph structures to replace sparse operations with dense computations, enabling hardware acceleration through GPU tensor cores. Additionally, balanced thread block scheduling (BTS) distributes sparse thread blocks evenly across streaming multiprocessors in GPUs, and zero-aware warp skipping (ZAWS) eliminates ineffective threads that operate on meaningless zeros. Experimental results show that Graphite achieves an average compression rate of 84.1% for adjacency matrices using HAS. Combined with BTS and ZAWS, Graphite delivers an average 1.55x speedup over the conventional SpMM-based GNN computation method.},
  archive      = {J_TPDS},
  author       = {Hyeonjin Kim and Taesoo Lim and William J. Song},
  doi          = {10.1109/TPDS.2025.3549180},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {918-931},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Graphite: Hardware-aware GNN reshaping for acceleration with GPU tensor cores},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMore: Enhancing GPU utilization in deep learning clusters by serverless-based co-location scheduling. <em>TPDS</em>, <em>36</em>(5), 903-917. (<a href='https://doi.org/10.1109/TPDS.2025.3548320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) clusters allow machine learning practitioners to submit their computation-intensive tasks, with GPUs accelerating their execution process. However, GPUs in current deep learning clusters are often under-utilized, which hampers the job performance and overall cluster throughput. It is urgent to improve GPU utilization, but existing works lack research on fine-grained allocation for GPU resources, as it typically allocates GPUs as indivisible units. Serverless computing reveals an opportunity to optimize utilization with fine-grained resource allocation methods, but it requires addressing three main challenges: co-location performance degradation, service level objectives guarantee of serverless functions, and cold start overhead. We propose SMore, a framework based on serverless computing to optimize GPU resource utilization of DL clusters. SMore dynamically predicts the possible co-location performance degradation and leverages a degradation-aware scheduling algorithm to ensure that the co-location decisions do not impact workload performance. It also dynamically preloads or offloads DL models by predicting the request numbers of the subsequent period to address the cold start issue. Through actual trace testing on the prototype of SMore, we find that the average GPU utilization can be increased by 34% with degradation being controlled effectively.},
  archive      = {J_TPDS},
  author       = {Junhan Liu and Zinuo Cai and Yumou Liu and Hao Li and Zongpu Zhang and Ruhui Ma and Rajkumar Buyya},
  doi          = {10.1109/TPDS.2025.3548320},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {903-917},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SMore: Enhancing GPU utilization in deep learning clusters by serverless-based co-location scheduling},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward load-balanced redundancy transitioning for erasure-coded storage. <em>TPDS</em>, <em>36</em>(5), 889-902. (<a href='https://doi.org/10.1109/TPDS.2025.3547872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redundancy transitioning enables erasure-coded storage to adapt to varying performance and reliability requirements by re-encoding data with new coding parameters on-the-fly. Existing studies focus on bandwidth-driven redundancy transitioning that reduces the transitioning bandwidth across storage nodes, yet the actual redundancy transitioning performance remains bottlenecked by the most loaded node. We present BART, a load-balanced redundancy transitioning scheme that aims to reduce the redundancy transitioning time via carefully scheduled parallelization. We show that finding an optimal load-balanced solution is difficult due to the large solution space. Given this challenge, BART decomposes the redundancy transitioning problem into multiple sub-problems and solves the sub-problems via efficient heuristics. We evaluate BART using both simulations for large-scale storage and HDFS prototype experiments on Alibaba Cloud. We show that BART significantly reduces the redundancy transitioning time compared with the bandwidth-driven approach.},
  archive      = {J_TPDS},
  author       = {Keyun Cheng and Huancheng Puyang and Xiaolu Li and Patrick P. C. Lee and Yuchong Hu and Jie Li and Ting-Yi Wu},
  doi          = {10.1109/TPDS.2025.3547872},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {889-902},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Toward load-balanced redundancy transitioning for erasure-coded storage},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Libfork: Portable continuation-stealing with stackless coroutines. <em>TPDS</em>, <em>36</em>(5), 877-888. (<a href='https://doi.org/10.1109/TPDS.2025.3543442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully-strict fork-join parallelism is a powerful model for shared-memory programming due to its optimal time-scaling and strong bounds on memory scaling. The latter is rarely achieved due to the difficulty of implementing continuation-stealing in traditional High Performance Computing (HPC) languages – where it is often impossible without modifying the compiler or resorting to non-portable techniques. We demonstrate how stackless-coroutines (a new feature in C++$\bm {20}$) can enable fully-portable continuation stealing and present libfork a wait-free fine-grained parallelism library, combining coroutines with user-space, geometric segmented-stacks. We show our approach is able to achieve optimal time/memory scaling, both theoretically and empirically, across a variety of benchmarks. Compared to openMP (libomp), libfork is on average $7.2\times$ faster and consumes $10\times$ less memory. Similarly, compared to Intel's TBB, libfork is on average $2.7\times$ faster and consumes $6.2\times$ less memory. Additionally, we introduce non-uniform memory access (NUMA) optimizations for schedulers that demonstrate performance matching busy-waiting schedulers.},
  archive      = {J_TPDS},
  author       = {Conor J. Williams and James Elliott},
  doi          = {10.1109/TPDS.2025.3543442},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {877-888},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Libfork: Portable continuation-stealing with stackless coroutines},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Courier: A unified communication agent to support concurrent flow scheduling in cluster computing. <em>TPDS</em>, <em>36</em>(5), 861-876. (<a href='https://doi.org/10.1109/TPDS.2025.3543882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the pillars in cluster computing frameworks, coflow scheduling algorithms can effectively shorten the network transmission time of cluster computing jobs, thus reducing the job completion times and improving the execution performance. However, most of existing coflow scheduling algorithms failed to consider the influences of concurrent flows, which can degrade their performance under a massive number of concurrent flows. To fill the gap, we propose a unified communication agent named Courier to minimize the number of concurrent flows in cluster computing applications, which is compatible with the mainstream coflow scheduling approaches. To maintain the scheduling order given by the scheduling algorithms, Courier merges multiple flows between each pair of hosts into a unified flow, and determines its order based on that of origin flows. In addition, in order to adapt to various types of topologies, Courier introduces a control mechanism to adjust the number of flows while maintaining the scheduling order. Extensive large-scale trace-driven simulations have shown that Courier is compatible with existing scheduling algorithms, and outperforms the state-of-the-art approaches by about 30% under a variety of workloads and topologies.},
  archive      = {J_TPDS},
  author       = {Zhaochen Zhang and Xu Zhang and Zhaoxiang Bao and Liang Wei and Chaohong Tan and Wanchun Dou and Guihai Chen and Chen Tian},
  doi          = {10.1109/TPDS.2025.3543882},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {861-876},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Courier: A unified communication agent to support concurrent flow scheduling in cluster computing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IoT-dedup: Device relationship-based IoT data deduplication scheme. <em>TPDS</em>, <em>36</em>(5), 847-860. (<a href='https://doi.org/10.1109/TPDS.2025.3544315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cyclical and continuous working characteristics of Internet of Things (IoT) devices make a large amount of the same or similar data, which can significantly consume storage space. To solve this problem, various secure data deduplication schemes have been proposed. However, existing deduplication schemes only perform deduplication based on data similarity, ignoring the internal connection among devices, making the existing schemes not directly applicable to parallel and distributed scenarios like IoT. Furthermore, since secure data deduplication leads to multiple users sharing same encryption key, which may lead to security issues. To this end, we propose a device relationship-based IoT data deduplication scheme that fully considers the IoT data characteristics and devices internal connections. Specifically, we propose a device relationship prediction approach, which can obtain device collaborative relationships by clustering the topology of their communication graph, and classifies the data types based on device relationships to achieve data deduplication with different security levels. Then, we design a similarity-preserving encryption algorithm, so that the security level of encryption key is determined by the data type, ensuring the security of the deduplicated data. In addition, two different data deduplication methods, identical deduplication and similar deduplication, have been designed to meet the privacy requirement of different data types, improving the efficiency of deduplication while ensuring data privacy as much as possible. We evaluate the performance of our scheme using five real datasets, and the results show that our scheme has favorable results in terms of both deduplication performance and computational cost.},
  archive      = {J_TPDS},
  author       = {Yuan Gao and Liquan Chen and Jianchang Lai and Tianyi Wang and Xiaoming Wu and Shui Yu},
  doi          = {10.1109/TPDS.2025.3544315},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {847-860},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IoT-dedup: Device relationship-based IoT data deduplication scheme},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reproducibility of the DaCe framework on NPBench benchmarks. <em>TPDS</em>, <em>36</em>(5), 841-846. (<a href='https://doi.org/10.1109/TPDS.2024.3427130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DaCe is a framework for Python that claims to provide massive speedups with C-like speeds compared to already existing high-performance Python frameworks (e.g. Numba or Pythran). In this work, we take a closer look at reproducing the NPBench work. We use performance results to confirm that NPBench achieves higher performance than NumPy in a variety of benchmarks and provide reasons as to why DaCe is not truly as portable as it claims to be, but with a small adjustment it can run anywhere.},
  archive      = {J_TPDS},
  author       = {Anish Govind and Yuchen Jing and Stefanie Dao and Michael Granado and Rachel Handran and Davit Margarian and Matthew Mikhailov and Danny Vo and Matei-Alexandru Gardus and Khai Vu and Derek Bouius and Bryan Chin and Mahidhar Tatineni and Mary Thomas},
  doi          = {10.1109/TPDS.2024.3427130},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {841-846},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reproducibility of the DaCe framework on NPBench benchmarks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis and reproducibility of “Productivity, portability, performance: Data-centric python”. <em>TPDS</em>, <em>36</em>(5), 835-840. (<a href='https://doi.org/10.1109/TPDS.2024.3366571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This report analyses the reproducibility of the results obtained in the NPBench (Ziogas et al. 2021) paper. We begin by providing the reader with some background information and a demonstration on the simplicity of DaCe. We then reproduce a subset of the results presented in the original paper, specifically: the comparison of DaCe on CPU and GPU over NumPy and its parallel efficiency in a distributed environment. For most benchmarks we show that we can obtain similar results on our machine. Despite that, for some benchmarks we cannot conclude the same without reasonable doubt. The experimental runs were performed during the SC22 Student Cluster Competition in Dallas, TX.},
  archive      = {J_TPDS},
  author       = {Christopher Lompa and Piotr Luczynski},
  doi          = {10.1109/TPDS.2024.3366571},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {835-840},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Analysis and reproducibility of “Productivity, portability, performance: Data-centric python”},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Critique of “Productivity, portability, performance data-centric python” by SCC team from sun yat-sen university. <em>TPDS</em>, <em>36</em>(5), 830-834. (<a href='https://doi.org/10.1109/TPDS.2024.3372291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In SC21, Ziogas et al. proposedData-Centric (DaCe) Python. It attains high performance and portability, and further extends the original productivity of Python. This paper analyzes the reproducibility of the DaCe paper as part of the SC22 Student Cluster Competition (SCC). The reproduction experiments are conducted on the Azure CycleCloud. Different from the DaCe paper, we use AMD EPYC 7V73X processors for CPU-based experiments. We successfully reproduce most of the results of the DaCe paper. The remaining results are also explainable.},
  archive      = {J_TPDS},
  author       = {Han Huang and Tengyang Zheng and Tianxing Yang and Yang Ye and Siran Liu and Zhe Tang and Shengyou Lu and Guangnan Feng and Zhiguang Chen and Dan Huang},
  doi          = {10.1109/TPDS.2024.3372291},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {830-834},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “Productivity, portability, performance data-centric python” by SCC team from sun yat-sen university},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Critique of “Productivity, portability, performance: Data-centric python” by SCC team from zhejiang university. <em>TPDS</em>, <em>36</em>(5), 826-829. (<a href='https://doi.org/10.1109/TPDS.2023.3333805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In SC’21, Alexandros Nikolaos Ziogas et al. proposed a Data-Centric Python workflow in their DaCe paper. DaCe provides high productivity, performance, and portability with language extensions and automatic optimizations. We reproduce the performance evaluation results from the paper on both CPU and GPU on the Azure CycleCloud cluster. We also reproduce the scaling results with up to 32 nodes and 64 processes. Our results show that the proposed workflow in that paper has outstanding performance and scalability in the provided cluster, in accordance with the SC paper.},
  archive      = {J_TPDS},
  author       = {Zihan Yang and Yi Chen and Kaiqi Chen and Xingjian Qian and Shaojun Xu and Yun Pan and Chong Zeng and Jianhai Chen and Yin Zhang and Zeke Wang},
  doi          = {10.1109/TPDS.2023.3333805},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {826-829},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “Productivity, portability, performance: Data-centric python” by SCC team from zhejiang university},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reproducing performance of data-centric python by SCC team from national tsing hua university. <em>TPDS</em>, <em>36</em>(5), 821-825. (<a href='https://doi.org/10.1109/TPDS.2024.3355441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As part of the Student Cluster Competition at the SC22 conference, this work aims to reproduce the performance evaluations of the Data Centric (DaCe) Python framework by leveraging Intel MKL and NVIDIA CUDA interface. The evaluations are conducted on a single CPU-based node, NVIDIA A100 GPUs, and an eight-node cloud supercomputer. Our experimental results successfully reproduce the performance evaluations on our cluster. Additionally, we provide insightful analysis and propose effective methods for achieving higher performance when utilizing DaCe as an acceleration library.},
  archive      = {J_TPDS},
  author       = {Fu-Chiang Chang and En-Ming Huang and Pin-Yi Kuo and Chan-Yu Mou and Hsu-Tzu Ting and Pang-Ning Wu and Jerry Chou},
  doi          = {10.1109/TPDS.2024.3355441},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {821-825},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reproducing performance of data-centric python by SCC team from national tsing hua university},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Productivity, portability, performance, and reproducibility: Data-centric python. <em>TPDS</em>, <em>36</em>(5), 804-820. (<a href='https://doi.org/10.1109/TPDS.2025.3549310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Python has become the de facto language for scientific computing. Programming in Python is highly productive, mainly due to its rich science-oriented software ecosystem built around the NumPy module. As a result, the demand for Python support in High-Performance Computing (HPC) has skyrocketed. However, the Python language itself does not necessarily offer high performance. This work presents a workflow that retains Python’s high productivity while achieving portable performance across different architectures. The workflow’s key features are HPC-oriented language extensions and a set of automatic optimizations powered by a data-centric intermediate representation. We show performance results and scaling across CPU, GPU, FPGA, and the Piz Daint supercomputer (up to 23,328 cores), with 2.47x and 3.75x speedups over previous-best solutions, first-ever Xilinx and Intel FPGA results of annotated Python, and up to 93.16% scaling efficiency on 512 nodes. Our benchmarks were reproduced in the Student Cluster Competition (SCC) during the Supercomputing Conference (SC) 2022. We present and discuss the student teams’ results.},
  archive      = {J_TPDS},
  author       = {Alexandros Nikolaos Ziogas and Timo Schneider and Tal Ben-Nun and Alexandru Calotoiu and Tiziano De Matteis and Johannes de Fine Licht and Luca Lavarini and Torsten Hoefler},
  doi          = {10.1109/TPDS.2025.3549310},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {804-820},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Productivity, portability, performance, and reproducibility: Data-centric python},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest Editorial:Special section on SC22 student cluster competition. <em>TPDS</em>, <em>36</em>(5), 803. (<a href='https://doi.org/10.1109/TPDS.2025.3549281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TPDS},
  author       = {Omer Rana and Josef Spillner and Stephen Leak and Gerald F Lofstead II and Rafael Tolosana Calasanz},
  doi          = {10.1109/TPDS.2025.3549281},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {803},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Guest Editorial:Special section on SC22 student cluster competition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedTune-SGM: A stackelberg-driven personalized federated learning strategy for edge networks. <em>TPDS</em>, <em>36</em>(4), 791-802. (<a href='https://doi.org/10.1109/TPDS.2025.3543368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as a prominent solution for distributed learning environments, enabling collaborative model training without centralized data collection. However, FL faces significant challenges such as data heterogeneity and resource-constraint edge devices for model training and analysis, leading to accuracy degradation and bias in model performance. To address these critical issues, we propose a novel FL strategy named FedTune-SGM, designed to optimize model training in decentralized settings. In this strategy, a cloud-based model is initially trained and fine-tuned on the edge devices with additional layers tailored to the specific data characteristics. This fine-tuning process effectively mitigates the impact of data heterogeneity, enhancing the robustness and generalization capability of the model. FedTune-SGM employs a strategic weighting mechanism that ensures a balanced and equitable contribution from participating edge devices to prevent dominant influences from resource-rich devices and promote a fairer and more accurate aggregated model. Additionally, the proposed strategy integrates a Stackelberg Game model to foster an interactive and dynamic cloud-edge setup that motivates edge devices to invest more effort in model training and ensures the effectiveness of resource-constraint edge devices. Extensive experiments conducted on three diverse datasets highlight the superior performance of the proposed FedTune-SGM strategy compared to state-of-the-art FL techniques in terms of accuracy and robustness while meeting the critical challenges of data heterogeneity and resource limitations in FL environments. Through these innovations, FedTune-SGM paves the way for more reliable and efficient distributed learning systems, unlocking the full potential of FL in practical applications.},
  archive      = {J_TPDS},
  author       = {Neha Singh and Mainak Adhikari},
  doi          = {10.1109/TPDS.2025.3543368},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {791-802},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedTune-SGM: A stackelberg-driven personalized federated learning strategy for edge networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Loci: Federated continual learning of heterogeneous tasks at edge. <em>TPDS</em>, <em>36</em>(4), 775-790. (<a href='https://doi.org/10.1109/TPDS.2025.3531123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated continual learning (FCL) has attracted growing attention in achieving collaborative model training among edge clients, each of which learns its local model for a sequence of tasks. Most existing FCL approaches aggregate clients’ latest local models to exchange knowledge. This unfortunately deviates from real-world scenarios where each model is optimized independently using the client’s own dynamic data and different clients have heterogeneous tasks. These tasks not only have distinct class labels (e.g., animals or vehicles) but also differ in input feature distributions. The aggregated model thus often shifts to a higher loss value and incurs accuracy degradation. In this article, we depart from the model-grained view of aggregation and transform it into multiple task-grained aggregations. Each aggregation allows a client to learn from other clients to improve its model accuracy on one task. To this end, we propose Loci to provide abstractions for clients’ past and peer task knowledge using compact model weights, and develop a communication-efficient approach to train each client’s local model by exchanging its tasks’ knowledge with the most accuracy relevant one from other clients. Through its general-purpose API, Loci can be used to provide efficient on-device training for existing deep learning applications of graph, image, nature language processing, and multimodal data. Using extensive comparative evaluations, we show Loci improves the model accuracy by 32.48% without increasing training time, reduces communication cost by 83.6%, and achieves more improvements when scale (task/client number) increases.},
  archive      = {J_TPDS},
  author       = {Yaxin Luopan and Rui Han and Qinglong Zhang and Xiaojiang Zuo and Chi Harold Liu and Guoren Wang and Lydia Y. Chen},
  doi          = {10.1109/TPDS.2025.3531123},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {775-790},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Loci: Federated continual learning of heterogeneous tasks at edge},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A tail latency SLO guaranteed task scheduling scheme for user-facing services. <em>TPDS</em>, <em>36</em>(4), 759-774. (<a href='https://doi.org/10.1109/TPDS.2025.3542638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A primary design objective for user-facing services for cloud and edge computing is to maximize query throughput, while meeting query tail latency Service Level Objectives (SLOs) for individual queries. Unfortunately, the existing solutions fall short of achieving this design objective, which we argue, is largely attributed to the fact that they fail to take the query fanout explicitly into account. In this paper, we propose TailGuard based on a Tail-latency-SLO-and-Fanout-aware Earliest-Deadline-First Queuing policy (TF-EDFQ) for task queuing at individual task servers the query tasks are fanned out to. With the task pre-dequeuing time deadline for each task being derived based on both query tail latency SLO and query fanout, TailGuard takes an important first step towards achieving the design objective. A query admission control scheme is also developed to provide tail latency SLO guarantee in the presence of resource shortages. TailGuard is evaluated against First-In-First-Out (FIFO) task queuing, task PRIority Queuing (PRIQ) and Tail-latency-SLO-aware EDFQ (T-EDFQ) policies by both simulation and testing in the Amazon EC2 cloud. It is driven by three types of applications in the Tailbench benchmark suite, featuring web search, in-memory key-value store, and transactional database applications. The results demonstrate that TailGuard can significantly improve resource utilization (e.g., up to 80% compared to FIFO), while also meeting the targeted tail latency SLOs, as compared with the other three policies. TailGuard is also implemented and tested in a highly heterogeneous Sensing-$a$s-a-Service (SaS) testbed for a data sensing service, demonstrating performance gains of up to 33% . These results are consistent with both the simulation and Amazon EC2 results.},
  archive      = {J_TPDS},
  author       = {Zhijun Wang and Huiyang Li and Lin Sun and Stoddard Rosenkrantz and Hao Che and Hong Jiang},
  doi          = {10.1109/TPDS.2025.3542638},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {759-774},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A tail latency SLO guaranteed task scheduling scheme for user-facing services},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flips: A flexible partitioning strategy near memory processing architecture for recommendation system. <em>TPDS</em>, <em>36</em>(4), 745-758. (<a href='https://doi.org/10.1109/TPDS.2025.3539534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized recommendation systems are massively deployed in production data centers. The memory-intensive embedding layers of recommendation systems are the crucial performance bottleneck, with operations manifesting as sparse memory lookups and simple reduction computations. Recent studies propose near-memory processing (NMP) architectures to speed up embedding operations by utilizing high internal memory bandwidth. However, these solutions typically employ a fixed vector partitioning strategy that fail to adapt to changes in data center deployment scenarios and lack practicality. We propose Flips, a flexible partitioning strategy NMP architecture that accelerates embedding layers. Flips supports more than ten partitioning strategies through hardware-software co-design. Novel hardware architectures and address mapping schemes are designed for the memory-side and host-side. We provide two approaches to determine the optimal partitioning strategy for each embedding table, enabling the architecture to accommodate changes in deployment scenarios. Importantly, Flips is decoupled from the NMP level and can utilize rank-level, bank-group-level and bank-level parallelism. In peer-level NMP evaluations, Flips outperforms state-of-the-art NMP solutions, RecNMP, TRiM, and ReCross by up to 4.0×, 4.1×, and 3.5×, respectively.},
  archive      = {J_TPDS},
  author       = {Yudi Qiu and Lingfei Lu and Shiyan Yi and Minge Jing and Xiaoyang Zeng and Yang Kong and Yibo Fan},
  doi          = {10.1109/TPDS.2025.3539534},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {745-758},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Flips: A flexible partitioning strategy near memory processing architecture for recommendation system},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-aware service placement for distributed learning in wireless edge networks. <em>TPDS</em>, <em>36</em>(4), 731-744. (<a href='https://doi.org/10.1109/TPDS.2025.3539620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has been a driving force in the evolution of tremendous computing services and applications in the past decade. Traditional learning systems rely on centralized training and inference, which poses serious privacy and security concerns. To solve this problem, distributed learning over wireless edge networks (DLWENs) emerges as a trending solution and has attracted increasing research interests. In DLWENs, corresponding services need to be placed onto the edge servers to process the distributed tasks. Apparently, different placement of training services can significantly affect the performance of all distributed learning tasks. In this article, we propose TASP, a task-aware service placement scheme for distributed learning in wireless edge networks. By carefully considering the structures (directed acyclic graphs) of the distributed learning tasks, the fine-grained task requests and inter-task dependencies are incorporated into the placement strategies to realize the parallel computation of learning services. We also exploit queuing theory to characterize the dynamics caused by task uncertainties. Extensive experiments based on the Alibaba ML dataset show that, compared to the state-of-the-art schemes, the proposed work reduces the overall delay of distributed learning tasks by 38.6% on average.},
  archive      = {J_TPDS},
  author       = {Rong Cong and Zhiwei Zhao and Mengfan Wang and Geyong Min and Jiangshu Liu and Jiwei Mo},
  doi          = {10.1109/TPDS.2025.3539620},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {731-744},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Task-aware service placement for distributed learning in wireless edge networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-designing transformer architectures for distributed inference with low communication. <em>TPDS</em>, <em>36</em>(4), 717-730. (<a href='https://doi.org/10.1109/TPDS.2024.3521582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer models have shown significant success in a wide range of tasks. However, the massive resources required for its inference prevent deployment on a single device with relatively constrainted resources, thus leaving a high threshold of integrating their advancements. Observing scenarios such as smart home applications on edge devices and cloud deployment on commodity hardware, it is promising to distribute Transformer inference across multiple devices. Unfortunately, due to the tightly-coupled feature of Transformer model, existing model parallelism approaches necessitate frequent communication to resolve data dependencies, making them unacceptable for distributed inference, especially under relatively weak interconnection. In this paper, we propose DeTransformer, a communication-efficient distributed Transformer inference system. The key idea of DeTransformer involves the co-design of Transformer architecture to reduce the communication during distributed inference. In detail, DeTransformer is based on a novel block parallelism approach, which restructures the original Transformer layer with a single block to the decoupled layer with multiple sub-blocks. Thus, it can exploit model parallelism between sub-blocks. Next, DeTransformer contains an adaptive execution approach that strikes a trade-off among communication capability, computing power and memory budget over multiple devices. It incorporates a two-phase planning for execution, namely static planning and runtime planning. The static planning runs offline, containing a profiling procedure and a weight placement strategy before execution. The runtime planning dynamically determines the optimal parallel computing strategy from an expertly crafted search space based on real-time requests. Notably, this execution approach can adapt to heterogeneous devices by distributing workload based on devices’ computing capabilities. We conduct experiments for both auto-regressive and auto-encoder tasks of Transformer models. Experimental results show that DeTransformer can reduce distributed inference latency by up to 2.81× compared to the SOTA approach on 4 devices, while effectively maintaining task accuracy and a consistent model size.},
  archive      = {J_TPDS},
  author       = {Jiangsu Du and Yuanxin Wei and Shengyuan Ye and Jiazhi Jiang and Xu Chen and Dan Huang and Yutong Lu},
  doi          = {10.1109/TPDS.2024.3521582},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {717-730},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Co-designing transformer architectures for distributed inference with low communication},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spread+: Scalable model aggregation in federated learning with non-IID data. <em>TPDS</em>, <em>36</em>(4), 701-716. (<a href='https://doi.org/10.1109/TPDS.2025.3539738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) addresses privacy concerns by training models without sharing raw data, overcoming the limitations of traditional machine learning paradigms. However, the rise of smart applications has accentuated the heterogeneity in data and devices, which presents significant challenges for FL. In particular, data skewness among participants can compromise model accuracy, while diverse device capabilities lead to aggregation bottlenecks, causing severe model congestion. In this article, we introduce Spread+, a hierarchical system that enhances FL by organizing clients into clusters and delegating model aggregation to edge devices, thus mitigating these challenges. Spread+ leverages hedonic coalition formation game to optimize customer organization and adaptive algorithms to regulate aggregation intervals within and across clusters. Moreover, it refines the aggregation algorithm to boost model accuracy. Our experiments demonstrate that Spread+ significantly alleviates the central aggregation bottleneck and surpasses mainstream benchmarks, achieving performance improvements of 49.58% over FAVG and 22.78% over Ring-allreduce.},
  archive      = {J_TPDS},
  author       = {Huanghuang Liang and Xin Yang and Xiaoming Han and Boan Liu and Chuang Hu and Dan Wang and Xiaobo Zhou and Dazhao Cheng},
  doi          = {10.1109/TPDS.2025.3539738},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {701-716},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Spread+: Scalable model aggregation in federated learning with non-IID data},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beelog: Online log compaction for dependable systems. <em>TPDS</em>, <em>36</em>(4), 689-700. (<a href='https://doi.org/10.1109/TPDS.2025.3541628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logs are a known abstraction used to develop dependable and secure distributed systems. By logging entries on a sequential global log, systems can synchronize updates over replicas and provide a consistent state recovery in the presence of faults. However, their usage incurs a non-negligible overhead on the application's performance. This article presents Beelog, an approach to reduce logging impact and accelerate recovery on log-based protocols by safely discarding entries from logs. The technique involves executing a log compaction during run-time concurrently with the persistence and execution of commands. Besides compacting logging information, the proposed technique splits the log file and incorporates strategies to reduce logging overhead, such as batching and parallel I/O. We evaluate the proposed approach by implementing it as a new feature of the etcd key-value store and comparing it against etcd's standard logging. Utilizing workloads from the YCSB benchmark and experimenting with different configurations for batch size and number of storage devices, our results indicate that Beelog can reduce application recovery time, especially in write-intensive workloads with a small number of keys and a probability favoring the most recent keys to be updated. In such scenarios, we observed up to a 50% compaction in the log file size and a 65% improvement in recovery time compared to etcd's standard recovery protocol. As a side effect, batching results in higher command execution latency, ranging from $ \text{100 ms}$ to $ \text{350 ms}$ with Beelog, compared to the default etcd's $ \text{90 ms}$. Except for the latency increase, the proposed technique does not impose other significant performance costs, making it a practical solution for systems where fast recovery and reduced storage are priorities.},
  archive      = {J_TPDS},
  author       = {Luiz Gustavo C. Xavier and Cristina Meinhardt and Odorico Machado Mendizabal},
  doi          = {10.1109/TPDS.2025.3541628},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {689-700},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Beelog: Online log compaction for dependable systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EfficientMoE: Optimizing mixture-of-experts model training with adaptive load balance. <em>TPDS</em>, <em>36</em>(4), 677-688. (<a href='https://doi.org/10.1109/TPDS.2025.3539297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture-of-Experts (MoE) efficiently trains large models by using sparse activation to lower costs, selecting a few experts based on data characteristics. However, it faces challenges such as All-to-All communication overhead and load imbalance, with most optimizations targeting dynamic graphs rather than the more efficient static graphs. This study identifies two key challenges in training MoE on static graphs: 1) excessive All-to-All communication (up to 75% of iteration time) and load imbalance (70% of tokens handled by two experts) between experts due to the sparse structure of the MoE model and the token distribution; and 2) inefficient zero-padding for static shapes, leading to unnecessary computational overhead(wasting approximately 50% of resources). Thus, EfficientMoE, a scheduling method based on expert load and data characteristics, is introduced. EfficientMoE first designs a sampler to collect real-time information about token distribution, expert load, etc. It constructs a load prediction model to evaluate expert load. Subsequently, EfficientMoE proposes a dynamic schedule strategy for experts with evaluated expert load, reducing All-to-All communication and addressing load-balancing issues. Additionally, an expert capacity model is proposed to set different capacities for replicas of hot experts before static graph compilation, minimizing computation and storage overhead caused by significant padding. This study implements EfficientMoE in MindSpore and uses 32 Ascend AI accelerators to train an MoE model with 21 billion parameters and evaluate its validity. EfficientMoE demonstrated an improvement of 30% in model training time, approximately 12% reduction in communication time, and saved 35% computational resources across different clusters, compared with Switch transformers, and the Fastermoe method for static graphs.},
  archive      = {J_TPDS},
  author       = {Yan Zeng and Chengchuang Huang and Yipeng Mei and Lifu Zhang and Teng Su and Wei Ye and Wenqi Shi and Shengnan Wang},
  doi          = {10.1109/TPDS.2025.3539297},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {677-688},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EfficientMoE: Optimizing mixture-of-experts model training with adaptive load balance},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A note on “AESM2 attribute-based encrypted search for multi-owner and multi-user distributed systems”. <em>TPDS</em>, <em>36</em>(4), 675-676. (<a href='https://doi.org/10.1109/TPDS.2025.3531446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the attribute-based encrypted search protocol [IEEE TPDS, 2023, 34(1), 92–107] is insecure against unauthorized user querying attack, because an adversary can convert a valid query from any authorized user into a new legitimate query, while the server cannot detect the fraud.},
  archive      = {J_TPDS},
  author       = {Zhengjun Cao},
  doi          = {10.1109/TPDS.2025.3531446},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {675-676},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A note on “AESM2 attribute-based encrypted search for multi-owner and multi-user distributed systems”},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMDP-based dynamic batching for improving responsiveness and energy efficiency of batch services. <em>TPDS</em>, <em>36</em>(4), 659-674. (<a href='https://doi.org/10.1109/TPDS.2025.3526283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For servers incorporating parallel computing resources, batching is a pivotal technique for providing efficient and economical services at scale. Parallel computing resources exhibit heightened computational and energy efficiency when operating with larger batch sizes. However, in the realm of online services, the adoption of a larger batch size may lead to longer response times. This paper aims to provide a dynamic batching scheme that delicately balances latency and efficiency. The system is modeled as a batch service queue with size-dependent service times. Then, the design of dynamic batching is formulated as a semi-Markov decision process (SMDP) problem, with the objective of minimizing the weighted sum of average response time and average power consumption. A method is proposed to derive an approximate optimal SMDP solution, representing the chosen dynamic batching policy. By introducing an abstract cost to reflect the impact of “tail” states, the space complexity and the time complexity of the procedure can decrease by 63.5% and 98%, respectively. Numerical results showcase the superiority of SMDP-based batching policies across various parameter setups. Additionally, the proposed scheme exhibits noteworthy flexibility in balancing power consumption and latency.},
  archive      = {J_TPDS},
  author       = {Yaodan Xu and Sheng Zhou and Zhisheng Niu},
  doi          = {10.1109/TPDS.2025.3526283},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {659-674},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SMDP-based dynamic batching for improving responsiveness and energy efficiency of batch services},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FHE4DMM: A low-latency distributed matrix multiplication with fully homomorphic encryption. <em>TPDS</em>, <em>36</em>(4), 645-658. (<a href='https://doi.org/10.1109/TPDS.2025.3534846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully Homomorphic Encryption (FHE) is a promising technology for secure, non-interactive outsourced computation. One notable method to increase the throughput of FHE-based outsourcing is batching, which typically involves large-scale matrix-matrix multiplications (MM). However, the substantial overhead inherent in existing FHE schemes poses a major challenge for processing these large-scale tasks, often resulting in insufficient memory or prolonged delays on a single machine, making it practically unviable. Utilizing multi-machine parallelism in cloud clusters for outsourced computation offers a natural solution to these obstacles. In this work, we propose FHE4DMM, a distributed algorithm that provides a unified view on encrypted matrices, accommodating various FHE schemes and any matrix dimensions, to accelerate large-scale encrypted MM. A key innovation is its reuse optimizations for parallelized homomorphic computations, which can offer valuable insights for broader FHE-based applications. We utilized FHE4DMM to conduct large-scale square ($4096\times 4096$) and rectangular ($32768\times 32768,32768\times 16$ ) matrix multiplications on 256 machines, achieving computation time of 172.2 s and 76.1 s, respectively, while ensuring a 128-bit security level. For scalability, the experiments demonstrate that FHE4DMM achieves linear speedup for $2^{i}$ ($i$ is from 0 to 6) machines across various matrix dimension cases. In addition, within the range of matrix dimensions that the state-of-the-art (SOTA) distributed FHE-MM algorithm (Huang et al. 2023) can handle, FHE4DMM attains a maximum speedup of 16.62x. To assess its practical performance, FHE4DMM is applied in a basic multi-layer feedforward network. We used 64 machines to perform secure outsourced inference on MNIST and CIFAR-10 datasets with encrypted models and data. Compared to using the SOTA, our method achieved speedups of up to 3.54x and 4.22x respectively, with the MM module obtaining a 4.09x and 4.87x speedup.},
  archive      = {J_TPDS},
  author       = {Yi Chen and Qiang-Sheng Hua and Zixiao Hong and Lin Zhu and Hai Jin},
  doi          = {10.1109/TPDS.2025.3534846},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {645-658},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FHE4DMM: A low-latency distributed matrix multiplication with fully homomorphic encryption},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monte: SFCs migration scheme in the distributed programmable data plane. <em>TPDS</em>, <em>36</em>(4), 633-644. (<a href='https://doi.org/10.1109/TPDS.2025.3532467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service function chains (SFCs) are sequences of network functions that provide specific services to meet operators’ needs in today's ISPs and datacenter networks. To improve the performance of SFCs, programmable data planes are used to leverage their low latency and high performance packet processing. However, SFCs need to be adaptable to dynamics such as changes in requirements and attributes. Therefore, the ability to migrate SFCs is essential. Unfortunately, migrating SFCs in distributed programmable data planes is challenging due to the risk of degraded performance and failure to meet SFCs requirements and resource constraints in switches. In this paper, we propose Monte, which provides an effective SFCs migration scheme in distributed programmable data planes. We build a novel integer programming model to represent the migration process with constraints on resource limitations of switches and SFCs attributes in the distributed data plane. Additionally, an SFCs migration algorithm is designed to optimize the migration cost by deeply analyzing resource allocation in the switch pipeline. Monte has been implemented on both P4 software switches (Bmv2) and hardware switches (Intel Tofino ASIC). Extensive evaluation results show that the migration cost in Monte is 94.03% lower on average than the state-of-the-art deployment scheme, and Monte can effectively save pipeline resources.},
  archive      = {J_TPDS},
  author       = {Xiaoquan Zhang and Lin Cui and Fung Po Tso and Yuhui Deng and Zhetao Li and Weijia Jia},
  doi          = {10.1109/TPDS.2025.3532467},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {633-644},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Monte: SFCs migration scheme in the distributed programmable data plane},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Paralfetch: Fast application launch on personal Computing/Communication devices. <em>TPDS</em>, <em>36</em>(4), 616-632. (<a href='https://doi.org/10.1109/TPDS.2024.3525337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paralfetch speeds up application launches on personal computing/communication devices, by means of: 1) accurate collection of launch-related disk read requests, 2) pre-scheduling of these requests to improve I/O throughput during prefetching, and 3) overlapping application execution with disk prefetching for hiding disk access time from the execution of the application. We implemented Paralfetch under Linux kernels on a desktop/laptop PC, a Raspberry Pi 3 board, and an Android smartphone. Tests with popular applications show that Paralfetch significantly reduces application launch times on flash-based drives and hard disk drives, and it outperforms GSoC Prefetch Lichota et al. 2007 and FAST Joo et al. 2011, which are representative application prefetchers available for Linux-based systems.},
  archive      = {J_TPDS},
  author       = {Junhee Ryu and Dongeun Lee and Kang G. Shin and Kyungtae Kang},
  doi          = {10.1109/TPDS.2024.3525337},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {616-632},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Paralfetch: Fast application launch on personal Computing/Communication devices},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative service composition approach considering providers’ self-interest and minimal service sharing. <em>TPDS</em>, <em>36</em>(3), 598-615. (<a href='https://doi.org/10.1109/TPDS.2025.3534283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service composition dynamically integrates various services from multiple providers to meet complex user requirements. However, most existing methods assume centralized control over all services, which is often unrealistic because providers typically prefer to independently manage their own services, posing challenges to the application of traditional methods. Collaborative service composition offers a solution by enabling providers to work together to complete service composition. However, this approach also faces its own challenges. Driven by self-interest, providers may be reluctant to offer services needed by others, and due to business competition, they may wish to share as few services as possible (where sharing services means disclosing service information to other providers). To address these challenges, we propose a novel collaborative service composition approach that comprehensively considers each provider’s self-interest and achieves service composition with minimal service sharing. First, we introduce a “self-interest degree” model to capture providers’ self-interest. This behavior may lead to service refusal, so we design a service availability prediction method based on a reputation model to minimize rejections. Then, we propose a decentralized service composition method. It utilizes historical composition records to mine empirical rules between requirements and services, constructing a correlations matrix, and collaboratively trains a multi-label classification model with other providers under a distributed federated learning framework. Combining the matrix and model outputs, we design a service composition method and a node coordination protocol that completes service composition with minimal service sharing. Experimental results demonstrate the effectiveness of the proposed method in capturing providers’ self-interest and showcase its superior performance compared to existing methods.},
  archive      = {J_TPDS},
  author       = {Xiao Wang and Hanchuan Xu and Jian Yang and Xiaofei Xu and Zhongjie Wang},
  doi          = {10.1109/TPDS.2025.3534283},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {598-615},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A collaborative service composition approach considering providers’ self-interest and minimal service sharing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative edge-cloud data transfer optimization for industrial internet of things. <em>TPDS</em>, <em>36</em>(3), 580-597. (<a href='https://doi.org/10.1109/TPDS.2025.3532261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Industrial Internet of Things, it is necessary to reserve enough bandwidth resources according to the maximum traffic peak. However, bandwidth reservation based on the maximum traffic peak leads to low resource utilization. In this paper, we propose a data transfer optimization solution, based on the cooperation of different entities in the local area, which strives to deliver data acquired by sensors to the cloud in a reliable manner and improve bandwidth utilization to save limited network resources. In our solution, the data transfers from the sensors in a local network are controlled by a local controller and some edge gateways with acceptable cost such that no congestion occurs in the path to the cloud and the bandwidth requirement of each flow can be met. To obtain a tradeoff between resource utilization and transfer delay, we study the problem of minimizing the maximum rate peak of periodic real-time traffic from distributed sensors and propose an algorithm to solve this problem with a desirable lower boundary of the performance. In addition, we design an application-level forwarding method that significantly improves resource utilization and a method of implementing reliable sampling instant adjustment. The experimental results show that our solution significantly improves resource utilization without producing network congestion.},
  archive      = {J_TPDS},
  author       = {Xinchang Zhang and Maoli Wang and Xiaomin Zhu and Zhiwei Yan and Guanggang Geng},
  doi          = {10.1109/TPDS.2025.3532261},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {580-597},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Collaborative edge-cloud data transfer optimization for industrial internet of things},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative study of sampling methods with cross-validation in the FedHome framework. <em>TPDS</em>, <em>36</em>(3), 570-579. (<a href='https://doi.org/10.1109/TPDS.2025.3526238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comparative study of sampling methods within the FedHome framework, designed for personalized in-home health monitoring. FedHome leverages federated learning (FL) and generative convolutional autoencoders (GCAE) to train models on decentralized edge devices while prioritizing data privacy. A notable challenge in this domain is the class imbalance in health data, where critical events such as falls are underrepresented, adversely affecting model performance. To address this, the research evaluates six oversampling techniques using Stratified K-fold cross-validation: SMOTE, Borderline-SMOTE, Random OverSampler, SMOTE-Tomek, SVM-SMOTE, and SMOTE-ENN. These methods are tested on FedHome's public implementation over 200 training rounds with and without stratified K-fold cross-validation. The findings indicate that SMOTE-ENN achieves the most consistent test accuracy, with a standard deviation range of 0.0167–0.0176, demonstrating stable performance compared to other samplers. In contrast, SMOTE and SVM-SMOTE exhibit higher variability in performance, as reflected by their wider standard deviation ranges of 0.0157–0.0180 and 0.0155–0.0180, respectively. Similarly, the Random OverSampler method shows a significant deviation range of 0.0155–0.0176. SMOTE-Tomek, with a deviation range of 0.0160–0.0175, also shows greater stability but not as much as SMOTE-ENN. This finding highlights the potential of SMOTE-ENN to enhance the reliability and accuracy of personalized health monitoring systems within the FedHome framework.},
  archive      = {J_TPDS},
  author       = {Arash Ahmadi and Sarah S. Sharif and Yaser M. Banad},
  doi          = {10.1109/TPDS.2025.3526238},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {570-579},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A comparative study of sampling methods with cross-validation in the FedHome framework},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AsyncFedGAN: An efficient and staleness-aware asynchronous federated learning framework for generative adversarial networks. <em>TPDS</em>, <em>36</em>(3), 553-569. (<a href='https://doi.org/10.1109/TPDS.2024.3521016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) are deep learning models that learn and generate new samples similar to existing ones. Traditionally, GANs are trained in centralized data centers, raising data privacy concerns due to the need for clients to upload their data. To address this, Federated Learning (FL) integrates with GANs, allowing collaborative training without sharing local data. However, this integration is complex because GANs involve two interdependent models—the generator and the discriminator—while FL typically handles a single model over distributed datasets. In this article, we propose a novel asynchronous FL framework for GANs, called AsyncFedGAN, designed to efficiently and distributively train both models tailored for molecule generation. AsyncFedGAN addresses the challenges of training interactive models, resolves the straggler issue in synchronous FL, reduces model staleness in asynchronous FL, and lowers client energy consumption. Our extensive simulations for molecular discovery show that AsyncFedGAN achieves convergence with proper settings, outperforms baseline methods, and balances model performance with client energy usage.},
  archive      = {J_TPDS},
  author       = {Daniel Manu and Abee Alazzwi and Jingjing Yao and Youzuo Lin and Xiang Sun},
  doi          = {10.1109/TPDS.2024.3521016},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {553-569},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AsyncFedGAN: An efficient and staleness-aware asynchronous federated learning framework for generative adversarial networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey on characterizing and understanding GNNs from a computer architecture perspective. <em>TPDS</em>, <em>36</em>(3), 537-552. (<a href='https://doi.org/10.1109/TPDS.2025.3532089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterizing and understanding graph neural networks (GNNs) is essential for identifying performance bottlenecks and facilitating their deployment in parallel and distributed systems. Despite substantial work in this area, a comprehensive survey on characterizing and understanding GNNs from a computer architecture perspective is lacking. This article presents a comprehensive survey, proposing a triple-level classification method to categorize, summarize, and compare existing efforts, particularly focusing on their implications for parallel architectures and distributed systems. We identify promising future directions for GNN characterization that align with the challenges of optimizing hardware and software in parallel and distributed systems. Our survey aims to help scholars systematically understand GNN performance bottlenecks and execution patterns from a computer architecture perspective, thereby contributing to the development of more efficient GNN implementations across diverse parallel architectures and distributed systems.},
  archive      = {J_TPDS},
  author       = {Meng Wu and Mingyu Yan and Wenming Li and Xiaochun Ye and Dongrui Fan and Yuan Xie},
  doi          = {10.1109/TPDS.2025.3532089},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {537-552},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Survey on characterizing and understanding GNNs from a computer architecture perspective},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $ \mathsf{GPABE} $<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="sans-serif">GPABE</mml:mi></mml:math>: GPU-based parallelization framework for attribute-based encryption schemes. <em>TPDS</em>, <em>36</em>(3), 520-536. (<a href='https://doi.org/10.1109/TPDS.2025.3529776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-based encryption (ABE) has emerged as a new paradigm for access control in cloud computing. However, despite the many promising features of ABE, its deployment in real-world systems is still limited, partially due to the expensive cost of its underlying mathematical operations, which often grow linearly with the size and complexity of the system's security policies. This becomes particularly challenging in data-intensive applications, where multiple users may simultaneously access and manipulate large volumes of data, resulting in high levels of concurrency and demand for computing resources, which are too heavy even for high-end servers. Further exacerbating the issues are the functionality and security requirements of a cloud, as they introduce additional computations to both the client and the server. Therefore, in this work, we introduce $ \mathsf{GPABE} $, the first GPU-based parallelization framework for ABE to facilitate its batch processing in cloud computing. By analyzing ABE's major computational workload, we identify multiple arithmetic modules that are common in the design of pairing-based ABEs. Based on the analysis, we further propose to decompose the ABE algorithm into computation graph, which can be efficiently implemented on the GPU platform. Our graph representation bridges the gap between ABE's high-level design and their low-level implementation on GPUs, and is applicable to a variety of popular schemes in the realm of ABE. We then implement $ \mathsf{GPABE} $ as a heterogeneous computing server, with several optimization techniques to improve its throughput. Finally, we evaluate the GPU implementation of several ABE schemes using $ \mathsf{GPABE} $. The results show a speedup of least 51.0× and at most 253.6× for the throughput of ABE algorithms, compared to their state-of-the-art CPU implementations, which preliminarily demonstrated the effectiveness of $ \mathsf{GPABE} $.},
  archive      = {J_TPDS},
  author       = {Wenhan Xu and Hui Ma and Rui Zhang and Jianhao Li},
  doi          = {10.1109/TPDS.2025.3529776},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {520-536},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {$ \mathsf{GPABE} $<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="sans-serif">GPABE</mml:mi></mml:math>: GPU-based parallelization framework for attribute-based encryption schemes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViTeGNN: Towards versatile inference of temporal graph neural networks on FPGA. <em>TPDS</em>, <em>36</em>(3), 502-519. (<a href='https://doi.org/10.1109/TPDS.2024.3521897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Graph Neural Networks (TGNNs) are powerful models to capture temporal, structural, and contextual information on temporal graphs, outperforming other methods in many high-impact downstream tasks. However, achieving high-performance TGNN inference in production environments is challenging because TGNN models suffer from high computation complexity and intrinsic temporal data dependency that hinders data parallelism. In addition, real-world TGNN applications have different latency and throughput requirements. This work presents ViTeGNN, a versatile TGNN inference solution for memory-based TGNNs on FPGAs. ViTeGNN performs algorithm-model-architecture co-design to meet the latency and throughput requirements of real-world TGNN applications. Besides the vanilla inference mode ViTeGNN-bal that updates embeddings for nodes interacting with others, we propose ViTeGNN-lat and ViTeGNN-thpt, optimized for latency and throughput. Our model optimizations include a lightweight method to compute attention scores and a related temporal neighbor pruning strategy to reduce computation and memory accesses. These are holistically coupled with key hardware optimizations that leverage the FPGA hardware. We propose a novel hardware module to execute the complex neighbor update process efficiently. To ensure similar accuracy vis-á-vis the original model, the simplified models are trained using the knowledge distillation technique. We propose a unified hardware design that supports all of these three inference modes without FPGA reconfiguration. Enabled by our flexible hardware architecture, we further propose ViTeGNN-auto, which automatically selects the best inference mode at runtime based on latency and throughput requirements, guided by our accurate performance model. We evaluate the performance of the proposed hardware accelerator on five real-world datasets. ViTeGNN-bal reduces the computation complexity by an average of 62% and memory accesses by an average of 36% with only 0.0042 accuracy loss. Compared with state-of-the-art implementations on CPU and GPU, our FPGA implementation achieves $53.9/26.0/16.1\times$ speedup and $8.2/4.0/2.5\times$ speedup for ViTeGNN-lat/-bal/-thpt, respectively.},
  archive      = {J_TPDS},
  author       = {Hongkuan Zhou and Bingyi Zhang and Rajgopal Kannan and Carl Busart and Viktor K. Prasanna},
  doi          = {10.1109/TPDS.2024.3521897},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {502-519},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ViTeGNN: Towards versatile inference of temporal graph neural networks on FPGA},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient verifiable cloud storage and distribution for large-scale data streaming. <em>TPDS</em>, <em>36</em>(3), 487-501. (<a href='https://doi.org/10.1109/TPDS.2025.3526642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data streaming is an ordered sequence of data continuously generated over time, whose dynamic scale is hard to be predicated in advance. Since the traditional integrity verification primitives are not qualified to check the integrity of the retrieved data and the outsourced database in streaming setting, some specific schemes were proposed by adopting the tree-like authentication structure or the combination of signature and accumulator. However, these schemes are not optimal for the owner. The main concerns can be generalized as how to reduce the size of the authentication information to be less than the scale of the data streaming, and enable the resource-constrained owner to check the data integrity without using challenge. To address the problems, we intend to find a new approach to design the scheme by exploiting the novel technique called decentralized vector commitment (DVC). Towards this goal, we first propose a key exposure-freeness chameleon vector commitment scheme, and then present the efficient DVC technique based on our key exposure-freeness chameleon vector commitment scheme. The scheme is finally constructed by leveraging the efficient DVC technique. Besides the integrity verification, our scheme is also sufficient to efficiently distribute the data to a user who is protected from receiving the stale data. To optimize the performance in concurrently retrieving multiple data, we introduce the batch query that reduces large amounts of communication and computation overheads. The security analysis and performance evaluation show that our solutions are secure and efficient.},
  archive      = {J_TPDS},
  author       = {Haining Yang and Dengguo Feng and Jing Qin},
  doi          = {10.1109/TPDS.2025.3526642},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {487-501},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards efficient verifiable cloud storage and distribution for large-scale data streaming},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HTLL: Latency-aware scalable blocking mutex. <em>TPDS</em>, <em>36</em>(3), 471-486. (<a href='https://doi.org/10.1109/TPDS.2025.3526859'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article finds that existing mutex locks suffer from throughput collapses or latency collapses, or both, in the oversubscribed scenarios where applications create more threads than the CPU core number, e.g., database applications like mysql use per thread per connection. We make an in-depth performance analysis on existing locks and then identify three design rules for the lock primitive to achieve scalable performance in oversubscribed scenarios. First, to achieve ideal throughput, the lock design should keep adequate number of active competitors. Second, the active competitors should be arranged carefully to avoid the lock-holder preemption problem. Third, to meet latency requirements, the lock design should track the latency of each competitor and reorder the competitors according to the latency requirement. We propose a new lock library called HTLL that satisfies these rules and achieves both high throughput and low latency even when the cores are oversubscribed. HTLL only requires minimal human effort (e.g., add several lines of code) to annotate the latency requirement. Evaluation results show that HTLL achieves scalable performance in the oversubscribed scenarios. Specifically, for the real-world database, LMDB, HTLL can reduce the tail latency by up to 97% with only an average 5% degradation in throughput, compared with state-of-the-art alternatives such as Malthusian, CST, and Mutexee locks; In comparison to the widely used pthread_mutex_lock, it can increase the throughput by up to 22% and decrease the latency by up to 80%. Meanwhile, for the under-subscribed scenarios, it also shows comparable performance than state-of-the-art blocking locks.},
  archive      = {J_TPDS},
  author       = {Ziqu Yu and Jinyu Gu and Zijian Wu and Nian Liu and Jian Guo},
  doi          = {10.1109/TPDS.2025.3526859},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {471-486},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HTLL: Latency-aware scalable blocking mutex},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Response time analysis and optimal priority assignment for global non-preemptive fixed-priority rigid gang scheduling. <em>TPDS</em>, <em>36</em>(3), 455-470. (<a href='https://doi.org/10.1109/TPDS.2025.3529218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-preemptive rigid gang scheduling combines the efficiency of parallel execution with the reduced overhead of non-preemptive scheduling. This approach is particularly advantageous for parallel hardware accelerators, such as Google's Edge Tensor Processing Unit (TPU), which is widely used for deep neural network (DNN) inference on embedded systems. This paper studies sporadic global non-preemptive fixed-priority (NP-FP) rigid gang scheduling, which is well-suited for DNN applications in Edge TPU pipelines. Each gang task spawns a fixed number of threads that must execute concurrently across distinct processing units. We introduce the first carry-in limitation technique specifically designed for gang task response time analysis, addressing the unique challenges posed by intra-task parallelism. This technique is formulated as a generalized knapsack problem, and we develop both a linear programming relaxation and a dynamic programming approach to solve it under different time complexities. Additionally, we propose the first optimal priority assignment policy for NP-FP gang schedulability tests. Our proposed schedulability analysis and optimal priority assignment policy are evaluated through extensive experiments, including both synthetic task sets and a case study using DNN benchmarks on commercial off-the-shelf Edge TPU accelerators. The results demonstrate that the proposed approaches effectively enhance the state-of-the-art global NP-FP gang schedulability tests, achieving improvements of up to 57.9% for synthetic task sets and 76.7% for Edge TPU benchmarks. Furthermore, we conduct an ablations study to examine the impact of different algorithmic components in the proposed technique, providing valuable insights for future research.},
  archive      = {J_TPDS},
  author       = {Binqi Sun and Tomasz Kloda and Jiyang Chen and Cen Lu and Marco Caccamo},
  doi          = {10.1109/TPDS.2025.3529218},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {455-470},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Response time analysis and optimal priority assignment for global non-preemptive fixed-priority rigid gang scheduling},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chasing common knowledge: Joint large model selection and pulling in MEC with parameter sharing. <em>TPDS</em>, <em>36</em>(3), 437-454. (<a href='https://doi.org/10.1109/TPDS.2025.3527649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pretrained Foundation Models (PFMs) are regarded as a promising accelerator for the development of various Artificial Intelligence (AI) applications, and have recently been widely fine-tuned to satisfy users’ personalized inference demands. As many users are attracted to PFM-based AI applications, remote data centers are increasingly unable to solely bear the enormous computational demands and meet the delay requirements of inference requests. Mobile edge computing (MEC) offers a viable solution for delivering low-latency inference services by pulling fine-tuned PFMs from the remote data center to cloudlets in the proximity of users. However, a fine-tuned PFM typically comprises billions of model parameters, which are highly resource-intensive, time-consuming, and cost-prohibitive to execute at the edge. To address this, we investigate a novel joint large model selection and pulling problem in MEC networks. The novelty of our study lies in exploring parameter sharing among fine-tuned PFMs based on their common knowledge. Specifically, we first formulate a Non-Linear Integer Programming (NLIP) for the problem to minimize the total delay of implementing all inference requests. We then transform the NLIP into an equivalent Integer Linear Program (ILP) that is much simpler to solve. We further propose a randomized algorithm with a provable approximation ratio for the problem. We also consider the online version of the problem with uncertain request demand, and develop an online learning algorithm with a bounded regret. The crux of the online algorithm is the adoption of the multi-armed bandit technique with restricted context for dynamic admissions of inference requests. We finally conduct extensive experiments based on real datasets. Experimental results demonstrate that our algorithms reduce at least 38% in total delays and average costs, while achieving a 5% improvement in average accuracies.},
  archive      = {J_TPDS},
  author       = {Lizhen Zhou and Zichuan Xu and Qiufen Xia and Zhou Xu and Wenhao Ren and Wenbo Qi and Jinjing Ma and Song Yan and Yuan Yang},
  doi          = {10.1109/TPDS.2025.3527649},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {437-454},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Chasing common knowledge: Joint large model selection and pulling in MEC with parameter sharing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High performance householder QR factorization on emerging GPU architectures using tensor cores. <em>TPDS</em>, <em>36</em>(3), 422-436. (<a href='https://doi.org/10.1109/TPDS.2024.3522776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 2017, NVIDIA GPUs have been equipped with specialized units known as Tensor Cores, which demonstrate remarkable efficiency in processing matrix multiplications (GEMMs). Beyond GEMMs, researchers have explored the potential applications of Tensor Cores in matrix factorization, such as QR factorization. However, the inside GEMMs in QR factorization are typically tall and skinny. Compared to compute-bound square GEMMs, these tall and skinny GEMMs are memory bound, leading to suboptimal performance on Tensor Cores. To solve this problem, we indicate the recursive QR factorization can convert the tall and skinny GEMMs to relatively square and large GEMMs, resulting in better performance on Tensor Cores. Besides, we extend the FP16 Tensor-Cores-based QR factorization to accommodate FP32 and FP64 on FP16 and INT8 Tensor Cores, respectively. Additionally, to address the issue of orthogonality loss in the preceding Tensor Cores-based QR factorization, we transition from the Gram-Schmidt to the Householder algorithm while preserving high performance. According to our experimental evaluation conducted on NVIDIA's A100 and GeForce RTX 3090 GPU, the precision levels of FP64, FP32, and FP16 are up to 6.22x, 8.67x, and 4.03x faster, respectively, than the current state-of-the-art implementations.},
  archive      = {J_TPDS},
  author       = {Yuhan Leng and Gaoyuan Zou and Hansheng Wang and Panruo Wu and Shaoshuai Zhang},
  doi          = {10.1109/TPDS.2024.3522776},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {422-436},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High performance householder QR factorization on emerging GPU architectures using tensor cores},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HpT: Hybrid acceleration of spatio-temporal attention model training on heterogeneous manycore architectures. <em>TPDS</em>, <em>36</em>(3), 407-421. (<a href='https://doi.org/10.1109/TPDS.2024.3522781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer models have become widely popular in numerous applications, and especially for building foundation large language models (LLMs). Recently, there has been a surge in the exploration of transformer-based architectures in non-LLM applications. In particular, the self-attention mechanism within the transformer architecture offers a way to exploit any hidden relations within data, making it widely applicable for a variety of spatio-temporal tasks in scientific computing domains (e.g., weather, traffic, agriculture). Most of these efforts have primarily focused on accelerating the inference phase. However, the computational resources required to train these attention-based models for scientific applications remain a significant challenge to address. Emerging non-volatile memory (NVM)-based processing-in-memory (PIM) architectures can achieve higher performance and better energy efficiency than their GPU-based counterparts. However, the frequent weight updates during training would necessitate write operations to NVM cells, posing a significant barrier for considering stand-alone NVM-based PIM architectures. In this paper, we present HpT, a new hybrid approach to accelerate the training of attention-based models for scientific applications. Our approach is hybrid at two different layers: at the software layer, our approach dynamically switches from a full-parameter training mode to a lower-parameter training mode by incorporating intrinsic dimensionality; and at the hardware layer, our approach harnesses the combined power of GPUs, resistive random-access memory (ReRAM)-based PIM devices, and systolic arrays. This software-hardware co-design approach is aimed at adaptively reducing both runtime and energy costs during the training phase, without compromising on quality. Experiments on four concrete real-world scientific applications demonstrate that our hybrid approach is able to significantly reduce training time (up to $11.9\times$) and energy consumption (up to $12.05\times$), compared to the corresponding full-parameter training executing on only GPUs. Our approach serves as an example for accelerating the training of attention-based models on heterogeneous platforms including ReRAMs.},
  archive      = {J_TPDS},
  author       = {Saiman Dahal and Pratyush Dhingra and Krishu Kumar Thapa and Partha Pratim Pande and Ananth Kalyanaraman},
  doi          = {10.1109/TPDS.2024.3522781},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {407-421},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HpT: Hybrid acceleration of spatio-temporal attention model training on heterogeneous manycore architectures},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated and fungible scheduling of deep learning workloads using multi-agent reinforcement learning. <em>TPDS</em>, <em>36</em>(3), 391-406. (<a href='https://doi.org/10.1109/TPDS.2024.3522333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPU clusters have been widely used to co-locate various deep learning (DL) workloads in a multi-tenant way. Although such resource sharing can significantly reduce training cost, resource contention and interference among co-located workloads make task scheduling very complex and challenging. To simplify the scheduling problem, existing algorithms usually divide the procedure of scheduling into two sub-tasks, i.e., task placement and resource allocation, and allocate resources according to pre-defined and fixed resource demands. However, such a paradigm significantly constrains the selection of potential scheduling solutions. In this article, we present MAIFS, a novel multi-agent reinforcement learning based scheduling algorithm that handles task placement and resource allocation integratedly, and allows fungible resource allocation based on resource sensitivity of DL workloads. The core of MAIFS lies in two mechanisms. The multi-agent attention mechanism is designed to learn and share inter-related resource state features observed from different agents, which enables agents to explore fungible resource allocation solutions. The dynamic coordination graph mechanism is designed for coordinating interactive task placement decisions of agents during integrated scheduling, so as to mitigate potential task conflicts. Simulated experiments using two large scale production DL workload traces and physical deployment experiments based on a Kubernetes based GPU cluster show that MAIFS can outperform state-of-the-art scheduling algorithms by up to 44% in terms of makespan and 46% in terms of job completion time (JCT).},
  archive      = {J_TPDS},
  author       = {Jialun Li and Danyang Xiao and Diying Yang and Xuan Mo and Weigang Wu},
  doi          = {10.1109/TPDS.2024.3522333},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {391-406},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Integrated and fungible scheduling of deep learning workloads using multi-agent reinforcement learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparrow: Expediting smart contract execution for blockchain sharding via inter-shard caching. <em>TPDS</em>, <em>36</em>(3), 377-390. (<a href='https://doi.org/10.1109/TPDS.2024.3522016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharding is a promising solution to scale blockchain by separating the system into multiple shards to process transactions in parallel. However, due to state separation and shard isolation, it is still challenging to efficiently support smart contracts on a blockchain sharding system where smart contracts can interact with each other, involving states maintained by multiple shards. Specifically, existing sharding systems adopt a costly multi-step collaboration mechanism to execute smart contracts, resulting in long latency and low throughput. This article proposes Sparrow, a blockchain sharding protocol achieving one-step execution for smart contracts. To break shard isolation, inspired by non-local hotspot data caching in traditional databases, we propose a new idea of inter-shard caching, allowing a shard to prefetch and cache frequently accessed contract states of other shards. The miner can thus use the inter-shard cache to pre-execute a pending transaction, retrieve all its contract invocations, and commit it to multiple shards in one step. Particularly, we first propose a speculative dispersal cache synchronisation mechanism for efficient and secure cache synchronization across shards in Byzantine environments. Then, we propose a multi-branch exploration mechanism to solve the rollback problem during the optimistic one-step execution of contract invocations with dependencies. We also present a series of conflict resolution mechanisms to decrease the rollback caused by inherent transaction conflicts. We implement prototypes for Sparrow and existing sharding systems, and the evaluation shows that Sparrow improves the throughput by $2.44\times$ and reduces the transaction latency by 30% compared with the existing sharding systems.},
  archive      = {J_TPDS},
  author       = {Junyuan Liang and Peiyuan Yao and Wuhui Chen and Zicong Hong and Jianting Zhang and Ting Cai and Min Sun and Zibin Zheng},
  doi          = {10.1109/TPDS.2024.3522016},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {377-390},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Sparrow: Expediting smart contract execution for blockchain sharding via inter-shard caching},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online elastic resource provisioning with QoS guarantee in container-based cloud computing. <em>TPDS</em>, <em>36</em>(3), 361-376. (<a href='https://doi.org/10.1109/TPDS.2024.3522085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud data centers, the exponential growth of data places increasing demands on computing, storage, and network resources, especially in multi-tenant environments. While this growth is crucial for ensuring Quality of Service (QoS), it also introduces challenges such as fluctuating resource requirements and static container configurations, which can lead to resource underutilization and high energy consumption. This article addresses online resource provisioning and efficient scheduling for multi-tenant environments, aiming to minimize energy consumption while balancing elasticity and QoS requirements. To address this, we propose a novel optimization framework that reformulates the resource provisioning problem into a more manageable form. By reducing the original multi-constraint optimization to a container placement problem, we apply the interior-point barrier method to simplify the optimization, integrating constraints directly into the objective function for efficient computation. We also introduce elasticity as a key parameter to balance energy consumption with autonomous resource scaling, ensuring that resource consolidation does not compromise system flexibility. The proposed Energy-Efficient and Elastic Resource Provisioning (EEP) framework comprises three main modules: a distributed resource management module that employs vertical partitioning and dynamic leader election for adaptive resource allocation; a prediction module using $\omega$-step prediction for accurate resource demand forecasting; and an elastic scheduling module that dynamically adjusts to tenant scaling needs, optimizing resource allocation and minimizing energy consumption. Extensive experiments across diverse cloud scenarios demonstrate that the EEP framework significantly improves energy efficiency and resource utilization compared to established baselines, supporting sustainable cloud management practices.},
  archive      = {J_TPDS},
  author       = {Shuaibing Lu and Ran Yan and Jie Wu and Jackson Yang and Xinyu Deng and Shen Wu and Zhi Cai and Juan Fang},
  doi          = {10.1109/TPDS.2024.3522085},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {361-376},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online elastic resource provisioning with QoS guarantee in container-based cloud computing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAT: Cellular automata on tensor cores. <em>TPDS</em>, <em>36</em>(2), 341-355. (<a href='https://doi.org/10.1109/TPDS.2024.3520395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cellular automata (CA) are simulation models that can produce complex emergent behaviors from simple local rules. Although state-of-the-art GPU solutions are already fast due to their data-parallel nature, their performance can rapidly degrade in CA with a large neighborhood radius. With the inclusion of tensor cores across the entire GPU ecosystem, interest has grown in finding ways to leverage these fast units outside the field of artificial intelligence, which was their original purpose. In this work, we present CAT, a GPU tensor core approach that can accelerate CA in which the cell transition function acts on a weighted summation of its neighborhood. CAT is evaluated theoretically, using an extended PRAM cost model, as well as empirically using the Larger Than Life (LTL) family of CA as case studies. The results confirm that the cost model is accurate, showing that CAT exhibits constant time throughout the entire radius range $1 \leq r \leq 16$ , and its theoretical speedups agree with the empirical results. At low radius $r=1,2$ , CAT is competitive and is only surpassed by the fastest state-of-the-art GPU solution. Starting from $r=3$ , CAT progressively outperforms all other approaches, reaching speedups of up to $101\times$ over a GPU baseline and up to $\sim \!14\times$ over the fastest state-of-the-art GPU approach. In terms of energy efficiency, CAT is competitive in the range $1 \leq r \leq 4$ and from $r \geq 5$ it is the most energy efficient approach. As for performance scaling across GPU architectures, CAT shows a promising trend that, if continues for future generations, it would increase its performance at a higher rate than classical GPU solutions. A CPU version of CAT was also explored, using the recently introduced AMX instructions. Although its performance is still below GPU tensor cores, it is a promising approach as it can still outperform some GPU approaches at large radius. The results obtained in this work put CAT as an approach with great potential for scientists who need to study emerging phenomena in CA with a large neighborhood radius, both in the GPU and in the CPU.},
  archive      = {J_TPDS},
  author       = {Cristóbal A. Navarro and Felipe A. Quezada and Enzo Meneses and Héctor Ferrada and Nancy Hitschfeld},
  doi          = {10.1109/TPDS.2024.3520395},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {341-355},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CAT: Cellular automata on tensor cores},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained QoS control via tightly-coupled bandwidth monitoring and regulation for FPGA-based heterogeneous SoCs. <em>TPDS</em>, <em>36</em>(2), 326-340. (<a href='https://doi.org/10.1109/TPDS.2024.3513416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commercial embedded systems increasingly rely on heterogeneous architectures that integrate general-purpose, multi-core processors, and various hardware accelerators on the same chip. This provides the high performance required by modern applications at a low cost and low power consumption, but at the same time poses new challenges. Hardware resource sharing at various levels, and in particular at the main memory controller level, results in slower execution time for the application tasks, ultimately making the system unpredictable from the point of view of timing. To enable the adoption of heterogeneous systems-on-chip (System on Chips (SoCs)) in the domain of timing-critical applications several hardware and software approaches have been proposed, bandwidth regulation based on monitoring and throttling being one of the most widely adopted. Existing solutions, however, are either too coarse-grained, limiting the control over computing engines activities, or strongly platform-dependent, addressing the problem only for specific SoCs. This article proposes an innovative approach that can accurately control main memory bandwidth usage in FPGA-based heterogeneous SoCs. In particular, it controls system bandwidth by connecting a runtime bandwidth regulation component to FPGA-based accelerators. Our solution offers dynamically configurable, fine-grained bandwidth regulation – to adapt to the varying requirements of the application over time – at a very low overhead. Furthermore, it is entirely platform-independent, capable of integration with any FPGA-based accelerator. Developed at the register-transfer level using a reference SoC platform, it is designed for easy compatibility with any FPGA-based SoC. Experimental results conducted on the Xilinx Zynq UltraScale+ platform demonstrate that our approach (i) is more than $100\times$ faster than loosely-coupled, software controlled regulators; (ii) is capable of exploiting the system bandwidth 28.7% more efficiently than tightly-coupled hardware regulators (e.g., ARM CoreLink QoS-400, where available); (iii) enables task co-scheduling solutions not feasible with state-of-the-art bandwidth regulation methods.},
  archive      = {J_TPDS},
  author       = {Giacomo Valente and Gianluca Brilli and Tania Di Mascio and Alessandro Capotondi and Paolo Burgio and Paolo Valente and Andrea Marongiu},
  doi          = {10.1109/TPDS.2024.3513416},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {326-340},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fine-grained QoS control via tightly-coupled bandwidth monitoring and regulation for FPGA-based heterogeneous SoCs},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging graph analysis to pinpoint root causes of scalability issues for parallel applications. <em>TPDS</em>, <em>36</em>(2), 308-325. (<a href='https://doi.org/10.1109/TPDS.2024.3485789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to scale parallel applications to modern supercomputers because of load imbalance, resource contention, and communications between processes. Profiling and tracing are two main performance analysis approaches for detecting these scalability bottlenecks. Profiling is low-cost but lacks detailed dependence for identifying root causes. Tracing records plentiful information but incurs significant overheads. To address these issues, we present ScalAna , which employs static analysis techniques to combine the benefits of profiling and tracing - it enables tracing's analyzability with overhead similar to profiling. ScalAna uses static analysis to capture program structures and data dependence of parallel applications, and leverages lightweight profiling approaches to record performance data during runtime. Then a parallel performance graph is generated with both static and dynamic data. Based on this graph, we design a backtracking detection approach to automatically pinpoint the root causes of scaling issues. We evaluate the efficacy and efficiency of ScalAna using several real applications with up to 704K lines of code and demonstrate that our approach can effectively pinpoint the root causes of scaling loss with an average overhead of 5.65% for up to 16,384 processes. By fixing the root causes detected by our tool, it achieves up to 33.01% performance improvement.},
  archive      = {J_TPDS},
  author       = {Yuyang Jin and Haojie Wang and Xiongchao Tang and Zhenhua Guo and Yaqian Zhao and Torsten Hoefler and Tao Liu and Xu Liu and Jidong Zhai},
  doi          = {10.1109/TPDS.2024.3485789},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {308-325},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Leveraging graph analysis to pinpoint root causes of scalability issues for parallel applications},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMPIPE: Unequal microbatches-based pipeline parallelism for deep neural network training. <em>TPDS</em>, <em>36</em>(2), 293-307. (<a href='https://doi.org/10.1109/TPDS.2024.3515804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing need for large-scale deep neural networks (DNN) has made parallel training an area of intensive focus. One effective method, microbatch-based pipeline parallelism (notably GPipe), accelerates parallel training in various architectures. However, existing parallel training architectures normally use equal data partitioning (EDP), where each layer's process maintains identical microbatch-sizes. EDP may hinder training speed because different processes often require varying optimal microbatch-sizes. To address this, we introduce UMPIPE, a novel framework for unequal microbatches-based pipeline parallelism. UMPIPE enables unequal data partitions (UEDP) across processes to optimize resource utilization. We develop a recurrence formula to calculate the time cost in UMPIPE by considering both computation and communication processes. To further enhance UMPIPE's efficiency, we propose the Dual-Chromosome Genetic Algorithm for UMPIPE (DGAP) that accounts for the independent time costs of forward and backward propagation. Furthermore, we present TiDGAP, a two-level improvement on DGAP. TiDGAP accelerates the process by simultaneously calculating the end time for multiple individuals and microbatches using matrix operations. Our extensive experiments validate the dual-chromosome strategy's optimization benefits and TiDGAP's acceleration capabilities. TiDGAP can achieve better training schemes than baselines, such as the local greedy algorithm and the global greedy-based dynamic programming. Compared to (GPipe, PipeDream), UMPIPE achieves increases in training speed: $(13.89,11.09)\%$ for GPT1-14, $(17.11, 7.96)\%$ for VGG16 and $\geq (170,100)\%$ for simulation networks.},
  archive      = {J_TPDS},
  author       = {Guangyao Zhou and Wenhong Tian and Rajkumar Buyya and Kui Wu},
  doi          = {10.1109/TPDS.2024.3515804},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {293-307},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {UMPIPE: Unequal microbatches-based pipeline parallelism for deep neural network training},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spreeze: High-throughput parallel reinforcement learning framework. <em>TPDS</em>, <em>36</em>(2), 282-292. (<a href='https://doi.org/10.1109/TPDS.2024.3497986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The promotion of large-scale applications of reinforcement learning (RL) requires efficient training computation. While existing parallel RL frameworks encompass a variety of RL algorithms and parallelization techniques, the excessively burdensome communication frameworks hinder the attainment of the hardware's limit for final throughput and training effects on a single desktop. In this article, we propose Spreeze, a lightweight parallel framework for RL that efficiently utilizes a single desktop hardware resource to approach the throughput limit. We asynchronously parallelize the experience sampling, network update, performance evaluation, and visualization operations, and employ multiple efficient data transmission techniques to transfer various types of data between processes. The framework can automatically adjust the parallelization hyperparameters based on the computing ability of the hardware device in order to perform efficient large-batch updates. Based on the characteristics of the “Actor-Critic” RL algorithm, our framework uses dual GPUs to independently update the network of actors and critics in order to further improve throughput. Simulation results show that our framework can achieve up to 15,000 Hz experience sampling and 370,000 Hz network update frame rate using only a personal desktop computer, which is an order of magnitude higher than other mainstream parallel RL frameworks, resulting in a 73% reduction of training time. Our work on fully utilizing the hardware resources of a single desktop computer is fundamental to enabling efficient large-scale distributed RL training.},
  archive      = {J_TPDS},
  author       = {Jing Hou and Guang Chen and Ruiqi Zhang and Zhijun Li and Shangding Gu and Changjun Jiang},
  doi          = {10.1109/TPDS.2024.3497986},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {282-292},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Spreeze: High-throughput parallel reinforcement learning framework},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TOP: Task-based operator parallelism for asynchronous deep learning inference on GPU. <em>TPDS</em>, <em>36</em>(2), 266-281. (<a href='https://doi.org/10.1109/TPDS.2024.3511543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current deep learning compilers have made significant strides in optimizing computation graphs for single- and multi-model scenarios. However, they lack specific optimizations for asynchronous multi-task inference systems. In such systems, tasks arrive dynamically, leading to diverse inference progress for each model. This renders traditional optimization strategies based solely on the original computation graph suboptimal or even invalid. Furthermore, existing operator scheduling methods do not account for parallel task pipelines involving the same model. Task pipelines present additional opportunities for optimization. Therefore, we propose Task-based Operator Parallelism (TOP). TOP incorporates an understanding of the impact of task arrival patterns on the inference progress of each model. It leverages the multi-agent reinforcement learning algorithm MADDPG to cooperatively optimize the task launcher and model scheduler, generating an optimal pair of dequeue frequency and computation graph. The objective of TOP is to enhance resource utilization, increase throughput, and allocate resources judiciously to prevent task backlog. To expedite the optimization process in TOP, we introduce a novel stage partition method using the GNN-based Policy Gradient (GPG) algorithm. Through extensive experiments on various devices, we demonstrate the efficacy of TOP. It outperforms the state-of-the-art in operator scheduling for both single- and multi-model task processing scenarios. Benefiting from TOP, we can significantly enhance the throughput of a single model by increasing its concurrency or batch size, thereby achieving self-acceleration.},
  archive      = {J_TPDS},
  author       = {Changyao Lin and Zhenming Chen and Ziyang Zhang and Jie Liu},
  doi          = {10.1109/TPDS.2024.3511543},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {266-281},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TOP: Task-based operator parallelism for asynchronous deep learning inference on GPU},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object proxy patterns for accelerating distributed applications. <em>TPDS</em>, <em>36</em>(2), 253-265. (<a href='https://doi.org/10.1109/TPDS.2024.3511347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workflow and serverless frameworks have empowered new approaches to distributed application design by abstracting compute resources. However, their typically limited or one-size-fits-all support for advanced data flow patterns leaves optimization to the application programmer—optimization that becomes more difficult as data become larger. The transparent object proxy, which provides wide-area references that can resolve to data regardless of location, has been demonstrated as an effective low-level building block in such situations. Here we propose three high-level proxy-based programming patterns—distributed futures, streaming, and ownership—that make the power of the proxy pattern usable for more complex and dynamic distributed program structures. We motivate these patterns via careful review of application requirements and describe implementations of each pattern. We evaluate our implementations through a suite of benchmarks and by applying them in three meaningful scientific applications, in which we demonstrate substantial improvements in runtime, throughput, and memory usage.},
  archive      = {J_TPDS},
  author       = {J. Gregory Pauloski and Valerie Hayot-Sasson and Logan Ward and Alexander Brace and André Bauer and Kyle Chard and Ian Foster},
  doi          = {10.1109/TPDS.2024.3511347},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {253-265},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Object proxy patterns for accelerating distributed applications},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient GPU algorithm for lattice boltzmann method on sparse complex geometries. <em>TPDS</em>, <em>36</em>(2), 239-252. (<a href='https://doi.org/10.1109/TPDS.2024.3510810'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many fluid flow problems, such as the porous media, arterial blood flow and tissue fluid, contain sparse complex geometries. Although the lattice Boltzmann method is good at dealing with the complex boundaries, these sparse complex geometries cause the low computational performance and high memory consumption when the graphics processing unit (GPU) is used to accelerate the numerical computation. These problems would be addressed by compact memory layout, sophisticated memory access and enhanced thread utilization. This paper proposes a GPU-based algorithm to improve the lattice Boltzmann simulations with sparse complex geometries. An access pattern for a single set of distribution functions together with a semi-direct addressing is adopted to reduce memory consumption, while a collected structure of arrays is employed to enhance memory access efficiency. Furthermore, an address index array and a node classification coding scheme are employed to improve the GPU thread utilization ratio and reduce the GPU global memory access, respectively. The accuracy and mesh-independence has been verified by the numerical simulations of Poiseuille flow and porous media flow with face-centered filled spheres. The present algorithm has a significantly lower memory consumption than those based on direct or indirect addressing schemes. It improves the computational performance by several times compared to the other algorithms on the common GPU hardware.},
  archive      = {J_TPDS},
  author       = {Zhangrong Qin and Xusheng Lu and Long Lv and Zhongxiang Tang and Binghai Wen},
  doi          = {10.1109/TPDS.2024.3510810},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {239-252},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An efficient GPU algorithm for lattice boltzmann method on sparse complex geometries},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards universal performance modeling for machine learning training on multi-GPU platforms. <em>TPDS</em>, <em>36</em>(2), 226-238. (<a href='https://doi.org/10.1109/TPDS.2024.3507814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterizing and predicting the training performance of modern machine learning (ML) workloads on compute systems with compute and communication spread between CPUs, GPUs, and network devices is not only the key to optimization and planning but also a complex goal to achieve. The primary challenges include the complexity of synchronization and load balancing between CPUs and GPUs, the variance in input data distribution, and the use of different communication devices and topologies (e.g., NVLink, PCIe, network cards) that connect multiple compute devices, coupled with the desire for flexible training configurations. Built on top of our prior work for single-GPU platforms, we address these challenges and enable multi-GPU performance modeling 1 by incorporating (1) data-distribution-aware performance models for embedding table lookup, and (2) data movement prediction of communication collectives, into our upgraded performance modeling pipeline equipped with inter-and intra-rank synchronization for ML workloads trained on multi-GPU platforms. Beyond accurately predicting the per-iteration training time of deep learning recommendation models (DLRM) models with random configurations with a geomean error of 5.21% on two multi-GPU platforms, our prediction pipeline generalizes well to other types of ML workloads, such as Transformer-based natural language processing (NLP) models with a geomean error of 3.00%. Moreover, even without actually running ML workloads like DLRMs on the hardware, it is capable of generating insights such as quickly selecting the fastest embedding table sharding configuration (with a success rate of 85%).},
  archive      = {J_TPDS},
  author       = {Zhongyi Lin and Ning Sun and Pallab Bhattacharya and Xizhou Feng and Louis Feng and John D. Owens},
  doi          = {10.1109/TPDS.2024.3507814},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {226-238},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards universal performance modeling for machine learning training on multi-GPU platforms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DegaFL: Decentralized gradient aggregation for cross-silo federated learning. <em>TPDS</em>, <em>36</em>(2), 212-225. (<a href='https://doi.org/10.1109/TPDS.2024.3501581'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging promising paradigm of privacy-preserving machine learning (ML). An important type of FL is cross-silo FL, which enables a moderate number of organizations to cooperatively train a shared model by keeping confidential data locally and aggregating gradients on a central parameter server. However, the central server may be vulnerable to malicious attacks or software failures in practice. To address this issue, in this paper, we propose $\mathtt{DegaFL} $ , a novel decentralized gradient aggregation approach for cross-silo FL. $\mathtt{DegaFL} $ eliminates the central server by aggregating gradients on each participant, and maintains and synchronizes gradients of only the current training round. Besides, we propose $\mathtt{AdaAgg} $ to adaptively aggregate correct gradients from honest nodes and use HotStuff to ensure the consistency of the training round number and gradients among all nodes. Experimental results show that $\mathtt{DegaFL} $ defends against common threat models with minimal accuracy loss, and achieves up to $50\times$ reduction in storage overhead and up to $13\times$ reduction in network overhead, compared to state-of-the-art decentralized FL approaches.},
  archive      = {J_TPDS},
  author       = {Jialiang Han and Yudong Han and Xiang Jing and Gang Huang and Yun Ma},
  doi          = {10.1109/TPDS.2024.3501581},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {212-225},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DegaFL: Decentralized gradient aggregation for cross-silo federated learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Slark: A performance robust decentralized inter-datacenter deadline-aware coflows scheduling framework with local information. <em>TPDS</em>, <em>36</em>(2), 197-211. (<a href='https://doi.org/10.1109/TPDS.2024.3508275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inter-datacenter network applications generate massive coflows for purposes, e.g., backup, synchronization, and analytics, with deadline requirements. Decentralized coflow scheduling frameworks are desirable for their scalability in cross-domain deployment but grappling with the challenge of information agnosticism for lack of cross-domain privileges. Current information-agnostic coflow scheduling methods are incompatible with decentralized frameworks for relying on centralized controllers to continuously monitor and learn from coflow global transmission states to infer global coflow information. Alternative methods propose mechanisms for decentralized global coflow information gathering and synchronization. However, they require dedicated physical hardware or control logic, which could be impractical for incremental deployment. This article proposes Slark, a decentralized deadline-aware coflow scheduling framework, which meets coflows’ soft and hard deadline requirements using only local traffic information. It eschews requiring global coflow transmission states and dedicated hardware or control logic by leveraging multiple software-implemented scheduling agents working independently on each node and integrating such information agnosticism into node-specific bandwidth allocation by modeling it as a robust optimization problem with flow information on the other nodes represented as uncertain parameters. Subsequently, we validate the performance robustness of Slark by investigating how perturbations in the optimal objective function value and the associated optimal solution are affected by uncertain parameters. Finally, we propose a firebug-swarm-optimization-based heuristic algorithm to tackle the non-convexity in our problem. Experimental results demonstrate that Slark can significantly enhance transmission revenue and increase soft and hard deadline guarantee ratios by 10.52% and 7.99% on average.},
  archive      = {J_TPDS},
  author       = {Xiaodong Dong and Lihai Nie and Zheli Liu and Yang Xiang},
  doi          = {10.1109/TPDS.2024.3508275},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {197-211},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Slark: A performance robust decentralized inter-datacenter deadline-aware coflows scheduling framework with local information},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-effective and low-latency data placement in edge environment based on PageRank-inspired regional value. <em>TPDS</em>, <em>36</em>(2), 185-196. (<a href='https://doi.org/10.1109/TPDS.2024.3506625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge storage offers low-latency services to users. However, due to strained edge resources and high costs, enterprises must choose the data that most warrant placement at the edge and place it in the right location. In practice, data exhibit temporal and spatial properties, and variability, which have a significant impact on their placement, but have been largely ignored in research. To address this, we introduce the concept of data temperature, which considers data characteristics over time and space. To consider the influence of spatial relevance among different regions for placing data, inspired by PageRank, we present a model using data temperature to assess the regional value of data, which effectively leverages collaboration within the edge storage system. We also propose a regional value-based algorithm (RVA) that minimizes cost while meeting user response time requirements. By taking into account the correlation between regions, the RVA can achieve lower latency than current methods when creating an equal or even smaller number of replicas. Experimental results validate the efficacy of the proposed method in terms of latency, success rate, and cost efficiency.},
  archive      = {J_TPDS},
  author       = {Pengwei Wang and Junye Qiao and Yuying Zhao and Zhijun Ding},
  doi          = {10.1109/TPDS.2024.3506625},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {185-196},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cost-effective and low-latency data placement in edge environment based on PageRank-inspired regional value},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GreenFlow: A carbon-efficient scheduler for deep learning workloads. <em>TPDS</em>, <em>36</em>(2), 168-184. (<a href='https://doi.org/10.1109/TPDS.2024.3470074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has become a key component of modern software. Training DL models leads to huge carbon emissions. In data centers, it is important to reduce carbon emissions while completing DL training jobs early. In this article, we propose GreenFlow, a GPU cluster scheduler that reduces the average Job Completion Time (JCT) under a carbon emission budget. We first present performance models for DL training jobs to predict the throughput and energy consumption performance under different configurations. Based on the performance models and the carbon intensity of the grid, GreenFlow dynamically allocates GPUs, and adjusts the GPU-level and job-level configurations of DL training jobs. GreenFlow applies network packing and buddy allocation to job placement, thus avoiding extra carbon incurred by resource fragmentations. Evaluations on a real testbed show that when emitting the same amount of carbon, GreenFlow can improve the average JCT by up to 2.15×, compared to competitive baselines.},
  archive      = {J_TPDS},
  author       = {Diandian Gu and Yihao Zhao and Peng Sun and Xin Jin and Xuanzhe Liu},
  doi          = {10.1109/TPDS.2024.3470074},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {168-184},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GreenFlow: A carbon-efficient scheduler for deep learning workloads},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint dynamic data and model parallelism for distributed training of DNNs over heterogeneous infrastructure. <em>TPDS</em>, <em>36</em>(2), 150-167. (<a href='https://doi.org/10.1109/TPDS.2024.3506588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed training of deep neural networks (DNNs) suffers from efficiency declines in dynamic heterogeneous environments, due to the resource wastage brought by the straggler problem in data parallelism (DP) and pipeline bubbles in model parallelism (MP). Additionally, the limited resource availability requires a trade-off between training performance and long-term costs, particularly in online settings. To address these challenges, this article presents a novel online approach to maximize long-term training efficiency in heterogeneous environments through uneven data assignment and communication-aware model partitioning. A group-based hierarchical architecture combining DP and MP is developed to balance discrepant computation and communication capabilities, and offer a flexible parallel mechanism. In order to jointly optimize the performance and long-term cost of the online DL training process, we formulate this problem as a stochastic optimization with time-averaged constraints. By utilizing Lyapunov’s stochastic network optimization theory, we decompose it into several instantaneous sub-optimizations, and devise an effective online solution to address them based on tentative searching and linear solving. We have implemented a prototype system and evaluated the effectiveness of our solution based on realistic experiments, reducing batch training time by up to 68.59% over state-of-the-art methods.},
  archive      = {J_TPDS},
  author       = {Zhi Ling and Xiaofeng Jiang and Xiaobin Tan and Huasen He and Shiyin Zhu and Jian Yang},
  doi          = {10.1109/TPDS.2024.3506588},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {150-167},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Joint dynamic data and model parallelism for distributed training of DNNs over heterogeneous infrastructure},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-dimensional balanced partitioning and efficient caching for distributed graph analysis. <em>TPDS</em>, <em>36</em>(2), 133-149. (<a href='https://doi.org/10.1109/TPDS.2024.3501292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed graph analysis usually partitions a large graph into multiple small-sized subgraphs and distributes them into a cluster of machines for computing. Therefore, graph partitioning plays a crucial role in distributed graph analysis. However, the widely used existing graph partitioning schemes balance only in one dimension (number of edges or vertices) or incur a large number of edge cuts, so they degrade the performance of distributed graph analysis. In this article, we propose a novel graph partition scheme BPart and two enhanced algorithms BPart-C and BPart-S to achieve a balanced partition for both vertices and edges, and also reduce the number of edge cuts. Besides, we also propose a neighbor-aware caching scheme to further reduce the number of edge cuts so as to improve the efficiency of distributed graph analysis. Our experimental results show that BPart-C and BPart-S can achieve a better balance in both dimensions (the number of vertices and edges), and meanwhile reducing the number of edge cuts, compared to multiple existing graph partitioning algorithms, i.e., Chunk-V, Chunk-E, Fennel, and Hash. We also integrate these partitioning algorithms into two popular distributed graph systems, KnightKing and Gemini, to validate their impact on graph analysis efficiency. Results show that both BPart-C and BPart-S can significantly reduce the total running time of various graph applications by up to 60% and 70%, respectively. In addition, the neighbor-aware caching scheme can further improve the performance by up to 24%.},
  archive      = {J_TPDS},
  author       = {Shuai Lin and Rui Wang and Yongkun Li and Yinlong Xu and John C. S. Lui},
  doi          = {10.1109/TPDS.2024.3501292},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {133-149},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Two-dimensional balanced partitioning and efficient caching for distributed graph analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of service demand variability on data center performance. <em>TPDS</em>, <em>36</em>(2), 120-132. (<a href='https://doi.org/10.1109/TPDS.2024.3497792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data centers feature an extensive array of cores that handle quite a diverse range of jobs. Recent traces, shared by leading cloud data center enterprises like Google and Alibaba, reveal that the constant increase in data center services and computational power is accompanied by a growing variability in service demand requirements. The number of cores needed for a job can vary widely, ranging from one to several thousands, and the number of seconds a core is held by a job can span more than five orders of magnitude. In this context of extreme variability, the policies governing the allocation of cores to jobs play a crucial role in the performance of data centers. It is widely acknowledged that the First-In First-Out (FIFO) policy tends to underutilize available computing capacity due to the varying magnitudes of core requests. However, the impact of the extreme variability in service demands on job waiting and response times, that has been deeply investigated in traditional queuing models, is not as well understood in the case of data centers, as we will show. To address this issue, we investigate the dynamics of a data center cluster through analytical models in simple cases, and discrete event simulations based on real data. Our findings emphasize the significant impact of service demand variability, both in terms of requested cores and service times, and allow us to provide insight for enhancing data center performance. In particular, we show how data center performance can be improved thanks to the control of the interplay between service and waiting times through the assignment of cores to jobs.},
  archive      = {J_TPDS},
  author       = {Diletta Olliaro and Adityo Anggraito and Marco Ajmone Marsan and Simonetta Balsamo and Andrea Marin},
  doi          = {10.1109/TPDS.2024.3497792},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {120-132},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The impact of service demand variability on data center performance},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). H5Intent: Autotuning HDF5 with user intent. <em>TPDS</em>, <em>36</em>(2), 108-119. (<a href='https://doi.org/10.1109/TPDS.2024.3492704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of data management in HPC systems stems from the diversity in I/O behavior exhibited by new workloads, multistage workflows, and multitiered storage systems. The HDF5 library is a popular interface to interact with storage systems in HPC workloads. The library manages the complexity of diverse I/O behaviors by providing user-level configurations to optimize the I/O for HPC workloads. The HDF5 library exposes hundreds of configuration properties that can be set to alter how HDF5 manages I/O requests for better performance. However, determining which properties to set is quite challenging for users who lack expertise in HDF5 library internals. We propose a paradigm change through our H5Intent software, where users specify the intent of I/O operations and the software can set various HDF5 properties automatically to optimize the I/O behavior. This work demonstrates several use cases where mapping user-defined intents to HDF5 properties can be exploited to optimize I/O. In this study, we make three observations. First, I/O intents can accurately define HDF5 properties while managing conflicts between various properties and improving the I/O performance of microbenchmarks by up to 22×. Second, I/O intents can be efficiently passed to HDF5 with a small footprint of 6.74MB per node for thousands of intents per process. Third, an H5Intent VOL connector can dynamically map I/O intents to HDF5 properties for various I/O behaviors exhibited by our microbenchmark and improve I/O performance by up to 8.8×. Overall, H5Intent software improves the I/O performance of complex large-scale workloads we studied by up to 11×.},
  archive      = {J_TPDS},
  author       = {Hariharan Devarajan and Gerd Heber and Kathryn Mohror},
  doi          = {10.1109/TPDS.2024.3492704},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {108-119},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {H5Intent: Autotuning HDF5 with user intent},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dissecting the software-based measurement of CPU energy consumption: A comparative analysis. <em>TPDS</em>, <em>36</em>(1), 96-107. (<a href='https://doi.org/10.1109/TPDS.2024.3492336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information and Communications Technologies (ICT) are an increasingly important contributor to the environmental crisis. Computer scientists need tools for measuring the footprint of the code they produce and for optimizing it. Running Average Power Limit (RAPL) is a low-level interface designed by Intel that provides a measure of the energy consumption of a CPU (and more) without the need for additional hardware. Since 2017, it is available on most x86 processors, including AMD processors. More and more people are using RAPL for energy measurement, mostly like a black box without deep knowledge of its behavior. Unfortunately, this causes mistakes when implementing measurement tools. In this article, we propose to come back to the basic mechanisms that allow to use RAPL measurements and present a critical analysis of their operations. In addition to long-established mechanisms, we explore the suitability of the recent eBPF technology (formerly and abbreviation for extended Berkeley Packet Filter) for working with RAPL. We release an implementation in Rust that avoids the pitfalls we detected in existing tools, improving correctness, timing accuracy and performance, with desirable properties for monitoring and profiling parallel applications. We provide an experimental study with multiple benchmarks and processor models to evaluate the efficiency of the various mechanisms and their impact on parallel software. We show that no mechanism provides a significant performance advantage over the others. However, they differ significantly in terms of ease-of-use and resiliency. We believe that this work will help the community to develop correct, resilient and lightweight measurement tools.},
  archive      = {J_TPDS},
  author       = {Guillaume Raffin and Denis Trystram},
  doi          = {10.1109/TPDS.2024.3492336},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {96-107},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Dissecting the software-based measurement of CPU energy consumption: A comparative analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge data deduplication under uncertainties: A robust optimization approach. <em>TPDS</em>, <em>36</em>(1), 84-95. (<a href='https://doi.org/10.1109/TPDS.2024.3493959'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of mobile edge computing (MEC) in distributed systems has sparked increased attention toward edge data management. A conflict arises from the disparity between limited edge resources and the continuously expanding data requests for data storage, making the reduction of data storage costs a critical objective. Despite the extensive studies of edge data deduplication as a data reduction technique, existing deduplication methods encounter numerous challenges in MEC environments. These challenges stem from disparities between edge servers and cloud data center edge servers, as well as uncertainties such as user mobility, leading to insufficient robustness in deduplication decision-making. Consequently, this paper presents a robust optimization-based approach for the edge data deduplication problem. By accounting for uncertainties including the number of data requirements and edge server failures, we propose two distinct solving algorithms: uEDDE-C, a two-stage algorithm based on column-and-constraint generation, and uEDDE-A, an approximation algorithm to address the high computation overhead of uEDDE-C. Our method facilitates efficient data deduplication in volatile edge network environments and maintains robustness across various uncertain scenarios. We validate the performance and robustness of uEDDE-C and uEDDE-A through theoretical analysis and experimental evaluations. The extensive experimental results demonstrate that our approach significantly reduces data storage cost and data retrieval latency while ensuring reliability in real-world MEC environments.},
  archive      = {J_TPDS},
  author       = {Ruikun Luo and Qiang He and Mengxi Xu and Feifei Chen and Song Wu and Jing Yang and Yuan Gao and Hai Jin},
  doi          = {10.1109/TPDS.2024.3493959},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {84-95},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Edge data deduplication under uncertainties: A robust optimization approach},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retrospecting available CPU resources: SMT-aware scheduling to prevent SLA violations in data centers. <em>TPDS</em>, <em>36</em>(1), 67-83. (<a href='https://doi.org/10.1109/TPDS.2024.3494879'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article focuses on an understudied yet fundamental problem: existing methods typically average the utilization of multiple hardware threads to evaluate the available CPU resources. However, the approach could underestimate the actual usage of the underlying physical core for Simultaneous Multi-Threading (SMT) processors, leading to an overestimation of remaining resources. The overestimation propagates from microarchitecture to operating systems and cloud schedulers, which may misguide scheduling decisions, exacerbate CPU overcommitment, and increase Service Level Agreement (SLA) violations. To address the potential overestimation problem, we propose an SMT-aware and purely data-driven approach named Remaining CPU (RCPU) that reserves more CPU resources to restrict CPU overcommitment and prevent SLA violations. RCPU requires only a few modifications to the existing cloud infrastructures and can be scaled up to large data centers. Extensive evaluations in the data center proved that RCPU contributes to a reduction of SLA violations by 18% on average for 98% of all latency-sensitive applications. Under a benchmarking experiment, we prove that RCPU increases the accuracy by 69% in terms of Mean Absolute Error (MAE) compared to the state-of-the-art.},
  archive      = {J_TPDS},
  author       = {Haoyu Liao and Tong-yu Liu and Jianmei Guo and Bo Huang and Dingyu Yang and Jonathan Ding},
  doi          = {10.1109/TPDS.2024.3494879},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {67-83},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Retrospecting available CPU resources: SMT-aware scheduling to prevent SLA violations in data centers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ripple: Enabling decentralized data deduplication at the edge. <em>TPDS</em>, <em>36</em>(1), 55-66. (<a href='https://doi.org/10.1109/TPDS.2024.3493953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With its advantages in ensuring low data retrieval latency and reducing backhaul network traffic, edge computing is becoming a backbone solution for many latency-sensitive applications. An increasingly large number of data is being generated at the edge, stretching the limited capacity of edge storage systems. Improving resource utilization for edge storage systems has become a significant challenge in recent years. Existing solutions attempt to achieve this goal through data placement optimization, data partitioning, data sharing, etc. These approaches overlook the data redundancy in edge storage systems, which produces substantial storage resource wastage. This motivates the need for an approach for data deduplication at the edge. However, existing data deduplication methods rely on centralized control, which is not always feasible in practical edge computing environments. This article presents Ripple, the first approach that enables edge servers to deduplicate their data in a decentralized manner. At its core, it builds a data index for each edge server, enabling them to deduplicate data without central control. With Ripple, edge servers can 1) identify data duplicates; 2) remove redundant data without violating data retrieval latency constraints; and 3) ensure data availability after deduplication. The results of trace-driven experiments conducted in a testbed system demonstrate the usefulness of Ripple in practice. Compared with the state-of-the-art approach, Ripple improves the deduplication ratio by up to 16.79% and reduces data retrieval latency by an average of 60.42%.},
  archive      = {J_TPDS},
  author       = {Ruikun Luo and Qiang He and Feifei Chen and Song Wu and Hai Jin and Yun Yang},
  doi          = {10.1109/TPDS.2024.3493953},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {55-66},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Ripple: Enabling decentralized data deduplication at the edge},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced splitting: A framework for achieving zero-wait in the multiserver-job model. <em>TPDS</em>, <em>36</em>(1), 43-54. (<a href='https://doi.org/10.1109/TPDS.2024.3493631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new framework for designing nonpreemptive and job-size oblivious scheduling policies in the multiserver-job queueing model. The main requirement is to identify a static and balanced sub-partition of the server set and ensure that the servers in each set of that sub-partition can only handle jobs of a given class and in a first-come first-served order. A job class is determined by the number of servers to which it has exclusive access during its entire execution and the probability distribution of its service time. This approach aims to reduce delays by preventing small jobs from being blocked by larger ones that arrived first, and it is particularly beneficial when the job size variability intra resp. inter classes is small resp. large. In this setting, we propose a new scheduling policy, Balanced-Splitting. In our main results, we provide a sufficient condition for the stability of Balanced-Splitting and show that the resulting queueing probability, i.e., the probability that an arriving job needs to wait for processing upon arrival, vanishes in both the subcritical (the load is kept fixed to a constant less than one) and critical (the load approaches one from below) many-server limiting regimes. Crucial to our analysis is a connection with the M/GI/ $s$ / $s$ queue and Erlang’s loss formula, which allows our analysis to rely on fundamental results from queueing theory. Numerical simulations show that the proposed policy performs better than several preemptive/nonpreemptive size-aware/oblivious policies in various practical scenarios. This is also confirmed by simulations running on real traces from High Performance Computing (HPC) workloads. The delays induced by Balanced-Splitting are also competitive with those induced by state-of-the-art policies such as First-Fit-SRPT and ServerFilling-SRPT, though our approach has the advantage of not requiring preemption, nor the knowledge of job sizes.},
  archive      = {J_TPDS},
  author       = {Jonatha Anselmi and Josu Doncel},
  doi          = {10.1109/TPDS.2024.3493631},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {43-54},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Balanced splitting: A framework for achieving zero-wait in the multiserver-job model},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeHydra: Fault-tolerant edge data distribution based on erasure coding. <em>TPDS</em>, <em>36</em>(1), 29-42. (<a href='https://doi.org/10.1109/TPDS.2024.3493034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the edge computing environment, app vendors can distribute popular data from the cloud to edge servers to provide low-latency data retrieval. A key problem is how to distribute these data from the cloud to edge servers cost-effectively. Under current schemes, a file is divided into some data blocks for parallel transmissions from the cloud to target edge servers. Edge servers can then combine received data blocks to reconstruct the file. While this method expedites the data distribution process, it presents potential drawbacks. It is sensitive to transmission delays and transmission failures caused by runtime exceptions like network fluctuations and server failures. This paper presents EdgeHydra, the first edge data distribution scheme that tackles this challenge through fault tolerance based on erasure coding. Under EdgeHydra, a file is encoded into data blocks and parity blocks for parallel transmission from the cloud to target edge servers. An edge server can reconstruct the file upon the receipt of a sufficient number of these blocks without having to wait for all the blocks in transmission. It also innovatively employs a leaderless block supplement mechanism to ensure the receipt of sufficient blocks for individual target edge servers. These improve the robustness of the data distribution process significantly. Extensive experiments show that EdgeHydra can tolerate delays and failures in individual transmission links effectively, outperforming the state-of-the-art scheme by up to 50.54% in distribution time.},
  archive      = {J_TPDS},
  author       = {Qiang He and Guobiao Zhang and Jiawei Wang and Ruikun Luo and Xiaohai Dai and Yuchong Hu and Feifei Chen and Hai Jin and Yun Yang},
  doi          = {10.1109/TPDS.2024.3493034},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {29-42},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EdgeHydra: Fault-tolerant edge data distribution based on erasure coding},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Algorithms for data sharing-aware task allocation in edge computing systems. <em>TPDS</em>, <em>36</em>(1), 15-28. (<a href='https://doi.org/10.1109/TPDS.2024.3486184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing has been developed as a low-latency data driven computation paradigm close to the end user to maximize profit, and/or minimize energy consumption. Edge computing allows each user’s task to analyze locally-acquired sensor data at the edge to reduce the resource congestion and improve the efficiency of data processing. To reduce application latency and data transferred to edge servers it is essential to consider data sharing for some user tasks that operate on the same data items. In this article, we formulate the data sharing-aware allocation problem which has as objectives the maximization of profit and minimization of network traffic by considering data-sharing characteristics of tasks on servers. Because the problem is ${\sf NP-hard}$ , we design the ${\sf DSTA}$ algorithm to find a feasible solution in polynomial time. We investigate the approximation guarantees of ${\sf DSTA}$ by determining the approximation ratios with respect to the total profit and the amount of total data traffic in the edge network. We also design a variant of ${\sf DSTA}$ , called ${\sf DSTAR}$ that uses a smart rearrangement of tasks to allocate some of the unallocated tasks for increased total profit. We perform extensive experiments to investigate the performance of ${\sf DSTA}$ and ${\sf DSTAR}$ , and compare them with a representative greedy baseline that only maximizes profit. Our experimental analysis shows that, compared to the baseline, ${\sf DSTA}$ reduces the total data traffic in the edge network by up to 20% across 45 case study instances at a small profit loss. In addition, ${\sf DSTAR}$ increases the total profit by up to 27% and the number of allocated tasks by 25% compared to ${\sf DSTA}$ , all while limiting the increase of total data traffic in the network.},
  archive      = {J_TPDS},
  author       = {Sanaz Rabinia and Niloofar Didar and Marco Brocanelli and Daniel Grosu},
  doi          = {10.1109/TPDS.2024.3486184},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {15-28},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Algorithms for data sharing-aware task allocation in edge computing systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real relative encoding genetic algorithm for workflow scheduling in heterogeneous distributed computing systems. <em>TPDS</em>, <em>36</em>(1), 1-14. (<a href='https://doi.org/10.1109/TPDS.2024.3492210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel Real Relative encoding Genetic Algorithm (R $^{2}$ GA) to tackle the workflow scheduling problem in heterogeneous distributed computing systems (HDCS). R $^{2}$ GA employs a unique encoding mechanism, using real numbers to represent the relative positions of tasks in the schedulable task set. Decoding is performed by interpreting these real numbers in relation to the directed acyclic graph (DAG) of the workflow. This approach ensures that any sequence of randomly generated real numbers, produced by cross-over and mutation operations, can always be decoded into a valid solution, as the precedence constraints between tasks are explicitly defined by the DAG. The proposed encoding and decoding mechanism simplifies genetic operations and facilitates efficient exploration of the solution space. This inherent flexibility also allows R $^{2}$ GA to be easily adapted to various optimization scenarios in workflow scheduling within HDCS. Additionally, R $^{2}$ GA overcomes several issues associated with traditional genetic algorithms (GAs) and existing real-number encoding GAs, such as the generation of chromosomes that violate task precedence constraints and the strict limitations on gene value ranges. Experimental results show that R $^{2}$ GA consistently delivers superior performance in terms of solution quality and efficiency compared to existing techniques.},
  archive      = {J_TPDS},
  author       = {Junqiang Jiang and Zhifang Sun and Ruiqi Lu and Li Pan and Zebo Peng},
  doi          = {10.1109/TPDS.2024.3492210},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Real relative encoding genetic algorithm for workflow scheduling in heterogeneous distributed computing systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
